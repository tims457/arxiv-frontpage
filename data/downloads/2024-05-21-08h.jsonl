{"created":"2024-05-20 17:59:59","title":"Images that Sound: Composing Images and Sounds on a Single Canvas","abstract":"Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these spectrograms images that sound. Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt. Please see our project page for video results: https://ificl.github.io/images-that-sound/","sentences":["Spectrograms are 2D representations of sound that look very different from the images found in our visual world.","And natural images, when played as spectrograms, make unnatural sounds.","In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio.","We call these spectrograms images that sound.","Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space.","During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models.","Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt.","Please see our project page for video results: https://ificl.github.io/images-that-sound/"],"url":"http://arxiv.org/abs/2405.12221v1","category":"cs.CV"}
{"created":"2024-05-20 17:59:30","title":"Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo","abstract":"We present MVSGaussian, a new generalizable 3D Gaussian representation approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct unseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware Gaussian representations and decode them into Gaussian parameters. 2) To further enhance performance, we propose a hybrid Gaussian rendering that integrates an efficient volume rendering design for novel view synthesis. 3) To support fast fine-tuning for specific scenes, we introduce a multi-view geometric consistent aggregation strategy to effectively aggregate the point clouds generated by the generalizable model, serving as the initialization for per-scene optimization. Compared with previous generalizable NeRF-based methods, which typically require minutes of fine-tuning and seconds of rendering per image, MVSGaussian achieves real-time rendering with better synthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian achieves better view synthesis with less training computational cost. Extensive experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples datasets validate that MVSGaussian attains state-of-the-art performance with convincing generalizability, real-time rendering speed, and fast per-scene optimization.","sentences":["We present MVSGaussian, a new generalizable 3D Gaussian representation approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct unseen scenes.","Specifically, 1) we leverage MVS to encode geometry-aware Gaussian representations and decode them into Gaussian parameters.","2) To further enhance performance, we propose a hybrid Gaussian rendering that integrates an efficient volume rendering design for novel view synthesis.","3) To support fast fine-tuning for specific scenes, we introduce a multi-view geometric consistent aggregation strategy to effectively aggregate the point clouds generated by the generalizable model, serving as the initialization for per-scene optimization.","Compared with previous generalizable NeRF-based methods, which typically require minutes of fine-tuning and seconds of rendering per image, MVSGaussian achieves real-time rendering with better synthesis quality for each scene.","Compared with the vanilla 3D-GS, MVSGaussian achieves better view synthesis with less training computational cost.","Extensive experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples datasets validate that MVSGaussian attains state-of-the-art performance with convincing generalizability, real-time rendering speed, and fast per-scene optimization."],"url":"http://arxiv.org/abs/2405.12218v1","category":"cs.CV"}
{"created":"2024-05-20 17:59:21","title":"Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning","abstract":"Recent studies indicate that large multimodal models (LMMs) are highly robust against natural distribution shifts, often surpassing previous baselines. Despite this, domain-specific adaptation is still necessary, particularly in specialized areas like healthcare. Due to the impracticality of fine-tuning LMMs given their vast parameter space, this work investigates in-context learning (ICL) as an effective alternative for enhancing LMMs' adaptability. We find that the success of ICL heavily relies on the choice of demonstration, mirroring challenges seen in large language models but introducing unique complexities for LMMs facing distribution shifts. Our study addresses this by evaluating an unsupervised ICL method, TopKNearestPR, which selects in-context examples through a nearest example search based on feature similarity. We uncover that its effectiveness is limited by the deficiencies of pre-trained vision encoders under distribution shift scenarios. To address these challenges, we propose InvariantSelectPR, a novel method leveraging Class-conditioned Contrastive Invariance (CCI) for more robust demonstration selection. Specifically, CCI enhances pre-trained vision encoders by improving their discriminative capabilities across different classes and ensuring invariance to domain-specific variations. This enhancement allows the encoders to effectively identify and retrieve the most informative examples, which are then used to guide LMMs in adapting to new query samples under varying distributions. Our experiments show that InvariantSelectPR substantially improves the adaptability of LMMs, achieving significant performance gains on benchmark datasets, with a 34.2%$\\uparrow$ accuracy increase in 7-shot on Camelyon17 and 16.9%$\\uparrow$ increase in 7-shot on HAM10000 compared to the baseline zero-shot performance.","sentences":["Recent studies indicate that large multimodal models (LMMs) are highly robust against natural distribution shifts, often surpassing previous baselines.","Despite this, domain-specific adaptation is still necessary, particularly in specialized areas like healthcare.","Due to the impracticality of fine-tuning LMMs given their vast parameter space, this work investigates in-context learning (ICL) as an effective alternative for enhancing LMMs' adaptability.","We find that the success of ICL heavily relies on the choice of demonstration, mirroring challenges seen in large language models but introducing unique complexities for LMMs facing distribution shifts.","Our study addresses this by evaluating an unsupervised ICL method, TopKNearestPR, which selects in-context examples through a nearest example search based on feature similarity.","We uncover that its effectiveness is limited by the deficiencies of pre-trained vision encoders under distribution shift scenarios.","To address these challenges, we propose InvariantSelectPR, a novel method leveraging Class-conditioned Contrastive Invariance (CCI) for more robust demonstration selection.","Specifically, CCI enhances pre-trained vision encoders by improving their discriminative capabilities across different classes and ensuring invariance to domain-specific variations.","This enhancement allows the encoders to effectively identify and retrieve the most informative examples, which are then used to guide LMMs in adapting to new query samples under varying distributions.","Our experiments show that InvariantSelectPR substantially improves the adaptability of LMMs, achieving significant performance gains on benchmark datasets, with a 34.2%$\\uparrow$ accuracy increase in 7-shot on Camelyon17 and 16.9%$\\uparrow$ increase in 7-shot on HAM10000 compared to the baseline zero-shot performance."],"url":"http://arxiv.org/abs/2405.12217v1","category":"cs.CV"}
{"created":"2024-05-20 17:58:21","title":"Optimal tail estimates in $\u03b2$-ensembles and applications to last passage percolation","abstract":"Hermite and Laguerre $\\beta$-ensembles are important and well studied models in random matrix theory with special cases $\\beta=1,2,4$ corresponding to eigenvalues of classical random matrix ensembles. It is well known that the largest eigenvalues in these, under appropriate scaling, converge weakly to the Tracy-Widom $\\beta$ distribution whose distribution function $F_{\\beta}$ has asymptotics given by $1-F_{\\beta}(x)=\\exp\\left(-\\frac{2\\beta}{3}(1+o(1))x^{3/2}\\right)$ as $x\\to \\infty$ and $F_{\\beta}(x)=\\exp\\left(-\\frac{\\beta}{24}(1+o(1))|x|^3\\right)$ as $x\\to -\\infty$. Although tail estimates for the largest eigenvalues with correct exponents have been proved for the pre-limiting models, estimates with matching constants had not so far been established for general $\\beta$; even in the exactly solvable cases, some of the bounds were missing. In this paper, we prove upper and lower moderate deviation estimates for both tails with matching constants.   We illustrate the usefulness of these estimates by considering certain questions in planar exponential last passage percolation (LPP), a well-studied model in the KPZ universality class in which certain statistics have same distributions as largest eigenvalues in Laguerre $\\beta$-ensembles (for $\\beta=1,2,4$). Using our estimates in conjunction with a combination of old and new results on the LPP geometry, we obtain three laws of iterated logarithm including one which settles a conjecture of Ledoux (J. Theor. Probab., 2018). We expect that the sharp moderate deviation estimates will find many further applications in LPP problems and beyond.","sentences":["Hermite and Laguerre $\\beta$-ensembles are important and well studied models in random matrix theory with special cases $\\beta=1,2,4$ corresponding to eigenvalues of classical random matrix ensembles.","It is well known that the largest eigenvalues in these, under appropriate scaling, converge weakly to the Tracy-Widom $\\beta$ distribution whose distribution function $F_{\\beta}$ has asymptotics given by $1-F_{\\beta}(x)=\\exp\\left(-\\frac{2\\beta}{3}(1+o(1))x^{3/2}\\right)$ as $x\\to \\infty$ and $F_{\\beta}(x)=\\exp\\left(-\\frac{\\beta}{24}(1+o(1))|x|^3\\right)$ as $x\\to -\\infty$. Although tail estimates for the largest eigenvalues with correct exponents have been proved for the pre-limiting models, estimates with matching constants had not so far been established for general $\\beta$; even in the exactly solvable cases, some of the bounds were missing.","In this paper, we prove upper and lower moderate deviation estimates for both tails with matching constants.   ","We illustrate the usefulness of these estimates by considering certain questions in planar exponential last passage percolation (LPP), a well-studied model in the KPZ universality class in which certain statistics have same distributions as largest eigenvalues in Laguerre $\\beta$-ensembles (for $\\beta=1,2,4$).","Using our estimates in conjunction with a combination of old and new results on the LPP geometry, we obtain three laws of iterated logarithm including one which settles a conjecture of Ledoux (J. Theor.","Probab., 2018).","We expect that the sharp moderate deviation estimates will find many further applications in LPP problems and beyond."],"url":"http://arxiv.org/abs/2405.12215v1","category":"math.PR"}
{"created":"2024-05-20 17:57:01","title":"Octo: An Open-Source Generalist Robot Policy","abstract":"Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.","sentences":["Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly.","However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains.","In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation.","As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date.","It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs.","In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces.","We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models."],"url":"http://arxiv.org/abs/2405.12213v1","category":"cs.RO"}
{"created":"2024-05-20 17:55:56","title":"Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices","abstract":"Text-to-image (T2I) diffusion models achieve state-of-the-art results in image synthesis and editing. However, leveraging such pretrained models for video editing is considered a major challenge. Many existing works attempt to enforce temporal consistency in the edited video through explicit correspondence mechanisms, either in pixel space or between deep features. These methods, however, struggle with strong nonrigid motion. In this paper, we introduce a fundamentally different approach, which is based on the observation that spatiotemporal slices of natural videos exhibit similar characteristics to natural images. Thus, the same T2I diffusion model that is normally used only as a prior on video frames, can also serve as a strong prior for enhancing temporal consistency by applying it on spatiotemporal slices. Based on this observation, we present Slicedit, a method for text-based video editing that utilizes a pretrained T2I diffusion model to process both spatial and spatiotemporal slices. Our method generates videos that retain the structure and motion of the original video while adhering to the target text. Through extensive experiments, we demonstrate Slicedit's ability to edit a wide range of real-world videos, confirming its clear advantages compared to existing competing methods. Webpage: https://matankleiner.github.io/slicedit/","sentences":["Text-to-image (T2I) diffusion models achieve state-of-the-art results in image synthesis and editing.","However, leveraging such pretrained models for video editing is considered a major challenge.","Many existing works attempt to enforce temporal consistency in the edited video through explicit correspondence mechanisms, either in pixel space or between deep features.","These methods, however, struggle with strong nonrigid motion.","In this paper, we introduce a fundamentally different approach, which is based on the observation that spatiotemporal slices of natural videos exhibit similar characteristics to natural images.","Thus, the same T2I diffusion model that is normally used only as a prior on video frames, can also serve as a strong prior for enhancing temporal consistency by applying it on spatiotemporal slices.","Based on this observation, we present Slicedit, a method for text-based video editing that utilizes a pretrained T2I diffusion model to process both spatial and spatiotemporal slices.","Our method generates videos that retain the structure and motion of the original video while adhering to the target text.","Through extensive experiments, we demonstrate Slicedit's ability to edit a wide range of real-world videos, confirming its clear advantages compared to existing competing methods.","Webpage: https://matankleiner.github.io/slicedit/"],"url":"http://arxiv.org/abs/2405.12211v1","category":"cs.CV"}
{"created":"2024-05-20 17:45:26","title":"Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving","abstract":"Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans.   To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.","sentences":["Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes.","Today's best LLMs clearly possess some reasoning processes.","The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task.","We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels.","These coarse skill labels look interpretable to humans.   ","To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments.","(a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH.","(b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed.","Then it is presented with randomly selected exemplar solved questions associated with that skill label.","This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models.","The methodology presented is domain-agnostic, even though this article applies it to math problems."],"url":"http://arxiv.org/abs/2405.12205v1","category":"cs.AI"}
{"created":"2024-05-20 17:41:19","title":"Accelerating Relative Entropy Coding with Space Partitioning","abstract":"Relative entropy coding (REC) algorithms encode a random sample following a target distribution $Q$, using a coding distribution $P$ shared between the sender and receiver. Sadly, general REC algorithms suffer from prohibitive encoding times, at least on the order of $2^{D_{\\text{KL}}[Q||P]}$, and faster algorithms are limited to very specific settings. This work addresses this issue by introducing a REC scheme utilizing space partitioning to reduce runtime in practical scenarios. We provide theoretical analyses of our method and demonstrate its effectiveness with both toy examples and practical applications. Notably, our method successfully handles REC tasks with $D_{\\text{KL}}[Q||P]$ about three times what previous methods can manage and reduces the compression rate by approximately 5-15\\% in VAE-based lossless compression on MNIST and INR-based lossy compression on CIFAR-10 compared to previous methods, significantly improving the practicality of REC for neural compression.","sentences":["Relative entropy coding (REC) algorithms encode a random sample following a target distribution $Q$, using a coding distribution $P$ shared between the sender and receiver.","Sadly, general REC algorithms suffer from prohibitive encoding times, at least on the order of $2^{D_{\\text{KL}}[Q||P]}$, and faster algorithms are limited to very specific settings.","This work addresses this issue by introducing a REC scheme utilizing space partitioning to reduce runtime in practical scenarios.","We provide theoretical analyses of our method and demonstrate its effectiveness with both toy examples and practical applications.","Notably, our method successfully handles REC tasks with $D_{\\text{KL}}[Q||P]$ about three times what previous methods can manage and reduces the compression rate by approximately 5-15\\% in VAE-based lossless compression on MNIST and INR-based lossy compression on CIFAR-10 compared to previous methods, significantly improving the practicality of REC for neural compression."],"url":"http://arxiv.org/abs/2405.12203v1","category":"cs.IT"}
{"created":"2024-05-20 17:39:29","title":"Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution","abstract":"In this work, we present an arbitrary-scale super-resolution (SR) method to enhance the resolution of scientific data, which often involves complex challenges such as continuity, multi-scale physics, and the intricacies of high-frequency signals. Grounded in operator learning, the proposed method is resolution-invariant. The core of our model is a hierarchical neural operator that leverages a Galerkin-type self-attention mechanism, enabling efficient learning of mappings between function spaces. Sinc filters are used to facilitate the information transfer across different levels in the hierarchy, thereby ensuring representation equivalence in the proposed neural operator. Additionally, we introduce a learnable prior structure that is derived from the spectral resizing of the input data. This loss prior is model-agnostic and is designed to dynamically adjust the weighting of pixel contributions, thereby balancing gradients effectively across the model. We conduct extensive experiments on diverse datasets from different domains and demonstrate consistent improvements compared to strong baselines, which consist of various state-of-the-art SR methods.","sentences":["In this work, we present an arbitrary-scale super-resolution (SR) method to enhance the resolution of scientific data, which often involves complex challenges such as continuity, multi-scale physics, and the intricacies of high-frequency signals.","Grounded in operator learning, the proposed method is resolution-invariant.","The core of our model is a hierarchical neural operator that leverages a Galerkin-type self-attention mechanism, enabling efficient learning of mappings between function spaces.","Sinc filters are used to facilitate the information transfer across different levels in the hierarchy, thereby ensuring representation equivalence in the proposed neural operator.","Additionally, we introduce a learnable prior structure that is derived from the spectral resizing of the input data.","This loss prior is model-agnostic and is designed to dynamically adjust the weighting of pixel contributions, thereby balancing gradients effectively across the model.","We conduct extensive experiments on diverse datasets from different domains and demonstrate consistent improvements compared to strong baselines, which consist of various state-of-the-art SR methods."],"url":"http://arxiv.org/abs/2405.12202v1","category":"cs.CV"}
{"created":"2024-05-20 17:37:10","title":"Bond percolation games on the $2$-dimensional square lattice, and ergodicity of associated probabilistic cellular automata","abstract":"We consider bond percolation games on the $2$-dimensional square lattice in which each edge (that is either between the sites $(x,y)$ and $(x+1,y)$ or between the sites $(x,y)$ and $(x,y+1)$, for all $(x,y) \\in \\mathbb{Z}^{2}$) has been assigned, independently, a label that reads \"trap\" with probability $p$, \"target\" with probability $q$, and \"open\" with probability $1-p-q$. Once a realization of this labeling is generated, it is revealed in its entirety to the players before the game starts. The game involves a single token, initially placed at the origin, and two players who take turns to make moves. A move involves relocating the token from where it is currently located, say the site $(x,y)$, to any one of $(x+1,y)$ and $(x,y+1)$. A player wins if she is able to move the token along an edge labeled a target, or if she is able to force her opponent to move the token along an edge labeled a trap. The game is said to result in a draw if it continues indefinitely (i.e.\\ with the token always being moved along open edges). We ask the question: for what values of $p$ and $q$ is the probability of draw equal to $0$? By establishing a close connection between the event of draw and the ergodicity of a suitably defined probabilistic cellualar automaton, we are able to show that the probability of draw is $0$ when $p > 0.157175$ and $q=0$, and when $p=q \\geqslant 0.10883$.","sentences":["We consider bond percolation games on the $2$-dimensional square lattice in which each edge (that is either between the sites $(x,y)$ and $(x+1,y)$ or between the sites $(x,y)$ and $(x,y+1)$, for all $(x,y) \\in \\mathbb{Z}^{2}$) has been assigned, independently, a label that reads \"trap\" with probability $p$, \"target\" with probability $q$, and \"open\" with probability $1-p-q$. Once a realization of this labeling is generated, it is revealed in its entirety to the players before the game starts.","The game involves a single token, initially placed at the origin, and two players who take turns to make moves.","A move involves relocating the token from where it is currently located, say the site $(x,y)$, to any one of $(x+1,y)$ and $(x,y+1)$.","A player wins if she is able to move the token along an edge labeled a target, or if she is able to force her opponent to move the token along an edge labeled a trap.","The game is said to result in a draw if it continues indefinitely (i.e.\\ with the token always being moved along open edges).","We ask the question: for what values of $p$ and $q$ is the probability of draw equal to $0$?","By establishing a close connection between the event of draw and the ergodicity of a suitably defined probabilistic cellualar automaton, we are able to show that the probability of draw is $0$ when $p > 0.157175$ and $q=0$, and when $p=q \\geqslant 0.10883$."],"url":"http://arxiv.org/abs/2405.12199v1","category":"math.PR"}
{"created":"2024-05-20 17:33:36","title":"Near-horizon properties of trajectories with finite force relevant for Ba\u00f1ados-Silk-West effect","abstract":"According to the Banados-SIlk-West (BSW) effect, two particles moving towards a black hole, can collide near the horizon with an unbounded energy in the center of mass frame. This requires one of particles to have fine-tuned parameters in such a way that the time component of generalized momentum is zero $X=0$. Thus the existence of such trjectories is a necessary condition for the BSW effect. However, it is insufficient since the forward-in-time condition requires $X>0$ outside the horizon. We examine this condition for different types of partricles and horizons and find configurations for which the BSW effect is possible. In doing so, we take into account a finite force of unspesified nature exerted on particles. It includes relationships between numbers characterizing the rate with which four-velocity, acceleration and metric functions change near the horizon. For some aforementioned relations, parameters of a system control the sign of $X$, in other cases they are required for $X$ to be real quantity. In the simplest case of free particles the BSW effect for the Kerr or Kerr-Newman black hole is impossible if a fine-tuned particle has a negative energy, so in this sense combination of the Penrose process and the BSW effect is forbidden.","sentences":["According to the Banados-SIlk-West (BSW) effect, two particles moving towards a black hole, can collide near the horizon with an unbounded energy in the center of mass frame.","This requires one of particles to have fine-tuned parameters in such a way that the time component of generalized momentum is zero $X=0$.","Thus the existence of such trjectories is a necessary condition for the BSW effect.","However, it is insufficient since the forward-in-time condition requires $X>0$ outside the horizon.","We examine this condition for different types of partricles and horizons and find configurations for which the BSW effect is possible.","In doing so, we take into account a finite force of unspesified nature exerted on particles.","It includes relationships between numbers characterizing the rate with which four-velocity, acceleration and metric functions change near the horizon.","For some aforementioned relations, parameters of a system control the sign of $X$, in other cases they are required for $X$ to be real quantity.","In the simplest case of free particles the BSW effect for the Kerr or Kerr-Newman black hole is impossible if a fine-tuned particle has a negative energy, so in this sense combination of the Penrose process and the BSW effect is forbidden."],"url":"http://arxiv.org/abs/2405.12198v1","category":"gr-qc"}
{"created":"2024-05-20 17:33:00","title":"Automated Hardware Logic Obfuscation Framework Using GPT","abstract":"Obfuscation stands as a promising solution for safeguarding hardware intellectual property (IP) against a spectrum of threats including reverse engineering, IP piracy, and tampering. In this paper, we introduce Obfus-chat, a novel framework leveraging Generative Pre-trained Transformer (GPT) models to automate the obfuscation process. The proposed framework accepts hardware design netlists and key sizes as inputs, and autonomously generates obfuscated code tailored to enhance security. To evaluate the effectiveness of our approach, we employ the Trust-Hub Obfuscation Benchmark for comparative analysis. We employed SAT attacks to assess the security of the design, along with functional verification procedures to ensure that the obfuscated design remains consistent with the original. Our results demonstrate the efficacy and efficiency of the proposed framework in fortifying hardware IP against potential threats, thus providing a valuable contribution to the field of hardware security.","sentences":["Obfuscation stands as a promising solution for safeguarding hardware intellectual property (IP) against a spectrum of threats including reverse engineering, IP piracy, and tampering.","In this paper, we introduce Obfus-chat, a novel framework leveraging Generative Pre-trained Transformer (GPT) models to automate the obfuscation process.","The proposed framework accepts hardware design netlists and key sizes as inputs, and autonomously generates obfuscated code tailored to enhance security.","To evaluate the effectiveness of our approach, we employ the Trust-Hub Obfuscation Benchmark for comparative analysis.","We employed SAT attacks to assess the security of the design, along with functional verification procedures to ensure that the obfuscated design remains consistent with the original.","Our results demonstrate the efficacy and efficiency of the proposed framework in fortifying hardware IP against potential threats, thus providing a valuable contribution to the field of hardware security."],"url":"http://arxiv.org/abs/2405.12197v1","category":"cs.CR"}
{"created":"2024-05-20 17:31:16","title":"Developers' Perceptions on the Impact of ChatGPT in Software Development: A Survey","abstract":"As Large Language Models (LLMs), including ChatGPT and analogous systems, continue to advance, their robust natural language processing capabilities and diverse applications have garnered considerable attention. Nonetheless, despite the increasing acknowledgment of the convergence of Artificial Intelligence (AI) and Software Engineering (SE), there is a lack of studies involving the impact of this convergence on the practices and perceptions of software developers. Understanding how software developers perceive and engage with AI tools, such as ChatGPT, is essential for elucidating the impact and potential challenges of incorporating AI-driven tools in the software development process. In this paper, we conducted a survey with 207 software developers to understand the impact of ChatGPT on software quality, productivity, and job satisfaction. Furthermore, the study delves into developers' expectations regarding future adaptations of ChatGPT, concerns about potential job displacement, and perspectives on regulatory interventions.","sentences":["As Large Language Models (LLMs), including ChatGPT and analogous systems, continue to advance, their robust natural language processing capabilities and diverse applications have garnered considerable attention.","Nonetheless, despite the increasing acknowledgment of the convergence of Artificial Intelligence (AI) and Software Engineering (SE), there is a lack of studies involving the impact of this convergence on the practices and perceptions of software developers.","Understanding how software developers perceive and engage with AI tools, such as ChatGPT, is essential for elucidating the impact and potential challenges of incorporating AI-driven tools in the software development process.","In this paper, we conducted a survey with 207 software developers to understand the impact of ChatGPT on software quality, productivity, and job satisfaction.","Furthermore, the study delves into developers' expectations regarding future adaptations of ChatGPT, concerns about potential job displacement, and perspectives on regulatory interventions."],"url":"http://arxiv.org/abs/2405.12195v1","category":"cs.SE"}
{"created":"2024-05-20 17:29:44","title":"Histotripsy of blood clots within a hollow cylindrical transducer for aspiration thrombectomy applications","abstract":"Thrombolytic occlusions in stroke, pulmonary embolism and the peripheral vasculature are increasingly treated with aspiration, a catheter-based approach that employs suction to extract clots through a hollow catheter lumen. Unfortunately, aspiration is frequently unsuccessful in extracting more challenging clots, which can become corked in the distal tip. We hypothesize that clot extraction can be enhanced by using histotripsy to degrade the mechanical integrity of clot material within the lumen of a hollow cylindrical transducer which can be situated at the tip of an aspiration catheter. To demonstrate the feasibility of degrading clot material within the lumen of a hollow cylindrical transducer, the effect of pulsing schemes on lesion generation within clots was assessed using a retracted clot model. A radially polarized cylindrical transducer (2.5 mm and 3.3 mm for inner and outer diameter, 2.5 mm length, PZT) working at 6.1 MHz was used to degrade retracted porcine clots with pulse lengths of 10, 20, and 100 us, pulse repetition frequencies (PRF) of 100, 500, and 1000 Hz, using treatment times from 0.1 to 10 seconds (n = 5 clots per condition). 3D ultrasound scans and bisected optical examinations of treated clots confirmed the formation of liquified zones. Lesions could form within 0.1 seconds along the central axis of the transducer and then grow in diameter and length over time. The lesion volume was found to be highly dependent on the exposure scheme, with the largest lesion volume associated with the 10 us pulse length 1000 kHz PRF case. Collectively these results demonstrate the feasibility of degrading blood clots within hollow cylindrical transducers, which suggests their potential for enhancing aspiration based mechanical thrombectomy.","sentences":["Thrombolytic occlusions in stroke, pulmonary embolism and the peripheral vasculature are increasingly treated with aspiration, a catheter-based approach that employs suction to extract clots through a hollow catheter lumen.","Unfortunately, aspiration is frequently unsuccessful in extracting more challenging clots, which can become corked in the distal tip.","We hypothesize that clot extraction can be enhanced by using histotripsy to degrade the mechanical integrity of clot material within the lumen of a hollow cylindrical transducer which can be situated at the tip of an aspiration catheter.","To demonstrate the feasibility of degrading clot material within the lumen of a hollow cylindrical transducer, the effect of pulsing schemes on lesion generation within clots was assessed using a retracted clot model.","A radially polarized cylindrical transducer (2.5 mm and 3.3 mm for inner and outer diameter, 2.5 mm length, PZT) working at 6.1 MHz was used to degrade retracted porcine clots with pulse lengths of 10, 20, and 100 us, pulse repetition frequencies (PRF) of 100, 500, and 1000 Hz, using treatment times from 0.1 to 10 seconds (n = 5 clots per condition).","3D ultrasound scans and bisected optical examinations of treated clots confirmed the formation of liquified zones.","Lesions could form within 0.1 seconds along the central axis of the transducer and then grow in diameter and length over time.","The lesion volume was found to be highly dependent on the exposure scheme, with the largest lesion volume associated with the 10 us pulse length 1000 kHz","PRF case.","Collectively these results demonstrate the feasibility of degrading blood clots within hollow cylindrical transducers, which suggests their potential for enhancing aspiration based mechanical thrombectomy."],"url":"http://arxiv.org/abs/2405.12194v1","category":"physics.med-ph"}
{"created":"2024-05-20 17:26:43","title":"The Narrow Depth and Breadth of Corporate Responsible AI Research","abstract":"The transformative potential of AI presents remarkable opportunities, but also significant risks, underscoring the importance of responsible AI development and deployment. Despite a growing emphasis on this area, there is limited understanding of industry's engagement in responsible AI research, i.e., the critical examination of AI's ethical, social, and legal dimensions. To address this gap, we analyzed over 6 million peer-reviewed articles and 32 million patent citations using multiple methods across five distinct datasets to quantify industry's engagement. Our findings reveal that the majority of AI firms show limited or no engagement in this critical subfield of AI. We show a stark disparity between industry's dominant presence in conventional AI research and its limited engagement in responsible AI. Leading AI firms exhibit significantly lower output in responsible AI research compared to their conventional AI research and the contributions of leading academic institutions. Our linguistic analysis documents a narrower scope of responsible AI research within industry, with a lack of diversity in key topics addressed. Our large-scale patent citation analysis uncovers a pronounced disconnect between responsible AI research and the commercialization of AI technologies, suggesting that industry patents rarely build upon insights generated by the responsible AI literature. This gap highlights the potential for AI development to diverge from a socially optimal path, risking unintended consequences due to insufficient consideration of ethical and societal implications. Our results highlight the urgent need for industry to publicly engage in responsible AI research to absorb academic knowledge, cultivate public trust, and proactively mitigate AI-induced societal harms.","sentences":["The transformative potential of AI presents remarkable opportunities, but also significant risks, underscoring the importance of responsible AI development and deployment.","Despite a growing emphasis on this area, there is limited understanding of industry's engagement in responsible AI research, i.e., the critical examination of AI's ethical, social, and legal dimensions.","To address this gap, we analyzed over 6 million peer-reviewed articles and 32 million patent citations using multiple methods across five distinct datasets to quantify industry's engagement.","Our findings reveal that the majority of AI firms show limited or no engagement in this critical subfield of AI.","We show a stark disparity between industry's dominant presence in conventional AI research and its limited engagement in responsible AI.","Leading AI firms exhibit significantly lower output in responsible AI research compared to their conventional AI research and the contributions of leading academic institutions.","Our linguistic analysis documents a narrower scope of responsible AI research within industry, with a lack of diversity in key topics addressed.","Our large-scale patent citation analysis uncovers a pronounced disconnect between responsible AI research and the commercialization of AI technologies, suggesting that industry patents rarely build upon insights generated by the responsible AI literature.","This gap highlights the potential for AI development to diverge from a socially optimal path, risking unintended consequences due to insufficient consideration of ethical and societal implications.","Our results highlight the urgent need for industry to publicly engage in responsible AI research to absorb academic knowledge, cultivate public trust, and proactively mitigate AI-induced societal harms."],"url":"http://arxiv.org/abs/2405.12193v1","category":"cs.CY"}
{"created":"2024-05-20 17:25:40","title":"Disorder effects in planar semiconductor-superconductor structures: Majorana wires versus Josephson junctions","abstract":"Disorder effects in hybrid semiconductor-superconductor (SM-SC) nanowires, widely recognized as the main obstacle to realizing stable Majorana zero modes (MZMs) in these structures, have been systematically investigated theoretically in recent years. However, there are no corresponding detailed studies of disorder effects in planar Josephson junction (JJ) structures, which represent a promising alternative to the Majorana nanowire platform. In this paper, we perform a numerical analysis of the low-energy physics of JJ structures based on an effective microscopic model that includes two types of disorder, charge impurities inside the semiconductor and roughness on the surface of the superconducting film. We consider different parameter regimes, including low and high chemical potential values, weak and strong effective SM-SC coupling strengths, and weak and strong disorder strengths. The results are benchmarked using disordered hybrid nanowires realized in planar SM-SC structures similar to those involved in the fabrication of Josephson junctions and having similar model parameters and disorder strengths. We find that the topological superconducting phase hosted by a JJ structure is, generally, more robust against disorder than the topological superconductivity realized in a hybrid nanowire with similar parameters. On the other hand, we find that operating the JJ in a regime characterized by large values of chemical potential results in huge finite-size effects that can destroy the stability of MZMs.","sentences":["Disorder effects in hybrid semiconductor-superconductor (SM-SC) nanowires, widely recognized as the main obstacle to realizing stable Majorana zero modes (MZMs) in these structures, have been systematically investigated theoretically in recent years.","However, there are no corresponding detailed studies of disorder effects in planar Josephson junction (JJ) structures, which represent a promising alternative to the Majorana nanowire platform.","In this paper, we perform a numerical analysis of the low-energy physics of JJ structures based on an effective microscopic model that includes two types of disorder, charge impurities inside the semiconductor and roughness on the surface of the superconducting film.","We consider different parameter regimes, including low and high chemical potential values, weak and strong effective SM-SC coupling strengths, and weak and strong disorder strengths.","The results are benchmarked using disordered hybrid nanowires realized in planar SM-SC structures similar to those involved in the fabrication of Josephson junctions and having similar model parameters and disorder strengths.","We find that the topological superconducting phase hosted by a JJ structure is, generally, more robust against disorder than the topological superconductivity realized in a hybrid nanowire with similar parameters.","On the other hand, we find that operating the JJ in a regime characterized by large values of chemical potential results in huge finite-size effects that can destroy the stability of MZMs."],"url":"http://arxiv.org/abs/2405.12192v1","category":"cond-mat.supr-con"}
{"created":"2024-05-20 17:20:56","title":"Cosserat elasticity as the weak-field limit of Einstein--Cartan relativity","abstract":"The weak-field limit of Einstein--Cartan (EC) relativity is studied. The equations of EC theory are rewritten such that they formally resemble those of Einstein General Relativity (EGR); this allows ideas from post-Newtonian theory to be imported without essential change. The equations of motion are then written both at first post-Newtonian (1PN) order and at 1.5PN order. EC theory's 1PN equations of motion are found to be those of a micropolar/Cosserat elastic medium, along with a decoupled evolution equation for non-classical, spin-related fields. It seems that a necessary condition for these results to hold is that one chooses the non-classical fields to scale with the speed of light in a certain empirically reasonable way. Finally, the 1.5PN equations give greater insight into the coupling between energy-momentum and spin within slowly moving, weakly gravitating matter. Specifically, the weakly relativistic modifications to Cosserat theory involve a gravitational torque and an augmentation of the gravitational force due to a `dynamic mass moment density' with an accompanying `dynamic mass moment density flux', and new forms of linear momentum density captured by a `dynamic mass density flux' and a `dynamic momentum density'.","sentences":["The weak-field limit of Einstein--Cartan (EC) relativity is studied.","The equations of EC theory are rewritten such that they formally resemble those of Einstein General Relativity (EGR); this allows ideas from post-Newtonian theory to be imported without essential change.","The equations of motion are then written both at first post-Newtonian (1PN) order and at 1.5PN order.","EC theory's 1PN equations of motion are found to be those of a micropolar/Cosserat elastic medium, along with a decoupled evolution equation for non-classical, spin-related fields.","It seems that a necessary condition for these results to hold is that one chooses the non-classical fields to scale with the speed of light in a certain empirically reasonable way.","Finally, the 1.5PN equations give greater insight into the coupling between energy-momentum and spin within slowly moving, weakly gravitating matter.","Specifically, the weakly relativistic modifications to Cosserat theory involve a gravitational torque and an augmentation of the gravitational force due to a `dynamic mass moment density' with an accompanying `dynamic mass moment density flux', and new forms of linear momentum density captured by a `dynamic mass density flux' and a `dynamic momentum density'."],"url":"http://arxiv.org/abs/2405.12188v1","category":"gr-qc"}
{"created":"2024-05-20 17:11:37","title":"Robust VAR Capability Curve of DER with Uncertain Renewable Generation","abstract":"Active distribution system with high penetration of inverter based distributed energy resources (DER) can be utilized for VAR-related ancillary services. To utilize the DER flexibility, transmission system operator (TSO) must be presented with the aggregated DER flexibility of distribution system. However, the uncertainty in renewable generation questions the credibility of aggregated capability curve in practice. In this paper, we incorporate the uncertainty into aggregation process to develop a robust capability curve while preserving the real physics (unbalance and lossy nature) of distribution system. Statistical inference method is employed to quantify uncertainty in solar generation and quantified uncertainty is integrated into a chance constrained optimal power flow (OPF). It provides the grid operator with the dispatchable aggregated reactive power capability. The resulting capability curve with the associated probability can be harnessed by the TSO for decision making for both planning and operation.","sentences":["Active distribution system with high penetration of inverter based distributed energy resources (DER) can be utilized for VAR-related ancillary services.","To utilize the DER flexibility, transmission system operator (TSO) must be presented with the aggregated DER flexibility of distribution system.","However, the uncertainty in renewable generation questions the credibility of aggregated capability curve in practice.","In this paper, we incorporate the uncertainty into aggregation process to develop a robust capability curve while preserving the real physics (unbalance and lossy nature) of distribution system.","Statistical inference method is employed to quantify uncertainty in solar generation and quantified uncertainty is integrated into a chance constrained optimal power flow (OPF).","It provides the grid operator with the dispatchable aggregated reactive power capability.","The resulting capability curve with the associated probability can be harnessed by the TSO for decision making for both planning and operation."],"url":"http://arxiv.org/abs/2405.12184v1","category":"eess.SY"}
{"created":"2024-05-20 17:09:58","title":"Multi-order Graph Clustering with Adaptive Node-level Weight Learning","abstract":"Current graph clustering methods emphasize individual node and edge con nections, while ignoring higher-order organization at the level of motif. Re cently, higher-order graph clustering approaches have been designed by motif based hypergraphs. However, these approaches often suffer from hypergraph fragmentation issue seriously, which degrades the clustering performance greatly. Moreover, real-world graphs usually contain diverse motifs, with nodes participating in multiple motifs. A key challenge is how to achieve precise clustering results by integrating information from multiple motifs at the node level. In this paper, we propose a multi-order graph clustering model (MOGC) to integrate multiple higher-order structures and edge connections at node level. MOGC employs an adaptive weight learning mechanism to au tomatically adjust the contributions of different motifs for each node. This not only tackles hypergraph fragmentation issue but enhances clustering accuracy. MOGC is efficiently solved by an alternating minimization algo rithm. Experiments on seven real-world datasets illustrate the effectiveness of MOGC.","sentences":["Current graph clustering methods emphasize individual node and edge con nections, while ignoring higher-order organization at the level of motif.","Re cently, higher-order graph clustering approaches have been designed by motif based hypergraphs.","However, these approaches often suffer from hypergraph fragmentation issue seriously, which degrades the clustering performance greatly.","Moreover, real-world graphs usually contain diverse motifs, with nodes participating in multiple motifs.","A key challenge is how to achieve precise clustering results by integrating information from multiple motifs at the node level.","In this paper, we propose a multi-order graph clustering model (MOGC) to integrate multiple higher-order structures and edge connections at node level.","MOGC employs an adaptive weight learning mechanism to au tomatically adjust the contributions of different motifs for each node.","This not only tackles hypergraph fragmentation issue but enhances clustering accuracy.","MOGC is efficiently solved by an alternating minimization algo rithm.","Experiments on seven real-world datasets illustrate the effectiveness of MOGC."],"url":"http://arxiv.org/abs/2405.12183v1","category":"cs.LG"}
{"created":"2024-05-20 17:06:24","title":"Building Temporal Kernels with Orthogonal Polynomials","abstract":"We introduce a class of models named PLEIADES (PoLynomial Expansion In Adaptive Distributed Event-based Systems), which contains temporal convolution kernels generated from orthogonal polynomial basis functions. We focus on interfacing these networks with event-based data to perform online spatiotemporal classification and detection with low latency. By virtue of using structured temporal kernels and event-based data, we have the freedom to vary the sample rate of the data along with the discretization step-size of the network without additional finetuning. We experimented with three event-based benchmarks and obtained state-of-the-art results on all three by large margins with significantly smaller memory and compute costs. We achieved: 1) 99.59% accuracy with 192K parameters on the DVS128 hand gesture recognition dataset and 100% with a small additional output filter; 2) 99.58% test accuracy with 277K parameters on the AIS 2024 eye tracking challenge; and 3) 0.556 mAP with 576k parameters on the PROPHESEE 1 Megapixel Automotive Detection Dataset.","sentences":["We introduce a class of models named PLEIADES (PoLynomial Expansion In Adaptive Distributed Event-based Systems), which contains temporal convolution kernels generated from orthogonal polynomial basis functions.","We focus on interfacing these networks with event-based data to perform online spatiotemporal classification and detection with low latency.","By virtue of using structured temporal kernels and event-based data, we have the freedom to vary the sample rate of the data along with the discretization step-size of the network without additional finetuning.","We experimented with three event-based benchmarks and obtained state-of-the-art results on all three by large margins with significantly smaller memory and compute costs.","We achieved: 1) 99.59% accuracy with 192K parameters on the DVS128 hand gesture recognition dataset and 100% with a small additional output filter; 2) 99.58% test accuracy with 277K parameters on the AIS 2024 eye tracking challenge; and 3) 0.556 mAP with 576k parameters on the PROPHESEE 1 Megapixel Automotive Detection Dataset."],"url":"http://arxiv.org/abs/2405.12179v1","category":"cs.LG"}
{"created":"2024-05-20 17:03:24","title":"Topological superconductivity in Fibonacci quasicrystals","abstract":"We investigate the properties of a Fibonacci quasicrystal (QC) arrangement of a one-dimensional topological superconductor, such as a magnetic atom chain deposited on a superconducting surface. We uncover a general mutually exclusive competition between the QC properties and the topological superconducting phase with Majorana bound states (MBS): there are no MBS inside the QC gaps and the MBS never behaves as QC subgap states, and likewise, no critical, or winding, QC subgap states exist inside the topological superconducting gaps. Surprisingly, despite this competition, we find that the QC is still highly beneficial for realizing topological superconductivity with MBS. It both leads to additional large nontrivial regions with MBS in parameter space, that are topologically trivial in crystalline systems, and increases the topological gap protecting the MBS. We also find that shorter approximants of the Fibonacci QC display the largest benefits. As a consequence, our results promote QCs, and especially their short approximants, as an appealing platform for improved experimental possibilities to realize MBS as well as generally highlights the fundamental interplay between different topologies.","sentences":["We investigate the properties of a Fibonacci quasicrystal (QC) arrangement of a one-dimensional topological superconductor, such as a magnetic atom chain deposited on a superconducting surface.","We uncover a general mutually exclusive competition between the QC properties and the topological superconducting phase with Majorana bound states (MBS): there are no MBS inside the QC gaps and the MBS never behaves as QC subgap states, and likewise, no critical, or winding, QC subgap states exist inside the topological superconducting gaps.","Surprisingly, despite this competition, we find that the QC is still highly beneficial for realizing topological superconductivity with MBS.","It both leads to additional large nontrivial regions with MBS in parameter space, that are topologically trivial in crystalline systems, and increases the topological gap protecting the MBS.","We also find that shorter approximants of the Fibonacci QC display the largest benefits.","As a consequence, our results promote QCs, and especially their short approximants, as an appealing platform for improved experimental possibilities to realize MBS as well as generally highlights the fundamental interplay between different topologies."],"url":"http://arxiv.org/abs/2405.12178v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 17:02:18","title":"Establishing Trust in the Beyond-5G Core Network using Trusted Execution Environments","abstract":"The fifth generation (5G) of cellular networks starts a paradigm shift from the traditional monolithic system design to a Service Based Architecture, that fits modern performance requirements and scales efficiently to new services. This paradigm will be the foundation of future cellular core networks beyond 5G. The new architecture splits network functionalities into smaller logical entities that can be disaggregated logically, physically, and geographically. This affords interoperability between the mobile network operators and commercial software and hardware vendors or cloud providers. By making use of commodity services and products, this system construct inherits the vulnerabilities in those underlying technologies, thereby increasing its attack surface and requiring a rigorous security analysis. In this work, we review the security implications introduced in B5G networks, and the security mechanisms that are supported by the 5G standard. We emphasize on the support of Zero Trust Architecture in 5G and its relevance in decentralized deployments. We revisit the definition of trust in modern enterprise network operations and identify important Zero Trust properties that are weakened by the nature of cloud deployments. To that end, we propose a vertical extension of Zero Trust, namely, Zero Trust Execution, to model untrusted execution environments, and we provide an analysis on how to establish trust in Beyond-5G network architectures using Trusted Execution Environments. Our analysis shows how our model architecture handles the increased attack surface and reinforces the Zero Trust Architecture principles in the 5G Core, without any changes to the 5G standard. Finally, we provide experimental results over a 5G testbed using Open5GS and UERANSIM that demonstrate minimal performance overhead, and a monetary cost evaluation.","sentences":["The fifth generation (5G) of cellular networks starts a paradigm shift from the traditional monolithic system design to a Service Based Architecture, that fits modern performance requirements and scales efficiently to new services.","This paradigm will be the foundation of future cellular core networks beyond 5G.","The new architecture splits network functionalities into smaller logical entities that can be disaggregated logically, physically, and geographically.","This affords interoperability between the mobile network operators and commercial software and hardware vendors or cloud providers.","By making use of commodity services and products, this system construct inherits the vulnerabilities in those underlying technologies, thereby increasing its attack surface and requiring a rigorous security analysis.","In this work, we review the security implications introduced in B5G networks, and the security mechanisms that are supported by the 5G standard.","We emphasize on the support of Zero Trust Architecture in 5G and its relevance in decentralized deployments.","We revisit the definition of trust in modern enterprise network operations and identify important Zero Trust properties that are weakened by the nature of cloud deployments.","To that end, we propose a vertical extension of Zero Trust, namely, Zero Trust Execution, to model untrusted execution environments, and we provide an analysis on how to establish trust in Beyond-5G network architectures using Trusted Execution Environments.","Our analysis shows how our model architecture handles the increased attack surface and reinforces the Zero Trust Architecture principles in the 5G Core, without any changes to the 5G standard.","Finally, we provide experimental results over a 5G testbed using Open5GS and UERANSIM that demonstrate minimal performance overhead, and a monetary cost evaluation."],"url":"http://arxiv.org/abs/2405.12177v1","category":"cs.CR"}
{"created":"2024-05-20 16:58:02","title":"CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models","abstract":"Text-to-Table aims to generate structured tables to convey the key information from unstructured documents. Existing text-to-table datasets are typically oriented English, limiting the research in non-English languages. Meanwhile, the emergence of large language models (LLMs) has shown great success as general task solvers in multi-lingual settings (e.g., ChatGPT), theoretically enabling text-to-table in other languages. In this paper, we propose a Chinese text-to-table dataset, CT-Eval, to benchmark LLMs on this task. Our preliminary analysis of English text-to-table datasets highlights two key factors for dataset construction: data diversity and data hallucination. Inspired by this, the CT-Eval dataset selects a popular Chinese multidisciplinary online encyclopedia as the source and covers 28 domains to ensure data diversity. To minimize data hallucination, we first train an LLM to judge and filter out the task samples with hallucination, then employ human annotators to clean the hallucinations in the validation and testing sets. After this process, CT-Eval contains 88.6K task samples. Using CT-Eval, we evaluate the performance of open-source and closed-source LLMs. Our results reveal that zero-shot LLMs (including GPT-4) still have a significant performance gap compared with human judgment. Furthermore, after fine-tuning, open-source LLMs can significantly improve their text-to-table ability, outperforming GPT-4 by a large margin. In short, CT-Eval not only helps researchers evaluate and quickly understand the Chinese text-to-table ability of existing LLMs but also serves as a valuable resource to significantly improve the text-to-table performance of LLMs.","sentences":["Text-to-Table aims to generate structured tables to convey the key information from unstructured documents.","Existing text-to-table datasets are typically oriented English, limiting the research in non-English languages.","Meanwhile, the emergence of large language models (LLMs) has shown great success as general task solvers in multi-lingual settings (e.g., ChatGPT), theoretically enabling text-to-table in other languages.","In this paper, we propose a Chinese text-to-table dataset, CT-Eval, to benchmark LLMs on this task.","Our preliminary analysis of English text-to-table datasets highlights two key factors for dataset construction: data diversity and data hallucination.","Inspired by this, the CT-Eval dataset selects a popular Chinese multidisciplinary online encyclopedia as the source and covers 28 domains to ensure data diversity.","To minimize data hallucination, we first train an LLM to judge and filter out the task samples with hallucination, then employ human annotators to clean the hallucinations in the validation and testing sets.","After this process, CT-Eval contains 88.6K task samples.","Using CT-Eval, we evaluate the performance of open-source and closed-source LLMs.","Our results reveal that zero-shot LLMs (including GPT-4) still have a significant performance gap compared with human judgment.","Furthermore, after fine-tuning, open-source LLMs can significantly improve their text-to-table ability, outperforming GPT-4 by a large margin.","In short, CT-Eval not only helps researchers evaluate and quickly understand the Chinese text-to-table ability of existing LLMs but also serves as a valuable resource to significantly improve the text-to-table performance of LLMs."],"url":"http://arxiv.org/abs/2405.12174v1","category":"cs.CL"}
{"created":"2024-05-20 16:55:05","title":"State of the Practice for Medical Imaging Software","abstract":"We selected 29 medical imaging projects from 48 candidates, assessed 10 software qualities by answering 108 questions for each software project, and interviewed 8 of the 29 development teams. Based on the quantitative data, we ranked the MI software with the Analytic Hierarchy Process (AHP). The four top-ranked software products are 3D Slicer, ImageJ, Fiji, and OHIF Viewer. Generally, MI software is in a healthy state as shown by the following: we observed 88% of the documentation artifacts recommended by research software development guidelines, 100% of MI projects use version control tools, and developers appear to use the common quasi-agile research software development process. However, the current state of the practice deviates from the existing guidelines because of the rarity of some recommended artifacts, low usage of continuous integration (17% of the projects), low use of unit testing (about 50% of projects), and room for improvement with documentation (six of nine developers felt their documentation was not clear enough). From interviewing the developers, we identified five pain points and two qualities of potential concern: lack of development time, lack of funding, technology hurdles, ensuring correctness, usability, maintainability, and reproducibility. The interviewees proposed strategies to improve the state of the practice, to address the identified pain points, and to improve software quality. Combining their ideas with ours, we have the following list of recommendations: increase documentation, increase testing by enriching datasets, increase continuous integration usage, move to web applications, employ linters, use peer reviews, design for change, add assurance cases, and incorporate a \"Generate All Things\" approach.","sentences":["We selected 29 medical imaging projects from 48 candidates, assessed 10 software qualities by answering 108 questions for each software project, and interviewed 8 of the 29 development teams.","Based on the quantitative data, we ranked the MI software with the Analytic Hierarchy Process (AHP).","The four top-ranked software products are 3D Slicer, ImageJ, Fiji, and OHIF Viewer.","Generally, MI software is in a healthy state as shown by the following: we observed 88% of the documentation artifacts recommended by research software development guidelines, 100% of MI projects use version control tools, and developers appear to use the common quasi-agile research software development process.","However, the current state of the practice deviates from the existing guidelines because of the rarity of some recommended artifacts, low usage of continuous integration (17% of the projects), low use of unit testing (about 50% of projects), and room for improvement with documentation (six of nine developers felt their documentation was not clear enough).","From interviewing the developers, we identified five pain points and two qualities of potential concern: lack of development time, lack of funding, technology hurdles, ensuring correctness, usability, maintainability, and reproducibility.","The interviewees proposed strategies to improve the state of the practice, to address the identified pain points, and to improve software quality.","Combining their ideas with ours, we have the following list of recommendations: increase documentation, increase testing by enriching datasets, increase continuous integration usage, move to web applications, employ linters, use peer reviews, design for change, add assurance cases, and incorporate a \"Generate All Things\" approach."],"url":"http://arxiv.org/abs/2405.12171v1","category":"cs.SE"}
{"created":"2024-05-20 16:54:20","title":"Deformation of Residual Intersections","abstract":"It is shown that in a Cohen-Macaulay local ring, the generic linkage of an ideal $I$ is a deformation of the arbitrary linkage of $I$. This fact does not need $I$ to be a Cohen-Macaulay ideal. The same holds for $s$-residual intersections of $I$ when $s$ does not exceed the height of $I$ by one. Under some slight conditions on $I$, one further generalizes this principle to encompass any $s$-residual intersection.","sentences":["It is shown that in a Cohen-Macaulay local ring, the generic linkage of an ideal $I$ is a deformation of the arbitrary linkage of $I$. This fact does not need $I$ to be a Cohen-Macaulay ideal.","The same holds for $s$-residual intersections of $I$ when $s$ does not exceed the height of $I$ by one.","Under some slight conditions on $I$, one further generalizes this principle to encompass any $s$-residual intersection."],"url":"http://arxiv.org/abs/2405.12170v1","category":"math.AC"}
{"created":"2024-05-20 16:52:58","title":"K-theory and lozalizing invariants of large categories","abstract":"In this paper we introduce and study the so-called continuous $K$-theory for a certain class of \"large\" stable $\\infty$-categories, more precisely, for dualizable presentable categories. For compactly generated categories, the continuous $K$-theory is simply the usual (non-connective) $K$-theory of the full subcategory of compact objects. More generally, we show that any localizing invariant of small stable $\\infty$-categories can be uniquely extended to a localizing invariant of dualizable categories.   We compute the continuous $K$-theory for categories of sheaves on locally compact Hausdorff spaces. Using the special case for sheaves on the real line, we give an alternative proof of the theorem of Kasprowski and Winges \\cite{KW19} on the commutation of $K$-theory with infinite products for small stable $\\infty$-categories.   We also study the general theory of dualizable categories. In particular, we give an \"explicit\" proof of Ramzi's theorem \\cite{Ram} on $\\omega_1$-presentability of the category of dualizable categories. Among other things, we prove that dualizability is equivalent to \"flatness\" in the category of presentable stable categories.","sentences":["In this paper we introduce and study the so-called continuous $K$-theory for a certain class of \"large\" stable $\\infty$-categories, more precisely, for dualizable presentable categories.","For compactly generated categories, the continuous $K$-theory is simply the usual (non-connective) $K$-theory of the full subcategory of compact objects.","More generally, we show that any localizing invariant of small stable $\\infty$-categories can be uniquely extended to a localizing invariant of dualizable categories.   ","We compute the continuous $K$-theory for categories of sheaves on locally compact Hausdorff spaces.","Using the special case for sheaves on the real line, we give an alternative proof of the theorem of Kasprowski and Winges \\cite{KW19} on the commutation of $K$-theory with infinite products for small stable $\\infty$-categories.   ","We also study the general theory of dualizable categories.","In particular, we give an \"explicit\" proof of Ramzi's theorem \\cite{Ram} on $\\omega_1$-presentability of the category of dualizable categories.","Among other things, we prove that dualizability is equivalent to \"flatness\" in the category of presentable stable categories."],"url":"http://arxiv.org/abs/2405.12169v1","category":"math.KT"}
{"created":"2024-05-20 16:51:25","title":"Open-Source Assessments of AI Capabilities: The Proliferation of AI Analysis Tools, Replicating Competitor Models, and the Zhousidun Dataset","abstract":"The integration of artificial intelligence (AI) into military capabilities has become a norm for major military power across the globe. Understanding how these AI models operate is essential for maintaining strategic advantages and ensuring security. This paper demonstrates an open-source methodology for analyzing military AI models through a detailed examination of the Zhousidun dataset, a Chinese-originated dataset that exhaustively labels critical components on American and Allied destroyers. By demonstrating the replication of a state-of-the-art computer vision model on this dataset, we illustrate how open-source tools can be leveraged to assess and understand key military AI capabilities. This methodology offers a robust framework for evaluating the performance and potential of AI-enabled military capabilities, thus enhancing the accuracy and reliability of strategic assessments.","sentences":["The integration of artificial intelligence (AI) into military capabilities has become a norm for major military power across the globe.","Understanding how these AI models operate is essential for maintaining strategic advantages and ensuring security.","This paper demonstrates an open-source methodology for analyzing military AI models through a detailed examination of the Zhousidun dataset, a Chinese-originated dataset that exhaustively labels critical components on American and Allied destroyers.","By demonstrating the replication of a state-of-the-art computer vision model on this dataset, we illustrate how open-source tools can be leveraged to assess and understand key military AI capabilities.","This methodology offers a robust framework for evaluating the performance and potential of AI-enabled military capabilities, thus enhancing the accuracy and reliability of strategic assessments."],"url":"http://arxiv.org/abs/2405.12167v1","category":"cs.CY"}
{"created":"2024-05-20 16:47:22","title":"Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging","abstract":"The rapid advancement of large language models has given rise to a plethora of applications across a myriad of real-world tasks, mainly centered on aligning with human intent. However, the complexities inherent in human intent necessitate a dependence on labor-intensive and time-consuming human evaluation. To alleviate this constraint, we delve into the paradigm of employing open-source large language models as evaluators, aligning with the prevailing trend of utilizing GPT-4. Particularly, we present a step-by-step evaluation framework: \\textbf{Fennec}, capable of \\textbf{F}ine-grained \\textbf{E}valuatio\\textbf{N} and correctio\\textbf{N} \\textbf{E}xtended through bran\\textbf{C}hing and bridging. Specifically, the branching operation dissects the evaluation task into various dimensions and granularities, thereby alleviating the challenges associated with evaluation. Concurrently, the bridging operation amalgamates diverse training datasets, augmenting the variety of evaluation tasks. In experimental trials, our 7B model consistently outperforms open-source larger-scale evaluation models across various widely adopted benchmarks in terms of both \\textit{Agreement} and \\textit{Consistency}, closely approaching the capabilities of GPT-4. We employ the fine-grained correction capabilities induced by the evaluation model to refine multiple model responses, and the results show that the refinement elevates the quality of responses, leading to an improvement of 1-2 points on the MT-Bench. Our code is available at Github\\footnote{\\url{https://github.com/dropreg/Fennec}}.","sentences":["The rapid advancement of large language models has given rise to a plethora of applications across a myriad of real-world tasks, mainly centered on aligning with human intent.","However, the complexities inherent in human intent necessitate a dependence on labor-intensive and time-consuming human evaluation.","To alleviate this constraint, we delve into the paradigm of employing open-source large language models as evaluators, aligning with the prevailing trend of utilizing GPT-4.","Particularly, we present a step-by-step evaluation framework: \\textbf{Fennec}, capable of \\textbf{F}ine-grained \\textbf{E}valuatio\\textbf{N} and correctio\\textbf{N} \\textbf{E}xtended through bran\\textbf{C}hing and bridging.","Specifically, the branching operation dissects the evaluation task into various dimensions and granularities, thereby alleviating the challenges associated with evaluation.","Concurrently, the bridging operation amalgamates diverse training datasets, augmenting the variety of evaluation tasks.","In experimental trials, our 7B model consistently outperforms open-source larger-scale evaluation models across various widely adopted benchmarks in terms of both \\textit{Agreement} and \\textit{Consistency}, closely approaching the capabilities of GPT-4.","We employ the fine-grained correction capabilities induced by the evaluation model to refine multiple model responses, and the results show that the refinement elevates the quality of responses, leading to an improvement of 1-2 points on the MT-Bench.","Our code is available at Github\\footnote{\\url{https://github.com/dropreg/Fennec}}."],"url":"http://arxiv.org/abs/2405.12163v1","category":"cs.CL"}
{"created":"2024-05-20 16:38:44","title":"Geant4: a Game Changer in High Energy Physics and Related Applicative Fields","abstract":"Geant4 is an object-oriented toolkit for the simulation of the passage of particles through matter. Its development was initially motivated by the requirements of physics experiments at high energy hadron colliders under construction in the last decade of the 20th century. Since its release in 1998, it has been exploited in many different applicative fields, including space science, nuclear physics, medical physics and archaeology. Its valuable support to scientific discovery is demonstrated by more than 16000 citations received in the past 25 years, including notable citations for main discoveries in different fields. This accomplishment shows that well designed software plays a key role in enabling scientific advancement. In this paper we discuss the key principles and the innovative decisions at the basis of Geant4, which made it a game changer in high energy physics and related fields, and outline some considerations regarding future directions.","sentences":["Geant4 is an object-oriented toolkit for the simulation of the passage of particles through matter.","Its development was initially motivated by the requirements of physics experiments at high energy hadron colliders under construction in the last decade of the 20th century.","Since its release in 1998, it has been exploited in many different applicative fields, including space science, nuclear physics, medical physics and archaeology.","Its valuable support to scientific discovery is demonstrated by more than 16000 citations received in the past 25 years, including notable citations for main discoveries in different fields.","This accomplishment shows that well designed software plays a key role in enabling scientific advancement.","In this paper we discuss the key principles and the innovative decisions at the basis of Geant4, which made it a game changer in high energy physics and related fields, and outline some considerations regarding future directions."],"url":"http://arxiv.org/abs/2405.12159v1","category":"physics.comp-ph"}
{"created":"2024-05-20 16:37:44","title":"Heat Transfer Rate Measurements in a Shock-Focused Region in Air","abstract":"An experimental investigation was carried out to study heat transfer rates in a high-temperature, high-pressure region generated using the shock focusing technique. A shock tube test facility with a specially designed spherically converging test section was used in the present study. Two test cases, a shock of initial strength Mach 2 and Mach 4, were investigated. An in-house 10 developed K -type thermocouple was used in the present investigations, and the measured heat transfer rates were of the order of KW/cm2.","sentences":["An experimental investigation was carried out to study heat transfer rates in a high-temperature, high-pressure region generated using the shock focusing technique.","A shock tube test facility with a specially designed spherically converging test section was used in the present study.","Two test cases, a shock of initial strength Mach 2 and Mach 4, were investigated.","An in-house 10 developed K -type thermocouple was used in the present investigations, and the measured heat transfer rates were of the order of KW/cm2."],"url":"http://arxiv.org/abs/2405.12158v1","category":"physics.flu-dyn"}
{"created":"2024-05-20 16:32:37","title":"Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents","abstract":"The efficient representation, transmission, and reconstruction of three-dimensional (3D) contents are becoming increasingly important for sixth-generation (6G) networks that aim to merge virtual and physical worlds for offering immersive communication experiences. Neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) have recently emerged as two promising 3D representation techniques based on radiance field rendering, which are able to provide photorealistic rendering results for complex scenes. Therefore, embracing NeRF and 3D-GS in 6G networks is envisioned to be a prominent solution to support emerging 3D applications with enhanced quality of experience. This paper provides a comprehensive overview on the integration of NeRF and 3D-GS in 6G. First, we review the basics of the radiance field rendering techniques, and highlight their applications and implementation challenges over wireless networks. Next, we consider the over-the-air training of NeRF and 3D-GS models over wireless networks by presenting various learning techniques. We particularly focus on the federated learning design over a hierarchical device-edge-cloud architecture. Then, we discuss three practical rendering architectures of NeRF and 3D-GS models at wireless network edge. We provide model compression approaches to facilitate the transmission of radiance field models, and present rendering acceleration approaches and joint computation and communication designs to enhance the rendering efficiency. In particular, we propose a new semantic communication enabled 3D content transmission design, in which the radiance field models are exploited as the semantic knowledge base to reduce the communication overhead for distributed inference. Furthermore, we present the utilization of radiance field rendering in wireless applications like radio mapping and radio imaging.","sentences":["The efficient representation, transmission, and reconstruction of three-dimensional (3D) contents are becoming increasingly important for sixth-generation (6G) networks that aim to merge virtual and physical worlds for offering immersive communication experiences.","Neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) have recently emerged as two promising 3D representation techniques based on radiance field rendering, which are able to provide photorealistic rendering results for complex scenes.","Therefore, embracing NeRF and 3D-GS in 6G networks is envisioned to be a prominent solution to support emerging 3D applications with enhanced quality of experience.","This paper provides a comprehensive overview on the integration of NeRF and 3D-GS in 6G. First, we review the basics of the radiance field rendering techniques, and highlight their applications and implementation challenges over wireless networks.","Next, we consider the over-the-air training of NeRF and 3D-GS models over wireless networks by presenting various learning techniques.","We particularly focus on the federated learning design over a hierarchical device-edge-cloud architecture.","Then, we discuss three practical rendering architectures of NeRF and 3D-GS models at wireless network edge.","We provide model compression approaches to facilitate the transmission of radiance field models, and present rendering acceleration approaches and joint computation and communication designs to enhance the rendering efficiency.","In particular, we propose a new semantic communication enabled 3D content transmission design, in which the radiance field models are exploited as the semantic knowledge base to reduce the communication overhead for distributed inference.","Furthermore, we present the utilization of radiance field rendering in wireless applications like radio mapping and radio imaging."],"url":"http://arxiv.org/abs/2405.12155v1","category":"cs.IT"}
{"created":"2024-05-20 16:31:54","title":"Risk, utility and sensitivity to large losses","abstract":"Risk and utility functionals are fundamental building blocks in economics and finance. In this paper we investigate under which conditions a risk or utility functional is sensitive to the accumulation of losses in the sense that any sufficiently large multiple of a position that exposes an agent to future losses has positive risk or negative utility. We call this property sensitivity to large losses and provide necessary and sufficient conditions thereof that are easy to check for a very large class of risk and utility functionals. In particular, our results do not rely on convexity and can therefore also be applied to most examples discussed in the recent literature, including (non-convex) star-shaped risk measures or S-shaped utility functions encountered in prospect theory. As expected, Value at Risk generally fails to be sensitive to large losses. More surprisingly, this is also true of Expected Shortfall. By contrast, expected utility functionals as well as (optimized) certainty equivalents are proved to be sensitive to large losses for many standard choices of concave and nonconcave utility functions, including $S$-shaped utility functions. We also show that Value at Risk and Expected Shortfall become sensitive to large losses if they are either properly adjusted or if the property is suitably localized.","sentences":["Risk and utility functionals are fundamental building blocks in economics and finance.","In this paper we investigate under which conditions a risk or utility functional is sensitive to the accumulation of losses in the sense that any sufficiently large multiple of a position that exposes an agent to future losses has positive risk or negative utility.","We call this property sensitivity to large losses and provide necessary and sufficient conditions thereof that are easy to check for a very large class of risk and utility functionals.","In particular, our results do not rely on convexity and can therefore also be applied to most examples discussed in the recent literature, including (non-convex) star-shaped risk measures or S-shaped utility functions encountered in prospect theory.","As expected, Value at Risk generally fails to be sensitive to large losses.","More surprisingly, this is also true of Expected Shortfall.","By contrast, expected utility functionals as well as (optimized) certainty equivalents are proved to be sensitive to large losses for many standard choices of concave and nonconcave utility functions, including $S$-shaped utility functions.","We also show that Value at Risk and Expected Shortfall become sensitive to large losses if they are either properly adjusted or if the property is suitably localized."],"url":"http://arxiv.org/abs/2405.12154v1","category":"q-fin.RM"}
{"created":"2024-05-20 16:23:40","title":"Bangladeshi Native Vehicle Detection in Wild","abstract":"The success of autonomous navigation relies on robust and precise vehicle recognition, hindered by the scarcity of region-specific vehicle detection datasets, impeding the development of context-aware systems. To advance terrestrial object detection research, this paper proposes a native vehicle detection dataset for the most commonly appeared vehicle classes in Bangladesh. 17 distinct vehicle classes have been taken into account, with fully annotated 81542 instances of 17326 images. Each image width is set to at least 1280px. The dataset's average vehicle bounding box-to-image ratio is 4.7036. This Bangladesh Native Vehicle Dataset (BNVD) has accounted for several geographical, illumination, variety of vehicle sizes, and orientations to be more robust on surprised scenarios. In the context of examining the BNVD dataset, this work provides a thorough assessment with four successive You Only Look Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset's effectiveness is methodically evaluated and contrasted with other vehicle datasets already in use. The BNVD dataset exhibits mean average precision(mAP) at 50% intersection over union (IoU) is 0.848 corresponding precision and recall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643 at an IoU range of 0.5 to 0.95. The experiments show that the BNVD dataset serves as a reliable representation of vehicle distribution and presents considerable complexities.","sentences":["The success of autonomous navigation relies on robust and precise vehicle recognition, hindered by the scarcity of region-specific vehicle detection datasets, impeding the development of context-aware systems.","To advance terrestrial object detection research, this paper proposes a native vehicle detection dataset for the most commonly appeared vehicle classes in Bangladesh.","17 distinct vehicle classes have been taken into account, with fully annotated 81542 instances of 17326 images.","Each image width is set to at least 1280px.","The dataset's average vehicle bounding box-to-image ratio is 4.7036.","This Bangladesh Native Vehicle Dataset (BNVD) has accounted for several geographical, illumination, variety of vehicle sizes, and orientations to be more robust on surprised scenarios.","In the context of examining the BNVD dataset, this work provides a thorough assessment with four successive You Only Look Once (YOLO) models, namely YOLO v5, v6, v7, and v8.","These dataset's effectiveness is methodically evaluated and contrasted with other vehicle datasets already in use.","The BNVD dataset exhibits mean average precision(mAP) at 50% intersection over union (IoU) is 0.848 corresponding precision and recall values of 0.841 and 0.774.","The research findings indicate a mAP of 0.643 at an IoU range of 0.5 to 0.95.","The experiments show that the BNVD dataset serves as a reliable representation of vehicle distribution and presents considerable complexities."],"url":"http://arxiv.org/abs/2405.12150v1","category":"cs.CV"}
{"created":"2024-05-20 16:21:24","title":"Generating large primordial fluctuations in single-field inflation for PBH formation","abstract":"In order to produce appreciable amount of primordial black holes (PBHs), the square amplitude of curvature perturbation must take a large value of $\\mathcal{O}(0.01)$, namely, seven digits larger than the value observed by cosmic microwave background radiation (CMB) on large scales. Such a large fluctuation can be achieved by violating slow-roll (SR) condition within a short duration. The best known of such possibilities is the ultraslow-roll (USR) inflation. We calculate the power spectrum of curvature perturbation in a simple single-field inflation model which evolves through SR-USR-SR regimes so that both large-amplitude small-scale fluctuation for PBH formation and small-amplitude large-scale fluctuation as observed by CMB are realized. We further calculate the bispectrum and one-loop correction to the power spectrum induced by the third-order action of curvature perturbation as the beginning of precision cosmology on small scale. As a result we show that single-field inflation model realizing PBH formation is severely constrained by the quantum correction.","sentences":["In order to produce appreciable amount of primordial black holes (PBHs), the square amplitude of curvature perturbation must take a large value of $\\mathcal{O}(0.01)$, namely, seven digits larger than the value observed by cosmic microwave background radiation (CMB) on large scales.","Such a large fluctuation can be achieved by violating slow-roll (SR) condition within a short duration.","The best known of such possibilities is the ultraslow-roll (USR) inflation.","We calculate the power spectrum of curvature perturbation in a simple single-field inflation model which evolves through SR-USR-SR regimes so that both large-amplitude small-scale fluctuation for PBH formation and small-amplitude large-scale fluctuation as observed by CMB are realized.","We further calculate the bispectrum and one-loop correction to the power spectrum induced by the third-order action of curvature perturbation as the beginning of precision cosmology on small scale.","As a result we show that single-field inflation model realizing PBH formation is severely constrained by the quantum correction."],"url":"http://arxiv.org/abs/2405.12149v1","category":"astro-ph.CO"}
{"created":"2024-05-20 16:19:02","title":"Eliciting Problem Specifications via Large Language Models","abstract":"Cognitive systems generally require a human to translate a problem definition into some specification that the cognitive system can use to attempt to solve the problem or perform the task. In this paper, we illustrate that large language models (LLMs) can be utilized to map a problem class, defined in natural language, into a semi-formal specification that can then be utilized by an existing reasoning and learning system to solve instances from the problem class. We present the design of LLM-enabled cognitive task analyst agent(s). Implemented with LLM agents, this system produces a definition of problem spaces for tasks specified in natural language. LLM prompts are derived from the definition of problem spaces in the AI literature and general problem-solving strategies (Polya's How to Solve It). A cognitive system can then use the problem-space specification, applying domain-general problem solving strategies (\"weak methods\" such as search), to solve multiple instances of problems from the problem class. This result, while preliminary, suggests the potential for speeding cognitive systems research via disintermediation of problem formulation while also retaining core capabilities of cognitive systems, such as robust inference and online learning.","sentences":["Cognitive systems generally require a human to translate a problem definition into some specification that the cognitive system can use to attempt to solve the problem or perform the task.","In this paper, we illustrate that large language models (LLMs) can be utilized to map a problem class, defined in natural language, into a semi-formal specification that can then be utilized by an existing reasoning and learning system to solve instances from the problem class.","We present the design of LLM-enabled cognitive task analyst agent(s).","Implemented with LLM agents, this system produces a definition of problem spaces for tasks specified in natural language.","LLM prompts are derived from the definition of problem spaces in the AI literature and general problem-solving strategies (Polya's How to Solve It).","A cognitive system can then use the problem-space specification, applying domain-general problem solving strategies (\"weak methods\" such as search), to solve multiple instances of problems from the problem class.","This result, while preliminary, suggests the potential for speeding cognitive systems research via disintermediation of problem formulation while also retaining core capabilities of cognitive systems, such as robust inference and online learning."],"url":"http://arxiv.org/abs/2405.12147v1","category":"cs.AI"}
{"created":"2024-05-20 16:16:59","title":"Cosmic Ray Diffusion in the Turbulent Interstellar Medium: Effects of Mirror Diffusion and Pitch Angle Scattering","abstract":"Cosmic rays (CRs) interact with turbulent magnetic fields in the intestellar medium, generating nonthermal emission. After many decades of studies, the theoretical understanding of their diffusion in the ISM continues to pose a challenge. This study numerically explores a recent prediction termed \"mirror diffusion\" and its synergy with traditional diffusion mechanism based on gyroresonant scattering. Our study combines 3D MHD simulations of star-forming regions with test particle simulations to analyze CR diffusion. We demonstrate the significance of mirror diffusion in CR diffusion parallel to the magnetic field, when the mirroring condition is satisfied. Our results support the theoretical expectation that the resulting particle propagation arising from mirror diffusion in combination with much faster diffusion induced by gyroresonant scattering resembles a Levy-flight-like propagation. Our study highlights the necessity to reevaluate the diffusion coefficients traditionally adopeted in the ISM based on gyroresonant scattering alone. For instance, our simulations imply a diffusion coefficient $\\sim10^{27}cm^2/s$ for particles with a few hundred TeV within regions spanning a few parsecs around the source. This estimate is in agreement with gamma-ray observations, which shows the relevance of our results for understanding of diffuse gamma-ray emission in star-forming regions.","sentences":["Cosmic rays (CRs) interact with turbulent magnetic fields in the intestellar medium, generating nonthermal emission.","After many decades of studies, the theoretical understanding of their diffusion in the ISM continues to pose a challenge.","This study numerically explores a recent prediction termed \"mirror diffusion\" and its synergy with traditional diffusion mechanism based on gyroresonant scattering.","Our study combines 3D MHD simulations of star-forming regions with test particle simulations to analyze CR diffusion.","We demonstrate the significance of mirror diffusion in CR diffusion parallel to the magnetic field, when the mirroring condition is satisfied.","Our results support the theoretical expectation that the resulting particle propagation arising from mirror diffusion in combination with much faster diffusion induced by gyroresonant scattering resembles a Levy-flight-like propagation.","Our study highlights the necessity to reevaluate the diffusion coefficients traditionally adopeted in the ISM based on gyroresonant scattering alone.","For instance, our simulations imply a diffusion coefficient $\\sim10^{27}cm^2/s$ for particles with a few hundred TeV within regions spanning a few parsecs around the source.","This estimate is in agreement with gamma-ray observations, which shows the relevance of our results for understanding of diffuse gamma-ray emission in star-forming regions."],"url":"http://arxiv.org/abs/2405.12146v1","category":"astro-ph.HE"}
{"created":"2024-05-20 16:15:23","title":"Comparing sharp and smooth transition of the second slow-roll parameter in single-field inflation","abstract":"In single-field inflation, violation of the slow-roll approximation can lead to growth of curvature perturbation outside the horizon. This violation is characterized by a period with a large negative value of the second slow-roll parameter. At an early time, inflation must satisfy the slow-roll approximation, so the large-scale curvature perturbation can explain the cosmic microwave background fluctuations. At intermediate time, it is viable to have a theory that violates the slow-roll approximation, which implies amplification of the curvature perturbation on small scales. Specifically, we consider ultraslow-roll inflation as the intermediate period. At late time, inflation should go back to the slow roll period so that it can end. This means that there are two transitions of the second slow-roll parameter. In this paper, we compare two different possibilities for the second transition: sharp and smooth transitions. Focusing on effects generated by the relevant cubic self-interaction of the curvature perturbation, we find that the bispectrum and one-loop correction to the power spectrum due to the change of the second slow-roll parameter vanish if and only if the Mukhanov-Sasaki equation for perturbation satisfies a specific condition called Wands duality. We also find in the case of sharp transition that, even though this duality is satisfied in the ultraslow-roll and slow-roll phases, it is severely violated at the transition so that the resultant one-loop correction is extremely large inversely proportional to the duration of the transition.","sentences":["In single-field inflation, violation of the slow-roll approximation can lead to growth of curvature perturbation outside the horizon.","This violation is characterized by a period with a large negative value of the second slow-roll parameter.","At an early time, inflation must satisfy the slow-roll approximation, so the large-scale curvature perturbation can explain the cosmic microwave background fluctuations.","At intermediate time, it is viable to have a theory that violates the slow-roll approximation, which implies amplification of the curvature perturbation on small scales.","Specifically, we consider ultraslow-roll inflation as the intermediate period.","At late time, inflation should go back to the slow roll period so that it can end.","This means that there are two transitions of the second slow-roll parameter.","In this paper, we compare two different possibilities for the second transition: sharp and smooth transitions.","Focusing on effects generated by the relevant cubic self-interaction of the curvature perturbation, we find that the bispectrum and one-loop correction to the power spectrum due to the change of the second slow-roll parameter vanish if and only if the Mukhanov-Sasaki equation for perturbation satisfies a specific condition called Wands duality.","We also find in the case of sharp transition that, even though this duality is satisfied in the ultraslow-roll and slow-roll phases, it is severely violated at the transition so that the resultant one-loop correction is extremely large inversely proportional to the duration of the transition."],"url":"http://arxiv.org/abs/2405.12145v1","category":"astro-ph.CO"}
{"created":"2024-05-20 16:01:01","title":"DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on LLM","abstract":"Visual Language Tracking (VLT) enhances single object tracking (SOT) by integrating natural language descriptions from a video, for the precise tracking of a specified object. By leveraging high-level semantic information, VLT guides object tracking, alleviating the constraints associated with relying on a visual modality. Nevertheless, most VLT benchmarks are annotated in a single granularity and lack a coherent semantic framework to provide scientific guidance. Moreover, coordinating human annotators for high-quality annotations is laborious and time-consuming. To address these challenges, we introduce DTLLM-VLT, which automatically generates extensive and multi-granularity text to enhance environmental diversity. (1) DTLLM-VLT generates scientific and multi-granularity text descriptions using a cohesive prompt framework. Its succinct and highly adaptable design allows seamless integration into various visual tracking benchmarks. (2) We select three prominent benchmarks to deploy our approach: short-term tracking, long-term tracking, and global instance tracking. We offer four granularity combinations for these benchmarks, considering the extent and density of semantic information, thereby showcasing the practicality and versatility of DTLLM-VLT. (3) We conduct comparative experiments on VLT benchmarks with different text granularities, evaluating and analyzing the impact of diverse text on tracking performance. Conclusionally, this work leverages LLM to provide multi-granularity semantic information for VLT task from efficient and diverse perspectives, enabling fine-grained evaluation of multi-modal trackers. In the future, we believe this work can be extended to more datasets to support vision datasets understanding.","sentences":["Visual Language Tracking (VLT) enhances single object tracking (SOT) by integrating natural language descriptions from a video, for the precise tracking of a specified object.","By leveraging high-level semantic information, VLT guides object tracking, alleviating the constraints associated with relying on a visual modality.","Nevertheless, most VLT benchmarks are annotated in a single granularity and lack a coherent semantic framework to provide scientific guidance.","Moreover, coordinating human annotators for high-quality annotations is laborious and time-consuming.","To address these challenges, we introduce DTLLM-VLT, which automatically generates extensive and multi-granularity text to enhance environmental diversity.","(1) DTLLM-VLT generates scientific and multi-granularity text descriptions using a cohesive prompt framework.","Its succinct and highly adaptable design allows seamless integration into various visual tracking benchmarks.","(2) We select three prominent benchmarks to deploy our approach: short-term tracking, long-term tracking, and global instance tracking.","We offer four granularity combinations for these benchmarks, considering the extent and density of semantic information, thereby showcasing the practicality and versatility of DTLLM-VLT.","(3) We conduct comparative experiments on VLT benchmarks with different text granularities, evaluating and analyzing the impact of diverse text on tracking performance.","Conclusionally, this work leverages LLM to provide multi-granularity semantic information for VLT task from efficient and diverse perspectives, enabling fine-grained evaluation of multi-modal trackers.","In the future, we believe this work can be extended to more datasets to support vision datasets understanding."],"url":"http://arxiv.org/abs/2405.12139v1","category":"cs.CV"}
{"created":"2024-05-20 15:52:33","title":"Auger photoemission as a laser-like coherent cathode","abstract":"In pursuit of quantum advancements across disciplines, a bright and coherent electron source is expected to be a cornerstone of diverse applications including electron microscopy, laser accelerators, and free electron lasers. Current cathodes, such as cold field and photoemission, can generate high-quality electron beams with different cathode materials, geometric configurations, and laser excitation profiles, but their maintenance of both quantum coherence and high beam brightness suffers from the space-charge repulsion of many electrons. Here, we propose a new mechanism to provide collective emission of coherent electrons based on Auger photoemission. Our approach leverages a photon-induced four-level Auger process that necessitates a combination of photoemission and Auger recombination. The Auger electrons, energized through a recycling process of photoelectrons, emit collectively into the vacuum as secondary electrons. We compare coherent and incoherent Auger photoemission, identifying that the working condition of the coherent photoemission requires population inversion, akin to the four-level laser system. Our work provides insights for experimental realization and nanofabrication of Auger photocathodes, addressing a critical need in advancing quantum technologies relating to correlated coherent sources.","sentences":["In pursuit of quantum advancements across disciplines, a bright and coherent electron source is expected to be a cornerstone of diverse applications including electron microscopy, laser accelerators, and free electron lasers.","Current cathodes, such as cold field and photoemission, can generate high-quality electron beams with different cathode materials, geometric configurations, and laser excitation profiles, but their maintenance of both quantum coherence and high beam brightness suffers from the space-charge repulsion of many electrons.","Here, we propose a new mechanism to provide collective emission of coherent electrons based on Auger photoemission.","Our approach leverages a photon-induced four-level Auger process that necessitates a combination of photoemission and Auger recombination.","The Auger electrons, energized through a recycling process of photoelectrons, emit collectively into the vacuum as secondary electrons.","We compare coherent and incoherent Auger photoemission, identifying that the working condition of the coherent photoemission requires population inversion, akin to the four-level laser system.","Our work provides insights for experimental realization and nanofabrication of Auger photocathodes, addressing a critical need in advancing quantum technologies relating to correlated coherent sources."],"url":"http://arxiv.org/abs/2405.12133v1","category":"quant-ph"}
{"created":"2024-05-20 15:47:33","title":"Double extensions of quasi-Frobenius Lie superalgebras with degenerate center","abstract":"We develop the process of symplectic double extensions for Lie superalgebras with degenerate center. The construction is a superization of a recent work by Fischer, and generalize our previous work. We provide a standard model for such double extensions, where the symplectic form is either orthosymplectic or periplectic. Additionally, we show that every double extension is naturally equivalent to either of these two standard types of extensions. Several examples in low dimensions are given to illustrate the concept.","sentences":["We develop the process of symplectic double extensions for Lie superalgebras with degenerate center.","The construction is a superization of a recent work by Fischer, and generalize our previous work.","We provide a standard model for such double extensions, where the symplectic form is either orthosymplectic or periplectic.","Additionally, we show that every double extension is naturally equivalent to either of these two standard types of extensions.","Several examples in low dimensions are given to illustrate the concept."],"url":"http://arxiv.org/abs/2405.12128v1","category":"math.RT"}
{"created":"2024-05-20 15:45:23","title":"Neutron-superfluid vortices and proton-superconductor flux tubes: Development of a minimal model for pulsar glitches","abstract":"We develop a theoretical framework that allows us to explore the coupled motion of neutron-superfluid vortices and proton-superconductor flux tubes in a gravitationally collapsed condensate, which describe neutron stars that form pulsars. Our framework uses the 3D Gross-Pitaevskii-Poisson-Equation (GPPE) for neutron Cooper pairs, the Real-Time-Ginzburg-Landau equation (RTGLE) for proton Cooper pairs, the Maxwell equations for the vector potential ${\\bf A}$, and Newtonian gravity and interactions, both direct and induced by the Poisson equation, between the neutron and proton subsystems. For a pulsar we include a crust potential, characterized by an angle $\\theta$, and frictional drag. By carrying out extensive direct numerical simulations, we obtain a variety of interesting results. We show that a rotating proton superconductor generates a uniform London magnetic field, which changes the field distribution inside flux tubes. In the absence of any direct interaction between the two species, they interact through the gravitational Poisson equation. The presence of attractive (repulsive) density-density interaction leads to the attraction (repulsion) between neutron vortices and proton flux tubes. The inclusion of the current-current interaction and the complete Maxwell equations allows us to quantify the entrainment effect that leads to induced magnetization of neutron vortices. We show that, with a strong external magnetic field ${\\bf B}_{\\rm ext}$, proton flux tubes are anchored to the crust, whereas neutron vortices leave the condensate and lead to abrupt changes of the crust angular momentum ${\\rm J}_c$. The frictional term in the dynamical equation for $\\theta$ yields stick-slip dynamics that leads, in turn, to glitches in the time series of ${\\rm J}_c$. By calculating various statistical properties of this time series, we demonstrate that they display self-organised criticality(SOC).","sentences":["We develop a theoretical framework that allows us to explore the coupled motion of neutron-superfluid vortices and proton-superconductor flux tubes in a gravitationally collapsed condensate, which describe neutron stars that form pulsars.","Our framework uses the 3D Gross-Pitaevskii-Poisson-Equation (GPPE) for neutron Cooper pairs, the Real-Time-Ginzburg-Landau equation (RTGLE) for proton Cooper pairs, the Maxwell equations for the vector potential ${\\bf A}$, and Newtonian gravity and interactions, both direct and induced by the Poisson equation, between the neutron and proton subsystems.","For a pulsar we include a crust potential, characterized by an angle $\\theta$, and frictional drag.","By carrying out extensive direct numerical simulations, we obtain a variety of interesting results.","We show that a rotating proton superconductor generates a uniform London magnetic field, which changes the field distribution inside flux tubes.","In the absence of any direct interaction between the two species, they interact through the gravitational Poisson equation.","The presence of attractive (repulsive) density-density interaction leads to the attraction (repulsion) between neutron vortices and proton flux tubes.","The inclusion of the current-current interaction and the complete Maxwell equations allows us to quantify the entrainment effect that leads to induced magnetization of neutron vortices.","We show that, with a strong external magnetic field ${\\bf B}_{\\rm ext}$, proton flux tubes are anchored to the crust, whereas neutron vortices leave the condensate and lead to abrupt changes of the crust angular momentum ${\\rm J}_c$. The frictional term in the dynamical equation for $\\theta$ yields stick-slip dynamics that leads, in turn, to glitches in the time series of ${\\rm J}_c$. By calculating various statistical properties of this time series, we demonstrate that they display self-organised criticality(SOC)."],"url":"http://arxiv.org/abs/2405.12127v1","category":"astro-ph.HE"}
{"created":"2024-05-20 15:43:40","title":"Design, Control, and Motion-Planning for a Root-Perching Rotor-Distributed Manipulator","abstract":"Manipulation performance improvement is crucial for aerial robots. For aerial manipulators, the baselink position and attitude errors directly affect the precision at the end effector. To address this stability problem, fixed-body approaches such as perching on the environment using the rotor suction force are useful. Additionally, conventional arm-equipped multirotors, called rotor-concentrated manipulators (RCMs), find it difficult to generate a large wrench at the end effector due to joint torque limitations. Using distributed rotors to each link, the thrust can support each link weight, decreasing the arm joints' torque. Based on this approach, rotor-distributed manipulators (RDMs) can increase feasible wrench and reachability of the end-effector. This paper introduces a minimal configuration of a rotor-distributed manipulator that can perch on surfaces, especially ceilings, using a part of their body. First, we design a minimal rotor-distributed arm considering the flight and end-effector performance. Second, a flight controller is proposed for this minimal RDM along with a perching controller adaptable for various types of aerial robots. Third, we propose a motion planning method based on inverse kinematics (IK), considering specific constraints to the proposed RDMs such as perching force. Finally, we evaluate flight and perching motions and \\revise{confirm} that the proposed manipulator can significantly improve the manipulation performance.","sentences":["Manipulation performance improvement is crucial for aerial robots.","For aerial manipulators, the baselink position and attitude errors directly affect the precision at the end effector.","To address this stability problem, fixed-body approaches such as perching on the environment using the rotor suction force are useful.","Additionally, conventional arm-equipped multirotors, called rotor-concentrated manipulators (RCMs), find it difficult to generate a large wrench at the end effector due to joint torque limitations.","Using distributed rotors to each link, the thrust can support each link weight, decreasing the arm joints' torque.","Based on this approach, rotor-distributed manipulators (RDMs) can increase feasible wrench and reachability of the end-effector.","This paper introduces a minimal configuration of a rotor-distributed manipulator that can perch on surfaces, especially ceilings, using a part of their body.","First, we design a minimal rotor-distributed arm considering the flight and end-effector performance.","Second, a flight controller is proposed for this minimal RDM along with a perching controller adaptable for various types of aerial robots.","Third, we propose a motion planning method based on inverse kinematics (IK), considering specific constraints to the proposed RDMs such as perching force.","Finally, we evaluate flight and perching motions and \\revise{confirm} that the proposed manipulator can significantly improve the manipulation performance."],"url":"http://arxiv.org/abs/2405.12125v1","category":"eess.SY"}
{"created":"2024-05-20 15:39:30","title":"Insecurity of Quantum Two-Party Computation with Applications to Cheat-Sensitive Protocols and Oblivious Transfer Reductions","abstract":"Oblivious transfer (OT) is a fundamental primitive for secure two-party computation. It is well known that OT cannot be implemented with information-theoretic security if the two players only have access to noiseless communication channels, even in the quantum case. As a result, weaker variants of OT have been studied. In this work, we rigorously establish the impossibility of cheat-sensitive OT, where a dishonest party can cheat, but risks being detected. We construct a general attack on any quantum protocol that allows the receiver to compute all inputs of the sender and provide an explicit upper bound on the success probability of this attack. This implies that cheat-sensitive quantum Symmetric Private Information Retrieval cannot be implemented with statistical information-theoretic security. Leveraging the techniques devised for our proofs, we provide entropic bounds on primitives needed for secure function evaluation. They imply impossibility results for protocols where the players have access to OT as a resource. This result significantly improves upon existing bounds and yields tight bounds for reductions of 1-out-of-n OT to a resource primitive. Our results hold in particular for transformations between a finite number of primitives and for any error.","sentences":["Oblivious transfer (OT) is a fundamental primitive for secure two-party computation.","It is well known that OT cannot be implemented with information-theoretic security if the two players only have access to noiseless communication channels, even in the quantum case.","As a result, weaker variants of OT have been studied.","In this work, we rigorously establish the impossibility of cheat-sensitive OT, where a dishonest party can cheat, but risks being detected.","We construct a general attack on any quantum protocol that allows the receiver to compute all inputs of the sender and provide an explicit upper bound on the success probability of this attack.","This implies that cheat-sensitive quantum Symmetric Private Information Retrieval cannot be implemented with statistical information-theoretic security.","Leveraging the techniques devised for our proofs, we provide entropic bounds on primitives needed for secure function evaluation.","They imply impossibility results for protocols where the players have access to OT as a resource.","This result significantly improves upon existing bounds and yields tight bounds for reductions of 1-out-of-n OT to a resource primitive.","Our results hold in particular for transformations between a finite number of primitives and for any error."],"url":"http://arxiv.org/abs/2405.12121v1","category":"quant-ph"}
{"created":"2024-05-20 15:37:55","title":"Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation","abstract":"Large language models (LLMs) are revolutionizing conversational recommender systems by adeptly indexing item content, understanding complex conversational contexts, and generating relevant item titles. However, controlling the distribution of recommended items remains a challenge. This leads to suboptimal performance due to the failure to capture rapidly changing data distributions, such as item popularity, on targeted conversational recommendation platforms. In conversational recommendation, LLMs recommend items by generating the titles (as multiple tokens) autoregressively, making it difficult to obtain and control the recommendations over all items. Thus, we propose a Reindex-Then-Adapt (RTA) framework, which converts multi-token item titles into single tokens within LLMs, and then adjusts the probability distributions over these single-token item titles accordingly. The RTA framework marries the benefits of both LLMs and traditional recommender systems (RecSys): understanding complex queries as LLMs do; while efficiently controlling the recommended item distributions in conversational recommendations as traditional RecSys do. Our framework demonstrates improved accuracy metrics across three different conversational recommendation datasets and two adaptation settings","sentences":["Large language models (LLMs) are revolutionizing conversational recommender systems by adeptly indexing item content, understanding complex conversational contexts, and generating relevant item titles.","However, controlling the distribution of recommended items remains a challenge.","This leads to suboptimal performance due to the failure to capture rapidly changing data distributions, such as item popularity, on targeted conversational recommendation platforms.","In conversational recommendation, LLMs recommend items by generating the titles (as multiple tokens) autoregressively, making it difficult to obtain and control the recommendations over all items.","Thus, we propose a Reindex-Then-Adapt (RTA) framework, which converts multi-token item titles into single tokens within LLMs, and then adjusts the probability distributions over these single-token item titles accordingly.","The RTA framework marries the benefits of both LLMs and traditional recommender systems (RecSys): understanding complex queries as LLMs do; while efficiently controlling the recommended item distributions in conversational recommendations as traditional RecSys do.","Our framework demonstrates improved accuracy metrics across three different conversational recommendation datasets and two adaptation settings"],"url":"http://arxiv.org/abs/2405.12119v1","category":"cs.IR"}
{"created":"2024-05-20 15:36:14","title":"Group completion via the action $\\infty$-category","abstract":"We give a generalization of Quillen's $S^{-1}S$ construction for arbitrary $E_n$-monoids as an $E_{n-1}$-monoidal $\\infty$-category and show that its realization models the group completion provided that $n \\geq 2$. We will also show how this construction is related to a variety of other constructions of the group completion.","sentences":["We give a generalization of Quillen's $S^{-1}S$ construction for arbitrary $E_n$-monoids as an $E_{n-1}$-monoidal $\\infty$-category and show that its realization models the group completion provided that $n \\geq 2$.","We will also show how this construction is related to a variety of other constructions of the group completion."],"url":"http://arxiv.org/abs/2405.12118v1","category":"math.KT"}
{"created":"2024-05-20 15:31:32","title":"Clap: a Rust eDSL for PlonKish Proof Systems with a Semantics-preserving Optimizing Compiler","abstract":"Writing Plonkish constraint systems by hand is tedious and error-prone; as a result, several libraries and DSL's have emerged over the years to facilitate this task as well as techniques to directly analyze constraint systems. However, standalone languages require developers to use a foreign toolchain and leave gaps between the application and its circuits. On the other hand, Rust-embedded DSL like Halo2 or Boojum lack in modularity; furthermore, it is usually impossible to tease apart the circuit from the proof system, making it hard to reuse circuits and even to compare performance of different proof systems on the same circuits.   In this paper we introduce Clap, the first Rust eDSL to propose a prover-agnostic circuit format that enables extensibility, automatic optimizations, and formal guarantees for the resulting constraint system. Clap generates Plonkish constraint systems and witness generators that are sound and complete with respect to each other, leaving no room for subtle bugs due to under- or over-constraining. A model of this equivalence is proved in the Agda proof assistant for a subset of Clap's Rust implementation that is expressive enough to capture the compositional properties of our format. In order to increase the reuse of circuits, a number of optimizations are carried out automatically, sparing the developer from over-specifying low-level constraint system details in their circuit descriptions. We test the expressivity and efficiency of Clap on an implementation of the Poseidon2 hash function that produces a constraint system that is competitive in terms of size with hand-optimized Boojum circuits.","sentences":["Writing Plonkish constraint systems by hand is tedious and error-prone; as a result, several libraries and DSL's have emerged over the years to facilitate this task as well as techniques to directly analyze constraint systems.","However, standalone languages require developers to use a foreign toolchain and leave gaps between the application and its circuits.","On the other hand, Rust-embedded DSL like Halo2 or Boojum lack in modularity; furthermore, it is usually impossible to tease apart the circuit from the proof system, making it hard to reuse circuits and even to compare performance of different proof systems on the same circuits.   ","In this paper we introduce Clap, the first Rust eDSL to propose a prover-agnostic circuit format that enables extensibility, automatic optimizations, and formal guarantees for the resulting constraint system.","Clap generates Plonkish constraint systems and witness generators that are sound and complete with respect to each other, leaving no room for subtle bugs due to under- or over-constraining.","A model of this equivalence is proved in the Agda proof assistant for a subset of Clap's Rust implementation that is expressive enough to capture the compositional properties of our format.","In order to increase the reuse of circuits, a number of optimizations are carried out automatically, sparing the developer from over-specifying low-level constraint system details in their circuit descriptions.","We test the expressivity and efficiency of Clap on an implementation of the Poseidon2 hash function that produces a constraint system that is competitive in terms of size with hand-optimized Boojum circuits."],"url":"http://arxiv.org/abs/2405.12115v1","category":"cs.CR"}
{"created":"2024-05-20 15:27:54","title":"Benedicks-type uncertainty principle for metaplectic time-frequency representations","abstract":"Metaplectic Wigner distributions are joint time-frequency representations that are parametrized by a symplectic matrix and generalize the short-time Fourier transform and the Wigner distribution.   We investigate the question which metaplectic Wigner distributions satisfy an uncertainty principle in the style of Benedicks and Amrein-Berthier. That is, if the metaplectic Wigner distribution is supported on a set of finite measure, must the functions then be zero? While this statement holds for the short-time Fourier transform, it is false for some other natural time-frequency representations. We provide a full characterization of the class of metaplectic Wigner distributions which exhibit an uncertainty principle of this type, both for sesquilinear and quadratic versions.","sentences":["Metaplectic Wigner distributions are joint time-frequency representations that are parametrized by a symplectic matrix and generalize the short-time Fourier transform and the Wigner distribution.   ","We investigate the question which metaplectic Wigner distributions satisfy an uncertainty principle in the style of Benedicks and Amrein-Berthier.","That is, if the metaplectic Wigner distribution is supported on a set of finite measure, must the functions then be zero?","While this statement holds for the short-time Fourier transform, it is false for some other natural time-frequency representations.","We provide a full characterization of the class of metaplectic Wigner distributions which exhibit an uncertainty principle of this type, both for sesquilinear and quadratic versions."],"url":"http://arxiv.org/abs/2405.12112v1","category":"math.FA"}
{"created":"2024-05-20 15:25:18","title":"Linguistic Structure from a Bottleneck on Sequential Information Processing","abstract":"Human language is a unique form of communication in the natural world, distinguished by its structured nature. Most fundamentally, it is systematic, meaning that signals can be broken down into component parts that are individually meaningful -- roughly, words -- which are combined in a regular way to form sentences. Furthermore, the way in which these parts are combined maintains a kind of locality: words are usually concatenated together, and they form contiguous phrases, keeping related parts of sentences close to each other. We address the challenge of understanding how these basic properties of language arise from broader principles of efficient communication under information processing constraints. Here we show that natural-language-like systematicity arises from minimization of excess entropy, a measure of statistical complexity that represents the minimum amount of information necessary for predicting the future of a sequence based on its past. In simulations, we show that codes that minimize excess entropy factorize their source distributions into approximately independent components, and then express those components systematically and locally. Next, in a series of massively cross-linguistic corpus studies, we show that human languages are structured to have low excess entropy at the level of phonology, morphology, syntax, and semantics. Our result suggests that human language performs a sequential generalization of Independent Components Analysis on the statistical distribution over meanings that need to be expressed. It establishes a link between the statistical and algebraic structure of human language, and reinforces the idea that the structure of human language may have evolved to minimize cognitive load while maximizing communicative expressiveness.","sentences":["Human language is a unique form of communication in the natural world, distinguished by its structured nature.","Most fundamentally, it is systematic, meaning that signals can be broken down into component parts that are individually meaningful -- roughly, words -- which are combined in a regular way to form sentences.","Furthermore, the way in which these parts are combined maintains a kind of locality: words are usually concatenated together, and they form contiguous phrases, keeping related parts of sentences close to each other.","We address the challenge of understanding how these basic properties of language arise from broader principles of efficient communication under information processing constraints.","Here we show that natural-language-like systematicity arises from minimization of excess entropy, a measure of statistical complexity that represents the minimum amount of information necessary for predicting the future of a sequence based on its past.","In simulations, we show that codes that minimize excess entropy factorize their source distributions into approximately independent components, and then express those components systematically and locally.","Next, in a series of massively cross-linguistic corpus studies, we show that human languages are structured to have low excess entropy at the level of phonology, morphology, syntax, and semantics.","Our result suggests that human language performs a sequential generalization of Independent Components Analysis on the statistical distribution over meanings that need to be expressed.","It establishes a link between the statistical and algebraic structure of human language, and reinforces the idea that the structure of human language may have evolved to minimize cognitive load while maximizing communicative expressiveness."],"url":"http://arxiv.org/abs/2405.12109v1","category":"cs.CL"}
{"created":"2024-05-20 15:23:38","title":"An effective advection induced by oscillating microstructure in a diffusion equation","abstract":"We consider the homogenisation of a diffusion equation in a porous medium. The microstructure is time-dependent and oscillating on a small time scale. This oscillation causes a novel advection in the homogenised equations. Allowing for a locally varying geometry, the oscillating microstructure demonstrates the ability to generate arbitrary and locally varying advection velocity fields.","sentences":["We consider the homogenisation of a diffusion equation in a porous medium.","The microstructure is time-dependent and oscillating on a small time scale.","This oscillation causes a novel advection in the homogenised equations.","Allowing for a locally varying geometry, the oscillating microstructure demonstrates the ability to generate arbitrary and locally varying advection velocity fields."],"url":"http://arxiv.org/abs/2405.12108v1","category":"math.AP"}
{"created":"2024-05-20 15:21:48","title":"Sheet Music Transformer ++: End-to-End Full-Page Optical Music Recognition for Pianoform Sheet Music","abstract":"Optical Music Recognition is a field that has progressed significantly, bringing accurate systems that transcribe effectively music scores into digital formats. Despite this, there are still several limitations that hinder OMR from achieving its full potential. Specifically, state of the art OMR still depends on multi-stage pipelines for performing full-page transcription, as well as it has only been demonstrated in monophonic cases, leaving behind very relevant engravings. In this work, we present the Sheet Music Transformer++, an end-to-end model that is able to transcribe full-page polyphonic music scores without the need of a previous Layout Analysis step. This is done thanks to an extensive curriculum learning-based pretraining with synthetic data generation. We conduct several experiments on a full-page extension of a public polyphonic transcription dataset. The experimental outcomes confirm that the model is competent at transcribing full-page pianoform scores, marking a noteworthy milestone in end-to-end OMR transcription.","sentences":["Optical Music Recognition is a field that has progressed significantly, bringing accurate systems that transcribe effectively music scores into digital formats.","Despite this, there are still several limitations that hinder OMR from achieving its full potential.","Specifically, state of the art OMR still depends on multi-stage pipelines for performing full-page transcription, as well as it has only been demonstrated in monophonic cases, leaving behind very relevant engravings.","In this work, we present the Sheet Music Transformer++, an end-to-end model that is able to transcribe full-page polyphonic music scores without the need of a previous Layout Analysis step.","This is done thanks to an extensive curriculum learning-based pretraining with synthetic data generation.","We conduct several experiments on a full-page extension of a public polyphonic transcription dataset.","The experimental outcomes confirm that the model is competent at transcribing full-page pianoform scores, marking a noteworthy milestone in end-to-end OMR transcription."],"url":"http://arxiv.org/abs/2405.12105v1","category":"cs.CV"}
{"created":"2024-05-20 15:15:14","title":"Sustainable business decision modelling with blockchain and digital twins: A survey","abstract":"Industry 4.0 and beyond will rely heavily on sustainable Business Decision Modelling (BDM) that can be accelerated by blockchain and Digital Twin (DT) solutions. BDM is built on models and frameworks refined by key identification factors, data analysis, and mathematical or computational aspects applicable to complex business scenarios. Gaining actionable intelligence from collected data for BDM requires a carefully considered infrastructure to ensure data transparency, security, accessibility and sustainability. Organisations should consider social, economic and environmental factors (based on the triple bottom line approach) to ensure sustainability when integrating such an infrastructure. These sustainability features directly impact BDM concerning resource optimisation, stakeholder engagement, regulatory compliance and environmental impacts. To further understand these segments, taxonomies are defined to evaluate blockchain and DT sustainability features based on an in-depth review of the current state-of-the-art research. Detailed comparative evaluations provide insight into the reachability of the sustainable solution in terms of ideologies, access control and performance overheads. Several research questions are put forward to motivate further research that significantly impacts BDM. Finally, a case study based on an exemplary supply chain management system is presented to show the interoperability of blockchain and DT with BDM.","sentences":["Industry 4.0 and beyond will rely heavily on sustainable Business Decision Modelling (BDM) that can be accelerated by blockchain and Digital Twin (DT) solutions.","BDM is built on models and frameworks refined by key identification factors, data analysis, and mathematical or computational aspects applicable to complex business scenarios.","Gaining actionable intelligence from collected data for BDM requires a carefully considered infrastructure to ensure data transparency, security, accessibility and sustainability.","Organisations should consider social, economic and environmental factors (based on the triple bottom line approach) to ensure sustainability when integrating such an infrastructure.","These sustainability features directly impact BDM concerning resource optimisation, stakeholder engagement, regulatory compliance and environmental impacts.","To further understand these segments, taxonomies are defined to evaluate blockchain and DT sustainability features based on an in-depth review of the current state-of-the-art research.","Detailed comparative evaluations provide insight into the reachability of the sustainable solution in terms of ideologies, access control and performance overheads.","Several research questions are put forward to motivate further research that significantly impacts BDM.","Finally, a case study based on an exemplary supply chain management system is presented to show the interoperability of blockchain and DT with BDM."],"url":"http://arxiv.org/abs/2405.12101v1","category":"cs.NI"}
{"created":"2024-05-20 15:13:03","title":"Chemical control of self-assembly by the electrosolvation force","abstract":"Self-assembly of matter in solution generally relies on attractive interactions that overcome entropy and drive the formation of higher-order molecular and particulate structures. Such interactions play key roles in a variety of contexts, e.g., crystallisation, biomolecular folding and condensation, pathological protein aggregation, pharmaceuticals and fine chemicals. The electrosolvation force entails a new conceptual paradigm in the known palette of interactions that drive the spontaneous accretion and organisation of matter. However, an understanding of the underlying physical chemistry, and therefore the ability to exert control over and tune the interaction, remains incomplete. Here we demonstrate that this force arises from the structure of the interfacial electrolyte. Neutral molecules such as a different solvent, osmolytes or surfactants, can - even at very low concentrations in the medium - disrupt or reinforce pre-existing interfacial solvent structure, thereby furnishing unanticipated chemical tuning of the ability of matter to self-assemble. The observations further present unexpected mechanistic elements that may explain the impact of co-solvents and osmolytes on protein structure, stability and pathological protein condensation. Our findings shed new light on microscopic mechanisms that drive the emergence of order and structure from molecular to macroscopic scales in the solution phase.","sentences":["Self-assembly of matter in solution generally relies on attractive interactions that overcome entropy and drive the formation of higher-order molecular and particulate structures.","Such interactions play key roles in a variety of contexts, e.g., crystallisation, biomolecular folding and condensation, pathological protein aggregation, pharmaceuticals and fine chemicals.","The electrosolvation force entails a new conceptual paradigm in the known palette of interactions that drive the spontaneous accretion and organisation of matter.","However, an understanding of the underlying physical chemistry, and therefore the ability to exert control over and tune the interaction, remains incomplete.","Here we demonstrate that this force arises from the structure of the interfacial electrolyte.","Neutral molecules such as a different solvent, osmolytes or surfactants, can - even at very low concentrations in the medium - disrupt or reinforce pre-existing interfacial solvent structure, thereby furnishing unanticipated chemical tuning of the ability of matter to self-assemble.","The observations further present unexpected mechanistic elements that may explain the impact of co-solvents and osmolytes on protein structure, stability and pathological protein condensation.","Our findings shed new light on microscopic mechanisms that drive the emergence of order and structure from molecular to macroscopic scales in the solution phase."],"url":"http://arxiv.org/abs/2405.12099v1","category":"cond-mat.soft"}
{"created":"2024-05-20 15:03:42","title":"A Possible Additional Formation Pathway for the Interstellar Diatomic SiS","abstract":"The formation of silicon monosulfide (SiS) in space appears to be a difficult process, but the present work is showing that a previously excluded pathway may contribute to its astronomical abundance. Reaction of the radicals SH + SiH produces SiS with a submerged transition state and generates a stabilizing H$_2$ molecule as a product to dissipate the kinetic energy. Such is a textbook chemical reaction for favorable gas-phase chemistry. While previously proposed mechanisms reacting atomic sulfur and silicon with SiH, SH, and H$_2$S will still be major contributors to the production of SiS, an abundance of SiS in certain regions could be a marker for the presence of SiH where it has previously been unobserved. These quantum chemically-computed reaction profiles imply that the silicon-chalcogen chemistry of molecular clouds, shocked regions, or protoplanetary disks may be richer than previously thought. Quantum chemical spectral data for the intermediate cis- and trans-HSiSH are also provided in order to aid in their potential spectroscopic characterization.","sentences":["The formation of silicon monosulfide (SiS) in space appears to be a difficult process, but the present work is showing that a previously excluded pathway may contribute to its astronomical abundance.","Reaction of the radicals SH + SiH produces SiS with a submerged transition state and generates a stabilizing H$_2$ molecule as a product to dissipate the kinetic energy.","Such is a textbook chemical reaction for favorable gas-phase chemistry.","While previously proposed mechanisms reacting atomic sulfur and silicon with SiH, SH, and H$_2$S will still be major contributors to the production of SiS, an abundance of SiS in certain regions could be a marker for the presence of SiH where it has previously been unobserved.","These quantum chemically-computed reaction profiles imply that the silicon-chalcogen chemistry of molecular clouds, shocked regions, or protoplanetary disks may be richer than previously thought.","Quantum chemical spectral data for the intermediate cis- and trans-HSiSH are also provided in order to aid in their potential spectroscopic characterization."],"url":"http://arxiv.org/abs/2405.12092v1","category":"astro-ph.GA"}
{"created":"2024-05-20 15:02:13","title":"Using Formal Verification to Evaluate Single Event Upsets in a RISC-V Core","abstract":"Reliability has been a major concern in embedded systems. Higher transistor density and lower voltage supply increase the vulnerability of embedded systems to soft errors. A Single Event Upset (SEU), which is also called a soft error, can reverse a bit in a sequential element, resulting in a system failure. Simulation-based fault injection has been widely used to evaluate reliability, as suggested by ISO26262. However, it is practically impossible to test all faults for a complex design. Random fault injection is a compromise that reduces accuracy and fault coverage. Formal verification is an alternative approach. In this paper, we use formal verification, in the form of model checking, to evaluate the hardware reliability of a RISC-V Ibex Core in the presence of soft errors. Backward tracing is performed to identify and categorize faults according to their effects (no effect, Silent Data Corruption, crashes, and hangs). By using formal verification, the entire state space and fault list can be exhaustively explored. It is found that misaligned instructions can amplify fault effects. It is also found that some bits are more vulnerable to SEUs than others. In general, most of the bits in the Ibex Core are vulnerable to Silent Data Corruption, and the second pipeline stage is more vulnerable to Silent Data Corruption than the first.","sentences":["Reliability has been a major concern in embedded systems.","Higher transistor density and lower voltage supply increase the vulnerability of embedded systems to soft errors.","A Single Event Upset (SEU), which is also called a soft error, can reverse a bit in a sequential element, resulting in a system failure.","Simulation-based fault injection has been widely used to evaluate reliability, as suggested by ISO26262.","However, it is practically impossible to test all faults for a complex design.","Random fault injection is a compromise that reduces accuracy and fault coverage.","Formal verification is an alternative approach.","In this paper, we use formal verification, in the form of model checking, to evaluate the hardware reliability of a RISC-V Ibex Core in the presence of soft errors.","Backward tracing is performed to identify and categorize faults according to their effects (no effect, Silent Data Corruption, crashes, and hangs).","By using formal verification, the entire state space and fault list can be exhaustively explored.","It is found that misaligned instructions can amplify fault effects.","It is also found that some bits are more vulnerable to SEUs than others.","In general, most of the bits in the Ibex Core are vulnerable to Silent Data Corruption, and the second pipeline stage is more vulnerable to Silent Data Corruption than the first."],"url":"http://arxiv.org/abs/2405.12089v1","category":"cs.AR"}
{"created":"2024-05-20 14:59:01","title":"Product representation of perfect cubes","abstract":"Let $F_{k,d}(n)$ be the maximal size of a set ${A}\\subseteq [n]$ such that the equation \\[a_1a_2\\dots a_k=x^d, \\; a_1<a_2<\\ldots<a_k\\] has no solution with $a_1,a_2,\\ldots,a_k\\in {A}$ and integer $x$. Erd\\H{o}s, S\\'ark\\\"ozy and T. S\\'os studied $F_{k,2}$, and gave bounds when $k=2,3,4,6$ and also in the general case. We study the problem for $d=3$, and provide bounds for $k=2,3,4,6$ and $9$, furthermore, in the general case, as well. In particular, we refute an 18 years old conjecture of Verstra\\\"ete.   We also introduce another function $f_{k,d}$ closely related to $F_{k,d}$: While the original problem requires $a_1, \\ldots , a_k$ to all be distinct, we can relax this and only require that the multiset of the $a_i$'s cannot be partitioned into $d$-tuples where each $d$-tuple consists of $d$ copies of the same number.","sentences":["Let $F_{k,d}(n)$ be the maximal size of a set ${A}\\subseteq [n]$ such that the equation \\[a_1a_2\\dots a_k=x^d, \\; a_1<a_2<\\ldots<a_k\\] has no solution with $a_1,a_2,\\ldots,a_k\\in {A}$ and integer $x$. Erd\\H{o}s, S\\'ark\\\"ozy and T. S\\'os studied $F_{k,2}$, and gave bounds when $k=2,3,4,6$ and also in the general case.","We study the problem for $d=3$, and provide bounds for $k=2,3,4,6$ and $9$, furthermore, in the general case, as well.","In particular, we refute an 18 years old conjecture of Verstra\\\"ete.   ","We also introduce another function $f_{k,d}$ closely related to $F_{k,d}$: While the original problem requires $a_1, \\ldots , a_k$ to all be distinct, we can relax this and only require that the multiset of the $a_i$'s cannot be partitioned into $d$-tuples where each $d$-tuple consists of $d$ copies of the same number."],"url":"http://arxiv.org/abs/2405.12088v1","category":"math.CO"}
{"created":"2024-05-20 14:43:46","title":"GAN-GRID: A Novel Generative Attack on Smart Grid Stability Prediction","abstract":"The smart grid represents a pivotal innovation in modernizing the electricity sector, offering an intelligent, digitalized energy network capable of optimizing energy delivery from source to consumer. It hence represents the backbone of the energy sector of a nation. Due to its central role, the availability of the smart grid is paramount and is hence necessary to have in-depth control of its operations and safety. To this aim, researchers developed multiple solutions to assess the smart grid's stability and guarantee that it operates in a safe state. Artificial intelligence and Machine learning algorithms have proven to be effective measures to accurately predict the smart grid's stability. Despite the presence of known adversarial attacks and potential solutions, currently, there exists no standardized measure to protect smart grids against this threat, leaving them open to new adversarial attacks. In this paper, we propose GAN-GRID a novel adversarial attack targeting the stability prediction system of a smart grid tailored to real-world constraints. Our findings reveal that an adversary armed solely with the stability model's output, devoid of data or model knowledge, can craft data classified as stable with an Attack Success Rate (ASR) of 0.99. Also by manipulating authentic data and sensor values, the attacker can amplify grid issues, potentially undetected due to a compromised stability prediction system. These results underscore the imperative of fortifying smart grid security mechanisms against adversarial manipulation to uphold system stability and reliability.","sentences":["The smart grid represents a pivotal innovation in modernizing the electricity sector, offering an intelligent, digitalized energy network capable of optimizing energy delivery from source to consumer.","It hence represents the backbone of the energy sector of a nation.","Due to its central role, the availability of the smart grid is paramount and is hence necessary to have in-depth control of its operations and safety.","To this aim, researchers developed multiple solutions to assess the smart grid's stability and guarantee that it operates in a safe state.","Artificial intelligence and Machine learning algorithms have proven to be effective measures to accurately predict the smart grid's stability.","Despite the presence of known adversarial attacks and potential solutions, currently, there exists no standardized measure to protect smart grids against this threat, leaving them open to new adversarial attacks.","In this paper, we propose GAN-GRID a novel adversarial attack targeting the stability prediction system of a smart grid tailored to real-world constraints.","Our findings reveal that an adversary armed solely with the stability model's output, devoid of data or model knowledge, can craft data classified as stable with an Attack Success Rate (ASR) of 0.99.","Also by manipulating authentic data and sensor values, the attacker can amplify grid issues, potentially undetected due to a compromised stability prediction system.","These results underscore the imperative of fortifying smart grid security mechanisms against adversarial manipulation to uphold system stability and reliability."],"url":"http://arxiv.org/abs/2405.12076v1","category":"cs.CR"}
{"created":"2024-05-20 14:42:28","title":"Quazinormal modes and greybody factor of black hole surrounded by a quintessence in the S-V-T modified gravity as well as shadow","abstract":"The purpose of this study is to investigate the quasinormal modes (QNMs), greybody factors (GFs) and shadows in a plasma of a black hole (BH) surrounded by an exotic fluid of quintessence type in a scalar-vector-tensor modified gravity. The effects of a quintessence scalar field and the modified gravity (MOG) field on the QNM, GF, and shadow are examined. Using the sixth-order WKB approach, we investigate the QNMs of massless scalar and electromagnetic perturbations. Our findings show that as the quintessence and the MOG parameter ($\\epsilon$ and $\\alpha$) increase, the oscillation frequencies decrease significantly. Gravitational wave damping, on the other hand, decreases with increasing $\\epsilon$ and $\\alpha$. In addition, we obtain an analytical solution for the transmission coefficients (GF) and demonstrate that more thermal radiation reaches the observer at spatial infinity as both the $\\epsilon$ and $\\alpha$ parameters increase. We also investigate the effect of the plasma background on the BH shadow and show that as the plasma background parameter increases, the shadow radius slightly shrinks. Nevertheless, the shadow radius increases as $\\alpha$ and $\\epsilon$ increase. Particularly intriguing is the fact that increasing $\\epsilon$ has a greater impact on the shadow radius than increasing $\\alpha$, indicating that the quintessence parameter has a greater impact than the MOG parameter.","sentences":["The purpose of this study is to investigate the quasinormal modes (QNMs), greybody factors (GFs) and shadows in a plasma of a black hole (BH) surrounded by an exotic fluid of quintessence type in a scalar-vector-tensor modified gravity.","The effects of a quintessence scalar field and the modified gravity (MOG) field on the QNM, GF, and shadow are examined.","Using the sixth-order WKB approach, we investigate the QNMs of massless scalar and electromagnetic perturbations.","Our findings show that as the quintessence and the MOG parameter ($\\epsilon$ and $\\alpha$) increase, the oscillation frequencies decrease significantly.","Gravitational wave damping, on the other hand, decreases with increasing $\\epsilon$ and","$\\alpha$. In addition, we obtain an analytical solution for the transmission coefficients (GF) and demonstrate that more thermal radiation reaches the observer at spatial infinity as both the $\\epsilon$ and $\\alpha$ parameters increase.","We also investigate the effect of the plasma background on the BH shadow and show that as the plasma background parameter increases, the shadow radius slightly shrinks.","Nevertheless, the shadow radius increases as $\\alpha$ and $\\epsilon$ increase.","Particularly intriguing is the fact that increasing $\\epsilon$ has a greater impact on the shadow radius than increasing $\\alpha$, indicating that the quintessence parameter has a greater impact than the MOG parameter."],"url":"http://arxiv.org/abs/2405.12074v1","category":"gr-qc"}
{"created":"2024-05-20 14:41:48","title":"Goal-Oriented Communication for Networked Control Assisted by Reconfigurable Meta-Surfaces","abstract":"In this paper, we develop a theoretical framework for goal-oriented communication assisted by reconfigurable meta-surfaces in the context of networked control systems. The relation to goal-oriented communication stems from the fact that optimization of the phase shifts of the meta-surfaces is guided by the performance of networked control systems tasks. To that end, we consider a networked control system in which a set of sensors observe the states of a set of physical processes, and communicate this information over an unreliable wireless channel assisted by a reconfigurable intelligent surface with multiple reflecting elements to a set of controllers that correct the behaviors of the physical processes based on the received information. Our objective is to find the optimal control policy for the controllers and the optimal phase policy for the reconfigurable intelligent surface that jointly minimize a regulation cost function associated with the networked control system. We characterize these policies, and also propose an approximate solution based on a semi-definite relaxation technique.","sentences":["In this paper, we develop a theoretical framework for goal-oriented communication assisted by reconfigurable meta-surfaces in the context of networked control systems.","The relation to goal-oriented communication stems from the fact that optimization of the phase shifts of the meta-surfaces is guided by the performance of networked control systems tasks.","To that end, we consider a networked control system in which a set of sensors observe the states of a set of physical processes, and communicate this information over an unreliable wireless channel assisted by a reconfigurable intelligent surface with multiple reflecting elements to a set of controllers that correct the behaviors of the physical processes based on the received information.","Our objective is to find the optimal control policy for the controllers and the optimal phase policy for the reconfigurable intelligent surface that jointly minimize a regulation cost function associated with the networked control system.","We characterize these policies, and also propose an approximate solution based on a semi-definite relaxation technique."],"url":"http://arxiv.org/abs/2405.12073v1","category":"cs.IT"}
{"created":"2024-05-20 14:40:26","title":"AutoSoccerPose: Automated 3D posture Analysis of Soccer Shot Movements","abstract":"Image understanding is a foundational task in computer vision, with recent applications emerging in soccer posture analysis. However, existing publicly available datasets lack comprehensive information, notably in the form of posture sequences and 2D pose annotations. Moreover, current analysis models often rely on interpretable linear models (e.g., PCA and regression), limiting their capacity to capture non-linear spatiotemporal relationships in complex and diverse scenarios. To address these gaps, we introduce the 3D Shot Posture (3DSP) dataset in soccer broadcast videos, which represents the most extensive sports image dataset with 2D pose annotations to our knowledge. Additionally, we present the 3DSP-GRAE (Graph Recurrent AutoEncoder) model, a non-linear approach for embedding pose sequences. Furthermore, we propose AutoSoccerPose, a pipeline aimed at semi-automating 2D and 3D pose estimation and posture analysis. While achieving full automation proved challenging, we provide a foundational baseline, extending its utility beyond the scope of annotated data. We validate AutoSoccerPose on SoccerNet and 3DSP datasets, and present posture analysis results based on 3DSP. The dataset, code, and models are available at: https://github.com/calvinyeungck/3D-Shot-Posture-Dataset.","sentences":["Image understanding is a foundational task in computer vision, with recent applications emerging in soccer posture analysis.","However, existing publicly available datasets lack comprehensive information, notably in the form of posture sequences and","2D pose annotations.","Moreover, current analysis models often rely on interpretable linear models (e.g., PCA and regression), limiting their capacity to capture non-linear spatiotemporal relationships in complex and diverse scenarios.","To address these gaps, we introduce the 3D Shot Posture (3DSP) dataset in soccer broadcast videos, which represents the most extensive sports image dataset with 2D pose annotations to our knowledge.","Additionally, we present the 3DSP-GRAE (Graph Recurrent AutoEncoder) model, a non-linear approach for embedding pose sequences.","Furthermore, we propose AutoSoccerPose, a pipeline aimed at semi-automating 2D and 3D pose estimation and posture analysis.","While achieving full automation proved challenging, we provide a foundational baseline, extending its utility beyond the scope of annotated data.","We validate AutoSoccerPose on SoccerNet and 3DSP datasets, and present posture analysis results based on 3DSP.","The dataset, code, and models are available at: https://github.com/calvinyeungck/3D-Shot-Posture-Dataset."],"url":"http://arxiv.org/abs/2405.12070v1","category":"cs.CV"}
{"created":"2024-05-20 14:34:01","title":"CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models","abstract":"Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct ~12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs. Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge. In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs. Our dataset is available at https://github.com/zt991211/CLAMBER","sentences":["Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction.","To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy.","Building upon the taxonomy, we construct ~12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs.","Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting.","These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity.","Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge.","In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs.","Our dataset is available at https://github.com/zt991211/CLAMBER"],"url":"http://arxiv.org/abs/2405.12063v1","category":"cs.CL"}
{"created":"2024-05-20 14:15:40","title":"Cosmological inhomogeneities, primordial black holes, and a hypothesis on the death of the universe","abstract":"We study the impact of the expansion of the universe on a broad class of objects, including black holes, neutron stars, white dwarfs, and others. Using metrics that incorporate primordial inhomogeneities, the effects of a hypothetical \"center of the universe\" on inflation are calculated. Dynamic coordinates for black holes that account for expansions or contractions with arbitrary rates are provided. We consider the possibility that the universe may be bound to evolve into an ultimate state of \"total dilution\", wherein stable particles are so widely separated that physical communication among them will be impossible for eternity. This is also a scenario of \"cosmic virtuality\", as no wave-function collapse would occur again. We provide classical models evolving this way, based on the Majumdar-Papapetrou geometries. More realistic configurations, instead, indicate that gravitational forces locally counteract expansion, except in the universe's early stages. We comment on whether quantum phenomena may dictate that total dilution is indeed the cosmos' ultimate destiny.","sentences":["We study the impact of the expansion of the universe on a broad class of objects, including black holes, neutron stars, white dwarfs, and others.","Using metrics that incorporate primordial inhomogeneities, the effects of a hypothetical \"center of the universe\" on inflation are calculated.","Dynamic coordinates for black holes that account for expansions or contractions with arbitrary rates are provided.","We consider the possibility that the universe may be bound to evolve into an ultimate state of \"total dilution\", wherein stable particles are so widely separated that physical communication among them will be impossible for eternity.","This is also a scenario of \"cosmic virtuality\", as no wave-function collapse would occur again.","We provide classical models evolving this way, based on the Majumdar-Papapetrou geometries.","More realistic configurations, instead, indicate that gravitational forces locally counteract expansion, except in the universe's early stages.","We comment on whether quantum phenomena may dictate that total dilution is indeed the cosmos' ultimate destiny."],"url":"http://arxiv.org/abs/2405.12050v1","category":"gr-qc"}
{"created":"2024-05-20 14:15:19","title":"Micro-cosmos model of a nucleon","abstract":"This study explores the age-old quest to construct a geometric model of a quantum particle. While static classical particle models have largely been dismissed, the focus has now shifted to intricate dynamic models that hold the promise of reconciling general relativity with quantum mechanics. We propose that matter particles can be described as radiation confined within dynamically curved spacetime regions, without the need for quantization of space and time, and using standard field equations and natural Planck units. Specifically, we investigate a cyclic or oscillating radiation-dominated micro cosmos undergoing repeated bouncing. Our methodology employs integration, with carefully defined initial conditions. The results include several observable properties characteristic of quantum particles. We calculate the total mass, revealing a compelling inverse proportionality between mass and radius identical with the de Broglie relationship. Applying this model to protons, we discover a profound and surprisingly simple relationship between the proton's radius and mass expressed in Planck units. This enables a definition of the proton radius that aligns remarkably well with the 2018 CODATA value. Furthermore, our analysis demonstrates that the radial density profile of the proton (or nucleon), averaged over a cycle time, increases toward the center. The problem of embedding the micro cosmos within a background spacetime is also described. These results underscore the relevance of general relativity in the domain of nuclear physics. Moreover, the model offers a fresh perspective that can stimulate new ideas in the ongoing quest to unify general relativity with quantum physics.","sentences":["This study explores the age-old quest to construct a geometric model of a quantum particle.","While static classical particle models have largely been dismissed, the focus has now shifted to intricate dynamic models that hold the promise of reconciling general relativity with quantum mechanics.","We propose that matter particles can be described as radiation confined within dynamically curved spacetime regions, without the need for quantization of space and time, and using standard field equations and natural Planck units.","Specifically, we investigate a cyclic or oscillating radiation-dominated micro cosmos undergoing repeated bouncing.","Our methodology employs integration, with carefully defined initial conditions.","The results include several observable properties characteristic of quantum particles.","We calculate the total mass, revealing a compelling inverse proportionality between mass and radius identical with the de Broglie relationship.","Applying this model to protons, we discover a profound and surprisingly simple relationship between the proton's radius and mass expressed in Planck units.","This enables a definition of the proton radius that aligns remarkably well with the 2018 CODATA value.","Furthermore, our analysis demonstrates that the radial density profile of the proton (or nucleon), averaged over a cycle time, increases toward the center.","The problem of embedding the micro cosmos within a background spacetime is also described.","These results underscore the relevance of general relativity in the domain of nuclear physics.","Moreover, the model offers a fresh perspective that can stimulate new ideas in the ongoing quest to unify general relativity with quantum physics."],"url":"http://arxiv.org/abs/2405.12049v1","category":"gr-qc"}
{"created":"2024-05-20 14:13:22","title":"Energy-Efficient Federated Edge Learning with Streaming Data: A Lyapunov Optimization Approach","abstract":"Federated learning (FL) has received significant attention in recent years for its advantages in efficient training of machine learning models across distributed clients without disclosing user-sensitive data. Specifically, in federated edge learning (FEEL) systems, the time-varying nature of wireless channels introduces inevitable system dynamics in the communication process, thereby affecting training latency and energy consumption. In this work, we further consider a streaming data scenario where new training data samples are randomly generated over time at edge devices. Our goal is to develop a dynamic scheduling and resource allocation algorithm to address the inherent randomness in data arrivals and resource availability under long-term energy constraints. To achieve this, we formulate a stochastic network optimization problem and use the Lyapunov drift-plus-penalty framework to obtain a dynamic resource management design. Our proposed algorithm makes adaptive decisions on device scheduling, computational capacity adjustment, and allocation of bandwidth and transmit power in every round. We provide convergence analysis for the considered setting with heterogeneous data and time-varying objective functions, which supports the rationale behind our proposed scheduling design. The effectiveness of our scheme is verified through simulation results, demonstrating improved learning performance and energy efficiency as compared to baseline schemes.","sentences":["Federated learning (FL) has received significant attention in recent years for its advantages in efficient training of machine learning models across distributed clients without disclosing user-sensitive data.","Specifically, in federated edge learning (FEEL) systems, the time-varying nature of wireless channels introduces inevitable system dynamics in the communication process, thereby affecting training latency and energy consumption.","In this work, we further consider a streaming data scenario where new training data samples are randomly generated over time at edge devices.","Our goal is to develop a dynamic scheduling and resource allocation algorithm to address the inherent randomness in data arrivals and resource availability under long-term energy constraints.","To achieve this, we formulate a stochastic network optimization problem and use the Lyapunov drift-plus-penalty framework to obtain a dynamic resource management design.","Our proposed algorithm makes adaptive decisions on device scheduling, computational capacity adjustment, and allocation of bandwidth and transmit power in every round.","We provide convergence analysis for the considered setting with heterogeneous data and time-varying objective functions, which supports the rationale behind our proposed scheduling design.","The effectiveness of our scheme is verified through simulation results, demonstrating improved learning performance and energy efficiency as compared to baseline schemes."],"url":"http://arxiv.org/abs/2405.12046v1","category":"cs.LG"}
{"created":"2024-05-20 14:10:35","title":"The Incoherency Risk in the EU's New Cyber Security Policies","abstract":"The European Union (EU) has been pursuing new cyber security policies in recent years. This paper presents a short evaluation of four such policies. The focus is on potential incoherency, meaning a lack of integration, divergence between the member states, institutional dysfunction, and other related problems that should be at least partially avoidable by sound policy-making. According to the evaluation, the four policies have substantially increased the complexity of the EU's cyber security framework. In addition, there are potential problems with trust, divergence between industry sectors and different technologies, bureaucratic conflicts, and technical issues, among other things. With these insights, the paper not only contributes to the study of EU policies but also advances the understanding of cyber security policies in general.","sentences":["The European Union (EU) has been pursuing new cyber security policies in recent years.","This paper presents a short evaluation of four such policies.","The focus is on potential incoherency, meaning a lack of integration, divergence between the member states, institutional dysfunction, and other related problems that should be at least partially avoidable by sound policy-making.","According to the evaluation, the four policies have substantially increased the complexity of the EU's cyber security framework.","In addition, there are potential problems with trust, divergence between industry sectors and different technologies, bureaucratic conflicts, and technical issues, among other things.","With these insights, the paper not only contributes to the study of EU policies but also advances the understanding of cyber security policies in general."],"url":"http://arxiv.org/abs/2405.12043v1","category":"cs.CR"}
{"created":"2024-05-20 14:08:32","title":"Earthquakes and the wealth of nations: The cases of Chile and New Zealand","abstract":"The consequences of natural disasters, such as earthquakes, are evident: death, coordination problems, destruction of infrastructure, and displacement of population. However, according to empirical research, the impact of a natural disaster on economic activity is mixed. Natural disasters could have significant economic effects, especially in developing economies. This is particularly important for highly seismic countries such as Chile and New Zealand. This paper contributes to the literature on natural disasters and economic development by analyzing the cases of two affected regions within these countries in the wake of major earthquakes experienced during 2010-2011: Maule (Chile) and Canterbury (New Zealand). We examine the impact of natural disasters on GDP per capita by applying the synthetic control method. Using the synthetic approach, we assess the effects of these two earthquakes by building counterfactuals to compare their recovery trajectories. We find that Chile and New Zealand experienced opposite economic effects. The Canterbury region grew 10% more in three years than its synthetic counterfactual without the earthquake, while the Maule region declined by 5%. We build synthetic controls at a regional and economic-sector level, looking at aggregated and sectoral effects. The difference in institutions, such as property rights and the large amount of government spending given for reconstruction after the New Zealand earthquake relative to Chile's, help to explain the difference in outcomes.","sentences":["The consequences of natural disasters, such as earthquakes, are evident: death, coordination problems, destruction of infrastructure, and displacement of population.","However, according to empirical research, the impact of a natural disaster on economic activity is mixed.","Natural disasters could have significant economic effects, especially in developing economies.","This is particularly important for highly seismic countries such as Chile and New Zealand.","This paper contributes to the literature on natural disasters and economic development by analyzing the cases of two affected regions within these countries in the wake of major earthquakes experienced during 2010-2011:","Maule (Chile) and Canterbury (New Zealand).","We examine the impact of natural disasters on GDP per capita by applying the synthetic control method.","Using the synthetic approach, we assess the effects of these two earthquakes by building counterfactuals to compare their recovery trajectories.","We find that Chile and New Zealand experienced opposite economic effects.","The Canterbury region grew 10% more in three years than its synthetic counterfactual without the earthquake, while the Maule region declined by 5%.","We build synthetic controls at a regional and economic-sector level, looking at aggregated and sectoral effects.","The difference in institutions, such as property rights and the large amount of government spending given for reconstruction after the New Zealand earthquake relative to Chile's, help to explain the difference in outcomes."],"url":"http://arxiv.org/abs/2405.12041v1","category":"econ.GN"}
{"created":"2024-05-20 14:05:04","title":"Constraints and Time Evolution in Generic $f$(Riemann) Gravity","abstract":"We give a detailed canonical analysis of the $n$-dimensional $f$(Riemann) gravity, correcting the earlier results in the literature. We also write the field equations in the Fischer-Marsden form which is amenable to identifying the non-stationary energy on a spacelike hypersurface. We give pure $R^{2}$ theory as an example.","sentences":["We give a detailed canonical analysis of the $n$-dimensional $f$(Riemann) gravity, correcting the earlier results in the literature.","We also write the field equations in the Fischer-Marsden form which is amenable to identifying the non-stationary energy on a spacelike hypersurface.","We give pure $R^{2}$ theory as an example."],"url":"http://arxiv.org/abs/2405.12037v1","category":"gr-qc"}
{"created":"2024-05-20 14:03:05","title":"KG-RAG: Bridging the Gap Between Knowledge and Creativity","abstract":"Ensuring factual accuracy while maintaining the creative capabilities of Large Language Model Agents (LMAs) poses significant challenges in the development of intelligent agent systems. LMAs face prevalent issues such as information hallucinations, catastrophic forgetting, and limitations in processing long contexts when dealing with knowledge-intensive tasks. This paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation) pipeline, a novel framework designed to enhance the knowledge capabilities of LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities of LLMs, thereby significantly reducing the reliance on the latent knowledge of LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then performs information retrieval over the newly created graph to perform KGQA (Knowledge Graph Question Answering). The retrieval methodology leverages a novel algorithm called Chain of Explorations (CoE) which benefits from LLMs reasoning to explore nodes and relationships within the KG sequentially. Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable improvements in the reduction of hallucinated content and suggest a promising path toward developing intelligent systems adept at handling knowledge-intensive tasks.","sentences":["Ensuring factual accuracy while maintaining the creative capabilities of Large Language Model Agents (LMAs) poses significant challenges in the development of intelligent agent systems.","LMAs face prevalent issues such as information hallucinations, catastrophic forgetting, and limitations in processing long contexts when dealing with knowledge-intensive tasks.","This paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation) pipeline, a novel framework designed to enhance the knowledge capabilities of LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities of LLMs, thereby significantly reducing the reliance on the latent knowledge of LLMs.","The KG-RAG pipeline constructs a KG from unstructured text and then performs information retrieval over the newly created graph to perform KGQA (Knowledge Graph Question Answering).","The retrieval methodology leverages a novel algorithm called Chain of Explorations (CoE) which benefits from LLMs reasoning to explore nodes and relationships within the KG sequentially.","Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable improvements in the reduction of hallucinated content and suggest a promising path toward developing intelligent systems adept at handling knowledge-intensive tasks."],"url":"http://arxiv.org/abs/2405.12035v1","category":"cs.AI"}
{"created":"2024-05-20 14:01:38","title":"Count-Min Sketch with Conservative Updates: Worst-Case Analysis","abstract":"Count-Min Sketch with Conservative Updates (\\texttt{CMS-CU}) is a memory-efficient hash-based data structure used to estimate the occurrences of items within a data stream. \\texttt{CMS-CU} stores~$m$ counters and employs~$d$ hash functions to map items to these counters. We first argue that the estimation error in \\texttt{CMS-CU} is maximal when each item appears at most once in the stream. Next, we study \\texttt{CMS-CU} in this setting. Precisely, \\begin{enumerate}   \\item In the case where~$d=m-1$, we prove that the average estimation error and the average counter rate converge almost surely to~$\\frac{1}{2}$, contrasting with the vanilla Count-Min Sketch, where the average counter rate is equal to~$\\frac{m-1}{m}$.   \\item For any given~$m$ and~$d$, we prove novel lower and upper bounds on the average estimation error, incorporating a positive integer parameter~$g$. Larger values of this parameter improve the accuracy of the bounds. Moreover, the computation of each bound involves examining an ergodic Markov process with a state space of size~$\\binom{m+g-d}{g}$ and a sparse transition probabilities matrix containing~$\\mathcal{O}(m\\binom{m+g-d}{g})$ non-zero entries.   \\item For~$d=m-1$, $g=1$, and as $m\\to \\infty$, we show that the lower and upper bounds coincide. In general, our bounds exhibit high accuracy for small values of $g$, as shown by numerical computation. For example, for $m=50$, $d=4$, and $g=5$, the difference between the lower and upper bounds is smaller than~$10^{-4}$.   \\end{enumerate}","sentences":["Count-Min Sketch with Conservative Updates (\\texttt{CMS-CU}) is a memory-efficient hash-based data structure used to estimate the occurrences of items within a data stream.","\\texttt{CMS-CU} stores~$m$ counters and employs~$d$ hash functions to map items to these counters.","We first argue that the estimation error in \\texttt{CMS-CU} is maximal when each item appears at most once in the stream.","Next, we study \\texttt{CMS-CU} in this setting.","Precisely, \\begin{enumerate}   \\item","In the case where~$d=m-1$, we prove that the average estimation error and the average counter rate converge almost surely to~$\\frac{1}{2}$, contrasting with the vanilla Count-Min Sketch, where the average counter rate is equal to~$\\frac{m-1}{m}$.   \\item","For any given~$m$ and~$d$, we prove novel lower and upper bounds on the average estimation error, incorporating a positive integer parameter~$g$. Larger values of this parameter improve the accuracy of the bounds.","Moreover, the computation of each bound involves examining an ergodic Markov process with a state space of size~$\\binom{m+g-d}{g}$ and a sparse transition probabilities matrix containing~$\\mathcal{O}(m\\binom{m+g-d}{g})$ non-zero entries.   ","\\item For~$d=m-1$, $g=1$, and as $m\\to \\infty$, we show that the lower and upper bounds coincide.","In general, our bounds exhibit high accuracy for small values of $g$, as shown by numerical computation.","For example, for $m=50$, $d=4$, and $g=5$, the difference between the lower and upper bounds is smaller than~$10^{-4}$.   \\end{enumerate}"],"url":"http://arxiv.org/abs/2405.12034v1","category":"cs.DS"}
{"created":"2024-05-20 13:55:19","title":"Neighborhood Attention Transformer with Progressive Channel Fusion for Speaker Verification","abstract":"Transformer-based architectures for speaker verification typically require more training data than ECAPA-TDNN. Therefore, recent work has generally been trained on VoxCeleb1&2. We propose a backbone network based on self-attention, which can achieve competitive results when trained on VoxCeleb2 alone. The network alternates between neighborhood attention and global attention to capture local and global features, then aggregates features of different hierarchical levels, and finally performs attentive statistical pooling. Additionally, we employ a progressive channel fusion strategy to expand the receptive field in the channel dimension as the network deepens. We trained the proposed PCF-NAT model on VoxCeleb2 and evaluated it on VoxCeleb1 and the validation sets of VoxSRC. The EER and minDCF of the shallow PCF-NAT are on average more than 20% lower than those of similarly sized ECAPA-TDNN. Deep PCF-NAT achieves an EER lower than 0.5% on VoxCeleb1-O.","sentences":["Transformer-based architectures for speaker verification typically require more training data than ECAPA-TDNN.","Therefore, recent work has generally been trained on VoxCeleb1&2.","We propose a backbone network based on self-attention, which can achieve competitive results when trained on VoxCeleb2 alone.","The network alternates between neighborhood attention and global attention to capture local and global features, then aggregates features of different hierarchical levels, and finally performs attentive statistical pooling.","Additionally, we employ a progressive channel fusion strategy to expand the receptive field in the channel dimension as the network deepens.","We trained the proposed PCF-NAT model on VoxCeleb2 and evaluated it on VoxCeleb1 and the validation sets of VoxSRC.","The EER and minDCF of the shallow PCF-NAT are on average more than 20% lower than those of similarly sized ECAPA-TDNN.","Deep PCF-NAT achieves an EER lower than 0.5% on VoxCeleb1-O."],"url":"http://arxiv.org/abs/2405.12031v1","category":"cs.SD"}
{"created":"2024-05-20 13:47:45","title":"Linearized gravity and soft graviton theorem in de Sitter spacetime","abstract":"We study the linearized gravity theory in the Newman-Unti gauge in the near horizon region of the de Sitter spacetime. The linearized Einstein equation involves the cosmological constant. The near horizon symmetry consists of near horizon supertranslation and near horizon superrotation. We compute the near horizon supertranslation charge and find the proper near horizon fall-off conditions which uncover a soft graviton theorem from the Ward identity of the near horizon supertranslation.","sentences":["We study the linearized gravity theory in the Newman-Unti gauge in the near horizon region of the de Sitter spacetime.","The linearized Einstein equation involves the cosmological constant.","The near horizon symmetry consists of near horizon supertranslation and near horizon superrotation.","We compute the near horizon supertranslation charge and find the proper near horizon fall-off conditions which uncover a soft graviton theorem from the Ward identity of the near horizon supertranslation."],"url":"http://arxiv.org/abs/2405.12027v1","category":"hep-th"}
{"created":"2024-05-20 13:43:20","title":"Estimating transmission noise on networks from stationary local order","abstract":"In this paper we study networks of nodes characterised by binary traits that change both endogenously and through nearest-neighbour interaction. Our analytical results show that those traits can be ranked according to the noisiness of their transmission using only measures of order in the stationary state. Crucially, this ranking is independent of network topology. As an example, we explain why, in line with a long-standing hypothesis, the relative stability of the structural traits of languages can be estimated from their geospatial distribution. We conjecture that similar inferences may be possible in a more general class of Markovian systems. Consequently, in many empirical domains where longitudinal information is not easily available the propensities of traits to change could be estimated from spatial data alone.","sentences":["In this paper we study networks of nodes characterised by binary traits that change both endogenously and through nearest-neighbour interaction.","Our analytical results show that those traits can be ranked according to the noisiness of their transmission using only measures of order in the stationary state.","Crucially, this ranking is independent of network topology.","As an example, we explain why, in line with a long-standing hypothesis, the relative stability of the structural traits of languages can be estimated from their geospatial distribution.","We conjecture that similar inferences may be possible in a more general class of Markovian systems.","Consequently, in many empirical domains where longitudinal information is not easily available the propensities of traits to change could be estimated from spatial data alone."],"url":"http://arxiv.org/abs/2405.12023v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-20 13:42:54","title":"Lattice physics approaches for neural networks","abstract":"Modern neuroscience has evolved into a frontier field that draws on numerous disciplines, resulting in the flourishing of novel conceptual frames primarily inspired by physics and complex systems science. Contributing in this direction, we recently introduced a mathematical framework to describe the spatiotemporal interactions of systems of neurons using lattice field theory, the reference paradigm for theoretical particle physics. In this note, we provide a concise summary of the basics of the theory, aiming to be intuitive to the interdisciplinary neuroscience community. We contextualize our methods, illustrating how to readily connect the parameters of our formulation to experimental variables using well-known renormalization procedures. This synopsis yields the key concepts needed to describe neural networks using lattice physics. Such classes of methods are attention-worthy in an era of blistering improvements in numerical computations, as they can facilitate relating the observation of neural activity to generative models underpinned by physical principles.","sentences":["Modern neuroscience has evolved into a frontier field that draws on numerous disciplines, resulting in the flourishing of novel conceptual frames primarily inspired by physics and complex systems science.","Contributing in this direction, we recently introduced a mathematical framework to describe the spatiotemporal interactions of systems of neurons using lattice field theory, the reference paradigm for theoretical particle physics.","In this note, we provide a concise summary of the basics of the theory, aiming to be intuitive to the interdisciplinary neuroscience community.","We contextualize our methods, illustrating how to readily connect the parameters of our formulation to experimental variables using well-known renormalization procedures.","This synopsis yields the key concepts needed to describe neural networks using lattice physics.","Such classes of methods are attention-worthy in an era of blistering improvements in numerical computations, as they can facilitate relating the observation of neural activity to generative models underpinned by physical principles."],"url":"http://arxiv.org/abs/2405.12022v1","category":"q-bio.NC"}
{"created":"2024-05-20 13:42:27","title":"Can AI Relate: Testing Large Language Model Response for Mental Health Support","abstract":"Large language models (LLMs) are already being piloted for clinical use in hospital systems like NYU Langone, Dana-Farber and the NHS. A proposed deployment use case is psychotherapy, where a LLM-powered chatbot can treat a patient undergoing a mental health crisis. Deployment of LLMs for mental health response could hypothetically broaden access to psychotherapy and provide new possibilities for personalizing care. However, recent high-profile failures, like damaging dieting advice offered by the Tessa chatbot to patients with eating disorders, have led to doubt about their reliability in high-stakes and safety-critical settings.   In this work, we develop an evaluation framework for determining whether LLM response is a viable and ethical path forward for the automation of mental health treatment. Using human evaluation with trained clinicians and automatic quality-of-care metrics grounded in psychology research, we compare the responses provided by peer-to-peer responders to those provided by a state-of-the-art LLM.   We show that LLMs like GPT-4 use implicit and explicit cues to infer patient demographics like race. We then show that there are statistically significant discrepancies between patient subgroups: Responses to Black posters consistently have lower empathy than for any other demographic group (2%-13% lower than the control group). Promisingly, we do find that the manner in which responses are generated significantly impacts the quality of the response. We conclude by proposing safety guidelines for the potential deployment of LLMs for mental health response.","sentences":["Large language models (LLMs) are already being piloted for clinical use in hospital systems like NYU Langone, Dana-Farber and the NHS.","A proposed deployment use case is psychotherapy, where a LLM-powered chatbot can treat a patient undergoing a mental health crisis.","Deployment of LLMs for mental health response could hypothetically broaden access to psychotherapy and provide new possibilities for personalizing care.","However, recent high-profile failures, like damaging dieting advice offered by the Tessa chatbot to patients with eating disorders, have led to doubt about their reliability in high-stakes and safety-critical settings.   ","In this work, we develop an evaluation framework for determining whether LLM response is a viable and ethical path forward for the automation of mental health treatment.","Using human evaluation with trained clinicians and automatic quality-of-care metrics grounded in psychology research, we compare the responses provided by peer-to-peer responders to those provided by a state-of-the-art LLM.   ","We show that LLMs like GPT-4 use implicit and explicit cues to infer patient demographics like race.","We then show that there are statistically significant discrepancies between patient subgroups: Responses to Black posters consistently have lower empathy than for any other demographic group (2%-13% lower than the control group).","Promisingly, we do find that the manner in which responses are generated significantly impacts the quality of the response.","We conclude by proposing safety guidelines for the potential deployment of LLMs for mental health response."],"url":"http://arxiv.org/abs/2405.12021v1","category":"cs.CL"}
{"created":"2024-05-20 13:41:41","title":"On model emulation and closure tests for 3+1D relativistic heavy-ion collisions","abstract":"In nuclear and particle physics, reconciling sophisticated simulations with experimental data is vital for understanding complex systems like the Quark Gluon Plasma (QGP) generated in heavy-ion collisions. However, computational demands pose challenges, motivating using Gaussian Process emulators for efficient parameter extraction via Bayesian calibration. We conduct a comparative analysis of Gaussian Process emulators in heavy-ion physics to identify the most adept emulator for parameter extraction with minimal uncertainty. Our study contributes to advancing computational techniques in heavy-ion physics, enhancing our ability to interpret experimental data and understand QGP properties.","sentences":["In nuclear and particle physics, reconciling sophisticated simulations with experimental data is vital for understanding complex systems like the Quark Gluon Plasma (QGP) generated in heavy-ion collisions.","However, computational demands pose challenges, motivating using Gaussian Process emulators for efficient parameter extraction via Bayesian calibration.","We conduct a comparative analysis of Gaussian Process emulators in heavy-ion physics to identify the most adept emulator for parameter extraction with minimal uncertainty.","Our study contributes to advancing computational techniques in heavy-ion physics, enhancing our ability to interpret experimental data and understand QGP properties."],"url":"http://arxiv.org/abs/2405.12019v1","category":"nucl-th"}
{"created":"2024-05-20 13:36:41","title":"Measurement of the absolute efficiency of the X-ARAPUCA photon detector for the DUNE Far Detector 1","abstract":"The Photon Detection System (PDS) of the first DUNE far detector (FD1) is composed of 6000 photon detection units, named X-ARAPUCA. The detection of the prompt light pulse generated by the particle energy release in liquid argon (LAr) will complement and boost the DUNE Liquid Argon Time Projection Chamber (LArTPC). It will improve the non-beam events tagging and enable at low energies the trigger and the calorimetry of the supernova neutrinos. The X-ARAPUCA unit is an assembly of several components. Its Photon Detection Efficiency (PDE) depends both on the design of the assembly, on the grade of the individual components and finally on their coupling. The X-ARAPUCA PDE is one of the leading parameters for the Photon Detection System sensitivity, that in turn determines the sensitivity of the DUNE for the detection of core-collapse supernova within the galaxy and for nucleon decay searches. In this work we present the final assessment of the absolute PDE of the FD1 X-ARAPUCA baseline design, measured in two laboratories with independent methods and setups. One hundred sixty units of these X-ARAPUCA devices have been deployed in the NP04 facility at the CERN Neutrino Platform, the 1:20 scale FD1 prototype, and will be operated during the year 2024. The assessed value of the PDE is a key parameter both in the NP04 and in the DUNE analysis and reconstruction studies.","sentences":["The Photon Detection System (PDS) of the first DUNE far detector (FD1) is composed of 6000 photon detection units, named X-ARAPUCA.","The detection of the prompt light pulse generated by the particle energy release in liquid argon (LAr) will complement and boost the DUNE Liquid Argon Time Projection Chamber (LArTPC).","It will improve the non-beam events tagging and enable at low energies the trigger and the calorimetry of the supernova neutrinos.","The X-ARAPUCA unit is an assembly of several components.","Its Photon Detection Efficiency (PDE) depends both on the design of the assembly, on the grade of the individual components and finally on their coupling.","The X-ARAPUCA PDE is one of the leading parameters for the Photon Detection System sensitivity, that in turn determines the sensitivity of the DUNE for the detection of core-collapse supernova within the galaxy and for nucleon decay searches.","In this work we present the final assessment of the absolute PDE of the FD1 X-ARAPUCA baseline design, measured in two laboratories with independent methods and setups.","One hundred sixty units of these X-ARAPUCA devices have been deployed in the NP04 facility at the CERN Neutrino Platform, the 1:20 scale FD1 prototype, and will be operated during the year 2024.","The assessed value of the PDE is a key parameter both in the NP04 and in the DUNE analysis and reconstruction studies."],"url":"http://arxiv.org/abs/2405.12014v1","category":"physics.ins-det"}
{"created":"2024-05-20 13:31:57","title":"Higher weight spectra of ternary codes associated to the quadratic Veronese $3$-fold","abstract":"The problem studied in this work is to determine the higher weight spectra of the Projective Reed-Muller codes associated to the Veronese $3$-fold $\\mathcal V$ in $PG(9,q)$, which is the image of the quadratic Veronese embedding of $PG(3,q)$ in $PG(9,q)$. We reduce the problem to the following combinatorial problem in finite geometry: For each subset $S$ of $\\mathcal V$, determine the dimension of the linear subspace of $PG(9,q)$ generated by $S$. We develop a systematic method to solve the latter problem. We implement the method for $q=3$, and use it to obtain the higher weight spectra of the associated code. The case of a general finite field $\\mathbb F_q$ will be treated in a future work.","sentences":["The problem studied in this work is to determine the higher weight spectra of the Projective Reed-Muller codes associated to the Veronese $3$-fold $\\mathcal V$ in $PG(9,q)$, which is the image of the quadratic Veronese embedding of $PG(3,q)$ in $PG(9,q)$. We reduce the problem to the following combinatorial problem in finite geometry: For each subset $S$ of $\\mathcal V$, determine the dimension of the linear subspace of $PG(9,q)$ generated by $S$. We develop a systematic method to solve the latter problem.","We implement the method for $q=3$, and use it to obtain the higher weight spectra of the associated code.","The case of a general finite field $\\mathbb F_q$ will be treated in a future work."],"url":"http://arxiv.org/abs/2405.12011v1","category":"math.CO"}
{"created":"2024-05-20 13:26:59","title":"Degenerations and Fibrations of K3 Surfaces: Lattice Polarisations and Mirror Symmetry","abstract":"Tyurin degenerations of K3 surfaces are degenerations whose central fibre consists of a pair of rational surfaces glued along a smooth elliptic curve. We study the lattice theory of such Tyurin degenerations, establishing a notion of lattice polarisation that is compatible with existing definitions for the general fibre and the rational surfaces comprising the central fibre. We separately consider elliptically fibred K3 surfaces, where the base of the fibration admits a splitting into a pair of discs with specified monodromy around the boundary. In this setting we establish a notion of lattice polarisation for the induced elliptic fibrations over discs, which is compatible with the existing definition for K3 surfaces. Finally, we discuss the mirror symmetric correspondence between these two settings.","sentences":["Tyurin degenerations of K3 surfaces are degenerations whose central fibre consists of a pair of rational surfaces glued along a smooth elliptic curve.","We study the lattice theory of such Tyurin degenerations, establishing a notion of lattice polarisation that is compatible with existing definitions for the general fibre and the rational surfaces comprising the central fibre.","We separately consider elliptically fibred K3 surfaces, where the base of the fibration admits a splitting into a pair of discs with specified monodromy around the boundary.","In this setting we establish a notion of lattice polarisation for the induced elliptic fibrations over discs, which is compatible with the existing definition for K3 surfaces.","Finally, we discuss the mirror symmetric correspondence between these two settings."],"url":"http://arxiv.org/abs/2405.12009v1","category":"math.AG"}
{"created":"2024-05-20 13:22:13","title":"Binary neutron star mergers as the source of the highest energy cosmic rays","abstract":"We propose that ultrahigh energy cosmic rays are produced in binary neutron star mergers. Interpreting the highest energy events as r-process nuclei eliminates the need for exotic sources, while the observed near-universal maximum rigidity of ultrahigh energy sources can be understood as due to the uniformity of jets generated by the gravitationally-driven dynamo, given the narrow range of total binary neutron star masses. We discuss evidence for this scenario, and its prediction of coincidences between neutrinos above 10 PeV and gravitational waves.","sentences":["We propose that ultrahigh energy cosmic rays are produced in binary neutron star mergers.","Interpreting the highest energy events as r-process nuclei eliminates the need for exotic sources, while the observed near-universal maximum rigidity of ultrahigh energy sources can be understood as due to the uniformity of jets generated by the gravitationally-driven dynamo, given the narrow range of total binary neutron star masses.","We discuss evidence for this scenario, and its prediction of coincidences between neutrinos above 10 PeV and gravitational waves."],"url":"http://arxiv.org/abs/2405.12004v1","category":"astro-ph.HE"}
{"created":"2024-05-20 13:19:02","title":"Mamba-in-Mamba: Centralized Mamba-Cross-Scan in Tokenized Mamba Model for Hyperspectral Image Classification","abstract":"Hyperspectral image (HSI) classification is pivotal in the remote sensing (RS) field, particularly with the advancement of deep learning techniques. Sequential models, adapted from the natural language processing (NLP) field such as Recurrent Neural Networks (RNNs) and Transformers, have been tailored to this task, offering a unique viewpoint. However, several challenges persist 1) RNNs struggle with centric feature aggregation and are sensitive to interfering pixels, 2) Transformers require significant computational resources and often underperform with limited HSI training samples, and 3) Current scanning methods for converting images into sequence-data are simplistic and inefficient. In response, this study introduces the innovative Mamba-in-Mamba (MiM) architecture for HSI classification, the first attempt of deploying State Space Model (SSM) in this task. The MiM model includes 1) A novel centralized Mamba-Cross-Scan (MCS) mechanism for transforming images into sequence-data, 2) A Tokenized Mamba (T-Mamba) encoder that incorporates a Gaussian Decay Mask (GDM), a Semantic Token Learner (STL), and a Semantic Token Fuser (STF) for enhanced feature generation and concentration, and 3) A Weighted MCS Fusion (WMF) module coupled with a Multi-Scale Loss Design to improve decoding efficiency. Experimental results from three public HSI datasets with fixed and disjoint training-testing samples demonstrate that our method outperforms existing baselines and state-of-the-art approaches, highlighting its efficacy and potential in HSI applications.","sentences":["Hyperspectral image (HSI) classification is pivotal in the remote sensing (RS) field, particularly with the advancement of deep learning techniques.","Sequential models, adapted from the natural language processing (NLP) field such as Recurrent Neural Networks (RNNs) and Transformers, have been tailored to this task, offering a unique viewpoint.","However, several challenges persist 1) RNNs struggle with centric feature aggregation and are sensitive to interfering pixels, 2) Transformers require significant computational resources and often underperform with limited HSI training samples, and 3) Current scanning methods for converting images into sequence-data are simplistic and inefficient.","In response, this study introduces the innovative Mamba-in-Mamba (MiM) architecture for HSI classification, the first attempt of deploying State Space Model (SSM) in this task.","The MiM model includes 1) A novel centralized Mamba-Cross-Scan (MCS) mechanism for transforming images into sequence-data, 2) A Tokenized Mamba (T-Mamba) encoder that incorporates a Gaussian Decay Mask (GDM), a Semantic Token Learner (STL), and a Semantic Token Fuser (STF) for enhanced feature generation and concentration, and 3) A Weighted MCS Fusion (WMF) module coupled with a Multi-Scale Loss Design to improve decoding efficiency.","Experimental results from three public HSI datasets with fixed and disjoint training-testing samples demonstrate that our method outperforms existing baselines and state-of-the-art approaches, highlighting its efficacy and potential in HSI applications."],"url":"http://arxiv.org/abs/2405.12003v1","category":"cs.CV"}
{"created":"2024-05-20 13:14:26","title":"Scrutinize What We Ignore: Reining Task Representation Shift In Context-Based Offline Meta Reinforcement Learning","abstract":"Offline meta reinforcement learning (OMRL) has emerged as a promising approach for interaction avoidance and strong generalization performance by leveraging pre-collected data and meta-learning techniques. Previous context-based approaches predominantly rely on the intuition that maximizing the mutual information between the task and the task representation ($I(Z;M)$) can lead to performance improvements. Despite achieving attractive results, the theoretical justification of performance improvement for such intuition has been lacking. Motivated by the return discrepancy scheme in the model-based RL field, we find that maximizing $I(Z;M)$ can be interpreted as consistently raising the lower bound of the expected return for a given policy conditioning on the optimal task representation. However, this optimization process ignores the task representation shift between two consecutive updates, which may lead to performance improvement collapse. To address this problem, we turn to use the framework of performance difference bound to consider the impacts of task representation shift explicitly. We demonstrate that by reining the task representation shift, it is possible to achieve monotonic performance improvements, thereby showcasing the advantage against previous approaches. To make it practical, we design an easy yet highly effective algorithm RETRO (\\underline{RE}ining \\underline{T}ask \\underline{R}epresentation shift in context-based \\underline{O}ffline meta reinforcement learning) with only adding one line of code compared to the backbone. Empirical results validate its state-of-the-art (SOTA) asymptotic performance, training stability and training-time consumption on MuJoCo and MetaWorld benchmarks.","sentences":["Offline meta reinforcement learning (OMRL) has emerged as a promising approach for interaction avoidance and strong generalization performance by leveraging pre-collected data and meta-learning techniques.","Previous context-based approaches predominantly rely on the intuition that maximizing the mutual information between the task and the task representation ($I(Z;M)$) can lead to performance improvements.","Despite achieving attractive results, the theoretical justification of performance improvement for such intuition has been lacking.","Motivated by the return discrepancy scheme in the model-based RL field, we find that maximizing $I(Z;M)$ can be interpreted as consistently raising the lower bound of the expected return for a given policy conditioning on the optimal task representation.","However, this optimization process ignores the task representation shift between two consecutive updates, which may lead to performance improvement collapse.","To address this problem, we turn to use the framework of performance difference bound to consider the impacts of task representation shift explicitly.","We demonstrate that by reining the task representation shift, it is possible to achieve monotonic performance improvements, thereby showcasing the advantage against previous approaches.","To make it practical, we design an easy yet highly effective algorithm RETRO (\\underline{RE}ining \\underline{T}ask \\underline{R}epresentation shift in context-based \\underline{O}ffline meta reinforcement learning) with only adding one line of code compared to the backbone.","Empirical results validate its state-of-the-art (SOTA) asymptotic performance, training stability and training-time consumption on MuJoCo and MetaWorld benchmarks."],"url":"http://arxiv.org/abs/2405.12001v1","category":"cs.LG"}
{"created":"2024-05-20 13:13:50","title":"Lifetime Characterization of Extreme Wave Localizations in Crossing Seas","abstract":"Rogue waves (RWs) can form on the ocean surface due to quasi-four wave resonant interaction or superposition principle. Both mechanisms have been acutely studied. The first of the two is known as the nonlinear focusing mechanism and leads to an increased probability of rogue waves when wave conditions are favourable, i.e., when unidirectionality and high narrowband energy of the wave field are satisfied. This work delves into the dynamics of extreme wave focusing in crossing seas, revealing a distinct type of nonlinear RWs, characterized by a decisive longevity compared to those generated by the dispersive focusing mechanism. In fact, through fully nonlinear hydrodynamic numerical simulations, we show that the interactions between two crossing unidirectional wave beams can trigger fully localized and robust development of RWs. These coherent structures, characterized by a typical spectral broadening then spreading in the form of dual bimodality and recurrent wave group focusing, not only defy the weakening expectation of quasi-four wave resonant interaction in directionally spread wave fields, but also differ from classical focusing mechanisms already mentioned. This has been determined following a rigorous lifespan-based statistical analysis of extreme wave events in our fully nonlinear simulations. Utilizing the coupled nonlinear Schr\\\"odinger framework, we also show that such intrinsic focusing dynamics can also be captured by weakly nonlinear wave evolution equations. This opens new research avenues for further explorations of these complex and intriguing wave phenomena in hydrodynamics as well as other nonlinear and dispersive multi-wave systems.","sentences":["Rogue waves (RWs) can form on the ocean surface due to quasi-four wave resonant interaction or superposition principle.","Both mechanisms have been acutely studied.","The first of the two is known as the nonlinear focusing mechanism and leads to an increased probability of rogue waves when wave conditions are favourable, i.e., when unidirectionality and high narrowband energy of the wave field are satisfied.","This work delves into the dynamics of extreme wave focusing in crossing seas, revealing a distinct type of nonlinear RWs, characterized by a decisive longevity compared to those generated by the dispersive focusing mechanism.","In fact, through fully nonlinear hydrodynamic numerical simulations, we show that the interactions between two crossing unidirectional wave beams can trigger fully localized and robust development of RWs.","These coherent structures, characterized by a typical spectral broadening then spreading in the form of dual bimodality and recurrent wave group focusing, not only defy the weakening expectation of quasi-four wave resonant interaction in directionally spread wave fields, but also differ from classical focusing mechanisms already mentioned.","This has been determined following a rigorous lifespan-based statistical analysis of extreme wave events in our fully nonlinear simulations.","Utilizing the coupled nonlinear Schr\\\"odinger framework, we also show that such intrinsic focusing dynamics can also be captured by weakly nonlinear wave evolution equations.","This opens new research avenues for further explorations of these complex and intriguing wave phenomena in hydrodynamics as well as other nonlinear and dispersive multi-wave systems."],"url":"http://arxiv.org/abs/2405.12000v1","category":"physics.flu-dyn"}
{"created":"2024-05-20 13:09:52","title":"Learning to connect in action: Measuring and understanding the emergence of boundary spanners in volatile times","abstract":"Collective intelligence of diverse groups is key for tackling many of today's grand challenges such as fostering resilience and climate adaptation. Information exchange across such diverse groups is crucial for collective intelligence, especially in volatile environments. To facilitate inter-group information exchange, Informational Boundary Spanners (IBSs) as pivotal information exchange 'hubs' are promising. However, the mechanisms that drive the emergence of IBSs remain poorly understood. To address this gap there is first a need for a method to identify and measure the emergence of IBSs. Second, an Agent-Based Modelling (ABM) framework is not available to systematically study mechanisms for the emergence of IBSs in volatile environments. Third, even though the ability to learn who provides high-quality information is thought to be essential to explain the emergence of IBSs, a rigorous test of this mechanism is missing. The learning mechanism is formalized using an ABM framework, with the model's outputs analyzed using the proposed IBS emergence measurement method. To illustrate both the method and the learning mechanism, we present a case study focused on information sharing in the volatile environment of a disaster. The study shows that learning constitutes a mechanism for the emergence of effective IBSs in (a) low-volatility environments characterised by low uncertainty and (b) in high-volatility environments characterised by rapid change if the number of inter-group connections is sufficient. With the method and model, this paper aims to lay the foundations for exploring mechanisms for the emergence of IBSs that facilitate inter-group information exchange. This article advances collective intelligence by providing the essential elements for measuring and understanding the emergence of IBSs and exploring the effect of learning on their emergence in volatile environments.","sentences":["Collective intelligence of diverse groups is key for tackling many of today's grand challenges such as fostering resilience and climate adaptation.","Information exchange across such diverse groups is crucial for collective intelligence, especially in volatile environments.","To facilitate inter-group information exchange, Informational Boundary Spanners (IBSs) as pivotal information exchange 'hubs' are promising.","However, the mechanisms that drive the emergence of IBSs remain poorly understood.","To address this gap there is first a need for a method to identify and measure the emergence of IBSs.","Second, an Agent-Based Modelling (ABM) framework is not available to systematically study mechanisms for the emergence of IBSs in volatile environments.","Third, even though the ability to learn who provides high-quality information is thought to be essential to explain the emergence of IBSs, a rigorous test of this mechanism is missing.","The learning mechanism is formalized using an ABM framework, with the model's outputs analyzed using the proposed IBS emergence measurement method.","To illustrate both the method and the learning mechanism, we present a case study focused on information sharing in the volatile environment of a disaster.","The study shows that learning constitutes a mechanism for the emergence of effective IBSs in (a) low-volatility environments characterised by low uncertainty and (b) in high-volatility environments characterised by rapid change if the number of inter-group connections is sufficient.","With the method and model, this paper aims to lay the foundations for exploring mechanisms for the emergence of IBSs that facilitate inter-group information exchange.","This article advances collective intelligence by providing the essential elements for measuring and understanding the emergence of IBSs and exploring the effect of learning on their emergence in volatile environments."],"url":"http://arxiv.org/abs/2405.11998v1","category":"cs.MA"}
{"created":"2024-05-20 12:56:29","title":"Covering Spaces of Symplectic Toric Orbifolds","abstract":"In this article we study covering spaces of symplectic toric orbifolds and symplectic toric orbifold bundles. In particular, we show that all symplectic toric orbifold coverings are quotients of some symplectic toric orbifold by a finite subgroup of a torus. We then give a general description of the labeled polytope of a toric orbifold bundle in terms of the polytopes of the fiber and the base. Finally, we apply our findings to study the number of toric structures on products of labeled projective spaces.","sentences":["In this article we study covering spaces of symplectic toric orbifolds and symplectic toric orbifold bundles.","In particular, we show that all symplectic toric orbifold coverings are quotients of some symplectic toric orbifold by a finite subgroup of a torus.","We then give a general description of the labeled polytope of a toric orbifold bundle in terms of the polytopes of the fiber and the base.","Finally, we apply our findings to study the number of toric structures on products of labeled projective spaces."],"url":"http://arxiv.org/abs/2405.11994v1","category":"math.SG"}
{"created":"2024-05-20 12:33:42","title":"A review on the use of large language models as virtual tutors","abstract":"Transformer architectures contribute to managing long-term dependencies for Natural Language Processing, representing one of the most recent changes in the field. These architectures are the basis of the innovative, cutting-edge Large Language Models (LLMs) that have produced a huge buzz in several fields and industrial sectors, among the ones education stands out. Accordingly, these generative Artificial Intelligence-based solutions have directed the change in techniques and the evolution in educational methods and contents, along with network infrastructure, towards high-quality learning. Given the popularity of LLMs, this review seeks to provide a comprehensive overview of those solutions designed specifically to generate and evaluate educational materials and which involve students and teachers in their design or experimental plan. To the best of our knowledge, this is the first review of educational applications (e.g., student assessment) of LLMs. As expected, the most common role of these systems is as virtual tutors for automatic question generation. Moreover, the most popular models are GTP-3 and BERT. However, due to the continuous launch of new generative models, new works are expected to be published shortly.","sentences":["Transformer architectures contribute to managing long-term dependencies for Natural Language Processing, representing one of the most recent changes in the field.","These architectures are the basis of the innovative, cutting-edge Large Language Models (LLMs) that have produced a huge buzz in several fields and industrial sectors, among the ones education stands out.","Accordingly, these generative Artificial Intelligence-based solutions have directed the change in techniques and the evolution in educational methods and contents, along with network infrastructure, towards high-quality learning.","Given the popularity of LLMs, this review seeks to provide a comprehensive overview of those solutions designed specifically to generate and evaluate educational materials and which involve students and teachers in their design or experimental plan.","To the best of our knowledge, this is the first review of educational applications (e.g., student assessment) of LLMs.","As expected, the most common role of these systems is as virtual tutors for automatic question generation.","Moreover, the most popular models are GTP-3 and BERT.","However, due to the continuous launch of new generative models, new works are expected to be published shortly."],"url":"http://arxiv.org/abs/2405.11983v1","category":"cs.CL"}
{"created":"2024-05-20 12:31:11","title":"Robust Deep Reinforcement Learning with Adaptive Adversarial Perturbations in Action Space","abstract":"Deep reinforcement learning (DRL) algorithms can suffer from modeling errors between the simulation and the real world. Many studies use adversarial learning to generate perturbation during training process to model the discrepancy and improve the robustness of DRL. However, most of these approaches use a fixed parameter to control the intensity of the adversarial perturbation, which can lead to a trade-off between average performance and robustness. In fact, finding the optimal parameter of the perturbation is challenging, as excessive perturbations may destabilize training and compromise agent performance, while insufficient perturbations may not impart enough information to enhance robustness. To keep the training stable while improving robustness, we propose a simple but effective method, namely, Adaptive Adversarial Perturbation (A2P), which can dynamically select appropriate adversarial perturbations for each sample. Specifically, we propose an adaptive adversarial coefficient framework to adjust the effect of the adversarial perturbation during training. By designing a metric for the current intensity of the perturbation, our method can calculate the suitable perturbation levels based on the current relative performance. The appealing feature of our method is that it is simple to deploy in real-world applications and does not require accessing the simulator in advance. The experiments in MuJoCo show that our method can improve the training stability and learn a robust policy when migrated to different test environments. The code is available at https://github.com/Lqm00/A2P-SAC.","sentences":["Deep reinforcement learning (DRL) algorithms can suffer from modeling errors between the simulation and the real world.","Many studies use adversarial learning to generate perturbation during training process to model the discrepancy and improve the robustness of DRL.","However, most of these approaches use a fixed parameter to control the intensity of the adversarial perturbation, which can lead to a trade-off between average performance and robustness.","In fact, finding the optimal parameter of the perturbation is challenging, as excessive perturbations may destabilize training and compromise agent performance, while insufficient perturbations may not impart enough information to enhance robustness.","To keep the training stable while improving robustness, we propose a simple but effective method, namely, Adaptive Adversarial Perturbation (A2P), which can dynamically select appropriate adversarial perturbations for each sample.","Specifically, we propose an adaptive adversarial coefficient framework to adjust the effect of the adversarial perturbation during training.","By designing a metric for the current intensity of the perturbation, our method can calculate the suitable perturbation levels based on the current relative performance.","The appealing feature of our method is that it is simple to deploy in real-world applications and does not require accessing the simulator in advance.","The experiments in MuJoCo show that our method can improve the training stability and learn a robust policy when migrated to different test environments.","The code is available at https://github.com/Lqm00/A2P-SAC."],"url":"http://arxiv.org/abs/2405.11982v1","category":"cs.LG"}
{"created":"2024-05-20 12:18:15","title":"SM-DTW: Stability Modulated Dynamic Time Warping for signature verification","abstract":"Building upon findings in computational model of handwriting learning and execution, we introduce the concept of stability to explain the difference between the actual movements performed during multiple execution of the subject's signature, and conjecture that the most stable parts of the signature should play a paramount role in evaluating the similarity between a questioned signature and the reference ones during signature verification. We then introduce the Stability Modulated Dynamic Time Warping algorithm for incorporating the stability regions, i.e. the most similar parts between two signatures, into the distance measure between a pair of signatures computed by the Dynamic Time Warping for signature verification. Experiments were conducted on two datasets largely adopted for performance evaluation. Experimental results show that the proposed algorithm improves the performance of the baseline system and compares favourably with other top performing signature verification systems.","sentences":["Building upon findings in computational model of handwriting learning and execution, we introduce the concept of stability to explain the difference between the actual movements performed during multiple execution of the subject's signature, and conjecture that the most stable parts of the signature should play a paramount role in evaluating the similarity between a questioned signature and the reference ones during signature verification.","We then introduce the Stability Modulated Dynamic Time Warping algorithm for incorporating the stability regions, i.e. the most similar parts between two signatures, into the distance measure between a pair of signatures computed by the Dynamic Time Warping for signature verification.","Experiments were conducted on two datasets largely adopted for performance evaluation.","Experimental results show that the proposed algorithm improves the performance of the baseline system and compares favourably with other top performing signature verification systems."],"url":"http://arxiv.org/abs/2405.11978v1","category":"cs.CV"}
{"created":"2024-05-20 12:13:22","title":"GuidedRec: Guiding Ill-Posed Unsupervised Volumetric Recovery","abstract":"We introduce a novel unsupervised approach to reconstructing a 3D volume from only two planar projections that exploits a previous\\-ly-captured 3D volume of the patient. Such volume is readily available in many important medical procedures and previous methods already used such a volume. Earlier methods that work by deforming this volume to match the projections typically fail when the number of projections is very low as the alignment becomes underconstrained. We show how to use a generative model of the volume structures to constrain the deformation and obtain a correct estimate. Moreover, our method is not bounded to a specific sensor calibration and can be applied to new calibrations without retraining. We evaluate our approach on a challenging dataset and show it outperforms state-of-the-art methods. As a result, our method could be used in treatment scenarios such as surgery and radiotherapy while drastically reducing patient radiation exposure.","sentences":["We introduce a novel unsupervised approach to reconstructing a 3D volume from only two planar projections that exploits a previous\\-ly-captured 3D volume of the patient.","Such volume is readily available in many important medical procedures and previous methods already used such a volume.","Earlier methods that work by deforming this volume to match the projections typically fail when the number of projections is very low as the alignment becomes underconstrained.","We show how to use a generative model of the volume structures to constrain the deformation and obtain a correct estimate.","Moreover, our method is not bounded to a specific sensor calibration and can be applied to new calibrations without retraining.","We evaluate our approach on a challenging dataset and show it outperforms state-of-the-art methods.","As a result, our method could be used in treatment scenarios such as surgery and radiotherapy while drastically reducing patient radiation exposure."],"url":"http://arxiv.org/abs/2405.11977v1","category":"cs.CV"}
{"created":"2024-05-20 12:10:26","title":"A Stochastic Sampling Approach to Privacy","abstract":"This paper proposes an optimal stochastic sampling approach to privacy, in which a sensor observes a process which is correlated to private information. In out set-up, a sampler decides to keep or discard the sensor's observations. The kept samples are shared with an adversary who might attempt to infer the private process based on the sampler's output. The privacy leakages are captured with the mutual information between the private process and sampler's output. We cast the optimal sampling design as an optimization problem with two objectives: (i) minimizing the reconstruction error of the observed process using the sampler's output, (ii) reducing the privacy leakages. We first show the optimal reconstruction policy is deterministic and can be obtained by solving a one-step optimization problem at each time step. We also derive the optimality equations of the privacy-sampler for a general class of processes via the dynamic decomposition method, and show the sampler controls the adversary's belief about the private input. Also, we propose a simplified design for linear Gaussian processes by restricting the sampling policy to a special collection. We show that the optimal reconstruction of the system state and the private process is similar to Kalman filter in the linear Gaussian case, and the objective of the sampler design problem can be analytically expressed based on a conditional mean and covariance matrix. Furthermore, we develop an numerical algorithm to optimize the sampling and reconstruction policies, wherein the policy gradient theorem for the optimal sampling design is derived based on the implicit function theorem. Finally, we verify our design and show it capabilities in state reconstruction, privacy protection and data size reduction via simulations.","sentences":["This paper proposes an optimal stochastic sampling approach to privacy, in which a sensor observes a process which is correlated to private information.","In out set-up, a sampler decides to keep or discard the sensor's observations.","The kept samples are shared with an adversary who might attempt to infer the private process based on the sampler's output.","The privacy leakages are captured with the mutual information between the private process and sampler's output.","We cast the optimal sampling design as an optimization problem with two objectives: (i) minimizing the reconstruction error of the observed process using the sampler's output, (ii) reducing the privacy leakages.","We first show the optimal reconstruction policy is deterministic and can be obtained by solving a one-step optimization problem at each time step.","We also derive the optimality equations of the privacy-sampler for a general class of processes via the dynamic decomposition method, and show the sampler controls the adversary's belief about the private input.","Also, we propose a simplified design for linear Gaussian processes by restricting the sampling policy to a special collection.","We show that the optimal reconstruction of the system state and the private process is similar to Kalman filter in the linear Gaussian case, and the objective of the sampler design problem can be analytically expressed based on a conditional mean and covariance matrix.","Furthermore, we develop an numerical algorithm to optimize the sampling and reconstruction policies, wherein the policy gradient theorem for the optimal sampling design is derived based on the implicit function theorem.","Finally, we verify our design and show it capabilities in state reconstruction, privacy protection and data size reduction via simulations."],"url":"http://arxiv.org/abs/2405.11975v1","category":"eess.SY"}
{"created":"2024-05-20 12:01:56","title":"Addendum to \"Quantum Search with Noisy Oracle\"","abstract":"In this note, I generalize the techniques of my recent work (arXiv:2309.14944) and show that, even if just a single known qubit of query registers is affected by the depolarizing noise of rate p, quantum search among n elements cannot be done any faster than in O(np) queries. This holds both when the affected qubit is one of the log(n) index qubits and when it is the target qubit.","sentences":["In this note, I generalize the techniques of my recent work (arXiv:2309.14944) and show that, even if just a single known qubit of query registers is affected by the depolarizing noise of rate p, quantum search among n elements cannot be done any faster than in O(np) queries.","This holds both when the affected qubit is one of the log(n) index qubits and when it is the target qubit."],"url":"http://arxiv.org/abs/2405.11973v1","category":"quant-ph"}
{"created":"2024-05-20 12:00:57","title":"Confinement for 3d $\\mathcal{N}=2$ $SU(N)$ with a Symmetric tensor","abstract":"In this paper we study 3d $\\mathcal{N}=2$ $SU(N)$ confining gauge theories with a matter field in the rank-two index symmetric representation. The models found here are obtained from the application of the duplication formula for hyperbolic gamma functions from \\emph{parent} confining models, with antisymmetric fields and (anti)-fundamental matter by \\emph{freezing} some of the mass parameters for the latter. We find a series of identities that can give rise to candidate confining theories with $SU(N)$ gauge group, a symmetric tensor and in addition other charged matter fields, in general with a non-vanishing superpotential. We provide for each case further checks of the proposed dualities, by studying the Coulomb branch and by deconfining the tensorial matter by using other known 3d dualities. From the final picture a refined classification emerges, distinguishing the confining theories obtained in the paper in classes with common properties.","sentences":["In this paper we study 3d $\\mathcal{N}=2$ $SU(N)$ confining gauge theories with a matter field in the rank-two index symmetric representation.","The models found here are obtained from the application of the duplication formula for hyperbolic gamma functions from \\emph{parent} confining models, with antisymmetric fields and (anti)-fundamental matter by \\emph{freezing} some of the mass parameters for the latter.","We find a series of identities that can give rise to candidate confining theories with $SU(N)$ gauge group, a symmetric tensor and in addition other charged matter fields, in general with a non-vanishing superpotential.","We provide for each case further checks of the proposed dualities, by studying the Coulomb branch and by deconfining the tensorial matter by using other known 3d dualities.","From the final picture a refined classification emerges, distinguishing the confining theories obtained in the paper in classes with common properties."],"url":"http://arxiv.org/abs/2405.11972v1","category":"hep-th"}
{"created":"2024-05-20 11:56:07","title":"Generalized $\u03b2$ and $(q,t)$-deformed partition functions with $W$-representations and Nekrasov partition functions","abstract":"We construct the generalized $\\beta$ and $(q,t)$-deformed partition functions through $W$ representations, where the expansions are respectively with respect to the generalized Jack and Macdonald polynomials labeled by $N$-tuple of Young diagrams. We find that there are the profound interrelations between our deformed partition functions and the $4d$ and $5d$ Nekrasov partition functions. Since the corresponding Nekrasov partition functions can be given by vertex operators, the remarkable connection between our $\\beta$ and $(q,t)$-deformed $W$-operators and vertex operators is revealed in this paper. In addition, we investigate the higher Hamiltonians for the generalized Jack and Macdonald polynomials.","sentences":["We construct the generalized $\\beta$ and $(q,t)$-deformed partition functions through $W$ representations, where the expansions are respectively with respect to the generalized Jack and Macdonald polynomials labeled by $N$-tuple of Young diagrams.","We find that there are the profound interrelations between our deformed partition functions and the $4d$ and $5d$ Nekrasov partition functions.","Since the corresponding Nekrasov partition functions can be given by vertex operators, the remarkable connection between our $\\beta$ and $(q,t)$-deformed $W$-operators and vertex operators is revealed in this paper.","In addition, we investigate the higher Hamiltonians for the generalized Jack and Macdonald polynomials."],"url":"http://arxiv.org/abs/2405.11970v1","category":"hep-th"}
{"created":"2024-05-20 11:47:31","title":"Conditional Shift-Robust Conformal Prediction for Graph Neural Network","abstract":"Graph Neural Networks (GNNs) have emerged as potent tools for predicting outcomes in graph-structured data. Despite their efficacy, a significant drawback of GNNs lies in their limited ability to provide robust uncertainty estimates, posing challenges to their reliability in contexts where errors carry significant consequences. Moreover, GNNs typically excel in in-distribution settings, assuming that training and test data follow identical distributions: a condition often unmet in real-world graph data scenarios. In this article, we leverage conformal prediction, a widely recognized statistical technique for quantifying uncertainty by transforming predictive model outputs into prediction sets, to address uncertainty quantification in GNN predictions amidst conditional shift \\footnote{Representing the change in conditional probability distribution $P(label |input)$ from source domain to target domain.} in graph-based semi-supervised learning (SSL). Additionally, we propose a novel loss function aimed at refining model predictions by minimizing conditional shift in latent stages. Termed Conditional Shift Robust (CondSR) conformal prediction for GNNs, our approach CondSR is model-agnostic and adaptable to various classification models. We validate the effectiveness of our method on standard graph benchmark datasets, integrating it with state-of-the-art GNNs in node classification tasks. The code implementation is publicly available for further exploration and experimentation.","sentences":["Graph Neural Networks (GNNs) have emerged as potent tools for predicting outcomes in graph-structured data.","Despite their efficacy, a significant drawback of GNNs lies in their limited ability to provide robust uncertainty estimates, posing challenges to their reliability in contexts where errors carry significant consequences.","Moreover, GNNs typically excel in in-distribution settings, assuming that training and test data follow identical distributions: a condition often unmet in real-world graph data scenarios.","In this article, we leverage conformal prediction, a widely recognized statistical technique for quantifying uncertainty by transforming predictive model outputs into prediction sets, to address uncertainty quantification in GNN predictions amidst conditional shift \\footnote{Representing the change in conditional probability distribution $P(label |input)$ from source domain to target domain.}","in graph-based semi-supervised learning (SSL).","Additionally, we propose a novel loss function aimed at refining model predictions by minimizing conditional shift in latent stages.","Termed Conditional Shift Robust (CondSR) conformal prediction for GNNs, our approach CondSR is model-agnostic and adaptable to various classification models.","We validate the effectiveness of our method on standard graph benchmark datasets, integrating it with state-of-the-art GNNs in node classification tasks.","The code implementation is publicly available for further exploration and experimentation."],"url":"http://arxiv.org/abs/2405.11968v1","category":"cs.LG"}
{"created":"2024-05-20 11:47:19","title":"Recommender Algorithm for Supporting Self-Management of CVD Risk Factors in an Adult Population at Home","abstract":"One of the new trends in the development of recommendation algorithms is the dissemination of their capabilities to support the population in managing their health. This article focuses on the problem of improving the effectiveness of cardiovascular diseases (CVD) prevention, since CVD is the leading cause of death worldwide. To address this issue, a knowledge-based recommendation algorithm was proposed to support self-management of CVD risk factors in adults at home. The proposed algorithm is based on the original multidimensional recommendation model and on a new user profile model, which includes predictive assessments of CVD health in addition to its current ones as outlined in official guidelines. The main feature of the proposed algorithm is the combination of rule-based logic with the capabilities of a large language model in generating human-like text for explanatory component of multidimensional recommendation. The verification and evaluation of the proposed algorithm showed the usefulness of the proposed recommendation algorithm for supporting adults in self-management of their CVD risk factors at home. As follows from the comparison with similar knowledge-based recommendation algorithms, the proposed algorithm evaluates a larger number of CVD risk factors and has a greater information and semantic capacity of the generated recommendations.","sentences":["One of the new trends in the development of recommendation algorithms is the dissemination of their capabilities to support the population in managing their health.","This article focuses on the problem of improving the effectiveness of cardiovascular diseases (CVD) prevention, since CVD is the leading cause of death worldwide.","To address this issue, a knowledge-based recommendation algorithm was proposed to support self-management of CVD risk factors in adults at home.","The proposed algorithm is based on the original multidimensional recommendation model and on a new user profile model, which includes predictive assessments of CVD health in addition to its current ones as outlined in official guidelines.","The main feature of the proposed algorithm is the combination of rule-based logic with the capabilities of a large language model in generating human-like text for explanatory component of multidimensional recommendation.","The verification and evaluation of the proposed algorithm showed the usefulness of the proposed recommendation algorithm for supporting adults in self-management of their CVD risk factors at home.","As follows from the comparison with similar knowledge-based recommendation algorithms, the proposed algorithm evaluates a larger number of CVD risk factors and has a greater information and semantic capacity of the generated recommendations."],"url":"http://arxiv.org/abs/2405.11967v1","category":"cs.IR"}
{"created":"2024-05-20 11:40:01","title":"No Free Lunch: Research Software Testing in Teaching","abstract":"Software is at the core of most scientific discoveries today. Therefore, the quality of research results highly depends on the quality of the research software. Rigorous testing, as we know it from software engineering in the industry, could ensure the quality of the research software but it also requires a substantial effort that is often not rewarded in academia. Therefore, this research explores the effects of research software testing integrated into teaching on research software. In an in-vivo experiment, we integrated the engineering of a test suite for a large-scale network simulation as group projects into a course on software testing at the Blekinge Institute of Technology, Sweden, and qualitatively measured the effects of this integration on the research software. We found that the research software benefited from the integration through substantially improved documentation and fewer hardware and software dependencies. However, this integration was effortful and although the student teams developed elegant and thoughtful test suites, no code by students went directly into the research software since we were not able to make the integration back into the research software obligatory or even remunerative. Although we strongly believe that integrating research software engineering such as testing into teaching is not only valuable for the research software itself but also for students, the research of the next generation, as they get in touch with research software engineering and bleeding-edge research in their field as part of their education, the uncertainty about the intellectual properties of students' code substantially limits the potential of integrating research software testing into teaching.","sentences":["Software is at the core of most scientific discoveries today.","Therefore, the quality of research results highly depends on the quality of the research software.","Rigorous testing, as we know it from software engineering in the industry, could ensure the quality of the research software but it also requires a substantial effort that is often not rewarded in academia.","Therefore, this research explores the effects of research software testing integrated into teaching on research software.","In an in-vivo experiment, we integrated the engineering of a test suite for a large-scale network simulation as group projects into a course on software testing at the Blekinge Institute of Technology, Sweden, and qualitatively measured the effects of this integration on the research software.","We found that the research software benefited from the integration through substantially improved documentation and fewer hardware and software dependencies.","However, this integration was effortful and although the student teams developed elegant and thoughtful test suites, no code by students went directly into the research software since we were not able to make the integration back into the research software obligatory or even remunerative.","Although we strongly believe that integrating research software engineering such as testing into teaching is not only valuable for the research software itself but also for students, the research of the next generation, as they get in touch with research software engineering and bleeding-edge research in their field as part of their education, the uncertainty about the intellectual properties of students' code substantially limits the potential of integrating research software testing into teaching."],"url":"http://arxiv.org/abs/2405.11965v1","category":"cs.SE"}
{"created":"2024-05-20 11:36:47","title":"Gait controllability of length-changing slender microswimmers","abstract":"Controllability results of four models of two-link microscale swimmers that are able to change the length of their links are obtained. The problems are formulated in the framework of Geometric Control Theory, within which the notions of fiber, total, and gait controllability are presented, together with sufficient conditions for the latter two. The dynamics of a general two-link swimmer is described by resorting to Resistive Force Theory and different mechanisms to produce a length-change in the links, namely, active deformation, a sliding hinge, growth at the tip, and telescopic links. Total controllability is proved via gait controllability in all four cases, and illustrated with the aid of numerical simulations.","sentences":["Controllability results of four models of two-link microscale swimmers that are able to change the length of their links are obtained.","The problems are formulated in the framework of Geometric Control Theory, within which the notions of fiber, total, and gait controllability are presented, together with sufficient conditions for the latter two.","The dynamics of a general two-link swimmer is described by resorting to Resistive Force Theory and different mechanisms to produce a length-change in the links, namely, active deformation, a sliding hinge, growth at the tip, and telescopic links.","Total controllability is proved via gait controllability in all four cases, and illustrated with the aid of numerical simulations."],"url":"http://arxiv.org/abs/2405.11961v1","category":"math.OC"}
{"created":"2024-05-20 11:17:33","title":"Note on post-Minkowskian expansion and Bondi coordinates","abstract":"In this note, we transform the linear order (first order in $G$) metric from a system of pointlike bodies source to the Bondi coordinates. We confirm that the Bondi 4-momentum and angular momentum of the system computed at null infinity in Bondi coordinates coincide with the relativistic definitions of 4-momentum and angular momentum for the system of pointlike bodies.","sentences":["In this note, we transform the linear order (first order in $G$) metric from a system of pointlike bodies source to the Bondi coordinates.","We confirm that the Bondi 4-momentum and angular momentum of the system computed at null infinity in Bondi coordinates coincide with the relativistic definitions of 4-momentum and angular momentum for the system of pointlike bodies."],"url":"http://arxiv.org/abs/2405.11953v1","category":"gr-qc"}
{"created":"2024-05-20 10:54:47","title":"WisPerMed at BioLaySumm: Adapting Autoregressive Large Language Models for Lay Summarization of Scientific Articles","abstract":"This paper details the efforts of the WisPerMed team in the BioLaySumm2024 Shared Task on automatic lay summarization in the biomedical domain, aimed at making scientific publications accessible to non-specialists. Large language models (LLMs), specifically the BioMistral and Llama3 models, were fine-tuned and employed to create lay summaries from complex scientific texts. The summarization performance was enhanced through various approaches, including instruction tuning, few-shot learning, and prompt variations tailored to incorporate specific context information. The experiments demonstrated that fine-tuning generally led to the best performance across most evaluated metrics. Few-shot learning notably improved the models' ability to generate relevant and factually accurate texts, particularly when using a well-crafted prompt. Additionally, a Dynamic Expert Selection (DES) mechanism to optimize the selection of text outputs based on readability and factuality metrics was developed. Out of 54 participants, the WisPerMed team reached the 4th place, measured by readability, factuality, and relevance. Determined by the overall score, our approach improved upon the baseline by approx. 5.5 percentage points and was only approx 1.5 percentage points behind the first place.","sentences":["This paper details the efforts of the WisPerMed team in the BioLaySumm2024 Shared Task on automatic lay summarization in the biomedical domain, aimed at making scientific publications accessible to non-specialists.","Large language models (LLMs), specifically the BioMistral and Llama3 models, were fine-tuned and employed to create lay summaries from complex scientific texts.","The summarization performance was enhanced through various approaches, including instruction tuning, few-shot learning, and prompt variations tailored to incorporate specific context information.","The experiments demonstrated that fine-tuning generally led to the best performance across most evaluated metrics.","Few-shot learning notably improved the models' ability to generate relevant and factually accurate texts, particularly when using a well-crafted prompt.","Additionally, a Dynamic Expert Selection (DES) mechanism to optimize the selection of text outputs based on readability and factuality metrics was developed.","Out of 54 participants, the WisPerMed team reached the 4th place, measured by readability, factuality, and relevance.","Determined by the overall score, our approach improved upon the baseline by approx.","5.5 percentage points and was only approx 1.5 percentage points behind the first place."],"url":"http://arxiv.org/abs/2405.11950v1","category":"cs.CL"}
{"created":"2024-05-20 10:53:27","title":"The extremal values of the ratio of differences of power mean, arithmetic mean, and geometric mean","abstract":"In the paper the maximum and the minimum of the ratio of the difference of the arithmetic mean and the geometric mean, and the difference of the power mean and the geometric mean of $n$ variables, is studied. A new optimization argument was used which reduces $n$ variable optimization problem to a single variable. All possible cases of the choice of the power mean and the choice of the number of variables of the means is studied. The obtained results generalize and complete the earlier results which were either for specific intervals of power means or for small number of variables of the means. Some of the results are formulated as the best constant inequalities involving interpolation of the arithmetic mean and the geometric mean.","sentences":["In the paper the maximum and the minimum of the ratio of the difference of the arithmetic mean and the geometric mean, and the difference of the power mean and the geometric mean of $n$ variables, is studied.","A new optimization argument was used which reduces $n$ variable optimization problem to a single variable.","All possible cases of the choice of the power mean and the choice of the number of variables of the means is studied.","The obtained results generalize and complete the earlier results which were either for specific intervals of power means or for small number of variables of the means.","Some of the results are formulated as the best constant inequalities involving interpolation of the arithmetic mean and the geometric mean."],"url":"http://arxiv.org/abs/2405.11947v1","category":"math.CA"}
{"created":"2024-05-20 10:53:27","title":"Self-induced spin pumping and inverse spin Hall effect in single FePt thin films","abstract":"In this study, we investigate the spin-charge current conversion characteristics of chemically disordered ferromagnetic single FePt thin films by spin-pumping ferromagnetic resonance experiments performed on both a resonance cavity and on patterned devices. We clearly observe a self-induced signal in a single FePt layer. The sign of a single FePt spin pumping voltage signal is consistent with a typical bilayer with a positive spin Hall angle layer such as that of Pt on top of a ferromagnet (FM), substrate//FM/Pt. Structural analysis shows a strong composition gradient due to natural oxidation at both FePt interfaces, with the Si substrate and with the air. The FePt-thickness dependence of the self-induced charge current produced allowed us to obtain $\\lambda _ \\text{FePt}=(1.5\\pm 0.1)$ nm and self-induced $\\theta_ \\text{self-FePt}=0.047 \\pm 0.003$, with efficiency for reciprocal effects applications $\\theta _ \\text{self-FePt} \\times \\lambda _ \\text{FePt} = 0.071$ nm which is comparable to that of Pt, $\\theta _ \\text{SH-Pt} \\times \\lambda _ \\text{Pt} = 0.2$ nm. Moreover, by studying bilayer systems such as Si//FePt/Pt and Si//Pt//FePt we independently could extract the individual contributions of the external inverse spin Hall effect of Pt and the self-induced inverse spin Hall effect of FePt. Notably, this method gives consistent values of charge currents produced due to only self-induced inverse spin Hall effect in FePt layers. These results advance our understanding of spin-to-charge interconversion mechanisms in composite thin films and pave the way for the development of next-generation spintronics devices based on self-torque.","sentences":["In this study, we investigate the spin-charge current conversion characteristics of chemically disordered ferromagnetic single FePt thin films by spin-pumping ferromagnetic resonance experiments performed on both a resonance cavity and on patterned devices.","We clearly observe a self-induced signal in a single FePt layer.","The sign of a single FePt spin pumping voltage signal is consistent with a typical bilayer with a positive spin Hall angle layer such as that of Pt on top of a ferromagnet (FM), substrate//FM/Pt.","Structural analysis shows a strong composition gradient due to natural oxidation at both FePt interfaces, with the Si substrate and with the air.","The FePt-thickness dependence of the self-induced charge current produced allowed us to obtain $\\lambda _","\\text{FePt}=(1.5\\pm 0.1)$ nm and self-induced $\\theta_ \\text{self-FePt}=0.047 \\pm 0.003$, with efficiency for reciprocal effects applications $\\theta _ \\text{self-FePt} \\times \\lambda _ \\text{FePt} = 0.071$ nm which is comparable to that of Pt, $\\theta _","\\text{SH-Pt} \\times \\lambda _ \\text{Pt} = 0.2$ nm.","Moreover, by studying bilayer systems such as Si//FePt/","Pt and Si//Pt//FePt we independently could extract the individual contributions of the external inverse spin Hall effect of Pt and the self-induced inverse spin Hall effect of FePt.","Notably, this method gives consistent values of charge currents produced due to only self-induced inverse spin Hall effect in FePt layers.","These results advance our understanding of spin-to-charge interconversion mechanisms in composite thin films and pave the way for the development of next-generation spintronics devices based on self-torque."],"url":"http://arxiv.org/abs/2405.11948v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 10:44:54","title":"The global structure of astrospheres: effect of Knudsen number","abstract":"The interaction between stellar winds and the partially ionized local interstellar medium (LISM) is quite common in astrophysics. However, the main difficulty in describing the neutral components lies in the fact that the mean free path of an interstellar atom, l, can be comparable to the characteristic size of an astrosphere, L (i.e., the Knudsen number, which is equal to l/L, is approximately equal to 1, as in the case of the heliosphere). In such cases, a single-fluid approximation becomes invalid, and a kinetic description must be used for the neutral component. In this study, we consider a general astrosphere and use a kinetic-gas dynamics model to investigate how the global structure of the astrosphere depends on the Knudsen number. We present numerical results covering an extremely wide range of Knudsen numbers (from 0.0001 to 100). Additionally, we explore the applicability of single-fluid approaches for modeling astrospheres of various sizes. We have excluded the influence of interstellar and stellar magnetic fields in our model to make parametric study of the kinetic effects feasible. The main conclusion of this work is that, for large astrospheres (with a distance to the bow shock greater than 600 AU) a heated rarefied plasma layer forms in the outer shock layer near the astropause. The formation of this layer is linked to localized heating of the plasma by atoms (specifically, ENAs) that undergo charge exchange again behind the astropause. This process significantly alters the flow structure in the outer shock layer and the location of the bow shock, and it cannot be described by a single-fluid model. Additionally, this paper discusses how atoms weaken the bow shocks at near-heliospheric conditions.","sentences":["The interaction between stellar winds and the partially ionized local interstellar medium (LISM) is quite common in astrophysics.","However, the main difficulty in describing the neutral components lies in the fact that the mean free path of an interstellar atom, l, can be comparable to the characteristic size of an astrosphere, L (i.e., the Knudsen number, which is equal to l/L, is approximately equal to 1, as in the case of the heliosphere).","In such cases, a single-fluid approximation becomes invalid, and a kinetic description must be used for the neutral component.","In this study, we consider a general astrosphere and use a kinetic-gas dynamics model to investigate how the global structure of the astrosphere depends on the Knudsen number.","We present numerical results covering an extremely wide range of Knudsen numbers (from 0.0001 to 100).","Additionally, we explore the applicability of single-fluid approaches for modeling astrospheres of various sizes.","We have excluded the influence of interstellar and stellar magnetic fields in our model to make parametric study of the kinetic effects feasible.","The main conclusion of this work is that, for large astrospheres (with a distance to the bow shock greater than 600 AU) a heated rarefied plasma layer forms in the outer shock layer near the astropause.","The formation of this layer is linked to localized heating of the plasma by atoms (specifically, ENAs) that undergo charge exchange again behind the astropause.","This process significantly alters the flow structure in the outer shock layer and the location of the bow shock, and it cannot be described by a single-fluid model.","Additionally, this paper discusses how atoms weaken the bow shocks at near-heliospheric conditions."],"url":"http://arxiv.org/abs/2405.11945v1","category":"astro-ph.SR"}
{"created":"2024-05-20 10:35:30","title":"FAME-MT Dataset: Formality Awareness Made Easy for Machine Translation Purposes","abstract":"People use language for various purposes. Apart from sharing information, individuals may use it to express emotions or to show respect for another person. In this paper, we focus on the formality level of machine-generated translations and present FAME-MT -- a dataset consisting of 11.2 million translations between 15 European source languages and 8 European target languages classified to formal and informal classes according to target sentence formality. This dataset can be used to fine-tune machine translation models to ensure a given formality level for each European target language considered. We describe the dataset creation procedure, the analysis of the dataset's quality showing that FAME-MT is a reliable source of language register information, and we present a publicly available proof-of-concept machine translation model that uses the dataset to steer the formality level of the translation. Currently, it is the largest dataset of formality annotations, with examples expressed in 112 European language pairs. The dataset is published online: https://github.com/laniqo-public/fame-mt/ .","sentences":["People use language for various purposes.","Apart from sharing information, individuals may use it to express emotions or to show respect for another person.","In this paper, we focus on the formality level of machine-generated translations and present FAME-MT -- a dataset consisting of 11.2 million translations between 15 European source languages and 8 European target languages classified to formal and informal classes according to target sentence formality.","This dataset can be used to fine-tune machine translation models to ensure a given formality level for each European target language considered.","We describe the dataset creation procedure, the analysis of the dataset's quality showing that FAME-MT is a reliable source of language register information, and we present a publicly available proof-of-concept machine translation model that uses the dataset to steer the formality level of the translation.","Currently, it is the largest dataset of formality annotations, with examples expressed in 112 European language pairs.","The dataset is published online: https://github.com/laniqo-public/fame-mt/ ."],"url":"http://arxiv.org/abs/2405.11942v1","category":"cs.CL"}
{"created":"2024-05-20 10:30:36","title":"Biomedical Entity Linking for Dutch: Fine-tuning a Self-alignment BERT Model on an Automatically Generated Wikipedia Corpus","abstract":"Biomedical entity linking, a main component in automatic information extraction from health-related texts, plays a pivotal role in connecting textual entities (such as diseases, drugs and body parts mentioned by patients) to their corresponding concepts in a structured biomedical knowledge base. The task remains challenging despite recent developments in natural language processing. This paper presents the first evaluated biomedical entity linking model for the Dutch language. We use MedRoBERTa.nl as base model and perform second-phase pretraining through self-alignment on a Dutch biomedical ontology extracted from the UMLS and Dutch SNOMED. We derive a corpus from Wikipedia of ontology-linked Dutch biomedical entities in context and fine-tune our model on this dataset. We evaluate our model on the Dutch portion of the Mantra GSC-corpus and achieve 54.7% classification accuracy and 69.8% 1-distance accuracy. We then perform a case study on a collection of unlabeled, patient-support forum data and show that our model is hampered by the limited quality of the preceding entity recognition step. Manual evaluation of small sample indicates that of the correctly extracted entities, around 65% is linked to the correct concept in the ontology. Our results indicate that biomedical entity linking in a language other than English remains challenging, but our Dutch model can be used to for high-level analysis of patient-generated text.","sentences":["Biomedical entity linking, a main component in automatic information extraction from health-related texts, plays a pivotal role in connecting textual entities (such as diseases, drugs and body parts mentioned by patients) to their corresponding concepts in a structured biomedical knowledge base.","The task remains challenging despite recent developments in natural language processing.","This paper presents the first evaluated biomedical entity linking model for the Dutch language.","We use MedRoBERTa.nl as base model and perform second-phase pretraining through self-alignment on a Dutch biomedical ontology extracted from the UMLS and Dutch SNOMED.","We derive a corpus from Wikipedia of ontology-linked Dutch biomedical entities in context and fine-tune our model on this dataset.","We evaluate our model on the Dutch portion of the Mantra GSC-corpus and achieve 54.7% classification accuracy and 69.8% 1-distance accuracy.","We then perform a case study on a collection of unlabeled, patient-support forum data and show that our model is hampered by the limited quality of the preceding entity recognition step.","Manual evaluation of small sample indicates that of the correctly extracted entities, around 65% is linked to the correct concept in the ontology.","Our results indicate that biomedical entity linking in a language other than English remains challenging, but our Dutch model can be used to for high-level analysis of patient-generated text."],"url":"http://arxiv.org/abs/2405.11941v1","category":"cs.CL"}
{"created":"2024-05-20 10:28:41","title":"Optimal balanced-norm error estimate of the LDG method for reaction-diffusion problems II: the two-dimensional case with layer-upwind flux","abstract":"A singularly perturbed reaction-diffusion problem posed on the unit square in $\\mathbb{R}^2$ is solved numerically by a local discontinuous Galerkin (LDG) finite element method. Typical solutions of this class of problem exhibit boundary layers along the sides of the domain; these layers generally cause difficulties for numerical methods. Our LDG method handles the boundary layers by using a Shishkin mesh and also introducing the new concept of a ``layer-upwind flux\" -- a discrete flux whose values are chosen on the fine mesh (which lies inside the boundary layers) in the direction where the layer weakens. On the coarse mesh, one can use a standard central flux. No penalty terms are needed with these fluxes, unlike many other variants of the LDG method. Our choice of discrete flux makes it feasible to derive an optimal-order error analysis in a balanced norm; this norm is stronger than the usual energy norm and is a more appropriate measure for errors in computed solutions for singularly perturbed reaction-diffusion problems. It will be proved that the LDG method is usually convergent of order $O((N^{-1}\\ln N)^{k+1})$ in the balanced norm, where $N$ is the number of mesh intervals in each coordinate direction and tensor-product piecewise polynomials of degree~$k$ in each coordinate variable are used in the LDG method. This result is the first of its kind for the LDG method applied to this class of problem and is optimal for convergence on a Shishkin mesh. Its sharpness is confirmed by numerical experiments.","sentences":["A singularly perturbed reaction-diffusion problem posed on the unit square in $\\mathbb{R}^2$ is solved numerically by a local discontinuous Galerkin (LDG) finite element method.","Typical solutions of this class of problem exhibit boundary layers along the sides of the domain; these layers generally cause difficulties for numerical methods.","Our LDG method handles the boundary layers by using a Shishkin mesh and also introducing the new concept of a ``layer-upwind flux\" -- a discrete flux whose values are chosen on the fine mesh (which lies inside the boundary layers) in the direction where the layer weakens.","On the coarse mesh, one can use a standard central flux.","No penalty terms are needed with these fluxes, unlike many other variants of the LDG method.","Our choice of discrete flux makes it feasible to derive an optimal-order error analysis in a balanced norm; this norm is stronger than the usual energy norm and is a more appropriate measure for errors in computed solutions for singularly perturbed reaction-diffusion problems.","It will be proved that the LDG method is usually convergent of order $O((N^{-1}\\ln N)^{k+1})$ in the balanced norm, where $N$ is the number of mesh intervals in each coordinate direction and tensor-product piecewise polynomials of degree~$k$ in each coordinate variable are used in the LDG method.","This result is the first of its kind for the LDG method applied to this class of problem and is optimal for convergence on a Shishkin mesh.","Its sharpness is confirmed by numerical experiments."],"url":"http://arxiv.org/abs/2405.11939v1","category":"math.NA"}
{"created":"2024-05-20 10:25:03","title":"Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving Machine Translation","abstract":"This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in machine translation (MT), particularly for domain adaptation and low-resource languages. We implement the self-improvement process by fine-tuning the model on its MBR-decoded forward translations. By employing COMET as the MBR utility metric, we aim to achieve the reranking of translations that better aligns with human preferences. The paper explores the iterative application of this approach and the potential need for language-specific MBR utility metrics. The results demonstrate significant enhancements in translation quality for all examined language pairs, including successful application to domain-adapted models and generalisation to low-resource settings. This highlights the potential of COMET-guided MBR for efficient MT self-improvement in various scenarios.","sentences":["This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in machine translation (MT), particularly for domain adaptation and low-resource languages.","We implement the self-improvement process by fine-tuning the model on its MBR-decoded forward translations.","By employing COMET as the MBR utility metric, we aim to achieve the reranking of translations that better aligns with human preferences.","The paper explores the iterative application of this approach and the potential need for language-specific MBR utility metrics.","The results demonstrate significant enhancements in translation quality for all examined language pairs, including successful application to domain-adapted models and generalisation to low-resource settings.","This highlights the potential of COMET-guided MBR for efficient MT self-improvement in various scenarios."],"url":"http://arxiv.org/abs/2405.11937v1","category":"cs.CL"}
{"created":"2024-05-20 10:21:29","title":"Elucidating the role of electron transfer in the photoluminescence of $\\mathrm{MoS_{2}}$ quantum dots synthesized by fs-pulse ablation","abstract":"Herein, $\\mathrm{MoS_{2}}$ quantum dot (QDs) with controlled optical, structural, and electronic properties are synthesized using the femtosecond pulsed laser ablation in liquid (fs-PLAL) technique by varying pulse-width, ablation power, and ablation time to harness the potential for next-generation optoelectronics and quantum technology. Furthermore, this work elucidates key aspects of the mechanisms underlying the near-UV and blue emission, the accompanying large Stokes-shift, and the consequent change in sample color with laser exposure parameters pertaining to $\\mathrm{MoS_{2}}$ QDs. Through spectroscopic analysis, including UV-visible absorption, photoluminescence, and Raman spectroscopy, we successfully unravelled the mechanisms for the change in optoelectronic properties of $\\mathrm{MoS_{2}}$ QDs with laser parameters. We realize that the occurrence of a secondary phase, specifically $\\mathrm{MoO_{3-x}}$, is responsible for the significant Stokes-shift and blue emission observed in this QDs system. The primary factor influencing these activities is the electron transfer observed between these two phases, as validated by excitation dependent photoluminescence, XPS and Raman spectroscopies.","sentences":["Herein, $\\mathrm{MoS_{2}}$ quantum dot (QDs) with controlled optical, structural, and electronic properties are synthesized using the femtosecond pulsed laser ablation in liquid (fs-PLAL) technique by varying pulse-width, ablation power, and ablation time to harness the potential for next-generation optoelectronics and quantum technology.","Furthermore, this work elucidates key aspects of the mechanisms underlying the near-UV and blue emission, the accompanying large Stokes-shift, and the consequent change in sample color with laser exposure parameters pertaining to $\\mathrm{MoS_{2}}$ QDs.","Through spectroscopic analysis, including UV-visible absorption, photoluminescence, and Raman spectroscopy, we successfully unravelled the mechanisms for the change in optoelectronic properties of $\\mathrm{MoS_{2}}$ QDs with laser parameters.","We realize that the occurrence of a secondary phase, specifically $\\mathrm{MoO_{3-x}}$, is responsible for the significant Stokes-shift and blue emission observed in this QDs system.","The primary factor influencing these activities is the electron transfer observed between these two phases, as validated by excitation dependent photoluminescence, XPS and Raman spectroscopies."],"url":"http://arxiv.org/abs/2405.11934v1","category":"physics.app-ph"}
{"created":"2024-05-20 10:16:26","title":"Nonequilbrium physics of generative diffusion models","abstract":"Generative diffusion models apply the concept of Langevin dynamics in physics to machine leaning, attracting a lot of interest from industrial application, but a complete picture about inherent mechanisms is still lacking. In this paper, we provide a transparent physics analysis of the diffusion models, deriving the fluctuation theorem, entropy production, Franz-Parisi potential to understand the intrinsic phase transitions discovered recently. Our analysis is rooted in non-equlibrium physics and concepts from equilibrium physics, i.e., treating both forward and backward dynamics as a Langevin dynamics, and treating the reverse diffusion generative process as a statistical inference, where the time-dependent state variables serve as quenched disorder studied in spin glass theory. This unified principle is expected to guide machine learning practitioners to design better algorithms and theoretical physicists to link the machine learning to non-equilibrium thermodynamics.","sentences":["Generative diffusion models apply the concept of Langevin dynamics in physics to machine leaning, attracting a lot of interest from industrial application, but a complete picture about inherent mechanisms is still lacking.","In this paper, we provide a transparent physics analysis of the diffusion models, deriving the fluctuation theorem, entropy production, Franz-Parisi potential to understand the intrinsic phase transitions discovered recently.","Our analysis is rooted in non-equlibrium physics and concepts from equilibrium physics, i.e., treating both forward and backward dynamics as a Langevin dynamics, and treating the reverse diffusion generative process as a statistical inference, where the time-dependent state variables serve as quenched disorder studied in spin glass theory.","This unified principle is expected to guide machine learning practitioners to design better algorithms and theoretical physicists to link the machine learning to non-equilibrium thermodynamics."],"url":"http://arxiv.org/abs/2405.11932v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-20 10:06:33","title":"\"Set It Up!\": Functional Object Arrangement with Compositional Generative Models","abstract":"This paper studies the challenge of developing robots capable of understanding under-specified instructions for creating functional object arrangements, such as \"set up a dining table for two\"; previous arrangement approaches have focused on much more explicit instructions, such as \"put object A on the table.\" We introduce a framework, SetItUp, for learning to interpret under-specified instructions. SetItUp takes a small number of training examples and a human-crafted program sketch to uncover arrangement rules for specific scene types. By leveraging an intermediate graph-like representation of abstract spatial relationships among objects, SetItUp decomposes the arrangement problem into two subproblems: i) learning the arrangement patterns from limited data and ii) grounding these abstract relationships into object poses. SetItUp leverages large language models (LLMs) to propose the abstract spatial relationships among objects in novel scenes as the constraints to be satisfied; then, it composes a library of diffusion models associated with these abstract relationships to find object poses that satisfy the constraints. We validate our framework on a dataset comprising study desks, dining tables, and coffee tables, with the results showing superior performance in generating physically plausible, functional, and aesthetically pleasing object arrangements compared to existing models.","sentences":["This paper studies the challenge of developing robots capable of understanding under-specified instructions for creating functional object arrangements, such as \"set up a dining table for two\"; previous arrangement approaches have focused on much more explicit instructions, such as \"put object A on the table.\"","We introduce a framework, SetItUp, for learning to interpret under-specified instructions.","SetItUp takes a small number of training examples and a human-crafted program sketch to uncover arrangement rules for specific scene types.","By leveraging an intermediate graph-like representation of abstract spatial relationships among objects, SetItUp decomposes the arrangement problem into two subproblems: i) learning the arrangement patterns from limited data and ii) grounding these abstract relationships into object poses.","SetItUp leverages large language models (LLMs) to propose the abstract spatial relationships among objects in novel scenes as the constraints to be satisfied; then, it composes a library of diffusion models associated with these abstract relationships to find object poses that satisfy the constraints.","We validate our framework on a dataset comprising study desks, dining tables, and coffee tables, with the results showing superior performance in generating physically plausible, functional, and aesthetically pleasing object arrangements compared to existing models."],"url":"http://arxiv.org/abs/2405.11928v1","category":"cs.RO"}
{"created":"2024-05-20 10:06:19","title":"Response time in a pair of processor sharing queues with Join-the-Shortest-Queue scheduling","abstract":"Join-the-Shortest-Queue (JSQ) is the scheduling policy of choice for many network providers, cloud servers and traffic management systems, where individual queues are served under processor sharing (PS) queueing discipline. A numerical solution for the response time distribution in two parallel PS queues with JSQ scheduling is derived for the first time. Using the generating function method, two partial differential equations (PDEs) are obtained corresponding to conditional response times, where the conditioning is on a particular traced task joining the first or the second queue. These PDEs are functional equations that contain partial generating functions and their partial derivatives, and therefore cannot be solved by commonly used techniques. We are able to solve these PDEs numerically with good accuracy and perform the deconditioning with respect to the queue-length probabilities by evaluating a certain complex integral. Numerical results for the density and the first four moments compare well against regenerative simulation with 500,000 regeneration cycles.","sentences":["Join-the-Shortest-Queue (JSQ) is the scheduling policy of choice for many network providers, cloud servers and traffic management systems, where individual queues are served under processor sharing (PS) queueing discipline.","A numerical solution for the response time distribution in two parallel PS queues with JSQ scheduling is derived for the first time.","Using the generating function method, two partial differential equations (PDEs) are obtained corresponding to conditional response times, where the conditioning is on a particular traced task joining the first or the second queue.","These PDEs are functional equations that contain partial generating functions and their partial derivatives, and therefore cannot be solved by commonly used techniques.","We are able to solve these PDEs numerically with good accuracy and perform the deconditioning with respect to the queue-length probabilities by evaluating a certain complex integral.","Numerical results for the density and the first four moments compare well against regenerative simulation with 500,000 regeneration cycles."],"url":"http://arxiv.org/abs/2405.11927v1","category":"cs.PF"}
{"created":"2024-05-20 10:00:27","title":"Volkov-Pankratov states in a driven semimetal for a generic interface","abstract":"Volkov-Pankratov states are the non-topological massive bound states which generally arise across the smooth interface between two adjacent regions of a two-band semimetal, over which a gap parameter changes sign smoothly. In this work, we show that these modes can be engineered even for a generic smooth interface without any sign inversion. We consider a threefold and a twofold topological semimetal in which two adjacent regions are illuminated by light from two different sources with different light parameters (amplitudes, frequency, phase etc). We show that the interface can exhibit a quantum well for a certain parameter regime even without any sign change of the gap term. Such quantum well can host a number of Volkov-Pankratov states. Finally, we discuss the stability of these states against the deformation of the interfacial well and its transport signatures (Ramsauer Townsend effect).","sentences":["Volkov-Pankratov states are the non-topological massive bound states which generally arise across the smooth interface between two adjacent regions of a two-band semimetal, over which a gap parameter changes sign smoothly.","In this work, we show that these modes can be engineered even for a generic smooth interface without any sign inversion.","We consider a threefold and a twofold topological semimetal in which two adjacent regions are illuminated by light from two different sources with different light parameters (amplitudes, frequency, phase etc).","We show that the interface can exhibit a quantum well for a certain parameter regime even without any sign change of the gap term.","Such quantum well can host a number of Volkov-Pankratov states.","Finally, we discuss the stability of these states against the deformation of the interfacial well and its transport signatures (Ramsauer Townsend effect)."],"url":"http://arxiv.org/abs/2405.11924v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 09:59:03","title":"Rate Optimality and Phase Transition for User-Level Local Differential Privacy","abstract":"Most of the literature on differential privacy considers the item-level case where each user has a single observation, but a growing field of interest is that of user-level privacy where each of the $n$ users holds $T$ observations and wishes to maintain the privacy of their entire collection.   In this paper, we derive a general minimax lower bound, which shows that, for locally private user-level estimation problems, the risk cannot, in general, be made to vanish for a fixed number of users even when each user holds an arbitrarily large number of observations. We then derive matching, up to logarithmic factors, lower and upper bounds for univariate and multidimensional mean estimation, sparse mean estimation and non-parametric density estimation. In particular, with other model parameters held fixed, we observe phase transition phenomena in the minimax rates as $T$ the number of observations each user holds varies.   In the case of (non-sparse) mean estimation and density estimation, we see that, for $T$ below a phase transition boundary, the rate is the same as having $nT$ users in the item-level setting. Different behaviour is however observed in the case of $s$-sparse $d$-dimensional mean estimation, wherein consistent estimation is impossible when $d$ exceeds the number of observations in the item-level setting, but is possible in the user-level setting when $T \\gtrsim s \\log (d)$, up to logarithmic factors. This may be of independent interest for applications as an example of a high-dimensional problem that is feasible under local privacy constraints.","sentences":["Most of the literature on differential privacy considers the item-level case where each user has a single observation, but a growing field of interest is that of user-level privacy where each of the $n$ users holds $T$ observations and wishes to maintain the privacy of their entire collection.   ","In this paper, we derive a general minimax lower bound, which shows that, for locally private user-level estimation problems, the risk cannot, in general, be made to vanish for a fixed number of users even when each user holds an arbitrarily large number of observations.","We then derive matching, up to logarithmic factors, lower and upper bounds for univariate and multidimensional mean estimation, sparse mean estimation and non-parametric density estimation.","In particular, with other model parameters held fixed, we observe phase transition phenomena in the minimax rates as $T$ the number of observations each user holds varies.   ","In the case of (non-sparse) mean estimation and density estimation, we see that, for $T$ below a phase transition boundary, the rate is the same as having $nT$ users in the item-level setting.","Different behaviour is however observed in the case of $s$-sparse $d$-dimensional mean estimation, wherein consistent estimation is impossible when $d$ exceeds the number of observations in the item-level setting, but is possible in the user-level setting when $T \\gtrsim s \\log (d)$, up to logarithmic factors.","This may be of independent interest for applications as an example of a high-dimensional problem that is feasible under local privacy constraints."],"url":"http://arxiv.org/abs/2405.11923v1","category":"math.ST"}
{"created":"2024-05-20 09:57:37","title":"Analysis of $^{115}$In $\u03b2$ decay through the spectral moment method","abstract":"We analyze the $^{115}$In $\\beta$-decay energy spectrum through the spectral moment method (SMM), previously introduced in the context of $^{113}$Cd $\\beta$ decay. The spectral moments $\\mu_n$ are defined as averaged $n^{\\rm th}$ powers of the $\\beta$ particle energy, characterizing the spectrum normalization ($n=0$) and shape ($n\\geq 1$) above a given threshold. For $^{115}$In, we consider three independent datasets characterized by different thresholds. We also consider three nuclear model calculations with two free parameters: the ratio of axial-vector to vector couplings, $r=g_{\\rm A}/g_{\\rm V}$, and the small vector-like relativistic nuclear matrix element (NME), $s=s$-NME. By using the most recent of the three datasets, we show that the first few spectral moments can determine $(r,\\, s)$ values in good agreement with those obtained by full-fledged experimental fits. We then work out the SMM results for the other datasets. We find that, although $g_{\\rm A}$ quenching is generally favored, the preferred quenching factors may differ considerably depending on the chosen experimental data and nuclear models. We discuss various issues affecting both the overall normalization and the low-energy behaviour of the measured and computed spectra, and their joint effects on the experimentally quoted half-life values. Further $^{115}$In $\\beta$-decay data at the lowest possible energy threshold appear to be crucial to clarify these issues.","sentences":["We analyze the $^{115}$In $\\beta$-decay energy spectrum through the spectral moment method (SMM), previously introduced in the context of $^{113}$Cd $\\beta$ decay.","The spectral moments $\\mu_n$ are defined as averaged $n^{\\rm th}$ powers of the $\\beta$ particle energy, characterizing the spectrum normalization ($n=0$) and shape ($n\\geq 1$) above a given threshold.","For $^{115}$In, we consider three independent datasets characterized by different thresholds.","We also consider three nuclear model calculations with two free parameters: the ratio of axial-vector to vector couplings, $r=g_{\\rm A}/g_{\\rm V}$, and the small vector-like relativistic nuclear matrix element (NME), $s=s$-NME.","By using the most recent of the three datasets, we show that the first few spectral moments can determine $(r,\\, s)$ values in good agreement with those obtained by full-fledged experimental fits.","We then work out the SMM results for the other datasets.","We find that, although $g_{\\rm A}$ quenching is generally favored, the preferred quenching factors may differ considerably depending on the chosen experimental data and nuclear models.","We discuss various issues affecting both the overall normalization and the low-energy behaviour of the measured and computed spectra, and their joint effects on the experimentally quoted half-life values.","Further $^{115}$In $\\beta$-decay data at the lowest possible energy threshold appear to be crucial to clarify these issues."],"url":"http://arxiv.org/abs/2405.11920v1","category":"nucl-th"}
{"created":"2024-05-20 09:57:29","title":"On Efficient and Statistical Quality Estimation for Data Annotation","abstract":"Annotated datasets are an essential ingredient to train, evaluate, compare and productionalize supervised machine learning models. It is therefore imperative that annotations are of high quality. For their creation, good quality management and thereby reliable quality estimates are needed. Then, if quality is insufficient during the annotation process, rectifying measures can be taken to improve it. Quality estimation is often performed by having experts manually label instances as correct or incorrect. But checking all annotated instances tends to be expensive. Therefore, in practice, usually only subsets are inspected; sizes are chosen mostly without justification or regard to statistical power and more often than not, are relatively small. Basing estimates on small sample sizes, however, can lead to imprecise values for the error rate. Using unnecessarily large sample sizes costs money that could be better spent, for instance on more annotations. Therefore, we first describe in detail how to use confidence intervals for finding the minimal sample size needed to estimate the annotation error rate. Then, we propose applying acceptance sampling as an alternative to error rate estimation We show that acceptance sampling can reduce the required sample sizes up to 50% while providing the same statistical guarantees.","sentences":["Annotated datasets are an essential ingredient to train, evaluate, compare and productionalize supervised machine learning models.","It is therefore imperative that annotations are of high quality.","For their creation, good quality management and thereby reliable quality estimates are needed.","Then, if quality is insufficient during the annotation process, rectifying measures can be taken to improve it.","Quality estimation is often performed by having experts manually label instances as correct or incorrect.","But checking all annotated instances tends to be expensive.","Therefore, in practice, usually only subsets are inspected; sizes are chosen mostly without justification or regard to statistical power and more often than not, are relatively small.","Basing estimates on small sample sizes, however, can lead to imprecise values for the error rate.","Using unnecessarily large sample sizes costs money that could be better spent, for instance on more annotations.","Therefore, we first describe in detail how to use confidence intervals for finding the minimal sample size needed to estimate the annotation error rate.","Then, we propose applying acceptance sampling as an alternative to error rate estimation We show that acceptance sampling can reduce the required sample sizes up to 50% while providing the same statistical guarantees."],"url":"http://arxiv.org/abs/2405.11919v1","category":"cs.LG"}
{"created":"2024-05-20 09:55:59","title":"The general position number under vertex and edge removal","abstract":"Let ${\\rm gp}(G)$ be the general position number of a graph $G$. It is proved that ${\\rm gp}(G-x)\\leq 2{\\rm gp}(G)$ holds for any vertex $x$ of a connected graph $G$ and that if $x$ lies in some ${\\rm gp}$-set of $G$, then ${\\rm gp}(G) - 1 \\le {\\rm gp}(G-x)$. Constructions are given which show that ${\\rm gp}(G-x)$ can be much larger than ${\\rm gp}(G)$ also when $G-x$ is connected. For diameter $2$ graphs it is proved that ${\\rm gp}(G-x) \\le {\\rm gp}(G)$, and that ${\\rm gp}(G-x) \\ge {\\rm gp}(G) - 1$ when the diameter of $G-x$ remains $2$. It is demonstrated that ${\\rm gp}(G)/2\\le {\\rm gp}(G-e)\\leq 2{\\rm gp}(G)$ holds for any edge $e$ of a graph $G$. For diameter $2$ graphs these results sharpens to ${\\rm gp}(G)-1\\le {\\rm gp}(G-e)\\leq\\ {\\rm gp}(G) + 1$. All these bounds are proved to be sharp.","sentences":["Let ${\\rm gp}(G)$ be the general position number of a graph","$G$.","It is proved that ${\\rm gp}(G-x)\\leq 2{\\rm gp}(G)$ holds for any vertex $x$ of a connected graph $G$ and that if $x$ lies in some ${\\rm gp}$-set of $G$, then ${\\rm gp}(G) - 1 \\le {\\rm gp}(G-x)$. Constructions are given which show that ${\\rm gp}(G-x)$ can be much larger than ${\\rm gp}(G)$ also when $G-x$ is connected.","For diameter $2$ graphs it is proved that ${\\rm gp}(G-x) \\le {\\rm gp}(G)$, and that ${\\rm gp}(G-x) \\ge {\\rm gp}(G) - 1$ when the diameter of $G-x$ remains $2$.","It is demonstrated that ${\\rm gp}(G)/2\\le {\\rm gp}(G-e)\\leq 2{\\rm gp}(G)$ holds for any edge $e$ of a graph $G$. For diameter $2$ graphs these results sharpens to ${\\rm gp}(G)-1\\le {\\rm gp}(G-e)\\leq\\ {\\rm gp}(G)","+","1$.","All these bounds are proved to be sharp."],"url":"http://arxiv.org/abs/2405.11918v1","category":"math.CO"}
{"created":"2024-05-20 09:55:20","title":"A Competitive Showcase of Quantum versus Classical Algorithms in Energy Coalition Formation","abstract":"The formation of energy communities is pivotal for advancing decentralized and sustainable energy management. Within this context, Coalition Structure Generation (CSG) emerges as a promising framework. The complexity of CSG grows rapidly with the number of agents, making classical solvers impractical for even moderate sizes (number of agents>30). Therefore, the development of advanced computational methods is essential. Motivated by this challenge, this study conducts a benchmark comparing classical solvers with quantum annealing on Dwave hardware and the Quantum Approximation Optimization Algorithm (QAOA) on both simulator and IBMQ hardware to address energy community formation. Our classical solvers include Tabu search, simulated annealing, and an exact classical solver. Our findings reveal that Dwave surpasses QAOA on hardware in terms of solution quality. Remarkably, QAOA demonstrates comparable runtime scaling with Dwave, albeit with a significantly larger prefactor. Notably, Dwave exhibits competitive performance compared to the classical solvers, achieving solutions of equal quality with more favorable runtime scaling.","sentences":["The formation of energy communities is pivotal for advancing decentralized and sustainable energy management.","Within this context, Coalition Structure Generation (CSG) emerges as a promising framework.","The complexity of CSG grows rapidly with the number of agents, making classical solvers impractical for even moderate sizes (number of agents>30).","Therefore, the development of advanced computational methods is essential.","Motivated by this challenge, this study conducts a benchmark comparing classical solvers with quantum annealing on Dwave hardware and the Quantum Approximation Optimization Algorithm (QAOA) on both simulator and IBMQ hardware to address energy community formation.","Our classical solvers include Tabu search, simulated annealing, and an exact classical solver.","Our findings reveal that Dwave surpasses QAOA on hardware in terms of solution quality.","Remarkably, QAOA demonstrates comparable runtime scaling with Dwave, albeit with a significantly larger prefactor.","Notably, Dwave exhibits competitive performance compared to the classical solvers, achieving solutions of equal quality with more favorable runtime scaling."],"url":"http://arxiv.org/abs/2405.11917v1","category":"quant-ph"}
{"created":"2024-05-20 09:51:56","title":"Two new calibration techniques of lumped-parameter mathematical models for the cardiovascular system","abstract":"Cardiocirculatory mathematical models serve as valuable tools for investigating physiological and pathological conditions of the circulatory system. To investigate the clinical condition of an individual, cardiocirculatory models need to be personalized by means of calibration methods. In this study we propose a new calibration method for a lumped-parameter cardiocirculatory model. This calibration method utilizes the correlation matrix between parameters and model outputs to calibrate the latter according to data. We test this calibration method and its combination with L-BFGS-B (Limited memory Broyden - Fletcher - Goldfarb - Shanno with Bound constraints) comparing them with the performances of L-BFGS-B alone. We show that the correlation matrix calibration method and the combined one effectively reduce the loss function of the associated optimization problem. In the case of in silico generated data, we show that the two new calibration methods are robust with respect to the initial guess of parameters and to the presence of noise in the data. Notably, the correlation matrix calibration method achieves the best results in estimating the parameters in the case of noisy data and it is faster than the combined calibration method and L-BFGS-B. Finally, we present real test case where the two new calibration methods yield results comparable to those obtained using L-BFGS-B in terms of minimizing the loss function and estimating the clinical data. This highlights the effectiveness of the new calibration methods for clinical applications.","sentences":["Cardiocirculatory mathematical models serve as valuable tools for investigating physiological and pathological conditions of the circulatory system.","To investigate the clinical condition of an individual, cardiocirculatory models need to be personalized by means of calibration methods.","In this study we propose a new calibration method for a lumped-parameter cardiocirculatory model.","This calibration method utilizes the correlation matrix between parameters and model outputs to calibrate the latter according to data.","We test this calibration method and its combination with L-BFGS-B (Limited memory Broyden - Fletcher - Goldfarb - Shanno with Bound constraints) comparing them with the performances of L-BFGS-B alone.","We show that the correlation matrix calibration method and the combined one effectively reduce the loss function of the associated optimization problem.","In the case of in silico generated data, we show that the two new calibration methods are robust with respect to the initial guess of parameters and to the presence of noise in the data.","Notably, the correlation matrix calibration method achieves the best results in estimating the parameters in the case of noisy data and it is faster than the combined calibration method and L-BFGS-B. Finally, we present real test case where the two new calibration methods yield results comparable to those obtained using L-BFGS-B in terms of minimizing the loss function and estimating the clinical data.","This highlights the effectiveness of the new calibration methods for clinical applications."],"url":"http://arxiv.org/abs/2405.11915v1","category":"math.NA"}
{"created":"2024-05-20 09:49:13","title":"PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single Highly-Ambiguous RGB Images","abstract":"Generating 3D shapes from single RGB images is essential in various applications such as robotics. Current approaches typically target images containing clear and complete visual descriptions of the object, without considering common realistic cases where observations of objects that are largely occluded or truncated. We thus propose a transformer-based autoregressive model to generate the probabilistic distribution of 3D shapes conditioned on an RGB image containing potentially highly ambiguous observations of the object. To handle realistic scenarios such as occlusion or field-of-view truncation, we create simulated image-to-shape training pairs that enable improved fine-tuning for real-world scenarios. We then adopt cross-attention to effectively identify the most relevant region of interest from the input image for shape generation. This enables inference of sampled shapes with reasonable diversity and strong alignment with the input image. We train and test our model on our synthetic data then fine-tune and test it on real-world data. Experiments demonstrate that our model outperforms state of the art in both scenarios","sentences":["Generating 3D shapes from single RGB images is essential in various applications such as robotics.","Current approaches typically target images containing clear and complete visual descriptions of the object, without considering common realistic cases where observations of objects that are largely occluded or truncated.","We thus propose a transformer-based autoregressive model to generate the probabilistic distribution of 3D shapes conditioned on an RGB image containing potentially highly ambiguous observations of the object.","To handle realistic scenarios such as occlusion or field-of-view truncation, we create simulated image-to-shape training pairs that enable improved fine-tuning for real-world scenarios.","We then adopt cross-attention to effectively identify the most relevant region of interest from the input image for shape generation.","This enables inference of sampled shapes with reasonable diversity and strong alignment with the input image.","We train and test our model on our synthetic data then fine-tune and test it on real-world data.","Experiments demonstrate that our model outperforms state of the art in both scenarios"],"url":"http://arxiv.org/abs/2405.11914v1","category":"cs.CV"}
{"created":"2024-05-20 09:48:36","title":"Diff-BGM: A Diffusion Model for Video Background Music Generation","abstract":"When editing a video, a piece of attractive background music is indispensable. However, video background music generation tasks face several challenges, for example, the lack of suitable training datasets, and the difficulties in flexibly controlling the music generation process and sequentially aligning the video and music. In this work, we first propose a high-quality music-video dataset BGM909 with detailed annotation and shot detection to provide multi-modal information about the video and music. We then present evaluation metrics to assess music quality, including music diversity and alignment between music and video with retrieval precision metrics. Finally, we propose the Diff-BGM framework to automatically generate the background music for a given video, which uses different signals to control different aspects of the music during the generation process, i.e., uses dynamic video features to control music rhythm and semantic features to control the melody and atmosphere. We propose to align the video and music sequentially by introducing a segment-aware cross-attention layer. Experiments verify the effectiveness of our proposed method. The code and models are available at https://github.com/sizhelee/Diff-BGM.","sentences":["When editing a video, a piece of attractive background music is indispensable.","However, video background music generation tasks face several challenges, for example, the lack of suitable training datasets, and the difficulties in flexibly controlling the music generation process and sequentially aligning the video and music.","In this work, we first propose a high-quality music-video dataset BGM909 with detailed annotation and shot detection to provide multi-modal information about the video and music.","We then present evaluation metrics to assess music quality, including music diversity and alignment between music and video with retrieval precision metrics.","Finally, we propose the Diff-BGM framework to automatically generate the background music for a given video, which uses different signals to control different aspects of the music during the generation process, i.e., uses dynamic video features to control music rhythm and semantic features to control the melody and atmosphere.","We propose to align the video and music sequentially by introducing a segment-aware cross-attention layer.","Experiments verify the effectiveness of our proposed method.","The code and models are available at https://github.com/sizhelee/Diff-BGM."],"url":"http://arxiv.org/abs/2405.11913v1","category":"cs.CV"}
{"created":"2024-05-20 09:48:15","title":"ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation","abstract":"Human annotation is a time-consuming task that requires a significant amount of effort. To address this issue, interactive data annotation utilizes an annotation model to provide suggestions for humans to approve or correct. However, annotation models trained with limited labeled data are prone to generating incorrect suggestions, leading to extra human correction effort. To tackle this challenge, we propose Araida, an analogical reasoning-based approach that enhances automatic annotation accuracy in the interactive data annotation setting and reduces the need for human corrections. Araida involves an error-aware integration strategy that dynamically coordinates an annotation model and a k-nearest neighbors (KNN) model, giving more importance to KNN's predictions when predictions from the annotation model are deemed inaccurate. Empirical studies demonstrate that Araida is adaptable to different annotation tasks and models. On average, it reduces human correction labor by 11.02% compared to vanilla interactive data annotation methods.","sentences":["Human annotation is a time-consuming task that requires a significant amount of effort.","To address this issue, interactive data annotation utilizes an annotation model to provide suggestions for humans to approve or correct.","However, annotation models trained with limited labeled data are prone to generating incorrect suggestions, leading to extra human correction effort.","To tackle this challenge, we propose Araida, an analogical reasoning-based approach that enhances automatic annotation accuracy in the interactive data annotation setting and reduces the need for human corrections.","Araida involves an error-aware integration strategy that dynamically coordinates an annotation model and a k-nearest neighbors (KNN) model, giving more importance to KNN's predictions when predictions from the annotation model are deemed inaccurate.","Empirical studies demonstrate that Araida is adaptable to different annotation tasks and models.","On average, it reduces human correction labor by 11.02% compared to vanilla interactive data annotation methods."],"url":"http://arxiv.org/abs/2405.11912v1","category":"cs.CL"}
{"created":"2024-05-20 09:47:22","title":"PULL: PU-Learning-based Accurate Link Prediction","abstract":"Given an edge-incomplete graph, how can we accurately find the missing links? The link prediction in edge-incomplete graphs aims to discover the missing relations between entities when their relationships are represented as a graph. Edge-incomplete graphs are prevalent in real-world due to practical limitations, such as not checking all users when adding friends in a social network. Addressing the problem is crucial for various tasks, including recommending friends in social networks and finding references in citation networks. However, previous approaches rely heavily on the given edge-incomplete (observed) graph, making it challenging to consider the missing (unobserved) links during training. In this paper, we propose PULL (PU-Learning-based Link predictor), an accurate link prediction method based on the positive-unlabeled (PU) learning. PULL treats the observed edges in the training graph as positive examples, and the unconnected node pairs as unlabeled ones. PULL effectively prevents the link predictor from overfitting to the observed graph by proposing latent variables for every edge, and leveraging the expected graph structure with respect to the variables. Extensive experiments on five real-world datasets show that PULL consistently outperforms the baselines for predicting links in edge-incomplete graphs.","sentences":["Given an edge-incomplete graph, how can we accurately find the missing links?","The link prediction in edge-incomplete graphs aims to discover the missing relations between entities when their relationships are represented as a graph.","Edge-incomplete graphs are prevalent in real-world due to practical limitations, such as not checking all users when adding friends in a social network.","Addressing the problem is crucial for various tasks, including recommending friends in social networks and finding references in citation networks.","However, previous approaches rely heavily on the given edge-incomplete (observed) graph, making it challenging to consider the missing (unobserved) links during training.","In this paper, we propose PULL (PU-Learning-based Link predictor), an accurate link prediction method based on the positive-unlabeled (PU) learning.","PULL treats the observed edges in the training graph as positive examples, and the unconnected node pairs as unlabeled ones.","PULL effectively prevents the link predictor from overfitting to the observed graph by proposing latent variables for every edge, and leveraging the expected graph structure with respect to the variables.","Extensive experiments on five real-world datasets show that PULL consistently outperforms the baselines for predicting links in edge-incomplete graphs."],"url":"http://arxiv.org/abs/2405.11911v1","category":"cs.AI"}
{"created":"2024-05-20 09:44:24","title":"3D Reconfigurable Intelligent Surfaces for Satellite-Terrestrial Networks","abstract":"This letter proposes a three-dimensional (3D) satellite-terrestrial communication network assisted with reconfigurable intelligent surfaces (RISs). Using stochastic geometry models, we present an original framework to derive tractable yet accurate closed-form expressions for coverage probability and ergodic capacity in the presence of fading. A homogeneous Poisson point process models the satellites on a sphere, while RISs are randomly deployed in a 3D cylindrical region. We consider nonidentical channels that correspond to different RISs and follow the \\kappa-\\mu fading distribution. We verify the accuracy of the adopted approach through Monte Carlo simulations, demonstrating the significant improvement in system performance due to using RISs. Furthermore, we show that increasing the number of reflecting elements or RISs and placing them closer to the user improves performance considerably.","sentences":["This letter proposes a three-dimensional (3D) satellite-terrestrial communication network assisted with reconfigurable intelligent surfaces (RISs).","Using stochastic geometry models, we present an original framework to derive tractable yet accurate closed-form expressions for coverage probability and ergodic capacity in the presence of fading.","A homogeneous Poisson point process models the satellites on a sphere, while RISs are randomly deployed in a 3D cylindrical region.","We consider nonidentical channels that correspond to different RISs and follow the \\kappa-\\mu fading distribution.","We verify the accuracy of the adopted approach through Monte Carlo simulations, demonstrating the significant improvement in system performance due to using RISs.","Furthermore, we show that increasing the number of reflecting elements or RISs and placing them closer to the user improves performance considerably."],"url":"http://arxiv.org/abs/2405.11909v1","category":"eess.SP"}
{"created":"2024-05-20 09:42:44","title":"Ensemble and Mixture-of-Experts DeepONets For Operator Learning","abstract":"We present a novel deep operator network (DeepONet) architecture for operator learning, the ensemble DeepONet, that allows for enriching the trunk network of a single DeepONet with multiple distinct trunk networks. This trunk enrichment allows for greater expressivity and generalization capabilities over a range of operator learning problems. We also present a spatial mixture-of-experts (MoE) DeepONet trunk network architecture that utilizes a partition-of-unity (PoU) approximation to promote spatial locality and model sparsity in the operator learning problem. We first prove that both the ensemble and PoU-MoE DeepONets are universal approximators. We then demonstrate that ensemble DeepONets containing a trunk ensemble of a standard trunk, the PoU-MoE trunk, and/or a proper orthogonal decomposition (POD) trunk can achieve 2-4x lower relative $\\ell_2$ errors than standard DeepONets and POD-DeepONets on both standard and challenging new operator learning problems involving partial differential equations (PDEs) in two and three dimensions. Our new PoU-MoE formulation provides a natural way to incorporate spatial locality and model sparsity into any neural network architecture, while our new ensemble DeepONet provides a powerful and general framework for incorporating basis enrichment in scientific machine learning architectures for operator learning.","sentences":["We present a novel deep operator network (DeepONet) architecture for operator learning, the ensemble DeepONet, that allows for enriching the trunk network of a single DeepONet with multiple distinct trunk networks.","This trunk enrichment allows for greater expressivity and generalization capabilities over a range of operator learning problems.","We also present a spatial mixture-of-experts (MoE) DeepONet trunk network architecture that utilizes a partition-of-unity (PoU) approximation to promote spatial locality and model sparsity in the operator learning problem.","We first prove that both the ensemble and PoU-MoE DeepONets are universal approximators.","We then demonstrate that ensemble DeepONets containing a trunk ensemble of a standard trunk, the PoU-MoE trunk, and/or a proper orthogonal decomposition (POD) trunk can achieve 2-4x lower relative $\\ell_2$ errors than standard DeepONets and POD-DeepONets on both standard and challenging new operator learning problems involving partial differential equations (PDEs) in two and three dimensions.","Our new PoU-MoE formulation provides a natural way to incorporate spatial locality and model sparsity into any neural network architecture, while our new ensemble DeepONet provides a powerful and general framework for incorporating basis enrichment in scientific machine learning architectures for operator learning."],"url":"http://arxiv.org/abs/2405.11907v1","category":"cs.LG"}
{"created":"2024-05-20 09:40:06","title":"Pole structure of $P_\u03c8^N(4312)^+$ via machine learning and uniformized S-matrix","abstract":"We probed the pole structure of the $P_\\psi^{N}(4312)^{+}$ using a trained deep neural network. The training dataset was generated using uniformized independent S-matrix poles to ensure that the obtained interpretation is as model-independent as possible. To prevent possible ambiguity in the interpretation of the pole structure, we included the contribution from the off-diagonal element of the S-matrix. Five out of the six neural networks we trained favor $P_\\psi^{N}(4312)^{+}$ as possibly having a three-pole structure, with one pole on each of the unphysical sheets - a first in its report. The two poles can be associated to a pole-shadow pair which is a characteristic of a true resonance. On the other hand, the last pole is most likely associated with the coupled-channel effect. The combined effect of these poles produced a peak below the $\\Sigma^{+}_C\\bar{D}^0$ which mimic the line shape of a hadronic molecule.","sentences":["We probed the pole structure of the $P_\\psi^{N}(4312)^{+}$ using a trained deep neural network.","The training dataset was generated using uniformized independent S-matrix poles to ensure that the obtained interpretation is as model-independent as possible.","To prevent possible ambiguity in the interpretation of the pole structure, we included the contribution from the off-diagonal element of the S-matrix.","Five out of the six neural networks we trained favor $P_\\psi^{N}(4312)^{+}$ as possibly having a three-pole structure, with one pole on each of the unphysical sheets - a first in its report.","The two poles can be associated to a pole-shadow pair which is a characteristic of a true resonance.","On the other hand, the last pole is most likely associated with the coupled-channel effect.","The combined effect of these poles produced a peak below the $\\Sigma^{+}_C\\bar{D}^0$ which mimic the line shape of a hadronic molecule."],"url":"http://arxiv.org/abs/2405.11906v1","category":"hep-ph"}
{"created":"2024-05-20 09:38:37","title":"CSTA: CNN-based Spatiotemporal Attention for Video Summarization","abstract":"Video summarization aims to generate a concise representation of a video, capturing its essential content and key moments while reducing its overall length. Although several methods employ attention mechanisms to handle long-term dependencies, they often fail to capture the visual significance inherent in frames. To address this limitation, we propose a CNN-based SpatioTemporal Attention (CSTA) method that stacks each feature of frames from a single video to form image-like frame representations and applies 2D CNN to these frame features. Our methodology relies on CNN to comprehend the inter and intra-frame relations and to find crucial attributes in videos by exploiting its ability to learn absolute positions within images. In contrast to previous work compromising efficiency by designing additional modules to focus on spatial importance, CSTA requires minimal computational overhead as it uses CNN as a sliding window. Extensive experiments on two benchmark datasets (SumMe and TVSum) demonstrate that our proposed approach achieves state-of-the-art performance with fewer MACs compared to previous methods. Codes are available at https://github.com/thswodnjs3/CSTA.","sentences":["Video summarization aims to generate a concise representation of a video, capturing its essential content and key moments while reducing its overall length.","Although several methods employ attention mechanisms to handle long-term dependencies, they often fail to capture the visual significance inherent in frames.","To address this limitation, we propose a CNN-based SpatioTemporal Attention (CSTA) method that stacks each feature of frames from a single video to form image-like frame representations and applies 2D CNN to these frame features.","Our methodology relies on CNN to comprehend the inter and intra-frame relations and to find crucial attributes in videos by exploiting its ability to learn absolute positions within images.","In contrast to previous work compromising efficiency by designing additional modules to focus on spatial importance, CSTA requires minimal computational overhead as it uses CNN as a sliding window.","Extensive experiments on two benchmark datasets (SumMe and TVSum) demonstrate that our proposed approach achieves state-of-the-art performance with fewer MACs compared to previous methods.","Codes are available at https://github.com/thswodnjs3/CSTA."],"url":"http://arxiv.org/abs/2405.11905v1","category":"cs.CV"}
{"created":"2024-05-20 09:33:43","title":"A Constraint-Enforcing Reward for Adversarial Attacks on Text Classifiers","abstract":"Text classifiers are vulnerable to adversarial examples -- correctly-classified examples that are deliberately transformed to be misclassified while satisfying acceptability constraints. The conventional approach to finding adversarial examples is to define and solve a combinatorial optimisation problem over a space of allowable transformations. While effective, this approach is slow and limited by the choice of transformations. An alternate approach is to directly generate adversarial examples by fine-tuning a pre-trained language model, as is commonly done for other text-to-text tasks. This approach promises to be much quicker and more expressive, but is relatively unexplored. For this reason, in this work we train an encoder-decoder paraphrase model to generate a diverse range of adversarial examples. For training, we adopt a reinforcement learning algorithm and propose a constraint-enforcing reward that promotes the generation of valid adversarial examples. Experimental results over two text classification datasets show that our model has achieved a higher success rate than the original paraphrase model, and overall has proved more effective than other competitive attacks. Finally, we show how key design choices impact the generated examples and discuss the strengths and weaknesses of the proposed approach.","sentences":["Text classifiers are vulnerable to adversarial examples -- correctly-classified examples that are deliberately transformed to be misclassified while satisfying acceptability constraints.","The conventional approach to finding adversarial examples is to define and solve a combinatorial optimisation problem over a space of allowable transformations.","While effective, this approach is slow and limited by the choice of transformations.","An alternate approach is to directly generate adversarial examples by fine-tuning a pre-trained language model, as is commonly done for other text-to-text tasks.","This approach promises to be much quicker and more expressive, but is relatively unexplored.","For this reason, in this work we train an encoder-decoder paraphrase model to generate a diverse range of adversarial examples.","For training, we adopt a reinforcement learning algorithm and propose a constraint-enforcing reward that promotes the generation of valid adversarial examples.","Experimental results over two text classification datasets show that our model has achieved a higher success rate than the original paraphrase model, and overall has proved more effective than other competitive attacks.","Finally, we show how key design choices impact the generated examples and discuss the strengths and weaknesses of the proposed approach."],"url":"http://arxiv.org/abs/2405.11904v1","category":"cs.CL"}
{"created":"2024-05-20 09:30:03","title":"CReMa: Crisis Response through Computational Identification and Matching of Cross-Lingual Requests and Offers Shared on Social Media","abstract":"During times of crisis, social media platforms play a vital role in facilitating communication and coordinating resources. Amidst chaos and uncertainty, communities often rely on these platforms to share urgent pleas for help, extend support, and organize relief efforts. However, the sheer volume of conversations during such periods, which can escalate to unprecedented levels, necessitates the automated identification and matching of requests and offers to streamline relief operations. This study addresses the challenge of efficiently identifying and matching assistance requests and offers on social media platforms during emergencies. We propose CReMa (Crisis Response Matcher), a systematic approach that integrates textual, temporal, and spatial features for multi-lingual request-offer matching. By leveraging CrisisTransformers, a set of pre-trained models specific to crises, and a cross-lingual embedding space, our methodology enhances the identification and matching tasks while outperforming strong baselines such as RoBERTa, MPNet, and BERTweet, in classification tasks, and Universal Sentence Encoder, Sentence Transformers in crisis embeddings generation tasks. We introduce a novel multi-lingual dataset that simulates scenarios of help-seeking and offering assistance on social media across the 16 most commonly used languages in Australia. We conduct comprehensive cross-lingual experiments across these 16 languages, also while examining trade-offs between multiple vector search strategies and accuracy. Additionally, we analyze a million-scale geotagged global dataset to comprehend patterns in relation to seeking help and offering assistance on social media. Overall, these contributions advance the field of crisis informatics and provide benchmarks for future research in the area.","sentences":["During times of crisis, social media platforms play a vital role in facilitating communication and coordinating resources.","Amidst chaos and uncertainty, communities often rely on these platforms to share urgent pleas for help, extend support, and organize relief efforts.","However, the sheer volume of conversations during such periods, which can escalate to unprecedented levels, necessitates the automated identification and matching of requests and offers to streamline relief operations.","This study addresses the challenge of efficiently identifying and matching assistance requests and offers on social media platforms during emergencies.","We propose CReMa (Crisis Response Matcher), a systematic approach that integrates textual, temporal, and spatial features for multi-lingual request-offer matching.","By leveraging CrisisTransformers, a set of pre-trained models specific to crises, and a cross-lingual embedding space, our methodology enhances the identification and matching tasks while outperforming strong baselines such as RoBERTa, MPNet, and BERTweet, in classification tasks, and Universal Sentence Encoder, Sentence Transformers in crisis embeddings generation tasks.","We introduce a novel multi-lingual dataset that simulates scenarios of help-seeking and offering assistance on social media across the 16 most commonly used languages in Australia.","We conduct comprehensive cross-lingual experiments across these 16 languages, also while examining trade-offs between multiple vector search strategies and accuracy.","Additionally, we analyze a million-scale geotagged global dataset to comprehend patterns in relation to seeking help and offering assistance on social media.","Overall, these contributions advance the field of crisis informatics and provide benchmarks for future research in the area."],"url":"http://arxiv.org/abs/2405.11897v1","category":"cs.CL"}
{"created":"2024-05-20 09:18:16","title":"Tunable moir\u00e9 bandgap in hBN-aligned bilayer graphene device with in-situ electrostatic gating","abstract":"Over the years, great efforts have been devoted in introducing a sizable and tunable band gap in graphene for its potential application in next-generation electronic devices. The primary challenge in modulating this gap has been the absence of a direct method for observing changes of the band gap in momentum space. In this study, we employ advanced spatial- and angle-resolved photoemission spectroscopy technique to directly visualize the gap formation in bilayer graphene, modulated by both displacement fields and moir\\'e potentials. The application of displacement field via in-situ electrostatic gating introduces a sizable and tunable electronic bandgap, proportional to the field strength up to 100 meV. Meanwhile, the moir\\'e potential, induced by aligning the underlying hexagonal boron nitride substrate, extends the bandgap by ~ 20 meV. Theoretical calculations, effectively capture the experimental observations. Our investigation provides a quantitative understanding of how these two mechanisms collaboratively modulate the band gap in bilayer graphene, offering valuable guidance for the design of graphene-based electronic devices.","sentences":["Over the years, great efforts have been devoted in introducing a sizable and tunable band gap in graphene for its potential application in next-generation electronic devices.","The primary challenge in modulating this gap has been the absence of a direct method for observing changes of the band gap in momentum space.","In this study, we employ advanced spatial- and angle-resolved photoemission spectroscopy technique to directly visualize the gap formation in bilayer graphene, modulated by both displacement fields and moir\\'e potentials.","The application of displacement field via in-situ electrostatic gating introduces a sizable and tunable electronic bandgap, proportional to the field strength up to 100 meV.","Meanwhile, the moir\\'e potential, induced by aligning the underlying hexagonal boron nitride substrate, extends the bandgap by ~ 20 meV. Theoretical calculations, effectively capture the experimental observations.","Our investigation provides a quantitative understanding of how these two mechanisms collaboratively modulate the band gap in bilayer graphene, offering valuable guidance for the design of graphene-based electronic devices."],"url":"http://arxiv.org/abs/2405.11893v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 09:15:36","title":"Unveiling and Manipulating Prompt Influence in Large Language Models","abstract":"Prompts play a crucial role in guiding the responses of Large Language Models (LLMs). However, the intricate role of individual tokens in prompts, known as input saliency, in shaping the responses remains largely underexplored. Existing saliency methods either misalign with LLM generation objectives or rely heavily on linearity assumptions, leading to potential inaccuracies. To address this, we propose Token Distribution Dynamics (TDD), a \\textcolor{black}{simple yet effective} approach to unveil and manipulate the role of prompts in generating LLM outputs. TDD leverages the robust interpreting capabilities of the language model head (LM head) to assess input saliency. It projects input tokens into the embedding space and then estimates their significance based on distribution dynamics over the vocabulary. We introduce three TDD variants: forward, backward, and bidirectional, each offering unique insights into token relevance. Extensive experiments reveal that the TDD surpasses state-of-the-art baselines with a big margin in elucidating the causal relationships between prompts and LLM outputs. Beyond mere interpretation, we apply TDD to two prompt manipulation tasks for controlled text generation: zero-shot toxic language suppression and sentiment steering. Empirical results underscore TDD's proficiency in identifying both toxic and sentimental cues in prompts, subsequently mitigating toxicity or modulating sentiment in the generated content.","sentences":["Prompts play a crucial role in guiding the responses of Large Language Models (LLMs).","However, the intricate role of individual tokens in prompts, known as input saliency, in shaping the responses remains largely underexplored.","Existing saliency methods either misalign with LLM generation objectives or rely heavily on linearity assumptions, leading to potential inaccuracies.","To address this, we propose Token Distribution Dynamics (TDD), a \\textcolor{black}{simple yet effective} approach to unveil and manipulate the role of prompts in generating LLM outputs.","TDD leverages the robust interpreting capabilities of the language model head (LM head) to assess input saliency.","It projects input tokens into the embedding space and then estimates their significance based on distribution dynamics over the vocabulary.","We introduce three TDD variants: forward, backward, and bidirectional, each offering unique insights into token relevance.","Extensive experiments reveal that the TDD surpasses state-of-the-art baselines with a big margin in elucidating the causal relationships between prompts and LLM outputs.","Beyond mere interpretation, we apply TDD to two prompt manipulation tasks for controlled text generation: zero-shot toxic language suppression and sentiment steering.","Empirical results underscore TDD's proficiency in identifying both toxic and sentimental cues in prompts, subsequently mitigating toxicity or modulating sentiment in the generated content."],"url":"http://arxiv.org/abs/2405.11891v1","category":"cs.CL"}
{"created":"2024-05-20 09:09:15","title":"Thermodynamic Circuits II: Nonequilibrium conductance matrix for a thermoelectric converter","abstract":"Starting from a linear flux-force relation of a thermoelectric material in local equilibrium, we derive non-linear relations between the currents and forces of a thermoelectric converter (TEC) driven far from equilibrium. Our investigation focuses on a one-dimensional TEC of finite thickness. Building on the achievements of the first paper of this series, we unveil the conservation laws governing physical currents (ie linearly dependent currents). Using the latter allows to put forward two relevant bases of physical and fundamental currents. For these bases, we introduce the concept of non equilibrium conductance matrix to describe the current-force relations of a TEC. This concept will be at the core of the third paper of this series in which we illustrate the serial and parallel association on composite TECs. Finally, we determine in a unified way the optimal working points of the TEC in its two operating modes: the electric generator and the heat pump.","sentences":["Starting from a linear flux-force relation of a thermoelectric material in local equilibrium, we derive non-linear relations between the currents and forces of a thermoelectric converter (TEC) driven far from equilibrium.","Our investigation focuses on a one-dimensional TEC of finite thickness.","Building on the achievements of the first paper of this series, we unveil the conservation laws governing physical currents (ie linearly dependent currents).","Using the latter allows to put forward two relevant bases of physical and fundamental currents.","For these bases, we introduce the concept of non equilibrium conductance matrix to describe the current-force relations of a TEC.","This concept will be at the core of the third paper of this series in which we illustrate the serial and parallel association on composite TECs.","Finally, we determine in a unified way the optimal working points of the TEC in its two operating modes: the electric generator and the heat pump."],"url":"http://arxiv.org/abs/2405.11886v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-20 08:54:03","title":"Out-of-Distribution Detection with a Single Unconditional Diffusion Model","abstract":"Out-of-distribution (OOD) detection is a critical task in machine learning that seeks to identify abnormal samples. Traditionally, unsupervised methods utilize a deep generative model for OOD detection. However, such approaches necessitate a different model when evaluating abnormality against a new distribution. With the emergence of foundational generative models, this paper explores whether a single generalist model can also perform OOD detection across diverse tasks. To that end, we introduce our method, Diffusion Paths, (DiffPath) in this work. DiffPath proposes to utilize a single diffusion model originally trained to perform unconditional generation for OOD detection. Specifically, we introduce a novel technique of measuring the rate-of-change and curvature of the diffusion paths connecting samples to the standard normal. Extensive experiments show that with a single model, DiffPath outperforms prior work on a variety of OOD tasks involving different distributions. Our code is publicly available at https://github.com/clear-nus/diffpath.","sentences":["Out-of-distribution (OOD) detection is a critical task in machine learning that seeks to identify abnormal samples.","Traditionally, unsupervised methods utilize a deep generative model for OOD detection.","However, such approaches necessitate a different model when evaluating abnormality against a new distribution.","With the emergence of foundational generative models, this paper explores whether a single generalist model can also perform OOD detection across diverse tasks.","To that end, we introduce our method, Diffusion Paths, (DiffPath) in this work.","DiffPath proposes to utilize a single diffusion model originally trained to perform unconditional generation for OOD detection.","Specifically, we introduce a novel technique of measuring the rate-of-change and curvature of the diffusion paths connecting samples to the standard normal.","Extensive experiments show that with a single model, DiffPath outperforms prior work on a variety of OOD tasks involving different distributions.","Our code is publicly available at https://github.com/clear-nus/diffpath."],"url":"http://arxiv.org/abs/2405.11881v1","category":"cs.LG"}
{"created":"2024-05-20 08:51:03","title":"Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs","abstract":"In this study, we propose an axiomatic system to define and quantify the precise memorization and in-context reasoning effects used by the large language model (LLM) for language generation. These effects are formulated as non-linear interactions between tokens/words encoded by the LLM. Specifically, the axiomatic system enables us to categorize the memorization effects into foundational memorization effects and chaotic memorization effects, and further classify in-context reasoning effects into enhanced inference patterns, eliminated inference patterns, and reversed inference patterns. Besides, the decomposed effects satisfy the sparsity property and the universal matching property, which mathematically guarantee that the LLM's confidence score can be faithfully decomposed into the memorization effects and in-context reasoning effects. Experiments show that the clear disentanglement of memorization effects and in-context reasoning effects enables a straightforward examination of detailed inference patterns encoded by LLMs.","sentences":["In this study, we propose an axiomatic system to define and quantify the precise memorization and in-context reasoning effects used by the large language model (LLM) for language generation.","These effects are formulated as non-linear interactions between tokens/words encoded by the LLM.","Specifically, the axiomatic system enables us to categorize the memorization effects into foundational memorization effects and chaotic memorization effects, and further classify in-context reasoning effects into enhanced inference patterns, eliminated inference patterns, and reversed inference patterns.","Besides, the decomposed effects satisfy the sparsity property and the universal matching property, which mathematically guarantee that the LLM's confidence score can be faithfully decomposed into the memorization effects and in-context reasoning effects.","Experiments show that the clear disentanglement of memorization effects and in-context reasoning effects enables a straightforward examination of detailed inference patterns encoded by LLMs."],"url":"http://arxiv.org/abs/2405.11880v1","category":"cs.LG"}
{"created":"2024-05-20 08:41:15","title":"A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus","abstract":"Natural language inference (NLI), the task of recognizing the entailment relationship in sentence pairs, is an actively studied topic serving as a proxy for natural language understanding. Despite the relevance of the task in building conversational agents and improving text classification, machine translation and other NLP tasks, to the best of our knowledge, there is no publicly available NLI corpus for the Romanian language. To this end, we introduce the first Romanian NLI corpus (RoNLI) comprising 58K training sentence pairs, which are obtained via distant supervision, and 6K validation and test sentence pairs, which are manually annotated with the correct labels. We conduct experiments with multiple machine learning methods based on distant learning, ranging from shallow models based on word embeddings to transformer-based neural networks, to establish a set of competitive baselines. Furthermore, we improve on the best model by employing a new curriculum learning strategy based on data cartography. Our dataset and code to reproduce the baselines are available https://github.com/Eduard6421/RONLI.","sentences":["Natural language inference (NLI), the task of recognizing the entailment relationship in sentence pairs, is an actively studied topic serving as a proxy for natural language understanding.","Despite the relevance of the task in building conversational agents and improving text classification, machine translation and other NLP tasks, to the best of our knowledge, there is no publicly available NLI corpus for the Romanian language.","To this end, we introduce the first Romanian NLI corpus (RoNLI) comprising 58K training sentence pairs, which are obtained via distant supervision, and 6K validation and test sentence pairs, which are manually annotated with the correct labels.","We conduct experiments with multiple machine learning methods based on distant learning, ranging from shallow models based on word embeddings to transformer-based neural networks, to establish a set of competitive baselines.","Furthermore, we improve on the best model by employing a new curriculum learning strategy based on data cartography.","Our dataset and code to reproduce the baselines are available https://github.com/Eduard6421/RONLI."],"url":"http://arxiv.org/abs/2405.11877v1","category":"cs.CL"}
{"created":"2024-05-20 08:30:13","title":"xFinder: Robust and Pinpoint Answer Extraction for Large Language Models","abstract":"The continuous advancement of large language models (LLMs) has brought increasing attention to the critical issue of developing fair and reliable methods for evaluating their performance. Particularly, the emergence of subjective or non-subjective cheating phenomena, such as test set leakage and prompt format overfitting, poses significant challenges to the reliable evaluation of LLMs. Since evaluation frameworks often utilize Regular Expression (RegEx) for answer extraction, some models may adjust their responses to comply with specific formats that are easily extractable by RegEx. Nevertheless, the key answer extraction module based on RegEx frequently suffers from extraction errors. This paper conducts a comprehensive analysis of the entire LLM evaluation chain, demonstrating that optimizing the key answer extraction module can improve extraction accuracy, reduce LLMs' reliance on specific answer formats, and enhance the reliability of LLM evaluation. To address these issues, we propose xFinder, a model specifically designed for key answer extraction. As part of this process, we create a specialized dataset, the Key Answer Finder (KAF) dataset, to ensure effective model training and evaluation. Through generalization testing and evaluation in real-world scenarios, the results demonstrate that the smallest xFinder model with only 500 million parameters achieves an average answer extraction accuracy of 93.42%. In contrast, RegEx accuracy in the best evaluation framework is 74.38%. xFinder exhibits stronger robustness and higher accuracy compared to existing evaluation frameworks. All resources for xFinder are available at \\url{https://github.com/IAAR-Shanghai/xFinder}.","sentences":["The continuous advancement of large language models (LLMs) has brought increasing attention to the critical issue of developing fair and reliable methods for evaluating their performance.","Particularly, the emergence of subjective or non-subjective cheating phenomena, such as test set leakage and prompt format overfitting, poses significant challenges to the reliable evaluation of LLMs.","Since evaluation frameworks often utilize Regular Expression (RegEx) for answer extraction, some models may adjust their responses to comply with specific formats that are easily extractable by RegEx.","Nevertheless, the key answer extraction module based on RegEx frequently suffers from extraction errors.","This paper conducts a comprehensive analysis of the entire LLM evaluation chain, demonstrating that optimizing the key answer extraction module can improve extraction accuracy, reduce LLMs' reliance on specific answer formats, and enhance the reliability of LLM evaluation.","To address these issues, we propose xFinder, a model specifically designed for key answer extraction.","As part of this process, we create a specialized dataset, the Key Answer Finder (KAF) dataset, to ensure effective model training and evaluation.","Through generalization testing and evaluation in real-world scenarios, the results demonstrate that the smallest xFinder model with only 500 million parameters achieves an average answer extraction accuracy of 93.42%.","In contrast, RegEx accuracy in the best evaluation framework is 74.38%.","xFinder exhibits stronger robustness and higher accuracy compared to existing evaluation frameworks.","All resources for xFinder are available at \\url{https://github.com/IAAR-Shanghai/xFinder}."],"url":"http://arxiv.org/abs/2405.11874v1","category":"cs.CL"}
{"created":"2024-05-20 08:27:15","title":"Open Quantum Dynamics: Memory Effects and Superactivation of Backflow of Information","abstract":"We investigate the divisibility properties of the tensor products $\\Lambda^{(1)}_t\\otimes\\Lambda^{(2)}_t$ of open quantum dynamics $\\Lambda^{(1,2)}_t$ with time-dependent generators. These dynamical maps emerge from a compound open system $S_1+S_2$ that interacts with its own environment in such a way that memory effects remain when the environment is traced away. This study is motivated by the following intriguing effect: one can have Backflow of Information (BFI) from the environment to $S_1+S_2$ without the same phenomenon occurring for either $S_1$ and $S_2$. We shall refer to this effect as the Superactivation of BFI (SBFI).","sentences":["We investigate the divisibility properties of the tensor products $\\Lambda^{(1)}_t\\otimes\\Lambda^{(2)}_t$ of open quantum dynamics $\\Lambda^{(1,2)}_t$ with time-dependent generators.","These dynamical maps emerge from a compound open system $S_1+S_2$ that interacts with its own environment in such a way that memory effects remain when the environment is traced away.","This study is motivated by the following intriguing effect: one can have Backflow of Information (BFI) from the environment to $S_1+S_2$ without the same phenomenon occurring for either $S_1$ and $S_2$. We shall refer to this effect as the Superactivation of BFI (SBFI)."],"url":"http://arxiv.org/abs/2405.11872v1","category":"quant-ph"}
{"created":"2024-05-20 08:23:28","title":"Intuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process","abstract":"Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) are two fundamental processes for enhancing the capabilities of Language Models (LMs) post pre-training, aligning them better with human preferences. Although SFT advances in training efficiency, RLHF delivers better alignment, thus they are often combined. However, common practices simply apply them sequentially without unifying their optimization targets, resulting in a trade-off between fitting different objectives, and ignoring the opportunities to bridge the paradigm gap and take the strength from both. To obtain a unified understanding, we interpret SFT and RLHF using two sub-processes -- Preference Estimation and Transition Optimization -- defined at token level within the Markov Decision Process (MDP) framework. This modeling shows that SFT is only a specialized case of RLHF with inferior estimation and optimization. RLHF evaluates the quality of model's entire generated answer, whereas SFT only scores predicted tokens based on preceding tokens from target answers. Therefore, SFT overestimates the ability of model, leading to inferior optimization. Building on this view, we introduce Intuitive Fine-tuning (IFT) to integrate SFT and RLHF into a single process. IFT captures LMs' intuitive sense of the entire answers through a temporal residual connection, while using a single policy and the same volume of non-preference-labeled data as SFT. Our experiments show that IFT performs comparably or even superiorly to sequential recipes of SFT and some typical alignment methods across several tasks, particularly those requires generation, reasoning, and fact-following abilities. An explainable Frozen Lake game further validates the effectiveness of IFT.","sentences":["Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) are two fundamental processes for enhancing the capabilities of Language Models (LMs) post pre-training, aligning them better with human preferences.","Although SFT advances in training efficiency, RLHF delivers better alignment, thus they are often combined.","However, common practices simply apply them sequentially without unifying their optimization targets, resulting in a trade-off between fitting different objectives, and ignoring the opportunities to bridge the paradigm gap and take the strength from both.","To obtain a unified understanding, we interpret SFT and RLHF using two sub-processes -- Preference Estimation and Transition Optimization -- defined at token level within the Markov Decision Process (MDP) framework.","This modeling shows that SFT is only a specialized case of RLHF with inferior estimation and optimization.","RLHF evaluates the quality of model's entire generated answer, whereas SFT only scores predicted tokens based on preceding tokens from target answers.","Therefore, SFT overestimates the ability of model, leading to inferior optimization.","Building on this view, we introduce Intuitive Fine-tuning (IFT) to integrate SFT and RLHF into a single process.","IFT captures LMs' intuitive sense of the entire answers through a temporal residual connection, while using a single policy and the same volume of non-preference-labeled data as SFT.","Our experiments show that IFT performs comparably or even superiorly to sequential recipes of SFT and some typical alignment methods across several tasks, particularly those requires generation, reasoning, and fact-following abilities.","An explainable Frozen Lake game further validates the effectiveness of IFT."],"url":"http://arxiv.org/abs/2405.11870v1","category":"cs.CL"}
{"created":"2024-05-20 08:19:10","title":"Towards Graph Contrastive Learning: A Survey and Beyond","abstract":"In recent years, deep learning on graphs has achieved remarkable success in various domains. However, the reliance on annotated graph data remains a significant bottleneck due to its prohibitive cost and time-intensive nature. To address this challenge, self-supervised learning (SSL) on graphs has gained increasing attention and has made significant progress. SSL enables machine learning models to produce informative representations from unlabeled graph data, reducing the reliance on expensive labeled data. While SSL on graphs has witnessed widespread adoption, one critical component, Graph Contrastive Learning (GCL), has not been thoroughly investigated in the existing literature. Thus, this survey aims to fill this gap by offering a dedicated survey on GCL. We provide a comprehensive overview of the fundamental principles of GCL, including data augmentation strategies, contrastive modes, and contrastive optimization objectives. Furthermore, we explore the extensions of GCL to other aspects of data-efficient graph learning, such as weakly supervised learning, transfer learning, and related scenarios. We also discuss practical applications spanning domains such as drug discovery, genomics analysis, recommender systems, and finally outline the challenges and potential future directions in this field.","sentences":["In recent years, deep learning on graphs has achieved remarkable success in various domains.","However, the reliance on annotated graph data remains a significant bottleneck due to its prohibitive cost and time-intensive nature.","To address this challenge, self-supervised learning (SSL) on graphs has gained increasing attention and has made significant progress.","SSL enables machine learning models to produce informative representations from unlabeled graph data, reducing the reliance on expensive labeled data.","While SSL on graphs has witnessed widespread adoption, one critical component, Graph Contrastive Learning (GCL), has not been thoroughly investigated in the existing literature.","Thus, this survey aims to fill this gap by offering a dedicated survey on GCL.","We provide a comprehensive overview of the fundamental principles of GCL, including data augmentation strategies, contrastive modes, and contrastive optimization objectives.","Furthermore, we explore the extensions of GCL to other aspects of data-efficient graph learning, such as weakly supervised learning, transfer learning, and related scenarios.","We also discuss practical applications spanning domains such as drug discovery, genomics analysis, recommender systems, and finally outline the challenges and potential future directions in this field."],"url":"http://arxiv.org/abs/2405.11868v1","category":"cs.LG"}
{"created":"2024-05-20 08:19:08","title":"Depth Prompting for Sensor-Agnostic Depth Estimation","abstract":"Dense depth maps have been used as a key element of visual perception tasks. There have been tremendous efforts to enhance the depth quality, ranging from optimization-based to learning-based methods. Despite the remarkable progress for a long time, their applicability in the real world is limited due to systematic measurement biases such as density, sensing pattern, and scan range. It is well-known that the biases make it difficult for these methods to achieve their generalization. We observe that learning a joint representation for input modalities (e.g., images and depth), which most recent methods adopt, is sensitive to the biases. In this work, we disentangle those modalities to mitigate the biases with prompt engineering. For this, we design a novel depth prompt module to allow the desirable feature representation according to new depth distributions from either sensor types or scene configurations. Our depth prompt can be embedded into foundation models for monocular depth estimation. Through this embedding process, our method helps the pretrained model to be free from restraint of depth scan range and to provide absolute scale depth maps. We demonstrate the effectiveness of our method through extensive evaluations. Source code is publicly available at https://github.com/JinhwiPark/DepthPrompting .","sentences":["Dense depth maps have been used as a key element of visual perception tasks.","There have been tremendous efforts to enhance the depth quality, ranging from optimization-based to learning-based methods.","Despite the remarkable progress for a long time, their applicability in the real world is limited due to systematic measurement biases such as density, sensing pattern, and scan range.","It is well-known that the biases make it difficult for these methods to achieve their generalization.","We observe that learning a joint representation for input modalities (e.g., images and depth), which most recent methods adopt, is sensitive to the biases.","In this work, we disentangle those modalities to mitigate the biases with prompt engineering.","For this, we design a novel depth prompt module to allow the desirable feature representation according to new depth distributions from either sensor types or scene configurations.","Our depth prompt can be embedded into foundation models for monocular depth estimation.","Through this embedding process, our method helps the pretrained model to be free from restraint of depth scan range and to provide absolute scale depth maps.","We demonstrate the effectiveness of our method through extensive evaluations.","Source code is publicly available at https://github.com/JinhwiPark/DepthPrompting ."],"url":"http://arxiv.org/abs/2405.11867v1","category":"cs.CV"}
{"created":"2024-05-20 08:16:34","title":"CoNLL#: Fine-grained Error Analysis and a Corrected Test Set for CoNLL-03 English","abstract":"Modern named entity recognition systems have steadily improved performance in the age of larger and more powerful neural models. However, over the past several years, the state-of-the-art has seemingly hit another plateau on the benchmark CoNLL-03 English dataset. In this paper, we perform a deep dive into the test outputs of the highest-performing NER models, conducting a fine-grained evaluation of their performance by introducing new document-level annotations on the test set. We go beyond F1 scores by categorizing errors in order to interpret the true state of the art for NER and guide future work. We review previous attempts at correcting the various flaws of the test set and introduce CoNLL#, a new corrected version of the test set that addresses its systematic and most prevalent errors, allowing for low-noise, interpretable error analysis.","sentences":["Modern named entity recognition systems have steadily improved performance in the age of larger and more powerful neural models.","However, over the past several years, the state-of-the-art has seemingly hit another plateau on the benchmark CoNLL-03 English dataset.","In this paper, we perform a deep dive into the test outputs of the highest-performing NER models, conducting a fine-grained evaluation of their performance by introducing new document-level annotations on the test set.","We go beyond F1 scores by categorizing errors in order to interpret the true state of the art for NER and guide future work.","We review previous attempts at correcting the various flaws of the test set and introduce CoNLL#, a new corrected version of the test set that addresses its systematic and most prevalent errors, allowing for low-noise, interpretable error analysis."],"url":"http://arxiv.org/abs/2405.11865v1","category":"cs.CL"}
{"created":"2024-05-20 08:03:27","title":"Highly versatile, two-color setup for high-order harmonic generation using spatial light modulators","abstract":"We present a novel, interferometric, two-color, high-order harmonic generation setup, based on a turn-key Ytterbium-doped femtosecond laser source and its second harmonic. Each interferometer arm contains a spatial light modulator, with individual capabilities to manipulate the spatial beam profiles and to stabilize the relative delay between the fundamental and the second harmonic. Additionally, separate control of the relative power and focusing geometries of the two color beams is implemented to conveniently perform automatized scans of multiple parameters. A live diagnostics system gives continuous information during ongoing measurements.","sentences":["We present a novel, interferometric, two-color, high-order harmonic generation setup, based on a turn-key Ytterbium-doped femtosecond laser source and its second harmonic.","Each interferometer arm contains a spatial light modulator, with individual capabilities to manipulate the spatial beam profiles and to stabilize the relative delay between the fundamental and the second harmonic.","Additionally, separate control of the relative power and focusing geometries of the two color beams is implemented to conveniently perform automatized scans of multiple parameters.","A live diagnostics system gives continuous information during ongoing measurements."],"url":"http://arxiv.org/abs/2405.11859v1","category":"physics.optics"}
{"created":"2024-05-20 07:54:03","title":"Evolving Storytelling: Benchmarks and Methods for New Character Customization with Diffusion Models","abstract":"Diffusion-based models for story visualization have shown promise in generating content-coherent images for storytelling tasks. However, how to effectively integrate new characters into existing narratives while maintaining character consistency remains an open problem, particularly with limited data. Two major limitations hinder the progress: (1) the absence of a suitable benchmark due to potential character leakage and inconsistent text labeling, and (2) the challenge of distinguishing between new and old characters, leading to ambiguous results. To address these challenges, we introduce the NewEpisode benchmark, comprising refined datasets designed to evaluate generative models' adaptability in generating new stories with fresh characters using just a single example story. The refined dataset involves refined text prompts and eliminates character leakage. Additionally, to mitigate the character confusion of generated results, we propose EpicEvo, a method that customizes a diffusion-based visual story generation model with a single story featuring the new characters seamlessly integrating them into established character dynamics. EpicEvo introduces a novel adversarial character alignment module to align the generated images progressively in the diffusive process, with exemplar images of new characters, while applying knowledge distillation to prevent forgetting of characters and background details. Our evaluation quantitatively demonstrates that EpicEvo outperforms existing baselines on the NewEpisode benchmark, and qualitative studies confirm its superior customization of visual story generation in diffusion models. In summary, EpicEvo provides an effective way to incorporate new characters using only one example story, unlocking new possibilities for applications such as serialized cartoons.","sentences":["Diffusion-based models for story visualization have shown promise in generating content-coherent images for storytelling tasks.","However, how to effectively integrate new characters into existing narratives while maintaining character consistency remains an open problem, particularly with limited data.","Two major limitations hinder the progress: (1) the absence of a suitable benchmark due to potential character leakage and inconsistent text labeling, and (2) the challenge of distinguishing between new and old characters, leading to ambiguous results.","To address these challenges, we introduce the NewEpisode benchmark, comprising refined datasets designed to evaluate generative models' adaptability in generating new stories with fresh characters using just a single example story.","The refined dataset involves refined text prompts and eliminates character leakage.","Additionally, to mitigate the character confusion of generated results, we propose EpicEvo, a method that customizes a diffusion-based visual story generation model with a single story featuring the new characters seamlessly integrating them into established character dynamics.","EpicEvo introduces a novel adversarial character alignment module to align the generated images progressively in the diffusive process, with exemplar images of new characters, while applying knowledge distillation to prevent forgetting of characters and background details.","Our evaluation quantitatively demonstrates that EpicEvo outperforms existing baselines on the NewEpisode benchmark, and qualitative studies confirm its superior customization of visual story generation in diffusion models.","In summary, EpicEvo provides an effective way to incorporate new characters using only one example story, unlocking new possibilities for applications such as serialized cartoons."],"url":"http://arxiv.org/abs/2405.11852v1","category":"cs.CV"}
{"created":"2024-05-20 07:53:58","title":"Lower classes and Chung's LILs of the fractional integrated generalized fractional Brownian motion","abstract":"Let $\\{X(t)\\}_{t\\geqslant0}$ be the generalized fractional Brownian motion introduced by Pang and Taqqu (2019):   \\begin{align*}   \\{X(t)\\}_{t\\ge0}\\overset{d}{=}&\\left\\{ \\int_{\\mathbb R} \\left((t-u)_+^{\\alpha}-(-u)_+^{\\alpha} \\right) |u|^{-\\gamma/2}   B(du) \\right\\}_{t\\ge0},   \\end{align*} where $ \\gamma\\in [0,1), \\ \\ \\alpha\\in \\left(-\\frac12+\\frac{\\gamma}{2}, \\ \\frac12+\\frac{\\gamma}{2} \\right)$ are constants. For any $\\theta>0$, let \\begin{align*}   Y(t)=\\frac{1}{\\Gamma(\\theta)}\\int_0^t (t-u)^{\\theta-1} X(u)du, \\quad t\\ge 0. \\end{align*} Building upon the arguments of Talagrand (1996), we give integral criteria for the lower classes of $Y$ at $t=0$ and at infinity, respectively. As a consequence, we derive its Chung-type laws of the iterated logarithm. In the proofs, the small ball probability estimates play important roles.","sentences":["Let $\\{X(t)\\}_{t\\geqslant0}$ be the generalized fractional Brownian motion introduced by Pang and Taqqu (2019):   \\begin{align*}   \\{X(t)\\}_{t\\ge0}\\overset{d}{=}&\\left\\{ \\int_{\\mathbb R} \\left((t-u)_+^{\\alpha}-(-u)_+^{\\alpha} \\right) |u|^{-\\gamma/2}   B(du) \\right\\}_{t\\ge0},   \\end{align*} where $ \\gamma\\in [0,1), \\ \\ \\alpha\\in \\left(-\\frac12+\\frac{\\gamma}{2}, \\ \\frac12+\\frac{\\gamma}{2} \\right)$ are constants.","For any $\\theta>0$, let \\begin{align*}   Y(t)=\\frac{1}{\\Gamma(\\theta)}\\int_0^t","(t-u)^{\\theta-1} X(u)du, \\quad t\\ge 0.","\\end{align*} Building upon the arguments of Talagrand (1996), we give integral criteria for the lower classes of $Y$ at $t=0$ and at infinity, respectively.","As a consequence, we derive its Chung-type laws of the iterated logarithm.","In the proofs, the small ball probability estimates play important roles."],"url":"http://arxiv.org/abs/2405.11851v1","category":"math.PR"}
{"created":"2024-05-20 07:47:06","title":"Alternators For Sequence Modeling","abstract":"This paper introduces alternators, a novel family of non-Markovian dynamical models for sequences. An alternator features two neural networks: the observation trajectory network (OTN) and the feature trajectory network (FTN). The OTN and the FTN work in conjunction, alternating between outputting samples in the observation space and some feature space, respectively, over a cycle. The parameters of the OTN and the FTN are not time-dependent and are learned via a minimum cross-entropy criterion over the trajectories. Alternators are versatile. They can be used as dynamical latent-variable generative models or as sequence-to-sequence predictors. When alternators are used as generative models, the FTN produces interpretable low-dimensional latent variables that capture the dynamics governing the observations. When alternators are used as sequence-to-sequence predictors, the FTN learns to predict the observed features. In both cases, the OTN learns to produce sequences that match the data. Alternators can uncover the latent dynamics underlying complex sequential data, accurately forecast and impute missing data, and sample new trajectories. We showcase the capabilities of alternators in three applications. We first used alternators to model the Lorenz equations, often used to describe chaotic behavior. We then applied alternators to Neuroscience, to map brain activity to physical activity. Finally, we applied alternators to Climate Science, focusing on sea-surface temperature forecasting. In all our experiments, we found alternators are stable to train, fast to sample from, yield high-quality generated samples and latent variables, and outperform strong baselines such as neural ODEs and diffusion models in the domains we studied.","sentences":["This paper introduces alternators, a novel family of non-Markovian dynamical models for sequences.","An alternator features two neural networks: the observation trajectory network (OTN) and the feature trajectory network (FTN).","The OTN and the FTN work in conjunction, alternating between outputting samples in the observation space and some feature space, respectively, over a cycle.","The parameters of the OTN and the FTN are not time-dependent and are learned via a minimum cross-entropy criterion over the trajectories.","Alternators are versatile.","They can be used as dynamical latent-variable generative models or as sequence-to-sequence predictors.","When alternators are used as generative models, the FTN produces interpretable low-dimensional latent variables that capture the dynamics governing the observations.","When alternators are used as sequence-to-sequence predictors, the FTN learns to predict the observed features.","In both cases, the OTN learns to produce sequences that match the data.","Alternators can uncover the latent dynamics underlying complex sequential data, accurately forecast and impute missing data, and sample new trajectories.","We showcase the capabilities of alternators in three applications.","We first used alternators to model the Lorenz equations, often used to describe chaotic behavior.","We then applied alternators to Neuroscience, to map brain activity to physical activity.","Finally, we applied alternators to Climate Science, focusing on sea-surface temperature forecasting.","In all our experiments, we found alternators are stable to train, fast to sample from, yield high-quality generated samples and latent variables, and outperform strong baselines such as neural ODEs and diffusion models in the domains we studied."],"url":"http://arxiv.org/abs/2405.11848v1","category":"stat.ML"}
{"created":"2024-05-20 07:38:19","title":"NeRTCAM: CAM-Based CMOS Implementation of Reference Frames for Neuromorphic Processors","abstract":"Neuromorphic architectures mimicking biological neural networks have been proposed as a much more efficient alternative to conventional von Neumann architectures for the exploding compute demands of AI workloads. Recent neuroscience theory on intelligence suggests that Cortical Columns (CCs) are the fundamental compute units in the neocortex and intelligence arises from CC's ability to store, predict and infer information via structured Reference Frames (RFs). Based on this theory, recent works have demonstrated brain-like visual object recognition using software simulation. Our work is the first attempt towards direct CMOS implementation of Reference Frames for building CC-based neuromorphic processors. We propose NeRTCAM (Neuromorphic Reverse Ternary Content Addressable Memory), a CAM-based building block that supports the key operations (store, predict, infer) required to perform inference using RFs. NeRTCAM architecture is presented in detail including its key components. All designs are implemented in SystemVerilog and synthesized in 7nm CMOS, and hardware complexity scaling is evaluated for varying storage sizes. NeRTCAM system for biologically motivated MNIST inference with a storage size of 1024 entries incurs just 0.15 mm^2 area, 400 mW power and 9.18 us critical path latency, demonstrating the feasibility of direct CMOS implementation of CAM-based Reference Frames.","sentences":["Neuromorphic architectures mimicking biological neural networks have been proposed as a much more efficient alternative to conventional von Neumann architectures for the exploding compute demands of AI workloads.","Recent neuroscience theory on intelligence suggests that Cortical Columns (CCs) are the fundamental compute units in the neocortex and intelligence arises from CC's ability to store, predict and infer information via structured Reference Frames (RFs).","Based on this theory, recent works have demonstrated brain-like visual object recognition using software simulation.","Our work is the first attempt towards direct CMOS implementation of Reference Frames for building CC-based neuromorphic processors.","We propose NeRTCAM (Neuromorphic Reverse Ternary Content Addressable Memory), a CAM-based building block that supports the key operations (store, predict, infer) required to perform inference using RFs.","NeRTCAM architecture is presented in detail including its key components.","All designs are implemented in SystemVerilog and synthesized in 7nm CMOS, and hardware complexity scaling is evaluated for varying storage sizes.","NeRTCAM system for biologically motivated MNIST inference with a storage size of 1024 entries incurs just 0.15 mm^2 area, 400 mW power and 9.18 us critical path latency, demonstrating the feasibility of direct CMOS implementation of CAM-based Reference Frames."],"url":"http://arxiv.org/abs/2405.11844v1","category":"cs.AR"}
{"created":"2024-05-20 07:34:48","title":"Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities","abstract":"Facing the current debate on whether Large Language Models (LLMs) attain near-human intelligence levels (Mitchell & Krakauer, 2023; Bubeck et al., 2023; Kosinski, 2023; Shiffrin & Mitchell, 2023; Ullman, 2023), the current study introduces a benchmark for evaluating social intelligence, one of the most distinctive aspects of human cognition. We developed a comprehensive theoretical framework for social dynamics and introduced two evaluation tasks: Inverse Reasoning (IR) and Inverse Inverse Planning (IIP). Our approach also encompassed a computational model based on recursive Bayesian inference, adept at elucidating diverse human behavioral patterns. Extensive experiments and detailed analyses revealed that humans surpassed the latest GPT models in overall performance, zero-shot learning, one-shot generalization, and adaptability to multi-modalities. Notably, GPT models demonstrated social intelligence only at the most basic order (order = 0), in stark contrast to human social intelligence (order >= 2). Further examination indicated a propensity of LLMs to rely on pattern recognition for shortcuts, casting doubt on their possession of authentic human-level social intelligence. Our codes, dataset, appendix and human data are released at https://github.com/bigai-ai/Evaluate-n-Model-Social-Intelligence.","sentences":["Facing the current debate on whether Large Language Models (LLMs) attain near-human intelligence levels (Mitchell & Krakauer, 2023; Bubeck et al., 2023; Kosinski, 2023; Shiffrin & Mitchell, 2023; Ullman, 2023), the current study introduces a benchmark for evaluating social intelligence, one of the most distinctive aspects of human cognition.","We developed a comprehensive theoretical framework for social dynamics and introduced two evaluation tasks: Inverse Reasoning (IR) and Inverse Inverse Planning (IIP).","Our approach also encompassed a computational model based on recursive Bayesian inference, adept at elucidating diverse human behavioral patterns.","Extensive experiments and detailed analyses revealed that humans surpassed the latest GPT models in overall performance, zero-shot learning, one-shot generalization, and adaptability to multi-modalities.","Notably, GPT models demonstrated social intelligence only at the most basic order (order = 0), in stark contrast to human social intelligence (order >= 2).","Further examination indicated a propensity of LLMs to rely on pattern recognition for shortcuts, casting doubt on their possession of authentic human-level social intelligence.","Our codes, dataset, appendix and human data are released at https://github.com/bigai-ai/Evaluate-n-Model-Social-Intelligence."],"url":"http://arxiv.org/abs/2405.11841v1","category":"cs.AI"}
{"created":"2024-05-20 07:33:07","title":"Growth of cosmic perturbations in the modified $f(R,T)$ gravity","abstract":"We explore the generalized $f(R,T)$ modified theory of gravity, where the gravitational Lagrangian is a function of Ricci scalar $R$ and the trace of the energy-momentum tensor $T$. We derive modified field equations to the linear order of perturbations in the context of $f(R,T)$ model. We then investigate the growth of perturbations in the context of $f(R,T)$ modified gravity. Primary numerical investigations based on matter power spectra diagrams indicates a structure growth suppression in $f(R,T)$ gravity, which exhibits consistency with local measurements. Also, we notice that matter-geometry interaction in $f(R,T)$ model would results in the specific feature named as \"matter acoustic oscillations\" appeared in matter power spectra diagrams. Moreover, we put constraints on the cosmological parameters of $f(R,T)$ model, utilizing current observations, chiefly cosmic microwave background (CMB), weak lensing, supernovae, baryon acoustic oscillations (BAO), and redshift-space distortions (RSD) data. Numerical results based on MCMC calculations imply that $f(R,T)$ is a qualified theory of modified gravity in reconciling Planck CMB data with local probes of large scale structures, by reporting lower values for the structure growth parameter $\\sigma_8$ compared to the standard model of cosmology.","sentences":["We explore the generalized $f(R,T)$ modified theory of gravity, where the gravitational Lagrangian is a function of Ricci scalar $R$ and the trace of the energy-momentum tensor $T$. We derive modified field equations to the linear order of perturbations in the context of $f(R,T)$ model.","We then investigate the growth of perturbations in the context of $f(R,T)$ modified gravity.","Primary numerical investigations based on matter power spectra diagrams indicates a structure growth suppression in $f(R,T)$ gravity, which exhibits consistency with local measurements.","Also, we notice that matter-geometry interaction in $f(R,T)$ model would results in the specific feature named as \"matter acoustic oscillations\" appeared in matter power spectra diagrams.","Moreover, we put constraints on the cosmological parameters of $f(R,T)$ model, utilizing current observations, chiefly cosmic microwave background (CMB), weak lensing, supernovae, baryon acoustic oscillations (BAO), and redshift-space distortions (RSD) data.","Numerical results based on MCMC calculations imply that $f(R,T)$ is a qualified theory of modified gravity in reconciling Planck CMB data with local probes of large scale structures, by reporting lower values for the structure growth parameter $\\sigma_8$ compared to the standard model of cosmology."],"url":"http://arxiv.org/abs/2405.11840v1","category":"gr-qc"}
{"created":"2024-05-20 07:25:09","title":"Improving the Explain-Any-Concept by Introducing Nonlinearity to the Trainable Surrogate Model","abstract":"In the evolving field of Explainable AI (XAI), interpreting the decisions of deep neural networks (DNNs) in computer vision tasks is an important process. While pixel-based XAI methods focus on identifying significant pixels, existing concept-based XAI methods use pre-defined or human-annotated concepts. The recently proposed Segment Anything Model (SAM) achieved a significant step forward to prepare automatic concept sets via comprehensive instance segmentation. Building upon this, the Explain Any Concept (EAC) model emerged as a flexible method for explaining DNN decisions. EAC model is based on using a surrogate model which has one trainable linear layer to simulate the target model. In this paper, by introducing an additional nonlinear layer to the original surrogate model, we show that we can improve the performance of the EAC model. We compare our proposed approach to the original EAC model and report improvements obtained on both ImageNet and MS COCO datasets.","sentences":["In the evolving field of Explainable AI (XAI), interpreting the decisions of deep neural networks (DNNs) in computer vision tasks is an important process.","While pixel-based XAI methods focus on identifying significant pixels, existing concept-based XAI methods use pre-defined or human-annotated concepts.","The recently proposed Segment Anything Model (SAM) achieved a significant step forward to prepare automatic concept sets via comprehensive instance segmentation.","Building upon this, the Explain Any Concept (EAC) model emerged as a flexible method for explaining DNN decisions.","EAC model is based on using a surrogate model which has one trainable linear layer to simulate the target model.","In this paper, by introducing an additional nonlinear layer to the original surrogate model, we show that we can improve the performance of the EAC model.","We compare our proposed approach to the original EAC model and report improvements obtained on both ImageNet and MS COCO datasets."],"url":"http://arxiv.org/abs/2405.11837v1","category":"cs.CV"}
{"created":"2024-05-20 07:14:22","title":"Demo Paper: A Game Agents Battle Driven by Free-Form Text Commands Using Code-Generation LLM","abstract":"This paper presents a demonstration of our monster battle game, in which the game agents fight in accordance with their player's language commands. The commands were translated into the knowledge expression called behavior branches by a code-generation large language model. This work facilitated the design of the commanding system more easily, enabling the game agent to comprehend more various and continuous commands than rule-based methods. The results of the commanding and translation process were stored in a database on an Amazon Web Services server for more comprehensive validation. This implementation would provide a sufficient evaluation of this ongoing work, and give insights to the industry that they could use this to develop their interactive game agents.","sentences":["This paper presents a demonstration of our monster battle game, in which the game agents fight in accordance with their player's language commands.","The commands were translated into the knowledge expression called behavior branches by a code-generation large language model.","This work facilitated the design of the commanding system more easily, enabling the game agent to comprehend more various and continuous commands than rule-based methods.","The results of the commanding and translation process were stored in a database on an Amazon Web Services server for more comprehensive validation.","This implementation would provide a sufficient evaluation of this ongoing work, and give insights to the industry that they could use this to develop their interactive game agents."],"url":"http://arxiv.org/abs/2405.11835v1","category":"cs.HC"}
{"created":"2024-05-20 07:11:53","title":"Modified Greenwood statistic and its application for statistical testing","abstract":"In this paper, we explore the modified Greenwood statistic, which, in contrast to the classical Greenwood statistic, is properly defined for random samples from any distribution. The classical Greenwood statistic, extensively examined in the existing literature, has found diverse and interesting applications across various domains. Furthermore, numerous modifications to the classical statistic have been proposed. The modified Greenwood statistic, as proposed and discussed in this paper, shares several key properties with its classical counterpart. Emphasizing its stochastic monotonicity within three broad classes of distributions - namely, generalized Pareto, $\\alpha-$stable, and Student's t distributions - we advocate for the utilization of the modified Greenwood statistic in testing scenarios. Our exploration encompasses three distinct directions. In the first direction, we employ the modified Greenwood statistic for Gaussian distribution testing. Our empirical results compellingly illustrate that the proposed approach consistently outperforms alternative goodness-of-fit tests documented in the literature, particularly exhibiting superior efficacy for small sample sizes. The second considered problem involves testing the infinite-variance distribution of a given random sample. The last proposition suggests using the modified Greenwood statistic for testing of a given distribution. The presented simulation study strongly supports the efficiency of the proposed approach in the considered problems. Theoretical results and power simulation studies are further validated by real data analysis.","sentences":["In this paper, we explore the modified Greenwood statistic, which, in contrast to the classical Greenwood statistic, is properly defined for random samples from any distribution.","The classical Greenwood statistic, extensively examined in the existing literature, has found diverse and interesting applications across various domains.","Furthermore, numerous modifications to the classical statistic have been proposed.","The modified Greenwood statistic, as proposed and discussed in this paper, shares several key properties with its classical counterpart.","Emphasizing its stochastic monotonicity within three broad classes of distributions - namely, generalized Pareto, $\\alpha-$stable, and Student's t distributions - we advocate for the utilization of the modified Greenwood statistic in testing scenarios.","Our exploration encompasses three distinct directions.","In the first direction, we employ the modified Greenwood statistic for Gaussian distribution testing.","Our empirical results compellingly illustrate that the proposed approach consistently outperforms alternative goodness-of-fit tests documented in the literature, particularly exhibiting superior efficacy for small sample sizes.","The second considered problem involves testing the infinite-variance distribution of a given random sample.","The last proposition suggests using the modified Greenwood statistic for testing of a given distribution.","The presented simulation study strongly supports the efficiency of the proposed approach in the considered problems.","Theoretical results and power simulation studies are further validated by real data analysis."],"url":"http://arxiv.org/abs/2405.11834v1","category":"math.ST"}
{"created":"2024-05-20 07:09:34","title":"Braiding Topology of Non-Hermitian Open-Boundary Bands","abstract":"There has been much recent interest and progress on topological structures of the non-Hermitian Bloch bands. Here, we study the topological structures of non-Bloch bands of non-Hermitian multi-band quantum systems under open boundary conditions. Using a continuity criterion and an efficient sub-GBZ algorithm, we establish a homotopic characterization -- braiding topology, e.g., characterized by the band's total vorticity -- for open-boundary bands and sub-GBZs. Such topological identification is robust without topological transition and emergent degenerate points, such as exceptional points. We further analyze the transition's impact on bands and spectral flows, including interesting properties unique to open boundaries, and numerically demonstrate our conclusions with tight-binding model examples. Our results enrich our foundational understanding of topological characterizations for generic non-Hermitian quantum systems.","sentences":["There has been much recent interest and progress on topological structures of the non-Hermitian Bloch bands.","Here, we study the topological structures of non-Bloch bands of non-Hermitian multi-band quantum systems under open boundary conditions.","Using a continuity criterion and an efficient sub-GBZ algorithm, we establish a homotopic characterization -- braiding topology, e.g., characterized by the band's total vorticity -- for open-boundary bands and sub-GBZs.","Such topological identification is robust without topological transition and emergent degenerate points, such as exceptional points.","We further analyze the transition's impact on bands and spectral flows, including interesting properties unique to open boundaries, and numerically demonstrate our conclusions with tight-binding model examples.","Our results enrich our foundational understanding of topological characterizations for generic non-Hermitian quantum systems."],"url":"http://arxiv.org/abs/2405.11832v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 06:58:47","title":"SSAMBA: Self-Supervised Audio Representation Learning with Mamba State Space Model","abstract":"Transformers have revolutionized deep learning across various tasks, including audio representation learning, due to their powerful modeling capabilities. However, they often suffer from quadratic complexity in both GPU memory usage and computational inference time, affecting their efficiency. Recently, state space models (SSMs) like Mamba have emerged as a promising alternative, offering a more efficient approach by avoiding these complexities. Given these advantages, we explore the potential of SSM-based models in audio tasks. In this paper, we introduce Self-Supervised Audio Mamba (SSAMBA), the first self-supervised, attention-free, and SSM-based model for audio representation learning. SSAMBA leverages the bidirectional Mamba to capture complex audio patterns effectively. We incorporate a self-supervised pretraining framework that optimizes both discriminative and generative objectives, enabling the model to learn robust audio representations from large-scale, unlabeled datasets. We evaluated SSAMBA on various tasks such as audio classification, keyword spotting, and speaker identification. Our results demonstrate that SSAMBA outperforms the Self-Supervised Audio Spectrogram Transformer (SSAST) in most tasks. Notably, SSAMBA is approximately 92.7% faster in batch inference speed and 95.4% more memory-efficient than SSAST for the tiny model size with an input token size of 22k. These efficiency gains, combined with superior performance, underscore the effectiveness of SSAMBA's architectural innovation, making it a compelling choice for a wide range of audio processing applications.","sentences":["Transformers have revolutionized deep learning across various tasks, including audio representation learning, due to their powerful modeling capabilities.","However, they often suffer from quadratic complexity in both GPU memory usage and computational inference time, affecting their efficiency.","Recently, state space models (SSMs) like Mamba have emerged as a promising alternative, offering a more efficient approach by avoiding these complexities.","Given these advantages, we explore the potential of SSM-based models in audio tasks.","In this paper, we introduce Self-Supervised Audio Mamba (SSAMBA), the first self-supervised, attention-free, and SSM-based model for audio representation learning.","SSAMBA leverages the bidirectional Mamba to capture complex audio patterns effectively.","We incorporate a self-supervised pretraining framework that optimizes both discriminative and generative objectives, enabling the model to learn robust audio representations from large-scale, unlabeled datasets.","We evaluated SSAMBA on various tasks such as audio classification, keyword spotting, and speaker identification.","Our results demonstrate that SSAMBA outperforms the Self-Supervised Audio Spectrogram Transformer (SSAST) in most tasks.","Notably, SSAMBA is approximately 92.7% faster in batch inference speed and 95.4% more memory-efficient than SSAST for the tiny model size with an input token size of 22k.","These efficiency gains, combined with superior performance, underscore the effectiveness of SSAMBA's architectural innovation, making it a compelling choice for a wide range of audio processing applications."],"url":"http://arxiv.org/abs/2405.11831v1","category":"eess.AS"}
{"created":"2024-05-20 06:56:43","title":"Adversarially Diversified Rehearsal Memory (ADRM): Mitigating Memory Overfitting Challenge in Continual Learning","abstract":"Continual learning focuses on learning non-stationary data distribution without forgetting previous knowledge. Rehearsal-based approaches are commonly used to combat catastrophic forgetting. However, these approaches suffer from a problem called \"rehearsal memory overfitting, \" where the model becomes too specialized on limited memory samples and loses its ability to generalize effectively. As a result, the effectiveness of the rehearsal memory progressively decays, ultimately resulting in catastrophic forgetting of the learned tasks.   We introduce the Adversarially Diversified Rehearsal Memory (ADRM) to address the memory overfitting challenge. This novel method is designed to enrich memory sample diversity and bolster resistance against natural and adversarial noise disruptions. ADRM employs the FGSM attacks to introduce adversarially modified memory samples, achieving two primary objectives: enhancing memory diversity and fostering a robust response to continual feature drifts in memory samples.   Our contributions are as follows: Firstly, ADRM addresses overfitting in rehearsal memory by employing FGSM to diversify and increase the complexity of the memory buffer. Secondly, we demonstrate that ADRM mitigates memory overfitting and significantly improves the robustness of CL models, which is crucial for safety-critical applications. Finally, our detailed analysis of features and visualization demonstrates that ADRM mitigates feature drifts in CL memory samples, significantly reducing catastrophic forgetting and resulting in a more resilient CL model. Additionally, our in-depth t-SNE visualizations of feature distribution and the quantification of the feature similarity further enrich our understanding of feature representation in existing CL approaches. Our code is publically available at https://github.com/hikmatkhan/ADRM.","sentences":["Continual learning focuses on learning non-stationary data distribution without forgetting previous knowledge.","Rehearsal-based approaches are commonly used to combat catastrophic forgetting.","However, these approaches suffer from a problem called \"rehearsal memory overfitting, \" where the model becomes too specialized on limited memory samples and loses its ability to generalize effectively.","As a result, the effectiveness of the rehearsal memory progressively decays, ultimately resulting in catastrophic forgetting of the learned tasks.   ","We introduce the Adversarially Diversified Rehearsal Memory (ADRM) to address the memory overfitting challenge.","This novel method is designed to enrich memory sample diversity and bolster resistance against natural and adversarial noise disruptions.","ADRM employs the FGSM attacks to introduce adversarially modified memory samples, achieving two primary objectives: enhancing memory diversity and fostering a robust response to continual feature drifts in memory samples.   ","Our contributions are as follows: Firstly, ADRM addresses overfitting in rehearsal memory by employing FGSM to diversify and increase the complexity of the memory buffer.","Secondly, we demonstrate that ADRM mitigates memory overfitting and significantly improves the robustness of CL models, which is crucial for safety-critical applications.","Finally, our detailed analysis of features and visualization demonstrates that ADRM mitigates feature drifts in CL memory samples, significantly reducing catastrophic forgetting and resulting in a more resilient CL model.","Additionally, our in-depth t-SNE visualizations of feature distribution and the quantification of the feature similarity further enrich our understanding of feature representation in existing CL approaches.","Our code is publically available at https://github.com/hikmatkhan/ADRM."],"url":"http://arxiv.org/abs/2405.11829v1","category":"cs.LG"}
{"created":"2024-05-20 06:43:07","title":"Time evolution of the von Neumann entropy in open quantum system","abstract":"Control of open quantum dynamics is of great interest for realizing quantum technologies. Therefore, it is an important task to quantify and characterize the entropy for open quantum systems under decoherence. In this paper, we study the time evolution of the von Neumann entropy for open quantum systems described by the Lindblad master equation. Note that, in particular, when the decoherence corresponds to the measurement for the observable in the system, the von Neumann entropy tends to monotonically increases as the variance becomes larger. Furthermore, we present a lower bound of the von Neumann entropy in the long-time limit. This lower bound has advantages of being straightforwardly calculated and applicable to a general Markovian open quantum system.","sentences":["Control of open quantum dynamics is of great interest for realizing quantum technologies.","Therefore, it is an important task to quantify and characterize the entropy for open quantum systems under decoherence.","In this paper, we study the time evolution of the von Neumann entropy for open quantum systems described by the Lindblad master equation.","Note that, in particular, when the decoherence corresponds to the measurement for the observable in the system, the von Neumann entropy tends to monotonically increases as the variance becomes larger.","Furthermore, we present a lower bound of the von Neumann entropy in the long-time limit.","This lower bound has advantages of being straightforwardly calculated and applicable to a general Markovian open quantum system."],"url":"http://arxiv.org/abs/2405.11824v1","category":"quant-ph"}
{"created":"2024-05-20 06:29:24","title":"Testing neutrino mass hierarchy under type-II seesaw scenario in $U(1)_X$ at colliders","abstract":"The origin of tiny neutrino mass is a long standing unsolved puzzle of the Standard Model (SM) which allows us to consider scenarios beyond the Standard Model (BSM) in a variety of ways. One of them being the gauge extension of the SM could be realized as in the form of an anomaly free, general $U(1)_X$ extension of the SM where an $SU(2)_L$ triplet scalar being charged under $U(1)_X$ gauge group is introduced through a Dirac Yukawa coupling with the SM lepton doublet. Once the triplet scalar generates VEV, light neutrinos could acquire tiny Majorana mass and hence affecting the decay modes of the triplet scalar involving the neutrino oscillation data for different neutrino mass hierarchies. After the breaking of $U(1)_X$ scenarios, a neutral BSM, neutral gauge boson $(Z^\\prime)$ acquires mass which interact differently with the left and right handed fermions. Satisfying the recent LHC bounds on the triplet scalar and $Z^\\prime$ production, we study the pair production of the triplet scalar at LHC, $e^-e^+$ and $\\mu^-\\mu^+$ colliders followed by its decay into dominant mode depending on the neutrino mass hierarchy. Generating the SM generic backgrounds, we study the possible signal significance of four lepton final states. We also compare our results with the purely SM gauge mediated triplet scalar pair production followed by four lepton final states which could be significant only in $\\mu^- \\mu^+$ collider.","sentences":["The origin of tiny neutrino mass is a long standing unsolved puzzle of the Standard Model (SM) which allows us to consider scenarios beyond the Standard Model (BSM) in a variety of ways.","One of them being the gauge extension of the SM could be realized as in the form of an anomaly free, general $U(1)_X$ extension of the SM where an $SU(2)_L$ triplet scalar being charged under $U(1)_X$ gauge group is introduced through a Dirac Yukawa coupling with the SM lepton doublet.","Once the triplet scalar generates VEV, light neutrinos could acquire tiny Majorana mass and hence affecting the decay modes of the triplet scalar involving the neutrino oscillation data for different neutrino mass hierarchies.","After the breaking of $U(1)_X$ scenarios, a neutral BSM, neutral gauge boson $(Z^\\prime)$ acquires mass which interact differently with the left and right handed fermions.","Satisfying the recent LHC bounds on the triplet scalar and $Z^\\prime$ production, we study the pair production of the triplet scalar at LHC, $e^-e^+$ and $\\mu^-\\mu^+$ colliders followed by its decay into dominant mode depending on the neutrino mass hierarchy.","Generating the SM generic backgrounds, we study the possible signal significance of four lepton final states.","We also compare our results with the purely SM gauge mediated triplet scalar pair production followed by four lepton final states which could be significant only in $\\mu^- \\mu^+$ collider."],"url":"http://arxiv.org/abs/2405.11820v1","category":"hep-ph"}
{"created":"2024-05-20 06:25:59","title":"A Rate-Distortion Analysis for Composite Sources Under Subsource-Dependent Fidelity Criteria","abstract":"A composite source, consisting of multiple subsources and a memoryless switch, outputs one symbol at a time from the subsource selected by the switch. If some data should be encoded more accurately than other data from an information source, the composite source model is suitable because in this model different distortion constraints can be put on the subsources. In this context, we propose subsource-dependent fidelity criteria for composite sources and use them to formulate a rate-distortion problem. We solve the problem and obtain a single-letter expression for the rate-distortion function. Further rate-distortion analysis characterizes the performance of classify-then-compress (CTC) coding, which is frequently used in practice when subsource-dependent fidelity criteria are considered. Our analysis shows that CTC coding generally has performance loss relative to optimal coding, even if the classification is perfect. We also identify the cause of the performance loss, that is, class labels have to be reproduced in CTC coding. Last but not least, we show that the performance loss is negligible for asymptotically small distortion if CTC coding is appropriately designed and some mild conditions are satisfied.","sentences":["A composite source, consisting of multiple subsources and a memoryless switch, outputs one symbol at a time from the subsource selected by the switch.","If some data should be encoded more accurately than other data from an information source, the composite source model is suitable because in this model different distortion constraints can be put on the subsources.","In this context, we propose subsource-dependent fidelity criteria for composite sources and use them to formulate a rate-distortion problem.","We solve the problem and obtain a single-letter expression for the rate-distortion function.","Further rate-distortion analysis characterizes the performance of classify-then-compress (CTC) coding, which is frequently used in practice when subsource-dependent fidelity criteria are considered.","Our analysis shows that CTC coding generally has performance loss relative to optimal coding, even if the classification is perfect.","We also identify the cause of the performance loss, that is, class labels have to be reproduced in CTC coding.","Last but not least, we show that the performance loss is negligible for asymptotically small distortion if CTC coding is appropriately designed and some mild conditions are satisfied."],"url":"http://arxiv.org/abs/2405.11818v1","category":"cs.IT"}
{"created":"2024-05-20 06:23:22","title":"Transfer Learning for CSI-based Positioning with Multi-environment Meta-learning","abstract":"Utilizing deep learning (DL) techniques for radio-based positioning of user equipment (UE) through channel state information (CSI) fingerprints has demonstrated significant potential. DL models can extract complex characteristics from the CSI fingerprints of a particular environment and accurately predict the position of a UE. Nonetheless, the effectiveness of the DL model trained on CSI fingerprints is highly dependent on the particular training environment, limiting the trained model's applicability across different environments. This paper proposes a novel DL model structure consisting of two parts, where the first part aims at identifying features that are independent from any specific environment, while the second part combines those features in an environment specific way with the goal of positioning. To train such a two-part model, we propose the multi-environment meta-learning (MEML) approach for the first part to facilitate training across various environments, while the second part of the model is trained solely on data from a specific environment. Our findings indicate that employing the MEML approach for initializing the weights of the DL model for a new unseen environment significantly boosts the accuracy of UE positioning in the new target environment as well the reliability of its uncertainty estimation. This method outperforms traditional transfer learning methods, whether direct transfer learning (DTL) between environments or completely training from scratch with data from a new environment. The proposed approach is verified with real measurements for both line-of-sight (LOS) and non-LOS (NLOS) environments.","sentences":["Utilizing deep learning (DL) techniques for radio-based positioning of user equipment (UE) through channel state information (CSI) fingerprints has demonstrated significant potential.","DL models can extract complex characteristics from the CSI fingerprints of a particular environment and accurately predict the position of a UE.","Nonetheless, the effectiveness of the DL model trained on CSI fingerprints is highly dependent on the particular training environment, limiting the trained model's applicability across different environments.","This paper proposes a novel DL model structure consisting of two parts, where the first part aims at identifying features that are independent from any specific environment, while the second part combines those features in an environment specific way with the goal of positioning.","To train such a two-part model, we propose the multi-environment meta-learning (MEML) approach for the first part to facilitate training across various environments, while the second part of the model is trained solely on data from a specific environment.","Our findings indicate that employing the MEML approach for initializing the weights of the DL model for a new unseen environment significantly boosts the accuracy of UE positioning in the new target environment as well the reliability of its uncertainty estimation.","This method outperforms traditional transfer learning methods, whether direct transfer learning (DTL) between environments or completely training from scratch with data from a new environment.","The proposed approach is verified with real measurements for both line-of-sight (LOS) and non-LOS (NLOS) environments."],"url":"http://arxiv.org/abs/2405.11816v1","category":"eess.SP"}
{"created":"2024-05-20 06:21:47","title":"Method of Filtration in first passage time problems","abstract":"Statistics of stochastic processes crucially depends on the imposed boundary condition. In one spatial dimension, for example, first passage time distribution in semi-infinite space (one absorbing boundary) is markedly different from that in a finite interval with two absorbing boundaries. Here we propose a method, which we call method of filtration, that allows us to construct the latter from the knowledge of the former only. We argue that our method is a sort of generalization of the method of image. We also demonstrate that in terms of the performance, it is complementary to another standard method based on the eigenfunction expansion solution of the Fokker-Planck equation.","sentences":["Statistics of stochastic processes crucially depends on the imposed boundary condition.","In one spatial dimension, for example, first passage time distribution in semi-infinite space (one absorbing boundary) is markedly different from that in a finite interval with two absorbing boundaries.","Here we propose a method, which we call method of filtration, that allows us to construct the latter from the knowledge of the former only.","We argue that our method is a sort of generalization of the method of image.","We also demonstrate that in terms of the performance, it is complementary to another standard method based on the eigenfunction expansion solution of the Fokker-Planck equation."],"url":"http://arxiv.org/abs/2405.11815v1","category":"math-ph"}
{"created":"2024-05-20 06:21:15","title":"Climatic & Anthropogenic Hazards to the Nasca World Heritage: Application of Remote Sensing, AI, and Flood Modelling","abstract":"Preservation of the Nasca geoglyphs at the UNESCO World Heritage Site in Peru is urgent as natural and human impact accelerates. More frequent weather extremes such as flashfloods threaten Nasca artifacts. We demonstrate that runoff models based on (sub-)meter scale, LiDAR-derived digital elevation data can highlight AI-detected geoglyphs that are in danger of erosion. We recommend measures of mitigation to protect the famous \"lizard\", \"tree\", and \"hand\" geoglyphs located close by, or even cut by the Pan-American Highway.","sentences":["Preservation of the Nasca geoglyphs at the UNESCO World Heritage Site in Peru is urgent as natural and human impact accelerates.","More frequent weather extremes such as flashfloods threaten Nasca artifacts.","We demonstrate that runoff models based on (sub-)meter scale, LiDAR-derived digital elevation data can highlight AI-detected geoglyphs that are in danger of erosion.","We recommend measures of mitigation to protect the famous \"lizard\", \"tree\", and \"hand\" geoglyphs located close by, or even cut by the Pan-American Highway."],"url":"http://arxiv.org/abs/2405.11814v1","category":"cs.CV"}
{"created":"2024-05-20 06:03:55","title":"Distill-then-prune: An Efficient Compression Framework for Real-time Stereo Matching Network on Edge Devices","abstract":"In recent years, numerous real-time stereo matching methods have been introduced, but they often lack accuracy. These methods attempt to improve accuracy by introducing new modules or integrating traditional methods. However, the improvements are only modest. In this paper, we propose a novel strategy by incorporating knowledge distillation and model pruning to overcome the inherent trade-off between speed and accuracy. As a result, we obtained a model that maintains real-time performance while delivering high accuracy on edge devices. Our proposed method involves three key steps. Firstly, we review state-of-the-art methods and design our lightweight model by removing redundant modules from those efficient models through a comparison of their contributions. Next, we leverage the efficient model as the teacher to distill knowledge into the lightweight model. Finally, we systematically prune the lightweight model to obtain the final model. Through extensive experiments conducted on two widely-used benchmarks, Sceneflow and KITTI, we perform ablation studies to analyze the effectiveness of each module and present our state-of-the-art results.","sentences":["In recent years, numerous real-time stereo matching methods have been introduced, but they often lack accuracy.","These methods attempt to improve accuracy by introducing new modules or integrating traditional methods.","However, the improvements are only modest.","In this paper, we propose a novel strategy by incorporating knowledge distillation and model pruning to overcome the inherent trade-off between speed and accuracy.","As a result, we obtained a model that maintains real-time performance while delivering high accuracy on edge devices.","Our proposed method involves three key steps.","Firstly, we review state-of-the-art methods and design our lightweight model by removing redundant modules from those efficient models through a comparison of their contributions.","Next, we leverage the efficient model as the teacher to distill knowledge into the lightweight model.","Finally, we systematically prune the lightweight model to obtain the final model.","Through extensive experiments conducted on two widely-used benchmarks, Sceneflow and KITTI, we perform ablation studies to analyze the effectiveness of each module and present our state-of-the-art results."],"url":"http://arxiv.org/abs/2405.11809v1","category":"cs.CV"}
{"created":"2024-05-20 05:55:38","title":"The Geometry of Tangent Spaces on Causal Sets","abstract":"In this paper, we expand on previous work describing partial derivatives and metric component estimators to define tangent spaces on causal sets. Partial derivative operators are the basis vectors of the tangent space, and the metric defines the inner product. First, we use partial derivatives of the metric components to define the connection and partial derivatives of the connection to define the curvature. Numerical results show that both of these approach the expected values for a flat spacetime as density increases. Then we used the connection to define parallel transport and geodesics.","sentences":["In this paper, we expand on previous work describing partial derivatives and metric component estimators to define tangent spaces on causal sets.","Partial derivative operators are the basis vectors of the tangent space, and the metric defines the inner product.","First, we use partial derivatives of the metric components to define the connection and partial derivatives of the connection to define the curvature.","Numerical results show that both of these approach the expected values for a flat spacetime as density increases.","Then we used the connection to define parallel transport and geodesics."],"url":"http://arxiv.org/abs/2405.11805v1","category":"gr-qc"}
{"created":"2024-05-20 05:48:20","title":"Counterfactual Explanation-Based Badminton Motion Guidance Generation Using Wearable Sensors","abstract":"This study proposes a framework for enhancing the stroke quality of badminton players by generating personalized motion guides, utilizing a multimodal wearable dataset. These guides are based on counterfactual algorithms and aim to reduce the performance gap between novice and expert players. Our approach provides joint-level guidance through visualizable data to assist players in improving their movements without requiring expert knowledge. The method was evaluated against a traditional algorithm using metrics to assess validity, proximity, and plausibility, including arithmetic measures and motion-specific evaluation metrics. Our evaluation demonstrates that the proposed framework can generate motions that maintain the essence of original movements while enhancing stroke quality, providing closer guidance than direct expert motion replication. The results highlight the potential of our approach for creating personalized sports motion guides by generating counterfactual motion guidance for arbitrary input motion samples of badminton strokes.","sentences":["This study proposes a framework for enhancing the stroke quality of badminton players by generating personalized motion guides, utilizing a multimodal wearable dataset.","These guides are based on counterfactual algorithms and aim to reduce the performance gap between novice and expert players.","Our approach provides joint-level guidance through visualizable data to assist players in improving their movements without requiring expert knowledge.","The method was evaluated against a traditional algorithm using metrics to assess validity, proximity, and plausibility, including arithmetic measures and motion-specific evaluation metrics.","Our evaluation demonstrates that the proposed framework can generate motions that maintain the essence of original movements while enhancing stroke quality, providing closer guidance than direct expert motion replication.","The results highlight the potential of our approach for creating personalized sports motion guides by generating counterfactual motion guidance for arbitrary input motion samples of badminton strokes."],"url":"http://arxiv.org/abs/2405.11802v1","category":"cs.HC"}
{"created":"2024-05-20 05:46:38","title":"Generative AI in Higher Education: A Global Perspective of Institutional Adoption Policies and Guidelines","abstract":"Integrating generative AI (GAI) into higher education is crucial for preparing a future generation of GAI-literate students. Yet a thorough understanding of the global institutional adoption policy remains absent, with most of the prior studies focused on the Global North and the promises and challenges of GAI, lacking a theoretical lens. This study utilizes the Diffusion of Innovations Theory to examine GAI adoption strategies in higher education across 40 universities from six global regions. It explores the characteristics of GAI innovation, including compatibility, trialability, and observability, and analyses the communication channels and roles and responsibilities outlined in university policies and guidelines. The findings reveal a proactive approach by universities towards GAI integration, emphasizing academic integrity, teaching and learning enhancement, and equity. Despite a cautious yet optimistic stance, a comprehensive policy framework is needed to evaluate the impacts of GAI integration and establish effective communication strategies that foster broader stakeholder engagement. The study highlights the importance of clear roles and responsibilities among faculty, students, and administrators for successful GAI integration, supporting a collaborative model for navigating the complexities of GAI in education. This study contributes insights for policymakers in crafting detailed strategies for its integration.","sentences":["Integrating generative AI (GAI) into higher education is crucial for preparing a future generation of GAI-literate students.","Yet a thorough understanding of the global institutional adoption policy remains absent, with most of the prior studies focused on the Global North and the promises and challenges of GAI, lacking a theoretical lens.","This study utilizes the Diffusion of Innovations Theory to examine GAI adoption strategies in higher education across 40 universities from six global regions.","It explores the characteristics of GAI innovation, including compatibility, trialability, and observability, and analyses the communication channels and roles and responsibilities outlined in university policies and guidelines.","The findings reveal a proactive approach by universities towards GAI integration, emphasizing academic integrity, teaching and learning enhancement, and equity.","Despite a cautious yet optimistic stance, a comprehensive policy framework is needed to evaluate the impacts of GAI integration and establish effective communication strategies that foster broader stakeholder engagement.","The study highlights the importance of clear roles and responsibilities among faculty, students, and administrators for successful GAI integration, supporting a collaborative model for navigating the complexities of GAI in education.","This study contributes insights for policymakers in crafting detailed strategies for its integration."],"url":"http://arxiv.org/abs/2405.11800v1","category":"cs.CY"}
{"created":"2024-05-20 05:34:13","title":"Constructing vortex functions and basis states of Chern insulators: ideal condition, inequality from index theorem, and coherent-like states on von Neumann lattice","abstract":"In the field of fractional Chern insulators, a great deal of effort has been devoted to characterizing Chern bands that exhibit properties similar to the Landau levels. Among them, the concept of the vortex function, which generalizes the complex coordinate used for the symmetric-gauge Landau-level basis, allows for a concise description. In this paper, we develop a theory of constructing the vortex function and basis states of Chern insulators. In the first half, we consider the optimization process of the vortex function, which minimizes an indicator that measures the difference from the ideal Chern insulators. In particular, we focus on the sublattice position dependence of the vortex function or the quantum geometric tensor. In the second half, we construct two types of basis sets for a given vortex function: radially localized basis set and coherent-like basis set. The former basis set is defined as the eigenstates of an analogy of the angular momentum operator. Remarkably, one can always find exact zero mode(s) for this operator, which is explained by the celebrated Atiyah-Singer index theorem. As a byproduct, we propose an inequality rooted in the band topology. We also discuss the subtle differences between our formalism and the previous works about the momentum-space Landau level. The latter basis set generalizes the concept of coherent states on von Neumann lattice. While this basis set is not orthogonal, it is useful to compare the LLL and the given Chern insulator directly in the Brillouin zone.","sentences":["In the field of fractional Chern insulators, a great deal of effort has been devoted to characterizing Chern bands that exhibit properties similar to the Landau levels.","Among them, the concept of the vortex function, which generalizes the complex coordinate used for the symmetric-gauge Landau-level basis, allows for a concise description.","In this paper, we develop a theory of constructing the vortex function and basis states of Chern insulators.","In the first half, we consider the optimization process of the vortex function, which minimizes an indicator that measures the difference from the ideal Chern insulators.","In particular, we focus on the sublattice position dependence of the vortex function or the quantum geometric tensor.","In the second half, we construct two types of basis sets for a given vortex function: radially localized basis set and coherent-like basis set.","The former basis set is defined as the eigenstates of an analogy of the angular momentum operator.","Remarkably, one can always find exact zero mode(s) for this operator, which is explained by the celebrated Atiyah-Singer index theorem.","As a byproduct, we propose an inequality rooted in the band topology.","We also discuss the subtle differences between our formalism and the previous works about the momentum-space Landau level.","The latter basis set generalizes the concept of coherent states on von Neumann lattice.","While this basis set is not orthogonal, it is useful to compare the LLL and the given Chern insulator directly in the Brillouin zone."],"url":"http://arxiv.org/abs/2405.11796v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 05:29:45","title":"Application of time-series quantum generative model to financial data","abstract":"Despite proposing a quantum generative model for time series that successfully learns correlated series with multiple Brownian motions, the model has not been adapted and evaluated for financial problems. In this study, a time-series generative model was applied as a quantum generative model to actual financial data. Future data for two correlated time series were generated and compared with classical methods such as long short-term memory and vector autoregression. Furthermore, numerical experiments were performed to complete missing values. Based on the results, we evaluated the practical applications of the time-series quantum generation model. It was observed that fewer parameter values were required compared with the classical method. In addition, the quantum time-series generation model was feasible for both stationary and nonstationary data. These results suggest that several parameters can be applied to various types of time-series data.","sentences":["Despite proposing a quantum generative model for time series that successfully learns correlated series with multiple Brownian motions, the model has not been adapted and evaluated for financial problems.","In this study, a time-series generative model was applied as a quantum generative model to actual financial data.","Future data for two correlated time series were generated and compared with classical methods such as long short-term memory and vector autoregression.","Furthermore, numerical experiments were performed to complete missing values.","Based on the results, we evaluated the practical applications of the time-series quantum generation model.","It was observed that fewer parameter values were required compared with the classical method.","In addition, the quantum time-series generation model was feasible for both stationary and nonstationary data.","These results suggest that several parameters can be applied to various types of time-series data."],"url":"http://arxiv.org/abs/2405.11795v1","category":"quant-ph"}
{"created":"2024-05-20 05:28:22","title":"ViViD: Video Virtual Try-on using Diffusion Models","abstract":"Video virtual try-on aims to transfer a clothing item onto the video of a target person. Directly applying the technique of image-based try-on to the video domain in a frame-wise manner will cause temporal-inconsistent outcomes while previous video-based try-on solutions can only generate low visual quality and blurring results. In this work, we present ViViD, a novel framework employing powerful diffusion models to tackle the task of video virtual try-on. Specifically, we design the Garment Encoder to extract fine-grained clothing semantic features, guiding the model to capture garment details and inject them into the target video through the proposed attention feature fusion mechanism. To ensure spatial-temporal consistency, we introduce a lightweight Pose Encoder to encode pose signals, enabling the model to learn the interactions between clothing and human posture and insert hierarchical Temporal Modules into the text-to-image stable diffusion model for more coherent and lifelike video synthesis. Furthermore, we collect a new dataset, which is the largest, with the most diverse types of garments and the highest resolution for the task of video virtual try-on to date. Extensive experiments demonstrate that our approach is able to yield satisfactory video try-on results. The dataset, codes, and weights will be publicly available. Project page: https://becauseimbatman0.github.io/ViViD.","sentences":["Video virtual try-on aims to transfer a clothing item onto the video of a target person.","Directly applying the technique of image-based try-on to the video domain in a frame-wise manner will cause temporal-inconsistent outcomes while previous video-based try-on solutions can only generate low visual quality and blurring results.","In this work, we present ViViD, a novel framework employing powerful diffusion models to tackle the task of video virtual try-on.","Specifically, we design the Garment Encoder to extract fine-grained clothing semantic features, guiding the model to capture garment details and inject them into the target video through the proposed attention feature fusion mechanism.","To ensure spatial-temporal consistency, we introduce a lightweight Pose Encoder to encode pose signals, enabling the model to learn the interactions between clothing and human posture and insert hierarchical Temporal Modules into the text-to-image stable diffusion model for more coherent and lifelike video synthesis.","Furthermore, we collect a new dataset, which is the largest, with the most diverse types of garments and the highest resolution for the task of video virtual try-on to date.","Extensive experiments demonstrate that our approach is able to yield satisfactory video try-on results.","The dataset, codes, and weights will be publicly available.","Project page: https://becauseimbatman0.github.io/ViViD."],"url":"http://arxiv.org/abs/2405.11794v1","category":"cs.CV"}
{"created":"2024-05-20 05:23:56","title":"MM-Retinal: Knowledge-Enhanced Foundational Pretraining with Fundus Image-Text Expertise","abstract":"Current fundus image analysis models are predominantly built for specific tasks relying on individual datasets. The learning process is usually based on data-driven paradigm without prior knowledge, resulting in poor transferability and generalizability. To address this issue, we propose MM-Retinal, a multi-modal dataset that encompasses high-quality image-text pairs collected from professional fundus diagram books. Moreover, enabled by MM-Retinal, we present a novel Knowledge-enhanced foundational pretraining model which incorporates Fundus Image-Text expertise, called KeepFIT. It is designed with image similarity-guided text revision and mixed training strategy to infuse expert knowledge. Our proposed fundus foundation model achieves state-of-the-art performance across six unseen downstream tasks and holds excellent generalization ability in zero-shot and few-shot scenarios. MM-Retinal and KeepFIT are available at https://github.com/lxirich/MM-Retinal.","sentences":["Current fundus image analysis models are predominantly built for specific tasks relying on individual datasets.","The learning process is usually based on data-driven paradigm without prior knowledge, resulting in poor transferability and generalizability.","To address this issue, we propose MM-Retinal, a multi-modal dataset that encompasses high-quality image-text pairs collected from professional fundus diagram books.","Moreover, enabled by MM-Retinal, we present a novel Knowledge-enhanced foundational pretraining model which incorporates Fundus Image-Text expertise, called KeepFIT.","It is designed with image similarity-guided text revision and mixed training strategy to infuse expert knowledge.","Our proposed fundus foundation model achieves state-of-the-art performance across six unseen downstream tasks and holds excellent generalization ability in zero-shot and few-shot scenarios.","MM-Retinal and KeepFIT are available at https://github.com/lxirich/MM-Retinal."],"url":"http://arxiv.org/abs/2405.11793v1","category":"cs.CV"}
{"created":"2024-05-20 05:16:52","title":"CaseGNN++: Graph Contrastive Learning for Legal Case Retrieval with Graph Augmentation","abstract":"Legal case retrieval (LCR) is a specialised information retrieval task that aims to find relevant cases to a given query case. LCR holds pivotal significance in facilitating legal practitioners in finding precedents. Most of existing LCR methods are based on traditional lexical models and language models, which have gained promising performance in retrieval. However, the domain-specific structural information inherent in legal documents is yet to be exploited to further improve the performance. Our previous work CaseGNN successfully harnesses text-attributed graphs and graph neural networks to address the problem of legal structural information neglect. Nonetheless, there remain two aspects for further investigation: (1) The underutilization of rich edge information within text-attributed case graphs limits CaseGNN to generate informative case representation. (2) The inadequacy of labelled data in legal datasets hinders the training of CaseGNN model. In this paper, CaseGNN++, which is extended from CaseGNN, is proposed to simultaneously leverage the edge information and additional label data to discover the latent potential of LCR models. Specifically, an edge feature-based graph attention layer (EUGAT) is proposed to comprehensively update node and edge features during graph modelling, resulting in a full utilisation of structural information of legal cases. Moreover, a novel graph contrastive learning objective with graph augmentation is developed in CaseGNN++ to provide additional training signals, thereby enhancing the legal comprehension capabilities of CaseGNN++ model. Extensive experiments on two benchmark datasets from COLIEE 2022 and COLIEE 2023 demonstrate that CaseGNN++ not only significantly improves CaseGNN but also achieves supreme performance compared to state-of-the-art LCR methods. Code has been released on https://github.com/yanran-tang/CaseGNN.","sentences":["Legal case retrieval (LCR) is a specialised information retrieval task that aims to find relevant cases to a given query case.","LCR holds pivotal significance in facilitating legal practitioners in finding precedents.","Most of existing LCR methods are based on traditional lexical models and language models, which have gained promising performance in retrieval.","However, the domain-specific structural information inherent in legal documents is yet to be exploited to further improve the performance.","Our previous work CaseGNN successfully harnesses text-attributed graphs and graph neural networks to address the problem of legal structural information neglect.","Nonetheless, there remain two aspects for further investigation: (1) The underutilization of rich edge information within text-attributed case graphs limits CaseGNN to generate informative case representation.","(2) The inadequacy of labelled data in legal datasets hinders the training of CaseGNN model.","In this paper, CaseGNN++, which is extended from CaseGNN, is proposed to simultaneously leverage the edge information and additional label data to discover the latent potential of LCR models.","Specifically, an edge feature-based graph attention layer (EUGAT) is proposed to comprehensively update node and edge features during graph modelling, resulting in a full utilisation of structural information of legal cases.","Moreover, a novel graph contrastive learning objective with graph augmentation is developed in CaseGNN++ to provide additional training signals, thereby enhancing the legal comprehension capabilities of CaseGNN++ model.","Extensive experiments on two benchmark datasets from COLIEE 2022 and COLIEE 2023 demonstrate that CaseGNN++ not only significantly improves CaseGNN but also achieves supreme performance compared to state-of-the-art LCR methods.","Code has been released on https://github.com/yanran-tang/CaseGNN."],"url":"http://arxiv.org/abs/2405.11791v1","category":"cs.IR"}
{"created":"2024-05-20 05:16:12","title":"Graviton physics: Quantum field theory of gravitons, graviton noise and gravitational decoherence -- a concise tutorial","abstract":"The detection of gravitational waves in 2015 ushered in a new era of gravitational wave astronomy capable of probing into the strong field dynamics of black holes and neutron stars. It has opened up an exciting new window for laboratory and space tests of Einstein's theory of classical general relativity. In recent years there are two interesting proposals aimed at revealing the quantum natures of perturbative gravity: 1) theoretical predictions in how graviton noise from the early universe after the vacuum of the gravitational field was strongly squeezed by inflationary expansion; 2) experimental proposals using the quantum entanglement between two masses each in a superposition state. The first proposal invokes the stochastic properties of quantum fields, the second invokes a key concept of quantum information. An equally basic and interesting idea is to ask whether and how gravity might be responsible for a quantum system becoming classical in appearance, known as gravitational decoherence. Decoherence due to gravity is of special interest because gravity is universal. This is an important issue in macroscopic quantum phenomena. To fully appreciate these exciting developments requires a working knowledge in classical GR, QF theory and QI plus some familiarity with stochastic processes, namely, noise in quantum fields. Traditionally a new researcher may be conversant in one or two of these four subjects: GR, QFT, QI, SP, depending on his/her background. This tutorial attempts to provide the necessary connections between them, helping an engaging reader from any one of these four subjects to leapfrog to the frontier of these interdisciplinary research topics. Here we shall treat the three topics listed in the title, save gravitational entanglement, because its nature and implications proclaimed in relation to quantum gravity still contain many controversial elements.","sentences":["The detection of gravitational waves in 2015 ushered in a new era of gravitational wave astronomy capable of probing into the strong field dynamics of black holes and neutron stars.","It has opened up an exciting new window for laboratory and space tests of Einstein's theory of classical general relativity.","In recent years there are two interesting proposals aimed at revealing the quantum natures of perturbative gravity: 1) theoretical predictions in how graviton noise from the early universe after the vacuum of the gravitational field was strongly squeezed by inflationary expansion; 2) experimental proposals using the quantum entanglement between two masses each in a superposition state.","The first proposal invokes the stochastic properties of quantum fields, the second invokes a key concept of quantum information.","An equally basic and interesting idea is to ask whether and how gravity might be responsible for a quantum system becoming classical in appearance, known as gravitational decoherence.","Decoherence due to gravity is of special interest because gravity is universal.","This is an important issue in macroscopic quantum phenomena.","To fully appreciate these exciting developments requires a working knowledge in classical GR, QF theory and QI plus some familiarity with stochastic processes, namely, noise in quantum fields.","Traditionally a new researcher may be conversant in one or two of these four subjects: GR, QFT, QI, SP, depending on his/her background.","This tutorial attempts to provide the necessary connections between them, helping an engaging reader from any one of these four subjects to leapfrog to the frontier of these interdisciplinary research topics.","Here we shall treat the three topics listed in the title, save gravitational entanglement, because its nature and implications proclaimed in relation to quantum gravity still contain many controversial elements."],"url":"http://arxiv.org/abs/2405.11790v1","category":"hep-th"}
{"created":"2024-05-20 05:08:56","title":"Graviton-photon conversions in Euler-Heisenberg nonlinear electrodynamics","abstract":"We study graviton-photon conversions in an environment of the uniform and constant magnetic field considering Euler-K\\\"ockel-Heisenberg-type nonlinear corrections in electrodynamics. We take the transverse-tracefree gauge for gravitons and employ both the field potential and the electric and magnetic (EM) fields for photons. The nonlinear correction causes parity violating (chiral) graviton propagation equations, which depend on the two treatments for photons. Thus, the medium becomes effectively birefringent for two polarizations of gravitational waves similar to the photon birefringence a characteristic of the nonlinear correction. In the presence of gravity, due to the nontrivial relation between the potential and EM fields, it is important to present the result using the EM fields.","sentences":["We study graviton-photon conversions in an environment of the uniform and constant magnetic field considering Euler-K\\\"ockel-Heisenberg-type nonlinear corrections in electrodynamics.","We take the transverse-tracefree gauge for gravitons and employ both the field potential and the electric and magnetic (EM) fields for photons.","The nonlinear correction causes parity violating (chiral) graviton propagation equations, which depend on the two treatments for photons.","Thus, the medium becomes effectively birefringent for two polarizations of gravitational waves similar to the photon birefringence a characteristic of the nonlinear correction.","In the presence of gravity, due to the nontrivial relation between the potential and EM fields, it is important to present the result using the EM fields."],"url":"http://arxiv.org/abs/2405.11786v1","category":"gr-qc"}
{"created":"2024-05-20 05:08:55","title":"Guided Multi-objective Generative AI to Enhance Structure-based Drug Design","abstract":"Generative AI has the potential to revolutionize drug discovery. Yet, despite recent advances in machine learning, existing models cannot generate molecules that satisfy all desired physicochemical properties. Herein, we describe IDOLpro, a novel generative chemistry AI combining deep diffusion with multi-objective optimization for structure-based drug design. The latent variables of the diffusion model are guided by differentiable scoring functions to explore uncharted chemical space and generate novel ligands in silico, optimizing a plurality of target physicochemical properties. We demonstrate its effectiveness by generating ligands with optimized binding affinity and synthetic accessibility on two benchmark sets. IDOLpro produces ligands with binding affinities over 10% higher than the next best state-of-the-art on each test set. On a test set of experimental complexes, IDOLpro is the first to surpass the performance of experimentally observed ligands. IDOLpro can accommodate other scoring functions (e.g. ADME-Tox) to accelerate hit-finding, hit-to-lead, and lead optimization for drug discovery.","sentences":["Generative AI has the potential to revolutionize drug discovery.","Yet, despite recent advances in machine learning, existing models cannot generate molecules that satisfy all desired physicochemical properties.","Herein, we describe IDOLpro, a novel generative chemistry AI combining deep diffusion with multi-objective optimization for structure-based drug design.","The latent variables of the diffusion model are guided by differentiable scoring functions to explore uncharted chemical space and generate novel ligands in silico, optimizing a plurality of target physicochemical properties.","We demonstrate its effectiveness by generating ligands with optimized binding affinity and synthetic accessibility on two benchmark sets.","IDOLpro produces ligands with binding affinities over 10% higher than the next best state-of-the-art on each test set.","On a test set of experimental complexes, IDOLpro is the first to surpass the performance of experimentally observed ligands.","IDOLpro can accommodate other scoring functions (e.g. ADME-Tox) to accelerate hit-finding, hit-to-lead, and lead optimization for drug discovery."],"url":"http://arxiv.org/abs/2405.11785v1","category":"physics.chem-ph"}
{"created":"2024-05-20 05:05:14","title":"Reward-Punishment Reinforcement Learning with Maximum Entropy","abstract":"We introduce the ``soft Deep MaxPain'' (softDMP) algorithm, which integrates the optimization of long-term policy entropy into reward-punishment reinforcement learning objectives. Our motivation is to facilitate a smoother variation of operators utilized in the updating of action values beyond traditional ``max'' and ``min'' operators, where the goal is enhancing sample efficiency and robustness. We also address two unresolved issues from the previous Deep MaxPain method. Firstly, we investigate how the negated (``flipped'') pain-seeking sub-policy, derived from the punishment action value, collaborates with the ``min'' operator to effectively learn the punishment module and how softDMP's smooth learning operator provides insights into the ``flipping'' trick. Secondly, we tackle the challenge of data collection for learning the punishment module to mitigate inconsistencies arising from the involvement of the ``flipped'' sub-policy (pain-avoidance sub-policy) in the unified behavior policy. We empirically explore the first issue in two discrete Markov Decision Process (MDP) environments, elucidating the crucial advancements of the DMP approach and the necessity for soft treatments on the hard operators. For the second issue, we propose a probabilistic classifier based on the ratio of the pain-seeking sub-policy to the sum of the pain-seeking and goal-reaching sub-policies. This classifier assigns roll-outs to separate replay buffers for updating reward and punishment action-value functions, respectively. Our framework demonstrates superior performance in Turtlebot 3's maze navigation tasks under the ROS Gazebo simulation.","sentences":["We introduce the ``soft Deep MaxPain'' (softDMP) algorithm, which integrates the optimization of long-term policy entropy into reward-punishment reinforcement learning objectives.","Our motivation is to facilitate a smoother variation of operators utilized in the updating of action values beyond traditional ``max'' and ``min'' operators, where the goal is enhancing sample efficiency and robustness.","We also address two unresolved issues from the previous Deep MaxPain method.","Firstly, we investigate how the negated (``flipped'') pain-seeking sub-policy, derived from the punishment action value, collaborates with the ``min'' operator to effectively learn the punishment module and how softDMP's smooth learning operator provides insights into the ``flipping'' trick.","Secondly, we tackle the challenge of data collection for learning the punishment module to mitigate inconsistencies arising from the involvement of the ``flipped'' sub-policy (pain-avoidance sub-policy) in the unified behavior policy.","We empirically explore the first issue in two discrete Markov Decision Process (MDP) environments, elucidating the crucial advancements of the DMP approach and the necessity for soft treatments on the hard operators.","For the second issue, we propose a probabilistic classifier based on the ratio of the pain-seeking sub-policy to the sum of the pain-seeking and goal-reaching sub-policies.","This classifier assigns roll-outs to separate replay buffers for updating reward and punishment action-value functions, respectively.","Our framework demonstrates superior performance in Turtlebot 3's maze navigation tasks under the ROS Gazebo simulation."],"url":"http://arxiv.org/abs/2405.11784v1","category":"cs.LG"}
{"created":"2024-05-20 05:02:12","title":"Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing","abstract":"In this study, we explore the potential of using quantum natural language processing (QNLP) to inverse design metal-organic frameworks (MOFs) with targeted properties. Specifically, by analyzing 150 hypothetical MOF structures consisting of 10 metal nodes and 15 organic ligands, we categorize these structures into four distinct classes for pore volume and $H_{2}$ uptake values. We then compare various QNLP models (i.e. the bag-of-words, DisCoCat (Distributional Compositional Categorical), and sequence-based models) to identify the most effective approach to process the MOF dataset. Using a classical simulator provided by the IBM Qiskit, the bag-of-words model is identified to be the optimum model, achieving validation accuracies of 85.7% and 86.7% for binary classification tasks on pore volume and $H_{2}$ uptake, respectively. Further, we developed multi-class classification models tailored to the probabilistic nature of quantum circuits, with average test accuracies of 88.4% and 80.7% across different classes for pore volume and $H_{2}$ uptake datasets. Finally, the performance of generating MOF with target properties showed accuracies of 93.5% for pore volume and 89% for $H_{2}$ uptake, respectively. Although our investigation covers only a fraction of the vast MOF search space, it marks a promising first step towards using quantum computing for materials design, offering a new perspective through which to explore the complex landscape of MOFs.","sentences":["In this study, we explore the potential of using quantum natural language processing (QNLP) to inverse design metal-organic frameworks (MOFs) with targeted properties.","Specifically, by analyzing 150 hypothetical MOF structures consisting of 10 metal nodes and 15 organic ligands, we categorize these structures into four distinct classes for pore volume and $H_{2}$ uptake values.","We then compare various QNLP models (i.e. the bag-of-words, DisCoCat (Distributional Compositional Categorical), and sequence-based models) to identify the most effective approach to process the MOF dataset.","Using a classical simulator provided by the IBM Qiskit, the bag-of-words model is identified to be the optimum model, achieving validation accuracies of 85.7% and 86.7% for binary classification tasks on pore volume and $H_{2}$ uptake, respectively.","Further, we developed multi-class classification models tailored to the probabilistic nature of quantum circuits, with average test accuracies of 88.4% and 80.7% across different classes for pore volume and $H_{2}$ uptake datasets.","Finally, the performance of generating MOF with target properties showed accuracies of 93.5% for pore volume and 89% for $H_{2}$ uptake, respectively.","Although our investigation covers only a fraction of the vast MOF search space, it marks a promising first step towards using quantum computing for materials design, offering a new perspective through which to explore the complex landscape of MOFs."],"url":"http://arxiv.org/abs/2405.11783v1","category":"cs.LG"}
{"created":"2024-05-20 04:46:14","title":"General bounds on the quality of Bayesian coresets","abstract":"Bayesian coresets speed up posterior inference in the large-scale data regime by approximating the full-data log-likelihood function with a surrogate log-likelihood based on a small, weighted subset of the data. But while Bayesian coresets and methods for construction are applicable in a wide range of models, existing theoretical analysis of the posterior inferential error incurred by coreset approximations only apply in restrictive settings -- i.e., exponential family models, or models with strong log-concavity and smoothness assumptions. This work presents general upper and lower bounds on the Kullback-Leibler (KL) divergence of coreset approximations that reflect the full range of applicability of Bayesian coresets. The lower bounds require only mild model assumptions typical of Bayesian asymptotic analyses, while the upper bounds require the log-likelihood functions to satisfy a generalized subexponentiality criterion that is weaker than conditions used in earlier work. The lower bounds are applied to obtain fundamental limitations on the quality of coreset approximations, and to provide a theoretical explanation for the previously-observed poor empirical performance of importance sampling-based construction methods. The upper bounds are used to analyze the performance of recent subsample-optimize methods. The flexibility of the theory is demonstrated in validation experiments involving multimodal, unidentifiable, heavy-tailed Bayesian posterior distributions.","sentences":["Bayesian coresets speed up posterior inference in the large-scale data regime by approximating the full-data log-likelihood function with a surrogate log-likelihood based on a small, weighted subset of the data.","But while Bayesian coresets and methods for construction are applicable in a wide range of models, existing theoretical analysis of the posterior inferential error incurred by coreset approximations only apply in restrictive settings -- i.e., exponential family models, or models with strong log-concavity and smoothness assumptions.","This work presents general upper and lower bounds on the Kullback-Leibler (KL) divergence of coreset approximations that reflect the full range of applicability of Bayesian coresets.","The lower bounds require only mild model assumptions typical of Bayesian asymptotic analyses, while the upper bounds require the log-likelihood functions to satisfy a generalized subexponentiality criterion that is weaker than conditions used in earlier work.","The lower bounds are applied to obtain fundamental limitations on the quality of coreset approximations, and to provide a theoretical explanation for the previously-observed poor empirical performance of importance sampling-based construction methods.","The upper bounds are used to analyze the performance of recent subsample-optimize methods.","The flexibility of the theory is demonstrated in validation experiments involving multimodal, unidentifiable, heavy-tailed Bayesian posterior distributions."],"url":"http://arxiv.org/abs/2405.11780v1","category":"stat.ML"}
{"created":"2024-05-20 04:36:13","title":"Tunnelling amplitudes through localised external potentials from Feynman diagram summation","abstract":"Currently there is no general theory of quantum tunnelling of a particle through a potential barrier which is compatible with QFT. We present a complete calculation of tunnelling amplitudes for a scalar field for some simple potentials using quantum field-theoretic methods. Using the perturbative S-matrix formalism, starting with the Klein-Gordon Lagrangian, we show that an infinite summation of Feynman diagrams can recover tunnelling amplitudes consistent with relativistic quantum mechanics. While this work does not include many-particle effects arising from a fully quantised QFT, it is necessary to investigate QFT corrections to tunnelling amplitudes.","sentences":["Currently there is no general theory of quantum tunnelling of a particle through a potential barrier which is compatible with QFT.","We present a complete calculation of tunnelling amplitudes for a scalar field for some simple potentials using quantum field-theoretic methods.","Using the perturbative S-matrix formalism, starting with the Klein-Gordon Lagrangian, we show that an infinite summation of Feynman diagrams can recover tunnelling amplitudes consistent with relativistic quantum mechanics.","While this work does not include many-particle effects arising from a fully quantised QFT, it is necessary to investigate QFT corrections to tunnelling amplitudes."],"url":"http://arxiv.org/abs/2405.11779v1","category":"hep-th"}
{"created":"2024-05-20 04:36:02","title":"Efficient Multi-agent Reinforcement Learning by Planning","abstract":"Multi-agent reinforcement learning (MARL) algorithms have accomplished remarkable breakthroughs in solving large-scale decision-making tasks. Nonetheless, most existing MARL algorithms are model-free, limiting sample efficiency and hindering their applicability in more challenging scenarios. In contrast, model-based reinforcement learning (MBRL), particularly algorithms integrating planning, such as MuZero, has demonstrated superhuman performance with limited data in many tasks. Hence, we aim to boost the sample efficiency of MARL by adopting model-based approaches. However, incorporating planning and search methods into multi-agent systems poses significant challenges. The expansive action space of multi-agent systems often necessitates leveraging the nearly-independent property of agents to accelerate learning. To tackle this issue, we propose the MAZero algorithm, which combines a centralized model with Monte Carlo Tree Search (MCTS) for policy search. We design a novel network structure to facilitate distributed execution and parameter sharing. To enhance search efficiency in deterministic environments with sizable action spaces, we introduce two novel techniques: Optimistic Search Lambda (OS($\\lambda$)) and Advantage-Weighted Policy Optimization (AWPO). Extensive experiments on the SMAC benchmark demonstrate that MAZero outperforms model-free approaches in terms of sample efficiency and provides comparable or better performance than existing model-based methods in terms of both sample and computational efficiency. Our code is available at https://github.com/liuqh16/MAZero.","sentences":["Multi-agent reinforcement learning (MARL) algorithms have accomplished remarkable breakthroughs in solving large-scale decision-making tasks.","Nonetheless, most existing MARL algorithms are model-free, limiting sample efficiency and hindering their applicability in more challenging scenarios.","In contrast, model-based reinforcement learning (MBRL), particularly algorithms integrating planning, such as MuZero, has demonstrated superhuman performance with limited data in many tasks.","Hence, we aim to boost the sample efficiency of MARL by adopting model-based approaches.","However, incorporating planning and search methods into multi-agent systems poses significant challenges.","The expansive action space of multi-agent systems often necessitates leveraging the nearly-independent property of agents to accelerate learning.","To tackle this issue, we propose the MAZero algorithm, which combines a centralized model with Monte Carlo Tree Search (MCTS) for policy search.","We design a novel network structure to facilitate distributed execution and parameter sharing.","To enhance search efficiency in deterministic environments with sizable action spaces, we introduce two novel techniques: Optimistic Search Lambda (OS($\\lambda$)) and Advantage-Weighted Policy Optimization (AWPO).","Extensive experiments on the SMAC benchmark demonstrate that MAZero outperforms model-free approaches in terms of sample efficiency and provides comparable or better performance than existing model-based methods in terms of both sample and computational efficiency.","Our code is available at https://github.com/liuqh16/MAZero."],"url":"http://arxiv.org/abs/2405.11778v1","category":"cs.LG"}
{"created":"2024-05-20 04:19:53","title":"On the Design and Study of an Installation for Office Workers to Amplify Temporal Diversity and Connection to Nature","abstract":"We present the design and user study of an installation for office workers, enabling moments of temporal diversity and connection to nature. The installation is a form of creative computing experience that departs from the traditional focus on office technologies for productivity. Drawing on neuroscience insights and the slowing effect of nature sounds on time perception, we created an immersive, slow interaction, generative AI installation that composes an audiovisual space - serving as a perceptual portal into temporal realms beyond the linear rhythm of the office. Our study investigates the lived experiences of 18 office workers, gathered via explicitation interviews, observational notes, and video recordings, analysed through an inductive thematic analysis. Key findings highlight the ephemeral qualities in creative computing experiences using generative AI, its potential to foster contemplative practices, amplify ecological temporalities, and reshape office workers' engagement with their environment. Our design and user study offer research and practical implications for utilising creative computing to enrich office experiences.","sentences":["We present the design and user study of an installation for office workers, enabling moments of temporal diversity and connection to nature.","The installation is a form of creative computing experience that departs from the traditional focus on office technologies for productivity.","Drawing on neuroscience insights and the slowing effect of nature sounds on time perception, we created an immersive, slow interaction, generative AI installation that composes an audiovisual space - serving as a perceptual portal into temporal realms beyond the linear rhythm of the office.","Our study investigates the lived experiences of 18 office workers, gathered via explicitation interviews, observational notes, and video recordings, analysed through an inductive thematic analysis.","Key findings highlight the ephemeral qualities in creative computing experiences using generative AI, its potential to foster contemplative practices, amplify ecological temporalities, and reshape office workers' engagement with their environment.","Our design and user study offer research and practical implications for utilising creative computing to enrich office experiences."],"url":"http://arxiv.org/abs/2405.11772v1","category":"cs.HC"}
{"created":"2024-05-20 04:05:30","title":"Uni-Mol Docking V2: Towards Realistic and Accurate Binding Pose Prediction","abstract":"In recent years, machine learning (ML) methods have emerged as promising alternatives for molecular docking, offering the potential for high accuracy without incurring prohibitive computational costs. However, recent studies have indicated that these ML models may overfit to quantitative metrics while neglecting the physical constraints inherent in the problem. In this work, we present Uni-Mol Docking V2, which demonstrates a remarkable improvement in performance, accurately predicting the binding poses of 77+% of ligands in the PoseBusters benchmark with an RMSD value of less than 2.0 {\\AA}, and 75+% passing all quality checks. This represents a significant increase from the 62% achieved by the previous Uni-Mol Docking model. Notably, our Uni-Mol Docking approach generates chemically accurate predictions, circumventing issues such as chirality inversions and steric clashes that have plagued previous ML models. Furthermore, we observe enhanced performance in terms of high-quality predictions (RMSD values of less than 1.0 {\\AA} and 1.5 {\\AA}) and physical soundness when Uni-Mol Docking is combined with more physics-based methods like Uni-Dock. Our results represent a significant advancement in the application of artificial intelligence for scientific research, adopting a holistic approach to ligand docking that is well-suited for industrial applications in virtual screening and drug design. The code, data and service for Uni-Mol Docking are publicly available for use and further development in https://github.com/dptech-corp/Uni-Mol.","sentences":["In recent years, machine learning (ML) methods have emerged as promising alternatives for molecular docking, offering the potential for high accuracy without incurring prohibitive computational costs.","However, recent studies have indicated that these ML models may overfit to quantitative metrics while neglecting the physical constraints inherent in the problem.","In this work, we present Uni-Mol Docking V2, which demonstrates a remarkable improvement in performance, accurately predicting the binding poses of 77+% of ligands in the PoseBusters benchmark with an RMSD value of less than 2.0 {\\AA}, and 75+% passing all quality checks.","This represents a significant increase from the 62% achieved by the previous Uni-Mol Docking model.","Notably, our Uni-Mol Docking approach generates chemically accurate predictions, circumventing issues such as chirality inversions and steric clashes that have plagued previous ML models.","Furthermore, we observe enhanced performance in terms of high-quality predictions (RMSD values of less than 1.0 {\\AA} and 1.5 {\\AA}) and physical soundness when Uni-Mol Docking is combined with more physics-based methods like Uni-Dock.","Our results represent a significant advancement in the application of artificial intelligence for scientific research, adopting a holistic approach to ligand docking that is well-suited for industrial applications in virtual screening and drug design.","The code, data and service for Uni-Mol Docking are publicly available for use and further development in https://github.com/dptech-corp/Uni-Mol."],"url":"http://arxiv.org/abs/2405.11769v1","category":"q-bio.BM"}
{"created":"2024-05-20 03:57:55","title":"An Improved Design for All-Photonic Quantum Repeaters","abstract":"All-photonic quantum repeaters use multi-qubit photonic graph states, called repeater graph states (RGS), instead of matter-based quantum memories, for protection against predominantly loss errors. The RGS comprises tree-graph-encoded logical qubits for error correction at the repeaters and physical {\\em link} qubits to create entanglement between neighboring repeaters. The two methods to generate the RGS are probabilistic stitching -- using linear optical Bell state measurements (fusion) -- of small entangled states prepared via multiplexed-probabilistic linear optical circuits fed with single photons, and a direct deterministic preparation using a small number of quantum-logic-capable solid-state emitters. The resource overhead due to fusions and the circuit depth of the quantum emitter system both increase with the size of the RGS. Therefore engineering a resource-efficient RGS is crucial. We propose a new RGS design, which achieves a higher entanglement rate for all-photonic quantum repeaters using fewer qubits than the previously known RGS would. We accomplish this by boosting the probability of entangling neighboring repeaters with tree-encoded link qubits. We also propose a new adaptive scheme to perform logical BSM on the link qubits for loss-only errors. The adaptive BSM outperforms the previous schemes for logical BSM on tree codes when the qubit loss probability is uniform. It reduces the number of optical modes required to perform logical BSM on link qubits to improve the entanglement rate further.","sentences":["All-photonic quantum repeaters use multi-qubit photonic graph states, called repeater graph states (RGS), instead of matter-based quantum memories, for protection against predominantly loss errors.","The RGS comprises tree-graph-encoded logical qubits for error correction at the repeaters and physical {\\em link} qubits to create entanglement between neighboring repeaters.","The two methods to generate the RGS are probabilistic stitching -- using linear optical Bell state measurements (fusion) -- of small entangled states prepared via multiplexed-probabilistic linear optical circuits fed with single photons, and a direct deterministic preparation using a small number of quantum-logic-capable solid-state emitters.","The resource overhead due to fusions and the circuit depth of the quantum emitter system both increase with the size of the RGS.","Therefore engineering a resource-efficient RGS is crucial.","We propose a new RGS design, which achieves a higher entanglement rate for all-photonic quantum repeaters using fewer qubits than the previously known RGS would.","We accomplish this by boosting the probability of entangling neighboring repeaters with tree-encoded link qubits.","We also propose a new adaptive scheme to perform logical BSM on the link qubits for loss-only errors.","The adaptive BSM outperforms the previous schemes for logical BSM on tree codes when the qubit loss probability is uniform.","It reduces the number of optical modes required to perform logical BSM on link qubits to improve the entanglement rate further."],"url":"http://arxiv.org/abs/2405.11768v1","category":"quant-ph"}
{"created":"2024-05-20 03:55:44","title":"Multi-speaker Text-to-speech Training with Speaker Anonymized Data","abstract":"The trend of scaling up speech generation models poses a threat of biometric information leakage of the identities of the voices in the training data, raising privacy and security concerns. In this paper, we investigate training multi-speaker text-to-speech (TTS) models using data that underwent speaker anonymization (SA), a process that tends to hide the speaker identity of the input speech while maintaining other attributes. Two signal processing-based and three deep neural network-based SA methods were used to anonymize VCTK, a multi-speaker TTS dataset, which is further used to train an end-to-end TTS model, VITS, to perform unseen speaker TTS during the testing phase. We conducted extensive objective and subjective experiments to evaluate the anonymized training data, as well as the performance of the downstream TTS model trained using those data. Importantly, we found that UTMOS, a data-driven subjective rating predictor model, and GVD, a metric that measures the gain of voice distinctiveness, are good indicators of the downstream TTS performance. We summarize insights in the hope of helping future researchers determine the goodness of the SA system for multi-speaker TTS training.","sentences":["The trend of scaling up speech generation models poses a threat of biometric information leakage of the identities of the voices in the training data, raising privacy and security concerns.","In this paper, we investigate training multi-speaker text-to-speech (TTS) models using data that underwent speaker anonymization (SA), a process that tends to hide the speaker identity of the input speech while maintaining other attributes.","Two signal processing-based and three deep neural network-based SA methods were used to anonymize VCTK, a multi-speaker TTS dataset, which is further used to train an end-to-end TTS model, VITS, to perform unseen speaker TTS during the testing phase.","We conducted extensive objective and subjective experiments to evaluate the anonymized training data, as well as the performance of the downstream TTS model trained using those data.","Importantly, we found that UTMOS, a data-driven subjective rating predictor model, and GVD, a metric that measures the gain of voice distinctiveness, are good indicators of the downstream TTS performance.","We summarize insights in the hope of helping future researchers determine the goodness of the SA system for multi-speaker TTS training."],"url":"http://arxiv.org/abs/2405.11767v1","category":"eess.AS"}
{"created":"2024-05-20 03:52:41","title":"From SHAP Scores to Feature Importance Scores","abstract":"A central goal of eXplainable Artificial Intelligence (XAI) is to assign relative importance to the features of a Machine Learning (ML) model given some prediction. The importance of this task of explainability by feature attribution is illustrated by the ubiquitous recent use of tools such as SHAP and LIME. Unfortunately, the exact computation of feature attributions, using the game-theoretical foundation underlying SHAP and LIME, can yield manifestly unsatisfactory results, that tantamount to reporting misleading relative feature importance. Recent work targeted rigorous feature attribution, by studying axiomatic aggregations of features based on logic-based definitions of explanations by feature selection. This paper shows that there is an essential relationship between feature attribution and a priori voting power, and that those recently proposed axiomatic aggregations represent a few instantiations of the range of power indices studied in the past. Furthermore, it remains unclear how some of the most widely used power indices might be exploited as feature importance scores (FISs), i.e. the use of power indices in XAI, and which of these indices would be the best suited for the purposes of XAI by feature attribution, namely in terms of not producing results that could be deemed as unsatisfactory. This paper proposes novel desirable properties that FISs should exhibit. In addition, the paper also proposes novel FISs exhibiting the proposed properties. Finally, the paper conducts a rigorous analysis of the best-known power indices in terms of the proposed properties.","sentences":["A central goal of eXplainable Artificial Intelligence (XAI) is to assign relative importance to the features of a Machine Learning (ML) model given some prediction.","The importance of this task of explainability by feature attribution is illustrated by the ubiquitous recent use of tools such as SHAP and LIME.","Unfortunately, the exact computation of feature attributions, using the game-theoretical foundation underlying SHAP and LIME, can yield manifestly unsatisfactory results, that tantamount to reporting misleading relative feature importance.","Recent work targeted rigorous feature attribution, by studying axiomatic aggregations of features based on logic-based definitions of explanations by feature selection.","This paper shows that there is an essential relationship between feature attribution and a priori voting power, and that those recently proposed axiomatic aggregations represent a few instantiations of the range of power indices studied in the past.","Furthermore, it remains unclear how some of the most widely used power indices might be exploited as feature importance scores (FISs), i.e. the use of power indices in XAI, and which of these indices would be the best suited for the purposes of XAI by feature attribution, namely in terms of not producing results that could be deemed as unsatisfactory.","This paper proposes novel desirable properties that FISs should exhibit.","In addition, the paper also proposes novel FISs exhibiting the proposed properties.","Finally, the paper conducts a rigorous analysis of the best-known power indices in terms of the proposed properties."],"url":"http://arxiv.org/abs/2405.11766v1","category":"cs.AI"}
{"created":"2024-05-20 03:48:45","title":"DATR: Unsupervised Domain Adaptive Detection Transformer with Dataset-Level Adaptation and Prototypical Alignment","abstract":"Object detectors frequently encounter significant performance degradation when confronted with domain gaps between collected data (source domain) and data from real-world applications (target domain). To address this task, numerous unsupervised domain adaptive detectors have been proposed, leveraging carefully designed feature alignment techniques. However, these techniques primarily align instance-level features in a class-agnostic manner, overlooking the differences between extracted features from different categories, which results in only limited improvement. Furthermore, the scope of current alignment modules is often restricted to a limited batch of images, failing to learn the entire dataset-level cues, thereby severely constraining the detector's generalization ability to the target domain. To this end, we introduce a strong DETR-based detector named Domain Adaptive detection TRansformer (DATR) for unsupervised domain adaptation of object detection. Firstly, we propose the Class-wise Prototypes Alignment (CPA) module, which effectively aligns cross-domain features in a class-aware manner by bridging the gap between object detection task and domain adaptation task. Then, the designed Dataset-level Alignment Scheme (DAS) explicitly guides the detector to achieve global representation and enhance inter-class distinguishability of instance-level features across the entire dataset, which spans both domains, by leveraging contrastive learning. Moreover, DATR incorporates a mean-teacher based self-training framework, utilizing pseudo-labels generated by the teacher model to further mitigate domain bias. Extensive experimental results demonstrate superior performance and generalization capabilities of our proposed DATR in multiple domain adaptation scenarios. Code is released at https://github.com/h751410234/DATR.","sentences":["Object detectors frequently encounter significant performance degradation when confronted with domain gaps between collected data (source domain) and data from real-world applications (target domain).","To address this task, numerous unsupervised domain adaptive detectors have been proposed, leveraging carefully designed feature alignment techniques.","However, these techniques primarily align instance-level features in a class-agnostic manner, overlooking the differences between extracted features from different categories, which results in only limited improvement.","Furthermore, the scope of current alignment modules is often restricted to a limited batch of images, failing to learn the entire dataset-level cues, thereby severely constraining the detector's generalization ability to the target domain.","To this end, we introduce a strong DETR-based detector named Domain Adaptive detection TRansformer (DATR) for unsupervised domain adaptation of object detection.","Firstly, we propose the Class-wise Prototypes Alignment (CPA) module, which effectively aligns cross-domain features in a class-aware manner by bridging the gap between object detection task and domain adaptation task.","Then, the designed Dataset-level Alignment Scheme (DAS) explicitly guides the detector to achieve global representation and enhance inter-class distinguishability of instance-level features across the entire dataset, which spans both domains, by leveraging contrastive learning.","Moreover, DATR incorporates a mean-teacher based self-training framework, utilizing pseudo-labels generated by the teacher model to further mitigate domain bias.","Extensive experimental results demonstrate superior performance and generalization capabilities of our proposed DATR in multiple domain adaptation scenarios.","Code is released at https://github.com/h751410234/DATR."],"url":"http://arxiv.org/abs/2405.11765v1","category":"cs.CV"}
{"created":"2024-05-20 03:35:13","title":"Fed-Credit: Robust Federated Learning with Credibility Management","abstract":"Aiming at privacy preservation, Federated Learning (FL) is an emerging machine learning approach enabling model training on decentralized devices or data sources. The learning mechanism of FL relies on aggregating parameter updates from individual clients. However, this process may pose a potential security risk due to the presence of malicious devices. Existing solutions are either costly due to the use of compute-intensive technology, or restrictive for reasons of strong assumptions such as the prior knowledge of the number of attackers and how they attack. Few methods consider both privacy constraints and uncertain attack scenarios. In this paper, we propose a robust FL approach based on the credibility management scheme, called Fed-Credit. Unlike previous studies, our approach does not require prior knowledge of the nodes and the data distribution. It maintains and employs a credibility set, which weighs the historical clients' contributions based on the similarity between the local models and global model, to adjust the global model update. The subtlety of Fed-Credit is that the time decay and attitudinal value factor are incorporated into the dynamic adjustment of the reputation weights and it boasts a computational complexity of O(n) (n is the number of the clients). We conducted extensive experiments on the MNIST and CIFAR-10 datasets under 5 types of attacks. The results exhibit superior accuracy and resilience against adversarial attacks, all while maintaining comparatively low computational complexity. Among these, on the Non-IID CIFAR-10 dataset, our algorithm exhibited performance enhancements of 19.5% and 14.5%, respectively, in comparison to the state-of-the-art algorithm when dealing with two types of data poisoning attacks.","sentences":["Aiming at privacy preservation, Federated Learning (FL) is an emerging machine learning approach enabling model training on decentralized devices or data sources.","The learning mechanism of FL relies on aggregating parameter updates from individual clients.","However, this process may pose a potential security risk due to the presence of malicious devices.","Existing solutions are either costly due to the use of compute-intensive technology, or restrictive for reasons of strong assumptions such as the prior knowledge of the number of attackers and how they attack.","Few methods consider both privacy constraints and uncertain attack scenarios.","In this paper, we propose a robust FL approach based on the credibility management scheme, called Fed-Credit.","Unlike previous studies, our approach does not require prior knowledge of the nodes and the data distribution.","It maintains and employs a credibility set, which weighs the historical clients' contributions based on the similarity between the local models and global model, to adjust the global model update.","The subtlety of Fed-Credit is that the time decay and attitudinal value factor are incorporated into the dynamic adjustment of the reputation weights and it boasts a computational complexity of O(n) (n is the number of the clients).","We conducted extensive experiments on the MNIST and CIFAR-10 datasets under 5 types of attacks.","The results exhibit superior accuracy and resilience against adversarial attacks, all while maintaining comparatively low computational complexity.","Among these, on the Non-IID CIFAR-10 dataset, our algorithm exhibited performance enhancements of 19.5% and 14.5%, respectively, in comparison to the state-of-the-art algorithm when dealing with two types of data poisoning attacks."],"url":"http://arxiv.org/abs/2405.11758v1","category":"cs.LG"}
{"created":"2024-05-20 03:32:04","title":"Testing Gravity with Frequency-Dependent Overlap Reduction Function in Pulsar Timing Array","abstract":"The positive evidence of a nano-hertz gravitational wave background recently found by several pulsar timing array (PTA) collaborations opened up a window to test modified gravity theories in a unique frequency band in parallel to other gravitational wave detection experiments. In particular, the overlap reduction function (ORF) in PTA observation is sensitive to the phase velocity of gravitational waves. In this work, we provide analytical expressions for the coefficients of the multipole moments in the ORF, and utilize these analytical results to study constraints on the phase velocity from the frequency dependent overlap reduction function obtained from the Chinese PTA (CPTA) data. While the data contain large error bars yet, interesting constraints are found in the frequency-dependent ORF in the case of subluminal phase velocity. This makes us expect that the nano-hertz band gravitational wave background will become one of the important arenas for exploring modified gravity theories.","sentences":["The positive evidence of a nano-hertz gravitational wave background recently found by several pulsar timing array (PTA) collaborations opened up a window to test modified gravity theories in a unique frequency band in parallel to other gravitational wave detection experiments.","In particular, the overlap reduction function (ORF) in PTA observation is sensitive to the phase velocity of gravitational waves.","In this work, we provide analytical expressions for the coefficients of the multipole moments in the ORF, and utilize these analytical results to study constraints on the phase velocity from the frequency dependent overlap reduction function obtained from the Chinese PTA (CPTA) data.","While the data contain large error bars yet, interesting constraints are found in the frequency-dependent ORF in the case of subluminal phase velocity.","This makes us expect that the nano-hertz band gravitational wave background will become one of the important arenas for exploring modified gravity theories."],"url":"http://arxiv.org/abs/2405.11755v1","category":"astro-ph.CO"}
{"created":"2024-05-20 03:31:43","title":"Versatile Teacher: A Class-aware Teacher-student Framework for Cross-domain Adaptation","abstract":"Addressing the challenge of domain shift between datasets is vital in maintaining model performance. In the context of cross-domain object detection, the teacher-student framework, a widely-used semi-supervised model, has shown significant accuracy improvements. However, existing methods often overlook class differences, treating all classes equally, resulting in suboptimal results. Furthermore, the integration of instance-level alignment with a one-stage detector, essential due to the absence of a Region Proposal Network (RPN), remains unexplored in this framework. In response to these shortcomings, we introduce a novel teacher-student model named Versatile Teacher (VT). VT differs from previous works by considering class-specific detection difficulty and employing a two-step pseudo-label selection mechanism, referred to as Class-aware Pseudo-label Adaptive Selection (CAPS), to generate more reliable pseudo labels. These labels are leveraged as saliency matrices to guide the discriminator for targeted instance-level alignment. Our method demonstrates promising results on three benchmark datasets, and extends the alignment methods for widely-used one-stage detectors, presenting significant potential for practical applications. Code is available at https://github.com/RicardooYoung/VersatileTeacher.","sentences":["Addressing the challenge of domain shift between datasets is vital in maintaining model performance.","In the context of cross-domain object detection, the teacher-student framework, a widely-used semi-supervised model, has shown significant accuracy improvements.","However, existing methods often overlook class differences, treating all classes equally, resulting in suboptimal results.","Furthermore, the integration of instance-level alignment with a one-stage detector, essential due to the absence of a Region Proposal Network (RPN), remains unexplored in this framework.","In response to these shortcomings, we introduce a novel teacher-student model named Versatile Teacher (VT).","VT differs from previous works by considering class-specific detection difficulty and employing a two-step pseudo-label selection mechanism, referred to as Class-aware Pseudo-label Adaptive Selection (CAPS), to generate more reliable pseudo labels.","These labels are leveraged as saliency matrices to guide the discriminator for targeted instance-level alignment.","Our method demonstrates promising results on three benchmark datasets, and extends the alignment methods for widely-used one-stage detectors, presenting significant potential for practical applications.","Code is available at https://github.com/RicardooYoung/VersatileTeacher."],"url":"http://arxiv.org/abs/2405.11754v1","category":"cs.CV"}
{"created":"2024-05-20 03:26:58","title":"Foundation Model for Chemical Process Modeling: Meta-Learning with Physics-Informed Adaptation","abstract":"In this work, we introduce a novel application of foundation models in the domain of nonlinear chemical process modeling. Given the challenges of obtaining accurate first-principles models for real-world chemical processes and the inefficiency of rebuilding and retraining models for new chemical processes, we pose a pivotal question: What if we could develop a single, universal neural network (i.e., foundation model) capable of rapidly adapting to modeling any new chemical process? To address this question, we propose a meta-learning-based approach using Reptile to construct the foundation model, followed by physics-informed adaptation to fine-tune it to new modeling tasks using only a few data samples. To assess the effectiveness of our methodology, we construct a foundation model for various chemical reactions in three classical generic reactors, including continuous stirred tank reactors (CSTRs), batch reactors (BRs), and plug flow reactors (PFRs). Our approach outperforms conventional methods such as data-driven learning, physics-informed learning, transfer learning, and pure meta-learning in a few-shot setting. Furthermore, our method achieves rapid adaptation to new CSTRs, BRs, and PFRs using only a few data samples from the designated tasks. Source code is available at https://github.com/killingbear999/chemical-process-foundation-model.","sentences":["In this work, we introduce a novel application of foundation models in the domain of nonlinear chemical process modeling.","Given the challenges of obtaining accurate first-principles models for real-world chemical processes and the inefficiency of rebuilding and retraining models for new chemical processes, we pose a pivotal question: What if we could develop a single, universal neural network (i.e., foundation model) capable of rapidly adapting to modeling any new chemical process?","To address this question, we propose a meta-learning-based approach using Reptile to construct the foundation model, followed by physics-informed adaptation to fine-tune it to new modeling tasks using only a few data samples.","To assess the effectiveness of our methodology, we construct a foundation model for various chemical reactions in three classical generic reactors, including continuous stirred tank reactors (CSTRs), batch reactors (BRs), and plug flow reactors (PFRs).","Our approach outperforms conventional methods such as data-driven learning, physics-informed learning, transfer learning, and pure meta-learning in a few-shot setting.","Furthermore, our method achieves rapid adaptation to new CSTRs, BRs, and PFRs using only a few data samples from the designated tasks.","Source code is available at https://github.com/killingbear999/chemical-process-foundation-model."],"url":"http://arxiv.org/abs/2405.11752v1","category":"cs.CE"}
{"created":"2024-05-20 03:24:24","title":"Asymptotic theory of in-context learning by linear attention","abstract":"Transformers have a remarkable ability to learn and execute tasks based on examples provided within the input itself, without explicit prior training. It has been argued that this capability, known as in-context learning (ICL), is a cornerstone of Transformers' success, yet questions about the necessary sample complexity, pretraining task diversity, and context length for successful ICL remain unresolved. Here, we provide a precise answer to these questions in an exactly solvable model of ICL of a linear regression task by linear attention. We derive sharp asymptotics for the learning curve in a phenomenologically-rich scaling regime where the token dimension is taken to infinity; the context length and pretraining task diversity scale proportionally with the token dimension; and the number of pretraining examples scales quadratically. We demonstrate a double-descent learning curve with increasing pretraining examples, and uncover a phase transition in the model's behavior between low and high task diversity regimes: In the low diversity regime, the model tends toward memorization of training tasks, whereas in the high diversity regime, it achieves genuine in-context learning and generalization beyond the scope of pretrained tasks. These theoretical insights are empirically validated through experiments with both linear attention and full nonlinear Transformer architectures.","sentences":["Transformers have a remarkable ability to learn and execute tasks based on examples provided within the input itself, without explicit prior training.","It has been argued that this capability, known as in-context learning (ICL), is a cornerstone of Transformers' success, yet questions about the necessary sample complexity, pretraining task diversity, and context length for successful ICL remain unresolved.","Here, we provide a precise answer to these questions in an exactly solvable model of ICL of a linear regression task by linear attention.","We derive sharp asymptotics for the learning curve in a phenomenologically-rich scaling regime where the token dimension is taken to infinity; the context length and pretraining task diversity scale proportionally with the token dimension; and the number of pretraining examples scales quadratically.","We demonstrate a double-descent learning curve with increasing pretraining examples, and uncover a phase transition in the model's behavior between low and high task diversity regimes:","In the low diversity regime, the model tends toward memorization of training tasks, whereas in the high diversity regime, it achieves genuine in-context learning and generalization beyond the scope of pretrained tasks.","These theoretical insights are empirically validated through experiments with both linear attention and full nonlinear Transformer architectures."],"url":"http://arxiv.org/abs/2405.11751v1","category":"stat.ML"}
{"created":"2024-05-20 03:10:22","title":"Configurable Mirror Descent: Towards a Unification of Decision Making","abstract":"Decision-making problems, categorized as single-agent, e.g., Atari, cooperative multi-agent, e.g., Hanabi, competitive multi-agent, e.g., Hold'em poker, and mixed cooperative and competitive, e.g., football, are ubiquitous in the real world. Various methods are proposed to address the specific decision-making problems. Despite the successes in specific categories, these methods typically evolve independently and cannot generalize to other categories. Therefore, a fundamental question for decision-making is: \\emph{Can we develop \\textbf{a single algorithm} to tackle \\textbf{ALL} categories of decision-making problems?} There are several main challenges to address this question: i) different decision-making categories involve different numbers of agents and different relationships between agents, ii) different categories have different solution concepts and evaluation measures, and iii) there lacks a comprehensive benchmark covering all the categories. This work presents a preliminary attempt to address the question with three main contributions. i) We propose the generalized mirror descent (GMD), a generalization of MD variants, which considers multiple historical policies and works with a broader class of Bregman divergences. ii) We propose the configurable mirror descent (CMD) where a meta-controller is introduced to dynamically adjust the hyper-parameters in GMD conditional on the evaluation measures. iii) We construct the \\textsc{GameBench} with 15 academic-friendly games across different decision-making categories. Extensive experiments demonstrate that CMD achieves empirically competitive or better outcomes compared to baselines while providing the capability of exploring diverse dimensions of decision making.","sentences":["Decision-making problems, categorized as single-agent, e.g., Atari, cooperative multi-agent, e.g., Hanabi, competitive multi-agent, e.g., Hold'em poker, and mixed cooperative and competitive, e.g., football, are ubiquitous in the real world.","Various methods are proposed to address the specific decision-making problems.","Despite the successes in specific categories, these methods typically evolve independently and cannot generalize to other categories.","Therefore, a fundamental question for decision-making is: \\emph{Can we develop \\textbf{a single algorithm} to tackle \\textbf{ALL} categories of decision-making problems?}","There are several main challenges to address this question: i) different decision-making categories involve different numbers of agents and different relationships between agents, ii) different categories have different solution concepts and evaluation measures, and iii) there lacks a comprehensive benchmark covering all the categories.","This work presents a preliminary attempt to address the question with three main contributions.","i)","We propose the generalized mirror descent (GMD), a generalization of MD variants, which considers multiple historical policies and works with a broader class of Bregman divergences.","ii)","We propose the configurable mirror descent (CMD) where a meta-controller is introduced to dynamically adjust the hyper-parameters in GMD conditional on the evaluation measures.","iii) We construct the \\textsc{GameBench} with 15 academic-friendly games across different decision-making categories.","Extensive experiments demonstrate that CMD achieves empirically competitive or better outcomes compared to baselines while providing the capability of exploring diverse dimensions of decision making."],"url":"http://arxiv.org/abs/2405.11746v1","category":"cs.AI"}
{"created":"2024-05-20 03:07:30","title":"Density functions for the overdamped generalized Langevin equation and its Euler--Maruyama method: smoothness and convergence","abstract":"This paper focuses on studying the convergence rate of the density function of the Euler--Maruyama (EM) method, when applied to the overdamped generalized Langevin equation with fractional noise which serves as an important model in many fields. Firstly, we give an improved upper bound estimate for the total variation distance between random variables by their Malliavin--Sobolev norms. Secondly, we establish the existence and smoothness of the density function for both the exact solution and the numerical one. Based on the above results, the convergence rate of the density function of the numerical solution is obtained, which relies on the regularity of the noise and kernel. This convergence result provides a powerful support for numerically capturing the statistical information of the exact solution through the EM method.","sentences":["This paper focuses on studying the convergence rate of the density function of the Euler--Maruyama (EM) method, when applied to the overdamped generalized Langevin equation with fractional noise which serves as an important model in many fields.","Firstly, we give an improved upper bound estimate for the total variation distance between random variables by their Malliavin--Sobolev norms.","Secondly, we establish the existence and smoothness of the density function for both the exact solution and the numerical one.","Based on the above results, the convergence rate of the density function of the numerical solution is obtained, which relies on the regularity of the noise and kernel.","This convergence result provides a powerful support for numerically capturing the statistical information of the exact solution through the EM method."],"url":"http://arxiv.org/abs/2405.11744v1","category":"math.NA"}
{"created":"2024-05-20 03:01:43","title":"A General Theory for Compositional Generalization","abstract":"Compositional Generalization (CG) embodies the ability to comprehend novel combinations of familiar concepts, representing a significant cognitive leap in human intellectual advancement. Despite its critical importance, the deep neural network (DNN) faces challenges in addressing the compositional generalization problem, prompting considerable research interest. However, existing theories often rely on task-specific assumptions, constraining the comprehensive understanding of CG. This study aims to explore compositional generalization from a task-agnostic perspective, offering a complementary viewpoint to task-specific analyses. The primary challenge is to define CG without overly restricting its scope, a feat achieved by identifying its fundamental characteristics and basing the definition on them. Using this definition, we seek to answer the question \"what does the ultimate solution to CG look like?\" through the following theoretical findings: 1) the first No Free Lunch theorem in CG, indicating the absence of general solutions; 2) a novel generalization bound applicable to any CG problem, specifying the conditions for an effective CG solution; and 3) the introduction of the generative effect to enhance understanding of CG problems and their solutions. This paper's significance lies in providing a general theory for CG problems, which, when combined with prior theorems under task-specific scenarios, can lead to a comprehensive understanding of CG.","sentences":["Compositional Generalization (CG) embodies the ability to comprehend novel combinations of familiar concepts, representing a significant cognitive leap in human intellectual advancement.","Despite its critical importance, the deep neural network (DNN) faces challenges in addressing the compositional generalization problem, prompting considerable research interest.","However, existing theories often rely on task-specific assumptions, constraining the comprehensive understanding of CG.","This study aims to explore compositional generalization from a task-agnostic perspective, offering a complementary viewpoint to task-specific analyses.","The primary challenge is to define CG without overly restricting its scope, a feat achieved by identifying its fundamental characteristics and basing the definition on them.","Using this definition, we seek to answer the question \"what does the ultimate solution to CG look like?\"","through the following theoretical findings: 1) the first No Free Lunch theorem in CG, indicating the absence of general solutions; 2) a novel generalization bound applicable to any CG problem, specifying the conditions for an effective CG solution; and 3) the introduction of the generative effect to enhance understanding of CG problems and their solutions.","This paper's significance lies in providing a general theory for CG problems, which, when combined with prior theorems under task-specific scenarios, can lead to a comprehensive understanding of CG."],"url":"http://arxiv.org/abs/2405.11743v1","category":"cs.LG"}
{"created":"2024-05-20 03:00:49","title":"Universal Organizer of SAM for Unsupervised Semantic Segmentation","abstract":"Unsupervised semantic segmentation (USS) aims to achieve high-quality segmentation without manual pixel-level annotations. Existing USS models provide coarse category classification for regions, but the results often have blurry and imprecise edges. Recently, a robust framework called the segment anything model (SAM) has been proven to deliver precise boundary object masks. Therefore, this paper proposes a universal organizer based on SAM, termed as UO-SAM, to enhance the mask quality of USS models. Specifically, using only the original image and the masks generated by the USS model, we extract visual features to obtain positional prompts for target objects. Then, we activate a local region optimizer that performs segmentation using SAM on a per-object basis. Finally, we employ a global region optimizer to incorporate global image information and refine the masks to obtain the final fine-grained masks. Compared to existing methods, our UO-SAM achieves state-of-the-art performance.","sentences":["Unsupervised semantic segmentation (USS) aims to achieve high-quality segmentation without manual pixel-level annotations.","Existing USS models provide coarse category classification for regions, but the results often have blurry and imprecise edges.","Recently, a robust framework called the segment anything model (SAM) has been proven to deliver precise boundary object masks.","Therefore, this paper proposes a universal organizer based on SAM, termed as UO-SAM, to enhance the mask quality of USS models.","Specifically, using only the original image and the masks generated by the USS model, we extract visual features to obtain positional prompts for target objects.","Then, we activate a local region optimizer that performs segmentation using SAM on a per-object basis.","Finally, we employ a global region optimizer to incorporate global image information and refine the masks to obtain the final fine-grained masks.","Compared to existing methods, our UO-SAM achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.11742v1","category":"cs.MM"}
{"created":"2024-05-20 02:43:04","title":"Learning Future Representation with Synthetic Observations for Sample-efficient Reinforcement Learning","abstract":"In visual Reinforcement Learning (RL), upstream representation learning largely determines the effect of downstream policy learning. Employing auxiliary tasks allows the agent to enhance visual representation in a targeted manner, thereby improving the sample efficiency and performance of downstream RL. Prior advanced auxiliary tasks all focus on how to extract as much information as possible from limited experience (including observations, actions, and rewards) through their different auxiliary objectives, whereas in this article, we first start from another perspective: auxiliary training data. We try to improve auxiliary representation learning for RL by enriching auxiliary training data, proposing \\textbf{L}earning \\textbf{F}uture representation with \\textbf{S}ynthetic observations \\textbf{(LFS)}, a novel self-supervised RL approach. Specifically, we propose a training-free method to synthesize observations that may contain future information, as well as a data selection approach to eliminate unqualified synthetic noise. The remaining synthetic observations and real observations then serve as the auxiliary data to achieve a clustering-based temporal association task for representation learning. LFS allows the agent to access and learn observations that have not yet appeared in advance, so as to quickly understand and exploit them when they occur later. In addition, LFS does not rely on rewards or actions, which means it has a wider scope of application (e.g., learning from video) than recent advanced auxiliary tasks. Extensive experiments demonstrate that our LFS exhibits state-of-the-art RL sample efficiency on challenging continuous control and enables advanced visual pre-training based on action-free video demonstrations.","sentences":["In visual Reinforcement Learning (RL), upstream representation learning largely determines the effect of downstream policy learning.","Employing auxiliary tasks allows the agent to enhance visual representation in a targeted manner, thereby improving the sample efficiency and performance of downstream RL.","Prior advanced auxiliary tasks all focus on how to extract as much information as possible from limited experience (including observations, actions, and rewards) through their different auxiliary objectives, whereas in this article, we first start from another perspective: auxiliary training data.","We try to improve auxiliary representation learning for RL by enriching auxiliary training data, proposing \\textbf{L}earning \\textbf{F}uture representation with \\textbf{S}ynthetic observations \\textbf{(LFS)}, a novel self-supervised RL approach.","Specifically, we propose a training-free method to synthesize observations that may contain future information, as well as a data selection approach to eliminate unqualified synthetic noise.","The remaining synthetic observations and real observations then serve as the auxiliary data to achieve a clustering-based temporal association task for representation learning.","LFS allows the agent to access and learn observations that have not yet appeared in advance, so as to quickly understand and exploit them when they occur later.","In addition, LFS does not rely on rewards or actions, which means it has a wider scope of application (e.g., learning from video) than recent advanced auxiliary tasks.","Extensive experiments demonstrate that our LFS exhibits state-of-the-art RL sample efficiency on challenging continuous control and enables advanced visual pre-training based on action-free video demonstrations."],"url":"http://arxiv.org/abs/2405.11740v1","category":"cs.LG"}
{"created":"2024-05-20 02:41:21","title":"Contactless Polysomnography: What Radio Waves Tell Us about Sleep","abstract":"The ability to assess sleep at home, capture sleep stages, and detect the occurrence of apnea (without on-body sensors) simply by analyzing the radio waves bouncing off people's bodies while they sleep is quite powerful. Such a capability would allow for longitudinal data collection in patients' homes, informing our understanding of sleep and its interaction with various diseases and their therapeutic responses, both in clinical trials and routine care. In this article, we develop an advanced machine learning algorithm for passively monitoring sleep and nocturnal breathing from radio waves reflected off people while asleep. Validation results in comparison with the gold standard (i.e., polysomnography) (n=849) demonstrate that the model captures the sleep hypnogram (with an accuracy of 81% for 30-second epochs categorized into Wake, Light Sleep, Deep Sleep, or REM), detects sleep apnea (AUROC = 0.88), and measures the patient's Apnea-Hypopnea Index (ICC=0.95; 95% CI = [0.93, 0.97]). Notably, the model exhibits equitable performance across race, sex, and age. Moreover, the model uncovers informative interactions between sleep stages and a range of diseases including neurological, psychiatric, cardiovascular, and immunological disorders. These findings not only hold promise for clinical practice and interventional trials but also underscore the significance of sleep as a fundamental component in understanding and managing various diseases.","sentences":["The ability to assess sleep at home, capture sleep stages, and detect the occurrence of apnea (without on-body sensors) simply by analyzing the radio waves bouncing off people's bodies while they sleep is quite powerful.","Such a capability would allow for longitudinal data collection in patients' homes, informing our understanding of sleep and its interaction with various diseases and their therapeutic responses, both in clinical trials and routine care.","In this article, we develop an advanced machine learning algorithm for passively monitoring sleep and nocturnal breathing from radio waves reflected off people while asleep.","Validation results in comparison with the gold standard (i.e., polysomnography) (n=849) demonstrate that the model captures the sleep hypnogram (with an accuracy of 81% for 30-second epochs categorized into Wake, Light Sleep, Deep Sleep, or REM), detects sleep apnea (AUROC = 0.88), and measures the patient's Apnea-Hypopnea Index (ICC=0.95; 95% CI =","[0.93, 0.97]).","Notably, the model exhibits equitable performance across race, sex, and age.","Moreover, the model uncovers informative interactions between sleep stages and a range of diseases including neurological, psychiatric, cardiovascular, and immunological disorders.","These findings not only hold promise for clinical practice and interventional trials but also underscore the significance of sleep as a fundamental component in understanding and managing various diseases."],"url":"http://arxiv.org/abs/2405.11739v1","category":"cs.LG"}
{"created":"2024-05-20 02:38:38","title":"Diffusion Models for Generating Ballistic Spacecraft Trajectories","abstract":"Generative modeling has drawn much attention in creative and scientific data generation tasks. Score-based Diffusion Models, a type of generative model that iteratively learns to denoise data, have shown state-of-the-art results on tasks such as image generation, multivariate time series forecasting, and robotic trajectory planning. Using score-based diffusion models, this work implements a novel generative framework to generate ballistic transfers from Earth to Mars. We further analyze the model's ability to learn the characteristics of the original dataset and its ability to produce transfers that follow the underlying dynamics. Ablation studies were conducted to determine how model performance varies with model size and trajectory temporal resolution. In addition, a performance benchmark is designed to assess the generative model's usefulness for trajectory design, conduct model performance comparisons, and lay the groundwork for evaluating different generative models for trajectory design beyond diffusion. The results of this analysis showcase several useful properties of diffusion models that, when taken together, can enable a future system for generative trajectory design powered by diffusion models.","sentences":["Generative modeling has drawn much attention in creative and scientific data generation tasks.","Score-based Diffusion Models, a type of generative model that iteratively learns to denoise data, have shown state-of-the-art results on tasks such as image generation, multivariate time series forecasting, and robotic trajectory planning.","Using score-based diffusion models, this work implements a novel generative framework to generate ballistic transfers from Earth to Mars.","We further analyze the model's ability to learn the characteristics of the original dataset and its ability to produce transfers that follow the underlying dynamics.","Ablation studies were conducted to determine how model performance varies with model size and trajectory temporal resolution.","In addition, a performance benchmark is designed to assess the generative model's usefulness for trajectory design, conduct model performance comparisons, and lay the groundwork for evaluating different generative models for trajectory design beyond diffusion.","The results of this analysis showcase several useful properties of diffusion models that, when taken together, can enable a future system for generative trajectory design powered by diffusion models."],"url":"http://arxiv.org/abs/2405.11738v1","category":"cs.RO"}
{"created":"2024-05-20 02:33:21","title":"Simulating a Chern Insulator with C = $\\pm$2 on Synthetic Floquet Lattice","abstract":"The synthetic Floquet lattice, generated by multiple strong drives with mutually incommensurate frequencies, provides a powerful platform for the quantum simulation of topological phenomena. In this study, we propose a 4-band tight-binding model of the Chern insulator with a Chern number C = $\\pm$2 by coupling two layers of the half-BHZ lattice and subsequently mapping it onto the Floquet lattice to simulate its topological properties. To determine the Chern number of our Floquet-version model, we extend the energy pumping method proposed by Martin et al. [Phys. Rev. X 7, 041008 (2017)] and the topological oscillation method introduced by Boyers et al. [Phys. Rev. Lett. 125, 160505 (2020)], followed by numerical simulations for both methodologies. The simulation results demonstrate the successful extraction of the Chern number using either of these methods, providing an excellent prediction of the phase diagram that closely aligns with the theoretical one derived from the original bilayer half-BHZ model. Finally, we briefly discuss a potential experimental implementation for our model. Our work demonstrates significant potential for simulating complex topological matter using quantum computing platforms, thereby paving the way for constructing a more universal simulator for non-interacting topological quantum states and advancing our understanding of these intriguing phenomena.","sentences":["The synthetic Floquet lattice, generated by multiple strong drives with mutually incommensurate frequencies, provides a powerful platform for the quantum simulation of topological phenomena.","In this study, we propose a 4-band tight-binding model of the Chern insulator with a Chern number C = $\\pm$2 by coupling two layers of the half-BHZ lattice and subsequently mapping it onto the Floquet lattice to simulate its topological properties.","To determine the Chern number of our Floquet-version model, we extend the energy pumping method proposed by Martin et al.","[Phys. Rev. X 7, 041008 (2017)] and the topological oscillation method introduced by Boyers et al.","[Phys. Rev. Lett.","125, 160505 (2020)], followed by numerical simulations for both methodologies.","The simulation results demonstrate the successful extraction of the Chern number using either of these methods, providing an excellent prediction of the phase diagram that closely aligns with the theoretical one derived from the original bilayer half-BHZ model.","Finally, we briefly discuss a potential experimental implementation for our model.","Our work demonstrates significant potential for simulating complex topological matter using quantum computing platforms, thereby paving the way for constructing a more universal simulator for non-interacting topological quantum states and advancing our understanding of these intriguing phenomena."],"url":"http://arxiv.org/abs/2405.11733v1","category":"quant-ph"}
{"created":"2024-05-20 02:32:46","title":"Quality assurance of organs-at-risk delineation in radiotherapy","abstract":"The delineation of tumor target and organs-at-risk is critical in the radiotherapy treatment planning. Automatic segmentation can be used to reduce the physician workload and improve the consistency. However, the quality assurance of the automatic segmentation is still an unmet need in clinical practice. The patient data used in our study was a standardized dataset from AAPM Thoracic Auto-Segmentation Challenge. The OARs included were left and right lungs, heart, esophagus, and spinal cord. Two groups of OARs were generated, the benchmark dataset manually contoured by experienced physicians and the test dataset automatically created using a software AccuContour. A resnet-152 network was performed as feature extractor, and one-class support vector classifier was used to determine the high or low quality. We evaluate the model performance with balanced accuracy, F-score, sensitivity, specificity and the area under the receiving operator characteristic curve. We randomly generated contour errors to assess the generalization of our method, explored the detection limit, and evaluated the correlations between detection limit and various metrics such as volume, Dice similarity coefficient, Hausdorff distance, and mean surface distance. The proposed one-class classifier outperformed in metrics such as balanced accuracy, AUC, and others. The proposed method showed significant improvement over binary classifiers in handling various types of errors. Our proposed model, which introduces residual network and attention mechanism in the one-class classification framework, was able to detect the various types of OAR contour errors with high accuracy. The proposed method can significantly reduce the burden of physician review for contour delineation.","sentences":["The delineation of tumor target and organs-at-risk is critical in the radiotherapy treatment planning.","Automatic segmentation can be used to reduce the physician workload and improve the consistency.","However, the quality assurance of the automatic segmentation is still an unmet need in clinical practice.","The patient data used in our study was a standardized dataset from AAPM Thoracic Auto-Segmentation Challenge.","The OARs included were left and right lungs, heart, esophagus, and spinal cord.","Two groups of OARs were generated, the benchmark dataset manually contoured by experienced physicians and the test dataset automatically created using a software AccuContour.","A resnet-152 network was performed as feature extractor, and one-class support vector classifier was used to determine the high or low quality.","We evaluate the model performance with balanced accuracy, F-score, sensitivity, specificity and the area under the receiving operator characteristic curve.","We randomly generated contour errors to assess the generalization of our method, explored the detection limit, and evaluated the correlations between detection limit and various metrics such as volume, Dice similarity coefficient, Hausdorff distance, and mean surface distance.","The proposed one-class classifier outperformed in metrics such as balanced accuracy, AUC, and others.","The proposed method showed significant improvement over binary classifiers in handling various types of errors.","Our proposed model, which introduces residual network and attention mechanism in the one-class classification framework, was able to detect the various types of OAR contour errors with high accuracy.","The proposed method can significantly reduce the burden of physician review for contour delineation."],"url":"http://arxiv.org/abs/2405.11732v1","category":"cs.CV"}
{"created":"2024-05-20 02:26:21","title":"Stabilization of vapor-rich bubble in ethanol/water mixtures and enhanced flow around the bubble","abstract":"This study investigates the behavior of microbubbles generated by the local heating of an ethanol/water mixture and the surrounding flow. The mixture is photothermally heated by focusing a continuous-wave laser on a FeSi$_2$ thin film. Although the liquid is not degassed, vapor-rich bubbles are stably generated in an ethanol concentration range of 1.5-50 wt% The vapor-rich bubbles absorb the air dissolved in the surrounding liquid and exhale it continuously as air-rich bubbles $\\sim$ 1 {\\mu}m in diameter. For the same ethanol concentration range, the solutal-Marangoni force becomes dominant relative to the thermal-Marangoni force, and the air-rich bubbles are pushed away from the high-temperature region in the fluid toward the low-temperature region. Further, it was experimentally demonstrated that Marangoni forces do not significantly affect the surface of vapor-rich bubbles generated in ethanol/water mixtures, and they produce a flow from the high-temperature to the low-temperature region on the vapor-rich bubbles, which moves the exhaled air-rich bubbles away from the vapor-rich bubbles near the heat source. These effects prevent the vapor-rich and exhaled air-rich bubbles from recombining, thereby resulting in the long-term stability of the former. Moreover, the flow produced by the vapor-rich bubbles in the non-degassed 0-20 wt% ethanol/water mixture was stronger than that in degassed water. The maximum flow speed is achieved for an ethanol concentration of 5 wt%, which is 6-11 times higher than that when degassed water is utilized. The ethanol/water mixture produces vapor-rich bubbles without a degassing liquid and enhances the flow speed generated by the vapor-rich bubbles. This flow is expected to apply to driving and mixing microfluids.","sentences":["This study investigates the behavior of microbubbles generated by the local heating of an ethanol/water mixture and the surrounding flow.","The mixture is photothermally heated by focusing a continuous-wave laser on a FeSi$_2$ thin film.","Although the liquid is not degassed, vapor-rich bubbles are stably generated in an ethanol concentration range of 1.5-50 wt% The vapor-rich bubbles absorb the air dissolved in the surrounding liquid and exhale it continuously as air-rich bubbles $\\sim$ 1 {\\mu}m in diameter.","For the same ethanol concentration range, the solutal-Marangoni force becomes dominant relative to the thermal-Marangoni force, and the air-rich bubbles are pushed away from the high-temperature region in the fluid toward the low-temperature region.","Further, it was experimentally demonstrated that Marangoni forces do not significantly affect the surface of vapor-rich bubbles generated in ethanol/water mixtures, and they produce a flow from the high-temperature to the low-temperature region on the vapor-rich bubbles, which moves the exhaled air-rich bubbles away from the vapor-rich bubbles near the heat source.","These effects prevent the vapor-rich and exhaled air-rich bubbles from recombining, thereby resulting in the long-term stability of the former.","Moreover, the flow produced by the vapor-rich bubbles in the non-degassed 0-20 wt% ethanol/water mixture was stronger than that in degassed water.","The maximum flow speed is achieved for an ethanol concentration of 5 wt%, which is 6-11 times higher than that when degassed water is utilized.","The ethanol/water mixture produces vapor-rich bubbles without a degassing liquid and enhances the flow speed generated by the vapor-rich bubbles.","This flow is expected to apply to driving and mixing microfluids."],"url":"http://arxiv.org/abs/2405.11731v1","category":"physics.flu-dyn"}
{"created":"2024-05-20 02:24:36","title":"Degree of Irrationality: Sentiment and Implied Volatility Surface","abstract":"In this study, we constructed daily high-frequency sentiment data and used the VAR method to attempt to predict the next day's implied volatility surface. We utilized 630,000 text data entries from the East Money Stock Forum from 2014 to 2023 and employed deep learning methods such as BERT and LSTM to build daily market sentiment indicators. By applying FFT and EMD methods for sentiment decomposition, we found that high-frequency sentiment had a stronger correlation with at-the-money (ATM) options' implied volatility, while low-frequency sentiment was more strongly correlated with deep out-of-the-money (DOTM) options' implied volatility. Further analysis revealed that the shape of the implied volatility surface contains richer market sentiment information beyond just market panic. We demonstrated that incorporating this sentiment information can improve the accuracy of implied volatility surface predictions.","sentences":["In this study, we constructed daily high-frequency sentiment data and used the VAR method to attempt to predict the next day's implied volatility surface.","We utilized 630,000 text data entries from the East Money Stock Forum from 2014 to 2023 and employed deep learning methods such as BERT and LSTM to build daily market sentiment indicators.","By applying FFT and EMD methods for sentiment decomposition, we found that high-frequency sentiment had a stronger correlation with at-the-money (ATM) options' implied volatility, while low-frequency sentiment was more strongly correlated with deep out-of-the-money (DOTM) options' implied volatility.","Further analysis revealed that the shape of the implied volatility surface contains richer market sentiment information beyond just market panic.","We demonstrated that incorporating this sentiment information can improve the accuracy of implied volatility surface predictions."],"url":"http://arxiv.org/abs/2405.11730v1","category":"cs.LG"}
{"created":"2024-05-20 02:09:07","title":"Highway Graph to Accelerate Reinforcement Learning","abstract":"Reinforcement Learning (RL) algorithms often suffer from low training efficiency. A strategy to mitigate this issue is to incorporate a model-based planning algorithm, such as Monte Carlo Tree Search (MCTS) or Value Iteration (VI), into the environmental model. The major limitation of VI is the need to iterate over a large tensor. These still lead to intensive computations. We focus on improving the training efficiency of RL algorithms by improving the efficiency of the value learning process. For the deterministic environments with discrete state and action spaces, a non-branching sequence of transitions moves the agent without deviating from intermediate states, which we call a highway. On such non-branching highways, the value-updating process can be merged as a one-step process instead of iterating the value step-by-step. Based on this observation, we propose a novel graph structure, named highway graph, to model the state transition. Our highway graph compresses the transition model into a concise graph, where edges can represent multiple state transitions to support value propagation across multiple time steps in each iteration. We thus can obtain a more efficient value learning approach by facilitating the VI algorithm on highway graphs. By integrating the highway graph into RL (as a model-based off-policy RL method), the RL training can be remarkably accelerated in the early stages (within 1 million frames). Comparison against various baselines on four categories of environments reveals that our method outperforms both representative and novel model-free and model-based RL algorithms, demonstrating 10 to more than 150 times more efficiency while maintaining an equal or superior expected return, as confirmed by carefully conducted analyses. Moreover, a deep neural network-based agent is trained using the highway graph, resulting in better generalization and lower storage costs.","sentences":["Reinforcement Learning (RL) algorithms often suffer from low training efficiency.","A strategy to mitigate this issue is to incorporate a model-based planning algorithm, such as Monte Carlo Tree Search (MCTS) or Value Iteration (VI), into the environmental model.","The major limitation of VI is the need to iterate over a large tensor.","These still lead to intensive computations.","We focus on improving the training efficiency of RL algorithms by improving the efficiency of the value learning process.","For the deterministic environments with discrete state and action spaces, a non-branching sequence of transitions moves the agent without deviating from intermediate states, which we call a highway.","On such non-branching highways, the value-updating process can be merged as a one-step process instead of iterating the value step-by-step.","Based on this observation, we propose a novel graph structure, named highway graph, to model the state transition.","Our highway graph compresses the transition model into a concise graph, where edges can represent multiple state transitions to support value propagation across multiple time steps in each iteration.","We thus can obtain a more efficient value learning approach by facilitating the VI algorithm on highway graphs.","By integrating the highway graph into RL (as a model-based off-policy RL method), the RL training can be remarkably accelerated in the early stages (within 1 million frames).","Comparison against various baselines on four categories of environments reveals that our method outperforms both representative and novel model-free and model-based RL algorithms, demonstrating 10 to more than 150 times more efficiency while maintaining an equal or superior expected return, as confirmed by carefully conducted analyses.","Moreover, a deep neural network-based agent is trained using the highway graph, resulting in better generalization and lower storage costs."],"url":"http://arxiv.org/abs/2405.11727v1","category":"cs.LG"}
{"created":"2024-05-20 01:57:34","title":"Token-wise Influential Training Data Retrieval for Large Language Models","abstract":"Given a Large Language Model (LLM) generation, how can we identify which training data led to this generation? In this paper, we proposed RapidIn, a scalable framework adapting to LLMs for estimating the influence of each training data. The proposed framework consists of two stages: caching and retrieval. First, we compress the gradient vectors by over 200,000x, allowing them to be cached on disk or in GPU/CPU memory. Then, given a generation, RapidIn efficiently traverses the cached gradients to estimate the influence within minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports multi-GPU parallelization to substantially accelerate caching and retrieval. Our empirical result confirms the efficiency and effectiveness of RapidIn.","sentences":["Given a Large Language Model (LLM) generation, how can we identify which training data led to this generation?","In this paper, we proposed RapidIn, a scalable framework adapting to LLMs for estimating the influence of each training data.","The proposed framework consists of two stages: caching and retrieval.","First, we compress the gradient vectors by over 200,000x, allowing them to be cached on disk or in GPU/CPU memory.","Then, given a generation, RapidIn efficiently traverses the cached gradients to estimate the influence within minutes, achieving over a 6,326x speedup.","Moreover, RapidIn supports multi-GPU parallelization to substantially accelerate caching and retrieval.","Our empirical result confirms the efficiency and effectiveness of RapidIn."],"url":"http://arxiv.org/abs/2405.11724v1","category":"cs.CL"}
{"created":"2024-05-20 01:50:35","title":"Inference with non-differentiable surrogate loss in a general high-dimensional classification framework","abstract":"Penalized empirical risk minimization with a surrogate loss function is often used to derive a high-dimensional linear decision rule in classification problems. Although much of the literature focuses on the generalization error, there is a lack of valid inference procedures to identify the driving factors of the estimated decision rule, especially when the surrogate loss is non-differentiable. In this work, we propose a kernel-smoothed decorrelated score to construct hypothesis testing and interval estimations for the linear decision rule estimated using a piece-wise linear surrogate loss, which has a discontinuous gradient and non-regular Hessian. Specifically, we adopt kernel approximations to smooth the discontinuous gradient near discontinuity points and approximate the non-regular Hessian of the surrogate loss. In applications where additional nuisance parameters are involved, we propose a novel cross-fitted version to accommodate flexible nuisance estimates and kernel approximations. We establish the limiting distribution of the kernel-smoothed decorrelated score and its cross-fitted version in a high-dimensional setup. Simulation and real data analysis are conducted to demonstrate the validity and superiority of the proposed method.","sentences":["Penalized empirical risk minimization with a surrogate loss function is often used to derive a high-dimensional linear decision rule in classification problems.","Although much of the literature focuses on the generalization error, there is a lack of valid inference procedures to identify the driving factors of the estimated decision rule, especially when the surrogate loss is non-differentiable.","In this work, we propose a kernel-smoothed decorrelated score to construct hypothesis testing and interval estimations for the linear decision rule estimated using a piece-wise linear surrogate loss, which has a discontinuous gradient and non-regular Hessian.","Specifically, we adopt kernel approximations to smooth the discontinuous gradient near discontinuity points and approximate the non-regular Hessian of the surrogate loss.","In applications where additional nuisance parameters are involved, we propose a novel cross-fitted version to accommodate flexible nuisance estimates and kernel approximations.","We establish the limiting distribution of the kernel-smoothed decorrelated score and its cross-fitted version in a high-dimensional setup.","Simulation and real data analysis are conducted to demonstrate the validity and superiority of the proposed method."],"url":"http://arxiv.org/abs/2405.11723v1","category":"stat.ME"}
{"created":"2024-05-20 01:47:28","title":"AI Algorithm for Predicting and Optimizing Trajectory of UAV Swarm","abstract":"This paper explores the application of Artificial Intelligence (AI) techniques for generating the trajectories of fleets of Unmanned Aerial Vehicles (UAVs). The two main challenges addressed include accurately predicting the paths of UAVs and efficiently avoiding collisions between them. Firstly, the paper systematically applies a diverse set of activation functions to a Feedforward Neural Network (FFNN) with a single hidden layer, which enhances the accuracy of the predicted path compared to previous work.   Secondly, we introduce a novel activation function, AdaptoSwelliGauss, which is a sophisticated fusion of Swish and Elliott activations, seamlessly integrated with a scaled and shifted Gaussian component. Swish facilitates smooth transitions, Elliott captures abrupt trajectory changes, and the scaled and shifted Gaussian enhances robustness against noise. This dynamic combination is specifically designed to excel in capturing the complexities of UAV trajectory prediction. This new activation function gives substantially better accuracy than all existing activation functions.   Thirdly, we propose a novel Integrated Collision Detection, Avoidance, and Batching (ICDAB) strategy that merges two complementary UAV collision avoidance techniques: changing UAV trajectories and altering their starting times, also referred to as batching. This integration helps overcome the disadvantages of both - reduction in the number of trajectory manipulations, which avoids overly convoluted paths in the first technique, and smaller batch sizes, which reduce overall takeoff time in the second.","sentences":["This paper explores the application of Artificial Intelligence (AI) techniques for generating the trajectories of fleets of Unmanned Aerial Vehicles (UAVs).","The two main challenges addressed include accurately predicting the paths of UAVs and efficiently avoiding collisions between them.","Firstly, the paper systematically applies a diverse set of activation functions to a Feedforward Neural Network (FFNN) with a single hidden layer, which enhances the accuracy of the predicted path compared to previous work.   ","Secondly, we introduce a novel activation function, AdaptoSwelliGauss, which is a sophisticated fusion of Swish and Elliott activations, seamlessly integrated with a scaled and shifted Gaussian component.","Swish facilitates smooth transitions, Elliott captures abrupt trajectory changes, and the scaled and shifted Gaussian enhances robustness against noise.","This dynamic combination is specifically designed to excel in capturing the complexities of UAV trajectory prediction.","This new activation function gives substantially better accuracy than all existing activation functions.   ","Thirdly, we propose a novel Integrated Collision Detection, Avoidance, and Batching (ICDAB) strategy that merges two complementary UAV collision avoidance techniques: changing UAV trajectories and altering their starting times, also referred to as batching.","This integration helps overcome the disadvantages of both - reduction in the number of trajectory manipulations, which avoids overly convoluted paths in the first technique, and smaller batch sizes, which reduce overall takeoff time in the second."],"url":"http://arxiv.org/abs/2405.11722v1","category":"cs.RO"}
{"created":"2024-05-20 01:44:50","title":"Estimating optimal tailored active surveillance strategy under interval censoring","abstract":"Active surveillance (AS) using repeated biopsies to monitor disease progression has been a popular alternative to immediate surgical intervention in cancer care. However, a biopsy procedure is invasive and sometimes leads to severe side effects of infection and bleeding. To reduce the burden of repeated surveillance biopsies, biomarker-assistant decision rules are sought to replace the fix-for-all regimen with tailored biopsy intensity for individual patients. Constructing or evaluating such decision rules is challenging. The key AS outcome is often ascertained subject to interval censoring. Furthermore, patients will discontinue their participation in the AS study once they receive a positive surveillance biopsy. Thus, patient dropout is affected by the outcomes of these biopsies. In this work, we propose a nonparametric kernel-based method to estimate the true positive rates (TPRs) and true negative rates (TNRs) of a tailored AS strategy, accounting for interval censoring and immediate dropouts. Based on these estimates, we develop a weighted classification framework to estimate the optimal tailored AS strategy and further incorporate the cost-benefit ratio for cost-effectiveness in medical decision-making. Theoretically, we provide a uniform generalization error bound of the derived AS strategy accommodating all possible trade-offs between TPRs and TNRs. Simulation and application to a prostate cancer surveillance study show the superiority of the proposed method.","sentences":["Active surveillance (AS) using repeated biopsies to monitor disease progression has been a popular alternative to immediate surgical intervention in cancer care.","However, a biopsy procedure is invasive and sometimes leads to severe side effects of infection and bleeding.","To reduce the burden of repeated surveillance biopsies, biomarker-assistant decision rules are sought to replace the fix-for-all regimen with tailored biopsy intensity for individual patients.","Constructing or evaluating such decision rules is challenging.","The key AS outcome is often ascertained subject to interval censoring.","Furthermore, patients will discontinue their participation in the AS study once they receive a positive surveillance biopsy.","Thus, patient dropout is affected by the outcomes of these biopsies.","In this work, we propose a nonparametric kernel-based method to estimate the true positive rates (TPRs) and true negative rates (TNRs) of a tailored AS strategy, accounting for interval censoring and immediate dropouts.","Based on these estimates, we develop a weighted classification framework to estimate the optimal tailored AS strategy and further incorporate the cost-benefit ratio for cost-effectiveness in medical decision-making.","Theoretically, we provide a uniform generalization error bound of the derived AS strategy accommodating all possible trade-offs between TPRs and TNRs.","Simulation and application to a prostate cancer surveillance study show the superiority of the proposed method."],"url":"http://arxiv.org/abs/2405.11720v1","category":"stat.ME"}
{"created":"2024-05-20 17:24:02","title":"Quantitative asymptotics for polynomial patterns in the primes","abstract":"We prove quantitative estimates for averages of the von Mangoldt and M\\\"obius functions along polynomial progressions $n+P_1(m),\\ldots, n+P_k(m)$ for a large class of polynomials $P_i$. The error terms obtained save an arbitrary power of logarithm, matching the classical Siegel--Walfisz error term. These results give the first quantitative bounds for the Tao--Ziegler polynomial patterns in the primes result, and in the M\\\"obius case they are new even qualitatively for some collections of polynomials.   The proofs are based on a quantitative generalised von Neumann theorem of Peluse, a recent result of Leng on strong bounds for the Gowers uniformity of the primes, and analysis of a ``Siegel model'' for the von Mangoldt function along polynomial progressions.","sentences":["We prove quantitative estimates for averages of the von Mangoldt and M\\\"obius functions along polynomial progressions $n+P_1(m),\\ldots, n+P_k(m)$ for a large class of polynomials $P_i$. The error terms obtained save an arbitrary power of logarithm, matching the classical Siegel--Walfisz error term.","These results give the first quantitative bounds for the Tao--Ziegler polynomial patterns in the primes result, and in the M\\\"obius case they are new even qualitatively for some collections of polynomials.   ","The proofs are based on a quantitative generalised von Neumann theorem of Peluse, a recent result of Leng on strong bounds for the Gowers uniformity of the primes, and analysis of a ``Siegel model'' for the von Mangoldt function along polynomial progressions."],"url":"http://arxiv.org/abs/2405.12190v1","category":"math.NT"}
{"created":"2024-05-20 15:41:38","title":"Monitored long-range interacting systems: spin-wave theory for quantum trajectories","abstract":"We introduce a stochastic spin-wave theory tailored to describe quantum trajectories in continuously monitored long-range interacting spin systems. Our method, based on the bosonization of spin-wave excitations on top of a strong collective polarization, enables the efficient simulation of large-scale interacting spins, offering insights into nonlinear features of the dynamics such as entanglement and trajectory correlations. We showcase the versatility of our framework by exploring an entanglement phase transition in a monitored spin system with power-law interactions and dwelling on how our method mitigates the experimental challenges of post-selection in detecting monitored quantum phases.","sentences":["We introduce a stochastic spin-wave theory tailored to describe quantum trajectories in continuously monitored long-range interacting spin systems.","Our method, based on the bosonization of spin-wave excitations on top of a strong collective polarization, enables the efficient simulation of large-scale interacting spins, offering insights into nonlinear features of the dynamics such as entanglement and trajectory correlations.","We showcase the versatility of our framework by exploring an entanglement phase transition in a monitored spin system with power-law interactions and dwelling on how our method mitigates the experimental challenges of post-selection in detecting monitored quantum phases."],"url":"http://arxiv.org/abs/2405.12124v1","category":"quant-ph"}
{"created":"2024-05-20 15:15:17","title":"Collective Quantum Entanglement in Molecular Cavity Optomechanics","abstract":"We propose an optomechanical scheme for reaching quantum entanglement in vibration polaritons. The system involves $N$ molecules, whose vibrations can be fairly entangled with plasmonic cavities. We find that the vibration-photon entanglement can exist at room temperature and is robust against thermal noise. We further demonstrate the quantum entanglement between the vibrational modes through the plasmonic cavities, which shows a delocalized nature and an incredible enhancement with the number of molecules. The underlying mechanism for the entanglement is attributed to the strong vibration-cavity coupling which possesses collectivity. Our results provide a molecular optomechanical scheme which offers a promising platform for the study of noise-free quantum resources and macroscopic quantum phenomena.","sentences":["We propose an optomechanical scheme for reaching quantum entanglement in vibration polaritons.","The system involves $N$ molecules, whose vibrations can be fairly entangled with plasmonic cavities.","We find that the vibration-photon entanglement can exist at room temperature and is robust against thermal noise.","We further demonstrate the quantum entanglement between the vibrational modes through the plasmonic cavities, which shows a delocalized nature and an incredible enhancement with the number of molecules.","The underlying mechanism for the entanglement is attributed to the strong vibration-cavity coupling which possesses collectivity.","Our results provide a molecular optomechanical scheme which offers a promising platform for the study of noise-free quantum resources and macroscopic quantum phenomena."],"url":"http://arxiv.org/abs/2405.12102v1","category":"quant-ph"}
{"created":"2024-05-20 14:10:53","title":"Tensor-network-based variational Monte Carlo approach to the non-equilibrium steady state of open quantum systems","abstract":"We introduce a novel method of efficiently simulating the non-equilibrium steady state of large many-body open quantum systems with highly non-local interactions, based on a variational Monte Carlo optimization of a matrix product operator ansatz. Our approach outperforms and offers several advantages over comparable algorithms, such as an improved scaling of the computational cost with respect to the bond dimension for periodic systems. We showcase the versatility of our approach by studying the phase diagrams and correlation functions of the dissipative quantum Ising model with collective dephasing and long-ranged power law interactions for spin chains of up to $N=100$ spins.","sentences":["We introduce a novel method of efficiently simulating the non-equilibrium steady state of large many-body open quantum systems with highly non-local interactions, based on a variational Monte Carlo optimization of a matrix product operator ansatz.","Our approach outperforms and offers several advantages over comparable algorithms, such as an improved scaling of the computational cost with respect to the bond dimension for periodic systems.","We showcase the versatility of our approach by studying the phase diagrams and correlation functions of the dissipative quantum Ising model with collective dephasing and long-ranged power law interactions for spin chains of up to $N=100$ spins."],"url":"http://arxiv.org/abs/2405.12044v1","category":"quant-ph"}
{"created":"2024-05-20 11:47:13","title":"Multiple-Choice Questions are Efficient and Robust LLM Evaluators","abstract":"We present GSM-MC and MATH-MC, two multiple-choice (MC) datasets constructed by collecting answers and incorrect predictions on GSM8K and MATH from over 50 open-source models. Through extensive experiments, we show that LLMs' performance on the MC versions of these two popular benchmarks is strongly correlated with their performance on the original versions, and is quite robust to distractor choices and option orders, while the evaluation time is reduced by a factor of up to 30. Following a similar procedure, we also introduce PythonIO, a new program output prediction MC dataset constructed from two other popular LLM evaluation benchmarks HumanEval and MBPP. Our data and code are available at https://github.com/Geralt-Targaryen/MC-Evaluation.","sentences":["We present GSM-MC and MATH-MC, two multiple-choice (MC) datasets constructed by collecting answers and incorrect predictions on GSM8K and MATH from over 50 open-source models.","Through extensive experiments, we show that LLMs' performance on the MC versions of these two popular benchmarks is strongly correlated with their performance on the original versions, and is quite robust to distractor choices and option orders, while the evaluation time is reduced by a factor of up to 30.","Following a similar procedure, we also introduce PythonIO, a new program output prediction MC dataset constructed from two other popular LLM evaluation benchmarks HumanEval and MBPP.","Our data and code are available at https://github.com/Geralt-Targaryen/MC-Evaluation."],"url":"http://arxiv.org/abs/2405.11966v1","category":"cs.CL"}
{"created":"2024-05-20 11:39:55","title":"Quantifying Individual and Joint Module Impact in Modular Optimization Frameworks","abstract":"This study explores the influence of modules on the performance of modular optimization frameworks for continuous single-objective black-box optimization. There is an extensive variety of modules to choose from when designing algorithm variants, however, there is a rather limited understanding of how each module individually influences the algorithm performance and how the modules interact with each other when combined. We use the functional ANOVA (f-ANOVA) framework to quantify the influence of individual modules and module combinations for two algorithms, the modular Covariance Matrix Adaptation (modCMA) and the modular Differential Evolution (modDE). We analyze the performance data from 324 modCMA and 576 modDE variants on the BBOB benchmark collection, for two problem dimensions, and three computational budgets. Noteworthy findings include the identification of important modules that strongly influence the performance of modCMA, such as the~\\textit{weights\\ option} and~\\textit{mirrored} modules for low dimensional problems, and the~\\textit{base\\ sampler} for high dimensional problems. The large individual influence of the~\\textit{lpsr} module makes it very important for the performance of modDE, regardless of the problem dimensionality and the computational budget. When comparing modCMA and modDE, modDE undergoes a shift from individual modules being more influential, to module combinations being more influential, while modCMA follows the opposite pattern, with an increase in problem dimensionality and computational budget.","sentences":["This study explores the influence of modules on the performance of modular optimization frameworks for continuous single-objective black-box optimization.","There is an extensive variety of modules to choose from when designing algorithm variants, however, there is a rather limited understanding of how each module individually influences the algorithm performance and how the modules interact with each other when combined.","We use the functional ANOVA (f-ANOVA) framework to quantify the influence of individual modules and module combinations for two algorithms, the modular Covariance Matrix Adaptation (modCMA) and the modular Differential Evolution (modDE).","We analyze the performance data from 324 modCMA and 576 modDE variants on the BBOB benchmark collection, for two problem dimensions, and three computational budgets.","Noteworthy findings include the identification of important modules that strongly influence the performance of modCMA, such as the~\\textit{weights\\ option} and~\\textit{mirrored} modules for low dimensional problems, and the~\\textit{base\\ sampler} for high dimensional problems.","The large individual influence of the~\\textit{lpsr} module makes it very important for the performance of modDE, regardless of the problem dimensionality and the computational budget.","When comparing modCMA and modDE, modDE undergoes a shift from individual modules being more influential, to module combinations being more influential, while modCMA follows the opposite pattern, with an increase in problem dimensionality and computational budget."],"url":"http://arxiv.org/abs/2405.11964v1","category":"cs.NE"}
{"created":"2024-05-20 10:24:10","title":"UAV-VisLoc: A Large-scale Dataset for UAV Visual Localization","abstract":"The application of unmanned aerial vehicles (UAV) has been widely extended recently. It is crucial to ensure accurate latitude and longitude coordinates for UAVs, especially when the global navigation satellite systems (GNSS) are disrupted and unreliable. Existing visual localization methods achieve autonomous visual localization without error accumulation by matching the ground-down view image of UAV with the ortho satellite maps. However, collecting UAV ground-down view images across diverse locations is costly, leading to a scarcity of large-scale datasets for real-world scenarios. Existing datasets for UAV visual localization are often limited to small geographic areas or are focused only on urban regions with distinct textures. To address this, we define the UAV visual localization task by determining the UAV's real position coordinates on a large-scale satellite map based on the captured ground-down view. In this paper, we present a large-scale dataset, UAV-VisLoc, to facilitate the UAV visual localization task. This dataset comprises images from diverse drones across 11 locations in China, capturing a range of topographical features. The dataset features images from fixed-wing drones and multi-terrain drones, captured at different altitudes and orientations. Our dataset includes 6,742 drone images and 11 satellite maps, with metadata such as latitude, longitude, altitude, and capture date. Our dataset is tailored to support both the training and testing of models by providing a diverse and extensive data.","sentences":["The application of unmanned aerial vehicles (UAV) has been widely extended recently.","It is crucial to ensure accurate latitude and longitude coordinates for UAVs, especially when the global navigation satellite systems (GNSS) are disrupted and unreliable.","Existing visual localization methods achieve autonomous visual localization without error accumulation by matching the ground-down view image of UAV with the ortho satellite maps.","However, collecting UAV ground-down view images across diverse locations is costly, leading to a scarcity of large-scale datasets for real-world scenarios.","Existing datasets for UAV visual localization are often limited to small geographic areas or are focused only on urban regions with distinct textures.","To address this, we define the UAV visual localization task by determining the UAV's real position coordinates on a large-scale satellite map based on the captured ground-down view.","In this paper, we present a large-scale dataset, UAV-VisLoc, to facilitate the UAV visual localization task.","This dataset comprises images from diverse drones across 11 locations in China, capturing a range of topographical features.","The dataset features images from fixed-wing drones and multi-terrain drones, captured at different altitudes and orientations.","Our dataset includes 6,742 drone images and 11 satellite maps, with metadata such as latitude, longitude, altitude, and capture date.","Our dataset is tailored to support both the training and testing of models by providing a diverse and extensive data."],"url":"http://arxiv.org/abs/2405.11936v1","category":"cs.CV"}
{"created":"2024-05-20 06:43:33","title":"Data quality control system and long-term performance monitor of the LHAASO-KM2A","abstract":"The KM2A is the largest sub-array of the Large High Altitude Air Shower Observatory (LHAASO). It consists of 5216 electromagnetic particle detectors (EDs) and 1188 muon detectors (MDs). The data recorded by the EDs and MDs are used to reconstruct primary information of cosmic ray and gamma-ray showers. This information is used for physical analysis in gamma-ray astronomy and cosmic ray physics. To ensure the reliability of the LHAASO-KM2A data, a three-level quality control system has been established. It is used to monitor the status of detector units, stability of reconstructed parameters and the performance of the array based on observations of the Crab Nebula and Moon shadow. This paper will introduce the control system and its application on the LHAASO-KM2A data collected from August 2021 to July 2023. During this period, the pointing and angular resolution of the array were stable. From the observations of the Moon shadow and Crab Nebula, the results achieved using the two methods are consistent with each other. According to the observation of the Crab Nebula at energies from 25 TeV to 100 TeV, the time averaged pointing errors are estimated to be $-0.003^{\\circ} \\pm 0.005^{\\circ}$ and $0.001^{\\circ} \\pm 0.006^{\\circ}$ in the R.A. and Dec directions, respectively.","sentences":["The KM2A is the largest sub-array of the Large High Altitude Air Shower Observatory (LHAASO).","It consists of 5216 electromagnetic particle detectors (EDs) and 1188 muon detectors (MDs).","The data recorded by the EDs and MDs are used to reconstruct primary information of cosmic ray and gamma-ray showers.","This information is used for physical analysis in gamma-ray astronomy and cosmic ray physics.","To ensure the reliability of the LHAASO-KM2A data, a three-level quality control system has been established.","It is used to monitor the status of detector units, stability of reconstructed parameters and the performance of the array based on observations of the Crab Nebula and Moon shadow.","This paper will introduce the control system and its application on the LHAASO-KM2A data collected from August 2021 to July 2023.","During this period, the pointing and angular resolution of the array were stable.","From the observations of the Moon shadow and Crab Nebula, the results achieved using the two methods are consistent with each other.","According to the observation of the Crab Nebula at energies from 25 TeV to 100 TeV, the time averaged pointing errors are estimated to be $-0.003^{\\circ} \\pm 0.005^{\\circ}$ and $0.001^{\\circ} \\pm 0.006^{\\circ}$ in the R.A. and Dec directions, respectively."],"url":"http://arxiv.org/abs/2405.11826v1","category":"hep-ex"}
{"created":"2024-05-20 06:34:47","title":"Stereo-Knowledge Distillation from dpMV to Dual Pixels for Light Field Video Reconstruction","abstract":"Dual pixels contain disparity cues arising from the defocus blur. This disparity information is useful for many vision tasks ranging from autonomous driving to 3D creative realism. However, directly estimating disparity from dual pixels is less accurate. This work hypothesizes that distilling high-precision dark stereo knowledge, implicitly or explicitly, to efficient dual-pixel student networks enables faithful reconstructions. This dark knowledge distillation should also alleviate stereo-synchronization setup and calibration costs while dramatically increasing parameter and inference time efficiency. We collect the first and largest 3-view dual-pixel video dataset, dpMV, to validate our explicit dark knowledge distillation hypothesis. We show that these methods outperform purely monocular solutions, especially in challenging foreground-background separation regions using faithful guidance from dual pixels. Finally, we demonstrate an unconventional use case unlocked by dpMV and implicit dark knowledge distillation from an ensemble of teachers for Light Field (LF) video reconstruction. Our LF video reconstruction method is the fastest and most temporally consistent to date. It remains competitive in reconstruction fidelity while offering many other essential properties like high parameter efficiency, implicit disocclusion handling, zero-shot cross-dataset transfer, geometrically consistent inference on higher spatial-angular resolutions, and adaptive baseline control. All source code is available at the anonymous repository https://github.com/Aryan-Garg.","sentences":["Dual pixels contain disparity cues arising from the defocus blur.","This disparity information is useful for many vision tasks ranging from autonomous driving to 3D creative realism.","However, directly estimating disparity from dual pixels is less accurate.","This work hypothesizes that distilling high-precision dark stereo knowledge, implicitly or explicitly, to efficient dual-pixel student networks enables faithful reconstructions.","This dark knowledge distillation should also alleviate stereo-synchronization setup and calibration costs while dramatically increasing parameter and inference time efficiency.","We collect the first and largest 3-view dual-pixel video dataset, dpMV, to validate our explicit dark knowledge distillation hypothesis.","We show that these methods outperform purely monocular solutions, especially in challenging foreground-background separation regions using faithful guidance from dual pixels.","Finally, we demonstrate an unconventional use case unlocked by dpMV and implicit dark knowledge distillation from an ensemble of teachers for Light Field (LF) video reconstruction.","Our LF video reconstruction method is the fastest and most temporally consistent to date.","It remains competitive in reconstruction fidelity while offering many other essential properties like high parameter efficiency, implicit disocclusion handling, zero-shot cross-dataset transfer, geometrically consistent inference on higher spatial-angular resolutions, and adaptive baseline control.","All source code is available at the anonymous repository https://github.com/Aryan-Garg."],"url":"http://arxiv.org/abs/2405.11823v1","category":"cs.CV"}
{"created":"2024-05-20 05:55:08","title":"(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts","abstract":"Recent advancements in machine translation (MT) have significantly enhanced translation quality across various domains. However, the translation of literary texts remains a formidable challenge due to their complex language, figurative expressions, and cultural nuances. In this work, we introduce a novel multi-agent framework based on large language models (LLMs) for literary translation, implemented as a company called TransAgents, which mirrors traditional translation publication process by leveraging the collective capabilities of multiple agents, to address the intricate demands of translating literary works. To evaluate the effectiveness of our system, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP) and Bilingual LLM Preference (BLP). MHP assesses translations from the perspective of monolingual readers of the target language, while BLP uses advanced LLMs to compare translations directly with the original texts. Empirical findings indicate that despite lower d-BLEU scores, translations from TransAgents are preferred by both human evaluators and LLMs over human-written references, particularly in genres requiring domain-specific knowledge. We also highlight the strengths and limitations of TransAgents through case studies and suggests directions for future research.","sentences":["Recent advancements in machine translation (MT) have significantly enhanced translation quality across various domains.","However, the translation of literary texts remains a formidable challenge due to their complex language, figurative expressions, and cultural nuances.","In this work, we introduce a novel multi-agent framework based on large language models (LLMs) for literary translation, implemented as a company called TransAgents, which mirrors traditional translation publication process by leveraging the collective capabilities of multiple agents, to address the intricate demands of translating literary works.","To evaluate the effectiveness of our system, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP) and Bilingual LLM Preference (BLP).","MHP assesses translations from the perspective of monolingual readers of the target language, while BLP uses advanced LLMs to compare translations directly with the original texts.","Empirical findings indicate that despite lower d-BLEU scores, translations from TransAgents are preferred by both human evaluators and LLMs over human-written references, particularly in genres requiring domain-specific knowledge.","We also highlight the strengths and limitations of TransAgents through case studies and suggests directions for future research."],"url":"http://arxiv.org/abs/2405.11804v1","category":"cs.CL"}
{"created":"2024-05-20 04:32:51","title":"Active Exploration for Real-Time Haptic Training","abstract":"Tactile perception is important for robotic systems that interact with the world through touch. Touch is an active sense in which tactile measurements depend on the contact properties of an interaction--e.g., velocity, force, acceleration--as well as properties of the sensor and object under test. These dependencies make training tactile perceptual models challenging. Additionally, the effects of limited sensor life and the near-field nature of tactile sensors preclude the practical collection of exhaustive data sets even for fairly simple objects. Active learning provides a mechanism for focusing on only the most informative aspects of an object during data collection. Here we employ an active learning approach that uses a data-driven model's entropy as an uncertainty measure and explore relative to that entropy conditioned on the sensor state variables. Using a coverage-based ergodic controller, we train perceptual models in near-real time. We demonstrate our approach using a biomimentic sensor, exploring \"tactile scenes\" composed of shapes, textures, and objects. Each learned representation provides a perceptual sensor model for a particular tactile scene. Models trained on actively collected data outperform their randomly collected counterparts in real-time training tests. Additionally, we find that the resulting network entropy maps can be used to identify high salience portions of a tactile scene.","sentences":["Tactile perception is important for robotic systems that interact with the world through touch.","Touch is an active sense in which tactile measurements depend on the contact properties of an interaction--e.g., velocity, force, acceleration--as well as properties of the sensor and object under test.","These dependencies make training tactile perceptual models challenging.","Additionally, the effects of limited sensor life and the near-field nature of tactile sensors preclude the practical collection of exhaustive data sets even for fairly simple objects.","Active learning provides a mechanism for focusing on only the most informative aspects of an object during data collection.","Here we employ an active learning approach that uses a data-driven model's entropy as an uncertainty measure and explore relative to that entropy conditioned on the sensor state variables.","Using a coverage-based ergodic controller, we train perceptual models in near-real time.","We demonstrate our approach using a biomimentic sensor, exploring \"tactile scenes\" composed of shapes, textures, and objects.","Each learned representation provides a perceptual sensor model for a particular tactile scene.","Models trained on actively collected data outperform their randomly collected counterparts in real-time training tests.","Additionally, we find that the resulting network entropy maps can be used to identify high salience portions of a tactile scene."],"url":"http://arxiv.org/abs/2405.11776v1","category":"cs.RO"}
{"created":"2024-05-20 01:29:45","title":"Semantic Trajectory Data Mining with LLM-Informed POI Classification","abstract":"Human travel trajectory mining is crucial for transportation systems, enhancing route optimization, traffic management, and the study of human travel patterns. Previous rule-based approaches without the integration of semantic information show a limitation in both efficiency and accuracy. Semantic information, such as activity types inferred from Points of Interest (POI) data, can significantly enhance the quality of trajectory mining. However, integrating these insights is challenging, as many POIs have incomplete feature information, and current learning-based POI algorithms require the integrity of datasets to do the classification. In this paper, we introduce a novel pipeline for human travel trajectory mining. Our approach first leverages the strong inferential and comprehension capabilities of large language models (LLMs) to annotate POI with activity types and then uses a Bayesian-based algorithm to infer activity for each stay point in a trajectory. In our evaluation using the OpenStreetMap (OSM) POI dataset, our approach achieves a 93.4% accuracy and a 96.1% F-1 score in POI classification, and a 91.7% accuracy with a 92.3% F-1 score in activity inference.","sentences":["Human travel trajectory mining is crucial for transportation systems, enhancing route optimization, traffic management, and the study of human travel patterns.","Previous rule-based approaches without the integration of semantic information show a limitation in both efficiency and accuracy.","Semantic information, such as activity types inferred from Points of Interest (POI) data, can significantly enhance the quality of trajectory mining.","However, integrating these insights is challenging, as many POIs have incomplete feature information, and current learning-based POI algorithms require the integrity of datasets to do the classification.","In this paper, we introduce a novel pipeline for human travel trajectory mining.","Our approach first leverages the strong inferential and comprehension capabilities of large language models (LLMs) to annotate POI with activity types and then uses a Bayesian-based algorithm to infer activity for each stay point in a trajectory.","In our evaluation using the OpenStreetMap (OSM) POI dataset, our approach achieves a 93.4% accuracy and a 96.1% F-1 score in POI classification, and a 91.7% accuracy with a 92.3% F-1 score in activity inference."],"url":"http://arxiv.org/abs/2405.11715v1","category":"cs.AI"}
{"created":"2024-05-20 01:22:21","title":"Decentralized Privacy Preservation for Critical Connections in Graphs","abstract":"Many real-world interconnections among entities can be characterized as graphs. Collecting local graph information with balanced privacy and data utility has garnered notable interest recently. This paper delves into the problem of identifying and protecting critical information of entity connections for individual participants in a graph based on cohesive subgraph searches. This problem has not been addressed in the literature. To address the problem, we propose to extract the critical connections of a queried vertex using a fortress-like cohesive subgraph model known as $p$-cohesion. A user's connections within a fortress are obfuscated when being released, to protect critical information about the user. Novel merit and penalty score functions are designed to measure each participant's critical connections in the minimal $p$-cohesion, facilitating effective identification of the connections. We further propose to preserve the privacy of a vertex enquired by only protecting its critical connections when responding to queries raised by data collectors. We prove that, under the decentralized differential privacy (DDP) mechanism, one's response satisfies $(\\varepsilon, \\delta)$-DDP when its critical connections are protected while the rest remains unperturbed. The effectiveness of our proposed method is demonstrated through extensive experiments on real-life graph datasets.","sentences":["Many real-world interconnections among entities can be characterized as graphs.","Collecting local graph information with balanced privacy and data utility has garnered notable interest recently.","This paper delves into the problem of identifying and protecting critical information of entity connections for individual participants in a graph based on cohesive subgraph searches.","This problem has not been addressed in the literature.","To address the problem, we propose to extract the critical connections of a queried vertex using a fortress-like cohesive subgraph model known as $p$-cohesion.","A user's connections within a fortress are obfuscated when being released, to protect critical information about the user.","Novel merit and penalty score functions are designed to measure each participant's critical connections in the minimal $p$-cohesion, facilitating effective identification of the connections.","We further propose to preserve the privacy of a vertex enquired by only protecting its critical connections when responding to queries raised by data collectors.","We prove that, under the decentralized differential privacy (DDP) mechanism, one's response satisfies $(\\varepsilon, \\delta)$-DDP when its critical connections are protected while the rest remains unperturbed.","The effectiveness of our proposed method is demonstrated through extensive experiments on real-life graph datasets."],"url":"http://arxiv.org/abs/2405.11713v1","category":"cs.CR"}
{"created":"2024-05-20 01:15:57","title":"Trust, Because You Can't Verify:Privacy and Security Hurdles in Education Technology Acquisition Practices","abstract":"The education technology (EdTech) landscape is expanding rapidly in higher education institutes (HEIs). This growth brings enormous complexity. Protecting the extensive data collected by these tools is crucial for HEIs. Privacy incidents of data breaches and misuses can have dire security and privacy consequences on the data subjects, particularly students, who are often compelled to use these tools. This urges an in-depth understanding of HEI and EdTech vendor dynamics, which is largely understudied.   To address this gap, we conduct a semi-structured interview study with 13 participants who are in the EdTech leadership roles at seven HEIs. Our study uncovers the EdTech acquisition process in the HEI context, the consideration of security and privacy issues throughout that process, the pain points of HEI personnel in establishing adequate security and privacy protection mechanisms in service contracts, and their struggle in holding vendors accountable due to a lack of visibility into their system and power-asymmetry, among other reasons. We discuss certain observations about the status quo and conclude with recommendations to improve the situation.","sentences":["The education technology (EdTech) landscape is expanding rapidly in higher education institutes (HEIs).","This growth brings enormous complexity.","Protecting the extensive data collected by these tools is crucial for HEIs.","Privacy incidents of data breaches and misuses can have dire security and privacy consequences on the data subjects, particularly students, who are often compelled to use these tools.","This urges an in-depth understanding of HEI and EdTech vendor dynamics, which is largely understudied.   ","To address this gap, we conduct a semi-structured interview study with 13 participants who are in the EdTech leadership roles at seven HEIs.","Our study uncovers the EdTech acquisition process in the HEI context, the consideration of security and privacy issues throughout that process, the pain points of HEI personnel in establishing adequate security and privacy protection mechanisms in service contracts, and their struggle in holding vendors accountable due to a lack of visibility into their system and power-asymmetry, among other reasons.","We discuss certain observations about the status quo and conclude with recommendations to improve the situation."],"url":"http://arxiv.org/abs/2405.11712v1","category":"cs.CY"}
{"created":"2024-05-20 01:04:40","title":"OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework","abstract":"As large language models (LLMs) continue to grow by scaling laws, reinforcement learning from human feedback (RLHF) has gained significant attention due to its outstanding performance. However, unlike pretraining or fine-tuning a single model, scaling reinforcement learning from human feedback (RLHF) for training large language models poses coordination challenges across four models. We present OpenRLHF, an open-source framework enabling efficient RLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the same GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters using Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and diverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF provides an out-of-the-box solution with optimized algorithms and launch scripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO, rejection sampling, and other alignment techniques. Empowering state-of-the-art LLM development, OpenRLHF's code is available at https://github.com/OpenLLMAI/OpenRLHF.","sentences":["As large language models (LLMs) continue to grow by scaling laws, reinforcement learning from human feedback (RLHF) has gained significant attention due to its outstanding performance.","However, unlike pretraining or fine-tuning a single model, scaling reinforcement learning from human feedback (RLHF) for training large language models poses coordination challenges across four models.","We present OpenRLHF, an open-source framework enabling efficient RLHF scaling.","Unlike existing RLHF frameworks that co-locate four models on the same GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters using Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and diverse training approaches.","Integrating seamlessly with Hugging Face, OpenRLHF provides an out-of-the-box solution with optimized algorithms and launch scripts, which ensures user-friendliness.","OpenRLHF implements RLHF, DPO, rejection sampling, and other alignment techniques.","Empowering state-of-the-art LLM development, OpenRLHF's code is available at https://github.com/OpenLLMAI/OpenRLHF."],"url":"http://arxiv.org/abs/2405.11143v1","category":"cs.AI"}
{"created":"2024-05-20 00:28:00","title":"Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!","abstract":"There is increasing evidence that question-answering (QA) systems with Large Language Models (LLMs), which employ a knowledge graph/semantic representation of an enterprise SQL database (i.e. Text-to-SPARQL), achieve higher accuracy compared to systems that answer questions directly on SQL databases (i.e. Text-to-SQL). Our previous benchmark research showed that by using a knowledge graph, the accuracy improved from 16% to 54%. The question remains: how can we further improve the accuracy and reduce the error rate? Building on the observations of our previous research where the inaccurate LLM-generated SPARQL queries followed incorrect paths, we present an approach that consists of 1) Ontology-based Query Check (OBQC): detects errors by leveraging the ontology of the knowledge graph to check if the LLM-generated SPARQL query matches the semantic of ontology and 2) LLM Repair: use the error explanations with an LLM to repair the SPARQL query. Using the chat with the data benchmark, our primary finding is that our approach increases the overall accuracy to 72% including an additional 8% of \"I don't know\" unknown results. Thus, the overall error rate is 20%. These results provide further evidence that investing knowledge graphs, namely the ontology, provides higher accuracy for LLM powered question answering systems.","sentences":["There is increasing evidence that question-answering (QA) systems with Large Language Models (LLMs), which employ a knowledge graph/semantic representation of an enterprise SQL database (i.e. Text-to-SPARQL), achieve higher accuracy compared to systems that answer questions directly on SQL databases (i.e. Text-to-SQL).","Our previous benchmark research showed that by using a knowledge graph, the accuracy improved from 16% to 54%.","The question remains: how can we further improve the accuracy and reduce the error rate?","Building on the observations of our previous research where the inaccurate LLM-generated SPARQL queries followed incorrect paths, we present an approach that consists of 1) Ontology-based Query Check (OBQC):","detects errors by leveraging the ontology of the knowledge graph to check if the LLM-generated SPARQL query matches the semantic of ontology and 2) LLM Repair: use the error explanations with an LLM to repair the SPARQL query.","Using the chat with the data benchmark, our primary finding is that our approach increases the overall accuracy to 72% including an additional 8% of \"I don't know\" unknown results.","Thus, the overall error rate is 20%.","These results provide further evidence that investing knowledge graphs, namely the ontology, provides higher accuracy for LLM powered question answering systems."],"url":"http://arxiv.org/abs/2405.11706v1","category":"cs.AI"}
{"created":"2024-05-20 00:10:00","title":"Efficiency optimization of large-scale language models based on deep learning in natural language processing tasks","abstract":"The internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.","sentences":["The internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies.","Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint.","By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice.","In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation.","By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy.","In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research.","In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models."],"url":"http://arxiv.org/abs/2405.11704v1","category":"cs.LG"}
{"created":"2024-05-19 23:08:15","title":"Multi-Objective Learning Model Predictive Control","abstract":"Multi-Objective Learning Model Predictive Control is a novel data-driven control scheme which improves a system's closed-loop performance with respect to several control objectives over iterations of a repeated task. At each task iteration, collected system data is used to construct terminal components of a Model Predictive Controller. The formulation presented in this paper ensures that closed-loop control performance improves between successive iterations with respect to each objective. We provide proofs of recursive feasibility and performance improvement, and show that the converged policy is Pareto optimal. Simulation results demonstrate the applicability of the proposed approach.","sentences":["Multi-Objective Learning Model Predictive Control is a novel data-driven control scheme which improves a system's closed-loop performance with respect to several control objectives over iterations of a repeated task.","At each task iteration, collected system data is used to construct terminal components of a Model Predictive Controller.","The formulation presented in this paper ensures that closed-loop control performance improves between successive iterations with respect to each objective.","We provide proofs of recursive feasibility and performance improvement, and show that the converged policy is Pareto optimal.","Simulation results demonstrate the applicability of the proposed approach."],"url":"http://arxiv.org/abs/2405.11698v1","category":"eess.SY"}
{"created":"2024-05-19 23:05:53","title":"AMMeBa: A Large-Scale Survey and Dataset of Media-Based Misinformation In-The-Wild","abstract":"The prevalence and harms of online misinformation is a perennial concern for internet platforms, institutions and society at large. Over time, information shared online has become more media-heavy and misinformation has readily adapted to these new modalities. The rise of generative AI-based tools, which provide widely-accessible methods for synthesizing realistic audio, images, video and human-like text, have amplified these concerns. Despite intense interest on the part of the public and significant press coverage, quantitative information on the prevalence and modality of media-based misinformation remains scarce. Here, we present the results of a two-year study using human raters to annotate online media-based misinformation, mostly focusing on images, based on claims assessed in a large sample of publicly-accessible fact checks with the ClaimReview markup. We present an image typology, designed to capture aspects of the image and manipulation relevant to the image's role in the misinformation claim. We visualize the distribution of these types over time. We show the the rise of generative AI-based content in misinformation claims, and that it's commonality is a relatively recent phenomenon, occurring significantly after heavy press coverage. We also show \"simple\" methods dominated historically, particularly context manipulations, and continued to hold a majority as of the end of data collection in November 2023. The dataset, Annotated Misinformation, Media-Based (AMMeBa), is publicly-available, and we hope that these data will serve as both a means of evaluating mitigation methods in a realistic setting and as a first-of-its-kind census of the types and modalities of online misinformation.","sentences":["The prevalence and harms of online misinformation is a perennial concern for internet platforms, institutions and society at large.","Over time, information shared online has become more media-heavy and misinformation has readily adapted to these new modalities.","The rise of generative AI-based tools, which provide widely-accessible methods for synthesizing realistic audio, images, video and human-like text, have amplified these concerns.","Despite intense interest on the part of the public and significant press coverage, quantitative information on the prevalence and modality of media-based misinformation remains scarce.","Here, we present the results of a two-year study using human raters to annotate online media-based misinformation, mostly focusing on images, based on claims assessed in a large sample of publicly-accessible fact checks with the ClaimReview markup.","We present an image typology, designed to capture aspects of the image and manipulation relevant to the image's role in the misinformation claim.","We visualize the distribution of these types over time.","We show the the rise of generative AI-based content in misinformation claims, and that it's commonality is a relatively recent phenomenon, occurring significantly after heavy press coverage.","We also show \"simple\" methods dominated historically, particularly context manipulations, and continued to hold a majority as of the end of data collection in November 2023.","The dataset, Annotated Misinformation, Media-Based (AMMeBa), is publicly-available, and we hope that these data will serve as both a means of evaluating mitigation methods in a realistic setting and as a first-of-its-kind census of the types and modalities of online misinformation."],"url":"http://arxiv.org/abs/2405.11697v1","category":"cs.CY"}
{"created":"2024-05-19 22:51:22","title":"PBI: Position-Based Dynamics Handles Updated Lagrangian Inelasticity","abstract":"Position-based Dynamics (PBD) and its extension, eXtended Position-based Dynamics (XPBD), have been predominantly applied to compliant constrained dynamics, with their potential in finite strain inelasticity remaining underexplored. XPBD stands in contrast to other meshless methods, such as the Material Point Method (MPM). MPM is based on discretizing the weak form of governing partial differential equations within a continuum domain, coupled with a hybrid Lagrangian-Eulerian method for tracking deformation gradients. In contrast, XPBD generally entails applying specific constraints, whether hard or compliant, to collections of point masses. This paper revisits this perception, investigating the potential of XPBD in handling inelastic materials that are described with continuum mechanics based yield surfaces and elastoplastic flow rules. Our inspiration is that a robust estimation of the velocity gradient is key to effectively tracking deformation gradients in any meshless context. By further incorporating implicit inelastic constitutive relationships, we introduce an updated Lagrangian augmentation to XPBD. This enhancement enables the simulation of elastoplastic, viscoplastic, and granular substances following their standard constitutive laws. We demonstrate the effectiveness of our method through high-resolution and real-time simulations of diverse materials such as snow, sand, and plasticine, and its integration with standard XPBD simulations of cloth and water.","sentences":["Position-based Dynamics (PBD) and its extension, eXtended Position-based Dynamics (XPBD), have been predominantly applied to compliant constrained dynamics, with their potential in finite strain inelasticity remaining underexplored.","XPBD stands in contrast to other meshless methods, such as the Material Point Method (MPM).","MPM is based on discretizing the weak form of governing partial differential equations within a continuum domain, coupled with a hybrid Lagrangian-Eulerian method for tracking deformation gradients.","In contrast, XPBD generally entails applying specific constraints, whether hard or compliant, to collections of point masses.","This paper revisits this perception, investigating the potential of XPBD in handling inelastic materials that are described with continuum mechanics based yield surfaces and elastoplastic flow rules.","Our inspiration is that a robust estimation of the velocity gradient is key to effectively tracking deformation gradients in any meshless context.","By further incorporating implicit inelastic constitutive relationships, we introduce an updated Lagrangian augmentation to XPBD.","This enhancement enables the simulation of elastoplastic, viscoplastic, and granular substances following their standard constitutive laws.","We demonstrate the effectiveness of our method through high-resolution and real-time simulations of diverse materials such as snow, sand, and plasticine, and its integration with standard XPBD simulations of cloth and water."],"url":"http://arxiv.org/abs/2405.11694v1","category":"cs.GR"}
{"created":"2024-05-19 21:26:11","title":"Deep Ensemble Art Style Recognition","abstract":"The massive digitization of artworks during the last decades created the need for categorization, analysis, and management of huge amounts of data related to abstract concepts, highlighting a challenging problem in the field of computer science. The rapid progress of artificial intelligence and neural networks has provided tools and technologies that seem worthy of the challenge. Recognition of various art features in artworks has gained attention in the deep learning society. In this paper, we are concerned with the problem of art style recognition using deep networks. We compare the performance of 8 different deep architectures (VGG16, VGG19, ResNet50, ResNet152, Inception-V3, DenseNet121, DenseNet201 and Inception-ResNet-V2), on two different art datasets, including 3 architectures that have never been used on this task before, leading to state-of-the-art performance. We study the effect of data preprocessing prior to applying a deep learning model. We introduce a stacking ensemble method combining the results of first-stage classifiers through a meta-classifier, with the innovation of a versatile approach based on multiple models that extract and recognize different characteristics of the input, creating a more consistent model compared to existing works and achieving state-of-the-art accuracy on the largest art dataset available (WikiArt - 68,55%). We also discuss the impact of the data and art styles themselves on the performance of our models forming a manifold perspective on the problem.","sentences":["The massive digitization of artworks during the last decades created the need for categorization, analysis, and management of huge amounts of data related to abstract concepts, highlighting a challenging problem in the field of computer science.","The rapid progress of artificial intelligence and neural networks has provided tools and technologies that seem worthy of the challenge.","Recognition of various art features in artworks has gained attention in the deep learning society.","In this paper, we are concerned with the problem of art style recognition using deep networks.","We compare the performance of 8 different deep architectures (VGG16, VGG19, ResNet50, ResNet152, Inception-V3, DenseNet121, DenseNet201 and Inception-ResNet-V2), on two different art datasets, including 3 architectures that have never been used on this task before, leading to state-of-the-art performance.","We study the effect of data preprocessing prior to applying a deep learning model.","We introduce a stacking ensemble method combining the results of first-stage classifiers through a meta-classifier, with the innovation of a versatile approach based on multiple models that extract and recognize different characteristics of the input, creating a more consistent model compared to existing works and achieving state-of-the-art accuracy on the largest art dataset available (WikiArt - 68,55%).","We also discuss the impact of the data and art styles themselves on the performance of our models forming a manifold perspective on the problem."],"url":"http://arxiv.org/abs/2405.11675v1","category":"cs.CV"}
{"created":"2024-05-19 20:33:21","title":"Do No Harm: A Counterfactual Approach to Safe Reinforcement Learning","abstract":"Reinforcement Learning (RL) for control has become increasingly popular due to its ability to learn rich feedback policies that take into account uncertainty and complex representations of the environment. When considering safety constraints, constrained optimization approaches, where agents are penalized for constraint violations, are commonly used. In such methods, if agents are initialized in, or must visit, states where constraint violation might be inevitable, it is unclear how much they should be penalized. We address this challenge by formulating a constraint on the counterfactual harm of the learned policy compared to a default, safe policy. In a philosophical sense this formulation only penalizes the learner for constraint violations that it caused; in a practical sense it maintains feasibility of the optimal control problem. We present simulation studies on a rover with uncertain road friction and a tractor-trailer parking environment that demonstrate our constraint formulation enables agents to learn safer policies than contemporary constrained RL methods.","sentences":["Reinforcement Learning (RL) for control has become increasingly popular due to its ability to learn rich feedback policies that take into account uncertainty and complex representations of the environment.","When considering safety constraints, constrained optimization approaches, where agents are penalized for constraint violations, are commonly used.","In such methods, if agents are initialized in, or must visit, states where constraint violation might be inevitable, it is unclear how much they should be penalized.","We address this challenge by formulating a constraint on the counterfactual harm of the learned policy compared to a default, safe policy.","In a philosophical sense this formulation only penalizes the learner for constraint violations that it caused; in a practical sense it maintains feasibility of the optimal control problem.","We present simulation studies on a rover with uncertain road friction and a tractor-trailer parking environment that demonstrate our constraint formulation enables agents to learn safer policies than contemporary constrained RL methods."],"url":"http://arxiv.org/abs/2405.11669v1","category":"cs.LG"}
{"created":"2024-05-19 20:06:38","title":"On the Expressivity of Recurrent Neural Cascades with Identity","abstract":"Recurrent Neural Cascades (RNC) are the class of recurrent neural networks with no cyclic dependencies among recurrent neurons. Their subclass RNC+ with positive recurrent weights has been shown to be closely connected to the star-free regular languages, which are the expressivity of many well-established temporal logics. The existing expressivity results show that the regular languages captured by RNC+ are the star-free ones, and they leave open the possibility that RNC+ may capture languages beyond regular. We exclude this possibility for languages that include an identity element, i.e., an input that can occur an arbitrary number of times without affecting the output. Namely, in the presence of an identity element, we show that the languages captured by RNC+ are exactly the star-free regular languages. Identity elements are ubiquitous in temporal patterns, and hence our results apply to a large number of applications. The implications of our results go beyond expressivity. At their core, we establish a close structural correspondence between RNC+ and semiautomata cascades, showing that every neuron can be equivalently captured by a three-state semiautomaton. A notable consequence of this result is that RNC+ are no more succinct than cascades of three-state semiautomata.","sentences":["Recurrent Neural Cascades (RNC) are the class of recurrent neural networks with no cyclic dependencies among recurrent neurons.","Their subclass RNC+ with positive recurrent weights has been shown to be closely connected to the star-free regular languages, which are the expressivity of many well-established temporal logics.","The existing expressivity results show that the regular languages captured by RNC+ are the star-free ones, and they leave open the possibility that RNC+ may capture languages beyond regular.","We exclude this possibility for languages that include an identity element, i.e., an input that can occur an arbitrary number of times without affecting the output.","Namely, in the presence of an identity element, we show that the languages captured by RNC+ are exactly the star-free regular languages.","Identity elements are ubiquitous in temporal patterns, and hence our results apply to a large number of applications.","The implications of our results go beyond expressivity.","At their core, we establish a close structural correspondence between RNC+ and semiautomata cascades, showing that every neuron can be equivalently captured by a three-state semiautomaton.","A notable consequence of this result is that RNC+ are no more succinct than cascades of three-state semiautomata."],"url":"http://arxiv.org/abs/2405.11657v1","category":"cs.LG"}
{"created":"2024-05-19 20:01:29","title":"URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images","abstract":"Constructing simulation scenes that are both visually and physically realistic is a problem of practical interest in domains ranging from robotics to computer vision. This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems. However, building simulation models is often still done by hand. A graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties. While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes, complete with 'natural' kinematic and dynamic structures. To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets. To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models. We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism. We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures from real-world images and use these for training robotic control policies. We then robustly deploy in the real world for tasks like articulated object manipulation. In doing so, our work provides both a pipeline for large-scale generation of simulation environments and an integrated system for training robust robotic control policies in the resulting environments.","sentences":["Constructing simulation scenes that are both visually and physically realistic is a problem of practical interest in domains ranging from robotics to computer vision.","This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems.","However, building simulation models is often still done by hand.","A graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties.","While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes, complete with 'natural' kinematic and dynamic structures.","To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets.","To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models.","We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism.","We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures from real-world images and use these for training robotic control policies.","We then robustly deploy in the real world for tasks like articulated object manipulation.","In doing so, our work provides both a pipeline for large-scale generation of simulation environments and an integrated system for training robust robotic control policies in the resulting environments."],"url":"http://arxiv.org/abs/2405.11656v1","category":"cs.RO"}
{"created":"2024-05-19 19:51:41","title":"Track Anything Rapter(TAR)","abstract":"Object tracking is a fundamental task in computer vision with broad practical applications across various domains, including traffic monitoring, robotics, and autonomous vehicle tracking. In this project, we aim to develop a sophisticated aerial vehicle system known as Track Anything Raptor (TAR), designed to detect, segment, and track objects of interest based on user-provided multimodal queries, such as text, images, and clicks. TAR utilizes cutting-edge pre-trained models like DINO, CLIP, and SAM to estimate the relative pose of the queried object. The tracking problem is approached as a Visual Servoing task, enabling the UAV to consistently focus on the object through advanced motion planning and control algorithms. We showcase how the integration of these foundational models with a custom high-level control algorithm results in a highly stable and precise tracking system deployed on a custom-built PX4 Autopilot-enabled Voxl2 M500 drone. To validate the tracking algorithm's performance, we compare it against Vicon-based ground truth. Additionally, we evaluate the reliability of the foundational models in aiding tracking in scenarios involving occlusions. Finally, we test and validate the model's ability to work seamlessly with multiple modalities, such as click, bounding box, and image templates.","sentences":["Object tracking is a fundamental task in computer vision with broad practical applications across various domains, including traffic monitoring, robotics, and autonomous vehicle tracking.","In this project, we aim to develop a sophisticated aerial vehicle system known as Track Anything Raptor (TAR), designed to detect, segment, and track objects of interest based on user-provided multimodal queries, such as text, images, and clicks.","TAR utilizes cutting-edge pre-trained models like DINO, CLIP, and SAM to estimate the relative pose of the queried object.","The tracking problem is approached as a Visual Servoing task, enabling the UAV to consistently focus on the object through advanced motion planning and control algorithms.","We showcase how the integration of these foundational models with a custom high-level control algorithm results in a highly stable and precise tracking system deployed on a custom-built PX4 Autopilot-enabled Voxl2 M500 drone.","To validate the tracking algorithm's performance, we compare it against Vicon-based ground truth.","Additionally, we evaluate the reliability of the foundational models in aiding tracking in scenarios involving occlusions.","Finally, we test and validate the model's ability to work seamlessly with multiple modalities, such as click, bounding box, and image templates."],"url":"http://arxiv.org/abs/2405.11655v1","category":"cs.CV"}
{"created":"2024-05-19 19:32:12","title":"Movie Revenue Prediction using Machine Learning Models","abstract":"In the contemporary film industry, accurately predicting a movie's earnings is paramount for maximizing profitability. This project aims to develop a machine learning model for predicting movie earnings based on input features like the movie name, the MPAA rating of the movie, the genre of the movie, the year of release of the movie, the IMDb Rating, the votes by the watchers, the director, the writer and the leading cast, the country of production of the movie, the budget of the movie, the production company and the runtime of the movie. Through a structured methodology involving data collection, preprocessing, analysis, model selection, evaluation, and improvement, a robust predictive model is constructed. Linear Regression, Decision Trees, Random Forest Regression, Bagging, XGBoosting and Gradient Boosting have been trained and tested. Model improvement strategies include hyperparameter tuning and cross-validation. The resulting model offers promising accuracy and generalization, facilitating informed decision-making in the film industry to maximize profits.","sentences":["In the contemporary film industry, accurately predicting a movie's earnings is paramount for maximizing profitability.","This project aims to develop a machine learning model for predicting movie earnings based on input features like the movie name, the MPAA rating of the movie, the genre of the movie, the year of release of the movie, the IMDb Rating, the votes by the watchers, the director, the writer and the leading cast, the country of production of the movie, the budget of the movie, the production company and the runtime of the movie.","Through a structured methodology involving data collection, preprocessing, analysis, model selection, evaluation, and improvement, a robust predictive model is constructed.","Linear Regression, Decision Trees, Random Forest Regression, Bagging, XGBoosting and Gradient Boosting have been trained and tested.","Model improvement strategies include hyperparameter tuning and cross-validation.","The resulting model offers promising accuracy and generalization, facilitating informed decision-making in the film industry to maximize profits."],"url":"http://arxiv.org/abs/2405.11651v1","category":"cs.LG"}
{"created":"2024-05-19 18:57:25","title":"Hummer: Towards Limited Competitive Preference Dataset","abstract":"Preference datasets are essential for incorporating human preferences into pre-trained language models, playing a key role in the success of Reinforcement Learning from Human Feedback. However, these datasets often demonstrate conflicting alignment objectives, leading to increased vulnerability to jailbreak attacks and challenges in adapting downstream tasks to prioritize specific alignment objectives without negatively impacting others. In this work, we introduce a novel statistical metric, Alignment Dimension Conflict, to quantify the degree of conflict within preference datasets. We then present \\texttt{Hummer} and its fine-grained variant, \\texttt{Hummer-F}, as innovative pairwise preference datasets with reduced-conflict alignment objectives. \\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback from GPT-4, marking as the first preference dataset aimed at reducing the competition between alignment objectives. Furthermore, we develop reward models, HummerRM and HummerRM-F, which employ a hybrid sampling approach to balance diverse alignment objectives effectively. This sampling method positions HummerRM as an ideal model for domain-specific further fine-tuning and reducing vulnerabilities to attacks.","sentences":["Preference datasets are essential for incorporating human preferences into pre-trained language models, playing a key role in the success of Reinforcement Learning from Human Feedback.","However, these datasets often demonstrate conflicting alignment objectives, leading to increased vulnerability to jailbreak attacks and challenges in adapting downstream tasks to prioritize specific alignment objectives without negatively impacting others.","In this work, we introduce a novel statistical metric, Alignment Dimension Conflict, to quantify the degree of conflict within preference datasets.","We then present \\texttt{Hummer} and its fine-grained variant, \\texttt{Hummer-F}, as innovative pairwise preference datasets with reduced-conflict alignment objectives.","\\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback from GPT-4, marking as the first preference dataset aimed at reducing the competition between alignment objectives.","Furthermore, we develop reward models, HummerRM and HummerRM-F, which employ a hybrid sampling approach to balance diverse alignment objectives effectively.","This sampling method positions HummerRM as an ideal model for domain-specific further fine-tuning and reducing vulnerabilities to attacks."],"url":"http://arxiv.org/abs/2405.11647v1","category":"cs.AI"}
{"created":"2024-05-19 18:26:11","title":"Inquire, Interact, and Integrate: A Proactive Agent Collaborative Framework for Zero-Shot Multimodal Medical Reasoning","abstract":"The adoption of large language models (LLMs) in healthcare has attracted significant research interest. However, their performance in healthcare remains under-investigated and potentially limited, due to i) they lack rich domain-specific knowledge and medical reasoning skills; and ii) most state-of-the-art LLMs are unimodal, text-only models that cannot directly process multimodal inputs. To this end, we propose a multimodal medical collaborative reasoning framework \\textbf{MultiMedRes}, which incorporates a learner agent to proactively gain essential information from domain-specific expert models, to solve medical multimodal reasoning problems. Our method includes three steps: i) \\textbf{Inquire}: The learner agent first decomposes given complex medical reasoning problems into multiple domain-specific sub-problems; ii) \\textbf{Interact}: The agent then interacts with domain-specific expert models by repeating the ``ask-answer'' process to progressively obtain different domain-specific knowledge; iii) \\textbf{Integrate}: The agent finally integrates all the acquired domain-specific knowledge to accurately address the medical reasoning problem. We validate the effectiveness of our method on the task of difference visual question answering for X-ray images. The experiments demonstrate that our zero-shot prediction achieves state-of-the-art performance, and even outperforms the fully supervised methods. Besides, our approach can be incorporated into various LLMs and multimodal LLMs to significantly boost their performance.","sentences":["The adoption of large language models (LLMs) in healthcare has attracted significant research interest.","However, their performance in healthcare remains under-investigated and potentially limited, due to i) they lack rich domain-specific knowledge and medical reasoning skills; and ii) most state-of-the-art LLMs are unimodal, text-only models that cannot directly process multimodal inputs.","To this end, we propose a multimodal medical collaborative reasoning framework \\textbf{MultiMedRes}, which incorporates a learner agent to proactively gain essential information from domain-specific expert models, to solve medical multimodal reasoning problems.","Our method includes three steps: i) \\textbf{Inquire}: The learner agent first decomposes given complex medical reasoning problems into multiple domain-specific sub-problems; ii) \\textbf{Interact}: The agent then interacts with domain-specific expert models by repeating the ``ask-answer'' process to progressively obtain different domain-specific knowledge; iii) \\textbf{Integrate}: The agent finally integrates all the acquired domain-specific knowledge to accurately address the medical reasoning problem.","We validate the effectiveness of our method on the task of difference visual question answering for X-ray images.","The experiments demonstrate that our zero-shot prediction achieves state-of-the-art performance, and even outperforms the fully supervised methods.","Besides, our approach can be incorporated into various LLMs and multimodal LLMs to significantly boost their performance."],"url":"http://arxiv.org/abs/2405.11640v1","category":"cs.AI"}
{"created":"2024-05-19 18:16:03","title":"Fair Set Cover","abstract":"The potential harms of algorithmic decisions have ignited algorithmic fairness as a central topic in computer science. One of the fundamental problems in computer science is Set Cover, which has numerous applications with societal impacts, such as assembling a small team of individuals that collectively satisfy a range of expertise requirements. However, despite its broad application spectrum and significant potential impact, set cover has yet to be studied through the lens of fairness. Therefore, in this paper, we introduce Fair Set Cover, which aims not only to cover with a minimum-size set but also to satisfy demographic parity in its selection of sets. To this end, we develop multiple versions of fair set cover, study their hardness, and devise efficient approximation algorithms for each variant. Notably, under certain assumptions, our algorithms always guarantees zero-unfairness, with only a small increase in the approximation ratio compared to regular set cover. Furthermore, our experiments on various data sets and across different settings confirm the negligible price of fairness, as (a) the output size increases only slightly (if any) and (b) the time to compute the output does not significantly increase.","sentences":["The potential harms of algorithmic decisions have ignited algorithmic fairness as a central topic in computer science.","One of the fundamental problems in computer science is Set Cover, which has numerous applications with societal impacts, such as assembling a small team of individuals that collectively satisfy a range of expertise requirements.","However, despite its broad application spectrum and significant potential impact, set cover has yet to be studied through the lens of fairness.","Therefore, in this paper, we introduce Fair Set Cover, which aims not only to cover with a minimum-size set but also to satisfy demographic parity in its selection of sets.","To this end, we develop multiple versions of fair set cover, study their hardness, and devise efficient approximation algorithms for each variant.","Notably, under certain assumptions, our algorithms always guarantees zero-unfairness, with only a small increase in the approximation ratio compared to regular set cover.","Furthermore, our experiments on various data sets and across different settings confirm the negligible price of fairness, as (a) the output size increases only slightly (if any) and (b) the time to compute the output does not significantly increase."],"url":"http://arxiv.org/abs/2405.11639v1","category":"cs.DS"}
{"created":"2024-05-19 17:42:24","title":"Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems","abstract":"Numerous studies on adversarial attacks targeting self-driving policies fail to incorporate realistic-looking adversarial objects, limiting real-world applicability. Building upon prior research that facilitated the transition of adversarial objects from simulations to practical applications, this paper discusses a modified gradient-based texture optimization method to discover realistic-looking adversarial objects. While retaining the core architecture and techniques of the prior research, the proposed addition involves an entity termed the 'Judge'. This agent assesses the texture of a rendered object, assigning a probability score reflecting its realism. This score is integrated into the loss function to encourage the NeRF object renderer to concurrently learn realistic and adversarial textures. The paper analyzes four strategies for developing a robust 'Judge': 1) Leveraging cutting-edge vision-language models. 2) Fine-tuning open-sourced vision-language models. 3) Pretraining neurosymbolic systems. 4) Utilizing traditional image processing techniques. Our findings indicate that strategies 1) and 4) yield less reliable outcomes, pointing towards strategies 2) or 3) as more promising directions for future research.","sentences":["Numerous studies on adversarial attacks targeting self-driving policies fail to incorporate realistic-looking adversarial objects, limiting real-world applicability.","Building upon prior research that facilitated the transition of adversarial objects from simulations to practical applications, this paper discusses a modified gradient-based texture optimization method to discover realistic-looking adversarial objects.","While retaining the core architecture and techniques of the prior research, the proposed addition involves an entity termed the 'Judge'.","This agent assesses the texture of a rendered object, assigning a probability score reflecting its realism.","This score is integrated into the loss function to encourage the NeRF object renderer to concurrently learn realistic and adversarial textures.","The paper analyzes four strategies for developing a robust 'Judge': 1) Leveraging cutting-edge vision-language models.","2) Fine-tuning open-sourced vision-language models.","3) Pretraining neurosymbolic systems.","4) Utilizing traditional image processing techniques.","Our findings indicate that strategies 1) and 4) yield less reliable outcomes, pointing towards strategies 2) or 3) as more promising directions for future research."],"url":"http://arxiv.org/abs/2405.11629v1","category":"cs.CV"}
{"created":"2024-05-19 17:20:20","title":"Computer Vision in the Food Industry: Accurate, Real-time, and Automatic Food Recognition with Pretrained MobileNetV2","abstract":"In contemporary society, the application of artificial intelligence for automatic food recognition offers substantial potential for nutrition tracking, reducing food waste, and enhancing productivity in food production and consumption scenarios. Modern technologies such as Computer Vision and Deep Learning are highly beneficial, enabling machines to learn automatically, thereby facilitating automatic visual recognition. Despite some research in this field, the challenge of achieving accurate automatic food recognition quickly remains a significant research gap. Some models have been developed and implemented, but maintaining high performance swiftly, with low computational cost and low access to expensive hardware accelerators, still needs further exploration and research. This study employs the pretrained MobileNetV2 model, which is efficient and fast, for food recognition on the public Food11 dataset, comprising 16643 images. It also utilizes various techniques such as dataset understanding, transfer learning, data augmentation, regularization, dynamic learning rate, hyperparameter tuning, and consideration of images in different sizes to enhance performance and robustness. These techniques aid in choosing appropriate metrics, achieving better performance, avoiding overfitting and accuracy fluctuations, speeding up the model, and increasing the generalization of findings, making the study and its results applicable to practical applications. Despite employing a light model with a simpler structure and fewer trainable parameters compared to some deep and dense models in the deep learning area, it achieved commendable accuracy in a short time. This underscores the potential for practical implementation, which is the main intention of this study.","sentences":["In contemporary society, the application of artificial intelligence for automatic food recognition offers substantial potential for nutrition tracking, reducing food waste, and enhancing productivity in food production and consumption scenarios.","Modern technologies such as Computer Vision and Deep Learning are highly beneficial, enabling machines to learn automatically, thereby facilitating automatic visual recognition.","Despite some research in this field, the challenge of achieving accurate automatic food recognition quickly remains a significant research gap.","Some models have been developed and implemented, but maintaining high performance swiftly, with low computational cost and low access to expensive hardware accelerators, still needs further exploration and research.","This study employs the pretrained MobileNetV2 model, which is efficient and fast, for food recognition on the public Food11 dataset, comprising 16643 images.","It also utilizes various techniques such as dataset understanding, transfer learning, data augmentation, regularization, dynamic learning rate, hyperparameter tuning, and consideration of images in different sizes to enhance performance and robustness.","These techniques aid in choosing appropriate metrics, achieving better performance, avoiding overfitting and accuracy fluctuations, speeding up the model, and increasing the generalization of findings, making the study and its results applicable to practical applications.","Despite employing a light model with a simpler structure and fewer trainable parameters compared to some deep and dense models in the deep learning area, it achieved commendable accuracy in a short time.","This underscores the potential for practical implementation, which is the main intention of this study."],"url":"http://arxiv.org/abs/2405.11621v1","category":"cs.CV"}
{"created":"2024-05-19 17:18:27","title":"Novel Interpretable and Robust Web-based AI Platform for Phishing Email Detection","abstract":"Phishing emails continue to pose a significant threat, causing financial losses and security breaches. This study addresses limitations in existing research, such as reliance on proprietary datasets and lack of real-world application, by proposing a high-performance machine learning model for email classification. Utilizing a comprehensive and largest available public dataset, the model achieves a f1 score of 0.99 and is designed for deployment within relevant applications. Additionally, Explainable AI (XAI) is integrated to enhance user trust. This research offers a practical and highly accurate solution, contributing to the fight against phishing by empowering users with a real-time web-based application for phishing email detection.","sentences":["Phishing emails continue to pose a significant threat, causing financial losses and security breaches.","This study addresses limitations in existing research, such as reliance on proprietary datasets and lack of real-world application, by proposing a high-performance machine learning model for email classification.","Utilizing a comprehensive and largest available public dataset, the model achieves a f1 score of 0.99 and is designed for deployment within relevant applications.","Additionally, Explainable AI (XAI) is integrated to enhance user trust.","This research offers a practical and highly accurate solution, contributing to the fight against phishing by empowering users with a real-time web-based application for phishing email detection."],"url":"http://arxiv.org/abs/2405.11619v1","category":"cs.LG"}
{"created":"2024-05-19 17:17:35","title":"Transcriptomics-guided Slide Representation Learning in Computational Pathology","abstract":"Self-supervised learning (SSL) has been successful in building patch embeddings of small histology images (e.g., 224x224 pixels), but scaling these models to learn slide embeddings from the entirety of giga-pixel whole-slide images (WSIs) remains challenging. Here, we leverage complementary information from gene expression profiles to guide slide representation learning using multimodal pre-training. Expression profiles constitute highly detailed molecular descriptions of a tissue that we hypothesize offer a strong task-agnostic training signal for learning slide embeddings. Our slide and expression (S+E) pre-training strategy, called Tangle, employs modality-specific encoders, the outputs of which are aligned via contrastive learning. Tangle was pre-trained on samples from three different organs: liver (n=6,597 S+E pairs), breast (n=1,020), and lung (n=1,012) from two different species (Homo sapiens and Rattus norvegicus). Across three independent test datasets consisting of 1,265 breast WSIs, 1,946 lung WSIs, and 4,584 liver WSIs, Tangle shows significantly better few-shot performance compared to supervised and SSL baselines. When assessed using prototype-based classification and slide retrieval, Tangle also shows a substantial performance improvement over all baselines. Code available at https://github.com/mahmoodlab/TANGLE.","sentences":["Self-supervised learning (SSL) has been successful in building patch embeddings of small histology images (e.g., 224x224 pixels), but scaling these models to learn slide embeddings from the entirety of giga-pixel whole-slide images (WSIs) remains challenging.","Here, we leverage complementary information from gene expression profiles to guide slide representation learning using multimodal pre-training.","Expression profiles constitute highly detailed molecular descriptions of a tissue that we hypothesize offer a strong task-agnostic training signal for learning slide embeddings.","Our slide and expression (S+E) pre-training strategy, called Tangle, employs modality-specific encoders, the outputs of which are aligned via contrastive learning.","Tangle was pre-trained on samples from three different organs: liver (n=6,597 S+E pairs), breast (n=1,020), and lung (n=1,012) from two different species (Homo sapiens and Rattus norvegicus).","Across three independent test datasets consisting of 1,265 breast WSIs, 1,946 lung WSIs, and 4,584 liver WSIs, Tangle shows significantly better few-shot performance compared to supervised and SSL baselines.","When assessed using prototype-based classification and slide retrieval, Tangle also shows a substantial performance improvement over all baselines.","Code available at https://github.com/mahmoodlab/TANGLE."],"url":"http://arxiv.org/abs/2405.11618v1","category":"cs.CV"}
{"created":"2024-05-19 17:04:39","title":"Sociotechnical Implications of Generative Artificial Intelligence for Information Access","abstract":"Robust access to trustworthy information is a critical need for society with implications for knowledge production, public health education, and promoting informed citizenry in democratic societies. Generative AI technologies may enable new ways to access information and improve effectiveness of existing information retrieval systems but we are only starting to understand and grapple with their long-term social implications. In this chapter, we present an overview of some of the systemic consequences and risks of employing generative AI in the context of information access. We also provide recommendations for evaluation and mitigation, and discuss challenges for future research.","sentences":["Robust access to trustworthy information is a critical need for society with implications for knowledge production, public health education, and promoting informed citizenry in democratic societies.","Generative AI technologies may enable new ways to access information and improve effectiveness of existing information retrieval systems but we are only starting to understand and grapple with their long-term social implications.","In this chapter, we present an overview of some of the systemic consequences and risks of employing generative AI in the context of information access.","We also provide recommendations for evaluation and mitigation, and discuss challenges for future research."],"url":"http://arxiv.org/abs/2405.11612v1","category":"cs.IR"}
{"created":"2024-05-19 16:23:57","title":"Search for $CP$ violation in D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ decays in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search is reported for charge-parity D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ $CP$ violation in D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ decays, using data collected in proton-proton collisions at $\\sqrt{s}$ = 13 TeV recorded by the CMS experiment in 2018. The analysis uses a dedicated data set that corresponds to an integrated luminosity of 41.6 fb$^{-1}$, which consists of about 10 billion events containing a pair of \\b hadrons, nearly all of which decay to charm hadrons. The flavor of the neutral D meson is determined by the pion charge in the reconstructed decays D$^{*+}$ $\\to$ D$^0\\pi^+$ and D$^{*-}$ $\\to$ D$^0\\pi^-$. The D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ $CP$ asymmetry in D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ is measured to be $A_{CP}$( K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$) = (6.2 $\\pm$ 3.0 $\\pm$ 0.2 $\\pm$ 0.8)%, where the three uncertainties represent the statistical uncertainty, the systematic uncertainty, and the uncertainty in the measurement of the D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ $CP$ asymmetry in the D$^0$ $\\to$ K$^0_\\mathrm{S}\\pi^+\\pi^-$ decay. This is the first D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ $CP$ asymmetry measurement by CMS in the charm sector as well as the first to utilize a fully hadronic final state.","sentences":["A search is reported for charge-parity D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ $CP$ violation in D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ decays, using data collected in proton-proton collisions at $\\sqrt{s}$ = 13 TeV recorded by the CMS experiment in 2018.","The analysis uses a dedicated data set that corresponds to an integrated luminosity of 41.6 fb$^{-1}$, which consists of about 10 billion events containing a pair of \\b hadrons, nearly all of which decay to charm hadrons.","The flavor of the neutral D meson is determined by the pion charge in the reconstructed decays D$^{*+}$","$\\to$ D$^0\\pi^+$ and D$^{*-}$ $\\to$ D$^0\\pi^-$. The D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ $CP$ asymmetry in D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ is measured to be $A_{CP}$( K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$) = (6.2 $\\pm$ 3.0 $\\pm$ 0.2 $\\pm$ 0.8)%, where the three uncertainties represent the statistical uncertainty, the systematic uncertainty, and the uncertainty in the measurement of the D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ $CP$ asymmetry in the D$^0$ $\\to$ K$^0_\\mathrm{S}\\pi^+\\pi^-$ decay.","This is the first D$^0$ $\\to$ K$^0_\\mathrm{S}$K$^0_\\mathrm{S}$ $CP$ asymmetry measurement by CMS in the charm sector as well as the first to utilize a fully hadronic final state."],"url":"http://arxiv.org/abs/2405.11606v1","category":"hep-ex"}
{"created":"2024-05-19 16:10:03","title":"How to integrate cloud service, data analytic and machine learning technique to reduce cyber risks associated with the modern cloud based infrastructure","abstract":"The combination of cloud technology, machine learning, and data visualization techniques allows hybrid enterprise networks to hold massive volumes of data and provide employees and customers easy access to these cloud data. These massive collections of complex data sets are facing security challenges. While cloud platforms are more vulnerable to security threats and traditional security technologies are unable to cope with the rapid data explosion in cloud platforms, machine learning powered security solutions and data visualization techniques are playing instrumental roles in detecting security threat, data breaches, and automatic finding software vulnerabilities. The purpose of this paper is to present some of the widely used cloud services, machine learning techniques and data visualization approach and demonstrate how to integrate cloud service, data analytic and machine learning techniques that can be used to detect and reduce cyber risks associated with the modern cloud based infrastructure. In this paper I applied the machine learning supervised classifier to design a model based on well-known UNSW-NB15 dataset to predict the network behavior metrics and demonstrated how data analytics techniques can be integrated to visualize network traffics.","sentences":["The combination of cloud technology, machine learning, and data visualization techniques allows hybrid enterprise networks to hold massive volumes of data and provide employees and customers easy access to these cloud data.","These massive collections of complex data sets are facing security challenges.","While cloud platforms are more vulnerable to security threats and traditional security technologies are unable to cope with the rapid data explosion in cloud platforms, machine learning powered security solutions and data visualization techniques are playing instrumental roles in detecting security threat, data breaches, and automatic finding software vulnerabilities.","The purpose of this paper is to present some of the widely used cloud services, machine learning techniques and data visualization approach and demonstrate how to integrate cloud service, data analytic and machine learning techniques that can be used to detect and reduce cyber risks associated with the modern cloud based infrastructure.","In this paper I applied the machine learning supervised classifier to design a model based on well-known UNSW-NB15 dataset to predict the network behavior metrics and demonstrated how data analytics techniques can be integrated to visualize network traffics."],"url":"http://arxiv.org/abs/2405.11601v1","category":"cs.LG"}
{"created":"2024-05-19 16:06:26","title":"AI-Assisted Diagnosis for Covid-19 CXR Screening: From Data Collection to Clinical Validation","abstract":"In this paper, we present the major results from the Covid Radiographic imaging System based on AI (Co.R.S.A.) project, which took place in Italy. This project aims to develop a state-of-the-art AI-based system for diagnosing Covid-19 pneumonia from Chest X-ray (CXR) images. The contributions of this work are manyfold: the release of the public CORDA dataset, a deep learning pipeline for Covid-19 detection, and the clinical validation of the developed solution by expert radiologists. The proposed detection model is based on a two-step approach that, paired with state-of-the-art debiasing, provides reliable results. Most importantly, our investigation includes the actual usage of the diagnosis aid tool by radiologists, allowing us to assess the real benefits in terms of accuracy and time efficiency. Project homepage: https://corsa.di.unito.it/","sentences":["In this paper, we present the major results from the Covid Radiographic imaging System based on AI (Co.R.S.A.) project, which took place in Italy.","This project aims to develop a state-of-the-art AI-based system for diagnosing Covid-19 pneumonia from Chest X-ray (CXR) images.","The contributions of this work are manyfold: the release of the public CORDA dataset, a deep learning pipeline for Covid-19 detection, and the clinical validation of the developed solution by expert radiologists.","The proposed detection model is based on a two-step approach that, paired with state-of-the-art debiasing, provides reliable results.","Most importantly, our investigation includes the actual usage of the diagnosis aid tool by radiologists, allowing us to assess the real benefits in terms of accuracy and time efficiency.","Project homepage: https://corsa.di.unito.it/"],"url":"http://arxiv.org/abs/2405.11598v1","category":"eess.IV"}
{"created":"2024-05-19 16:06:02","title":"Language Reconstruction with Brain Predictive Coding from fMRI Data","abstract":"Many recent studies have shown that the perception of speech can be decoded from brain signals and subsequently reconstructed as continuous language. However, there is a lack of neurological basis for how the semantic information embedded within brain signals can be used more effectively to guide language reconstruction. The theory of predictive coding suggests that human brain naturally engages in continuously predicting future word representations that span multiple timescales. This implies that the decoding of brain signals could potentially be associated with a predictable future. To explore the predictive coding theory within the context of language reconstruction, this paper proposes a novel model \\textsc{PredFT} for jointly modeling neural decoding and brain prediction. It consists of a main decoding network for language reconstruction and a side network for predictive coding. The side network obtains brain predictive coding representation from related brain regions of interest with a multi-head self-attention module. This representation is fused into the main decoding network with cross-attention to facilitate the language models' generation process. Experiments are conducted on the largest naturalistic language comprehension fMRI dataset Narratives. \\textsc{PredFT} achieves current state-of-the-art decoding performance with a maximum BLEU-1 score of $27.8\\%$.","sentences":["Many recent studies have shown that the perception of speech can be decoded from brain signals and subsequently reconstructed as continuous language.","However, there is a lack of neurological basis for how the semantic information embedded within brain signals can be used more effectively to guide language reconstruction.","The theory of predictive coding suggests that human brain naturally engages in continuously predicting future word representations that span multiple timescales.","This implies that the decoding of brain signals could potentially be associated with a predictable future.","To explore the predictive coding theory within the context of language reconstruction, this paper proposes a novel model \\textsc{PredFT} for jointly modeling neural decoding and brain prediction.","It consists of a main decoding network for language reconstruction and a side network for predictive coding.","The side network obtains brain predictive coding representation from related brain regions of interest with a multi-head self-attention module.","This representation is fused into the main decoding network with cross-attention to facilitate the language models' generation process.","Experiments are conducted on the largest naturalistic language comprehension fMRI dataset Narratives.","\\textsc{PredFT} achieves current state-of-the-art decoding performance with a maximum BLEU-1 score of $27.8\\%$."],"url":"http://arxiv.org/abs/2405.11597v1","category":"cs.CL"}
{"created":"2024-05-19 15:28:07","title":"Improved measurement of the branching fraction of $h_{c}\\rightarrow\u03b3\u03b7^\\prime/\u03b7$ and search for $h_{c}\\rightarrow\u03b3\u03c0^0$","abstract":"The processes $h_c\\rightarrow\\gamma P(P = \\eta^\\prime,~\\eta,~\\pi^{0}))$ are studied with a sample of $(27.12\\pm0.14)\\times10^{8}$ $\\psi(3686)$ events collected by the BESIII detector at the BEPCII collider. The branching fractions of $h_c\\rightarrow\\gamma\\eta^\\prime$ and $h_c\\rightarrow\\gamma\\eta$ are measured to be $(1.40\\pm0.11\\pm0.04\\pm0.10)\\times10^{-3}$ and $(3.77\\pm0.55\\pm0.13\\pm0.26)\\times10^{-4}$, respectively, where the first uncertainties are statistical, the second systematic, and the third from the branching fraction of $\\psi(3686)\\rightarrow\\pi^{0}h_c$. The ratio $R_{h_c}=\\frac{\\mathscr{B}(h_c\\rightarrow\\gamma\\eta)}{\\mathscr{B}(h_c\\rightarrow\\gamma\\eta^\\prime)}$ is calculated to be $(27.0\\pm4.4\\pm1.0)\\%$. The measurements are consistent with the previous results with improved precision by a factor of 2. The results are valuable for gaining a deeper understanding of $\\eta-\\eta^\\prime$ mixing, and its manifestation within quantum chromodynamics. No significant signal is found for the decay $h_c\\rightarrow\\gamma\\pi^{0}$, and an upper limit is placed on its branching fraction of $\\mathscr{B}(h_c\\rightarrow\\gamma\\pi^{0})<5.0\\times10^{-5}$, at the 90\\% confidence level.","sentences":["The processes $h_c\\rightarrow\\gamma P(P = \\eta^\\prime,~\\eta,~\\pi^{0}))$ are studied with a sample of $(27.12\\pm0.14)\\times10^{8}$ $\\psi(3686)$ events collected by the BESIII detector at the BEPCII collider.","The branching fractions of $h_c\\rightarrow\\gamma\\eta^\\prime$ and $h_c\\rightarrow\\gamma\\eta$ are measured to be $(1.40\\pm0.11\\pm0.04\\pm0.10)\\times10^{-3}$ and $(3.77\\pm0.55\\pm0.13\\pm0.26)\\times10^{-4}$, respectively, where the first uncertainties are statistical, the second systematic, and the third from the branching fraction of $\\psi(3686)\\rightarrow\\pi^{0}h_c$. The ratio $R_{h_c}=\\frac{\\mathscr{B}(h_c\\rightarrow\\gamma\\eta)}{\\mathscr{B}(h_c\\rightarrow\\gamma\\eta^\\prime)}$ is calculated to be $(27.0\\pm4.4\\pm1.0)\\%$.","The measurements are consistent with the previous results with improved precision by a factor of 2.","The results are valuable for gaining a deeper understanding of $\\eta-\\eta^\\prime$ mixing, and its manifestation within quantum chromodynamics.","No significant signal is found for the decay $h_c\\rightarrow\\gamma\\pi^{0}$, and an upper limit is placed on its branching fraction of $\\mathscr{B}(h_c\\rightarrow\\gamma\\pi^{0})<5.0\\times10^{-5}$, at the 90\\% confidence level."],"url":"http://arxiv.org/abs/2405.11585v1","category":"hep-ex"}
{"created":"2024-05-19 15:15:18","title":"Securing Health Data on the Blockchain: A Differential Privacy and Federated Learning Framework","abstract":"This study proposes a framework to enhance privacy in Blockchain-based Internet of Things (BIoT) systems used in the healthcare sector. The framework addresses the challenge of leveraging health data for analytics while protecting patient privacy. To achieve this, the study integrates Differential Privacy (DP) with Federated Learning (FL) to protect sensitive health data collected by IoT nodes. The proposed framework utilizes dynamic personalization and adaptive noise distribution strategies to balance privacy and data utility. Additionally, blockchain technology ensures secure and transparent aggregation and storage of model updates. Experimental results on the SVHN dataset demonstrate that the proposed framework achieves strong privacy guarantees against various attack scenarios while maintaining high accuracy in health analytics tasks. For 15 rounds of federated learning with an epsilon value of 8.0, the model obtains an accuracy of 64.50%. The blockchain integration, utilizing Ethereum, Ganache, Web3.py, and IPFS, exhibits an average transaction latency of around 6 seconds and consistent gas consumption across rounds, validating the practicality and feasibility of the proposed approach.","sentences":["This study proposes a framework to enhance privacy in Blockchain-based Internet of Things (BIoT) systems used in the healthcare sector.","The framework addresses the challenge of leveraging health data for analytics while protecting patient privacy.","To achieve this, the study integrates Differential Privacy (DP) with Federated Learning (FL) to protect sensitive health data collected by IoT nodes.","The proposed framework utilizes dynamic personalization and adaptive noise distribution strategies to balance privacy and data utility.","Additionally, blockchain technology ensures secure and transparent aggregation and storage of model updates.","Experimental results on the SVHN dataset demonstrate that the proposed framework achieves strong privacy guarantees against various attack scenarios while maintaining high accuracy in health analytics tasks.","For 15 rounds of federated learning with an epsilon value of 8.0, the model obtains an accuracy of 64.50%.","The blockchain integration, utilizing Ethereum, Ganache, Web3.py, and IPFS, exhibits an average transaction latency of around 6 seconds and consistent gas consumption across rounds, validating the practicality and feasibility of the proposed approach."],"url":"http://arxiv.org/abs/2405.11580v1","category":"cs.CR"}
{"created":"2024-05-19 15:13:51","title":"Exploring the Capabilities of Prompted Large Language Models in Educational and Assessment Applications","abstract":"In the era of generative artificial intelligence (AI), the fusion of large language models (LLMs) offers unprecedented opportunities for innovation in the field of modern education. We embark on an exploration of prompted LLMs within the context of educational and assessment applications to uncover their potential. Through a series of carefully crafted research questions, we investigate the effectiveness of prompt-based techniques in generating open-ended questions from school-level textbooks, assess their efficiency in generating open-ended questions from undergraduate-level technical textbooks, and explore the feasibility of employing a chain-of-thought inspired multi-stage prompting approach for language-agnostic multiple-choice question (MCQ) generation. Additionally, we evaluate the ability of prompted LLMs for language learning, exemplified through a case study in the low-resource Indian language Bengali, to explain Bengali grammatical errors. We also evaluate the potential of prompted LLMs to assess human resource (HR) spoken interview transcripts. By juxtaposing the capabilities of LLMs with those of human experts across various educational tasks and domains, our aim is to shed light on the potential and limitations of LLMs in reshaping educational practices.","sentences":["In the era of generative artificial intelligence (AI), the fusion of large language models (LLMs) offers unprecedented opportunities for innovation in the field of modern education.","We embark on an exploration of prompted LLMs within the context of educational and assessment applications to uncover their potential.","Through a series of carefully crafted research questions, we investigate the effectiveness of prompt-based techniques in generating open-ended questions from school-level textbooks, assess their efficiency in generating open-ended questions from undergraduate-level technical textbooks, and explore the feasibility of employing a chain-of-thought inspired multi-stage prompting approach for language-agnostic multiple-choice question (MCQ) generation.","Additionally, we evaluate the ability of prompted LLMs for language learning, exemplified through a case study in the low-resource Indian language Bengali, to explain Bengali grammatical errors.","We also evaluate the potential of prompted LLMs to assess human resource (HR) spoken interview transcripts.","By juxtaposing the capabilities of LLMs with those of human experts across various educational tasks and domains, our aim is to shed light on the potential and limitations of LLMs in reshaping educational practices."],"url":"http://arxiv.org/abs/2405.11579v1","category":"cs.CL"}
{"created":"2024-05-19 15:00:50","title":"A Multi-Perspective Analysis of Memorization in Large Language Models","abstract":"Large Language Models (LLMs), trained on massive corpora with billions of parameters, show unprecedented performance in various fields. Though surprised by their excellent performances, researchers also noticed some special behaviors of those LLMs. One of those behaviors is memorization, in which LLMs can generate the same content used to train them. Though previous research has discussed memorization, the memorization of LLMs still lacks explanation, especially the cause of memorization and the dynamics of generating them. In this research, we comprehensively discussed memorization from various perspectives and extended the discussion scope to not only just the memorized content but also less and unmemorized content. Through various studies, we found that: (1) Through experiments, we revealed the relation of memorization between model size, continuation size, and context size. Further, we showed how unmemorized sentences transition to memorized sentences. (2) Through embedding analysis, we showed the distribution and decoding dynamics across model size in embedding space for sentences with different memorization scores. The n-gram statistics analysis presents d (3) An analysis over n-gram and entropy decoding dynamics discovered a boundary effect when the model starts to generate memorized sentences or unmemorized sentences. (4)We trained a Transformer model to predict the memorization of different models, showing that it is possible to predict memorizations by context.","sentences":["Large Language Models (LLMs), trained on massive corpora with billions of parameters, show unprecedented performance in various fields.","Though surprised by their excellent performances, researchers also noticed some special behaviors of those LLMs.","One of those behaviors is memorization, in which LLMs can generate the same content used to train them.","Though previous research has discussed memorization, the memorization of LLMs still lacks explanation, especially the cause of memorization and the dynamics of generating them.","In this research, we comprehensively discussed memorization from various perspectives and extended the discussion scope to not only just the memorized content but also less and unmemorized content.","Through various studies, we found that: (1) Through experiments, we revealed the relation of memorization between model size, continuation size, and context size.","Further, we showed how unmemorized sentences transition to memorized sentences.","(2) Through embedding analysis, we showed the distribution and decoding dynamics across model size in embedding space for sentences with different memorization scores.","The n-gram statistics analysis presents d (3) An analysis over n-gram and entropy decoding dynamics discovered a boundary effect when the model starts to generate memorized sentences or unmemorized sentences.","(4)We trained a Transformer model to predict the memorization of different models, showing that it is possible to predict memorizations by context."],"url":"http://arxiv.org/abs/2405.11577v1","category":"cs.CL"}
{"created":"2024-05-19 14:48:19","title":"Reproducibility Study of CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification","abstract":"This report is a reproducibility study of the paper \"CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification\" (Abdelfattah et al, ICCV 2023). Our report makes the following contributions: (1) We provide a reproducible, well commented and open-sourced code implementation for the entire method specified in the original paper. (2) We try to verify the effectiveness of the novel aggregation strategy which uses the CLIP model to initialize the pseudo labels for the subsequent unsupervised multi-label image classification task. (3) We try to verify the effectiveness of the gradient-alignment training method specified in the original paper, which is used to update the network parameters and pseudo labels. The code can be found at https://github.com/cs-mshah/CDUL","sentences":["This report is a reproducibility study of the paper \"CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification\" (Abdelfattah et al, ICCV 2023).","Our report makes the following contributions: (1) We provide a reproducible, well commented and open-sourced code implementation for the entire method specified in the original paper.","(2) We try to verify the effectiveness of the novel aggregation strategy which uses the CLIP model to initialize the pseudo labels for the subsequent unsupervised multi-label image classification task.","(3) We try to verify the effectiveness of the gradient-alignment training method specified in the original paper, which is used to update the network parameters and pseudo labels.","The code can be found at https://github.com/cs-mshah/CDUL"],"url":"http://arxiv.org/abs/2405.11574v1","category":"cs.CV"}
{"created":"2024-05-19 14:21:53","title":"DaVinci at SemEval-2024 Task 9: Few-shot prompting GPT-3.5 for Unconventional Reasoning","abstract":"While significant work has been done in the field of NLP on vertical thinking, which involves primarily logical thinking, little work has been done towards lateral thinking, which involves looking at problems from an unconventional perspective and defying existing conceptions and notions. Towards this direction, SemEval 2024 introduces the task of BRAINTEASER, which involves two types of questions -- Sentence Puzzles and Word Puzzles that defy conventional common-sense reasoning and constraints. In this paper, we tackle both types of questions using few-shot prompting on GPT-3.5 and gain insights regarding the difference in the nature of the two types. Our prompting strategy placed us 26th on the leaderboard for the Sentence Puzzle and 15th on the Word Puzzle task.","sentences":["While significant work has been done in the field of NLP on vertical thinking, which involves primarily logical thinking, little work has been done towards lateral thinking, which involves looking at problems from an unconventional perspective and defying existing conceptions and notions.","Towards this direction, SemEval 2024 introduces the task of BRAINTEASER, which involves two types of questions -- Sentence Puzzles and Word Puzzles that defy conventional common-sense reasoning and constraints.","In this paper, we tackle both types of questions using few-shot prompting on GPT-3.5 and gain insights regarding the difference in the nature of the two types.","Our prompting strategy placed us 26th on the leaderboard for the Sentence Puzzle and 15th on the Word Puzzle task."],"url":"http://arxiv.org/abs/2405.11559v1","category":"cs.CL"}
{"created":"2024-05-19 13:50:40","title":"An Invisible Backdoor Attack Based On Semantic Feature","abstract":"Backdoor attacks have severely threatened deep neural network (DNN) models in the past several years. These attacks can occur in almost every stage of the deep learning pipeline. Although the attacked model behaves normally on benign samples, it makes wrong predictions for samples containing triggers. However, most existing attacks use visible patterns (e.g., a patch or image transformations) as triggers, which are vulnerable to human inspection. In this paper, we propose a novel backdoor attack, making imperceptible changes. Concretely, our attack first utilizes the pre-trained victim model to extract low-level and high-level semantic features from clean images and generates trigger pattern associated with high-level features based on channel attention. Then, the encoder model generates poisoned images based on the trigger and extracted low-level semantic features without causing noticeable feature loss. We evaluate our attack on three prominent image classification DNN across three standard datasets. The results demonstrate that our attack achieves high attack success rates while maintaining robustness against backdoor defenses. Furthermore, we conduct extensive image similarity experiments to emphasize the stealthiness of our attack strategy.","sentences":["Backdoor attacks have severely threatened deep neural network (DNN) models in the past several years.","These attacks can occur in almost every stage of the deep learning pipeline.","Although the attacked model behaves normally on benign samples, it makes wrong predictions for samples containing triggers.","However, most existing attacks use visible patterns (e.g., a patch or image transformations) as triggers, which are vulnerable to human inspection.","In this paper, we propose a novel backdoor attack, making imperceptible changes.","Concretely, our attack first utilizes the pre-trained victim model to extract low-level and high-level semantic features from clean images and generates trigger pattern associated with high-level features based on channel attention.","Then, the encoder model generates poisoned images based on the trigger and extracted low-level semantic features without causing noticeable feature loss.","We evaluate our attack on three prominent image classification DNN across three standard datasets.","The results demonstrate that our attack achieves high attack success rates while maintaining robustness against backdoor defenses.","Furthermore, we conduct extensive image similarity experiments to emphasize the stealthiness of our attack strategy."],"url":"http://arxiv.org/abs/2405.11551v1","category":"cs.CV"}
{"created":"2024-05-19 13:39:56","title":"Experimental Study on Deuterium-Deuterium Thermonuclear Fusion with Interface Confinement","abstract":"Nuclear fusion is recognized as the energy of the future, and huge efforts and capitals have been put into the research of controlled nuclear fusion in the past decades. The most challenging thing for controlled nuclear fusion is to generate and keep a super high temperature. Here, a sonication system, combining with micro-scale fluid control techniques, was built to generate cavitation within a limited region. As bubbles being rapidly compressed, high temperature plasma generated interior leads to particle emissions, where a Cs2LiYCl6: Ce3+ (CLYC) scintillator was used to collect the emission events. The pulse shape discrimination methods applied on captured signals revealed that only gamma ray events were observed in sonication with normal water as excepted, while obvious separation of neutron and gamma ray events was surprisingly identified in sonication with deuterated water. This result suggested that neutrons were emitted from the sonicated deuterated water, i.e. deuterium-deuterium thermonuclear fusion was initiated. This study provides an alternative and feasible approach to achieve controllable nuclear fusion and makes great sense for future researches on the application of fusion energy.","sentences":["Nuclear fusion is recognized as the energy of the future, and huge efforts and capitals have been put into the research of controlled nuclear fusion in the past decades.","The most challenging thing for controlled nuclear fusion is to generate and keep a super high temperature.","Here, a sonication system, combining with micro-scale fluid control techniques, was built to generate cavitation within a limited region.","As bubbles being rapidly compressed, high temperature plasma generated interior leads to particle emissions, where a Cs2LiYCl6: Ce3+ (CLYC) scintillator was used to collect the emission events.","The pulse shape discrimination methods applied on captured signals revealed that only gamma ray events were observed in sonication with normal water as excepted, while obvious separation of neutron and gamma ray events was surprisingly identified in sonication with deuterated water.","This result suggested that neutrons were emitted from the sonicated deuterated water, i.e. deuterium-deuterium thermonuclear fusion was initiated.","This study provides an alternative and feasible approach to achieve controllable nuclear fusion and makes great sense for future researches on the application of fusion energy."],"url":"http://arxiv.org/abs/2405.11549v1","category":"nucl-ex"}
{"created":"2024-05-19 13:11:48","title":"R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments","abstract":"Recently, ray tracing has gained renewed interest with the advent of Reflective Intelligent Surfaces (RIS) technology, a key enabler of 6G wireless communications due to its capability of intelligent manipulation of electromagnetic waves. However, accurately modeling RIS-enabled wireless environments poses significant challenges due to the complex variations caused by various environmental factors and the mobility of RISs. In this paper, we propose a novel modeling approach using Neural Radiance Fields (NeRF) to characterize the dynamics of electromagnetic fields in such environments. Our method utilizes NeRF-based ray tracing to intuitively capture and visualize the complex dynamics of signal propagation, effectively modeling the complete signal pathways from the transmitter to the RIS, and from the RIS to the receiver. This two-stage process accurately characterizes multiple complex transmission paths, enhancing our understanding of signal behavior in real-world scenarios. Our approach predicts the signal field for any specified RIS placement and receiver location, facilitating efficient RIS deployment. Experimental evaluations using both simulated and real-world data validate the significant benefits of our methodology.","sentences":["Recently, ray tracing has gained renewed interest with the advent of Reflective Intelligent Surfaces (RIS) technology, a key enabler of 6G wireless communications due to its capability of intelligent manipulation of electromagnetic waves.","However, accurately modeling RIS-enabled wireless environments poses significant challenges due to the complex variations caused by various environmental factors and the mobility of RISs.","In this paper, we propose a novel modeling approach using Neural Radiance Fields (NeRF) to characterize the dynamics of electromagnetic fields in such environments.","Our method utilizes NeRF-based ray tracing to intuitively capture and visualize the complex dynamics of signal propagation, effectively modeling the complete signal pathways from the transmitter to the RIS, and from the RIS to the receiver.","This two-stage process accurately characterizes multiple complex transmission paths, enhancing our understanding of signal behavior in real-world scenarios.","Our approach predicts the signal field for any specified RIS placement and receiver location, facilitating efficient RIS deployment.","Experimental evaluations using both simulated and real-world data validate the significant benefits of our methodology."],"url":"http://arxiv.org/abs/2405.11541v1","category":"cs.IT"}
{"created":"2024-05-19 12:56:00","title":"VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications","abstract":"The advent of immersive Virtual Reality applications has transformed various domains, yet their integration with advanced artificial intelligence technologies like Visual Language Models remains underexplored. This study introduces a pioneering approach utilizing VLMs within VR environments to enhance user interaction and task efficiency. Leveraging the Unity engine and a custom-developed VLM, our system facilitates real-time, intuitive user interactions through natural language processing, without relying on visual text instructions. The incorporation of speech-to-text and text-to-speech technologies allows for seamless communication between the user and the VLM, enabling the system to guide users through complex tasks effectively. Preliminary experimental results indicate that utilizing VLMs not only reduces task completion times but also improves user comfort and task engagement compared to traditional VR interaction methods.","sentences":["The advent of immersive Virtual Reality applications has transformed various domains, yet their integration with advanced artificial intelligence technologies like Visual Language Models remains underexplored.","This study introduces a pioneering approach utilizing VLMs within VR environments to enhance user interaction and task efficiency.","Leveraging the Unity engine and a custom-developed VLM, our system facilitates real-time, intuitive user interactions through natural language processing, without relying on visual text instructions.","The incorporation of speech-to-text and text-to-speech technologies allows for seamless communication between the user and the VLM, enabling the system to guide users through complex tasks effectively.","Preliminary experimental results indicate that utilizing VLMs not only reduces task completion times but also improves user comfort and task engagement compared to traditional VR interaction methods."],"url":"http://arxiv.org/abs/2405.11537v1","category":"cs.RO"}
{"created":"2024-05-19 12:07:24","title":"Knowledge Graph Pruning for Recommendation","abstract":"Recent years have witnessed the prosperity of knowledge graph based recommendation system (KGRS), which enriches the representation of users, items, and entities by structural knowledge with striking improvement. Nevertheless, its unaffordable computational cost still limits researchers from exploring more sophisticated models. We observe that the bottleneck for training efficiency arises from the knowledge graph, which is plagued by the well-known issue of knowledge explosion. Recently, some works have attempted to slim the inflated KG via summarization techniques. However, these summarized nodes may ignore the collaborative signals and deviate from the facts that nodes in knowledge graph represent symbolic abstractions of entities from the real-world. To this end, in this paper, we propose a novel approach called KGTrimmer for knowledge graph pruning tailored for recommendation, to remove the unessential nodes while minimizing performance degradation. Specifically, we design an importance evaluator from a dual-view perspective. For the collective view, we embrace the idea of collective intelligence by extracting community consensus based on abundant collaborative signals, i.e. nodes are considered important if they attract attention of numerous users. For the holistic view, we learn a global mask to identify the valueless nodes from their inherent properties or overall popularity. Next, we build an end-to-end importance-aware graph neural network, which injects filtered knowledge to enhance the distillation of valuable user-item collaborative signals. Ultimately, we generate a pruned knowledge graph with lightweight, stable, and robust properties to facilitate the following-up recommendation task. Extensive experiments are conducted on three publicly available datasets to prove the effectiveness and generalization ability of KGTrimmer.","sentences":["Recent years have witnessed the prosperity of knowledge graph based recommendation system (KGRS), which enriches the representation of users, items, and entities by structural knowledge with striking improvement.","Nevertheless, its unaffordable computational cost still limits researchers from exploring more sophisticated models.","We observe that the bottleneck for training efficiency arises from the knowledge graph, which is plagued by the well-known issue of knowledge explosion.","Recently, some works have attempted to slim the inflated KG via summarization techniques.","However, these summarized nodes may ignore the collaborative signals and deviate from the facts that nodes in knowledge graph represent symbolic abstractions of entities from the real-world.","To this end, in this paper, we propose a novel approach called KGTrimmer for knowledge graph pruning tailored for recommendation, to remove the unessential nodes while minimizing performance degradation.","Specifically, we design an importance evaluator from a dual-view perspective.","For the collective view, we embrace the idea of collective intelligence by extracting community consensus based on abundant collaborative signals, i.e. nodes are considered important if they attract attention of numerous users.","For the holistic view, we learn a global mask to identify the valueless nodes from their inherent properties or overall popularity.","Next, we build an end-to-end importance-aware graph neural network, which injects filtered knowledge to enhance the distillation of valuable user-item collaborative signals.","Ultimately, we generate a pruned knowledge graph with lightweight, stable, and robust properties to facilitate the following-up recommendation task.","Extensive experiments are conducted on three publicly available datasets to prove the effectiveness and generalization ability of KGTrimmer."],"url":"http://arxiv.org/abs/2405.11531v1","category":"cs.IR"}
{"created":"2024-05-19 11:36:45","title":"Overcoming Data and Model Heterogeneities in Decentralized Federated Learning via Synthetic Anchors","abstract":"Conventional Federated Learning (FL) involves collaborative training of a global model while maintaining user data privacy. One of its branches, decentralized FL, is a serverless network that allows clients to own and optimize different local models separately, which results in saving management and communication resources. Despite the promising advancements in decentralized FL, it may reduce model generalizability due to lacking a global model. In this scenario, managing data and model heterogeneity among clients becomes a crucial problem, which poses a unique challenge that must be overcome: How can every client's local model learn generalizable representation in a decentralized manner? To address this challenge, we propose a novel Decentralized FL technique by introducing Synthetic Anchors, dubbed as DeSA. Based on the theory of domain adaptation and Knowledge Distillation (KD), we theoretically and empirically show that synthesizing global anchors based on raw data distribution facilitates mutual knowledge transfer. We further design two effective regularization terms for local training: 1) REG loss that regularizes the distribution of the client's latent embedding with the anchors and 2) KD loss that enables clients to learn from others. Through extensive experiments on diverse client data distributions, we showcase the effectiveness of DeSA in enhancing both inter- and intra-domain accuracy of each client.","sentences":["Conventional Federated Learning (FL) involves collaborative training of a global model while maintaining user data privacy.","One of its branches, decentralized FL, is a serverless network that allows clients to own and optimize different local models separately, which results in saving management and communication resources.","Despite the promising advancements in decentralized FL, it may reduce model generalizability due to lacking a global model.","In this scenario, managing data and model heterogeneity among clients becomes a crucial problem, which poses a unique challenge that must be overcome: How can every client's local model learn generalizable representation in a decentralized manner?","To address this challenge, we propose a novel Decentralized FL technique by introducing Synthetic Anchors, dubbed as DeSA.","Based on the theory of domain adaptation and Knowledge Distillation (KD), we theoretically and empirically show that synthesizing global anchors based on raw data distribution facilitates mutual knowledge transfer.","We further design two effective regularization terms for local training: 1) REG loss that regularizes the distribution of the client's latent embedding with the anchors and 2) KD loss that enables clients to learn from others.","Through extensive experiments on diverse client data distributions, we showcase the effectiveness of DeSA in enhancing both inter- and intra-domain accuracy of each client."],"url":"http://arxiv.org/abs/2405.11525v1","category":"cs.LG"}
{"created":"2024-05-19 10:12:20","title":"Machine Learning & Wi-Fi: Unveiling the Path Towards AI/ML-Native IEEE 802.11 Networks","abstract":"Artificial intelligence (AI) and machine learning (ML) are nowadays mature technologies considered essential for driving the evolution of future communications systems. Simultaneously, Wi-Fi technology has constantly evolved over the past three decades and incorporated new features generation after generation, thus gaining in complexity. As such, researchers have observed that AI/ML functionalities may be required to address the upcoming Wi-Fi challenges that will be otherwise difficult to solve with traditional approaches. This paper discusses the role of AI/ML in current and future Wi-Fi networks and depicts the ways forward. A roadmap towards AI/ML-native Wi-Fi, key challenges, standardization efforts, and major enablers are also discussed. An exemplary use case is provided to showcase the potential of AI/ML in Wi-Fi at different adoption stages.","sentences":["Artificial intelligence (AI) and machine learning (ML) are nowadays mature technologies considered essential for driving the evolution of future communications systems.","Simultaneously, Wi-Fi technology has constantly evolved over the past three decades and incorporated new features generation after generation, thus gaining in complexity.","As such, researchers have observed that AI/ML functionalities may be required to address the upcoming Wi-Fi challenges that will be otherwise difficult to solve with traditional approaches.","This paper discusses the role of AI/ML in current and future Wi-Fi networks and depicts the ways forward.","A roadmap towards AI/ML-native Wi-Fi, key challenges, standardization efforts, and major enablers are also discussed.","An exemplary use case is provided to showcase the potential of AI/ML in Wi-Fi at different adoption stages."],"url":"http://arxiv.org/abs/2405.11504v1","category":"cs.NI"}
{"created":"2024-05-19 09:48:59","title":"Towards in-situ Psychological Profiling of Cybercriminals Using Dynamically Generated Deception Environments","abstract":"Cybercrime is estimated to cost the global economy almost \\$10 trillion annually and with businesses and governments reporting an ever-increasing number of successful cyber-attacks there is a growing demand to rethink the strategy towards cyber security. The traditional, perimeter security approach to cyber defence has so far proved inadequate to combat the growing threat of cybercrime. Cyber deception offers a promising alternative by creating a dynamic defence environment. Deceptive techniques aim to mislead attackers, diverting them from critical assets whilst simultaneously gathering cyber threat intelligence on the threat actor. This article presents a proof-of-concept (POC) cyber deception system that has been developed to capture the profile of an attacker in-situ, during a simulated cyber-attack in real time. By dynamically and autonomously generating deception material based on the observed attacker behaviour and analysing how the attacker interacts with the deception material, the system outputs a prediction on the attacker's motive. The article also explores how this POC can be expanded to infer other features of the attacker's profile such as psychological characteristics. By dynamically and autonomously generating deception material based on observed attacker behaviour and analysing how the attacker interacts with the deception material, the system outputs a prediciton on the attacker's motive. The article also explores how this POC can be expanded to infer other features of the attacker's profile such as psychological characteristics.","sentences":["Cybercrime is estimated to cost the global economy almost \\$10 trillion annually and with businesses and governments reporting an ever-increasing number of successful cyber-attacks there is a growing demand to rethink the strategy towards cyber security.","The traditional, perimeter security approach to cyber defence has so far proved inadequate to combat the growing threat of cybercrime.","Cyber deception offers a promising alternative by creating a dynamic defence environment.","Deceptive techniques aim to mislead attackers, diverting them from critical assets whilst simultaneously gathering cyber threat intelligence on the threat actor.","This article presents a proof-of-concept (POC) cyber deception system that has been developed to capture the profile of an attacker in-situ, during a simulated cyber-attack in real time.","By dynamically and autonomously generating deception material based on the observed attacker behaviour and analysing how the attacker interacts with the deception material, the system outputs a prediction on the attacker's motive.","The article also explores how this POC can be expanded to infer other features of the attacker's profile such as psychological characteristics.","By dynamically and autonomously generating deception material based on observed attacker behaviour and analysing how the attacker interacts with the deception material, the system outputs a prediciton on the attacker's motive.","The article also explores how this POC can be expanded to infer other features of the attacker's profile such as psychological characteristics."],"url":"http://arxiv.org/abs/2405.11497v1","category":"cs.CR"}
{"created":"2024-05-19 08:06:14","title":"Unsupervised Image Prior via Prompt Learning and CLIP Semantic Guidance for Low-Light Image Enhancement","abstract":"Currently, low-light conditions present a significant challenge for machine cognition. In this paper, rather than optimizing models by assuming that human and machine cognition are correlated, we use zero-reference low-light enhancement to improve the performance of downstream task models. We propose to improve the zero-reference low-light enhancement method by leveraging the rich visual-linguistic CLIP prior without any need for paired or unpaired normal-light data, which is laborious and difficult to collect. We propose a simple but effective strategy to learn prompts that help guide the enhancement method and experimentally show that the prompts learned without any need for normal-light data improve image contrast, reduce over-enhancement, and reduce noise over-amplification. Next, we propose to reuse the CLIP model for semantic guidance via zero-shot open vocabulary classification to optimize low-light enhancement for task-based performance rather than human visual perception. We conduct extensive experimental results showing that the proposed method leads to consistent improvements across various datasets regarding task-based performance and compare our method against state-of-the-art methods, showing favorable results across various low-light datasets.","sentences":["Currently, low-light conditions present a significant challenge for machine cognition.","In this paper, rather than optimizing models by assuming that human and machine cognition are correlated, we use zero-reference low-light enhancement to improve the performance of downstream task models.","We propose to improve the zero-reference low-light enhancement method by leveraging the rich visual-linguistic CLIP prior without any need for paired or unpaired normal-light data, which is laborious and difficult to collect.","We propose a simple but effective strategy to learn prompts that help guide the enhancement method and experimentally show that the prompts learned without any need for normal-light data improve image contrast, reduce over-enhancement, and reduce noise over-amplification.","Next, we propose to reuse the CLIP model for semantic guidance via zero-shot open vocabulary classification to optimize low-light enhancement for task-based performance rather than human visual perception.","We conduct extensive experimental results showing that the proposed method leads to consistent improvements across various datasets regarding task-based performance and compare our method against state-of-the-art methods, showing favorable results across various low-light datasets."],"url":"http://arxiv.org/abs/2405.11478v1","category":"cs.CV"}
{"created":"2024-05-19 08:00:38","title":"NubbleDrop: A Simple Way to Improve Matching Strategy for Prompted One-Shot Segmentation","abstract":"Driven by large data trained segmentation models, such as SAM , research in one-shot segmentation has experienced significant advancements. Recent contributions like PerSAM and MATCHER , presented at ICLR 2024, utilize a similar approach by leveraging SAM with one or a few reference images to generate high quality segmentation masks for target images. Specifically, they utilize raw encoded features to compute cosine similarity between patches within reference and target images along the channel dimension, effectively generating prompt points or boxes for the target images a technique referred to as the matching strategy. However, relying solely on raw features might introduce biases and lack robustness for such a complex task. To address this concern, we delve into the issues of feature interaction and uneven distribution inherent in raw feature based matching. In this paper, we propose a simple and training-free method to enhance the validity and robustness of the matching strategy at no additional computational cost (NubbleDrop). The core concept involves randomly dropping feature channels (setting them to zero) during the matching process, thereby preventing models from being influenced by channels containing deceptive information. This technique mimics discarding pathological nubbles, and it can be seamlessly applied to other similarity computing scenarios. We conduct a comprehensive set of experiments, considering a wide range of factors, to demonstrate the effectiveness and validity of our proposed method. Our results showcase the significant improvements achieved through this simmple and straightforward approach.","sentences":["Driven by large data trained segmentation models, such as SAM , research in one-shot segmentation has experienced significant advancements.","Recent contributions like PerSAM and MATCHER , presented at ICLR 2024, utilize a similar approach by leveraging SAM with one or a few reference images to generate high quality segmentation masks for target images.","Specifically, they utilize raw encoded features to compute cosine similarity between patches within reference and target images along the channel dimension, effectively generating prompt points or boxes for the target images a technique referred to as the matching strategy.","However, relying solely on raw features might introduce biases and lack robustness for such a complex task.","To address this concern, we delve into the issues of feature interaction and uneven distribution inherent in raw feature based matching.","In this paper, we propose a simple and training-free method to enhance the validity and robustness of the matching strategy at no additional computational cost (NubbleDrop).","The core concept involves randomly dropping feature channels (setting them to zero) during the matching process, thereby preventing models from being influenced by channels containing deceptive information.","This technique mimics discarding pathological nubbles, and it can be seamlessly applied to other similarity computing scenarios.","We conduct a comprehensive set of experiments, considering a wide range of factors, to demonstrate the effectiveness and validity of our proposed method.","Our results showcase the significant improvements achieved through this simmple and straightforward approach."],"url":"http://arxiv.org/abs/2405.11476v1","category":"cs.CV"}
{"created":"2024-05-19 07:48:41","title":"FIFO-Diffusion: Generating Infinite Videos from Text without Training","abstract":"We propose a novel inference technique based on a pretrained diffusion model for text-conditional video generation. Our approach, called FIFO-Diffusion, is conceptually capable of generating infinitely long videos without training. This is achieved by iteratively performing diagonal denoising, which concurrently processes a series of consecutive frames with increasing noise levels in a queue; our method dequeues a fully denoised frame at the head while enqueuing a new random noise frame at the tail. However, diagonal denoising is a double-edged sword as the frames near the tail can take advantage of cleaner ones by forward reference but such a strategy induces the discrepancy between training and inference. Hence, we introduce latent partitioning to reduce the training-inference gap and lookahead denoising to leverage the benefit of forward referencing. We have demonstrated the promising results and effectiveness of the proposed methods on existing text-to-video generation baselines.","sentences":["We propose a novel inference technique based on a pretrained diffusion model for text-conditional video generation.","Our approach, called FIFO-Diffusion, is conceptually capable of generating infinitely long videos without training.","This is achieved by iteratively performing diagonal denoising, which concurrently processes a series of consecutive frames with increasing noise levels in a queue; our method dequeues a fully denoised frame at the head while enqueuing a new random noise frame at the tail.","However, diagonal denoising is a double-edged sword as the frames near the tail can take advantage of cleaner ones by forward reference but such a strategy induces the discrepancy between training and inference.","Hence, we introduce latent partitioning to reduce the training-inference gap and lookahead denoising to leverage the benefit of forward referencing.","We have demonstrated the promising results and effectiveness of the proposed methods on existing text-to-video generation baselines."],"url":"http://arxiv.org/abs/2405.11473v1","category":"cs.CV"}
{"created":"2024-05-19 07:39:22","title":"VCformer: Variable Correlation Transformer with Inherent Lagged Correlation for Multivariate Time Series Forecasting","abstract":"Multivariate time series (MTS) forecasting has been extensively applied across diverse domains, such as weather prediction and energy consumption. However, current studies still rely on the vanilla point-wise self-attention mechanism to capture cross-variable dependencies, which is inadequate in extracting the intricate cross-correlation implied between variables. To fill this gap, we propose Variable Correlation Transformer (VCformer), which utilizes Variable Correlation Attention (VCA) module to mine the correlations among variables. Specifically, based on the stochastic process theory, VCA calculates and integrates the cross-correlation scores corresponding to different lags between queries and keys, thereby enhancing its ability to uncover multivariate relationships. Additionally, inspired by Koopman dynamics theory, we also develop Koopman Temporal Detector (KTD) to better address the non-stationarity in time series. The two key components enable VCformer to extract both multivariate correlations and temporal dependencies. Our extensive experiments on eight real-world datasets demonstrate the effectiveness of VCformer, achieving top-tier performance compared to other state-of-the-art baseline models. Code is available at this repository: https://github.com/CSyyn/VCformer.","sentences":["Multivariate time series (MTS) forecasting has been extensively applied across diverse domains, such as weather prediction and energy consumption.","However, current studies still rely on the vanilla point-wise self-attention mechanism to capture cross-variable dependencies, which is inadequate in extracting the intricate cross-correlation implied between variables.","To fill this gap, we propose Variable Correlation Transformer (VCformer), which utilizes Variable Correlation Attention (VCA) module to mine the correlations among variables.","Specifically, based on the stochastic process theory, VCA calculates and integrates the cross-correlation scores corresponding to different lags between queries and keys, thereby enhancing its ability to uncover multivariate relationships.","Additionally, inspired by Koopman dynamics theory, we also develop Koopman Temporal Detector (KTD) to better address the non-stationarity in time series.","The two key components enable VCformer to extract both multivariate correlations and temporal dependencies.","Our extensive experiments on eight real-world datasets demonstrate the effectiveness of VCformer, achieving top-tier performance compared to other state-of-the-art baseline models.","Code is available at this repository: https://github.com/CSyyn/VCformer."],"url":"http://arxiv.org/abs/2405.11470v1","category":"cs.LG"}
{"created":"2024-05-19 06:43:12","title":"Efficient Prompt Tuning by Multi-Space Projection and Prompt Fusion","abstract":"Prompt tuning is a promising method to fine-tune a pre-trained language model without retraining its large-scale parameters. Instead, it attaches a soft prompt to the input text, whereby downstream tasks can be well adapted by merely learning the embeddings of prompt tokens. Nevertheless, existing methods still suffer from two challenges: (i) they are hard to balance accuracy and efficiency. A longer (shorter) soft prompt generally leads to a better (worse) accuracy but at the cost of more (less) training time. (ii) The performance may not be consistent when adapting to different downstream tasks. We attribute it to the same embedding space but responsible for different requirements of downstream tasks. To address these issues, we propose an Efficient Prompt Tuning method (EPT) by multi-space projection and prompt fusion. Specifically, it decomposes a given soft prompt into a shorter prompt and two low-rank matrices, whereby the number of parameters is greatly reduced as well as the training time. The accuracy is also enhanced by leveraging low-rank matrices and the short prompt as additional knowledge sources to enrich the semantics of the original short prompt. In addition, we project the soft prompt into multiple subspaces to improve the performance consistency, and then adaptively learn the combination weights of different spaces through a gating network. Experimental experiments on 13 natural language processing downstream tasks show that our method significantly and consistently outperforms 11 comparison methods with the relative percentage of improvements up to 28.8%, and training time decreased by 14%.","sentences":["Prompt tuning is a promising method to fine-tune a pre-trained language model without retraining its large-scale parameters.","Instead, it attaches a soft prompt to the input text, whereby downstream tasks can be well adapted by merely learning the embeddings of prompt tokens.","Nevertheless, existing methods still suffer from two challenges: (i) they are hard to balance accuracy and efficiency.","A longer (shorter) soft prompt generally leads to a better (worse) accuracy but at the cost of more (less) training time.","(ii) The performance may not be consistent when adapting to different downstream tasks.","We attribute it to the same embedding space but responsible for different requirements of downstream tasks.","To address these issues, we propose an Efficient Prompt Tuning method (EPT) by multi-space projection and prompt fusion.","Specifically, it decomposes a given soft prompt into a shorter prompt and two low-rank matrices, whereby the number of parameters is greatly reduced as well as the training time.","The accuracy is also enhanced by leveraging low-rank matrices and the short prompt as additional knowledge sources to enrich the semantics of the original short prompt.","In addition, we project the soft prompt into multiple subspaces to improve the performance consistency, and then adaptively learn the combination weights of different spaces through a gating network.","Experimental experiments on 13 natural language processing downstream tasks show that our method significantly and consistently outperforms 11 comparison methods with the relative percentage of improvements up to 28.8%, and training time decreased by 14%."],"url":"http://arxiv.org/abs/2405.11464v1","category":"cs.CL"}
{"created":"2024-05-19 06:30:22","title":"DocReLM: Mastering Document Retrieval with Language Model","abstract":"With over 200 million published academic documents and millions of new documents being written each year, academic researchers face the challenge of searching for information within this vast corpus. However, existing retrieval systems struggle to understand the semantics and domain knowledge present in academic papers. In this work, we demonstrate that by utilizing large language models, a document retrieval system can achieve advanced semantic understanding capabilities, significantly outperforming existing systems. Our approach involves training the retriever and reranker using domain-specific data generated by large language models. Additionally, we utilize large language models to identify candidates from the references of retrieved papers to further enhance the performance. We use a test set annotated by academic researchers in the fields of quantum physics and computer vision to evaluate our system's performance. The results show that DocReLM achieves a Top 10 accuracy of 44.12% in computer vision, compared to Google Scholar's 15.69%, and an increase to 36.21% in quantum physics, while that of Google Scholar is 12.96%.","sentences":["With over 200 million published academic documents and millions of new documents being written each year, academic researchers face the challenge of searching for information within this vast corpus.","However, existing retrieval systems struggle to understand the semantics and domain knowledge present in academic papers.","In this work, we demonstrate that by utilizing large language models, a document retrieval system can achieve advanced semantic understanding capabilities, significantly outperforming existing systems.","Our approach involves training the retriever and reranker using domain-specific data generated by large language models.","Additionally, we utilize large language models to identify candidates from the references of retrieved papers to further enhance the performance.","We use a test set annotated by academic researchers in the fields of quantum physics and computer vision to evaluate our system's performance.","The results show that DocReLM achieves a Top 10 accuracy of 44.12% in computer vision, compared to Google Scholar's 15.69%, and an increase to 36.21% in quantum physics, while that of Google Scholar is 12.96%."],"url":"http://arxiv.org/abs/2405.11461v1","category":"cs.IR"}
{"created":"2024-05-19 06:00:36","title":"Du-IN: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals","abstract":"Invasive brain-computer interfaces have garnered significant attention due to their high performance. The current intracranial stereoElectroEncephaloGraphy (sEEG) foundation models typically build univariate representations based on a single channel. Some of them further use Transformer to model the relationship among channels. However, due to the locality and specificity of brain computation, their performance on more difficult tasks, e.g., speech decoding, which demands intricate processing in specific brain regions, is yet to be fully investigated. We hypothesize that building multi-variate representations within certain brain regions can better capture the specific neural processing. To explore this hypothesis, we collect a well-annotated Chinese word-reading sEEG dataset, targeting language-related brain networks, over 12 subjects. Leveraging this benchmark dataset, we developed the Du-IN model that can extract contextual embeddings from specific brain regions through discrete codebook-guided mask modeling. Our model achieves SOTA performance on the downstream 61-word classification task, surpassing all baseline models. Model comparison and ablation analysis reveal that our design choices, including (i) multi-variate representation by fusing channels in vSMC and STG regions and (ii) self-supervision by discrete codebook-guided mask modeling, significantly contribute to these performances. Collectively, our approach, inspired by neuroscience findings, capitalizing on multi-variate neural representation from specific brain regions, is suitable for invasive brain modeling. It marks a promising neuro-inspired AI approach in BCI.","sentences":["Invasive brain-computer interfaces have garnered significant attention due to their high performance.","The current intracranial stereoElectroEncephaloGraphy (sEEG) foundation models typically build univariate representations based on a single channel.","Some of them further use Transformer to model the relationship among channels.","However, due to the locality and specificity of brain computation, their performance on more difficult tasks, e.g., speech decoding, which demands intricate processing in specific brain regions, is yet to be fully investigated.","We hypothesize that building multi-variate representations within certain brain regions can better capture the specific neural processing.","To explore this hypothesis, we collect a well-annotated Chinese word-reading sEEG dataset, targeting language-related brain networks, over 12 subjects.","Leveraging this benchmark dataset, we developed the Du-IN model that can extract contextual embeddings from specific brain regions through discrete codebook-guided mask modeling.","Our model achieves SOTA performance on the downstream 61-word classification task, surpassing all baseline models.","Model comparison and ablation analysis reveal that our design choices, including (i) multi-variate representation by fusing channels in vSMC and STG regions and (ii) self-supervision by discrete codebook-guided mask modeling, significantly contribute to these performances.","Collectively, our approach, inspired by neuroscience findings, capitalizing on multi-variate neural representation from specific brain regions, is suitable for invasive brain modeling.","It marks a promising neuro-inspired AI approach in BCI."],"url":"http://arxiv.org/abs/2405.11459v1","category":"eess.SP"}
{"created":"2024-05-19 06:00:18","title":"CPS-LLM: Large Language Model based Safe Usage Plan Generator for Human-in-the-Loop Human-in-the-Plant Cyber-Physical System","abstract":"We explore the usage of large language models (LLM) in human-in-the-loop human-in-the-plant cyber-physical systems (CPS) to translate a high-level prompt into a personalized plan of actions, and subsequently convert that plan into a grounded inference of sequential decision-making automated by a real-world CPS controller to achieve a control goal. We show that it is relatively straightforward to contextualize an LLM so it can generate domain-specific plans. However, these plans may be infeasible for the physical system to execute or the plan may be unsafe for human users. To address this, we propose CPS-LLM, an LLM retrained using an instruction tuning framework, which ensures that generated plans not only align with the physical system dynamics of the CPS but are also safe for human users. The CPS-LLM consists of two innovative components: a) a liquid time constant neural network-based physical dynamics coefficient estimator that can derive coefficients of dynamical models with some unmeasured state variables; b) the model coefficients are then used to train an LLM with prompts embodied with traces from the dynamical system and the corresponding model coefficients. We show that when the CPS-LLM is integrated with a contextualized chatbot such as BARD it can generate feasible and safe plans to manage external events such as meals for automated insulin delivery systems used by Type 1 Diabetes subjects.","sentences":["We explore the usage of large language models (LLM) in human-in-the-loop human-in-the-plant cyber-physical systems (CPS) to translate a high-level prompt into a personalized plan of actions, and subsequently convert that plan into a grounded inference of sequential decision-making automated by a real-world CPS controller to achieve a control goal.","We show that it is relatively straightforward to contextualize an LLM so it can generate domain-specific plans.","However, these plans may be infeasible for the physical system to execute or the plan may be unsafe for human users.","To address this, we propose CPS-LLM, an LLM retrained using an instruction tuning framework, which ensures that generated plans not only align with the physical system dynamics of the CPS but are also safe for human users.","The CPS-LLM consists of two innovative components: a) a liquid time constant neural network-based physical dynamics coefficient estimator that can derive coefficients of dynamical models with some unmeasured state variables; b) the model coefficients are then used to train an LLM with prompts embodied with traces from the dynamical system and the corresponding model coefficients.","We show that when the CPS-LLM is integrated with a contextualized chatbot such as BARD it can generate feasible and safe plans to manage external events such as meals for automated insulin delivery systems used by Type 1 Diabetes subjects."],"url":"http://arxiv.org/abs/2405.11458v1","category":"cs.AI"}
{"created":"2024-05-19 05:58:44","title":"Deep Dive into Model-free Reinforcement Learning for Biological and Robotic Systems: Theory and Practice","abstract":"Animals and robots exist in a physical world and must coordinate their bodies to achieve behavioral objectives. With recent developments in deep reinforcement learning, it is now possible for scientists and engineers to obtain sensorimotor strategies (policies) for specific tasks using physically simulated bodies and environments. However, the utility of these methods goes beyond the constraints of a specific task; they offer an exciting framework for understanding the organization of an animal sensorimotor system in connection to its morphology and physical interaction with the environment, as well as for deriving general design rules for sensing and actuation in robotic systems. Algorithms and code implementing both learning agents and environments are increasingly available, but the basic assumptions and choices that go into the formulation of an embodied feedback control problem using deep reinforcement learning may not be immediately apparent. Here, we present a concise exposition of the mathematical and algorithmic aspects of model-free reinforcement learning, specifically through the use of \\textit{actor-critic} methods, as a tool for investigating the feedback control underlying animal and robotic behavior.","sentences":["Animals and robots exist in a physical world and must coordinate their bodies to achieve behavioral objectives.","With recent developments in deep reinforcement learning, it is now possible for scientists and engineers to obtain sensorimotor strategies (policies) for specific tasks using physically simulated bodies and environments.","However, the utility of these methods goes beyond the constraints of a specific task; they offer an exciting framework for understanding the organization of an animal sensorimotor system in connection to its morphology and physical interaction with the environment, as well as for deriving general design rules for sensing and actuation in robotic systems.","Algorithms and code implementing both learning agents and environments are increasingly available, but the basic assumptions and choices that go into the formulation of an embodied feedback control problem using deep reinforcement learning may not be immediately apparent.","Here, we present a concise exposition of the mathematical and algorithmic aspects of model-free reinforcement learning, specifically through the use of \\textit{actor-critic} methods, as a tool for investigating the feedback control underlying animal and robotic behavior."],"url":"http://arxiv.org/abs/2405.11457v1","category":"cs.RO"}
{"created":"2024-05-19 05:07:09","title":"Error Analysis of Three-Layer Neural Network Trained with PGD for Deep Ritz Method","abstract":"Machine learning is a rapidly advancing field with diverse applications across various domains. One prominent area of research is the utilization of deep learning techniques for solving partial differential equations(PDEs). In this work, we specifically focus on employing a three-layer tanh neural network within the framework of the deep Ritz method(DRM) to solve second-order elliptic equations with three different types of boundary conditions. We perform projected gradient descent(PDG) to train the three-layer network and we establish its global convergence. To the best of our knowledge, we are the first to provide a comprehensive error analysis of using overparameterized networks to solve PDE problems, as our analysis simultaneously includes estimates for approximation error, generalization error, and optimization error. We present error bound in terms of the sample size $n$ and our work provides guidance on how to set the network depth, width, step size, and number of iterations for the projected gradient descent algorithm. Importantly, our assumptions in this work are classical and we do not require any additional assumptions on the solution of the equation. This ensures the broad applicability and generality of our results.","sentences":["Machine learning is a rapidly advancing field with diverse applications across various domains.","One prominent area of research is the utilization of deep learning techniques for solving partial differential equations(PDEs).","In this work, we specifically focus on employing a three-layer tanh neural network within the framework of the deep Ritz method(DRM) to solve second-order elliptic equations with three different types of boundary conditions.","We perform projected gradient descent(PDG) to train the three-layer network and we establish its global convergence.","To the best of our knowledge, we are the first to provide a comprehensive error analysis of using overparameterized networks to solve PDE problems, as our analysis simultaneously includes estimates for approximation error, generalization error, and optimization error.","We present error bound in terms of the sample size $n$ and our work provides guidance on how to set the network depth, width, step size, and number of iterations for the projected gradient descent algorithm.","Importantly, our assumptions in this work are classical and we do not require any additional assumptions on the solution of the equation.","This ensures the broad applicability and generality of our results."],"url":"http://arxiv.org/abs/2405.11451v1","category":"math.NA"}
{"created":"2024-05-19 03:55:02","title":"The First Swahili Language Scene Text Detection and Recognition Dataset","abstract":"Scene text recognition is essential in many applications, including automated translation, information retrieval, driving assistance, and enhancing accessibility for individuals with visual impairments. Much research has been done to improve the accuracy and performance of scene text detection and recognition models. However, most of this research has been conducted in the most common languages, English and Chinese. There is a significant gap in low-resource languages, especially the Swahili Language. Swahili is widely spoken in East African countries but is still an under-explored language in scene text recognition. No studies have been focused explicitly on Swahili natural scene text detection and recognition, and no dataset for Swahili language scene text detection and recognition is publicly available. We propose a comprehensive dataset of Swahili scene text images and evaluate the dataset on different scene text detection and recognition models. The dataset contains 976 images collected in different places and under various circumstances. Each image has its annotation at the word level. The proposed dataset can also serve as a benchmark dataset specific to the Swahili language for evaluating and comparing different approaches and fostering future research endeavors. The dataset is available on GitHub via this link: https://github.com/FadilaW/Swahili-STR-Dataset","sentences":["Scene text recognition is essential in many applications, including automated translation, information retrieval, driving assistance, and enhancing accessibility for individuals with visual impairments.","Much research has been done to improve the accuracy and performance of scene text detection and recognition models.","However, most of this research has been conducted in the most common languages, English and Chinese.","There is a significant gap in low-resource languages, especially the Swahili Language.","Swahili is widely spoken in East African countries but is still an under-explored language in scene text recognition.","No studies have been focused explicitly on Swahili natural scene text detection and recognition, and no dataset for Swahili language scene text detection and recognition is publicly available.","We propose a comprehensive dataset of Swahili scene text images and evaluate the dataset on different scene text detection and recognition models.","The dataset contains 976 images collected in different places and under various circumstances.","Each image has its annotation at the word level.","The proposed dataset can also serve as a benchmark dataset specific to the Swahili language for evaluating and comparing different approaches and fostering future research endeavors.","The dataset is available on GitHub via this link: https://github.com/FadilaW/Swahili-STR-Dataset"],"url":"http://arxiv.org/abs/2405.11437v1","category":"cs.CV"}
{"created":"2024-05-19 01:43:52","title":"Large Language Models are Biased Reinforcement Learners","abstract":"In-context learning enables large language models (LLMs) to perform a variety of tasks, including learning to make reward-maximizing choices in simple bandit tasks. Given their potential use as (autonomous) decision-making agents, it is important to understand how these models perform such reinforcement learning (RL) tasks and the extent to which they are susceptible to biases. Motivated by the fact that, in humans, it has been widely documented that the value of an outcome depends on how it compares to other local outcomes, the present study focuses on whether similar value encoding biases apply to how LLMs encode rewarding outcomes. Results from experiments with multiple bandit tasks and models show that LLMs exhibit behavioral signatures of a relative value bias. Adding explicit outcome comparisons to the prompt produces opposing effects on performance, enhancing maximization in trained choice sets but impairing generalization to new choice sets. Computational cognitive modeling reveals that LLM behavior is well-described by a simple RL algorithm that incorporates relative values at the outcome encoding stage. Lastly, we present preliminary evidence that the observed biases are not limited to fine-tuned LLMs, and that relative value processing is detectable in the final hidden layer activations of a raw, pretrained model. These findings have important implications for the use of LLMs in decision-making applications.","sentences":["In-context learning enables large language models (LLMs) to perform a variety of tasks, including learning to make reward-maximizing choices in simple bandit tasks.","Given their potential use as (autonomous) decision-making agents, it is important to understand how these models perform such reinforcement learning (RL) tasks and the extent to which they are susceptible to biases.","Motivated by the fact that, in humans, it has been widely documented that the value of an outcome depends on how it compares to other local outcomes, the present study focuses on whether similar value encoding biases apply to how LLMs encode rewarding outcomes.","Results from experiments with multiple bandit tasks and models show that LLMs exhibit behavioral signatures of a relative value bias.","Adding explicit outcome comparisons to the prompt produces opposing effects on performance, enhancing maximization in trained choice sets but impairing generalization to new choice sets.","Computational cognitive modeling reveals that LLM behavior is well-described by a simple RL algorithm that incorporates relative values at the outcome encoding stage.","Lastly, we present preliminary evidence that the observed biases are not limited to fine-tuned LLMs, and that relative value processing is detectable in the final hidden layer activations of a raw, pretrained model.","These findings have important implications for the use of LLMs in decision-making applications."],"url":"http://arxiv.org/abs/2405.11422v1","category":"cs.CL"}
{"created":"2024-05-19 01:41:04","title":"Assessing Group Fairness with Social Welfare Optimization","abstract":"Statistical parity metrics have been widely studied and endorsed in the AI community as a means of achieving fairness, but they suffer from at least two weaknesses. They disregard the actual welfare consequences of decisions and may therefore fail to achieve the kind of fairness that is desired for disadvantaged groups. In addition, they are often incompatible with each other, and there is no convincing justification for selecting one rather than another. This paper explores whether a broader conception of social justice, based on optimizing a social welfare function (SWF), can be useful for assessing various definitions of parity. We focus on the well-known alpha fairness SWF, which has been defended by axiomatic and bargaining arguments over a period of 70 years. We analyze the optimal solution and show that it can justify demographic parity or equalized odds under certain conditions, but frequently requires a departure from these types of parity. In addition, we find that predictive rate parity is of limited usefulness. These results suggest that optimization theory can shed light on the intensely discussed question of how to achieve group fairness in AI.","sentences":["Statistical parity metrics have been widely studied and endorsed in the AI community as a means of achieving fairness, but they suffer from at least two weaknesses.","They disregard the actual welfare consequences of decisions and may therefore fail to achieve the kind of fairness that is desired for disadvantaged groups.","In addition, they are often incompatible with each other, and there is no convincing justification for selecting one rather than another.","This paper explores whether a broader conception of social justice, based on optimizing a social welfare function (SWF), can be useful for assessing various definitions of parity.","We focus on the well-known alpha fairness SWF, which has been defended by axiomatic and bargaining arguments over a period of 70 years.","We analyze the optimal solution and show that it can justify demographic parity or equalized odds under certain conditions, but frequently requires a departure from these types of parity.","In addition, we find that predictive rate parity is of limited usefulness.","These results suggest that optimization theory can shed light on the intensely discussed question of how to achieve group fairness in AI."],"url":"http://arxiv.org/abs/2405.11421v1","category":"cs.AI"}
{"created":"2024-05-19 01:21:54","title":"Sketches-based join size estimation under local differential privacy","abstract":"Join size estimation on sensitive data poses a risk of privacy leakage. Local differential privacy (LDP) is a solution to preserve privacy while collecting sensitive data, but it introduces significant noise when dealing with sensitive join attributes that have large domains. Employing probabilistic structures such as sketches is a way to handle large domains, but it leads to hash-collision errors. To achieve accurate estimations, it is necessary to reduce both the noise error and hash-collision error. To tackle the noise error caused by protecting sensitive join values with large domains, we introduce a novel algorithm called LDPJoinSketch for sketch-based join size estimation under LDP. Additionally, to address the inherent hash-collision errors in sketches under LDP, we propose an enhanced method called LDPJoinSketch+. It utilizes a frequency-aware perturbation mechanism that effectively separates high-frequency and low-frequency items without compromising privacy. The proposed methods satisfy LDP, and the estimation error is bounded. Experimental results show that our method outperforms existing methods, effectively enhancing the accuracy of join size estimation under LDP.","sentences":["Join size estimation on sensitive data poses a risk of privacy leakage.","Local differential privacy (LDP) is a solution to preserve privacy while collecting sensitive data, but it introduces significant noise when dealing with sensitive join attributes that have large domains.","Employing probabilistic structures such as sketches is a way to handle large domains, but it leads to hash-collision errors.","To achieve accurate estimations, it is necessary to reduce both the noise error and hash-collision error.","To tackle the noise error caused by protecting sensitive join values with large domains, we introduce a novel algorithm called LDPJoinSketch for sketch-based join size estimation under LDP.","Additionally, to address the inherent hash-collision errors in sketches under LDP, we propose an enhanced method called LDPJoinSketch+.","It utilizes a frequency-aware perturbation mechanism that effectively separates high-frequency and low-frequency items without compromising privacy.","The proposed methods satisfy LDP, and the estimation error is bounded.","Experimental results show that our method outperforms existing methods, effectively enhancing the accuracy of join size estimation under LDP."],"url":"http://arxiv.org/abs/2405.11419v1","category":"cs.DB"}
{"created":"2024-05-18 23:22:41","title":"High-Resolution Agent-Based Modeling of Campus Population Behaviors for Pandemic Response Planning","abstract":"This paper reports a case study of an application of high-resolution agent-based modeling and simulation to pandemic response planning on a university campus. In the summer of 2020, we were tasked with a COVID-19 pandemic response project to create a detailed behavioral simulation model of the entire campus population at Binghamton University. We conceptualized this problem as an agent migration process on a multilayer transportation network, in which each layer represented a different transportation mode. As no direct data were available about people's behaviors on campus, we collected as much indirect information as possible to inform the agents' behavioral rules. Each agent was assumed to move along the shortest path between two locations within each transportation layer and switch layers at a parking lot or a bus stop, along with several other behavioral assumptions. Using this model, we conducted simulations of the whole campus population behaviors on a typical weekday, involving more than 25,000 agents. We measured the frequency of close social contacts at each spatial location and identified several busy locations and corridors on campus that needed substantial behavioral intervention. Moreover, systematic simulations with varying population density revealed that the effect of population density reduction was nonlinear, and that reducing the population density to 40-45% would be optimal and sufficient to suppress disease spreading on campus. These results were reported to the university administration and utilized in the pandemic response planning, which led to successful outcomes.","sentences":["This paper reports a case study of an application of high-resolution agent-based modeling and simulation to pandemic response planning on a university campus.","In the summer of 2020, we were tasked with a COVID-19 pandemic response project to create a detailed behavioral simulation model of the entire campus population at Binghamton University.","We conceptualized this problem as an agent migration process on a multilayer transportation network, in which each layer represented a different transportation mode.","As no direct data were available about people's behaviors on campus, we collected as much indirect information as possible to inform the agents' behavioral rules.","Each agent was assumed to move along the shortest path between two locations within each transportation layer and switch layers at a parking lot or a bus stop, along with several other behavioral assumptions.","Using this model, we conducted simulations of the whole campus population behaviors on a typical weekday, involving more than 25,000 agents.","We measured the frequency of close social contacts at each spatial location and identified several busy locations and corridors on campus that needed substantial behavioral intervention.","Moreover, systematic simulations with varying population density revealed that the effect of population density reduction was nonlinear, and that reducing the population density to 40-45% would be optimal and sufficient to suppress disease spreading on campus.","These results were reported to the university administration and utilized in the pandemic response planning, which led to successful outcomes."],"url":"http://arxiv.org/abs/2405.11414v1","category":"cs.CY"}
{"created":"2024-05-18 23:17:00","title":"Simulating Petri nets with Boolean Matrix Logic Programming","abstract":"Recent attention to relational knowledge bases has sparked a demand for understanding how relations change between entities. Petri nets can represent knowledge structure and dynamically simulate interactions between entities, and thus they are well suited for achieving this goal. However, logic programs struggle to deal with extensive Petri nets due to the limitations of high-level symbol manipulations. To address this challenge, we introduce a novel approach called Boolean Matrix Logic Programming (BMLP), utilising boolean matrices as an alternative computation mechanism for Prolog to evaluate logic programs. Within this framework, we propose two novel BMLP algorithms for simulating a class of Petri nets known as elementary nets. This is done by transforming elementary nets into logically equivalent datalog programs. We demonstrate empirically that BMLP algorithms can evaluate these programs 40 times faster than tabled B-Prolog, SWI-Prolog, XSB-Prolog and Clingo. Our work enables the efficient simulation of elementary nets using Prolog, expanding the scope of analysis, learning and verification of complex systems with logic programming techniques.","sentences":["Recent attention to relational knowledge bases has sparked a demand for understanding how relations change between entities.","Petri nets can represent knowledge structure and dynamically simulate interactions between entities, and thus they are well suited for achieving this goal.","However, logic programs struggle to deal with extensive Petri nets due to the limitations of high-level symbol manipulations.","To address this challenge, we introduce a novel approach called Boolean Matrix Logic Programming (BMLP), utilising boolean matrices as an alternative computation mechanism for Prolog to evaluate logic programs.","Within this framework, we propose two novel BMLP algorithms for simulating a class of Petri nets known as elementary nets.","This is done by transforming elementary nets into logically equivalent datalog programs.","We demonstrate empirically that BMLP algorithms can evaluate these programs 40 times faster than tabled B-Prolog, SWI-Prolog, XSB-Prolog and Clingo.","Our work enables the efficient simulation of elementary nets using Prolog, expanding the scope of analysis, learning and verification of complex systems with logic programming techniques."],"url":"http://arxiv.org/abs/2405.11412v1","category":"cs.AI"}
{"created":"2024-05-18 22:10:15","title":"MapCoder: Multi-Agent Code Generation for Competitive Problem Solving","abstract":"Code synthesis, which requires a deep understanding of complex natural language problem descriptions, generation of code instructions for complex algorithms and data structures, and the successful execution of comprehensive unit tests, presents a significant challenge. While large language models (LLMs) demonstrate impressive proficiency in natural language processing, their performance in code generation tasks remains limited. In this paper, we introduce a new approach to code generation tasks leveraging multi-agent prompting that uniquely replicates the full cycle of program synthesis as observed in human developers. Our framework, MapCoder, consists of four LLM agents specifically designed to emulate the stages of this cycle: recalling relevant examples, planning, code generation, and debugging. After conducting thorough experiments, with multiple LLM ablations and analyses across eight challenging competitive problem-solving and program synthesis benchmarks, MapCoder showcases remarkable code generation capabilities, achieving new state-of-the-art results (pass@1) on HumanEval (93.9%), MBPP (83.1%), APPS (22.0%), CodeContests (28.5%), and xCodeEval (45.3%). Moreover, our method consistently delivers superior performance across various programming languages and varying problem difficulties. We open-source our framework at https://github.com/Md-Ashraful-Pramanik/MapCoder.","sentences":["Code synthesis, which requires a deep understanding of complex natural language problem descriptions, generation of code instructions for complex algorithms and data structures, and the successful execution of comprehensive unit tests, presents a significant challenge.","While large language models (LLMs) demonstrate impressive proficiency in natural language processing, their performance in code generation tasks remains limited.","In this paper, we introduce a new approach to code generation tasks leveraging multi-agent prompting that uniquely replicates the full cycle of program synthesis as observed in human developers.","Our framework, MapCoder, consists of four LLM agents specifically designed to emulate the stages of this cycle: recalling relevant examples, planning, code generation, and debugging.","After conducting thorough experiments, with multiple LLM ablations and analyses across eight challenging competitive problem-solving and program synthesis benchmarks, MapCoder showcases remarkable code generation capabilities, achieving new state-of-the-art results (pass@1) on HumanEval (93.9%), MBPP (83.1%), APPS (22.0%), CodeContests (28.5%), and xCodeEval (45.3%).","Moreover, our method consistently delivers superior performance across various programming languages and varying problem difficulties.","We open-source our framework at https://github.com/Md-Ashraful-Pramanik/MapCoder."],"url":"http://arxiv.org/abs/2405.11403v1","category":"cs.CL"}
{"created":"2024-05-18 22:07:38","title":"A Model for Optimal Resilient Planning Subject to Fallible Actuators","abstract":"Robots incurring component failures ought to adapt their behavior to best realize still-attainable goals under reduced capacity. We formulate the problem of planning with actuators known a priori to be susceptible to failure within the Markov Decision Processes (MDP) framework. The model captures utilization-driven malfunction and state-action dependent likelihoods of actuator failure in order to enable reasoning about potential impairment and the long-term implications of impoverished future control. This leads to behavior differing qualitatively from plans which ignore failure. As actuators malfunction, there are combinatorially many configurations which can arise. We identify opportunities to save computation through re-use, exploiting the observation that differing configurations yield closely related problems. Our results show how strategic solutions are obtained so robots can respond when failures do occur -- for instance, in prudently scheduling utilization in order to keep critical actuators in reserve.","sentences":["Robots incurring component failures ought to adapt their behavior to best realize still-attainable goals under reduced capacity.","We formulate the problem of planning with actuators known a priori to be susceptible to failure within the Markov Decision Processes (MDP) framework.","The model captures utilization-driven malfunction and state-action dependent likelihoods of actuator failure in order to enable reasoning about potential impairment and the long-term implications of impoverished future control.","This leads to behavior differing qualitatively from plans which ignore failure.","As actuators malfunction, there are combinatorially many configurations which can arise.","We identify opportunities to save computation through re-use, exploiting the observation that differing configurations yield closely related problems.","Our results show how strategic solutions are obtained so robots can respond when failures do occur -- for instance, in prudently scheduling utilization in order to keep critical actuators in reserve."],"url":"http://arxiv.org/abs/2405.11402v1","category":"cs.RO"}
{"created":"2024-05-18 22:01:55","title":"PDE Control Gym: A Benchmark for Data-Driven Boundary Control of Partial Differential Equations","abstract":"Over the last decade, data-driven methods have surged in popularity, emerging as valuable tools for control theory. As such, neural network approximations of control feedback laws, system dynamics, and even Lyapunov functions have attracted growing attention. With the ascent of learning based control, the need for accurate, fast, and easy-to-use benchmarks has increased. In this work, we present the first learning-based environment for boundary control of PDEs. In our benchmark, we introduce three foundational PDE problems - a 1D transport PDE, a 1D reaction-diffusion PDE, and a 2D Navier-Stokes PDE - whose solvers are bundled in an user-friendly reinforcement learning gym. With this gym, we then present the first set of model-free, reinforcement learning algorithms for solving this series of benchmark problems, achieving stability, although at a higher cost compared to model-based PDE backstepping. With the set of benchmark environments and detailed examples, this work significantly lowers the barrier to entry for learning-based PDE control - a topic largely unexplored by the data-driven control community. The entire benchmark is available on Github along with detailed documentation and the presented reinforcement learning models are open sourced.","sentences":["Over the last decade, data-driven methods have surged in popularity, emerging as valuable tools for control theory.","As such, neural network approximations of control feedback laws, system dynamics, and even Lyapunov functions have attracted growing attention.","With the ascent of learning based control, the need for accurate, fast, and easy-to-use benchmarks has increased.","In this work, we present the first learning-based environment for boundary control of PDEs.","In our benchmark, we introduce three foundational PDE problems - a 1D transport PDE, a 1D reaction-diffusion PDE, and a 2D Navier-Stokes PDE - whose solvers are bundled in an user-friendly reinforcement learning gym.","With this gym, we then present the first set of model-free, reinforcement learning algorithms for solving this series of benchmark problems, achieving stability, although at a higher cost compared to model-based PDE backstepping.","With the set of benchmark environments and detailed examples, this work significantly lowers the barrier to entry for learning-based PDE control - a topic largely unexplored by the data-driven control community.","The entire benchmark is available on Github along with detailed documentation and the presented reinforcement learning models are open sourced."],"url":"http://arxiv.org/abs/2405.11401v1","category":"eess.SY"}
{"created":"2024-05-18 21:32:29","title":"Preparing for Black Swans: The Antifragility Imperative for Machine Learning","abstract":"Operating safely and reliably despite continual distribution shifts is vital for high-stakes machine learning applications. This paper builds upon the transformative concept of ``antifragility'' introduced by (Taleb, 2014) as a constructive design paradigm to not just withstand but benefit from volatility. We formally define antifragility in the context of online decision making as dynamic regret's strictly concave response to environmental variability, revealing limitations of current approaches focused on resisting rather than benefiting from nonstationarity. Our contribution lies in proposing potential computational pathways for engineering antifragility, grounding the concept in online learning theory and drawing connections to recent advancements in areas such as meta-learning, safe exploration, continual learning, multi-objective/quality-diversity optimization, and foundation models. By identifying promising mechanisms and future research directions, we aim to put antifragility on a rigorous theoretical foundation in machine learning. We further emphasize the need for clear guidelines, risk assessment frameworks, and interdisciplinary collaboration to ensure responsible application.","sentences":["Operating safely and reliably despite continual distribution shifts is vital for high-stakes machine learning applications.","This paper builds upon the transformative concept of ``antifragility'' introduced by (Taleb, 2014) as a constructive design paradigm to not just withstand but benefit from volatility.","We formally define antifragility in the context of online decision making as dynamic regret's strictly concave response to environmental variability, revealing limitations of current approaches focused on resisting rather than benefiting from nonstationarity.","Our contribution lies in proposing potential computational pathways for engineering antifragility, grounding the concept in online learning theory and drawing connections to recent advancements in areas such as meta-learning, safe exploration, continual learning, multi-objective/quality-diversity optimization, and foundation models.","By identifying promising mechanisms and future research directions, we aim to put antifragility on a rigorous theoretical foundation in machine learning.","We further emphasize the need for clear guidelines, risk assessment frameworks, and interdisciplinary collaboration to ensure responsible application."],"url":"http://arxiv.org/abs/2405.11397v1","category":"cs.LG"}
{"created":"2024-05-18 20:43:59","title":"Search for Two-Body $B$ Meson Decays to $\u039b^{0}$ and $\u03a9^{(*)0}_{c}$","abstract":"We report the results of the first search for Standard Model and baryon-number-violating two-body decays of the neutral $B$ mesons to $\\Lambda^{0}$ and $\\Omega^{(*)0}_c$ using 711~${\\rm fb^{-1}}$ of data collected at the $\\Upsilon(4S)$ resonance with the Belle detector at the KEKB asymmetric-energy $e^+ e^-$ collider. We observe no evidence of signal from any such decays and set 95\\% confidence-level upper limits on the products of $B^0$ and $\\bar{B}^0$ branching fractions for these two-body decays with $\\mathcal{B}(\\Omega_{c}^{0} \\to \\pi^+ \\Omega^-)$ in the range between 9.5~$\\times 10^{-8}$ and 31.2~$\\times 10^{-8}$.","sentences":["We report the results of the first search for Standard Model and baryon-number-violating two-body decays of the neutral $B$ mesons to $\\Lambda^{0}$ and $\\Omega^{(*)0}_c$ using 711~${\\rm fb^{-1}}$ of data collected at the $\\Upsilon(4S)$ resonance with the Belle detector at the KEKB asymmetric-energy $e^+ e^-$ collider.","We observe no evidence of signal from any such decays and set 95\\% confidence-level upper limits on the products of $B^0$ and $\\bar{B}^0$ branching fractions for these two-body decays with $\\mathcal{B}(\\Omega_{c}^{0} \\to \\pi^+ \\Omega^-)$ in the range between 9.5~$\\times 10^{-8}$ and 31.2~$\\times 10^{-8}$."],"url":"http://arxiv.org/abs/2405.11390v1","category":"hep-ex"}
{"created":"2024-05-18 20:09:57","title":"Toward an auditory Virtual Observatory (preprint)","abstract":"The exploration of the universe is experiencing a huge development thanks to the success and possibilities of today's major space telescope missions which can generate measurements and images with a resolution 100 times higher than their precedents. This big ecosystem of observations, aimed at expanding the limits of known science, can be analyzed using personal computers thanks to the implementation of interoperable Virtual Observatory (VO) technologies and massive portals of stellar catalogs and databases. In this context of global analysis of astronomical big data, sonification has the potential of adding a complementary dimension to visualization, enhancing the accessibility of the archives, and offering an alternative strategy to be used when overlapping issues and masking effects are found in purely graphical representations. This article presents a collection of sonification and musification prototypes that explore the case studies of the MILES and STELIB stellar libraries from the Spanish Virtual Observatory (SVO), and the Kepler Objects of Interest light curve database from the Space Telescope Science Institute archive (STScI). The work makes use of automation, machine learning, and deep learning algorithms to offer a palette of resources that could be used in future developments oriented towards an auditory virtual observatory proposal. It includes a user study that provides qualitative and quantitative feedback from specialized and non-specialized users in the fields of Music and Astronomy.","sentences":["The exploration of the universe is experiencing a huge development thanks to the success and possibilities of today's major space telescope missions which can generate measurements and images with a resolution 100 times higher than their precedents.","This big ecosystem of observations, aimed at expanding the limits of known science, can be analyzed using personal computers thanks to the implementation of interoperable Virtual Observatory (VO) technologies and massive portals of stellar catalogs and databases.","In this context of global analysis of astronomical big data, sonification has the potential of adding a complementary dimension to visualization, enhancing the accessibility of the archives, and offering an alternative strategy to be used when overlapping issues and masking effects are found in purely graphical representations.","This article presents a collection of sonification and musification prototypes that explore the case studies of the MILES and STELIB stellar libraries from the Spanish Virtual Observatory (SVO), and the Kepler Objects of Interest light curve database from the Space Telescope Science Institute archive (STScI).","The work makes use of automation, machine learning, and deep learning algorithms to offer a palette of resources that could be used in future developments oriented towards an auditory virtual observatory proposal.","It includes a user study that provides qualitative and quantitative feedback from specialized and non-specialized users in the fields of Music and Astronomy."],"url":"http://arxiv.org/abs/2405.11382v1","category":"astro-ph.IM"}
{"created":"2024-05-18 19:58:44","title":"Meta-Control: Automatic Model-based Control Synthesis for Heterogeneous Robot Skills","abstract":"The requirements for real-world manipulation tasks are diverse and often conflicting; some tasks necessitate force constraints or collision avoidance, while others demand high-frequency feedback. Satisfying these varied requirements with a fixed state-action representation and control strategy is challenging, impeding the development of a universal robotic foundation model. In this work, we propose Meta-Control, the first LLM-enabled automatic control synthesis approach that creates customized state representations and control strategies tailored to specific tasks. Meta-Control leverages a generic hierarchical control framework to address a wide range of heterogeneous tasks. Our core insight is the decomposition of the state space into an abstract task space and a concrete tracking space. By harnessing LLM's extensive common sense and control knowledge, we enable the LLM to design these spaces, including states, dynamic models, and controllers, using pre-defined but abstract templates. Meta-Control stands out for its fully model-based nature, allowing for rigorous analysis, efficient parameter tuning, and reliable execution. It not only utilizes decades of control expertise encapsulated within LLMs to facilitate heterogeneous control but also ensures formal guarantees such as safety and stability. Our method is validated both in real-world scenarios and simulations across diverse tasks with conflicting requirements, such as collision avoidance versus convergence and compliance versus high precision. Videos and additional results are at meta-control-paper.github.io","sentences":["The requirements for real-world manipulation tasks are diverse and often conflicting; some tasks necessitate force constraints or collision avoidance, while others demand high-frequency feedback.","Satisfying these varied requirements with a fixed state-action representation and control strategy is challenging, impeding the development of a universal robotic foundation model.","In this work, we propose Meta-Control, the first LLM-enabled automatic control synthesis approach that creates customized state representations and control strategies tailored to specific tasks.","Meta-Control leverages a generic hierarchical control framework to address a wide range of heterogeneous tasks.","Our core insight is the decomposition of the state space into an abstract task space and a concrete tracking space.","By harnessing LLM's extensive common sense and control knowledge, we enable the LLM to design these spaces, including states, dynamic models, and controllers, using pre-defined but abstract templates.","Meta-Control stands out for its fully model-based nature, allowing for rigorous analysis, efficient parameter tuning, and reliable execution.","It not only utilizes decades of control expertise encapsulated within LLMs to facilitate heterogeneous control but also ensures formal guarantees such as safety and stability.","Our method is validated both in real-world scenarios and simulations across diverse tasks with conflicting requirements, such as collision avoidance versus convergence and compliance versus high precision.","Videos and additional results are at meta-control-paper.github.io"],"url":"http://arxiv.org/abs/2405.11380v1","category":"cs.RO"}
{"created":"2024-05-18 18:10:38","title":"A Bayesian Nonparametric Approach for Clustering Functional Trajectories over Time","abstract":"Functional concurrent, or varying-coefficient, regression models are commonly used in biomedical and clinical settings to investigate how the relation between an outcome and observed covariate varies as a function of another covariate. In this work, we propose a Bayesian nonparametric approach to investigate how clusters of these functional relations evolve over time. Our model clusters individual functional trajectories within and across time periods while flexibly accommodating the evolution of the partitions across time periods with covariates. Motivated by mobile health data collected in a novel, smartphone-based smoking cessation intervention study, we demonstrate how our proposed method can simultaneously cluster functional trajectories, accommodate temporal dependence, and provide insights into the transitions between functional clusters over time.","sentences":["Functional concurrent, or varying-coefficient, regression models are commonly used in biomedical and clinical settings to investigate how the relation between an outcome and observed covariate varies as a function of another covariate.","In this work, we propose a Bayesian nonparametric approach to investigate how clusters of these functional relations evolve over time.","Our model clusters individual functional trajectories within and across time periods while flexibly accommodating the evolution of the partitions across time periods with covariates.","Motivated by mobile health data collected in a novel, smartphone-based smoking cessation intervention study, we demonstrate how our proposed method can simultaneously cluster functional trajectories, accommodate temporal dependence, and provide insights into the transitions between functional clusters over time."],"url":"http://arxiv.org/abs/2405.11358v1","category":"stat.ME"}
{"created":"2024-05-18 17:31:26","title":"Cooperative Multi-agent Approach for Automated Computer Game Testing","abstract":"Automated testing of computer games is a challenging problem, especially when lengthy scenarios have to be tested. Automating such a scenario boils down to finding the right sequence of interactions given an abstract description of the scenario. Recent works have shown that an agent-based approach works well for the purpose, e.g. due to agents' reactivity, hence enabling a test agent to immediately react to game events and changing state. Many games nowadays are multi-player. This opens up an interesting possibility to deploy multiple cooperative test agents to test such a game, for example to speed up the execution of multiple testing tasks. This paper offers a cooperative multi-agent testing approach and a study of its performance based on a case study on a 3D game called Lab Recruits.","sentences":["Automated testing of computer games is a challenging problem, especially when lengthy scenarios have to be tested.","Automating such a scenario boils down to finding the right sequence of interactions given an abstract description of the scenario.","Recent works have shown that an agent-based approach works well for the purpose, e.g. due to agents' reactivity, hence enabling a test agent to immediately react to game events and changing state.","Many games nowadays are multi-player.","This opens up an interesting possibility to deploy multiple cooperative test agents to test such a game, for example to speed up the execution of multiple testing tasks.","This paper offers a cooperative multi-agent testing approach and a study of its performance based on a case study on a 3D game called Lab Recruits."],"url":"http://arxiv.org/abs/2405.11347v1","category":"cs.SE"}
{"created":"2024-05-18 17:30:30","title":"Decision support system for Forest fire management using Ontology with Big Data and LLMs","abstract":"Forests are crucial for ecological balance, but wildfires, a major cause of forest loss, pose significant risks. Fire weather indices, which assess wildfire risk and predict resource demands, are vital. With the rise of sensor networks in fields like healthcare and environmental monitoring, semantic sensor networks are increasingly used to gather climatic data such as wind speed, temperature, and humidity. However, processing these data streams to determine fire weather indices presents challenges, underscoring the growing importance of effective forest fire detection. This paper discusses using Apache Spark for early forest fire detection, enhancing fire risk prediction with meteorological and geographical data. Building on our previous development of Semantic Sensor Network (SSN) ontologies and Semantic Web Rules Language (SWRL) for managing forest fires in Monesterial Natural Park, we expanded SWRL to improve a Decision Support System (DSS) using a Large Language Models (LLMs) and Spark framework. We implemented real-time alerts with Spark streaming, tailored to various fire scenarios, and validated our approach using ontology metrics, query-based evaluations, LLMs score precision, F1 score, and recall measures.","sentences":["Forests are crucial for ecological balance, but wildfires, a major cause of forest loss, pose significant risks.","Fire weather indices, which assess wildfire risk and predict resource demands, are vital.","With the rise of sensor networks in fields like healthcare and environmental monitoring, semantic sensor networks are increasingly used to gather climatic data such as wind speed, temperature, and humidity.","However, processing these data streams to determine fire weather indices presents challenges, underscoring the growing importance of effective forest fire detection.","This paper discusses using Apache Spark for early forest fire detection, enhancing fire risk prediction with meteorological and geographical data.","Building on our previous development of Semantic Sensor Network (SSN) ontologies and Semantic Web Rules Language (SWRL) for managing forest fires in Monesterial Natural Park, we expanded SWRL to improve a Decision Support System (DSS) using a Large Language Models (LLMs) and Spark framework.","We implemented real-time alerts with Spark streaming, tailored to various fire scenarios, and validated our approach using ontology metrics, query-based evaluations, LLMs score precision, F1 score, and recall measures."],"url":"http://arxiv.org/abs/2405.11346v1","category":"cs.AI"}
{"created":"2024-05-18 17:28:35","title":"City-Scale Multi-Camera Vehicle Tracking System with Improved Self-Supervised Camera Link Model","abstract":"Multi-Target Multi-Camera Tracking (MTMCT) has broad applications and forms the basis for numerous future city-wide systems (e.g. traffic management, crash detection, etc.). However, the challenge of matching vehicle trajectories across different cameras based solely on feature extraction poses significant difficulties. This article introduces an innovative multi-camera vehicle tracking system that utilizes a self-supervised camera link model. In contrast to related works that rely on manual spatial-temporal annotations, our model automatically extracts crucial multi-camera relationships for vehicle matching. The camera link is established through a pre-matching process that evaluates feature similarities, pair numbers, and time variance for high-quality tracks. This process calculates the probability of spatial linkage for all camera combinations, selecting the highest scoring pairs to create camera links. Our approach significantly improves deployment times by eliminating the need for human annotation, offering substantial improvements in efficiency and cost-effectiveness when it comes to real-world application. This pairing process supports cross camera matching by setting spatial-temporal constraints, reducing the searching space for potential vehicle matches. According to our experimental results, the proposed method achieves a new state-of-the-art among automatic camera-link based methods in CityFlow V2 benchmarks with 61.07% IDF1 Score.","sentences":["Multi-Target Multi-Camera Tracking (MTMCT) has broad applications and forms the basis for numerous future city-wide systems (e.g. traffic management, crash detection, etc.).","However, the challenge of matching vehicle trajectories across different cameras based solely on feature extraction poses significant difficulties.","This article introduces an innovative multi-camera vehicle tracking system that utilizes a self-supervised camera link model.","In contrast to related works that rely on manual spatial-temporal annotations, our model automatically extracts crucial multi-camera relationships for vehicle matching.","The camera link is established through a pre-matching process that evaluates feature similarities, pair numbers, and time variance for high-quality tracks.","This process calculates the probability of spatial linkage for all camera combinations, selecting the highest scoring pairs to create camera links.","Our approach significantly improves deployment times by eliminating the need for human annotation, offering substantial improvements in efficiency and cost-effectiveness when it comes to real-world application.","This pairing process supports cross camera matching by setting spatial-temporal constraints, reducing the searching space for potential vehicle matches.","According to our experimental results, the proposed method achieves a new state-of-the-art among automatic camera-link based methods in CityFlow V2 benchmarks with 61.07% IDF1 Score."],"url":"http://arxiv.org/abs/2405.11345v1","category":"cs.CV"}
{"created":"2024-05-18 17:28:29","title":"Improved Content Understanding With Effective Use of Multi-task Contrastive Learning","abstract":"In enhancing LinkedIn core content recommendation models, a significant challenge lies in improving their semantic understanding capabilities. This paper addresses the problem by leveraging multi-task learning, a method that has shown promise in various domains. We fine-tune a pre-trained, transformer-based LLM using multi-task contrastive learning with data from a diverse set of semantic labeling tasks. We observe positive transfer, leading to superior performance across all tasks when compared to training independently on each. Our model outperforms the baseline on zero shot learning and offers improved multilingual support, highlighting its potential for broader application. The specialized content embeddings produced by our model outperform generalized embeddings offered by OpenAI on Linkedin dataset and tasks. This work provides a robust foundation for vertical teams across LinkedIn to customize and fine-tune the LLM to their specific applications. Our work offers insights and best practices for the field to build on.","sentences":["In enhancing LinkedIn core content recommendation models, a significant challenge lies in improving their semantic understanding capabilities.","This paper addresses the problem by leveraging multi-task learning, a method that has shown promise in various domains.","We fine-tune a pre-trained, transformer-based LLM using multi-task contrastive learning with data from a diverse set of semantic labeling tasks.","We observe positive transfer, leading to superior performance across all tasks when compared to training independently on each.","Our model outperforms the baseline on zero shot learning and offers improved multilingual support, highlighting its potential for broader application.","The specialized content embeddings produced by our model outperform generalized embeddings offered by OpenAI on Linkedin dataset and tasks.","This work provides a robust foundation for vertical teams across LinkedIn to customize and fine-tune the LLM to their specific applications.","Our work offers insights and best practices for the field to build on."],"url":"http://arxiv.org/abs/2405.11344v1","category":"cs.LG"}
{"created":"2024-05-18 17:27:34","title":"Sub-relativistic Outflow and Hours-Timescale Large-amplitude X-ray Dips during Super-Eddington Accretion onto a Low-mass Massive Black Hole in the Tidal Disruption Event AT2022lri","abstract":"We present the tidal disruption event (TDE) AT2022lri, hosted in a nearby ($\\approx\\!144$ Mpc) quiescent galaxy with a low-mass massive black hole ($10^4\\,M_\\odot < M_{\\rm BH} < 10^6\\,M_\\odot$). AT2022lri belongs to the TDE-H+He subtype. More than 1 Ms of X-ray data were collected with NICER, Swift, and XMM-Newton from 187 d to 672 d after peak. The X-ray luminosity gradually declined from $1.5\\times 10^{44}\\,{\\rm erg\\,s^{-1}}$ to $1.5\\times 10^{43}\\,{\\rm erg\\,s^{-1}}$ and remains much above the UV and optical luminosity, consistent with a super-Eddington accretion flow viewed face-on. Sporadic strong X-ray dips atop a long-term decline are observed, with variability timescale of $\\approx\\!0.5$ hr--1 d and amplitude of $\\approx\\!2$--8. When fitted with simple continuum models, the X-ray spectrum is dominated by a thermal disk component with inner temperature going from $\\sim\\! 146$ eV to $\\sim\\! 86$ eV. However, there are residual features that peak around 1 keV, which, in some cases, cannot be reproduced by a single broad emission line. We analyzed a subset of time-resolved spectra with two physically motivated models describing either a scenario where ionized absorbers contribute extra absorption and emission lines or where disk reflection plays an important role. Both models provide good and statistically comparable fits, show that the X-ray dips are correlated with drops in the inner disk temperature, and require the existence of sub-relativistic (0.1--0.3$c$) ionized outflows. We propose that the disk temperature fluctuation stems from episodic drops of the mass accretion rate triggered by magnetic instabilities or/and wobbling of the inner accretion disk along the black hole's spin axis.","sentences":["We present the tidal disruption event (TDE) AT2022lri, hosted in a nearby ($\\approx\\!144$ Mpc) quiescent galaxy with a low-mass massive black hole ($10^4\\,M_\\odot <","M_{\\rm BH} < 10^6\\,M_\\odot$).","AT2022lri belongs to the TDE-H+He subtype.","More than 1 Ms of X-ray data were collected with NICER, Swift, and XMM-Newton from 187 d to 672 d after peak.","The X-ray luminosity gradually declined from $1.5\\times 10^{44}\\,{\\rm erg\\,s^{-1}}$ to $1.5\\times 10^{43}\\,{\\rm erg\\,s^{-1}}$ and remains much above the UV and optical luminosity, consistent with a super-Eddington accretion flow viewed face-on.","Sporadic strong X-ray dips atop a long-term decline are observed, with variability timescale of $\\approx\\!0.5$ hr--1 d and amplitude of $\\approx\\!2$--8.","When fitted with simple continuum models, the X-ray spectrum is dominated by a thermal disk component with inner temperature going from $\\sim\\! 146$ eV to $\\sim\\! 86$ eV. However, there are residual features that peak around 1 keV, which, in some cases, cannot be reproduced by a single broad emission line.","We analyzed a subset of time-resolved spectra with two physically motivated models describing either a scenario where ionized absorbers contribute extra absorption and emission lines or where disk reflection plays an important role.","Both models provide good and statistically comparable fits, show that the X-ray dips are correlated with drops in the inner disk temperature, and require the existence of sub-relativistic (0.1--0.3$c$) ionized outflows.","We propose that the disk temperature fluctuation stems from episodic drops of the mass accretion rate triggered by magnetic instabilities or/and wobbling of the inner accretion disk along the black hole's spin axis."],"url":"http://arxiv.org/abs/2405.11343v1","category":"astro-ph.HE"}
{"created":"2024-05-18 17:03:39","title":"EyeFound: A Multimodal Generalist Foundation Model for Ophthalmic Imaging","abstract":"Artificial intelligence (AI) is vital in ophthalmology, tackling tasks like diagnosis, classification, and visual question answering (VQA). However, existing AI models in this domain often require extensive annotation and are task-specific, limiting their clinical utility. While recent developments have brought about foundation models for ophthalmology, they are limited by the need to train separate weights for each imaging modality, preventing a comprehensive representation of multi-modal features. This highlights the need for versatile foundation models capable of handling various tasks and modalities in ophthalmology. To address this gap, we present EyeFound, a multimodal foundation model for ophthalmic images. Unlike existing models, EyeFound learns generalizable representations from unlabeled multimodal retinal images, enabling efficient model adaptation across multiple applications. Trained on 2.78 million images from 227 hospitals across 11 ophthalmic modalities, EyeFound facilitates generalist representations and diverse multimodal downstream tasks, even for detecting challenging rare diseases. It outperforms previous work RETFound in diagnosing eye diseases, predicting systemic disease incidents, and zero-shot multimodal VQA. EyeFound provides a generalizable solution to improve model performance and lessen the annotation burden on experts, facilitating widespread clinical AI applications from retinal imaging.","sentences":["Artificial intelligence (AI) is vital in ophthalmology, tackling tasks like diagnosis, classification, and visual question answering (VQA).","However, existing AI models in this domain often require extensive annotation and are task-specific, limiting their clinical utility.","While recent developments have brought about foundation models for ophthalmology, they are limited by the need to train separate weights for each imaging modality, preventing a comprehensive representation of multi-modal features.","This highlights the need for versatile foundation models capable of handling various tasks and modalities in ophthalmology.","To address this gap, we present EyeFound, a multimodal foundation model for ophthalmic images.","Unlike existing models, EyeFound learns generalizable representations from unlabeled multimodal retinal images, enabling efficient model adaptation across multiple applications.","Trained on 2.78 million images from 227 hospitals across 11 ophthalmic modalities, EyeFound facilitates generalist representations and diverse multimodal downstream tasks, even for detecting challenging rare diseases.","It outperforms previous work RETFound in diagnosing eye diseases, predicting systemic disease incidents, and zero-shot multimodal VQA.","EyeFound provides a generalizable solution to improve model performance and lessen the annotation burden on experts, facilitating widespread clinical AI applications from retinal imaging."],"url":"http://arxiv.org/abs/2405.11338v1","category":"cs.CV"}
{"created":"2024-05-18 16:42:44","title":"GinAR: An End-To-End Multivariate Time Series Forecasting Model Suitable for Variable Missing","abstract":"Multivariate time series forecasting (MTSF) is crucial for decision-making to precisely forecast the future values/trends, based on the complex relationships identified from historical observations of multiple sequences. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have gradually become the theme of MTSF model as their powerful capability in mining spatial-temporal dependencies, but almost of them heavily rely on the assumption of historical data integrity. In reality, due to factors such as data collector failures and time-consuming repairment, it is extremely challenging to collect the whole historical observations without missing any variable. In this case, STGNNs can only utilize a subset of normal variables and easily suffer from the incorrect spatial-temporal dependency modeling issue, resulting in the degradation of their forecasting performance. To address the problem, in this paper, we propose a novel Graph Interpolation Attention Recursive Network (named GinAR) to precisely model the spatial-temporal dependencies over the limited collected data for forecasting. In GinAR, it consists of two key components, that is, interpolation attention and adaptive graph convolution to take place of the fully connected layer of simple recursive units, and thus are capable of recovering all missing variables and reconstructing the correct spatial-temporal dependencies for recursively modeling of multivariate time series data, respectively. Extensive experiments conducted on five real-world datasets demonstrate that GinAR outperforms 11 SOTA baselines, and even when 90% of variables are missing, it can still accurately predict the future values of all variables.","sentences":["Multivariate time series forecasting (MTSF) is crucial for decision-making to precisely forecast the future values/trends, based on the complex relationships identified from historical observations of multiple sequences.","Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have gradually become the theme of MTSF model as their powerful capability in mining spatial-temporal dependencies, but almost of them heavily rely on the assumption of historical data integrity.","In reality, due to factors such as data collector failures and time-consuming repairment, it is extremely challenging to collect the whole historical observations without missing any variable.","In this case, STGNNs can only utilize a subset of normal variables and easily suffer from the incorrect spatial-temporal dependency modeling issue, resulting in the degradation of their forecasting performance.","To address the problem, in this paper, we propose a novel Graph Interpolation Attention Recursive Network (named GinAR) to precisely model the spatial-temporal dependencies over the limited collected data for forecasting.","In GinAR, it consists of two key components, that is, interpolation attention and adaptive graph convolution to take place of the fully connected layer of simple recursive units, and thus are capable of recovering all missing variables and reconstructing the correct spatial-temporal dependencies for recursively modeling of multivariate time series data, respectively.","Extensive experiments conducted on five real-world datasets demonstrate that GinAR outperforms 11 SOTA baselines, and even when 90% of variables are missing, it can still accurately predict the future values of all variables."],"url":"http://arxiv.org/abs/2405.11333v1","category":"cs.LG"}
{"created":"2024-05-18 16:31:32","title":"Generalized Multi-Objective Reinforcement Learning with Envelope Updates in URLLC-enabled Vehicular Networks","abstract":"We develop a novel multi-objective reinforcement learning (MORL) framework to jointly optimize wireless network selection and autonomous driving policies in a multi-band vehicular network operating on conventional sub-6GHz spectrum and Terahertz frequencies. The proposed framework is designed to 1. maximize the traffic flow and 2. minimize collisions by controlling the vehicle's motion dynamics (i.e., speed and acceleration), and enhance the ultra-reliable low-latency communication (URLLC) while minimizing handoffs (HOs). We cast this problem as a multi-objective Markov Decision Process (MOMDP) and develop solutions for both predefined and unknown preferences of the conflicting objectives. Specifically, deep-Q-network and double deep-Q-network-based solutions are developed first that consider scalarizing the transportation and telecommunication rewards using predefined preferences. We then develop a novel envelope MORL solution which develop policies that address multiple objectives with unknown preferences to the agent. While this approach reduces reliance on scalar rewards, policy effectiveness varying with different preferences is a challenge. To address this, we apply a generalized version of the Bellman equation and optimize the convex envelope of multi-objective Q values to learn a unified parametric representation capable of generating optimal policies across all possible preference configurations. Following an initial learning phase, our agent can execute optimal policies under any specified preference or infer preferences from minimal data samples.Numerical results validate the efficacy of the envelope-based MORL solution and demonstrate interesting insights related to the inter-dependency of vehicle motion dynamics, HOs, and the communication data rate. The proposed policies enable autonomous vehicles to adopt safe driving behaviors with improved connectivity.","sentences":["We develop a novel multi-objective reinforcement learning (MORL) framework to jointly optimize wireless network selection and autonomous driving policies in a multi-band vehicular network operating on conventional sub-6GHz spectrum and Terahertz frequencies.","The proposed framework is designed to 1.","maximize the traffic flow and 2.","minimize collisions by controlling the vehicle's motion dynamics (i.e., speed and acceleration), and enhance the ultra-reliable low-latency communication (URLLC) while minimizing handoffs (HOs).","We cast this problem as a multi-objective Markov Decision Process (MOMDP) and develop solutions for both predefined and unknown preferences of the conflicting objectives.","Specifically, deep-Q-network and double deep-Q-network-based solutions are developed first that consider scalarizing the transportation and telecommunication rewards using predefined preferences.","We then develop a novel envelope MORL solution which develop policies that address multiple objectives with unknown preferences to the agent.","While this approach reduces reliance on scalar rewards, policy effectiveness varying with different preferences is a challenge.","To address this, we apply a generalized version of the Bellman equation and optimize the convex envelope of multi-objective Q values to learn a unified parametric representation capable of generating optimal policies across all possible preference configurations.","Following an initial learning phase, our agent can execute optimal policies under any specified preference or infer preferences from minimal data samples.","Numerical results validate the efficacy of the envelope-based MORL solution and demonstrate interesting insights related to the inter-dependency of vehicle motion dynamics, HOs, and the communication data rate.","The proposed policies enable autonomous vehicles to adopt safe driving behaviors with improved connectivity."],"url":"http://arxiv.org/abs/2405.11331v1","category":"cs.LG"}
{"created":"2024-05-18 15:54:17","title":"Transverse polarization measurement of $\u039b$ hyperons in $p$Ne collisions at $\\sqrt{s_{NN}}$ = 68.4 GeV with the $\\mbox{LHCb}$ detector","abstract":"A measurement of the transverse polarization of the $\\Lambda$ and $\\bar{\\Lambda}$ hyperons in $p$Ne fixed-target collisions at $\\sqrt{s_{NN}}$ = 68.4 GeV is presented using data collected by the LHCb detector. The polarization is studied using the decay $\\Lambda \\rightarrow p \\pi^-$ together with its charge conjugated process, the integrated values measured are   $$ P_{\\Lambda} = 0.029 \\pm 0.019 \\, (\\rm{stat}) \\pm 0.012 \\, (\\rm{syst}) \\, , $$ $$ P_{\\bar{\\Lambda}} = 0.003 \\pm 0.023 \\, (\\rm{stat}) \\pm 0.014 \\,(\\rm{syst}) \\,. $$   Furthermore, the results are shown as a function of the Feynman~$x$~variable, transverse momentum, pseudorapidity and rapidity of the hyperons, and are compared with previous measurements.","sentences":["A measurement of the transverse polarization of the $\\Lambda$ and $\\bar{\\Lambda}$ hyperons in $p$Ne fixed-target collisions at $\\sqrt{s_{NN}}$ = 68.4 GeV is presented using data collected by the LHCb detector.","The polarization is studied using the decay $\\Lambda \\rightarrow p \\pi^-$ together with its charge conjugated process, the integrated values measured are   $$ P_{\\Lambda} = 0.029 \\pm 0.019 \\, (\\rm{stat})","\\pm 0.012 \\, (\\rm{syst}) \\, , $$ $$ P_{\\bar{\\Lambda}} = 0.003 \\pm 0.023 \\, (\\rm{stat}) \\pm 0.014 \\,(\\rm{syst}) \\,.","$$   Furthermore, the results are shown as a function of the Feynman~$x$~variable, transverse momentum, pseudorapidity and rapidity of the hyperons, and are compared with previous measurements."],"url":"http://arxiv.org/abs/2405.11324v1","category":"hep-ex"}
{"created":"2024-05-18 15:27:14","title":"Smooth Kolmogorov Arnold networks enabling structural knowledge representation","abstract":"Kolmogorov-Arnold Networks (KANs) offer an efficient and interpretable alternative to traditional multi-layer perceptron (MLP) architectures due to their finite network topology. However, according to the results of Kolmogorov and Vitushkin, the representation of generic smooth functions by KAN implementations using analytic functions constrained to a finite number of cutoff points cannot be exact. Hence, the convergence of KAN throughout the training process may be limited. This paper explores the relevance of smoothness in KANs, proposing that smooth, structurally informed KANs can achieve equivalence to MLPs in specific function classes. By leveraging inherent structural knowledge, KANs may reduce the data required for training and mitigate the risk of generating hallucinated predictions, thereby enhancing model reliability and performance in computational biomedicine.","sentences":["Kolmogorov-Arnold Networks (KANs) offer an efficient and interpretable alternative to traditional multi-layer perceptron (MLP) architectures due to their finite network topology.","However, according to the results of Kolmogorov and Vitushkin, the representation of generic smooth functions by KAN implementations using analytic functions constrained to a finite number of cutoff points cannot be exact.","Hence, the convergence of KAN throughout the training process may be limited.","This paper explores the relevance of smoothness in KANs, proposing that smooth, structurally informed KANs can achieve equivalence to MLPs in specific function classes.","By leveraging inherent structural knowledge, KANs may reduce the data required for training and mitigate the risk of generating hallucinated predictions, thereby enhancing model reliability and performance in computational biomedicine."],"url":"http://arxiv.org/abs/2405.11318v1","category":"cs.LG"}
{"created":"2024-05-18 15:24:58","title":"MediCLIP: Adapting CLIP for Few-shot Medical Image Anomaly Detection","abstract":"In the field of medical decision-making, precise anomaly detection in medical imaging plays a pivotal role in aiding clinicians. However, previous work is reliant on large-scale datasets for training anomaly detection models, which increases the development cost. This paper first focuses on the task of medical image anomaly detection in the few-shot setting, which is critically significant for the medical field where data collection and annotation are both very expensive. We propose an innovative approach, MediCLIP, which adapts the CLIP model to few-shot medical image anomaly detection through self-supervised fine-tuning. Although CLIP, as a vision-language model, demonstrates outstanding zero-/fewshot performance on various downstream tasks, it still falls short in the anomaly detection of medical images. To address this, we design a series of medical image anomaly synthesis tasks to simulate common disease patterns in medical imaging, transferring the powerful generalization capabilities of CLIP to the task of medical image anomaly detection. When only few-shot normal medical images are provided, MediCLIP achieves state-of-the-art performance in anomaly detection and location compared to other methods. Extensive experiments on three distinct medical anomaly detection tasks have demonstrated the superiority of our approach. The code is available at https://github.com/cnulab/MediCLIP.","sentences":["In the field of medical decision-making, precise anomaly detection in medical imaging plays a pivotal role in aiding clinicians.","However, previous work is reliant on large-scale datasets for training anomaly detection models, which increases the development cost.","This paper first focuses on the task of medical image anomaly detection in the few-shot setting, which is critically significant for the medical field where data collection and annotation are both very expensive.","We propose an innovative approach, MediCLIP, which adapts the CLIP model to few-shot medical image anomaly detection through self-supervised fine-tuning.","Although CLIP, as a vision-language model, demonstrates outstanding zero-/fewshot performance on various downstream tasks, it still falls short in the anomaly detection of medical images.","To address this, we design a series of medical image anomaly synthesis tasks to simulate common disease patterns in medical imaging, transferring the powerful generalization capabilities of CLIP to the task of medical image anomaly detection.","When only few-shot normal medical images are provided, MediCLIP achieves state-of-the-art performance in anomaly detection and location compared to other methods.","Extensive experiments on three distinct medical anomaly detection tasks have demonstrated the superiority of our approach.","The code is available at https://github.com/cnulab/MediCLIP."],"url":"http://arxiv.org/abs/2405.11315v1","category":"cs.CV"}
{"created":"2024-05-18 14:37:43","title":"Large Neighborhood Prioritized Search for Combinatorial Optimization with Answer Set Programming","abstract":"We propose Large Neighborhood Prioritized Search (LNPS) for solving combinatorial optimization problems in Answer Set Programming (ASP). LNPS is a metaheuristic that starts with an initial solution and then iteratively tries to find better solutions by alternately destroying and prioritized searching for a current solution. Due to the variability of neighborhoods, LNPS allows for flexible search without strongly depending on the destroy operators. We present an implementation of LNPS based on ASP. The resulting heulingo solver demonstrates that LNPS can significantly enhance the solving performance of ASP for optimization. Furthermore, we establish the competitiveness of our LNPS approach by empirically contrasting it to (adaptive) large neighborhood search.","sentences":["We propose Large Neighborhood Prioritized Search (LNPS) for solving combinatorial optimization problems in Answer Set Programming (ASP).","LNPS is a metaheuristic that starts with an initial solution and then iteratively tries to find better solutions by alternately destroying and prioritized searching for a current solution.","Due to the variability of neighborhoods, LNPS allows for flexible search without strongly depending on the destroy operators.","We present an implementation of LNPS based on ASP.","The resulting heulingo solver demonstrates that LNPS can significantly enhance the solving performance of ASP for optimization.","Furthermore, we establish the competitiveness of our LNPS approach by empirically contrasting it to (adaptive) large neighborhood search."],"url":"http://arxiv.org/abs/2405.11305v1","category":"cs.AI"}
{"created":"2024-05-18 14:10:31","title":"Ensuring Safety at Intelligent Intersections: Temporal Logic Meets Reachability Analysis","abstract":"In this work, we propose an approach for ensuring the safety of vehicles passing through an intelligent intersection. There are many proposals for the design of intelligent intersections that introduce central decision-makers to intersections for enhancing the efficiency and safety of the vehicles. To guarantee the safety of such designs, we develop a safety framework for intersections based on temporal logic and reachability analysis. We start by specifying the required behavior for all the vehicles that need to pass through the intersection as linear temporal logic formula. Then, using temporal logic trees, we break down the linear temporal logic specification into a series of Hamilton-Jacobi reachability analyses in an automated fashion. By successfully constructing the temporal logic tree through reachability analysis, we verify the feasibility of the intersection specification. By taking this approach, we enable a safety framework that is able to automatically provide safety guarantees on new intersection behavior specifications. To evaluate our approach, we implement the framework on a simulated T-intersection, where we show that we can check and guarantee the safety of vehicles with potentially conflicting paths.","sentences":["In this work, we propose an approach for ensuring the safety of vehicles passing through an intelligent intersection.","There are many proposals for the design of intelligent intersections that introduce central decision-makers to intersections for enhancing the efficiency and safety of the vehicles.","To guarantee the safety of such designs, we develop a safety framework for intersections based on temporal logic and reachability analysis.","We start by specifying the required behavior for all the vehicles that need to pass through the intersection as linear temporal logic formula.","Then, using temporal logic trees, we break down the linear temporal logic specification into a series of Hamilton-Jacobi reachability analyses in an automated fashion.","By successfully constructing the temporal logic tree through reachability analysis, we verify the feasibility of the intersection specification.","By taking this approach, we enable a safety framework that is able to automatically provide safety guarantees on new intersection behavior specifications.","To evaluate our approach, we implement the framework on a simulated T-intersection, where we show that we can check and guarantee the safety of vehicles with potentially conflicting paths."],"url":"http://arxiv.org/abs/2405.11300v1","category":"eess.SY"}
{"created":"2024-05-18 13:43:43","title":"Medical Image Analysis for Detection, Treatment and Planning of Disease using Artificial Intelligence Approaches","abstract":"X-ray is one of the prevalent image modalities for the detection and diagnosis of the human body. X-ray provides an actual anatomical structure of an organ present with disease or absence of disease. Segmentation of disease in chest X-ray images is essential for the diagnosis and treatment. In this paper, a framework for the segmentation of X-ray images using artificial intelligence techniques has been discussed. Here data has been pre-processed and cleaned followed by segmentation using SegNet and Residual Net approaches to X-ray images. Finally, segmentation has been evaluated using well known metrics like Loss, Dice Coefficient, Jaccard Coefficient, Precision, Recall, Binary Accuracy, and Validation Accuracy. The experimental results reveal that the proposed approach performs better in all respect of well-known parameters with 16 batch size and 50 epochs. The value of validation accuracy, precision, and recall of SegNet and Residual Unet models are 0.9815, 0.9699, 0.9574, and 0.9901, 0.9864, 0.9750 respectively.","sentences":["X-ray is one of the prevalent image modalities for the detection and diagnosis of the human body.","X-ray provides an actual anatomical structure of an organ present with disease or absence of disease.","Segmentation of disease in chest X-ray images is essential for the diagnosis and treatment.","In this paper, a framework for the segmentation of X-ray images using artificial intelligence techniques has been discussed.","Here data has been pre-processed and cleaned followed by segmentation using SegNet and Residual Net approaches to X-ray images.","Finally, segmentation has been evaluated using well known metrics like Loss, Dice Coefficient, Jaccard Coefficient, Precision, Recall, Binary Accuracy, and Validation Accuracy.","The experimental results reveal that the proposed approach performs better in all respect of well-known parameters with 16 batch size and 50 epochs.","The value of validation accuracy, precision, and recall of SegNet and Residual Unet models are 0.9815, 0.9699, 0.9574, and 0.9901, 0.9864, 0.9750 respectively."],"url":"http://arxiv.org/abs/2405.11295v1","category":"eess.IV"}
{"created":"2024-05-18 13:09:33","title":"The Logic of Counterfactuals and the Epistemology of Causal Inference","abstract":"The 2021 Nobel Prize in Economics recognized a theory of causal inference, which deserves more attention from philosophers. To that end, I develop a dialectic that extends the Lewis-Stalnaker debate on a logical principle called Conditional Excluded Middle (CEM). I first play the good cop for CEM, and give a new argument for it: a Quine-Putnam indispensability argument based on the Nobel-Prize winning theory. But then I switch sides and play the bad cop: I undermine that argument with a new theory of causal inference that preserves the success of the original theory but dispenses with CEM.","sentences":["The 2021 Nobel Prize in Economics recognized a theory of causal inference, which deserves more attention from philosophers.","To that end, I develop a dialectic that extends the Lewis-Stalnaker debate on a logical principle called Conditional Excluded Middle (CEM).","I first play the good cop for CEM, and give a new argument for it: a Quine-Putnam indispensability argument based on the Nobel-Prize winning theory.","But then I switch sides and play the bad cop: I undermine that argument with a new theory of causal inference that preserves the success of the original theory but dispenses with CEM."],"url":"http://arxiv.org/abs/2405.11284v1","category":"cs.AI"}
{"created":"2024-05-18 12:58:02","title":"Estimating the Level of Dialectness Predicts Interannotator Agreement in Multi-dialect Arabic Datasets","abstract":"On annotating multi-dialect Arabic datasets, it is common to randomly assign the samples across a pool of native Arabic speakers. Recent analyses recommended routing dialectal samples to native speakers of their respective dialects to build higher-quality datasets. However, automatically identifying the dialect of samples is hard. Moreover, the pool of annotators who are native speakers of specific Arabic dialects might be scarce. Arabic Level of Dialectness (ALDi) was recently introduced as a quantitative variable that measures how sentences diverge from Standard Arabic. On randomly assigning samples to annotators, we hypothesize that samples of higher ALDi scores are harder to label especially if they are written in dialects that the annotators do not speak. We test this by analyzing the relation between ALDi scores and the annotators' agreement, on 15 public datasets having raw individual sample annotations for various sentence-classification tasks. We find strong evidence supporting our hypothesis for 11 of them. Consequently, we recommend prioritizing routing samples of high ALDi scores to native speakers of each sample's dialect, for which the dialect could be automatically identified at higher accuracies.","sentences":["On annotating multi-dialect Arabic datasets, it is common to randomly assign the samples across a pool of native Arabic speakers.","Recent analyses recommended routing dialectal samples to native speakers of their respective dialects to build higher-quality datasets.","However, automatically identifying the dialect of samples is hard.","Moreover, the pool of annotators who are native speakers of specific Arabic dialects might be scarce.","Arabic Level of Dialectness (ALDi) was recently introduced as a quantitative variable that measures how sentences diverge from Standard Arabic.","On randomly assigning samples to annotators, we hypothesize that samples of higher ALDi scores are harder to label especially if they are written in dialects that the annotators do not speak.","We test this by analyzing the relation between ALDi scores and the annotators' agreement, on 15 public datasets having raw individual sample annotations for various sentence-classification tasks.","We find strong evidence supporting our hypothesis for 11 of them.","Consequently, we recommend prioritizing routing samples of high ALDi scores to native speakers of each sample's dialect, for which the dialect could be automatically identified at higher accuracies."],"url":"http://arxiv.org/abs/2405.11282v1","category":"cs.CL"}
{"created":"2024-05-18 12:45:00","title":"Cooperative Cognitive Dynamic System in UAV Swarms: Reconfigurable Mechanism and Framework","abstract":"As the demands for immediate and effective responses increase in both civilian and military domains, the unmanned aerial vehicle (UAV) swarms emerge as effective solutions, in which multiple cooperative UAVs can work together to achieve specific goals. However, how to manage such complex systems to ensure real-time adaptability lack sufficient researches. Hence, in this paper, we propose the cooperative cognitive dynamic system (CCDS), to optimize the management for UAV swarms. CCDS leverages a hierarchical and cooperative control structure that enables real-time data processing and decision. Accordingly, CCDS optimizes the UAV swarm management via dynamic reconfigurability and adaptive intelligent optimization. In addition, CCDS can be integrated with the biomimetic mechanism to efficiently allocate tasks for UAV swarms. Further, the distributed coordination of CCDS ensures reliable and resilient control, thus enhancing the adaptability and robustness. Finally, the potential challenges and future directions are analyzed, to provide insights into managing UAV swarms in dynamic heterogeneous networking.","sentences":["As the demands for immediate and effective responses increase in both civilian and military domains, the unmanned aerial vehicle (UAV) swarms emerge as effective solutions, in which multiple cooperative UAVs can work together to achieve specific goals.","However, how to manage such complex systems to ensure real-time adaptability lack sufficient researches.","Hence, in this paper, we propose the cooperative cognitive dynamic system (CCDS), to optimize the management for UAV swarms.","CCDS leverages a hierarchical and cooperative control structure that enables real-time data processing and decision.","Accordingly, CCDS optimizes the UAV swarm management via dynamic reconfigurability and adaptive intelligent optimization.","In addition, CCDS can be integrated with the biomimetic mechanism to efficiently allocate tasks for UAV swarms.","Further, the distributed coordination of CCDS ensures reliable and resilient control, thus enhancing the adaptability and robustness.","Finally, the potential challenges and future directions are analyzed, to provide insights into managing UAV swarms in dynamic heterogeneous networking."],"url":"http://arxiv.org/abs/2405.11281v1","category":"cs.DC"}
{"created":"2024-05-18 12:26:31","title":"Action Controlled Paraphrasing","abstract":"Recent studies have demonstrated the potential to control paraphrase generation, such as through syntax, which has broad applications in various downstream tasks. However, these methods often require detailed parse trees or syntactic exemplars, which are not user-friendly. Furthermore, an inference gap exists, as control specifications are only available during training but not inference. In this work, we propose a new setup for controlled paraphrasing. Specifically, we represent user-intended actions as action tokens, allowing embedding and concatenating them with text embeddings, thus flowing together to a self-attention encoder for representation fusion. To address the inference gap, we introduce an optional action token as a placeholder that encourages the model to determine the appropriate action when control specifications are inaccessible. Experimental results show that our method successfully enables specific action-controlled paraphrasing and preserves the same or even better performance compared to conventional uncontrolled methods when actions are not given. Our findings thus promote the concept of optional action control for a more user-centered design via representation learning.","sentences":["Recent studies have demonstrated the potential to control paraphrase generation, such as through syntax, which has broad applications in various downstream tasks.","However, these methods often require detailed parse trees or syntactic exemplars, which are not user-friendly.","Furthermore, an inference gap exists, as control specifications are only available during training but not inference.","In this work, we propose a new setup for controlled paraphrasing.","Specifically, we represent user-intended actions as action tokens, allowing embedding and concatenating them with text embeddings, thus flowing together to a self-attention encoder for representation fusion.","To address the inference gap, we introduce an optional action token as a placeholder that encourages the model to determine the appropriate action when control specifications are inaccessible.","Experimental results show that our method successfully enables specific action-controlled paraphrasing and preserves the same or even better performance compared to conventional uncontrolled methods when actions are not given.","Our findings thus promote the concept of optional action control for a more user-centered design via representation learning."],"url":"http://arxiv.org/abs/2405.11277v1","category":"cs.CL"}
{"created":"2024-05-18 12:16:01","title":"Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts","abstract":"Recent advancements in Multimodal Large Language Models (MLLMs) underscore the significance of scalable models and data to boost performance, yet this often incurs substantial computational costs. Although the Mixture of Experts (MoE) architecture has been employed to efficiently scale large language and image-text models, these efforts typically involve fewer experts and limited modalities. To address this, our work presents the pioneering attempt to develop a unified MLLM with the MoE architecture, named Uni-MoE that can handle a wide array of modalities. Specifically, it features modality-specific encoders with connectors for a unified multimodal representation. We also implement a sparse MoE architecture within the LLMs to enable efficient training and inference through modality-level data parallelism and expert-level model parallelism. To enhance the multi-expert collaboration and generalization, we present a progressive training strategy: 1) Cross-modality alignment using various connectors with different cross-modality data, 2) Training modality-specific experts with cross-modality instruction data to activate experts' preferences, and 3) Tuning the Uni-MoE framework utilizing Low-Rank Adaptation (LoRA) on mixed multimodal instruction data. We evaluate the instruction-tuned Uni-MoE on a comprehensive set of multimodal datasets. The extensive experimental results demonstrate Uni-MoE's principal advantage of significantly reducing performance bias in handling mixed multimodal datasets, alongside improved multi-expert collaboration and generalization. Our findings highlight the substantial potential of MoE frameworks in advancing MLLMs and the code is available at https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs.","sentences":["Recent advancements in Multimodal Large Language Models (MLLMs) underscore the significance of scalable models and data to boost performance, yet this often incurs substantial computational costs.","Although the Mixture of Experts (MoE) architecture has been employed to efficiently scale large language and image-text models, these efforts typically involve fewer experts and limited modalities.","To address this, our work presents the pioneering attempt to develop a unified MLLM with the MoE architecture, named Uni-MoE that can handle a wide array of modalities.","Specifically, it features modality-specific encoders with connectors for a unified multimodal representation.","We also implement a sparse MoE architecture within the LLMs to enable efficient training and inference through modality-level data parallelism and expert-level model parallelism.","To enhance the multi-expert collaboration and generalization, we present a progressive training strategy: 1) Cross-modality alignment using various connectors with different cross-modality data, 2) Training modality-specific experts with cross-modality instruction data to activate experts' preferences, and 3) Tuning the Uni-MoE framework utilizing Low-Rank Adaptation (LoRA) on mixed multimodal instruction data.","We evaluate the instruction-tuned Uni-MoE on a comprehensive set of multimodal datasets.","The extensive experimental results demonstrate Uni-MoE's principal advantage of significantly reducing performance bias in handling mixed multimodal datasets, alongside improved multi-expert collaboration and generalization.","Our findings highlight the substantial potential of MoE frameworks in advancing MLLMs and the code is available at https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs."],"url":"http://arxiv.org/abs/2405.11273v1","category":"cs.AI"}
{"created":"2024-05-18 12:15:10","title":"Double Correction Framework for Denoising Recommendation","abstract":"As its availability and generality in online services, implicit feedback is more commonly used in recommender systems. However, implicit feedback usually presents noisy samples in real-world recommendation scenarios (such as misclicks or non-preferential behaviors), which will affect precise user preference learning. To overcome the noisy samples problem, a popular solution is based on dropping noisy samples in the model training phase, which follows the observation that noisy samples have higher training losses than clean samples. Despite the effectiveness, we argue that this solution still has limits. (1) High training losses can result from model optimization instability or hard samples, not just noisy samples. (2) Completely dropping of noisy samples will aggravate the data sparsity, which lacks full data exploitation. To tackle the above limitations, we propose a Double Correction Framework for Denoising Recommendation (DCF), which contains two correction components from views of more precise sample dropping and avoiding more sparse data. In the sample dropping correction component, we use the loss value of the samples over time to determine whether it is noise or not, increasing dropping stability. Instead of averaging directly, we use the damping function to reduce the bias effect of outliers. Furthermore, due to the higher variance exhibited by hard samples, we derive a lower bound for the loss through concentration inequality to identify and reuse hard samples. In progressive label correction, we iteratively re-label highly deterministic noisy samples and retrain them to further improve performance. Finally, extensive experimental results on three datasets and four backbones demonstrate the effectiveness and generalization of our proposed framework.","sentences":["As its availability and generality in online services, implicit feedback is more commonly used in recommender systems.","However, implicit feedback usually presents noisy samples in real-world recommendation scenarios (such as misclicks or non-preferential behaviors), which will affect precise user preference learning.","To overcome the noisy samples problem, a popular solution is based on dropping noisy samples in the model training phase, which follows the observation that noisy samples have higher training losses than clean samples.","Despite the effectiveness, we argue that this solution still has limits.","(1) High training losses can result from model optimization instability or hard samples, not just noisy samples.","(2) Completely dropping of noisy samples will aggravate the data sparsity, which lacks full data exploitation.","To tackle the above limitations, we propose a Double Correction Framework for Denoising Recommendation (DCF), which contains two correction components from views of more precise sample dropping and avoiding more sparse data.","In the sample dropping correction component, we use the loss value of the samples over time to determine whether it is noise or not, increasing dropping stability.","Instead of averaging directly, we use the damping function to reduce the bias effect of outliers.","Furthermore, due to the higher variance exhibited by hard samples, we derive a lower bound for the loss through concentration inequality to identify and reuse hard samples.","In progressive label correction, we iteratively re-label highly deterministic noisy samples and retrain them to further improve performance.","Finally, extensive experimental results on three datasets and four backbones demonstrate the effectiveness and generalization of our proposed framework."],"url":"http://arxiv.org/abs/2405.11272v1","category":"cs.IR"}
{"created":"2024-05-18 11:31:03","title":"EnviroExam: Benchmarking Environmental Science Knowledge of Large Language Models","abstract":"In the field of environmental science, it is crucial to have robust evaluation metrics for large language models to ensure their efficacy and accuracy. We propose EnviroExam, a comprehensive evaluation method designed to assess the knowledge of large language models in the field of environmental science. EnviroExam is based on the curricula of top international universities, covering undergraduate, master's, and doctoral courses, and includes 936 questions across 42 core courses. By conducting 0-shot and 5-shot tests on 31 open-source large language models, EnviroExam reveals the performance differences among these models in the domain of environmental science and provides detailed evaluation standards. The results show that 61.3% of the models passed the 5-shot tests, while 48.39% passed the 0-shot tests. By introducing the coefficient of variation as an indicator, we evaluate the performance of mainstream open-source large language models in environmental science from multiple perspectives, providing effective criteria for selecting and fine-tuning language models in this field. Future research will involve constructing more domain-specific test sets using specialized environmental science textbooks to further enhance the accuracy and specificity of the evaluation.","sentences":["In the field of environmental science, it is crucial to have robust evaluation metrics for large language models to ensure their efficacy and accuracy.","We propose EnviroExam, a comprehensive evaluation method designed to assess the knowledge of large language models in the field of environmental science.","EnviroExam is based on the curricula of top international universities, covering undergraduate, master's, and doctoral courses, and includes 936 questions across 42 core courses.","By conducting 0-shot and 5-shot tests on 31 open-source large language models, EnviroExam reveals the performance differences among these models in the domain of environmental science and provides detailed evaluation standards.","The results show that 61.3% of the models passed the 5-shot tests, while 48.39% passed the 0-shot tests.","By introducing the coefficient of variation as an indicator, we evaluate the performance of mainstream open-source large language models in environmental science from multiple perspectives, providing effective criteria for selecting and fine-tuning language models in this field.","Future research will involve constructing more domain-specific test sets using specialized environmental science textbooks to further enhance the accuracy and specificity of the evaluation."],"url":"http://arxiv.org/abs/2405.11265v1","category":"cs.CL"}
{"created":"2024-05-18 10:34:59","title":"A refined saturation theorem for polynomials and applications","abstract":"For a dynamical system $(X,T)$, $d\\in\\mathbb{N}$ and distinct non-constant integral polynomials $p_1,\\ldots, p_d$ vanishing at $0$, the notion of regionally proximal relation along $C=\\{p_1,\\ldots,p_d\\}$ (denoted by $RP_C^{[d]}(X,T)$) is introduced.   It turns out that for a minimal system, $RP_C^{[d]}(X,T)=\\Delta$ implies that $X$ is an almost one-to-one extension of $X_k$ for some $k\\in\\mathbb{N}$ only depending on a set of finite polynomials associated with $C$ and has zero entropy, where $X_k$ is the maximal $k$-step pro-nilfactor of $X$.   Particularly, when $C$ is a collection of linear polynomials, it is proved that $RP_C^{[d]}(X,T)=\\Delta$ implies $(X,T)$ is a $d$-step pro-nilsystem, which answers negatively a conjecture in \\cite{5p}. The results are obtained by proving a refined saturation theorem for polynomials.","sentences":["For a dynamical system $(X,T)$, $d\\in\\mathbb{N}$ and distinct non-constant integral polynomials $p_1,\\ldots, p_d$ vanishing at $0$, the notion of regionally proximal relation along $C=\\{p_1,\\ldots,p_d\\}$ (denoted by $RP_C^{[d]}(X,T)$) is introduced.   ","It turns out that for a minimal system, $RP_C^{[d]}(X,T)=\\Delta$ implies that $X$ is an almost one-to-one extension of $X_k$ for some $k\\in\\mathbb{N}$ only depending on a set of finite polynomials associated with $C$ and has zero entropy, where $X_k$ is the maximal $k$-step pro-nilfactor of $X$.   Particularly, when $C$ is a collection of linear polynomials, it is proved that $RP_C^{[d]}(X,T)=\\Delta$ implies $(X,T)$ is a $d$-step pro-nilsystem, which answers negatively a conjecture in \\cite{5p}.","The results are obtained by proving a refined saturation theorem for polynomials."],"url":"http://arxiv.org/abs/2405.11251v1","category":"math.DS"}
{"created":"2024-05-18 10:34:34","title":"Argumentative Causal Discovery","abstract":"Causal discovery amounts to unearthing causal relationships amongst features in data. It is a crucial companion to causal inference, necessary to build scientific knowledge without resorting to expensive or impossible randomised control trials. In this paper, we explore how reasoning with symbolic representations can support causal discovery. Specifically, we deploy assumption-based argumentation (ABA), a well-established and powerful knowledge representation formalism, in combination with causality theories, to learn graphs which reflect causal dependencies in the data. We prove that our method exhibits desirable properties, notably that, under natural conditions, it can retrieve ground-truth causal graphs. We also conduct experiments with an implementation of our method in answer set programming (ASP) on four datasets from standard benchmarks in causal discovery, showing that our method compares well against established baselines.","sentences":["Causal discovery amounts to unearthing causal relationships amongst features in data.","It is a crucial companion to causal inference, necessary to build scientific knowledge without resorting to expensive or impossible randomised control trials.","In this paper, we explore how reasoning with symbolic representations can support causal discovery.","Specifically, we deploy assumption-based argumentation (ABA), a well-established and powerful knowledge representation formalism, in combination with causality theories, to learn graphs which reflect causal dependencies in the data.","We prove that our method exhibits desirable properties, notably that, under natural conditions, it can retrieve ground-truth causal graphs.","We also conduct experiments with an implementation of our method in answer set programming (ASP) on four datasets from standard benchmarks in causal discovery, showing that our method compares well against established baselines."],"url":"http://arxiv.org/abs/2405.11250v1","category":"cs.AI"}
{"created":"2024-05-18 10:15:31","title":"Few-Shot API Attack Anomaly Detection in a Classification-by-Retrieval Framework","abstract":"Application Programming Interface (API) attacks refer to the unauthorized or malicious use of APIs, which are often exploited to gain access to sensitive data or manipulate online systems for illicit purposes. Identifying actors that deceitfully utilize an API poses a demanding problem. Although there have been notable advancements and contributions in the field of API security, there still remains a significant challenge when dealing with attackers who use novel approaches that don't match the well-known payloads commonly seen in attacks. Also, attackers may exploit standard functionalities in unconventional manners and with objectives surpassing their intended boundaries. This means API security needs to be more sophisticated and dynamic than ever, with advanced computational intelligence methods, such as machine learning models that can quickly identify and respond to anomalous behavior. In response to these challenges, we propose a novel few-shot anomaly detection framework, named FT-ANN. This framework is composed of two parts: First, we train a dedicated generic language model for API based on FastText embedding. Next, we use Approximate Nearest Neighbor search in a classification-by-retrieval approach. Our framework enables the development of a lightweight model that can be trained with minimal examples per class or even a model capable of classifying multiple classes. The results show that our framework effectively improves API attack detection accuracy compared to various baselines.","sentences":["Application Programming Interface (API) attacks refer to the unauthorized or malicious use of APIs, which are often exploited to gain access to sensitive data or manipulate online systems for illicit purposes.","Identifying actors that deceitfully utilize an API poses a demanding problem.","Although there have been notable advancements and contributions in the field of API security, there still remains a significant challenge when dealing with attackers who use novel approaches that don't match the well-known payloads commonly seen in attacks.","Also, attackers may exploit standard functionalities in unconventional manners and with objectives surpassing their intended boundaries.","This means API security needs to be more sophisticated and dynamic than ever, with advanced computational intelligence methods, such as machine learning models that can quickly identify and respond to anomalous behavior.","In response to these challenges, we propose a novel few-shot anomaly detection framework, named FT-ANN.","This framework is composed of two parts: First, we train a dedicated generic language model for API based on FastText embedding.","Next, we use Approximate Nearest Neighbor search in a classification-by-retrieval approach.","Our framework enables the development of a lightweight model that can be trained with minimal examples per class or even a model capable of classifying multiple classes.","The results show that our framework effectively improves API attack detection accuracy compared to various baselines."],"url":"http://arxiv.org/abs/2405.11247v1","category":"cs.CR"}
{"created":"2024-05-18 09:41:51","title":"Testing the Performance of Face Recognition for People with Down Syndrome","abstract":"The fairness of biometric systems, in particular facial recognition, is often analysed for larger demographic groups, e.g. female vs. male or black vs. white. In contrast to this, minority groups are commonly ignored. This paper investigates the performance of facial recognition algorithms on individuals with Down syndrome, a common chromosomal abnormality that affects approximately one in 1,000 births per year. To do so, a database of 98 individuals with Down syndrome, each represented by at least five facial images, is semi-automatically collected from YouTube. Subsequently, two facial image quality assessment algorithms and five recognition algorithms are evaluated on the newly collected database and on the public facial image databases CelebA and FRGCv2. The results show that the quality scores of facial images for individuals with Down syndrome are comparable to those of individuals without Down syndrome captured under similar conditions. Furthermore, it is observed that face recognition performance decreases significantly for individuals with Down syndrome, which is largely attributed to the increased likelihood of false matches.","sentences":["The fairness of biometric systems, in particular facial recognition, is often analysed for larger demographic groups, e.g. female vs. male or black vs. white.","In contrast to this, minority groups are commonly ignored.","This paper investigates the performance of facial recognition algorithms on individuals with Down syndrome, a common chromosomal abnormality that affects approximately one in 1,000 births per year.","To do so, a database of 98 individuals with Down syndrome, each represented by at least five facial images, is semi-automatically collected from YouTube.","Subsequently, two facial image quality assessment algorithms and five recognition algorithms are evaluated on the newly collected database and on the public facial image databases CelebA and FRGCv2.","The results show that the quality scores of facial images for individuals with Down syndrome are comparable to those of individuals without Down syndrome captured under similar conditions.","Furthermore, it is observed that face recognition performance decreases significantly for individuals with Down syndrome, which is largely attributed to the increased likelihood of false matches."],"url":"http://arxiv.org/abs/2405.11240v1","category":"cs.CV"}
{"created":"2024-05-18 09:37:04","title":"SimAD: A Simple Dissimilarity-based Approach for Time Series Anomaly Detection","abstract":"Despite the prevalence of reconstruction-based deep learning methods, time series anomaly detection remains challenging. Existing approaches often struggle with limited temporal contexts, inadequate representation of normal patterns, and flawed evaluation metrics, hindering their effectiveness in identifying aberrant behavior. To address these issues, we introduce $\\textbf{{SimAD}}$, a $\\textbf{{Sim}}$ple dissimilarity-based approach for time series $\\textbf{{A}}$nomaly $\\textbf{{D}}$etection. SimAD incorporates an advanced feature extractor adept at processing extended temporal windows, utilizes the EmbedPatch encoder to integrate normal behavioral patterns comprehensively, and introduces an innovative ContrastFusion module designed to accentuate distributional divergences between normal and abnormal data, thereby enhancing the robustness of anomaly discrimination. Additionally, we propose two robust evaluation metrics, UAff and NAff, addressing the limitations of existing metrics and demonstrating their reliability through theoretical and experimental analyses. Experiments across $\\textbf{seven}$ diverse time series datasets demonstrate SimAD's superior performance compared to state-of-the-art methods, achieving relative improvements of $\\textbf{19.85%}$ on F1, $\\textbf{4.44%}$ on Aff-F1, $\\textbf{77.79%}$ on NAff-F1, and $\\textbf{9.69%}$ on AUC on six multivariate datasets. Code and pre-trained models are available at https://github.com/EmorZz1G/SimAD.","sentences":["Despite the prevalence of reconstruction-based deep learning methods, time series anomaly detection remains challenging.","Existing approaches often struggle with limited temporal contexts, inadequate representation of normal patterns, and flawed evaluation metrics, hindering their effectiveness in identifying aberrant behavior.","To address these issues, we introduce $\\textbf{{SimAD}}$, a $\\textbf{{Sim}}$ple dissimilarity-based approach for time series $\\textbf{{A}}$nomaly $\\textbf{{D}}$etection.","SimAD incorporates an advanced feature extractor adept at processing extended temporal windows, utilizes the EmbedPatch encoder to integrate normal behavioral patterns comprehensively, and introduces an innovative ContrastFusion module designed to accentuate distributional divergences between normal and abnormal data, thereby enhancing the robustness of anomaly discrimination.","Additionally, we propose two robust evaluation metrics, UAff and NAff, addressing the limitations of existing metrics and demonstrating their reliability through theoretical and experimental analyses.","Experiments across $\\textbf{seven}$ diverse time series datasets demonstrate SimAD's superior performance compared to state-of-the-art methods, achieving relative improvements of $\\textbf{19.85%}$ on F1, $\\textbf{4.44%}$ on Aff-F1, $\\textbf{77.79%}$ on NAff-F1, and $\\textbf{9.69%}$ on AUC on six multivariate datasets.","Code and pre-trained models are available at https://github.com/EmorZz1G/SimAD."],"url":"http://arxiv.org/abs/2405.11238v1","category":"cs.LG"}
{"created":"2024-05-18 09:06:41","title":"Bridge and Hint: Extending Pre-trained Language Models for Long-Range Code","abstract":"In the field of code intelligence, effectively modeling long-range code poses a significant challenge. Existing pre-trained language models (PLMs) such as UniXcoder have achieved remarkable success, but they still face difficulties with long code inputs. This is mainly due to their limited capacity to maintain contextual continuity and memorize the key information over long-range code. To alleviate the difficulties, we propose EXPO, a framework for EXtending Pre-trained language models for lOng-range code. EXPO incorporates two innovative memory mechanisms we propose in this paper: Bridge Memory and Hint Memory. Bridge Memory uses a tagging mechanism to connect disparate snippets of long-range code, helping the model maintain contextual coherence. Hint Memory focuses on crucial code elements throughout the global context, such as package imports, by integrating a kNN attention layer to adaptively select the relevant code elements. This dual-memory approach bridges the gap between understanding local code snippets and maintaining global code coherence, thereby enhancing the model overall comprehension of long code sequences. We validate the effectiveness of EXPO on five popular pre-trained language models such as UniXcoder and two code intelligence tasks including API recommendation and vulnerability detection. Experimental results demonstrate that EXPO significantly improves the pre-training language models.","sentences":["In the field of code intelligence, effectively modeling long-range code poses a significant challenge.","Existing pre-trained language models (PLMs) such as UniXcoder have achieved remarkable success, but they still face difficulties with long code inputs.","This is mainly due to their limited capacity to maintain contextual continuity and memorize the key information over long-range code.","To alleviate the difficulties, we propose EXPO, a framework for EXtending Pre-trained language models for lOng-range code.","EXPO incorporates two innovative memory mechanisms we propose in this paper: Bridge Memory and Hint Memory.","Bridge Memory uses a tagging mechanism to connect disparate snippets of long-range code, helping the model maintain contextual coherence.","Hint Memory focuses on crucial code elements throughout the global context, such as package imports, by integrating a kNN attention layer to adaptively select the relevant code elements.","This dual-memory approach bridges the gap between understanding local code snippets and maintaining global code coherence, thereby enhancing the model overall comprehension of long code sequences.","We validate the effectiveness of EXPO on five popular pre-trained language models such as UniXcoder and two code intelligence tasks including API recommendation and vulnerability detection.","Experimental results demonstrate that EXPO significantly improves the pre-training language models."],"url":"http://arxiv.org/abs/2405.11233v1","category":"cs.SE"}
{"created":"2024-05-20 17:59:45","title":"Locational marginal burden: Quantifying the equity of optimal power flow solutions","abstract":"Fair distribution of benefits in electric power systems is a pertinent energy policymaking problem; however, these efforts cannot be easily quantified in power system engineering studies. Therefore, we propose locational marginal burden (LMB) to provide an interface between a well-studied measure of energy pricing equity, energy burden, with an optimal power flow problem (OPF). This is achieved by investigating the intrinsic link between the dual optimal solution of an OPF problem and the electricity prices, which are used to calculate the energy burden. By applying results from the field of differentiable optimization, locational marginal prices (LMPs) associated with an OPF solution can be differentiated with respect to demand. This enables electricity retail prices, and thereby, energy burden itself, to be differentiated, resulting in the proposed LMB. Simulation of a synthetic Hawaii network interfaced with real-world socioeconomic data shows how the LMB provides new insights into how the operation of the electricity network affects the equity of energy prices.","sentences":["Fair distribution of benefits in electric power systems is a pertinent energy policymaking problem; however, these efforts cannot be easily quantified in power system engineering studies.","Therefore, we propose locational marginal burden (LMB) to provide an interface between a well-studied measure of energy pricing equity, energy burden, with an optimal power flow problem (OPF).","This is achieved by investigating the intrinsic link between the dual optimal solution of an OPF problem and the electricity prices, which are used to calculate the energy burden.","By applying results from the field of differentiable optimization, locational marginal prices (LMPs) associated with an OPF solution can be differentiated with respect to demand.","This enables electricity retail prices, and thereby, energy burden itself, to be differentiated, resulting in the proposed LMB.","Simulation of a synthetic Hawaii network interfaced with real-world socioeconomic data shows how the LMB provides new insights into how the operation of the electricity network affects the equity of energy prices."],"url":"http://arxiv.org/abs/2405.12219v1","category":"eess.SY"}
{"created":"2024-05-20 17:18:26","title":"Brewer-Nash Scrutinised: Mechanised Checking of Policies featuring Write Revocation","abstract":"This paper revisits the Brewer-Nash security policy model inspired by ethical Chinese Wall policies. We draw attention to the fact that write access can be revoked in the Brewer-Nash model. The semantics of write access were underspecified originally, leading to multiple interpretations for which we provide a modern operational semantics. We go on to modernise the analysis of information flow in the Brewer-Nash model, by adopting a more precise definition adapted from Kessler. For our modernised reformulation, we provide full mechanised coverage for all theorems proposed by Brewer & Nash. Most theorems are established automatically using the tool {log} with the exception of a theorem regarding information flow, which combines a lemma in {log} with a theorem mechanised in Coq. Having covered all theorems originally posed by Brewer-Nash, achieving modern precision and mechanisation, we propose this work as a step towards a methodology for automated checking of more complex security policy models.","sentences":["This paper revisits the Brewer-Nash security policy model inspired by ethical Chinese Wall policies.","We draw attention to the fact that write access can be revoked in the Brewer-Nash model.","The semantics of write access were underspecified originally, leading to multiple interpretations for which we provide a modern operational semantics.","We go on to modernise the analysis of information flow in the Brewer-Nash model, by adopting a more precise definition adapted from Kessler.","For our modernised reformulation, we provide full mechanised coverage for all theorems proposed by Brewer & Nash.","Most theorems are established automatically using the tool {log} with the exception of a theorem regarding information flow, which combines a lemma in {log} with a theorem mechanised in Coq.","Having covered all theorems originally posed by Brewer-Nash, achieving modern precision and mechanisation, we propose this work as a step towards a methodology for automated checking of more complex security policy models."],"url":"http://arxiv.org/abs/2405.12187v1","category":"cs.CR"}
{"created":"2024-05-20 17:07:30","title":"Nearest Neighbors GParareal: Improving Scalability of Gaussian Processes for Parallel-in-Time Solvers","abstract":"With the advent of supercomputers, multi-processor environments and parallel-in-time (PinT) algorithms offer ways to solve initial value problems for ordinary and partial differential equations (ODEs and PDEs) over long time intervals, a task often unfeasible with sequential solvers within realistic time frames. A recent approach, GParareal, combines Gaussian Processes with traditional PinT methodology (Parareal) to achieve faster parallel speed-ups. The method is known to outperform Parareal for low-dimensional ODEs and a limited number of computer cores. Here, we present Nearest Neighbors GParareal (nnGParareal), a novel data-enriched PinT integration algorithm. nnGParareal builds upon GParareal by improving its scalability properties for higher-dimensional systems and increased processor count. Through data reduction, the model complexity is reduced from cubic to log-linear in the sample size, yielding a fast and automated procedure to integrate initial value problems over long time intervals. First, we provide both an upper bound for the error and theoretical details on the speed-up benefits. Then, we empirically illustrate the superior performance of nnGParareal, compared to GParareal and Parareal, on nine different systems with unique features (e.g., stiff, chaotic, high-dimensional, or challenging-to-learn systems).","sentences":["With the advent of supercomputers, multi-processor environments and parallel-in-time (PinT) algorithms offer ways to solve initial value problems for ordinary and partial differential equations (ODEs and PDEs) over long time intervals, a task often unfeasible with sequential solvers within realistic time frames.","A recent approach, GParareal, combines Gaussian Processes with traditional PinT methodology (Parareal) to achieve faster parallel speed-ups.","The method is known to outperform Parareal for low-dimensional ODEs and a limited number of computer cores.","Here, we present Nearest Neighbors GParareal (nnGParareal), a novel data-enriched PinT integration algorithm.","nnGParareal builds upon GParareal by improving its scalability properties for higher-dimensional systems and increased processor count.","Through data reduction, the model complexity is reduced from cubic to log-linear in the sample size, yielding a fast and automated procedure to integrate initial value problems over long time intervals.","First, we provide both an upper bound for the error and theoretical details on the speed-up benefits.","Then, we empirically illustrate the superior performance of nnGParareal, compared to GParareal and Parareal, on nine different systems with unique features (e.g., stiff, chaotic, high-dimensional, or challenging-to-learn systems)."],"url":"http://arxiv.org/abs/2405.12182v1","category":"stat.CO"}
{"created":"2024-05-20 16:58:24","title":"Enhancing Explainable AI: A Hybrid Approach Combining GradCAM and LRP for CNN Interpretability","abstract":"We present a new technique that explains the output of a CNN-based model using a combination of GradCAM and LRP methods. Both of these methods produce visual explanations by highlighting input regions that are important for predictions. In the new method, the explanation produced by GradCAM is first processed to remove noises. The processed output is then multiplied elementwise with the output of LRP. Finally, a Gaussian blur is applied on the product. We compared the proposed method with GradCAM and LRP on the metrics of Faithfulness, Robustness, Complexity, Localisation and Randomisation. It was observed that this method performs better on Complexity than both GradCAM and LRP and is better than atleast one of them in the other metrics.","sentences":["We present a new technique that explains the output of a CNN-based model using a combination of GradCAM and LRP methods.","Both of these methods produce visual explanations by highlighting input regions that are important for predictions.","In the new method, the explanation produced by GradCAM is first processed to remove noises.","The processed output is then multiplied elementwise with the output of LRP.","Finally, a Gaussian blur is applied on the product.","We compared the proposed method with GradCAM and LRP on the metrics of Faithfulness, Robustness, Complexity, Localisation and Randomisation.","It was observed that this method performs better on Complexity than both GradCAM and LRP and is better than atleast one of them in the other metrics."],"url":"http://arxiv.org/abs/2405.12175v1","category":"cs.CV"}
{"created":"2024-05-20 16:57:59","title":"Asymptotic stability of the three-dimensional Couette flow for the Stokes-transport equation","abstract":"In this paper, we investigate the asymptotic stability of the three-dimensional Couette flow in a stratified fluid governed by the Stokes-transport equation. We observe that a similar lift-up effect to the three-dimensional Navier-Stokes equation near Couette flow destabilizes the system. We find that the inviscid damping type decay due to the Couette flow together with the damping structure caused by the decreasing background density stabilizes the system. More precisely, we prove that if the initial density is close to a linearly decreasing function in the Gevrey-$\\frac{1}{s}$ class with $\\frac{1}{2}< s\\leq 1$, namely, $\\|\\varrho_{\\mathrm{in}}(X,Y,Z)-(-Y)\\|_{\\mathcal{G}^{s}}\\leq \\epsilon$, then the perturbed density remains close to $-Y$. Moreover, the associated velocity field converges to Couette flow $(Y, 0, 0)^{\\top}$ with a convergence rate of $\\frac{1}{\\langle t\\rangle^3}$.","sentences":["In this paper, we investigate the asymptotic stability of the three-dimensional Couette flow in a stratified fluid governed by the Stokes-transport equation.","We observe that a similar lift-up effect to the three-dimensional Navier-Stokes equation near Couette flow destabilizes the system.","We find that the inviscid damping type decay due to the Couette flow together with the damping structure caused by the decreasing background density stabilizes the system.","More precisely, we prove that if the initial density is close to a linearly decreasing function in the Gevrey-$\\frac{1}{s}$ class with $\\frac{1}{2}< s\\leq 1$, namely, $\\|\\varrho_{\\mathrm{in}}(X,Y,Z)-(-Y)\\|_{\\mathcal{G}^{s}}\\leq \\epsilon$, then the perturbed density remains close to $-Y$.","Moreover, the associated velocity field converges to Couette flow $(Y, 0, 0)^{\\top}$ with a convergence rate of $\\frac{1}{\\langle t\\rangle^3}$."],"url":"http://arxiv.org/abs/2405.12173v1","category":"math.AP"}
{"created":"2024-05-20 16:52:33","title":"WiDRa -- Enabling Millimeter-Level Differential Ranging Accuracy in Wi-Fi Using Carrier Phase","abstract":"Although Wi-Fi is an ideal technology for many ranging applications, the performance of current methods is limited by the system bandwidth, leading to low accuracy of $\\sim 1$ m. For many applications, measuring differential range, viz., the change in the range between adjacent measurements, is sufficient. Correspondingly, this work proposes WiDRa - a Wi-Fi based Differential Ranging solution that provides differential range estimates by using the sum-carrier-phase information. The proposed method is not limited by system bandwidth and can track range changes even smaller than the carrier wavelength. The proposed method is first theoretically justified, while taking into consideration the various hardware impairments affecting Wi-Fi chips. In the process, methods to isolate the sum-carrier phase from the hardware impairments are proposed. Extensive simulation results show that WiDRa can achieve a differential range estimation root-mean-square-error (RMSE) of $\\approx 1$ mm in channels with a Rician-factor $\\geq 7$ (a $100 \\times$ improvement to existing methods). The proposed methods are also validated on off-the-shelf Wi-Fi hardware to demonstrate feasibility, where they achieve an RMSE of $< 1$ mm in the differential range. Finally, limitations of current investigation and future directions of exploration are suggested, to further tap into the potential of WiDRa.","sentences":["Although Wi-Fi is an ideal technology for many ranging applications, the performance of current methods is limited by the system bandwidth, leading to low accuracy of $\\sim 1$ m. For many applications, measuring differential range, viz., the change in the range between adjacent measurements, is sufficient.","Correspondingly, this work proposes WiDRa - a Wi-Fi based Differential Ranging solution that provides differential range estimates by using the sum-carrier-phase information.","The proposed method is not limited by system bandwidth and can track range changes even smaller than the carrier wavelength.","The proposed method is first theoretically justified, while taking into consideration the various hardware impairments affecting Wi-Fi chips.","In the process, methods to isolate the sum-carrier phase from the hardware impairments are proposed.","Extensive simulation results show that WiDRa can achieve a differential range estimation root-mean-square-error (RMSE) of $\\approx 1$ mm in channels with a Rician-factor $\\geq 7$ (a $100 \\times$ improvement to existing methods).","The proposed methods are also validated on off-the-shelf Wi-Fi hardware to demonstrate feasibility, where they achieve an RMSE of $< 1$ mm in the differential range.","Finally, limitations of current investigation and future directions of exploration are suggested, to further tap into the potential of WiDRa."],"url":"http://arxiv.org/abs/2405.12168v1","category":"cs.IT"}
{"created":"2024-05-20 16:51:19","title":"Asymptotic Stability of the two-dimensional Couette flow for the Stokes-transport equation in a finite channel","abstract":"We study the Stokes-transport system in a two-dimensional channel with horizontally moving boundaries, which serves as a reduced model for oceanography and sedimentation. The density is transported by the velocity field, satisfying the momentum balance between viscosity, pressure, and gravity effects, described by the Stokes equation at any given time. Due to the presence of moving boundaries, stratified densities with the Couette flow constitute one class of steady states. In this paper, we investigate the asymptotic stability of these steady states. We prove that if the stratified density is close to a constant density and the perturbation belongs to the Gevrey-3 class with compact support away from the boundary, then the velocity will converge to the Couette flow as time approaches infinity. More precisely, we prove that the horizontal perturbed velocity decays as $\\frac{1}{\\langle t\\rangle^3}$ and the vertical perturbed velocity decays as $\\frac{1}{\\langle t\\rangle^4}$.","sentences":["We study the Stokes-transport system in a two-dimensional channel with horizontally moving boundaries, which serves as a reduced model for oceanography and sedimentation.","The density is transported by the velocity field, satisfying the momentum balance between viscosity, pressure, and gravity effects, described by the Stokes equation at any given time.","Due to the presence of moving boundaries, stratified densities with the Couette flow constitute one class of steady states.","In this paper, we investigate the asymptotic stability of these steady states.","We prove that if the stratified density is close to a constant density and the perturbation belongs to the Gevrey-3 class with compact support away from the boundary, then the velocity will converge to the Couette flow as time approaches infinity.","More precisely, we prove that the horizontal perturbed velocity decays as $\\frac{1}{\\langle t\\rangle^3}$ and the vertical perturbed velocity decays as $\\frac{1}{\\langle t\\rangle^4}$."],"url":"http://arxiv.org/abs/2405.12166v1","category":"math.AP"}
{"created":"2024-05-20 16:49:59","title":"Classifying multiply connected wandering domains","abstract":"We study the internal dynamics of multiply connected wandering domains of meromorphic functions. We do so by considering the sequence of injectivity radii along the orbit of a base point, together with the hyperbolic distortions along the same orbit. The latter sequence had been previously used in this context; the former introduces geometric information about the shape of the wandering domains that interacts with the dynamic information given by the hyperbolic distortions. Using this idea, we complete the description of the internal dynamics of any wandering domain of a meromorphic function, and also unify previous approaches to the question. We conclude that the internal dynamics of a wandering domain, from the point of view of hyperbolic geometry, can be classified into six different types. Five of these types are realised by wandering domains of entire functions, while the sixth can only arise for meromorphic functions: a locally but not globally eventually isometric wandering domain. We construct a meromorphic function with such a domain, demonstrating that this new phenomenon does in fact occur. Our results show that, on a local level, the dynamics of multiply connected wandering domains of meromorphic functions is similar to those of entire functions, although new global phenomena can arise for non-entire functions.","sentences":["We study the internal dynamics of multiply connected wandering domains of meromorphic functions.","We do so by considering the sequence of injectivity radii along the orbit of a base point, together with the hyperbolic distortions along the same orbit.","The latter sequence had been previously used in this context; the former introduces geometric information about the shape of the wandering domains that interacts with the dynamic information given by the hyperbolic distortions.","Using this idea, we complete the description of the internal dynamics of any wandering domain of a meromorphic function, and also unify previous approaches to the question.","We conclude that the internal dynamics of a wandering domain, from the point of view of hyperbolic geometry, can be classified into six different types.","Five of these types are realised by wandering domains of entire functions, while the sixth can only arise for meromorphic functions: a locally but not globally eventually isometric wandering domain.","We construct a meromorphic function with such a domain, demonstrating that this new phenomenon does in fact occur.","Our results show that, on a local level, the dynamics of multiply connected wandering domains of meromorphic functions is similar to those of entire functions, although new global phenomena can arise for non-entire functions."],"url":"http://arxiv.org/abs/2405.12165v1","category":"math.DS"}
{"created":"2024-05-20 16:48:20","title":"Feedback-regulated Seed Black Hole Growth in Star-Forming Molecular Clouds and Galactic Nuclei","abstract":"The detection of supermassive black holes (SMBHs) in high-redshift luminous quasars may require a phase of rapid accretion, and as a precondition, substantial gas influx toward seed black holes (BHs) from kilo-parsec or parsec scales. Our previous research demonstrated the plausibility of such gas supply for BH seeds within star-forming giant molecular clouds (GMCs) with high surface density ($\\sim 10^4\\,{\\rm {\\rm M_\\odot}\\, pc}^{-2}$), facilitating ``hyper-Eddington'' accretion via efficient feeding by dense clumps which are driven by turbulence and stellar feedback. This article investigates the impacts of feedback from accreting BHs on this process, including radiation, mechanical jets, and highly relativistic cosmic rays. We run a suite of numerical simulations to explore diverse parameter spaces of BH feedback, including the sub-grid accretion model, feedback energy efficiency, mass loading factor, and initial metallicity. Utilizing radiative feedback models inferred from the slim disk, we find that hyper-Eddington accretion is still achievable, yielding BH bolometric luminosities as high as $10^{41}$ -- $10^{44}\\,\\rm erg/s$, depending on the GMC properties and specific feedback model assumed. We find the maximum possible mass growth of seed BHs ($\\Delta M_{\\rm BH}^{\\rm max}$) is regulated by the momentum deposition rate from BH feedback, $\\dot{p}_{\\rm feedback}/(\\dot{M}_{\\rm BH} c)$, which leads to an analytic scaling that agrees well with simulations. This scenario predicts the rapid formation of $\\sim 10^4\\,\\rm M_\\odot$ intermediate-massive BHs (IMBHs) from stellar-mass BHs within $\\sim \\rm Myr$. Furthermore, we examine the impacts of sub-grid accretion models and how BH feedback may influence star formation within these cloud complexes.","sentences":["The detection of supermassive black holes (SMBHs) in high-redshift luminous quasars may require a phase of rapid accretion, and as a precondition, substantial gas influx toward seed black holes (BHs) from kilo-parsec or parsec scales.","Our previous research demonstrated the plausibility of such gas supply for BH seeds within star-forming giant molecular clouds (GMCs) with high surface density ($\\sim 10^4\\,{\\rm {\\rm M_\\odot}\\, pc}^{-2}$), facilitating ``hyper-Eddington'' accretion via efficient feeding by dense clumps which are driven by turbulence and stellar feedback.","This article investigates the impacts of feedback from accreting BHs on this process, including radiation, mechanical jets, and highly relativistic cosmic rays.","We run a suite of numerical simulations to explore diverse parameter spaces of BH feedback, including the sub-grid accretion model, feedback energy efficiency, mass loading factor, and initial metallicity.","Utilizing radiative feedback models inferred from the slim disk, we find that hyper-Eddington accretion is still achievable, yielding BH bolometric luminosities as high as $10^{41}$ -- $10^{44}\\,\\rm erg/s$, depending on the GMC properties and specific feedback model assumed.","We find the maximum possible mass growth of seed BHs ($\\Delta M_{\\rm BH}^{\\rm max}$) is regulated by the momentum deposition rate from BH feedback, $\\dot{p}_{\\rm feedback}/(\\dot{M}_{\\rm BH} c)$, which leads to an analytic scaling that agrees well with simulations.","This scenario predicts the rapid formation of $\\sim 10^4\\,\\rm M_\\odot$ intermediate-massive BHs (IMBHs) from stellar-mass BHs within $\\sim \\rm Myr$.","Furthermore, we examine the impacts of sub-grid accretion models and how BH feedback may influence star formation within these cloud complexes."],"url":"http://arxiv.org/abs/2405.12164v1","category":"astro-ph.GA"}
{"created":"2024-05-20 16:31:01","title":"Reconstruction of unknown nonlinear operators in semilinear elliptic models using optimal inputs","abstract":"Physical models often contain unknown functions and relations. The goal of our work is to answer the question of how one should excite or control a system under consideration in an appropriate way to be able to reconstruct an unknown nonlinear relation. To answer this question, we propose a greedy reconstruction algorithm within an offline-online strategy. We apply this strategy to a two-dimensional semilinear elliptic model. Our identification is based on the application of several space-dependent excitations (also called controls). These specific controls are designed by the algorithm in order to obtain a deeper insight into the underlying physical problem and a more precise reconstruction of the unknown relation. We perform numerical simulations that demonstrate the effectiveness of our approach which is not limited to the current type of equation. Since our algorithm provides not only a way to determine unknown operators by existing data but also protocols for new experiments, it is a holistic concept to tackle the problem of improving physical models.","sentences":["Physical models often contain unknown functions and relations.","The goal of our work is to answer the question of how one should excite or control a system under consideration in an appropriate way to be able to reconstruct an unknown nonlinear relation.","To answer this question, we propose a greedy reconstruction algorithm within an offline-online strategy.","We apply this strategy to a two-dimensional semilinear elliptic model.","Our identification is based on the application of several space-dependent excitations (also called controls).","These specific controls are designed by the algorithm in order to obtain a deeper insight into the underlying physical problem and a more precise reconstruction of the unknown relation.","We perform numerical simulations that demonstrate the effectiveness of our approach which is not limited to the current type of equation.","Since our algorithm provides not only a way to determine unknown operators by existing data but also protocols for new experiments, it is a holistic concept to tackle the problem of improving physical models."],"url":"http://arxiv.org/abs/2405.12153v1","category":"math.OC"}
{"created":"2024-05-20 16:09:45","title":"Machine learning for predicting ultralow thermal conductivity and high ZT in complex thermoelectric materials","abstract":"Efficient and precise calculations of thermal transport properties and figure of merit, alongside a deep comprehension of thermal transport mechanisms, are essential for the practical utilization of advanced thermoelectric materials. In this study, we explore the microscopic processes governing thermal transport in the distinguished crystalline material Tl$_9$SbTe$_6$ by integrating a unified thermal transport theory with machine learning-assisted self-consistent phonon calculations. Leveraging machine learning potentials, we expedite the analysis of phonon energy shifts, higher-order scattering mechanisms, and thermal conductivity arising from various contributing factors like population and coherence channels. Our finding unveils an exceptionally low thermal conductivity of 0.31 W m$^{-1}$ K$^{-1}$ at room temperature, a result that closely correlates with experimental observations. Notably, we observe that the off-diagonal terms of heat flux operators play a significant role in shaping the overall lattice thermal conductivity of Tl$_9$SbTe$_6$, where the ultralow thermal conductivity resembles that of glass due to limited group velocities. Furthermore, we achieve a maximum $ZT$ value of 3.17 in the $c$-axis orientation for \\textit{p}-type Tl$_9$SbTe$_6$ at 600 K, and an optimal $ZT$ value of 2.26 in the $a$-axis and $b$-axis direction for \\textit{n}-type Tl$_9$SbTe$_6$ at 500 K. The crystalline Tl$_9$SbTe$_6$ not only showcases remarkable thermal insulation but also demonstrates impressive electrical properties owing to the dual-degeneracy phenomenon within its valence band. These results not only elucidate the underlying reasons for the exceptional thermoelectric performance of Tl$_9$SbTe$_6$ but also suggest potential avenues for further experimental exploration.","sentences":["Efficient and precise calculations of thermal transport properties and figure of merit, alongside a deep comprehension of thermal transport mechanisms, are essential for the practical utilization of advanced thermoelectric materials.","In this study, we explore the microscopic processes governing thermal transport in the distinguished crystalline material Tl$_9$SbTe$_6$ by integrating a unified thermal transport theory with machine learning-assisted self-consistent phonon calculations.","Leveraging machine learning potentials, we expedite the analysis of phonon energy shifts, higher-order scattering mechanisms, and thermal conductivity arising from various contributing factors like population and coherence channels.","Our finding unveils an exceptionally low thermal conductivity of 0.31 W m$^{-1}$ K$^{-1}$ at room temperature, a result that closely correlates with experimental observations.","Notably, we observe that the off-diagonal terms of heat flux operators play a significant role in shaping the overall lattice thermal conductivity of Tl$_9$SbTe$_6$, where the ultralow thermal conductivity resembles that of glass due to limited group velocities.","Furthermore, we achieve a maximum $ZT$ value of 3.17 in the $c$-axis orientation for \\textit{p}-type Tl$_9$SbTe$_6$ at 600 K, and an optimal $ZT$ value of 2.26 in the $a$-axis and $b$-axis direction for \\textit{n}-type Tl$_9$SbTe$_6$ at 500 K. The crystalline Tl$_9$SbTe$_6$ not only showcases remarkable thermal insulation but also demonstrates impressive electrical properties owing to the dual-degeneracy phenomenon within its valence band.","These results not only elucidate the underlying reasons for the exceptional thermoelectric performance of Tl$_9$SbTe$_6$ but also suggest potential avenues for further experimental exploration."],"url":"http://arxiv.org/abs/2405.12143v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 15:55:44","title":"Spectroscopy of Eclipsing Compact Hierarchical Triples I","abstract":"Eclipsing Compact Hierarchical Triples (ECHTs) are systems with the tertiary star orbiting an eclipsing binary (EB) in an orbit of fewer than 1000 days. In a CHT, all three stars exist in a space less than 5 AU in separation. A low-mass CHT is an interesting case to understand multiple star and planet formation at such small scales. In this study, we combine spectroscopy and photometry to estimate the orbital, stellar and atmospheric parameters of stars in a sample of CHTs. Using the complete set of parameters we aim to constrain the metallicity and age of the systems. We use time-series spectroscopy to obtain radial velocities (RVs) and disentangled spectra. Using RV modelling, EB light curve modelling, and spectral analysis, we estimated the metallicities and temperatures. Using isochrone fitting, we constrain the ages of the system. We then combine observations of masses, outer eccentricities (e_2), orbital periods and age estimates of the systems from the literature. We compare the distributions of e_2, and tertiary mass ratio, q_3 = M_3/(M_1+M_2), for three different metallicity ranges and two ranges of age. We estimate masses, radii, temperatures, metallicities and age of 12 stars in 4 CHTs. The CHT CD-32 6459 shows signs of von Zeipel-Lidov-Kozai oscillations while CD-62 1257 can evolve to form a triple common envelope. The rest of the CHTs are old and have an M-dwarf tertiary. We find that the q_3 distribution for CHTs with sub-solar metallicity has a uniform distribution but the systems with solar and above-solar metallicity peak between 0.5 and 1. When dividing them according to their ages, we found the q_3 of old systems around 0.5. The eccentricity e_2 favours a value around 0.3 irrespective of metallicity or age. The distributions are biased by the lack of observations and observing methods and therefore call for more observations of low-mass CHT.","sentences":["Eclipsing Compact Hierarchical Triples (ECHTs) are systems with the tertiary star orbiting an eclipsing binary (EB) in an orbit of fewer than 1000 days.","In a CHT, all three stars exist in a space less than 5 AU in separation.","A low-mass CHT is an interesting case to understand multiple star and planet formation at such small scales.","In this study, we combine spectroscopy and photometry to estimate the orbital, stellar and atmospheric parameters of stars in a sample of CHTs.","Using the complete set of parameters we aim to constrain the metallicity and age of the systems.","We use time-series spectroscopy to obtain radial velocities (RVs) and disentangled spectra.","Using RV modelling, EB light curve modelling, and spectral analysis, we estimated the metallicities and temperatures.","Using isochrone fitting, we constrain the ages of the system.","We then combine observations of masses, outer eccentricities (e_2), orbital periods and age estimates of the systems from the literature.","We compare the distributions of e_2, and tertiary mass ratio, q_3 = M_3/(M_1+M_2), for three different metallicity ranges and two ranges of age.","We estimate masses, radii, temperatures, metallicities and age of 12 stars in 4 CHTs.","The CHT CD-32 6459 shows signs of von Zeipel-Lidov-Kozai oscillations while CD-62 1257 can evolve to form a triple common envelope.","The rest of the CHTs are old and have an M-dwarf tertiary.","We find that the q_3 distribution for CHTs with sub-solar metallicity has a uniform distribution but the systems with solar and above-solar metallicity peak between 0.5 and 1.","When dividing them according to their ages, we found the q_3 of old systems around 0.5.","The eccentricity e_2 favours a value around 0.3 irrespective of metallicity or age.","The distributions are biased by the lack of observations and observing methods and therefore call for more observations of low-mass CHT."],"url":"http://arxiv.org/abs/2405.12136v1","category":"astro-ph.SR"}
{"created":"2024-05-20 15:53:50","title":"Two-dimensional signal-dependent parabolic-elliptic Keller-Segel system and its means field derivation","abstract":"In this paper, the well-posedness of two-dimensional signal-dependent Keller-Segel system and its mean field derivation from a interacting particle system on the whole space are investigated. The signal dependence effect is reflected by the fact that the diffusion coefficient in the particle system depends nonlinearly on the interactions between the individuals. Therefore, the mathematical challenge in studying the well-posedness of this system lies in the possible degeneracy and the aggregation effect when the concentration of signal becomes unbounded. The well-established method on bounded domain, to obtain the appropriate estimates for the signal concentration, is invalid for the whole space case. Motivated by the entropy minimization method and Onofri's inequality, which has been successfully applied for parabolic-parabolic Keller-Segel system, we establish a complete entropy estimate benefited from linear diffusion term, which plays important role in obtaining the Lp estimates for the solution. Furthermore, the upper bound for the concentration of signal is obtained. Based on estimates we obtained for the density of cells, the rigorous mean-field derivation is proved by introducing an intermediate particle system with a mollified interaction potential with logarithmic scaling. By using this mollification, we obtain the convergence of the particle trajectories in expectation, which implies the weak propagation of chaos. Additionally, under a regularity assumption of the cell-density, we derive the strong L1 convergence for the propagation of chaos by using relative entropy method.","sentences":["In this paper, the well-posedness of two-dimensional signal-dependent Keller-Segel system and its mean field derivation from a interacting particle system on the whole space are investigated.","The signal dependence effect is reflected by the fact that the diffusion coefficient in the particle system depends nonlinearly on the interactions between the individuals.","Therefore, the mathematical challenge in studying the well-posedness of this system lies in the possible degeneracy and the aggregation effect when the concentration of signal becomes unbounded.","The well-established method on bounded domain, to obtain the appropriate estimates for the signal concentration, is invalid for the whole space case.","Motivated by the entropy minimization method and Onofri's inequality, which has been successfully applied for parabolic-parabolic Keller-Segel system, we establish a complete entropy estimate benefited from linear diffusion term, which plays important role in obtaining the Lp estimates for the solution.","Furthermore, the upper bound for the concentration of signal is obtained.","Based on estimates we obtained for the density of cells, the rigorous mean-field derivation is proved by introducing an intermediate particle system with a mollified interaction potential with logarithmic scaling.","By using this mollification, we obtain the convergence of the particle trajectories in expectation, which implies the weak propagation of chaos.","Additionally, under a regularity assumption of the cell-density, we derive the strong L1 convergence for the propagation of chaos by using relative entropy method."],"url":"http://arxiv.org/abs/2405.12134v1","category":"math.AP"}
{"created":"2024-05-20 15:44:07","title":"Alzheimer's Magnetic Resonance Imaging Classification Using Deep and Meta-Learning Models","abstract":"Deep learning, a cutting-edge machine learning approach, outperforms traditional machine learning in identifying intricate structures in complex high-dimensional data, particularly in the domain of healthcare. This study focuses on classifying Magnetic Resonance Imaging (MRI) data for Alzheimer's disease (AD) by leveraging deep learning techniques characterized by state-of-the-art CNNs. Brain imaging techniques such as MRI have enabled the measurement of pathophysiological brain changes related to Alzheimer's disease. Alzheimer's disease is the leading cause of dementia in the elderly, and it is an irreversible brain illness that causes gradual cognitive function disorder. In this paper, we train some benchmark deep models individually for the approach of the solution and later use an ensembling approach to combine the effect of multiple CNNs towards the observation of higher recall and accuracy. Here, the model's effectiveness is evaluated using various methods, including stacking, majority voting, and the combination of models with high recall values. The majority voting performs better than the alternative modelling approach as the majority voting approach typically reduces the variance in the predictions. We report a test accuracy of 90% with a precision score of 0.90 and a recall score of 0.89 in our proposed approach. In future, this study can be extended to incorporate other types of medical data, including signals, images, and other data. The same or alternative datasets can be used with additional classifiers, neural networks, and AI techniques to enhance Alzheimer's detection.","sentences":["Deep learning, a cutting-edge machine learning approach, outperforms traditional machine learning in identifying intricate structures in complex high-dimensional data, particularly in the domain of healthcare.","This study focuses on classifying Magnetic Resonance Imaging (MRI) data for Alzheimer's disease (AD) by leveraging deep learning techniques characterized by state-of-the-art CNNs.","Brain imaging techniques such as MRI have enabled the measurement of pathophysiological brain changes related to Alzheimer's disease.","Alzheimer's disease is the leading cause of dementia in the elderly, and it is an irreversible brain illness that causes gradual cognitive function disorder.","In this paper, we train some benchmark deep models individually for the approach of the solution and later use an ensembling approach to combine the effect of multiple CNNs towards the observation of higher recall and accuracy.","Here, the model's effectiveness is evaluated using various methods, including stacking, majority voting, and the combination of models with high recall values.","The majority voting performs better than the alternative modelling approach as the majority voting approach typically reduces the variance in the predictions.","We report a test accuracy of 90% with a precision score of 0.90 and a recall score of 0.89 in our proposed approach.","In future, this study can be extended to incorporate other types of medical data, including signals, images, and other data.","The same or alternative datasets can be used with additional classifiers, neural networks, and AI techniques to enhance Alzheimer's detection."],"url":"http://arxiv.org/abs/2405.12126v1","category":"cs.CV"}
{"created":"2024-05-20 15:40:49","title":"Minimal projections onto spaces of polynomials on real euclidean spheres","abstract":"We investigate projection constants within classes of multivariate polynomials over finite-dimensional real Hilbert spaces. Specifically, we consider the projection constant for spaces of spherical harmonics and spaces of homogeneous polynomials as well as for spaces of polynomials of finite degree on the unit sphere. We establish a connection between these quantities and certain weighted $L_1$-norms of specific Jacobi polynomials. As a consequence, we present exact formulas, computable expressions and asymptotically accurate estimates for them. The real case we address is considerably more nuanced than its complex counterpart.","sentences":["We investigate projection constants within classes of multivariate polynomials over finite-dimensional real Hilbert spaces.","Specifically, we consider the projection constant for spaces of spherical harmonics and spaces of homogeneous polynomials as well as for spaces of polynomials of finite degree on the unit sphere.","We establish a connection between these quantities and certain weighted $L_1$-norms of specific Jacobi polynomials.","As a consequence, we present exact formulas, computable expressions and asymptotically accurate estimates for them.","The real case we address is considerably more nuanced than its complex counterpart."],"url":"http://arxiv.org/abs/2405.12123v1","category":"math.FA"}
{"created":"2024-05-20 15:39:40","title":"An Active Learning Framework with a Class Balancing Strategy for Time Series Classification","abstract":"Training machine learning models for classification tasks often requires labeling numerous samples, which is costly and time-consuming, especially in time series analysis. This research investigates Active Learning (AL) strategies to reduce the amount of labeled data needed for effective time series classification. Traditional AL techniques cannot control the selection of instances per class for labeling, leading to potential bias in classification performance and instance selection, particularly in imbalanced time series datasets. To address this, we propose a novel class-balancing instance selection algorithm integrated with standard AL strategies. Our approach aims to select more instances from classes with fewer labeled examples, thereby addressing imbalance in time series datasets. We demonstrate the effectiveness of our AL framework in selecting informative data samples for two distinct domains of tactile texture recognition and industrial fault detection. In robotics, our method achieves high-performance texture categorization while significantly reducing labeled training data requirements to 70%. We also evaluate the impact of different sliding window time intervals on robotic texture classification using AL strategies. In synthetic fiber manufacturing, we adapt AL techniques to address the challenge of fault classification, aiming to minimize data annotation cost and time for industries. We also address real-life class imbalances in the multiclass industrial anomalous dataset using our class-balancing instance algorithm integrated with AL strategies. Overall, this thesis highlights the potential of our AL framework across these two distinct domains.","sentences":["Training machine learning models for classification tasks often requires labeling numerous samples, which is costly and time-consuming, especially in time series analysis.","This research investigates Active Learning (AL) strategies to reduce the amount of labeled data needed for effective time series classification.","Traditional AL techniques cannot control the selection of instances per class for labeling, leading to potential bias in classification performance and instance selection, particularly in imbalanced time series datasets.","To address this, we propose a novel class-balancing instance selection algorithm integrated with standard AL strategies.","Our approach aims to select more instances from classes with fewer labeled examples, thereby addressing imbalance in time series datasets.","We demonstrate the effectiveness of our AL framework in selecting informative data samples for two distinct domains of tactile texture recognition and industrial fault detection.","In robotics, our method achieves high-performance texture categorization while significantly reducing labeled training data requirements to 70%.","We also evaluate the impact of different sliding window time intervals on robotic texture classification using AL strategies.","In synthetic fiber manufacturing, we adapt AL techniques to address the challenge of fault classification, aiming to minimize data annotation cost and time for industries.","We also address real-life class imbalances in the multiclass industrial anomalous dataset using our class-balancing instance algorithm integrated with AL strategies.","Overall, this thesis highlights the potential of our AL framework across these two distinct domains."],"url":"http://arxiv.org/abs/2405.12122v1","category":"cs.LG"}
{"created":"2024-05-20 15:38:53","title":"EdgeLoc: A Communication-Adaptive Parallel System for Real-Time Localization in Infrastructure-Assisted Autonomous Driving","abstract":"This paper presents EdgeLoc, an infrastructure-assisted, real-time localization system for autonomous driving that addresses the incompatibility between traditional localization methods and deep learning approaches. The system is built on top of the Robot Operating System (ROS) and combines the real-time performance of traditional methods with the high accuracy of deep learning approaches. The system leverages edge computing capabilities of roadside units (RSUs) for precise localization to enhance on-vehicle localization that is based on the real-time visual odometry. EdgeLoc is a parallel processing system, utilizing a proposed uncertainty-aware pose fusion solution. It achieves communication adaptivity through online learning and addresses fluctuations via window-based detection. Moreover, it achieves optimal latency and maximum improvement by utilizing auto-splitting vehicle-infrastructure collaborative inference, as well as online distribution learning for decision-making. Even with the most basic end-to-end deep neural network for localization estimation, EdgeLoc realizes a 67.75\\% reduction in the localization error for real-time local visual odometry, a 29.95\\% reduction for non-real-time collaborative inference, and a 30.26\\% reduction compared to Kalman filtering. Finally, accuracy-to-latency conversion was experimentally validated, and an overall experiment was conducted on a practical cellular network. The system is open sourced at https://github.com/LoganCome/EdgeAssistedLocalization.","sentences":["This paper presents EdgeLoc, an infrastructure-assisted, real-time localization system for autonomous driving that addresses the incompatibility between traditional localization methods and deep learning approaches.","The system is built on top of the Robot Operating System (ROS) and combines the real-time performance of traditional methods with the high accuracy of deep learning approaches.","The system leverages edge computing capabilities of roadside units (RSUs) for precise localization to enhance on-vehicle localization that is based on the real-time visual odometry.","EdgeLoc is a parallel processing system, utilizing a proposed uncertainty-aware pose fusion solution.","It achieves communication adaptivity through online learning and addresses fluctuations via window-based detection.","Moreover, it achieves optimal latency and maximum improvement by utilizing auto-splitting vehicle-infrastructure collaborative inference, as well as online distribution learning for decision-making.","Even with the most basic end-to-end deep neural network for localization estimation, EdgeLoc realizes a 67.75\\% reduction in the localization error for real-time local visual odometry, a 29.95\\% reduction for non-real-time collaborative inference, and a 30.26\\% reduction compared to Kalman filtering.","Finally, accuracy-to-latency conversion was experimentally validated, and an overall experiment was conducted on a practical cellular network.","The system is open sourced at https://github.com/LoganCome/EdgeAssistedLocalization."],"url":"http://arxiv.org/abs/2405.12120v1","category":"cs.DC"}
{"created":"2024-05-20 15:32:47","title":"Strongly-Consistent Distributed Discrete-event Systems","abstract":"Discrete-event (DE) systems are concurrent programs where components communicate via tagged events, where tags are drawn from a totally ordered set. Reactors are an emerging model of computation based on DE and realized in the open-source coordination language Lingua Franca. Distributed DE (DDE) systems are DE systems where the components (reactors) communicate over networks. The prior art has required that for DDE systems with cycles, each cycle must contain at least one logical delay, where the tag of events is incremented. Such delays, however, are not required by the elegant fixed-point semantics of DE. The only requirement is that the program be constructive, meaning it is free of causality cycles. This paper gives a way to coordinate the execution of DDE systems that can execute any constructive program, even one with zero-delay cycles. It provides a formal model that exposes exactly the information that must be shared across networks for such execution to be possible. Furthermore, it describes a concrete implementation that is an extension of the coordination mechanisms in Lingua Franca.","sentences":["Discrete-event (DE) systems are concurrent programs where components communicate via tagged events, where tags are drawn from a totally ordered set.","Reactors are an emerging model of computation based on DE and realized in the open-source coordination language Lingua Franca.","Distributed DE (DDE) systems are DE systems where the components (reactors) communicate over networks.","The prior art has required that for DDE systems with cycles, each cycle must contain at least one logical delay, where the tag of events is incremented.","Such delays, however, are not required by the elegant fixed-point semantics of DE.","The only requirement is that the program be constructive, meaning it is free of causality cycles.","This paper gives a way to coordinate the execution of DDE systems that can execute any constructive program, even one with zero-delay cycles.","It provides a formal model that exposes exactly the information that must be shared across networks for such execution to be possible.","Furthermore, it describes a concrete implementation that is an extension of the coordination mechanisms in Lingua Franca."],"url":"http://arxiv.org/abs/2405.12117v1","category":"cs.DC"}
{"created":"2024-05-20 15:29:26","title":"A New Cross-Space Total Variation Regularization Model for Color Image Restoration with Quaternion Blur Operator","abstract":"The cross-channel deblurring problem in color image processing is difficult to solve due to the complex coupling and structural blurring of color pixels. Until now, there are few efficient algorithms that can reduce color infection in deblurring process. To solve this challenging problem, we present a novel cross-space total variation (CSTV) regularization model for color image deblurring by introducing a quaternion blur operator and a cross-color space regularization functional. The existence and uniqueness of the solution is proved and a new L-curve method is proposed to find a sweet balance of regularization functionals on different color spaces.   The Euler-Lagrange equation is derived to show that CSTV has taken into account the coupling of all color channels and the local smoothing within each color channel. A quaternion operator splitting method is firstly proposed to enhance the ability of color infection reduction of the CSTV regularization model. This strategy also applies to the well-known color deblurring models. Numerical experiments on color image databases illustrate the efficiency and manoeuvrability of the new model and algorithms. The color images restored by them successfully maintain the color and spatial information and are of higher quality in terms of PSNR, SSIM, MSE and CIEde2000 than the restorations of the-state-of-the-art methods.","sentences":["The cross-channel deblurring problem in color image processing is difficult to solve due to the complex coupling and structural blurring of color pixels.","Until now, there are few efficient algorithms that can reduce color infection in deblurring process.","To solve this challenging problem, we present a novel cross-space total variation (CSTV) regularization model for color image deblurring by introducing a quaternion blur operator and a cross-color space regularization functional.","The existence and uniqueness of the solution is proved and a new L-curve method is proposed to find a sweet balance of regularization functionals on different color spaces.   ","The Euler-Lagrange equation is derived to show that CSTV has taken into account the coupling of all color channels and the local smoothing within each color channel.","A quaternion operator splitting method is firstly proposed to enhance the ability of color infection reduction of the CSTV regularization model.","This strategy also applies to the well-known color deblurring models.","Numerical experiments on color image databases illustrate the efficiency and manoeuvrability of the new model and algorithms.","The color images restored by them successfully maintain the color and spatial information and are of higher quality in terms of PSNR, SSIM, MSE and CIEde2000 than the restorations of the-state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.12114v1","category":"cs.CV"}
{"created":"2024-05-20 15:21:55","title":"On Mirzakhani's twist torus conjecture","abstract":"We address a conjecture of Mirzakhani about the statistical behavior of certain expanding families of ``twist tori'' in the moduli space of hyperbolic surfaces, showing that they equidistribute to a certain Lebesgue-class measure along almost all sequences. We also identify a number of other expanding families of twist tori whose limiting distributions are mutually singular to Lebesgue.","sentences":["We address a conjecture of Mirzakhani about the statistical behavior of certain expanding families of ``twist tori'' in the moduli space of hyperbolic surfaces, showing that they equidistribute to a certain Lebesgue-class measure along almost all sequences.","We also identify a number of other expanding families of twist tori whose limiting distributions are mutually singular to Lebesgue."],"url":"http://arxiv.org/abs/2405.12106v1","category":"math.GT"}
{"created":"2024-05-20 15:21:12","title":"Deciding branching hyperproperties for real time systems","abstract":"Security properties of real-time systems often involve reasoning about hyper-properties, as opposed to properties of single executions or trees of executions. These hyper-properties need to additionally be expressive enough to reason about real-time constraints. Examples of such properties include information flow, side channel attacks and service-level agreements. In this paper we study computational problems related to a branching-time, hyper-property extension of metric temporal logic (MTL) that we call HCMTL*. We consider both the interval-based and point-based semantics of this logic. The verification problem that we consider is to determine if a given HCMTL* formula $\\varphi$ is true in a system represented by a timed automaton. We show that this problem is undecidable. We then show that the verification problem is decidable if we consider executions upto a fixed time horizon $T$. Our decidability result relies on reducing the verification problem to the truth of an MSO formula over reals with a bounded time interval.","sentences":["Security properties of real-time systems often involve reasoning about hyper-properties, as opposed to properties of single executions or trees of executions.","These hyper-properties need to additionally be expressive enough to reason about real-time constraints.","Examples of such properties include information flow, side channel attacks and service-level agreements.","In this paper we study computational problems related to a branching-time, hyper-property extension of metric temporal logic (MTL) that we call HCMTL*.","We consider both the interval-based and point-based semantics of this logic.","The verification problem that we consider is to determine if a given HCMTL* formula $\\varphi$ is true in a system represented by a timed automaton.","We show that this problem is undecidable.","We then show that the verification problem is decidable if we consider executions upto a fixed time horizon $T$. Our decidability result relies on reducing the verification problem to the truth of an MSO formula over reals with a bounded time interval."],"url":"http://arxiv.org/abs/2405.12104v1","category":"cs.CR"}
{"created":"2024-05-20 15:06:36","title":"PATE: Proximity-Aware Time series anomaly Evaluation","abstract":"Evaluating anomaly detection algorithms in time series data is critical as inaccuracies can lead to flawed decision-making in various domains where real-time analytics and data-driven strategies are essential. Traditional performance metrics assume iid data and fail to capture the complex temporal dynamics and specific characteristics of time series anomalies, such as early and delayed detections. We introduce Proximity-Aware Time series anomaly Evaluation (PATE), a novel evaluation metric that incorporates the temporal relationship between prediction and anomaly intervals. PATE uses proximity-based weighting considering buffer zones around anomaly intervals, enabling a more detailed and informed assessment of a detection. Using these weights, PATE computes a weighted version of the area under the Precision and Recall curve. Our experiments with synthetic and real-world datasets show the superiority of PATE in providing more sensible and accurate evaluations than other evaluation metrics. We also tested several state-of-the-art anomaly detectors across various benchmark datasets using the PATE evaluation scheme. The results show that a common metric like Point-Adjusted F1 Score fails to characterize the detection performances well, and that PATE is able to provide a more fair model comparison. By introducing PATE, we redefine the understanding of model efficacy that steers future studies toward developing more effective and accurate detection models.","sentences":["Evaluating anomaly detection algorithms in time series data is critical as inaccuracies can lead to flawed decision-making in various domains where real-time analytics and data-driven strategies are essential.","Traditional performance metrics assume iid data and fail to capture the complex temporal dynamics and specific characteristics of time series anomalies, such as early and delayed detections.","We introduce Proximity-Aware Time series anomaly Evaluation (PATE), a novel evaluation metric that incorporates the temporal relationship between prediction and anomaly intervals.","PATE uses proximity-based weighting considering buffer zones around anomaly intervals, enabling a more detailed and informed assessment of a detection.","Using these weights, PATE computes a weighted version of the area under the Precision and Recall curve.","Our experiments with synthetic and real-world datasets show the superiority of PATE in providing more sensible and accurate evaluations than other evaluation metrics.","We also tested several state-of-the-art anomaly detectors across various benchmark datasets using the PATE evaluation scheme.","The results show that a common metric like Point-Adjusted F1 Score fails to characterize the detection performances well, and that PATE is able to provide a more fair model comparison.","By introducing PATE, we redefine the understanding of model efficacy that steers future studies toward developing more effective and accurate detection models."],"url":"http://arxiv.org/abs/2405.12096v1","category":"cs.LG"}
{"created":"2024-05-20 15:05:01","title":"Quantum Dissipation at Conical Intersections of Quasienergies","abstract":"We introduce the notion of conical intersections to Floquet theory and work out consequences of the underlying spatio-temporal symmetries for a driven two-level system coupled to an ohmic heat bath. We find that on manifolds with constant quasienergy splitting, the mean energies of the Floquet states vary significantly. The stationary populations behave in the same way, which emphasizes the importance of the mean energies for dissipation. Owing to symmetries, the stationary state may be fully mixed even at zero temperature. In the limit of large driving frequency, such full mixture is found in the whole vicinity of the intersection.","sentences":["We introduce the notion of conical intersections to Floquet theory and work out consequences of the underlying spatio-temporal symmetries for a driven two-level system coupled to an ohmic heat bath.","We find that on manifolds with constant quasienergy splitting, the mean energies of the Floquet states vary significantly.","The stationary populations behave in the same way, which emphasizes the importance of the mean energies for dissipation.","Owing to symmetries, the stationary state may be fully mixed even at zero temperature.","In the limit of large driving frequency, such full mixture is found in the whole vicinity of the intersection."],"url":"http://arxiv.org/abs/2405.12093v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 15:03:06","title":"Weak lensing of strong lensing: beyond the tidal regime","abstract":"The analysis of strong lensing images usually involves an external convergence and shear, which are meant to model the effect of perturbations along the line of sight, on top of the main lens. Such a description of line-of-sight perturbations supposes that the corresponding gravitational fields can be treated in the tidal regime. Going one step further introduces additional effects, known as flexion, which have been hitherto neglected in strong lensing. In this work, we build a minimal model for the line-of-sight flexion, which adds four new complex parameters to the lens model. Contrary to convergence and shear, the line-of-sight flexion cannot be projected onto the main lens plane. For a $\\Lambda$CDM cosmology, we predict the typical line-of-sight flexion to be on the order of $10^{-3} \\mathrm{arcsec}^{-1}$ on galactic scales. Neglecting its effect in lens modelling is found to bias the recovery of other parameters; in particular, the line-of-sight shear can be biased up to $2\\sigma$. Accounting for the line-of-sight flexion in our minimal framework restores accuracy, at the the cost of degrading precision. With current imaging capabilities, the line-of-sight flexion is unlikely to be measurable on individual strong lensing images; it must therefore be considered a nuisance parameter rather than an observable in its own right.","sentences":["The analysis of strong lensing images usually involves an external convergence and shear, which are meant to model the effect of perturbations along the line of sight, on top of the main lens.","Such a description of line-of-sight perturbations supposes that the corresponding gravitational fields can be treated in the tidal regime.","Going one step further introduces additional effects, known as flexion, which have been hitherto neglected in strong lensing.","In this work, we build a minimal model for the line-of-sight flexion, which adds four new complex parameters to the lens model.","Contrary to convergence and shear, the line-of-sight flexion cannot be projected onto the main lens plane.","For a $\\Lambda$CDM cosmology, we predict the typical line-of-sight flexion to be on the order of $10^{-3} \\mathrm{arcsec}^{-1}$ on galactic scales.","Neglecting its effect in lens modelling is found to bias the recovery of other parameters; in particular, the line-of-sight shear can be biased up to $2\\sigma$. Accounting for the line-of-sight flexion in our minimal framework restores accuracy, at the the cost of degrading precision.","With current imaging capabilities, the line-of-sight flexion is unlikely to be measurable on individual strong lensing images; it must therefore be considered a nuisance parameter rather than an observable in its own right."],"url":"http://arxiv.org/abs/2405.12091v1","category":"astro-ph.CO"}
{"created":"2024-05-20 14:55:20","title":"Noise-tolerant learnability of shallow quantum circuits from statistics and the cost of quantum pseudorandomness","abstract":"This work studies the learnability of unknown quantum circuits in the near term. We prove the natural robustness of quantum statistical queries for learning quantum processes and provide an efficient way to benchmark various classes of noise from statistics, which gives us a powerful framework for developing noise-tolerant algorithms. We adapt a learning algorithm for constant-depth quantum circuits to the quantum statistical query setting with a small overhead in the query complexity. We prove average-case lower bounds for learning random quantum circuits of logarithmic and higher depths within diamond distance with statistical queries. Additionally, we show the hardness of the quantum threshold search problem from quantum statistical queries and discuss its implications for the learnability of shallow quantum circuits. Finally, we prove that pseudorandom unitaries (PRUs) cannot be constructed using circuits of constant depth by constructing an efficient distinguisher and proving a new variation of the quantum no-free lunch theorem.","sentences":["This work studies the learnability of unknown quantum circuits in the near term.","We prove the natural robustness of quantum statistical queries for learning quantum processes and provide an efficient way to benchmark various classes of noise from statistics, which gives us a powerful framework for developing noise-tolerant algorithms.","We adapt a learning algorithm for constant-depth quantum circuits to the quantum statistical query setting with a small overhead in the query complexity.","We prove average-case lower bounds for learning random quantum circuits of logarithmic and higher depths within diamond distance with statistical queries.","Additionally, we show the hardness of the quantum threshold search problem from quantum statistical queries and discuss its implications for the learnability of shallow quantum circuits.","Finally, we prove that pseudorandom unitaries (PRUs) cannot be constructed using circuits of constant depth by constructing an efficient distinguisher and proving a new variation of the quantum no-free lunch theorem."],"url":"http://arxiv.org/abs/2405.12085v1","category":"quant-ph"}
{"created":"2024-05-20 14:53:25","title":"Distributional Semantics, Holism, and the Instability of Meaning","abstract":"Current language models are built on the so-called distributional semantic approach to linguistic meaning that has the distributional hypothesis at its core. The distributional hypothesis involves a holistic conception of word meaning: the meaning of a word depends upon its relations to other words in the model. A standard objection to meaning holism is the charge of instability: any change in the meaning properties of a linguistic system (a human speaker, for example) would lead to many changes or possibly a complete change in the entire system. When the systems in question are trying to communicate with each other, it has been argued that instability of this kind makes communication impossible (Fodor and Lepore 1992, 1996, 1999). In this article, we examine whether the instability objection poses a problem for distributional models of meaning. First, we distinguish between distinct forms of instability that these models could exhibit, and we argue that only one such form is relevant for understanding the relation between instability and communication: what we call differential instability. Differential instability is variation in the relative distances between points in a space, rather than variation in the absolute position of those points. We distinguish differential and absolute instability by constructing two of our own models, a toy model constructed from the text of two novels, and a more sophisticated model constructed using the Word2vec algorithm from a combination of Wikipedia and SEP articles. We demonstrate the two forms of instability by showing how these models change as the corpora they are constructed from increase in size.","sentences":["Current language models are built on the so-called distributional semantic approach to linguistic meaning that has the distributional hypothesis at its core.","The distributional hypothesis involves a holistic conception of word meaning: the meaning of a word depends upon its relations to other words in the model.","A standard objection to meaning holism is the charge of instability: any change in the meaning properties of a linguistic system (a human speaker, for example) would lead to many changes or possibly a complete change in the entire system.","When the systems in question are trying to communicate with each other, it has been argued that instability of this kind makes communication impossible (Fodor and Lepore 1992, 1996, 1999).","In this article, we examine whether the instability objection poses a problem for distributional models of meaning.","First, we distinguish between distinct forms of instability that these models could exhibit, and we argue that only one such form is relevant for understanding the relation between instability and communication: what we call differential instability.","Differential instability is variation in the relative distances between points in a space, rather than variation in the absolute position of those points.","We distinguish differential and absolute instability by constructing two of our own models, a toy model constructed from the text of two novels, and a more sophisticated model constructed using the Word2vec algorithm from a combination of Wikipedia and SEP articles.","We demonstrate the two forms of instability by showing how these models change as the corpora they are constructed from increase in size."],"url":"http://arxiv.org/abs/2405.12084v1","category":"cs.CL"}
{"created":"2024-05-20 14:49:45","title":"PARALLELGPUOS: A Concurrent OS-level GPU Checkpoint and Restore System using Validated Speculation","abstract":"Checkpointing (C) and restoring (R) are key components for GPU tasks. POS is an OS-level GPU C/R system: It can transparently checkpoint or restore processes that use the GPU, without requiring any cooperation from the application, a key feature required by modern systems like the cloud. Moreover, POS is the first OS-level C/R system that can concurrently execute C/R with the application execution: a critical feature that can be trivially achieved when the processes only running on the CPU, but becomes challenging when the processes use GPU. The problem is how to ensure consistency during concurrent execution with the lack of application semantics due to transparency. CPU processes can leverage OS and hardware paging to fix inconsistency without application semantics. Unfortunately, GPU bypasses OS and paging for high performance. POS fills the semantic gap by speculatively extracting buffer access information of GPU kernels during runtime. Thanks to the simple and well-structured nature of GPU kernels, our speculative extraction (with runtime validation) achieves 100% accuracy on applications from training to inference whose domains span from vision, large language models, and reinforcement learning. Based on the extracted semantics, we systematically overlap C/R with application execution, and achieves orders of magnitude higher performance under various tasks compared with the state-of-the-art OS-level GPU C/R, including training fault tolerance, live GPU process migration, and cold starts acceleration in GPU-based serverless computing.","sentences":["Checkpointing (C) and restoring (R) are key components for GPU tasks.","POS is an OS-level GPU C/R system: It can transparently checkpoint or restore processes that use the GPU, without requiring any cooperation from the application, a key feature required by modern systems like the cloud.","Moreover, POS is the first OS-level C/R system that can concurrently execute C/R with the application execution: a critical feature that can be trivially achieved when the processes only running on the CPU, but becomes challenging when the processes use GPU.","The problem is how to ensure consistency during concurrent execution with the lack of application semantics due to transparency.","CPU processes can leverage OS and hardware paging to fix inconsistency without application semantics.","Unfortunately, GPU bypasses OS and paging for high performance.","POS fills the semantic gap by speculatively extracting buffer access information of GPU kernels during runtime.","Thanks to the simple and well-structured nature of GPU kernels, our speculative extraction (with runtime validation) achieves 100% accuracy on applications from training to inference whose domains span from vision, large language models, and reinforcement learning.","Based on the extracted semantics, we systematically overlap C/R with application execution, and achieves orders of magnitude higher performance under various tasks compared with the state-of-the-art OS-level GPU C/R, including training fault tolerance, live GPU process migration, and cold starts acceleration in GPU-based serverless computing."],"url":"http://arxiv.org/abs/2405.12079v1","category":"cs.DC"}
{"created":"2024-05-20 14:43:39","title":"Constraining the helium-to-metal enrichment ratio $\u0394Y/\u0394Z$ from main sequence binary stars. Theoretical analysis of the accuracy and precision of the age and helium abundance estimates","abstract":"We investigated the theoretical possibility of accurately determining the helium-to-metal enrichment ratio $\\Delta Y/\\Delta Z$ from precise observations of double lined eclipsing binary systems. Using Monte Carlo simulations, we drew synthetic binary systems with masses between 0.85 and 1.00 $M_{\\odot}$ from a grid of stellar models with $\\Delta Y/\\Delta Z = 2.0$ [...]. Subsequently, a broader grid with $\\Delta Y/\\Delta Z$ from 1.0 to 3.0 was used in the fitting process. To account for observational uncertainties, two scenarios were explored: S1 with realistic uncertainties of 100 K in temperature and 0.1 dex in [Fe/H], and S2 with halved uncertainties. We repeated the simulation at two baseline metallicities: [Fe/H] = 0.0 and -0.3. The posterior distributions of $\\Delta Y/\\Delta Z$ were severely biased towards the edge of the allowable range in the S1 errors scenario. The situation only marginally improved when considering the S2 scenario. The effect is due to the impact of changing $\\Delta Y/\\Delta Z$ in the stellar effective temperature and its interplay with [Fe/H] observational error, and it is therefore not restricted to the specific fitting method. Despite the presence of these systematic discrepancies, the age of the systems were recovered unbiased with 10% precision. Our findings indicate that the observational uncertainty in effective temperature and metallicity significantly hinders the accurate determination of the $\\Delta Y/\\Delta Z$ parameter from main sequence binary systems.","sentences":["We investigated the theoretical possibility of accurately determining the helium-to-metal enrichment ratio $\\Delta Y/\\Delta Z$ from precise observations of double lined eclipsing binary systems.","Using Monte Carlo simulations, we drew synthetic binary systems with masses between 0.85 and 1.00 $M_{\\odot}$ from a grid of stellar models with $\\Delta Y/\\Delta Z = 2.0$ [...].","Subsequently, a broader grid with $\\Delta Y/\\Delta Z$ from 1.0 to 3.0 was used in the fitting process.","To account for observational uncertainties, two scenarios were explored: S1 with realistic uncertainties of 100 K in temperature and 0.1 dex in [Fe/H], and S2 with halved uncertainties.","We repeated the simulation at two baseline metallicities:","[Fe/H] = 0.0 and -0.3.","The posterior distributions of $\\Delta Y/\\Delta Z$ were severely biased towards the edge of the allowable range in the S1 errors scenario.","The situation only marginally improved when considering the S2 scenario.","The effect is due to the impact of changing $\\Delta Y/\\Delta Z$ in the stellar effective temperature and its interplay with [Fe/H] observational error, and it is therefore not restricted to the specific fitting method.","Despite the presence of these systematic discrepancies, the age of the systems were recovered unbiased with 10% precision.","Our findings indicate that the observational uncertainty in effective temperature and metallicity significantly hinders the accurate determination of the $\\Delta Y/\\Delta Z$ parameter from main sequence binary systems."],"url":"http://arxiv.org/abs/2405.12075v1","category":"astro-ph.SR"}
{"created":"2024-05-20 14:41:38","title":"Real topological phonons in 3D carbon allotropes","abstract":"There has been a significant focus on real topological systems that enjoy space-time inversion symmetry (PT ) and lack spin-orbit coupling. While the theoretical classification of the real topology has been established, more progress has yet to be made in the materials realization of such real topological systems in three dimensions (3D). To address this crucial issue, by selecting the carbon-based material candidates as targets, we perform high-throughput computing to inspect the real topology in the phonon spectrums of the 3D carbon allotropes in the Samara Carbon Allotrope Database (SACADA). Among 1192 kinds of 3D carbon allotropes, we find 65 real topological systems with a phononic real Chern insulating (PRCI) state, 2 real topological systems with a phononic real nodal line (PRNL) state, 10 real topological systems with a phononic real Dirac point (PRDP) state, and 8 real topological systems with a phononic real triple-point pair (PRTPP) state. This extremely expands the material candidates with real topology, especially for the gapless topological phonons. We exhibit the PRCI, PRNL, PRTPP, and PRDP states of 27-SG. 166-pcu-h, 1081-SG. 194- 4 2T13-CA, 52-SG. 141-gis, and 132-SG. 191-3,4T157 as illustrative examples, and explore the second-order boundary mode, i.e., phononic hinge mode. Among the four examples, the materials 1081-SG. 194-42T13-CA and 52-SG. 141-gis are so ideal that the PRNL and PRTPP in them are well separated from other bands, and the phononic hinge mode can be clearly observed. This study aims to broaden the understanding of 3D topological phonons, and emphasizes the potential of 3D carbon allotropes as a valuable framework for exploring the fascinating physics related to phononic hinge modes and phononic real topology.","sentences":["There has been a significant focus on real topological systems that enjoy space-time inversion symmetry (PT ) and lack spin-orbit coupling.","While the theoretical classification of the real topology has been established, more progress has yet to be made in the materials realization of such real topological systems in three dimensions (3D).","To address this crucial issue, by selecting the carbon-based material candidates as targets, we perform high-throughput computing to inspect the real topology in the phonon spectrums of the 3D carbon allotropes in the Samara Carbon Allotrope Database (SACADA).","Among 1192 kinds of 3D carbon allotropes, we find 65 real topological systems with a phononic real Chern insulating (PRCI) state, 2 real topological systems with a phononic real nodal line (PRNL) state, 10 real topological systems with a phononic real Dirac point (PRDP) state, and 8 real topological systems with a phononic real triple-point pair (PRTPP) state.","This extremely expands the material candidates with real topology, especially for the gapless topological phonons.","We exhibit the PRCI, PRNL, PRTPP, and PRDP states of 27-SG.","166-pcu-h, 1081-SG.","194- 4 2T13-CA, 52-SG.","141-gis, and 132-SG.","191-3,4T157 as illustrative examples, and explore the second-order boundary mode, i.e., phononic hinge mode.","Among the four examples, the materials 1081-SG.","194-42T13-CA and 52-SG.","141-gis are so ideal that the PRNL and PRTPP in them are well separated from other bands, and the phononic hinge mode can be clearly observed.","This study aims to broaden the understanding of 3D topological phonons, and emphasizes the potential of 3D carbon allotropes as a valuable framework for exploring the fascinating physics related to phononic hinge modes and phononic real topology."],"url":"http://arxiv.org/abs/2405.12072v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 14:41:19","title":"The Projective Wave Theory of Consciousness","abstract":"Neural theories of consciousness face three difficulties: (1) The selection problem: how are those neurons which cause consciousness selected, from all the other neurons which do not? (2) the precision problem: how do neurons hold a detailed internal model of 3D space, as the origin of our spatial conscious experience? and (3) the decoding problem: how are the many distorted neural representations of space in the brain decoded, to give our largely undistorted conscious experience of space? These problems can all be addressed if the brains internal model of local 3D space is held not in neurons, but in a wave excitation (holding a projective transform of Euclidean space), and if the wave is the source of spatial consciousness. Such a wave has not yet been detected in the brain, but there are good reasons why it has not been detected; and there is indirect evidence for a wave, in the mammalian thalamus, and in the central body of the insect brain. The resulting projective wave theory of consciousness gives good agreement with the spatial form of our consciousness. It has a positive Bayesian balance between the complexity of its assumptions and the data it accounts for; this gives a basis to believe it.","sentences":["Neural theories of consciousness face three difficulties: (1) The selection problem: how are those neurons which cause consciousness selected, from all the other neurons which do not?","(2) the precision problem: how do neurons hold a detailed internal model of 3D space, as the origin of our spatial conscious experience?","and (3) the decoding problem: how are the many distorted neural representations of space in the brain decoded, to give our largely undistorted conscious experience of space?","These problems can all be addressed if the brains internal model of local 3D space is held not in neurons, but in a wave excitation (holding a projective transform of Euclidean space), and if the wave is the source of spatial consciousness.","Such a wave has not yet been detected in the brain, but there are good reasons why it has not been detected; and there is indirect evidence for a wave, in the mammalian thalamus, and in the central body of the insect brain.","The resulting projective wave theory of consciousness gives good agreement with the spatial form of our consciousness.","It has a positive Bayesian balance between the complexity of its assumptions and the data it accounts for; this gives a basis to believe it."],"url":"http://arxiv.org/abs/2405.12071v1","category":"q-bio.NC"}
{"created":"2024-05-20 14:38:46","title":"Cycles in spherical Deligne complexes and application to $K(\u03c0,1)$-conjecture for Artin groups","abstract":"We show the $K(\\pi,1)$-conjecture holds for Artin groups whose Dynkin diagrams are complete bipartite (edge labels are allowed to be arbitrary), answering a question of J. McCammond. Along the way, we treat several related families of hyperbolic type Artin groups, namely the $K(\\pi,1)$-conjecture holds for all 3-dimensional hyperbolic type Artin groups, except one single example with Dynkin diagram $[3,5,3]$; and the conjecture holds for all quasi-Lann\\'er hyperbolic type Artin groups up to dimension 4. We also treat several higher dimensional families.   Most of the article is about developing new methods of understanding combinatorial minimal fillings of certain types of cycles in spherical Deligne complexes or relative Artin complexes, via non-positive curvature geometry. Then we combine this with an approached to the $K(\\pi,1)$-conjecture introduced by a previous article of the author to settle new cases of $K(\\pi,1)$-conjecture.   In the appendix we list some related open questions and conjectures.","sentences":["We show the $K(\\pi,1)$-conjecture holds for Artin groups whose Dynkin diagrams are complete bipartite (edge labels are allowed to be arbitrary), answering a question of J. McCammond.","Along the way, we treat several related families of hyperbolic type Artin groups, namely the $K(\\pi,1)$-conjecture holds for all 3-dimensional hyperbolic type Artin groups, except one single example with Dynkin diagram $[3,5,3]$; and the conjecture holds for all quasi-Lann\\'er hyperbolic type Artin groups up to dimension 4.","We also treat several higher dimensional families.   ","Most of the article is about developing new methods of understanding combinatorial minimal fillings of certain types of cycles in spherical Deligne complexes or relative Artin complexes, via non-positive curvature geometry.","Then we combine this with an approached to the $K(\\pi,1)$-conjecture introduced by a previous article of the author to settle new cases of $K(\\pi,1)$-conjecture.   ","In the appendix we list some related open questions and conjectures."],"url":"http://arxiv.org/abs/2405.12068v1","category":"math.GR"}
{"created":"2024-05-20 14:21:32","title":"Solvent Selectivity controls Micro- versus Macro-phase Separation in Multiblock Chains","abstract":"Monte Carlo simulations in the grand canonical ensemble were used to obtain critical parameters and conditions leading to microphase separation for block copolymers with solvophilic and solvophobic segments. Solvent selectivity was systematically varied to distinguish between systems that undergo macrophase separation to ones that microphase separate in the dilute phase, prior to macrophase separating. Finite-size scaling was used to obtain the critical parameters. Interestingly, corrections to scaling increase significantly for systems that form finite aggregates. The threshold value of solvent selectivity for aggregation was determined for symmetric diblock chains of varying length. The results indicate that long diblock copolymers form micelles in the dilute phase prior to macrophase separation, even in marginally selective solvents. The dependence of critical temperature on solvent selectivity was obtained for triblock, multiblock, and alternating chains. For highly selective solvents, strong structuring of both dilute and dense phases makes it harder to reach equilibrium.","sentences":["Monte Carlo simulations in the grand canonical ensemble were used to obtain critical parameters and conditions leading to microphase separation for block copolymers with solvophilic and solvophobic segments.","Solvent selectivity was systematically varied to distinguish between systems that undergo macrophase separation to ones that microphase separate in the dilute phase, prior to macrophase separating.","Finite-size scaling was used to obtain the critical parameters.","Interestingly, corrections to scaling increase significantly for systems that form finite aggregates.","The threshold value of solvent selectivity for aggregation was determined for symmetric diblock chains of varying length.","The results indicate that long diblock copolymers form micelles in the dilute phase prior to macrophase separation, even in marginally selective solvents.","The dependence of critical temperature on solvent selectivity was obtained for triblock, multiblock, and alternating chains.","For highly selective solvents, strong structuring of both dilute and dense phases makes it harder to reach equilibrium."],"url":"http://arxiv.org/abs/2405.12054v1","category":"cond-mat.soft"}
{"created":"2024-05-20 14:18:53","title":"Complex Principle Kurtosis Analysis","abstract":"Independent component analysis (ICA) is a fundamental problem in the field of signal processing, and numerous algorithms have been developed to address this issue. The core principle of these algorithms is to find a transformation matrix that maximizes the non-Gaussianity of the separated signals. Most algorithms typically assume that the source signals are mutually independent (orthogonal to each other), thereby imposing an orthogonal constraint on the transformation matrix. However, this assumption is not always valid in practical scenarios, where the orthogonal constraint can lead to inaccurate results. Recently, tensor-based algorithms have attracted much attention due to their ability to reduce computational complexity and enhance separation performance. In these algorithms, ICA is reformulated as an eigenpair problem of a statistical tensor. Importantly, the eigenpairs of a tensor are not inherently orthogonal, making tensor-based algorithms more suitable for nonorthogonal cases. Despite this advantage, finding exact solutions to the tensor's eigenpair problem remains a challenging task. In this paper, we introduce a non-zero volume constraint and a Riemannian gradient-based algorithm to solve the tensor's eigenpair problem. The proposed algorithm can find exact solutions under nonorthogonal conditions, making it more effective for separating nonorthogonal sources. Additionally, existing tensor-based algorithms typically rely on third-order statistics and are limited to real-valued data. To overcome this limitation, we extend tensor-based algorithms to the complex domain by constructing a fourth-order statistical tensor. Experiments conducted on both synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithm.","sentences":["Independent component analysis (ICA) is a fundamental problem in the field of signal processing, and numerous algorithms have been developed to address this issue.","The core principle of these algorithms is to find a transformation matrix that maximizes the non-Gaussianity of the separated signals.","Most algorithms typically assume that the source signals are mutually independent (orthogonal to each other), thereby imposing an orthogonal constraint on the transformation matrix.","However, this assumption is not always valid in practical scenarios, where the orthogonal constraint can lead to inaccurate results.","Recently, tensor-based algorithms have attracted much attention due to their ability to reduce computational complexity and enhance separation performance.","In these algorithms, ICA is reformulated as an eigenpair problem of a statistical tensor.","Importantly, the eigenpairs of a tensor are not inherently orthogonal, making tensor-based algorithms more suitable for nonorthogonal cases.","Despite this advantage, finding exact solutions to the tensor's eigenpair problem remains a challenging task.","In this paper, we introduce a non-zero volume constraint and a Riemannian gradient-based algorithm to solve the tensor's eigenpair problem.","The proposed algorithm can find exact solutions under nonorthogonal conditions, making it more effective for separating nonorthogonal sources.","Additionally, existing tensor-based algorithms typically rely on third-order statistics and are limited to real-valued data.","To overcome this limitation, we extend tensor-based algorithms to the complex domain by constructing a fourth-order statistical tensor.","Experiments conducted on both synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithm."],"url":"http://arxiv.org/abs/2405.12053v1","category":"eess.SP"}
{"created":"2024-05-20 14:16:08","title":"The amount of nonhyperbolicity for partially hyperbolic diffeomorphisms","abstract":"We study the amount of nonhyperbolicity within a broad class of (nonhyperbolic) partially hyperbolic diffeomorphisms with a one-dimensional center. For that, we focus on the center Lyapunov exponent and the entropy of its level sets. We show that these entropies vary continuously and can be expressed in terms of restricted variational principles. In this study, no dynamical coherence is required.   Of particular interest is the case where the exponent is zero. To study this level set, we construct a compact set foliated by curves tangent to the central direction. Within this set, the entropy attains the maximal possible (and positive) value. Moreover, finite-time Lyapunov exponents converge uniformly to zero. In this construction, we introduce a mechanism to concatenate center curves.   The class studied consists of those robustly transitive diffeomorphisms that have a pair of blender-horseshoes with different types of hyperbolicity and possess minimal strong stable and unstable foliations. This classes includes flow-type and circle-fibered diffeomorphisms as well as some derived from Anosov diffeomorphisms. It also includes the so-called anomalous examples which are dynamically incoherent.","sentences":["We study the amount of nonhyperbolicity within a broad class of (nonhyperbolic) partially hyperbolic diffeomorphisms with a one-dimensional center.","For that, we focus on the center Lyapunov exponent and the entropy of its level sets.","We show that these entropies vary continuously and can be expressed in terms of restricted variational principles.","In this study, no dynamical coherence is required.   ","Of particular interest is the case where the exponent is zero.","To study this level set, we construct a compact set foliated by curves tangent to the central direction.","Within this set, the entropy attains the maximal possible (and positive) value.","Moreover, finite-time Lyapunov exponents converge uniformly to zero.","In this construction, we introduce a mechanism to concatenate center curves.   ","The class studied consists of those robustly transitive diffeomorphisms that have a pair of blender-horseshoes with different types of hyperbolicity and possess minimal strong stable and unstable foliations.","This classes includes flow-type and circle-fibered diffeomorphisms as well as some derived from Anosov diffeomorphisms.","It also includes the so-called anomalous examples which are dynamically incoherent."],"url":"http://arxiv.org/abs/2405.12051v1","category":"math.DS"}
{"created":"2024-05-20 14:11:32","title":"Theory of weights for log convergent cohomologies II: the case of a proper SNCL scheme in characteristic $p>0$","abstract":"For a flat $p$-adic formal family $S$ of log points over a complete discrete valuation ring with perfect residue field of mixed characteristics $(0,p)$ and for a simple normal crossing log scheme $X$ over an exact closed log subscheme of $S$ defined by an element of the maximal ideal of the dvr, we construct two fundamental filtered complexes in the convergent topos the underlying scheme of $X$ over the underlying scheme of $S$. We prove that they are canonically isomorphisc. Because one of the complex is shown to calculate the log convergent cohomology sheaf of $X/S$, the filtered complex produces the weight filtration on the log convergent cohomology sheaf if the underlying scheme of $X$ is proper over the underlying scheme of $S$. We give a comparison theorem between the projection of the filtered complex in the Zariski topos of the underlying scheme of $X$ and the isozariskian weight-filtered complex constructed in the author's previous work.","sentences":["For a flat $p$-adic formal family $S$ of log points over a complete discrete valuation ring with perfect residue field of mixed characteristics $(0,p)$ and for a simple normal crossing log scheme $X$ over an exact closed log subscheme of $S$ defined by an element of the maximal ideal of the dvr, we construct two fundamental filtered complexes in the convergent topos the underlying scheme of $X$ over the underlying scheme of $S$. We prove that they are canonically isomorphisc.","Because one of the complex is shown to calculate the log convergent cohomology sheaf of $X/S$, the filtered complex produces the weight filtration on the log convergent cohomology sheaf if the underlying scheme of $X$ is proper over the underlying scheme of $S$. We give a comparison theorem between the projection of the filtered complex in the Zariski topos of the underlying scheme of $X$ and the isozariskian weight-filtered complex constructed in the author's previous work."],"url":"http://arxiv.org/abs/2405.12045v1","category":"math.AG"}
{"created":"2024-05-20 14:00:34","title":"Propagation of equilibrium states in stable families of endomorphisms of $\\mathbb P^k(\\mathbb C)$","abstract":"We prove that, within any holomorphic family of endomorphisms of $\\mathbb P^k(\\mathbb C)$ in any dimension $k \\geq 1$ and algebraic degree $d \\geq 2$, the measurable holomorphic motion associated to dynamical stability in the sense of Berteloot-Bianchi-Dupont preserves the class of equilibrium states associated with weight functions $\\psi$ satisfying $\\sup\\psi - \\inf \\psi < \\log d$.","sentences":["We prove that, within any holomorphic family of endomorphisms of $\\mathbb P^k(\\mathbb C)$ in any dimension $k \\geq 1$ and algebraic degree $d \\geq 2$, the measurable holomorphic motion associated to dynamical stability in the sense of Berteloot-Bianchi-Dupont preserves the class of equilibrium states associated with weight functions $\\psi$ satisfying $\\sup\\psi - \\inf \\psi < \\log d$."],"url":"http://arxiv.org/abs/2405.12033v1","category":"math.DS"}
{"created":"2024-05-20 13:54:25","title":"Collisional thermometry for Gaussian systems","abstract":"We investigate a quantum thermometry scheme based collision model with Gaussian systems. A key open question of these schemes concerns the scaling of the Quantum Fisher Information (QFI) with the number of ancillae. In qubit-based implementations this question is difficult to assess, due to the exponentially growing size of the Hilbert space. Here we focus on Gaussian collision models, which allow for the scaling of the QFI to be evaluated for arbitrarily large sizes. This numerical flexibility enables us to explore the thermometric properties of the model for a wide range of configurations. Despite the infinite Markov order of the stochastic process of the model, we provide a simple phenomenological analysis for the behavior of the QFI, estimating the asymptotic Fisher information density and how the transient effects of correlations for an increasing number of ancillae depend on the physical parameters of the model.","sentences":["We investigate a quantum thermometry scheme based collision model with Gaussian systems.","A key open question of these schemes concerns the scaling of the Quantum Fisher Information (QFI) with the number of ancillae.","In qubit-based implementations this question is difficult to assess, due to the exponentially growing size of the Hilbert space.","Here we focus on Gaussian collision models, which allow for the scaling of the QFI to be evaluated for arbitrarily large sizes.","This numerical flexibility enables us to explore the thermometric properties of the model for a wide range of configurations.","Despite the infinite Markov order of the stochastic process of the model, we provide a simple phenomenological analysis for the behavior of the QFI, estimating the asymptotic Fisher information density and how the transient effects of correlations for an increasing number of ancillae depend on the physical parameters of the model."],"url":"http://arxiv.org/abs/2405.12030v1","category":"quant-ph"}
{"created":"2024-05-20 13:51:52","title":"The TRAPUM Small Magellanic Cloud pulsar survey with MeerKAT: I. Discovery of seven new pulsars and two Pulsar Wind Nebula associations","abstract":"The sensitivity of the MeerKAT radio interferometer is an opportunity to probe deeper into the population of rare and faint extragalactic pulsars. The TRAPUM (TRAnsients and PUlsars with MeerKAT) collaboration has conducted a radio-domain search for accelerated pulsars and transients in the Small Magellanic Cloud (SMC). This partially targeted survey, performed at L-band (856-1712 MHz) with the core array of the MeerKAT telescope in 2-h integrations, is twice as sensitive as the latest SMC radio pulsar survey. We report the discovery of seven new SMC pulsars, doubling this galaxy's radio pulsar population and increasing the total extragalactic population by nearly a quarter. We also carried out a search for accelerated millisecond pulsars in the SMC Globular Cluster NGC 121 using the full array of MeerKAT. This improved the previous upper limit on pulsed radio emission from this cluster by a factor of six. Our discoveries reveal the first radio pulsar-PWN systems in the SMC, with only one such system previously known outside our galaxy (the \"Crab pulsar twin\" in the Large Magellanic Cloud, PSR J0540$-$6919). We associate the 59 ms pulsar discovery PSR J0040$-$7337, now the fastest spinning radio pulsar in the SMC, with the bow-shock Pulsar Wind Nebula (PWN) of Supernova Remnant DEM S5. We also present a new young pulsar with a 79 ms period, PSR J0048$-$7317, in a PWN recently discovered in a MeerKAT radio continuum image. Using the multi-beam capability of MeerKAT, we localised our pulsar discoveries, and two previous Murriyang discoveries, to a positional uncertainty of a few arcseconds.","sentences":["The sensitivity of the MeerKAT radio interferometer is an opportunity to probe deeper into the population of rare and faint extragalactic pulsars.","The TRAPUM (TRAnsients and PUlsars with MeerKAT) collaboration has conducted a radio-domain search for accelerated pulsars and transients in the Small Magellanic Cloud (SMC).","This partially targeted survey, performed at L-band (856-1712 MHz) with the core array of the MeerKAT telescope in 2-h integrations, is twice as sensitive as the latest SMC radio pulsar survey.","We report the discovery of seven new SMC pulsars, doubling this galaxy's radio pulsar population and increasing the total extragalactic population by nearly a quarter.","We also carried out a search for accelerated millisecond pulsars in the SMC Globular Cluster NGC 121 using the full array of MeerKAT.","This improved the previous upper limit on pulsed radio emission from this cluster by a factor of six.","Our discoveries reveal the first radio pulsar-PWN systems in the SMC, with only one such system previously known outside our galaxy (the \"Crab pulsar twin\" in the Large Magellanic Cloud, PSR J0540$-$6919).","We associate the 59 ms pulsar discovery PSR J0040$-$7337, now the fastest spinning radio pulsar in the SMC, with the bow-shock Pulsar Wind Nebula (PWN) of Supernova Remnant DEM S5.","We also present a new young pulsar with a 79 ms period, PSR J0048$-$7317, in a PWN recently discovered in a MeerKAT radio continuum image.","Using the multi-beam capability of MeerKAT, we localised our pulsar discoveries, and two previous Murriyang discoveries, to a positional uncertainty of a few arcseconds."],"url":"http://arxiv.org/abs/2405.12029v1","category":"astro-ph.HE"}
{"created":"2024-05-20 13:51:12","title":"The Case for DeepSOH: Addressing Path Dependency for Remaining Useful Life","abstract":"The battery state of health (SOH) based on capacity fade and resistance increase is not sufficient for predicting Remaining Useful life (RUL). The electrochemical community blames the path-dependency of the battery degradation mechanisms for our inability to forecast the degradation. The control community knows that the path-dependency is addressed by full state estimation. We show that even the electrode-specific SOH (eSOH) estimation is not enough to fully define the degradation states by simulating infinite possible degradation trajectories and remaining useful lives (RUL) from a unique eSOH. We finally define the deepSOH states that capture the individual contributions of all the common degradation mechanisms, namely, SEI, plating, and mechanical fracture to the loss of lithium inventory. We show that the addition of cell expansion measurement may allow us to estimate the deepSOH and predict the remaining useful life.","sentences":["The battery state of health (SOH) based on capacity fade and resistance increase is not sufficient for predicting Remaining Useful life (RUL).","The electrochemical community blames the path-dependency of the battery degradation mechanisms for our inability to forecast the degradation.","The control community knows that the path-dependency is addressed by full state estimation.","We show that even the electrode-specific SOH (eSOH) estimation is not enough to fully define the degradation states by simulating infinite possible degradation trajectories and remaining useful lives (RUL) from a unique eSOH.","We finally define the deepSOH states that capture the individual contributions of all the common degradation mechanisms, namely, SEI, plating, and mechanical fracture to the loss of lithium inventory.","We show that the addition of cell expansion measurement may allow us to estimate the deepSOH and predict the remaining useful life."],"url":"http://arxiv.org/abs/2405.12028v1","category":"eess.SY"}
{"created":"2024-05-20 13:40:52","title":"Continuous Sign Language Recognition with Adapted Conformer via Unsupervised Pretraining","abstract":"Conventional Deep Learning frameworks for continuous sign language recognition (CSLR) are comprised of a single or multi-modal feature extractor, a sequence-learning module, and a decoder for outputting the glosses. The sequence learning module is a crucial part wherein transformers have demonstrated their efficacy in the sequence-to-sequence tasks. Analyzing the research progress in the field of Natural Language Processing and Speech Recognition, a rapid introduction of various transformer variants is observed. However, in the realm of sign language, experimentation in the sequence learning component is limited. In this work, the state-of-the-art Conformer model for Speech Recognition is adapted for CSLR and the proposed model is termed ConSignformer. This marks the first instance of employing Conformer for a vision-based task. ConSignformer has bimodal pipeline of CNN as feature extractor and Conformer for sequence learning. For improved context learning we also introduce Cross-Modal Relative Attention (CMRA). By incorporating CMRA into the model, it becomes more adept at learning and utilizing complex relationships within the data. To further enhance the Conformer model, unsupervised pretraining called Regressional Feature Extraction is conducted on a curated sign language dataset. The pretrained Conformer is then fine-tuned for the downstream recognition task. The experimental results confirm the effectiveness of the adopted pretraining strategy and demonstrate how CMRA contributes to the recognition process. Remarkably, leveraging a Conformer-based backbone, our model achieves state-of-the-art performance on the benchmark datasets: PHOENIX-2014 and PHOENIX-2014T.","sentences":["Conventional Deep Learning frameworks for continuous sign language recognition (CSLR) are comprised of a single or multi-modal feature extractor, a sequence-learning module, and a decoder for outputting the glosses.","The sequence learning module is a crucial part wherein transformers have demonstrated their efficacy in the sequence-to-sequence tasks.","Analyzing the research progress in the field of Natural Language Processing and Speech Recognition, a rapid introduction of various transformer variants is observed.","However, in the realm of sign language, experimentation in the sequence learning component is limited.","In this work, the state-of-the-art Conformer model for Speech Recognition is adapted for CSLR and the proposed model is termed ConSignformer.","This marks the first instance of employing Conformer for a vision-based task.","ConSignformer has bimodal pipeline of CNN as feature extractor and Conformer for sequence learning.","For improved context learning we also introduce Cross-Modal Relative Attention (CMRA).","By incorporating CMRA into the model, it becomes more adept at learning and utilizing complex relationships within the data.","To further enhance the Conformer model, unsupervised pretraining called Regressional Feature Extraction is conducted on a curated sign language dataset.","The pretrained Conformer is then fine-tuned for the downstream recognition task.","The experimental results confirm the effectiveness of the adopted pretraining strategy and demonstrate how CMRA contributes to the recognition process.","Remarkably, leveraging a Conformer-based backbone, our model achieves state-of-the-art performance on the benchmark datasets: PHOENIX-2014 and PHOENIX-2014T."],"url":"http://arxiv.org/abs/2405.12018v1","category":"cs.CV"}
{"created":"2024-05-20 13:40:50","title":"Spectrally resolved free electron-light coupling strength in a transition metal dichalcogenide","abstract":"Recent advancements in electron microscopy have introduced innovative techniques enabling the inelastic interaction of fast electrons with tightly confined and intense light fields. These techniques, commonly summarized under the term photon-induced nearfield electron microscopy now offer unprecedented capabilities for a precise mapping of the characteristics of optical near-fields with remarkable spatial resolution but their spectral resolution were only scarcely investigated. In this study, we employ a strongly chirped and temporally broadband light pulse to investigate the interaction between free electrons and light at the edge of a MoS2 thin film. Our approach unveils the details of electron-light coupling, revealing a pronounced dependence of the coupling strength on both the position and photon energy. Employing numerical simulations of a simplified model system we identify these modulations to be caused by optical interferences between the incident and reflected field as well as an optical mode guided within the transition metal dichalcogenide film.","sentences":["Recent advancements in electron microscopy have introduced innovative techniques enabling the inelastic interaction of fast electrons with tightly confined and intense light fields.","These techniques, commonly summarized under the term photon-induced nearfield electron microscopy now offer unprecedented capabilities for a precise mapping of the characteristics of optical near-fields with remarkable spatial resolution but their spectral resolution were only scarcely investigated.","In this study, we employ a strongly chirped and temporally broadband light pulse to investigate the interaction between free electrons and light at the edge of a MoS2 thin film.","Our approach unveils the details of electron-light coupling, revealing a pronounced dependence of the coupling strength on both the position and photon energy.","Employing numerical simulations of a simplified model system we identify these modulations to be caused by optical interferences between the incident and reflected field as well as an optical mode guided within the transition metal dichalcogenide film."],"url":"http://arxiv.org/abs/2405.12017v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 13:32:51","title":"Linear Singer-Hopf conjecture","abstract":"If $X$ is a closed $2n$-dimensional aspherical manifold, i.e., the universal cover of $X$ is contractible, then the Singer-Hopf conjecture predicts that $(-1)^n\\chi(X)\\geq 0$. We prove this conjecture when $X$ is a complex projective manifold whose fundamental group admits an almost faithful linear representation over any field. In fact, we prove a much stronger statement that if $X$ is a complex projective manifold with large fundamental group and $\\pi_1(X)$ admits an almost faithful linear representation, then $\\chi(X, {P})\\geq 0$ for any perverse sheaf ${P}$ on $X$.   To prove the main result, we introduce a vanishing cycle functor of multivalued one-forms. Then using techniques from non-abelian Hodge theories in both archimedean and non-archimedean settings, we deduce the desired positivity from the geometry of pure and mixed period maps.","sentences":["If $X$ is a closed $2n$-dimensional aspherical manifold, i.e., the universal cover of $X$ is contractible, then the Singer-Hopf conjecture predicts that $(-1)^n\\chi(X)\\geq 0$.","We prove this conjecture when $X$ is a complex projective manifold whose fundamental group admits an almost faithful linear representation over any field.","In fact, we prove a much stronger statement that if $X$ is a complex projective manifold with large fundamental group and $\\pi_1(X)$ admits an almost faithful linear representation, then $\\chi(X, {P})\\geq 0$ for any perverse sheaf ${P}$ on $X$.   To prove the main result, we introduce a vanishing cycle functor of multivalued one-forms.","Then using techniques from non-abelian Hodge theories in both archimedean and non-archimedean settings, we deduce the desired positivity from the geometry of pure and mixed period maps."],"url":"http://arxiv.org/abs/2405.12012v1","category":"math.AG"}
{"created":"2024-05-20 13:24:59","title":"The efficiency of black hole formation via collisions in stellar systems: An analysis of data from simulations and observations","abstract":"This paper explores the theoretical relation between star clusters and black holes within, focusing on the potential role of Nuclear Star Clusters (NSCs), Globular Clusters (GCs), and Ultra Compact Dwarf Galaxies (UCDs) as environments that lead to black hole formation through stellar collisions. The study aims to identify optimal conditions for stellar collisions in different stellar systems leading to the formation of very massive stars that subsequently collapse into black holes. Data from numerical simulations and observations of diverse stellar systems are analyzed, encompassing various initial conditions, initial mass functions, and stellar evolution scenarios. We compute a critical mass, determined by the interplay of collision time, system age, and initial properties of the star cluster. The efficiency of black hole formation ($\\epsilon_{\\mathrm{BH}}$) is defined as the ratio of initial stellar mass divided by critical mass. The study finds out that stellar systems with a ratio of initial stellar mass over critical mass above 1 exhibit high efficiencies of black hole formation, ranging from $30-100\\%$. While there is some scatter, potentially attributed to complex system histories and the presence of gas, the results highlight the potential for achieving high efficiencies through a purely collisional channel in black hole formation. In conclusion, this theoretical exploration elucidates the connection between star clusters and black hole formation. The study underscores the significance of UCDs, GCs, and NSCs as environments conducive to stellar collisions leading to black hole formation. The defined black hole formation efficiency ($\\epsilon_{\\mathrm{BH}}$) is shown to be influenced by the ratio of initial stellar mass to critical mass.","sentences":["This paper explores the theoretical relation between star clusters and black holes within, focusing on the potential role of Nuclear Star Clusters (NSCs), Globular Clusters (GCs), and Ultra Compact Dwarf Galaxies (UCDs) as environments that lead to black hole formation through stellar collisions.","The study aims to identify optimal conditions for stellar collisions in different stellar systems leading to the formation of very massive stars that subsequently collapse into black holes.","Data from numerical simulations and observations of diverse stellar systems are analyzed, encompassing various initial conditions, initial mass functions, and stellar evolution scenarios.","We compute a critical mass, determined by the interplay of collision time, system age, and initial properties of the star cluster.","The efficiency of black hole formation ($\\epsilon_{\\mathrm{BH}}$) is defined as the ratio of initial stellar mass divided by critical mass.","The study finds out that stellar systems with a ratio of initial stellar mass over critical mass above 1 exhibit high efficiencies of black hole formation, ranging from $30-100\\%$. While there is some scatter, potentially attributed to complex system histories and the presence of gas, the results highlight the potential for achieving high efficiencies through a purely collisional channel in black hole formation.","In conclusion, this theoretical exploration elucidates the connection between star clusters and black hole formation.","The study underscores the significance of UCDs, GCs, and NSCs as environments conducive to stellar collisions leading to black hole formation.","The defined black hole formation efficiency ($\\epsilon_{\\mathrm{BH}}$) is shown to be influenced by the ratio of initial stellar mass to critical mass."],"url":"http://arxiv.org/abs/2405.12008v1","category":"astro-ph.GA"}
{"created":"2024-05-20 13:24:35","title":"Depth Reconstruction with Neural Signed Distance Fields in Structured Light Systems","abstract":"We introduce a novel depth estimation technique for multi-frame structured light setups using neural implicit representations of 3D space. Our approach employs a neural signed distance field (SDF), trained through self-supervised differentiable rendering. Unlike passive vision, where joint estimation of radiance and geometry fields is necessary, we capitalize on known radiance fields from projected patterns in structured light systems. This enables isolated optimization of the geometry field, ensuring convergence and network efficacy with fixed device positioning. To enhance geometric fidelity, we incorporate an additional color loss based on object surfaces during training. Real-world experiments demonstrate our method's superiority in geometric performance for few-shot scenarios, while achieving comparable results with increased pattern availability.","sentences":["We introduce a novel depth estimation technique for multi-frame structured light setups using neural implicit representations of 3D space.","Our approach employs a neural signed distance field (SDF), trained through self-supervised differentiable rendering.","Unlike passive vision, where joint estimation of radiance and geometry fields is necessary, we capitalize on known radiance fields from projected patterns in structured light systems.","This enables isolated optimization of the geometry field, ensuring convergence and network efficacy with fixed device positioning.","To enhance geometric fidelity, we incorporate an additional color loss based on object surfaces during training.","Real-world experiments demonstrate our method's superiority in geometric performance for few-shot scenarios, while achieving comparable results with increased pattern availability."],"url":"http://arxiv.org/abs/2405.12006v1","category":"cs.CV"}
{"created":"2024-05-20 13:23:17","title":"The Baryon Junction and String Interactions","abstract":"We study junctions between confining strings. We show that the effective theory of such junctions is very predictive with only one new parameter, the junction's mass, controlling the first couple of terms in the expansion in the system size. By open-closed duality these considerations about the baryon junction map to interaction vertices of closed strings. Therefore, we calculate the interaction vertices of closed strings in theories such as Yang-Mills theory. We find some surprising selection rules for string interactions in 3+1 dimensions. Requiring perturbative stability and that the string coupling is weak, we suggest constraints on the junction's mass.","sentences":["We study junctions between confining strings.","We show that the effective theory of such junctions is very predictive with only one new parameter, the junction's mass, controlling the first couple of terms in the expansion in the system size.","By open-closed duality these considerations about the baryon junction map to interaction vertices of closed strings.","Therefore, we calculate the interaction vertices of closed strings in theories such as Yang-Mills theory.","We find some surprising selection rules for string interactions in 3+1 dimensions.","Requiring perturbative stability and that the string coupling is weak, we suggest constraints on the junction's mass."],"url":"http://arxiv.org/abs/2405.12005v1","category":"hep-th"}
{"created":"2024-05-20 13:11:57","title":"Multi-Agent Optimization and Learning: A Non-Expansive Operators Perspective","abstract":"Multi-agent systems are increasingly widespread in a range of application domains, with optimization and learning underpinning many of the tasks that arise in this context. Different approaches have been proposed to enable the cooperative solution of these optimization and learning problems, including first- and second-order methods, and dual (or Lagrangian) methods, all of which rely on consensus and message-passing. In this article we discuss these algorithms through the lens of non-expansive operator theory, providing a unifying perspective. We highlight the insights that this viewpoint delivers, and discuss how it can spark future original research.","sentences":["Multi-agent systems are increasingly widespread in a range of application domains, with optimization and learning underpinning many of the tasks that arise in this context.","Different approaches have been proposed to enable the cooperative solution of these optimization and learning problems, including first- and second-order methods, and dual (or Lagrangian) methods, all of which rely on consensus and message-passing.","In this article we discuss these algorithms through the lens of non-expansive operator theory, providing a unifying perspective.","We highlight the insights that this viewpoint delivers, and discuss how it can spark future original research."],"url":"http://arxiv.org/abs/2405.11999v1","category":"math.OC"}
{"created":"2024-05-20 13:06:04","title":"Max-Min Fairness and PHY-Layer Design of Uplink MIMO Rate-Splitting Multiple Access with Finite Blocklength","abstract":"Rate-Splitting Multiple Access (RSMA) has emerged as a potent and reliable multiple access and interference management technique in wireless communications. While downlink Multiple-Input Multiple-Ouput (MIMO) RSMA has been widely investigated, uplink MIMO RSMA has not been fully explored. In this paper, we investigate the performance of uplink RSMA in short-packet communications with perfect Channel State Information at Transmitter (CSIT) and Channel State Information at Receiver (CSIR). We propose an uplink MIMO RSMA framework and optimize both precoders and combiners with Max-Min Fairness (MMF) metric and Finite Blocklength (FBL) constraints. Due to the high coupling between precoders and combiners, we apply the Alternating Optimization (AO) to decompose the optimization problem into two subproblems. To tackle these subproblems, we propose a Successive Convex Approximation (SCA)-based approach. Additionally, we introduce a low-complexity scheme to design the decoding order at the receiver. Subsequently, the Physical (PHY)-layer of the uplink MIMO RSMA architecture is designed and evaluated using multi-user Link-Level Simulations (LLS), accounting for finite constellation modulation, finite length polar codes, message splitting, adaptive modulation and coding, and Successive Interference Cancellation (SIC) at the receiver. Numerical results demonstrate that applying RSMA in uplink MIMO with FBL constraints not only achieves MMF gains over conventional transmission schemes such as Space Division Multiple Access (SDMA) and Non-orthogonal Multiple Access (NOMA) but also exhibits robustness to network loads. The benefits of splitting messages from multiple users are also illustrated. LLS results confirm the improved max-min throughput benefits of RSMA over SDMA and NOMA.","sentences":["Rate-Splitting Multiple Access (RSMA) has emerged as a potent and reliable multiple access and interference management technique in wireless communications.","While downlink Multiple-Input Multiple-Ouput (MIMO) RSMA has been widely investigated, uplink MIMO RSMA has not been fully explored.","In this paper, we investigate the performance of uplink RSMA in short-packet communications with perfect Channel State Information at Transmitter (CSIT) and Channel State Information at Receiver (CSIR).","We propose an uplink MIMO RSMA framework and optimize both precoders and combiners with Max-Min Fairness (MMF) metric and Finite Blocklength (FBL) constraints.","Due to the high coupling between precoders and combiners, we apply the Alternating Optimization (AO) to decompose the optimization problem into two subproblems.","To tackle these subproblems, we propose a Successive Convex Approximation (SCA)-based approach.","Additionally, we introduce a low-complexity scheme to design the decoding order at the receiver.","Subsequently, the Physical (PHY)-layer of the uplink MIMO RSMA architecture is designed and evaluated using multi-user Link-Level Simulations (LLS), accounting for finite constellation modulation, finite length polar codes, message splitting, adaptive modulation and coding, and Successive Interference Cancellation (SIC) at the receiver.","Numerical results demonstrate that applying RSMA in uplink MIMO with FBL constraints not only achieves MMF gains over conventional transmission schemes such as Space Division Multiple Access (SDMA) and Non-orthogonal Multiple Access (NOMA) but also exhibits robustness to network loads.","The benefits of splitting messages from multiple users are also illustrated.","LLS results confirm the improved max-min throughput benefits of RSMA over SDMA and NOMA."],"url":"http://arxiv.org/abs/2405.11996v1","category":"cs.IT"}
{"created":"2024-05-20 12:58:25","title":"Safe by Design Autonomous Driving Systems","abstract":"Developing safe autonomous driving systems is a major scientific and technical challenge. Existing AI-based end-to-end solutions do not offer the necessary safety guarantees, while traditional systems engineering approaches are defeated by the complexity of the problem. Currently, there is an increasing interest in hybrid design solutions, integrating machine learning components, when necessary, while using model-based components for goal management and planning.   We study a method for building safe by design autonomous driving systems, based on the assumption that the capability to drive boils down to the coordinated execution of a given set of driving operations. The assumption is substantiated by a compositionality result considering that autopilots are dynamic systems receiving a small number of types of vistas as input, each vista defining a free space in its neighborhood. It is shown that safe driving for each type of vista in the corresponding free space, implies safe driving for any possible scenario under some easy-to-check conditions concerning the transition between vistas. The designed autopilot comprises distinct control policies one per type of vista, articulated in two consecutive phases. The first phase consists of carefully managing a potentially risky situation by virtually reducing speed, while the second phase consists of exiting the situation by accelerating.   The autopilots designed use for their predictions simple functions characterizing the acceleration and deceleration capabilities of the vehicles. They cover the main driving operations, including entering a main road, overtaking, crossing intersections protected by traffic lights or signals, and driving on freeways. The results presented reinforce the case for hybrid solutions that incorporate mathematically elegant and robust decision methods that are safe by design.","sentences":["Developing safe autonomous driving systems is a major scientific and technical challenge.","Existing AI-based end-to-end solutions do not offer the necessary safety guarantees, while traditional systems engineering approaches are defeated by the complexity of the problem.","Currently, there is an increasing interest in hybrid design solutions, integrating machine learning components, when necessary, while using model-based components for goal management and planning.   ","We study a method for building safe by design autonomous driving systems, based on the assumption that the capability to drive boils down to the coordinated execution of a given set of driving operations.","The assumption is substantiated by a compositionality result considering that autopilots are dynamic systems receiving a small number of types of vistas as input, each vista defining a free space in its neighborhood.","It is shown that safe driving for each type of vista in the corresponding free space, implies safe driving for any possible scenario under some easy-to-check conditions concerning the transition between vistas.","The designed autopilot comprises distinct control policies one per type of vista, articulated in two consecutive phases.","The first phase consists of carefully managing a potentially risky situation by virtually reducing speed, while the second phase consists of exiting the situation by accelerating.   ","The autopilots designed use for their predictions simple functions characterizing the acceleration and deceleration capabilities of the vehicles.","They cover the main driving operations, including entering a main road, overtaking, crossing intersections protected by traffic lights or signals, and driving on freeways.","The results presented reinforce the case for hybrid solutions that incorporate mathematically elegant and robust decision methods that are safe by design."],"url":"http://arxiv.org/abs/2405.11995v1","category":"cs.MA"}
{"created":"2024-05-20 12:54:57","title":"GGAvatar: Geometric Adjustment of Gaussian Head Avatar","abstract":"We propose GGAvatar, a novel 3D avatar representation designed to robustly model dynamic head avatars with complex identities and deformations. GGAvatar employs a coarse-to-fine structure, featuring two core modules: Neutral Gaussian Initialization Module and Geometry Morph Adjuster. Neutral Gaussian Initialization Module pairs Gaussian primitives with deformable triangular meshes, employing an adaptive density control strategy to model the geometric structure of the target subject with neutral expressions. Geometry Morph Adjuster introduces deformation bases for each Gaussian in global space, creating fine-grained low-dimensional representations of deformation behaviors to address the Linear Blend Skinning formula's limitations effectively. Extensive experiments show that GGAvatar can produce high-fidelity renderings, outperforming state-of-the-art methods in visual quality and quantitative metrics.","sentences":["We propose GGAvatar, a novel 3D avatar representation designed to robustly model dynamic head avatars with complex identities and deformations.","GGAvatar employs a coarse-to-fine structure, featuring two core modules: Neutral Gaussian Initialization Module and Geometry Morph Adjuster.","Neutral Gaussian Initialization Module pairs Gaussian primitives with deformable triangular meshes, employing an adaptive density control strategy to model the geometric structure of the target subject with neutral expressions.","Geometry Morph Adjuster introduces deformation bases for each Gaussian in global space, creating fine-grained low-dimensional representations of deformation behaviors to address the Linear Blend Skinning formula's limitations effectively.","Extensive experiments show that GGAvatar can produce high-fidelity renderings, outperforming state-of-the-art methods in visual quality and quantitative metrics."],"url":"http://arxiv.org/abs/2405.11993v1","category":"cs.CV"}
{"created":"2024-05-20 12:53:15","title":"Dimension filtration of the bounded Derived category of a Noetherian ring","abstract":"Let $A$ be a Noetherian ring of dimension $d$ and let $\\mathcal{D}^b(A)$ be the bounded derived category of $A$. Let $\\mathcal{D}_i^b(A)$ denote the thick subcategory of $\\mathcal{D}^b(A)$ consisting of complexes $\\mathbf{X}_\\bullet$ with $\\dim H^n(\\mathbf{X}_\\bullet) \\leq i$ for all $n$. Set $\\mathcal{D}_{-1}^b(A) = 0$. Consider the Verdier quotients $\\mathcal{C}_i(A) = \\mathcal{D}_i^b(A)/\\mathcal{D}_{i-1}^b(A)$. We show for $i = 0, \\ldots, d$, $\\mathcal{C}_i(A)$ is a Krull-Remak-Schmidt triangulated category with a bounded $t$-structure. We identify its heart. We also prove that if $A$ is regular then $\\mathcal{C}_i(A)$ has AR-triangles. We also prove that $$ \\mathcal{C}_i(A) \\cong \\bigoplus_{\\stackrel{P}{\\dim A/P = i}} \\mathcal{D}_0^b(A_P). $$","sentences":["Let $A$ be a Noetherian ring of dimension $d$ and let $\\mathcal{D}^b(A)$ be the bounded derived category of $A$.","Let $\\mathcal{D}_i^b(A)$ denote the thick subcategory of $\\mathcal{D}^b(A)$ consisting of complexes $\\mathbf{X}_\\bullet$ with $\\dim H^n(\\mathbf{X}_\\bullet)","\\leq i$ for all $n$. Set $\\mathcal{D}_{-1}^b(A) = 0$. Consider the Verdier quotients $\\mathcal{C}_i(A)","= \\mathcal{D}_i^b(A)/\\mathcal{D}_{i-1}^b(A)$.","We show for $i = 0, \\ldots, d$, $\\mathcal{C}_i(A)$ is a Krull-Remak-Schmidt triangulated category with a bounded $t$-structure.","We identify its heart.","We also prove that if $A$ is regular then $\\mathcal{C}_i(A)$ has AR-triangles.","We also prove that $$ \\mathcal{C}_i(A)","\\cong \\bigoplus_{\\stackrel{P}{\\dim A/P = i}} \\mathcal{D}_0^b(A_P).","$$"],"url":"http://arxiv.org/abs/2405.11991v1","category":"math.AC"}
{"created":"2024-05-20 12:52:06","title":"Coherent Quantum Communications Across National Scale Telecommunication Infrastructure","abstract":"Quantum communications harness quantum phenomena like superposition and entanglement to enhance information transfer between remote nodes. Coherent quantum communications, essential for phase-based quantum internet architecture, require optical coherence among nodes and typically involve single-photon interference. Challenges like preserving optical coherence and integrating advanced single-photon detectors have impeded their deployment in existing telecommunication networks. This study introduces innovative approaches to the architecture and techniques supporting coherent quantum communications, marking their first successful integration within a commercial telecom infrastructure between Frankfurt and Kehl, Germany. Employing the Twin Field Quantum Key Distribution protocol, we achieved encryption key distribution at 110 bit/s over 254 km. This system features measurement-device-independent properties and non-cryogenically cooled detectors, and represents the first effective quantum repeater implementation on telecom infrastructure, the longest practical quantum key distribution deployment to date, and validates the feasibility of a phase-based quantum internet architecture.","sentences":["Quantum communications harness quantum phenomena like superposition and entanglement to enhance information transfer between remote nodes.","Coherent quantum communications, essential for phase-based quantum internet architecture, require optical coherence among nodes and typically involve single-photon interference.","Challenges like preserving optical coherence and integrating advanced single-photon detectors have impeded their deployment in existing telecommunication networks.","This study introduces innovative approaches to the architecture and techniques supporting coherent quantum communications, marking their first successful integration within a commercial telecom infrastructure between Frankfurt and Kehl, Germany.","Employing the Twin Field Quantum Key Distribution protocol, we achieved encryption key distribution at 110 bit/s over 254 km.","This system features measurement-device-independent properties and non-cryogenically cooled detectors, and represents the first effective quantum repeater implementation on telecom infrastructure, the longest practical quantum key distribution deployment to date, and validates the feasibility of a phase-based quantum internet architecture."],"url":"http://arxiv.org/abs/2405.11990v1","category":"quant-ph"}
{"created":"2024-05-20 12:48:48","title":"Wrinkling of differentially growing bilayers with similar film and substrate moduli","abstract":"The study of growth-induced surface wrinkling in constrained bilayers comprising a thin film attached to a thick substrate is a canonical model for understanding pattern formation in many biological systems. While the bilayer model has received much prior attention, the nonlinear behaviour for arrangements with similar film and substrate properties, or substrate growth that outpaces film growth, remains poorly understood. This paper therefore focuses on these cases in which the substrate's elasticity dominates surface wrinkling. We study the critical states, and the initial and advanced post-critical behaviour of growing bilayers with film-to-substrate modulus ratios in the region of $2.5$--$50$, and cases where the substrate grows faster than the film. Based on nonlinear elasticity, we formulate analytical models for linear buckling analyses and asymptotic projections around the critical point, and use finite element (FE) models coupled to continuation and branch-switching algorithms to uncover the deep post-critical regime. It is shown that a rapidly growing substrate may change the critical mode from film-governed sinusoidal wrinkling to substrate-governed Biot wrinkling depending on the stiffness ratio and growth ratio. We present a phase change diagram of the post-critical modal landscape split into sinusoidal wrinkling, period doubling, period quadrupling, and creasing regimes in terms of the stiffness ratio and growth ratio. While the post-critical regime of film- and substrate-dominated bilayers (either in terms of dominant elasticity or growth rate) is governed by sinusoidal wrinkling and Biot creasing, respectively, the intermediate regions allow for period doubling and quadrupling bifurcations. Finally, we demonstrate the existence of multi-stability in the advanced post-buckling regimes for growing bilayers where growth in the substrate surpasses that of the film.","sentences":["The study of growth-induced surface wrinkling in constrained bilayers comprising a thin film attached to a thick substrate is a canonical model for understanding pattern formation in many biological systems.","While the bilayer model has received much prior attention, the nonlinear behaviour for arrangements with similar film and substrate properties, or substrate growth that outpaces film growth, remains poorly understood.","This paper therefore focuses on these cases in which the substrate's elasticity dominates surface wrinkling.","We study the critical states, and the initial and advanced post-critical behaviour of growing bilayers with film-to-substrate modulus ratios in the region of $2.5$--$50$, and cases where the substrate grows faster than the film.","Based on nonlinear elasticity, we formulate analytical models for linear buckling analyses and asymptotic projections around the critical point, and use finite element (FE) models coupled to continuation and branch-switching algorithms to uncover the deep post-critical regime.","It is shown that a rapidly growing substrate may change the critical mode from film-governed sinusoidal wrinkling to substrate-governed Biot wrinkling depending on the stiffness ratio and growth ratio.","We present a phase change diagram of the post-critical modal landscape split into sinusoidal wrinkling, period doubling, period quadrupling, and creasing regimes in terms of the stiffness ratio and growth ratio.","While the post-critical regime of film- and substrate-dominated bilayers (either in terms of dominant elasticity or growth rate) is governed by sinusoidal wrinkling and Biot creasing, respectively, the intermediate regions allow for period doubling and quadrupling bifurcations.","Finally, we demonstrate the existence of multi-stability in the advanced post-buckling regimes for growing bilayers where growth in the substrate surpasses that of the film."],"url":"http://arxiv.org/abs/2405.11989v1","category":"cond-mat.soft"}
{"created":"2024-05-20 12:39:28","title":"On Separation Logic, Computational Independence, and Pseudorandomness (Extended Version)","abstract":"Separation logic is a substructural logic which has proved to have numerous and fruitful applications to the verification of programs working on dynamic data structures. Recently, Barthe, Hsu and Liao have proposed a new way of giving semantics to separation logic formulas in which separating conjunction is interpreted in terms of probabilistic independence. The latter is taken in its exact form, i.e., two events are independent if and only if the joint probability is the product of the probabilities of the two events. There is indeed a literature on weaker notions of independence which are computational in nature, i.e. independence holds only against efficient adversaries and modulo a negligible probability of success. The aim of this work is to explore the nature of computational independence in a cryptographic scenario, in view of the aforementioned advances in separation logic. We show on the one hand that the semantics of separation logic can be adapted so as to account for complexity bounded adversaries, and on the other hand that the obtained logical system is useful for writing simple and compact proofs of standard cryptographic results in which the adversary remains hidden. Remarkably, this allows for a fruitful interplay between independence and pseudorandomness, itself a crucial notion in cryptography.","sentences":["Separation logic is a substructural logic which has proved to have numerous and fruitful applications to the verification of programs working on dynamic data structures.","Recently, Barthe, Hsu and Liao have proposed a new way of giving semantics to separation logic formulas in which separating conjunction is interpreted in terms of probabilistic independence.","The latter is taken in its exact form, i.e., two events are independent if and only if the joint probability is the product of the probabilities of the two events.","There is indeed a literature on weaker notions of independence which are computational in nature, i.e. independence holds only against efficient adversaries and modulo a negligible probability of success.","The aim of this work is to explore the nature of computational independence in a cryptographic scenario, in view of the aforementioned advances in separation logic.","We show on the one hand that the semantics of separation logic can be adapted so as to account for complexity bounded adversaries, and on the other hand that the obtained logical system is useful for writing simple and compact proofs of standard cryptographic results in which the adversary remains hidden.","Remarkably, this allows for a fruitful interplay between independence and pseudorandomness, itself a crucial notion in cryptography."],"url":"http://arxiv.org/abs/2405.11987v1","category":"cs.CR"}
{"created":"2024-05-20 12:20:33","title":"Non-equilibrium orbital edge magnetization","abstract":"Uncompensated non-equilibrium orbital magnetization may arise at sample edges in the presence of charge current. The value of the effect scales as the product of the current density and the electron mean free path without any additional smallness. This non-relativistic phenomenon originates in a lack of inversion symmetry of the electron wave functions in a vicinity of sample interfaces. In a conducting layer, where $z$ direction is chosen perpendicular to the surface, and the current flows in $x$ direction, the non-equilibrium orbital magnetization points in $y$ direction. In a top-bottom symmetric layer, the orbital magnetization has an opposite sign near the top and bottom interfaces thus mimicking the symmetry of the spin-Hall effect but can exceed the latter by orders of magnitude.","sentences":["Uncompensated non-equilibrium orbital magnetization may arise at sample edges in the presence of charge current.","The value of the effect scales as the product of the current density and the electron mean free path without any additional smallness.","This non-relativistic phenomenon originates in a lack of inversion symmetry of the electron wave functions in a vicinity of sample interfaces.","In a conducting layer, where $z$ direction is chosen perpendicular to the surface, and the current flows in $x$ direction, the non-equilibrium orbital magnetization points in $y$ direction.","In a top-bottom symmetric layer, the orbital magnetization has an opposite sign near the top and bottom interfaces thus mimicking the symmetry of the spin-Hall effect but can exceed the latter by orders of magnitude."],"url":"http://arxiv.org/abs/2405.11979v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 12:04:05","title":"Structured singular values and their application in computing eigenvalue backward errors of the Rosenbrock system matrix","abstract":"The structured singular values (aka the {\\mu}-values) are essential in analyzing the stability of control systems and in the structured eigenvalue perturbation theory of matrices and matrix polynomials. In this paper, we study the {\\mu}-value of a matrix under block-diagonal structured perturbations (full blocks but possibly rectangular). We provide an explicit expression for the {\\mu}-value and also obtain a computable upper bound in terms of minimizing the largest singular value of a parameter-dependent matrix. This upper bound equals the {\\mu}-value when the perturbation matrix has no more than three blocks on the diagonal. We then apply the {\\mu}-value results in computing eigenvalue backward errors of a Rosenbrock system matrix corresponding to a rational matrix function when some or all blocks of the Rosenbrock system matrix are subject to perturbation. The results are illustrated through numerical experiments.","sentences":["The structured singular values (aka the {\\mu}-values) are essential in analyzing the stability of control systems and in the structured eigenvalue perturbation theory of matrices and matrix polynomials.","In this paper, we study the {\\mu}-value of a matrix under block-diagonal structured perturbations (full blocks but possibly rectangular).","We provide an explicit expression for the {\\mu}-value and also obtain a computable upper bound in terms of minimizing the largest singular value of a parameter-dependent matrix.","This upper bound equals the {\\mu}-value when the perturbation matrix has no more than three blocks on the diagonal.","We then apply the {\\mu}-value results in computing eigenvalue backward errors of a Rosenbrock system matrix corresponding to a rational matrix function when some or all blocks of the Rosenbrock system matrix are subject to perturbation.","The results are illustrated through numerical experiments."],"url":"http://arxiv.org/abs/2405.11974v1","category":"math.OC"}
{"created":"2024-05-20 11:37:29","title":"A Simulation Tool for V2G Enabled Demand Response Based on Model Predictive Control","abstract":"Integrating electric vehicles (EVs) into the power grid can revolutionize energy management strategies, offering both challenges and opportunities for creating a more sustainable and resilient grid. In this context, model predictive control (MPC) emerges as a powerful tool for addressing the complexities of Grid-to-vehicle (G2V) and vehicle-to-grid (V2G) enabled demand response management. By leveraging advanced optimization techniques, MPC algorithms can anticipate future grid conditions and dynamically adjust EV charging and discharging schedules to balance supply and demand while minimizing operational costs and maximizing flexibility. However, no standard tools exist to evaluate novel energy management strategies based on MPC approaches. Our research focuses on harnessing the potential of MPC in G2V and V2G applications, by providing a simulation tool that allows to maximize EV flexibility and support demand response initiatives while mitigating the impact on EV battery health. In this paper, we propose an open-source MPC controller for G2V and V2G-enabled demand response management. The proposed approach is capable of tackling the uncertainties inherent in demand response operations. Through extensive simulation and analysis, we demonstrate the efficacy of our approach in maximizing the benefits of G2V and V2G while assessing the impact on the longevity and reliability of EV batteries. Specifically, our controller enables Charge Point Operators (CPOs) to optimize EV charging and discharging schedules in real-time, taking into account fluctuating energy prices, grid constraints, and EV user preferences.","sentences":["Integrating electric vehicles (EVs) into the power grid can revolutionize energy management strategies, offering both challenges and opportunities for creating a more sustainable and resilient grid.","In this context, model predictive control (MPC) emerges as a powerful tool for addressing the complexities of Grid-to-vehicle (G2V) and vehicle-to-grid (V2G) enabled demand response management.","By leveraging advanced optimization techniques, MPC algorithms can anticipate future grid conditions and dynamically adjust EV charging and discharging schedules to balance supply and demand while minimizing operational costs and maximizing flexibility.","However, no standard tools exist to evaluate novel energy management strategies based on MPC approaches.","Our research focuses on harnessing the potential of MPC in G2V and V2G applications, by providing a simulation tool that allows to maximize EV flexibility and support demand response initiatives while mitigating the impact on EV battery health.","In this paper, we propose an open-source MPC controller for G2V and V2G-enabled demand response management.","The proposed approach is capable of tackling the uncertainties inherent in demand response operations.","Through extensive simulation and analysis, we demonstrate the efficacy of our approach in maximizing the benefits of G2V and V2G while assessing the impact on the longevity and reliability of EV batteries.","Specifically, our controller enables Charge Point Operators (CPOs) to optimize EV charging and discharging schedules in real-time, taking into account fluctuating energy prices, grid constraints, and EV user preferences."],"url":"http://arxiv.org/abs/2405.11963v1","category":"eess.SY"}
{"created":"2024-05-20 11:36:37","title":"Dynamic classifier auditing by unsupervised anomaly detection methods: an application in packaging industry predictive maintenance","abstract":"Predictive maintenance in manufacturing industry applications is a challenging research field. Packaging machines are widely used in a large number of logistic companies' warehouses and must be working uninterruptedly. Traditionally, preventive maintenance strategies have been carried out to improve the performance of these machines. However, this kind of policies does not take into account the information provided by the sensors implemented in the machines. This paper presents an expert system for the automatic estimation of work orders to implement predictive maintenance policies for packaging machines. The key idea is that, from a set of alarms related to sensors implemented in the machine, the expert system should take a maintenance action while optimizing the response time. The work order estimator will act as a classifier, yielding a binary decision of whether a machine must undergo a maintenance action by a technician or not, followed by an unsupervised anomaly detection-based filtering stage to audit the classifier's output. The methods used for anomaly detection were: One-Class Support Vector Machine (OCSVM), Minimum Covariance Determinant (MCD) and a majority (hard) voting ensemble of them. All anomaly detection methods improve the performance of the baseline classifer but the best performance in terms of F1 score was obtained by the majority voting ensemble.","sentences":["Predictive maintenance in manufacturing industry applications is a challenging research field.","Packaging machines are widely used in a large number of logistic companies' warehouses and must be working uninterruptedly.","Traditionally, preventive maintenance strategies have been carried out to improve the performance of these machines.","However, this kind of policies does not take into account the information provided by the sensors implemented in the machines.","This paper presents an expert system for the automatic estimation of work orders to implement predictive maintenance policies for packaging machines.","The key idea is that, from a set of alarms related to sensors implemented in the machine, the expert system should take a maintenance action while optimizing the response time.","The work order estimator will act as a classifier, yielding a binary decision of whether a machine must undergo a maintenance action by a technician or not, followed by an unsupervised anomaly detection-based filtering stage to audit the classifier's output.","The methods used for anomaly detection were: One-Class Support Vector Machine (OCSVM), Minimum Covariance Determinant (MCD) and a majority (hard) voting ensemble of them.","All anomaly detection methods improve the performance of the baseline classifer but the best performance in terms of F1 score was obtained by the majority voting ensemble."],"url":"http://arxiv.org/abs/2405.11960v1","category":"cs.CE"}
{"created":"2024-05-20 11:26:00","title":"Variety of Attractors","abstract":"In this work, some variation of the concept of attractor are presented, called attractor, physical attractor and proper attractor. As a result, it is proved that the phase space of a topologically exact system is a proper attractor for its inverse and vice versa.   Furthermore, in definition of physical attractor, one face with systems in which their phase space attracts all of the open non-empty sets. It is established that a system is topologically mixing if and only if its phase space is physical attractor.   Finally, by appealing to IFSs, we demonstrate some classes of non-trivial topologically mixing and topologically exact IFSs.","sentences":["In this work, some variation of the concept of attractor are presented, called attractor, physical attractor and proper attractor.","As a result, it is proved that the phase space of a topologically exact system is a proper attractor for its inverse and vice versa.   ","Furthermore, in definition of physical attractor, one face with systems in which their phase space attracts all of the open non-empty sets.","It is established that a system is topologically mixing if and only if its phase space is physical attractor.   ","Finally, by appealing to IFSs, we demonstrate some classes of non-trivial topologically mixing and topologically exact IFSs."],"url":"http://arxiv.org/abs/2405.11957v1","category":"math.DS"}
{"created":"2024-05-20 11:22:03","title":"PET: Multi-agent Independent PPO-based Automatic ECN Tuning for High-Speed Data Center Networks","abstract":"Explicit Congestion Notification (ECN)-based congestion control schemes have been widely adopted in high-speed data center networks (DCNs), where the ECN marking threshold plays a determinant role in guaranteeing a packet lossless DCN. However, existing approaches either employ static settings with immutable thresholds that cannot be dynamically self-adjusted to adapt to network dynamics, or fail to take into account many-to-one traffic patterns and different requirements of different types of traffic, resulting in relatively poor performance. To address these problems, this paper proposes a novel learning-based automatic ECN tuning scheme, named PET, based on the multi-agent Independent Proximal Policy Optimization (IPPO) algorithm. PET dynamically adjusts ECN thresholds by fully considering pivotal congestion-contributing factors, including queue length, output data rate, output rate of ECN-marked packets, current ECN threshold, the extent of incast, and the ratio of mice and elephant flows. PET adopts the Decentralized Training and Decentralized Execution (DTDE) paradigm and combines offline and online training to accommodate network dynamics. PET is also fair and readily deployable with commodity hardware. Comprehensive experimental results demonstrate that, compared with state-of-the-art static schemes and the learning-based automatic scheme, our PET achieves better performance in terms of flow completion time, convergence rate, queue length variance, and system robustness.","sentences":["Explicit Congestion Notification (ECN)-based congestion control schemes have been widely adopted in high-speed data center networks (DCNs), where the ECN marking threshold plays a determinant role in guaranteeing a packet lossless DCN.","However, existing approaches either employ static settings with immutable thresholds that cannot be dynamically self-adjusted to adapt to network dynamics, or fail to take into account many-to-one traffic patterns and different requirements of different types of traffic, resulting in relatively poor performance.","To address these problems, this paper proposes a novel learning-based automatic ECN tuning scheme, named PET, based on the multi-agent Independent Proximal Policy Optimization (IPPO) algorithm.","PET dynamically adjusts ECN thresholds by fully considering pivotal congestion-contributing factors, including queue length, output data rate, output rate of ECN-marked packets, current ECN threshold, the extent of incast, and the ratio of mice and elephant flows.","PET adopts the Decentralized Training and Decentralized Execution (DTDE) paradigm and combines offline and online training to accommodate network dynamics.","PET is also fair and readily deployable with commodity hardware.","Comprehensive experimental results demonstrate that, compared with state-of-the-art static schemes and the learning-based automatic scheme, our PET achieves better performance in terms of flow completion time, convergence rate, queue length variance, and system robustness."],"url":"http://arxiv.org/abs/2405.11956v1","category":"cs.NI"}
{"created":"2024-05-20 11:21:23","title":"Shallow Recurrent Decoder for Reduced Order Modeling of Plasma Dynamics","abstract":"Reduced order models are becoming increasingly important for rendering complex and multiscale spatio-temporal dynamics computationally tractable. The computational efficiency of such surrogate models is especially important for design, exhaustive exploration and physical understanding. Plasma simulations, in particular those applied to the study of ${\\bf E}\\times {\\bf B}$ plasma discharges and technologies, such as Hall thrusters, require substantial computational resources in order to resolve the multidimentional dynamics that span across wide spatial and temporal scales. Although high-fidelity computational tools are available to simulate such systems over limited conditions and in highly simplified geometries, simulations of full-size systems and/or extensive parametric studies over many geometric configurations and under different physical conditions are computationally intractable with conventional numerical tools. Thus, scientific studies and industrially oriented modeling of plasma systems, including the important ${\\bf E}\\times {\\bf B}$ technologies, stand to significantly benefit from reduced order modeling algorithms. We develop a model reduction scheme based upon a {\\em Shallow REcurrent Decoder} (SHRED) architecture. The scheme uses a neural network for encoding limited sensor measurements in time (sequence-to-sequence encoding) to full state-space reconstructions via a decoder network. Based upon the theory of separation of variables, the SHRED architecture is capable of (i) reconstructing full spatio-temporal fields with as little as three point sensors, even the fields that are not measured with sensor feeds but that are in dynamic coupling with the measured field, and (ii) forecasting the future state of the system using neural network roll-outs from the trained time encoding model.","sentences":["Reduced order models are becoming increasingly important for rendering complex and multiscale spatio-temporal dynamics computationally tractable.","The computational efficiency of such surrogate models is especially important for design, exhaustive exploration and physical understanding.","Plasma simulations, in particular those applied to the study of ${\\bf E}\\times {\\bf B}$ plasma discharges and technologies, such as Hall thrusters, require substantial computational resources in order to resolve the multidimentional dynamics that span across wide spatial and temporal scales.","Although high-fidelity computational tools are available to simulate such systems over limited conditions and in highly simplified geometries, simulations of full-size systems and/or extensive parametric studies over many geometric configurations and under different physical conditions are computationally intractable with conventional numerical tools.","Thus, scientific studies and industrially oriented modeling of plasma systems, including the important ${\\bf E}\\times {\\bf B}$ technologies, stand to significantly benefit from reduced order modeling algorithms.","We develop a model reduction scheme based upon a {\\em Shallow REcurrent Decoder} (SHRED) architecture.","The scheme uses a neural network for encoding limited sensor measurements in time (sequence-to-sequence encoding) to full state-space reconstructions via a decoder network.","Based upon the theory of separation of variables, the SHRED architecture is capable of (i) reconstructing full spatio-temporal fields with as little as three point sensors, even the fields that are not measured with sensor feeds but that are in dynamic coupling with the measured field, and (ii) forecasting the future state of the system using neural network roll-outs from the trained time encoding model."],"url":"http://arxiv.org/abs/2405.11955v1","category":"physics.plasm-ph"}
{"created":"2024-05-20 10:23:09","title":"A Flat Dual-Polarized Millimeter-Wave Luneburg Lens Antenna Using Transformation Optics with Reduced Anisotropy and Impedance Mismatch","abstract":"In this paper, a compact wideband dual-polarized Luneburg lens antenna (LLA) with reduced anisotropy and improved impedance matching is proposed in Ka band with a wide 2D beamscanning capability. Based on transformation optics, the spherical Luneburg lens is compressed into a cylindrical one, while the merits of high gain, broad band, wide scanning, and free polarization are preserved. A trigonometric function is employed to the material property of the flattened Luneburg lens with reduced anisotropy, thus effectively alleviates the strong reflection, the high sidelobes and back radiation with a free cost on the antenna weight and volume. Furthermore, a light thin wideband 7-by-1 metasurface phased array is studied as the primary feed for the LLA. The proposed metantenna, shorted for metamaterial-based antenna, has a high potential for B5G, future wireless communication and radar sensing as an onboard system.","sentences":["In this paper, a compact wideband dual-polarized Luneburg lens antenna (LLA) with reduced anisotropy and improved impedance matching is proposed in Ka band with a wide 2D beamscanning capability.","Based on transformation optics, the spherical Luneburg lens is compressed into a cylindrical one, while the merits of high gain, broad band, wide scanning, and free polarization are preserved.","A trigonometric function is employed to the material property of the flattened Luneburg lens with reduced anisotropy, thus effectively alleviates the strong reflection, the high sidelobes and back radiation with a free cost on the antenna weight and volume.","Furthermore, a light thin wideband 7-by-1 metasurface phased array is studied as the primary feed for the LLA.","The proposed metantenna, shorted for metamaterial-based antenna, has a high potential for B5G, future wireless communication and radar sensing as an onboard system."],"url":"http://arxiv.org/abs/2405.11935v1","category":"eess.SY"}
{"created":"2024-05-20 10:08:16","title":"Essential normality of quotient submodules over strongly pseudoconvex finite manifolds","abstract":"We investigate the $p$-essential normality of Hilbert quotient submodules on a relatively compact smooth strongly pseudoconvex domain in a complex manifold satisfying Property (S). For analytic subvarieties that have compact singularities and transversely intersect the strongly pseudoconvex boundary, we prove that the corresponding Bergman-Sobolev quotient submodules are $p$-essentially normal whenever $p$ exceeds the dimension of the noncompact part of the analytic subvarieties. As a consequence, we partially confirm the geometric Arveson-Douglas Conjecture and resolve an open problem regarding the trace-class antisymmetric sum of truncated Toeplitz operators within a broader context. Moreover, we provide applications in $K$-homology and geometric invariant theory.","sentences":["We investigate the $p$-essential normality of Hilbert quotient submodules on a relatively compact smooth strongly pseudoconvex domain in a complex manifold satisfying Property (S).","For analytic subvarieties that have compact singularities and transversely intersect the strongly pseudoconvex boundary, we prove that the corresponding Bergman-Sobolev quotient submodules are $p$-essentially normal whenever $p$ exceeds the dimension of the noncompact part of the analytic subvarieties.","As a consequence, we partially confirm the geometric Arveson-Douglas Conjecture and resolve an open problem regarding the trace-class antisymmetric sum of truncated Toeplitz operators within a broader context.","Moreover, we provide applications in $K$-homology and geometric invariant theory."],"url":"http://arxiv.org/abs/2405.11929v1","category":"math.CV"}
{"created":"2024-05-20 09:58:27","title":"Effective Clustering on Large Attributed Bipartite Graphs","abstract":"Attributed bipartite graphs (ABGs) are an expressive data model for describing the interactions between two sets of heterogeneous nodes that are associated with rich attributes, such as customer-product purchase networks and author-paper authorship graphs. Partitioning the target node set in such graphs into k disjoint clusters (referred to as k-ABGC) finds widespread use in various domains, including social network analysis, recommendation systems, information retrieval, and bioinformatics. However, the majority of existing solutions towards k-ABGC either overlook attribute information or fail to capture bipartite graph structures accurately, engendering severely compromised result quality. The severity of these issues is accentuated in real ABGs, which often encompass millions of nodes and a sheer volume of attribute data, rendering effective k-ABGC over such graphs highly challenging.   In this paper, we propose TPO, an effective and efficient approach to k-ABGC that achieves superb clustering performance on multiple real datasets. TPO obtains high clustering quality through two major contributions: (i) a novel formulation and transformation of the k-ABGC problem based on multi-scale attribute affinity specialized for capturing attribute affinities between nodes with the consideration of their multi-hop connections in ABGs, and (ii) a highly efficient solver that includes a suite of carefully-crafted optimizations for sidestepping explicit affinity matrix construction and facilitating faster convergence. Extensive experiments, comparing TPO against 19 baselines over 5 real ABGs, showcase the superior clustering quality of TPO measured against ground-truth labels. Moreover, compared to the state of the arts, TPO is often more than 40x faster over both small and large ABGs.","sentences":["Attributed bipartite graphs (ABGs) are an expressive data model for describing the interactions between two sets of heterogeneous nodes that are associated with rich attributes, such as customer-product purchase networks and author-paper authorship graphs.","Partitioning the target node set in such graphs into k disjoint clusters (referred to as k-ABGC) finds widespread use in various domains, including social network analysis, recommendation systems, information retrieval, and bioinformatics.","However, the majority of existing solutions towards k-ABGC either overlook attribute information or fail to capture bipartite graph structures accurately, engendering severely compromised result quality.","The severity of these issues is accentuated in real ABGs, which often encompass millions of nodes and a sheer volume of attribute data, rendering effective k-ABGC over such graphs highly challenging.   ","In this paper, we propose TPO, an effective and efficient approach to k-ABGC that achieves superb clustering performance on multiple real datasets.","TPO obtains high clustering quality through two major contributions: (i) a novel formulation and transformation of the k-ABGC problem based on multi-scale attribute affinity specialized for capturing attribute affinities between nodes with the consideration of their multi-hop connections in ABGs, and (ii) a highly efficient solver that includes a suite of carefully-crafted optimizations for sidestepping explicit affinity matrix construction and facilitating faster convergence.","Extensive experiments, comparing TPO against 19 baselines over 5 real ABGs, showcase the superior clustering quality of TPO measured against ground-truth labels.","Moreover, compared to the state of the arts, TPO is often more than 40x faster over both small and large ABGs."],"url":"http://arxiv.org/abs/2405.11922v1","category":"cs.SI"}
{"created":"2024-05-20 09:52:31","title":"Information Leakage from Embedding in Large Language Models","abstract":"The widespread adoption of large language models (LLMs) has raised concerns regarding data privacy. This study aims to investigate the potential for privacy invasion through input reconstruction attacks, in which a malicious model provider could potentially recover user inputs from embeddings. We first propose two base methods to reconstruct original texts from a model's hidden states. We find that these two methods are effective in attacking the embeddings from shallow layers, but their effectiveness decreases when attacking embeddings from deeper layers. To address this issue, we then present Embed Parrot, a Transformer-based method, to reconstruct input from embeddings in deep layers. Our analysis reveals that Embed Parrot effectively reconstructs original inputs from the hidden states of ChatGLM-6B and Llama2-7B, showcasing stable performance across various token lengths and data distributions. To mitigate the risk of privacy breaches, we introduce a defense mechanism to deter exploitation of the embedding reconstruction process. Our findings emphasize the importance of safeguarding user privacy in distributed learning systems and contribute valuable insights to enhance the security protocols within such environments.","sentences":["The widespread adoption of large language models (LLMs) has raised concerns regarding data privacy.","This study aims to investigate the potential for privacy invasion through input reconstruction attacks, in which a malicious model provider could potentially recover user inputs from embeddings.","We first propose two base methods to reconstruct original texts from a model's hidden states.","We find that these two methods are effective in attacking the embeddings from shallow layers, but their effectiveness decreases when attacking embeddings from deeper layers.","To address this issue, we then present Embed Parrot, a Transformer-based method, to reconstruct input from embeddings in deep layers.","Our analysis reveals that Embed Parrot effectively reconstructs original inputs from the hidden states of ChatGLM-6B and Llama2-7B, showcasing stable performance across various token lengths and data distributions.","To mitigate the risk of privacy breaches, we introduce a defense mechanism to deter exploitation of the embedding reconstruction process.","Our findings emphasize the importance of safeguarding user privacy in distributed learning systems and contribute valuable insights to enhance the security protocols within such environments."],"url":"http://arxiv.org/abs/2405.11916v1","category":"cs.LG"}
{"created":"2024-05-20 09:33:16","title":"Ultrafast Carrier Relaxation Dynamics in a Nodal-Line Semimetal PtSn$_4$","abstract":"Topological Dirac nodal-line semimetals host topologically nontrivial electronic structure with nodal-line crossings around the Fermi level, which could affect the photocarrier dynamics and lead to novel relaxation mechanisms. Herein, by using time- and angle-resolved photoemission spectroscopy, we reveal the previously-inaccessible linear dispersions of the bulk conduction bands above the Fermi level in a Dirac nodal-line semimetal PtSn$_4$, as well as the momentum and temporal evolution of the gapless nodal lines. A surprisingly ultrafast relaxation dynamics within a few hundred femtoseconds is revealed for photoexcited carriers in the nodal line. Theoretical calculations suggest that such ultrafast carrier relaxation is attributed to the multichannel scatterings among the complex metallic bands of PtSn$_4$ via electron-phonon coupling. In addition, a unique dynamic relaxation mechanism contributed by the highly anisotropic Dirac nodal-line electronic structure is also identified. Our work provides a comprehensive understanding of the ultrafast carrier dynamics in a Dirac nodal-line semimetal.","sentences":["Topological Dirac nodal-line semimetals host topologically nontrivial electronic structure with nodal-line crossings around the Fermi level, which could affect the photocarrier dynamics and lead to novel relaxation mechanisms.","Herein, by using time- and angle-resolved photoemission spectroscopy, we reveal the previously-inaccessible linear dispersions of the bulk conduction bands above the Fermi level in a Dirac nodal-line semimetal PtSn$_4$, as well as the momentum and temporal evolution of the gapless nodal lines.","A surprisingly ultrafast relaxation dynamics within a few hundred femtoseconds is revealed for photoexcited carriers in the nodal line.","Theoretical calculations suggest that such ultrafast carrier relaxation is attributed to the multichannel scatterings among the complex metallic bands of PtSn$_4$ via electron-phonon coupling.","In addition, a unique dynamic relaxation mechanism contributed by the highly anisotropic Dirac nodal-line electronic structure is also identified.","Our work provides a comprehensive understanding of the ultrafast carrier dynamics in a Dirac nodal-line semimetal."],"url":"http://arxiv.org/abs/2405.11901v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 09:30:41","title":"TRAPUM search for pulsars in supernova remnants and pulsar wind nebulae -- I. Survey description and initial discoveries","abstract":"We present the description and initial results of the TRAPUM (TRAnsients And PUlsars with MeerKAT) search for pulsars associated with supernova remnants (SNRs), pulsar wind nebulae and unidentified TeV emission. The list of sources to be targeted includes a large number of well-known candidate pulsar locations but also new candidate SNRs identified using a range of criteria. Using the 64-dish MeerKAT radio telescope, we use an interferometric beamforming technique to tile the potential pulsar locations with coherent beams which we search for radio pulsations, above a signal-to-noise of 9, down to an average flux density upper limit of 30 $\\mu$Jy. This limit is target-dependent due to the contribution of the sky and nebula to the system temperature. Coherent beams are arranged to overlap at their 50 per cent power radius, so the sensitivity to pulsars is not degraded by more than this amount, though realistically averages around 65 per cent if every location in the beam is considered. We report the discovery of two new pulsars; PSR J1831$-$0941 is an adolescent pulsar likely to be the plerionic engine of the candidate PWN G20.0+0.0, and PSR J1818$-$1502 appears to be an old and faint pulsar that we serendipitously discovered near the centre of a SNR already hosting a compact central object. The survey holds importance for better understanding of neutron star birth rates and the energetics of young pulsars.","sentences":["We present the description and initial results of the TRAPUM (TRAnsients And PUlsars with MeerKAT) search for pulsars associated with supernova remnants (SNRs), pulsar wind nebulae and unidentified TeV emission.","The list of sources to be targeted includes a large number of well-known candidate pulsar locations but also new candidate SNRs identified using a range of criteria.","Using the 64-dish MeerKAT radio telescope, we use an interferometric beamforming technique to tile the potential pulsar locations with coherent beams which we search for radio pulsations, above a signal-to-noise of 9, down to an average flux density upper limit of 30 $\\mu$Jy.","This limit is target-dependent due to the contribution of the sky and nebula to the system temperature.","Coherent beams are arranged to overlap at their 50 per cent power radius, so the sensitivity to pulsars is not degraded by more than this amount, though realistically averages around 65 per cent if every location in the beam is considered.","We report the discovery of two new pulsars; PSR J1831$-$0941 is an adolescent pulsar likely to be the plerionic engine of the candidate PWN G20.0+0.0, and PSR J1818$-$1502 appears to be an old and faint pulsar that we serendipitously discovered near the centre of a SNR already hosting a compact central object.","The survey holds importance for better understanding of neutron star birth rates and the energetics of young pulsars."],"url":"http://arxiv.org/abs/2405.11899v1","category":"astro-ph.HE"}
{"created":"2024-05-20 09:30:38","title":"Model-Based Qubit Noise Spectroscopy","abstract":"Qubit noise spectroscopy (QNS) is a valuable tool for both the characterization of a qubit's environment and as a precursor to more effective qubit control to improve qubit fidelities. Existing approaches to QNS are what the classical spectrum estimation literature would call \"non-parametric\" approaches, in that a series of probe sequences are used to estimate noise power at a set of points or bands. In contrast, model-based approaches to spectrum estimation assume additional structure in the form of the spectrum and leverage this for improved statistical accuracy or other capabilities, such as superresolution. Here, we derive model-based QNS approaches using inspiration from classical signal processing, primarily though the recently developed Schrodinger wave autoregressive moving-average (SchWARMA) formalism for modeling correlated noise. We show, through both simulation and experimental data, how these model-based QNS approaches maintain the statistical and computational benefits of their classical counterparts, resulting in powerful new estimation approaches. Beyond the direct application of these approaches to QNS and quantum sensing, we anticipate that the flexibility of the underlying models will find utility in adaptive feedback control for quantum systems, in analogy with their role in classical adaptive signal processing and control.","sentences":["Qubit noise spectroscopy (QNS) is a valuable tool for both the characterization of a qubit's environment and as a precursor to more effective qubit control to improve qubit fidelities.","Existing approaches to QNS are what the classical spectrum estimation literature would call \"non-parametric\" approaches, in that a series of probe sequences are used to estimate noise power at a set of points or bands.","In contrast, model-based approaches to spectrum estimation assume additional structure in the form of the spectrum and leverage this for improved statistical accuracy or other capabilities, such as superresolution.","Here, we derive model-based QNS approaches using inspiration from classical signal processing, primarily though the recently developed Schrodinger wave autoregressive moving-average (SchWARMA) formalism for modeling correlated noise.","We show, through both simulation and experimental data, how these model-based QNS approaches maintain the statistical and computational benefits of their classical counterparts, resulting in powerful new estimation approaches.","Beyond the direct application of these approaches to QNS and quantum sensing, we anticipate that the flexibility of the underlying models will find utility in adaptive feedback control for quantum systems, in analogy with their role in classical adaptive signal processing and control."],"url":"http://arxiv.org/abs/2405.11898v1","category":"quant-ph"}
{"created":"2024-05-20 09:28:35","title":"The Milky Way Atlas for Linear Filaments","abstract":"Filamentary structure is important for the ISM and star formation. Galactic distribution of filaments may regulate the star formation rate in the Milky Way. However, interstellar filaments are intrinsically complex, making it difficult to study quantitatively. Here, we focus on linear filaments, the simplest morphology that can be treated as building blocks of any filamentary structure. We present the first catalog of 42 ``straight-line'' filaments across the full Galactic plane, identified by clustering of far-IR Herschel HiGAL clumps in position-position-velocity space. We use molecular line cubes to investigate the dynamics along the filaments; compare the filaments with Galactic spiral arms; and compare ambient magnetic fields with the filaments' orientation. The selected filaments show extreme linearity ($>$10), aspect ratio (7-48), and velocity coherence over a length of 3-40 pc (mostly $>$10 pc). About 1/3 of them are associated with spiral arms, but only one is located in arm center, a.k.a. ``bones'' of the Milky Way. A few of them extend perpendicular to the Galactic plane, and none is located in the Central Molecular Zone (CMZ) near the Galactic center. Along the filaments, prevalent periodic oscillation (both in velocity and density) is consistent with gas flows channeled by the filaments and feeding the clumps which harbor diverse star formation activities. No correlation is found between the filament orientations with Planck measured global magnetic field lines. This work highlights some of the fundamental properties of molecular filaments and provides a golden sample for follow-up studies on star formation, ISM structure, and Milky Way structure.","sentences":["Filamentary structure is important for the ISM and star formation.","Galactic distribution of filaments may regulate the star formation rate in the Milky Way.","However, interstellar filaments are intrinsically complex, making it difficult to study quantitatively.","Here, we focus on linear filaments, the simplest morphology that can be treated as building blocks of any filamentary structure.","We present the first catalog of 42 ``straight-line'' filaments across the full Galactic plane, identified by clustering of far-IR Herschel HiGAL clumps in position-position-velocity space.","We use molecular line cubes to investigate the dynamics along the filaments; compare the filaments with Galactic spiral arms; and compare ambient magnetic fields with the filaments' orientation.","The selected filaments show extreme linearity ($>$10), aspect ratio (7-48), and velocity coherence over a length of 3-40 pc (mostly $>$10 pc).","About 1/3 of them are associated with spiral arms, but only one is located in arm center, a.k.a. ``bones'' of the Milky Way.","A few of them extend perpendicular to the Galactic plane, and none is located in the Central Molecular Zone (CMZ) near the Galactic center.","Along the filaments, prevalent periodic oscillation (both in velocity and density) is consistent with gas flows channeled by the filaments and feeding the clumps which harbor diverse star formation activities.","No correlation is found between the filament orientations with Planck measured global magnetic field lines.","This work highlights some of the fundamental properties of molecular filaments and provides a golden sample for follow-up studies on star formation, ISM structure, and Milky Way structure."],"url":"http://arxiv.org/abs/2405.11896v1","category":"astro-ph.GA"}
{"created":"2024-05-20 09:28:23","title":"Sparse Attention-driven Quality Prediction for Production Process Optimization in Digital Twins","abstract":"In the process industry, optimizing production lines for long-term efficiency requires real-time monitoring and analysis of operation states to fine-tune production line parameters. However, the complexity in operational logic and the intricate coupling of production process parameters make it difficult to develop an accurate mathematical model for the entire process, thus hindering the deployment of efficient optimization mechanisms. In view of these difficulties, we propose to deploy a digital twin of the production line by digitally abstracting its physical layout and operational logic. By iteratively mapping the real-world data reflecting equipment operation status and product quality inspection in the digital twin, we adopt a quality prediction model for production process based on self-attention-enabled temporal convolutional neural networks. This model enables the data-driven state evolution of the digital twin. The digital twin takes a role of aggregating the information of actual operating conditions and the results of quality-sensitive analysis, which facilitates the optimization of process production quality with virtual-reality evolution under multi-dimensional constraints. Leveraging the digital twin model as an information-flow carrier, we extract temporal features from key process indicators and establish a production process quality prediction model based on the proposed composite neural network. Our operation experiments on a specific tobacco shredding line demonstrate that the proposed digital twin-based production process optimization method fosters seamless integration between virtual and real production lines. This integration achieves an average operating status prediction accuracy of over 98\\% and near-optimal production process control.","sentences":["In the process industry, optimizing production lines for long-term efficiency requires real-time monitoring and analysis of operation states to fine-tune production line parameters.","However, the complexity in operational logic and the intricate coupling of production process parameters make it difficult to develop an accurate mathematical model for the entire process, thus hindering the deployment of efficient optimization mechanisms.","In view of these difficulties, we propose to deploy a digital twin of the production line by digitally abstracting its physical layout and operational logic.","By iteratively mapping the real-world data reflecting equipment operation status and product quality inspection in the digital twin, we adopt a quality prediction model for production process based on self-attention-enabled temporal convolutional neural networks.","This model enables the data-driven state evolution of the digital twin.","The digital twin takes a role of aggregating the information of actual operating conditions and the results of quality-sensitive analysis, which facilitates the optimization of process production quality with virtual-reality evolution under multi-dimensional constraints.","Leveraging the digital twin model as an information-flow carrier, we extract temporal features from key process indicators and establish a production process quality prediction model based on the proposed composite neural network.","Our operation experiments on a specific tobacco shredding line demonstrate that the proposed digital twin-based production process optimization method fosters seamless integration between virtual and real production lines.","This integration achieves an average operating status prediction accuracy of over 98\\% and near-optimal production process control."],"url":"http://arxiv.org/abs/2405.11895v1","category":"cs.LG"}
{"created":"2024-05-20 09:18:05","title":"Emergence of giant orbital Hall and tunable spin Hall effects in centrosymmetric TMDs","abstract":"We demonstrate the formation of orbital and spin Hall effects (OHE/SHE) in the 1T phase of non-magnetic transition metal dichalcogenides. With the aid of density functional theory calculations and model Hamiltonian studies on MX$_2$ (M = Pt, Pd and X = S, Se, and Te), we show an intrinsic orbital Hall conductivity ($\\sim 10^3 \\hbar /e\\ \\Omega^{-1}cm^{-1}$) , which primarily emerges due to the orbital texture around the valleys in the momentum space. The robust spin-orbit coupling in these systems induces a sizable SHE out of OHE. Furthermore, to resemble the typical experimental setups, where the magnetic overlayers produce a proximity magnetic field, we examine the effect of magnetic field on OHE and SHE and showed that the latter can be doubled in these class of compounds. With a giant OHE and tunable SHE, the 1T-TMDs are promising candidates for spin and orbital driven quantum devices such as SOT-MRAM, spin nano-oscillators, spin logic devices etc., and to carry out spin-charge conversion experiments for fundamental research.","sentences":["We demonstrate the formation of orbital and spin Hall effects (OHE/SHE) in the 1T phase of non-magnetic transition metal dichalcogenides.","With the aid of density functional theory calculations and model Hamiltonian studies on MX$_2$ (M = Pt, Pd and X = S, Se, and Te), we show an intrinsic orbital Hall conductivity ($\\sim 10^3","\\hbar /e\\ \\Omega^{-1}cm^{-1}$) , which primarily emerges due to the orbital texture around the valleys in the momentum space.","The robust spin-orbit coupling in these systems induces a sizable SHE out of OHE.","Furthermore, to resemble the typical experimental setups, where the magnetic overlayers produce a proximity magnetic field, we examine the effect of magnetic field on OHE and SHE and showed that the latter can be doubled in these class of compounds.","With a giant OHE and tunable SHE, the 1T-TMDs are promising candidates for spin and orbital driven quantum devices such as SOT-MRAM, spin nano-oscillators, spin logic devices etc., and to carry out spin-charge conversion experiments for fundamental research."],"url":"http://arxiv.org/abs/2405.11892v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 09:09:47","title":"Social norm dynamics in a behavioral epidemic model on multiplex networks","abstract":"Understanding the social determinants influencing preventive measures adoption during epidemics is crucial for effective disease modeling and policy making. While traditional epidemic models focused on rational decision-making and psychological biases, recent studies highlight the role of social norms. We develop a behavioral epidemic model on a multiplex network, by integrating an Experience Weighted Attractor (EWA) learning mechanism and social norm dynamics. Incorporating social norms in our decision-making mechanism significantly reduces final infected fractions, offering an alternative to altruism for boosting vaccination coverage. Furthermore, we examine the importance of the dynamics of each one of the social norms, injunctive or descriptive, in reducing the infected fraction, finding that the former has a more significant effect in agreement with some experimental evidence. We also explore the effect of external interventions on epidemic expansion, aiding in refining public communication protocols. Enhanced models of social norm dynamics, if validated and tested, can better capture the complexities of human social behavior and mitigate various societal challenges beyond pandemics.","sentences":["Understanding the social determinants influencing preventive measures adoption during epidemics is crucial for effective disease modeling and policy making.","While traditional epidemic models focused on rational decision-making and psychological biases, recent studies highlight the role of social norms.","We develop a behavioral epidemic model on a multiplex network, by integrating an Experience Weighted Attractor (EWA) learning mechanism and social norm dynamics.","Incorporating social norms in our decision-making mechanism significantly reduces final infected fractions, offering an alternative to altruism for boosting vaccination coverage.","Furthermore, we examine the importance of the dynamics of each one of the social norms, injunctive or descriptive, in reducing the infected fraction, finding that the former has a more significant effect in agreement with some experimental evidence.","We also explore the effect of external interventions on epidemic expansion, aiding in refining public communication protocols.","Enhanced models of social norm dynamics, if validated and tested, can better capture the complexities of human social behavior and mitigate various societal challenges beyond pandemics."],"url":"http://arxiv.org/abs/2405.11887v1","category":"physics.soc-ph"}
{"created":"2024-05-20 08:57:30","title":"Asynchronous MIMO-OFDM Massive Unsourced Random Access with Codeword Collisions","abstract":"This paper investigates asynchronous MIMO massive unsourced random access in an orthogonal frequency division multiplexing (OFDM) system over frequency-selective fading channels, with the presence of both timing and carrier frequency offsets (TO and CFO) and non-negligible codeword collisions. The proposed coding framework segregates the data into two components, namely, preamble and coding parts, with the former being tree-coded and the latter LDPC-coded. By leveraging the dual sparsity of the equivalent channel across both codeword and delay domains (CD and DD), we develop a message passing-based sparse Bayesian learning algorithm, combined with belief propagation and mean field, to iteratively estimate DD channel responses, TO, and delay profiles. Furthermore, we establish a novel graph-based algorithm to iteratively separate the superimposed channels and compensate for the phase rotations. Additionally, the proposed algorithm is applied to the flat fading scenario to estimate both TO and CFO, where the channel and offset estimation is enhanced by leveraging the geometric characteristics of the signal constellation. Simulations reveal that the proposed algorithm achieves superior performance and substantial complexity reduction in both channel and offset estimation compared to the codebook enlarging-based counterparts, and enhanced data recovery performances compared to state-of-the-art URA schemes.","sentences":["This paper investigates asynchronous MIMO massive unsourced random access in an orthogonal frequency division multiplexing (OFDM) system over frequency-selective fading channels, with the presence of both timing and carrier frequency offsets (TO and CFO) and non-negligible codeword collisions.","The proposed coding framework segregates the data into two components, namely, preamble and coding parts, with the former being tree-coded and the latter LDPC-coded.","By leveraging the dual sparsity of the equivalent channel across both codeword and delay domains (CD and DD), we develop a message passing-based sparse Bayesian learning algorithm, combined with belief propagation and mean field, to iteratively estimate DD channel responses, TO, and delay profiles.","Furthermore, we establish a novel graph-based algorithm to iteratively separate the superimposed channels and compensate for the phase rotations.","Additionally, the proposed algorithm is applied to the flat fading scenario to estimate both TO and CFO, where the channel and offset estimation is enhanced by leveraging the geometric characteristics of the signal constellation.","Simulations reveal that the proposed algorithm achieves superior performance and substantial complexity reduction in both channel and offset estimation compared to the codebook enlarging-based counterparts, and enhanced data recovery performances compared to state-of-the-art URA schemes."],"url":"http://arxiv.org/abs/2405.11883v1","category":"cs.IT"}
{"created":"2024-05-20 08:55:11","title":"Non-Invasive Readout of the Kinetic Inductance of Superconducting Nanostructures","abstract":"The energy landscape of multiply connected superconducting structures is ruled by fluxoid quantization due to the implied single-valuedness of the complex wave function. The transitions and interaction between these energy states, each defined by a specific phase winding number, are governed by classical and/or quantum phase slips. Understanding these events requires the ability to probe, non-invasively, the state of the ring. Here, we employ a niobium resonator to examine the superconducting properties of an aluminum loop. By applying a magnetic field, adjusting temperature, and altering the loop's dimensions via focused ion beam milling, we correlate resonance frequency shifts with changes in the loop's kinetic inductance. This parameter is a unique indicator of the superconducting condensate's state, facilitating the detection of phase slips in nanodevices and providing insights into their dynamics. Our method presents a proof-of-principle spectroscopic technique with promising potential for investigating the Cooper pair density in inductively coupled superconducting nanostructures.","sentences":["The energy landscape of multiply connected superconducting structures is ruled by fluxoid quantization due to the implied single-valuedness of the complex wave function.","The transitions and interaction between these energy states, each defined by a specific phase winding number, are governed by classical and/or quantum phase slips.","Understanding these events requires the ability to probe, non-invasively, the state of the ring.","Here, we employ a niobium resonator to examine the superconducting properties of an aluminum loop.","By applying a magnetic field, adjusting temperature, and altering the loop's dimensions via focused ion beam milling, we correlate resonance frequency shifts with changes in the loop's kinetic inductance.","This parameter is a unique indicator of the superconducting condensate's state, facilitating the detection of phase slips in nanodevices and providing insights into their dynamics.","Our method presents a proof-of-principle spectroscopic technique with promising potential for investigating the Cooper pair density in inductively coupled superconducting nanostructures."],"url":"http://arxiv.org/abs/2405.11882v1","category":"physics.app-ph"}
{"created":"2024-05-20 08:30:11","title":"Equilibria in multiagent online problems with predictions","abstract":"We study the power of (competitive) algorithms with predictions in a multiagent setting. For this we introduce a multiagent version of the ski-rental problem. In this problem agents can collaborate by pooling resources to get a group license for some asset. If the license price is not met agents have to rent the asset individually for the day at a unit price. Otherwise the license becomes available forever to everyone at no extra cost. Our main contribution is a best-response analysis of a single-agent competitive algorithm that assumes perfect knowledge of other agents' actions (but no knowledge of its own renting time). We then analyze the setting when agents have a predictor for their own active time, yielding a tradeoff between robustness and consistency. We investigate the effect of using such a predictor in an equilibrium, as well as the new equilibria formed in this way.","sentences":["We study the power of (competitive) algorithms with predictions in a multiagent setting.","For this we introduce a multiagent version of the ski-rental problem.","In this problem agents can collaborate by pooling resources to get a group license for some asset.","If the license price is not met agents have to rent the asset individually for the day at a unit price.","Otherwise the license becomes available forever to everyone at no extra cost.","Our main contribution is a best-response analysis of a single-agent competitive algorithm that assumes perfect knowledge of other agents' actions (but no knowledge of its own renting time).","We then analyze the setting when agents have a predictor for their own active time, yielding a tradeoff between robustness and consistency.","We investigate the effect of using such a predictor in an equilibrium, as well as the new equilibria formed in this way."],"url":"http://arxiv.org/abs/2405.11873v1","category":"cs.GT"}
{"created":"2024-05-20 08:18:20","title":"Shrinking targets and recurrent behaviour for forward compositions of inner functions","abstract":"We prove sharp results about recurrent behaviour of orbits of forward compositions of inner functions, inspired by fundamental results about iterates of inner functions, and give examples to illustrate behaviours that cannot occur in the simpler case of iteration.   A result of Fern\\'andez, Meli\\'an and Pestana gives a precise version of the classical Poincar\\'e recurrence theorem for iterates of the boundary extension of an inner function that fixes~0. We generalise this to forward composition sequences $F_n=f_n\\circ \\dots\\circ f_1,$ $n\\in \\mathbb{N},$ where $f_n$ are inner functions that fix~0, giving conditions on the contraction of $(F_n)$ so that the radial boundary extension $F_n$ hits any shrinking target of arcs $(I_n)$ of a given size.   Next, Aaronson, and also Doering and Ma\\~n\\'e, gave a remarkable dichotomy for iterates of any inner function, showing that the behaviour of the boundary extension is of two entirely different types, depending on the size of the sequence $(|f^n(0)|)$. In earlier work, we showed that one part of this dichotomy holds in the non-autonomous setting of forward compositions.   It turns out that this dichotomy is closely related to the result of Fern\\'andez, Meli\\'an and Pestana, and here we show that a version of the second part of the dichotomy holds in the non-autonomous setting provided we impose a condition on the contraction of $(F_n)$ in relation to the size of the sequence $(|F_n(0)|)$. The techniques we use include a strong version of the second Borel--Cantelli lemma and strong mixing results of Pommerenke for contracting sequences of inner functions. We give examples to show that the contraction conditions that we need to impose in the non-autonomous setting are best possible.","sentences":["We prove sharp results about recurrent behaviour of orbits of forward compositions of inner functions, inspired by fundamental results about iterates of inner functions, and give examples to illustrate behaviours that cannot occur in the simpler case of iteration.   ","A result of Fern\\'andez, Meli\\'an and Pestana gives a precise version of the classical Poincar\\'e recurrence theorem for iterates of the boundary extension of an inner function that fixes~0.","We generalise this to forward composition sequences $F_n=f_n\\circ \\dots\\circ f_1,$ $n\\in \\mathbb{N},$ where $f_n$ are inner functions that fix~0, giving conditions on the contraction of $(F_n)$ so that the radial boundary extension $F_n$ hits any shrinking target of arcs $(I_n)$ of a given size.   ","Next, Aaronson, and also Doering and Ma\\~n\\'e, gave a remarkable dichotomy for iterates of any inner function, showing that the behaviour of the boundary extension is of two entirely different types, depending on the size of the sequence $(|f^n(0)|)$. In earlier work, we showed that one part of this dichotomy holds in the non-autonomous setting of forward compositions.   ","It turns out that this dichotomy is closely related to the result of Fern\\'andez, Meli\\'an and Pestana, and here we show that a version of the second part of the dichotomy holds in the non-autonomous setting provided we impose a condition on the contraction of $(F_n)$ in relation to the size of the sequence $(|F_n(0)|)$. The techniques we use include a strong version of the second Borel--Cantelli lemma and strong mixing results of Pommerenke for contracting sequences of inner functions.","We give examples to show that the contraction conditions that we need to impose in the non-autonomous setting are best possible."],"url":"http://arxiv.org/abs/2405.11866v1","category":"math.DS"}
{"created":"2024-05-20 08:14:40","title":"Unraveling the Spin Coulomb Drag Effect and Its Impact on Spin Transport in the Three-Dimensional Electron Gas","abstract":"The spin Coulomb drag effect, arising from the exchange of momentum between electrons of opposite spins, plays a crucial role in the spin transport of interacting electron systems. This effect leads to the emergence of a spin mass and the finite lifetime of spin currents, posing challenges for the accurate description of spin dynamics. Using the state-of-the-art Variational Diagrammatic Monte Carlo approach, we investigate the spin-resolved exchange-correlation (XC) kernel in the three-dimensional uniform electron gas. Our analysis reveals a distinct nature in the spin response, characterized by a $1/q^2$ divergence in the spin XC kernel at finite frequencies. This so-called ultranonlocal behavior, stemming from the spin Coulomb drag effect, is absent in the charge channel. By extracting precise values for the spin mass enhancement factor, we observe a trend of increasing enhancement with decreasing electron density. Furthermore, we find compelling evidence for the suppression of spin diffusion at low temperatures, characterized by vanishing inverse relaxation times. These findings deepen our understanding of the intricate relation between the Coulomb interaction and the spin transport, providing valuable insights for the development of accurate density functional approximations and the advancement of spintronics and quantum technologies.","sentences":["The spin Coulomb drag effect, arising from the exchange of momentum between electrons of opposite spins, plays a crucial role in the spin transport of interacting electron systems.","This effect leads to the emergence of a spin mass and the finite lifetime of spin currents, posing challenges for the accurate description of spin dynamics.","Using the state-of-the-art Variational Diagrammatic Monte Carlo approach, we investigate the spin-resolved exchange-correlation (XC) kernel in the three-dimensional uniform electron gas.","Our analysis reveals a distinct nature in the spin response, characterized by a $1/q^2$ divergence in the spin XC kernel at finite frequencies.","This so-called ultranonlocal behavior, stemming from the spin Coulomb drag effect, is absent in the charge channel.","By extracting precise values for the spin mass enhancement factor, we observe a trend of increasing enhancement with decreasing electron density.","Furthermore, we find compelling evidence for the suppression of spin diffusion at low temperatures, characterized by vanishing inverse relaxation times.","These findings deepen our understanding of the intricate relation between the Coulomb interaction and the spin transport, providing valuable insights for the development of accurate density functional approximations and the advancement of spintronics and quantum technologies."],"url":"http://arxiv.org/abs/2405.11864v1","category":"cond-mat.str-el"}
{"created":"2024-05-20 08:14:11","title":"Permittivity Characterization of Human Skin Based on a Quasi-optical System at Sub-THz","abstract":"This paper introduces a novel approach to experimentally characterize effective human skin permittivity at sub-Terahertz (sub-THz) frequencies, specifically from $140$~to $210$~GHz, utilizing a quasi-optical measurement system. To ensure accurate measurement of the reflection coefficients of human skin, a planar, rigid, and thick reference plate with a low-loss dielectric is utilized to flatten the human skin surface. A permittivity characterization method is proposed to reduce permittivity estimation deviations resulting from the pressure effects on the phase displacements of skins under the measurements but also to ensure repeatability of the measurement. In practical permittivity characterizations, the complex permittivities of the finger, palm, and arm of seven volunteers show small standard deviations for the repeated measurements, respectively, while those show significant variations across different regions of the skins and for different persons. The proposed measurement system holds significant potential for future skin permittivity estimation in sub-THz bands, facilitating further studies on human-electromagnetic-wave interactions based on the measured permittivity values.","sentences":["This paper introduces a novel approach to experimentally characterize effective human skin permittivity at sub-Terahertz (sub-THz) frequencies, specifically from $140$~to $210$~GHz, utilizing a quasi-optical measurement system.","To ensure accurate measurement of the reflection coefficients of human skin, a planar, rigid, and thick reference plate with a low-loss dielectric is utilized to flatten the human skin surface.","A permittivity characterization method is proposed to reduce permittivity estimation deviations resulting from the pressure effects on the phase displacements of skins under the measurements but also to ensure repeatability of the measurement.","In practical permittivity characterizations, the complex permittivities of the finger, palm, and arm of seven volunteers show small standard deviations for the repeated measurements, respectively, while those show significant variations across different regions of the skins and for different persons.","The proposed measurement system holds significant potential for future skin permittivity estimation in sub-THz bands, facilitating further studies on human-electromagnetic-wave interactions based on the measured permittivity values."],"url":"http://arxiv.org/abs/2405.11863v1","category":"physics.med-ph"}
{"created":"2024-05-20 08:10:59","title":"Separability and lower bounds of quantum entanglement based on realignment","abstract":"The detection and estimation of quantum entanglement are the essential issues in the theory of quantum entanglement. We construct matrices based on the realignment of density matrices and the vectorization of the reduced density matrices, from which a family of separability criteria are presented for both bipartite and multipartite systems. Moreover, new lower bounds of concurrence and convex-roof extended negativity are derived. Criteria are also given to detect the genuine tripartite entanglement. Lower bounds of the concurrence of genuine tripartite entanglement are presented. By detailed examples we show that our results are better than the corresponding ones in identifying and estimating quantum entanglement as well as genuine multipartite entanglement.","sentences":["The detection and estimation of quantum entanglement are the essential issues in the theory of quantum entanglement.","We construct matrices based on the realignment of density matrices and the vectorization of the reduced density matrices, from which a family of separability criteria are presented for both bipartite and multipartite systems.","Moreover, new lower bounds of concurrence and convex-roof extended negativity are derived.","Criteria are also given to detect the genuine tripartite entanglement.","Lower bounds of the concurrence of genuine tripartite entanglement are presented.","By detailed examples we show that our results are better than the corresponding ones in identifying and estimating quantum entanglement as well as genuine multipartite entanglement."],"url":"http://arxiv.org/abs/2405.11861v1","category":"quant-ph"}
{"created":"2024-05-20 08:06:47","title":"Fourier-Mukai transforms and normalization of nodal curves","abstract":"In this article we study the relation between Arinkin's Poincar\\'e sheaf on the compactified Jacobian over an integral nodal curve and the classical Poincar\\'e bundle on the Jacobian of the normalization. This is applied to derive a formula for the Fourier-Mukai transform of complexes on the compactified Jacobian that come from pushforward along the normalization map. This is achieved via a third intermediate geometry: the parabolic modules introduced by Bhosle and Cook, which enode encode fiber jumps at the nodes.","sentences":["In this article we study the relation between Arinkin's Poincar\\'e sheaf on the compactified Jacobian over an integral nodal curve and the classical Poincar\\'e bundle on the Jacobian of the normalization.","This is applied to derive a formula for the Fourier-Mukai transform of complexes on the compactified Jacobian that come from pushforward along the normalization map.","This is achieved via a third intermediate geometry: the parabolic modules introduced by Bhosle and Cook, which enode encode fiber jumps at the nodes."],"url":"http://arxiv.org/abs/2405.11860v1","category":"math.AG"}
{"created":"2024-05-20 08:00:25","title":"Modeling and simulation of a mechanism for suppressing the flipping problem of a jumping robot","abstract":"In order to solve the problem of stable jumping of micro robot, we design a special mechanism: elastic passive joint (EPJ). EPJ can assist in achieving smooth jumping through the opening-closing process when the robot jumps. First, we introduce the composition and operation principle of EPJ, and perform a dynamic modeling of the robot's jumping process. Then, in order to verify the effectiveness of EPJ in controlling the robot's smooth jump, we design a simulation experiment based on MATLAB. Through comparative experiments, it was proved that EPJ can greatly adjust the angular velocity of the robot and increase the jump distance of the robot. Finally, we analyze each parameter in EPJ and performs parameter optimization. After optimization, EPJ achieves a completely flip-free jump of the robot, laying an important foundation for improving the mobility of micro-robot.","sentences":["In order to solve the problem of stable jumping of micro robot, we design a special mechanism: elastic passive joint (EPJ).","EPJ can assist in achieving smooth jumping through the opening-closing process when the robot jumps.","First, we introduce the composition and operation principle of EPJ, and perform a dynamic modeling of the robot's jumping process.","Then, in order to verify the effectiveness of EPJ in controlling the robot's smooth jump, we design a simulation experiment based on MATLAB.","Through comparative experiments, it was proved that EPJ can greatly adjust the angular velocity of the robot and increase the jump distance of the robot.","Finally, we analyze each parameter in EPJ and performs parameter optimization.","After optimization, EPJ achieves a completely flip-free jump of the robot, laying an important foundation for improving the mobility of micro-robot."],"url":"http://arxiv.org/abs/2405.11856v1","category":"cs.RO"}
{"created":"2024-05-20 07:58:26","title":"Salience-guided Ground Factor for Robust Localization of Delivery Robots in Complex Urban Environments","abstract":"In urban environments for delivery robots, particularly in areas such as campuses and towns, many custom features defy standard road semantic categorizations. Addressing this challenge, our paper introduces a method leveraging Salient Object Detection (SOD) to extract these unique features, employing them as pivotal factors for enhanced robot loop closure and localization. Traditional geometric feature-based localization is hampered by fluctuating illumination and appearance changes. Our preference for SOD over semantic segmentation sidesteps the intricacies of classifying a myriad of non-standardized urban features. To achieve consistent ground features, the Motion Compensate IPM (MC-IPM) technique is implemented, capitalizing on motion for distortion compensation and subsequently selecting the most pertinent salient ground features through moment computations. For thorough evaluation, we validated the saliency detection and localization performances to the real urban scenarios. Project page: https://sites.google.com/view/salient-ground-feature/home.","sentences":["In urban environments for delivery robots, particularly in areas such as campuses and towns, many custom features defy standard road semantic categorizations.","Addressing this challenge, our paper introduces a method leveraging Salient Object Detection (SOD) to extract these unique features, employing them as pivotal factors for enhanced robot loop closure and localization.","Traditional geometric feature-based localization is hampered by fluctuating illumination and appearance changes.","Our preference for SOD over semantic segmentation sidesteps the intricacies of classifying a myriad of non-standardized urban features.","To achieve consistent ground features, the Motion Compensate IPM (MC-IPM) technique is implemented, capitalizing on motion for distortion compensation and subsequently selecting the most pertinent salient ground features through moment computations.","For thorough evaluation, we validated the saliency detection and localization performances to the real urban scenarios.","Project page: https://sites.google.com/view/salient-ground-feature/home."],"url":"http://arxiv.org/abs/2405.11855v1","category":"cs.RO"}
{"created":"2024-05-20 07:50:51","title":"Jumping Automata Must Pay","abstract":"Jumping automata are finite automata that read their input in a non-sequential manner, by allowing a reading head to ``jump'' between positions on the input, consuming a permutation of the input word. We argue that allowing the head to jump should incur some cost. To this end, we propose three quantitative semantics for jumping automata, whereby the jumps of the head in an accepting run define the cost of the run. The three semantics correspond to different interpretations of jumps: the \\emph{absolute distance} semantics counts the distance the head jumps, the \\emph{reversal} semantics counts the number of times the head changes direction, and the \\emph{Hamming distance} measures the number of letter-swaps the run makes.   We study these measures, with the main focus being the \\emph{boundedness problem}: given a jumping automaton, decide whether its (quantitative) languages is bounded by some given number $k$. We establish the decidability and complexity for this problem under several variants.","sentences":["Jumping automata are finite automata that read their input in a non-sequential manner, by allowing a reading head to ``jump'' between positions on the input, consuming a permutation of the input word.","We argue that allowing the head to jump should incur some cost.","To this end, we propose three quantitative semantics for jumping automata, whereby the jumps of the head in an accepting run define the cost of the run.","The three semantics correspond to different interpretations of jumps: the \\emph{absolute distance} semantics counts the distance the head jumps, the \\emph{reversal} semantics counts the number of times the head changes direction, and the \\emph{Hamming distance} measures the number of letter-swaps the run makes.   ","We study these measures, with the main focus being the \\emph{boundedness problem}: given a jumping automaton, decide whether its (quantitative) languages is bounded by some given number $k$.","We establish the decidability and complexity for this problem under several variants."],"url":"http://arxiv.org/abs/2405.11849v1","category":"cs.FL"}
{"created":"2024-05-20 07:37:06","title":"Investigation of superhumps in SU UMa-type dwarf novae based on the observations of TESS","abstract":"We report the superhumps analysis of seven SU UMa-type dwarf novae based on the observations of Transiting Exoplanet Survey Satellite (TESS). Superhumps are seen during superoutbursts of SU UMa-type dwarf novae. The month-long data sets of TESS are well suited for studying the variation of superhumps. We selected seven non-eclipsing SU UMa-type dwarf novae with superhumps in which TESS light curves are available and have not yet been studied. The stages A, B, and C of superhumps in superoutbursts were determined by O-C method. The results indicate that not all complete superoutbursts show obvious three stages, such as DT Oct and the second superoutburst of J1730+6247. We calculated the superhump periods for each stage and the mean periods for whole superoutbursts. Taking the stage A superhump method, the mass ratios (M2/M1) were estimated. According to the results, the seven stars are pre-bounce systems with mass ratios ranging from 0.1 to 0.2. By combining the orbital periods and the mean superhump periods, the precession periods were calculated. The results show that the precession periods of the seven SU UMa stars are about 2 days.","sentences":["We report the superhumps analysis of seven SU UMa-type dwarf novae based on the observations of Transiting Exoplanet Survey Satellite (TESS).","Superhumps are seen during superoutbursts of SU UMa-type dwarf novae.","The month-long data sets of TESS are well suited for studying the variation of superhumps.","We selected seven non-eclipsing SU UMa-type dwarf novae with superhumps in which TESS light curves are available and have not yet been studied.","The stages A, B, and C of superhumps in superoutbursts were determined by O-C method.","The results indicate that not all complete superoutbursts show obvious three stages, such as DT Oct and the second superoutburst of J1730+6247.","We calculated the superhump periods for each stage and the mean periods for whole superoutbursts.","Taking the stage A superhump method, the mass ratios (M2/M1) were estimated.","According to the results, the seven stars are pre-bounce systems with mass ratios ranging from 0.1 to 0.2.","By combining the orbital periods and the mean superhump periods, the precession periods were calculated.","The results show that the precession periods of the seven SU UMa stars are about 2 days."],"url":"http://arxiv.org/abs/2405.11843v1","category":"astro-ph.SR"}
{"created":"2024-05-20 07:31:56","title":"Highly charged ions of heavy actinides as sensitive probes for time variation of the fine structure constant","abstract":"Highly charged ions of heavy actinides from uranium to einsteinium are studied theoretically to find optical transitions sensitive to the variation of the fine structure constant. A number of promising transitions have been found in ions with ionisation degree $\\sim$~10. All these transitions correspond in single-electron approximation to the $6p$ - $5f$ transitions. Many of the transitions are between ground and excited metastable states of the ions which means that they can probably be used as optical clock transitions. Some of the ions have more than one clock transition with different sensitivity to the variation of the fine structure constant $\\alpha$. The most promising systems include the Np$^{10+}$, Np$^{9+}$, Pu$^{11+}$, Pu$^{10+}$, Pu$^{9+}$, Pu$^{8+}$, Bk$^{15+}$, Cm$^{12+}$, and Es$^{15+}$ ions.","sentences":["Highly charged ions of heavy actinides from uranium to einsteinium are studied theoretically to find optical transitions sensitive to the variation of the fine structure constant.","A number of promising transitions have been found in ions with ionisation degree $\\sim$~10.","All these transitions correspond in single-electron approximation to the $6p$ - $5f$ transitions.","Many of the transitions are between ground and excited metastable states of the ions which means that they can probably be used as optical clock transitions.","Some of the ions have more than one clock transition with different sensitivity to the variation of the fine structure constant $\\alpha$. The most promising systems include the Np$^{10+}$, Np$^{9+}$, Pu$^{11+}$, Pu$^{10+}$, Pu$^{9+}$, Pu$^{8+}$, Bk$^{15+}$, Cm$^{12+}$, and Es$^{15+}$ ions."],"url":"http://arxiv.org/abs/2405.11839v1","category":"physics.atom-ph"}
{"created":"2024-05-20 06:53:55","title":"Federated Learning with Incomplete Sensing Modalities","abstract":"Many mobile sensing applications utilize data from various modalities, including motion and physiological sensors in mobile and wearable devices. Federated Learning (FL) is particularly suitable for these applications thanks to its privacy-preserving feature. However, challenges such as limited battery life, poor network conditions, and sensor malfunctions can restrict the use of all available modalities for local model training. Additionally, existing multimodal FL systems also struggle with scalability and efficiency as the number of modality sources increases. To address these issues, we introduce FLISM, a framework designed to enable multimodal FL with incomplete modalities. FLISM leverages simulation technique to learn robust representations that can handle missing modalities and transfers model knowledge across clients with varying set of modalities. The evaluation results using three real-world datasets and simulations demonstrate FLISM's effective balance between model performance and system efficiency. It shows an average improvement of .067 in F1-score, while also reducing communication (2.69x faster) and computational (2.28x more efficient) overheads compared to existing methods addressing incomplete modalities. Moreover, in simulated scenarios involving tasks with a larger number of modalities, FLISM achieves a significant speedup of 3.23x~85.10x in communication and 3.73x~32.29x in computational efficiency.","sentences":["Many mobile sensing applications utilize data from various modalities, including motion and physiological sensors in mobile and wearable devices.","Federated Learning (FL) is particularly suitable for these applications thanks to its privacy-preserving feature.","However, challenges such as limited battery life, poor network conditions, and sensor malfunctions can restrict the use of all available modalities for local model training.","Additionally, existing multimodal FL systems also struggle with scalability and efficiency as the number of modality sources increases.","To address these issues, we introduce FLISM, a framework designed to enable multimodal FL with incomplete modalities.","FLISM leverages simulation technique to learn robust representations that can handle missing modalities and transfers model knowledge across clients with varying set of modalities.","The evaluation results using three real-world datasets and simulations demonstrate FLISM's effective balance between model performance and system efficiency.","It shows an average improvement of .067 in F1-score, while also reducing communication (2.69x faster) and computational (2.28x more efficient) overheads compared to existing methods addressing incomplete modalities.","Moreover, in simulated scenarios involving tasks with a larger number of modalities, FLISM achieves a significant speedup of 3.23x~85.10x in communication and 3.73x~32.29x in computational efficiency."],"url":"http://arxiv.org/abs/2405.11828v1","category":"cs.LG"}
{"created":"2024-05-20 06:43:26","title":"Measuring Technical Debt in AI-Based Competition Platforms","abstract":"Advances in AI have led to new types of technical debt in software engineering projects. AI-based competition platforms face challenges due to rapid prototyping and a lack of adherence to software engineering principles by participants, resulting in technical debt. Additionally, organizers often lack methods to evaluate platform quality, impacting sustainability and maintainability. In this research, we identify and categorize types of technical debt in AI systems through a scoping review. We develop a questionnaire for assessing technical debt in AI competition platforms, categorizing debt into various types, such as algorithm, architectural, code, configuration, data etc. We introduce Accessibility Debt, specific to AI competition platforms, highlighting challenges participants face due to inadequate platform usability. Our framework for managing technical debt aims to improve the sustainability and effectiveness of these platforms, providing tools for researchers, organizers, and participants.","sentences":["Advances in AI have led to new types of technical debt in software engineering projects.","AI-based competition platforms face challenges due to rapid prototyping and a lack of adherence to software engineering principles by participants, resulting in technical debt.","Additionally, organizers often lack methods to evaluate platform quality, impacting sustainability and maintainability.","In this research, we identify and categorize types of technical debt in AI systems through a scoping review.","We develop a questionnaire for assessing technical debt in AI competition platforms, categorizing debt into various types, such as algorithm, architectural, code, configuration, data etc.","We introduce Accessibility Debt, specific to AI competition platforms, highlighting challenges participants face due to inadequate platform usability.","Our framework for managing technical debt aims to improve the sustainability and effectiveness of these platforms, providing tools for researchers, organizers, and participants."],"url":"http://arxiv.org/abs/2405.11825v1","category":"cs.SE"}
{"created":"2024-05-20 06:33:50","title":"FeTT: Continual Class Incremental Learning via Feature Transformation Tuning","abstract":"Continual learning (CL) aims to extend deep models from static and enclosed environments to dynamic and complex scenarios, enabling systems to continuously acquire new knowledge of novel categories without forgetting previously learned knowledge. Recent CL models have gradually shifted towards the utilization of pre-trained models (PTMs) with parameter-efficient fine-tuning (PEFT) strategies. However, continual fine-tuning still presents a serious challenge of catastrophic forgetting due to the absence of previous task data. Additionally, the fine-tune-then-frozen mechanism suffers from performance limitations due to feature channels suppression and insufficient training data in the first CL task. To this end, this paper proposes feature transformation tuning (FeTT) model to non-parametrically fine-tune backbone features across all tasks, which not only operates independently of CL training data but also smooths feature channels to prevent excessive suppression. Then, the extended ensemble strategy incorporating different PTMs with FeTT model facilitates further performance improvement. We further elaborate on the discussions of the fine-tune-then-frozen paradigm and the FeTT model from the perspectives of discrepancy in class marginal distributions and feature channels. Extensive experiments on CL benchmarks validate the effectiveness of our proposed method.","sentences":["Continual learning (CL) aims to extend deep models from static and enclosed environments to dynamic and complex scenarios, enabling systems to continuously acquire new knowledge of novel categories without forgetting previously learned knowledge.","Recent CL models have gradually shifted towards the utilization of pre-trained models (PTMs) with parameter-efficient fine-tuning (PEFT) strategies.","However, continual fine-tuning still presents a serious challenge of catastrophic forgetting due to the absence of previous task data.","Additionally, the fine-tune-then-frozen mechanism suffers from performance limitations due to feature channels suppression and insufficient training data in the first CL task.","To this end, this paper proposes feature transformation tuning (FeTT) model to non-parametrically fine-tune backbone features across all tasks, which not only operates independently of CL training data but also smooths feature channels to prevent excessive suppression.","Then, the extended ensemble strategy incorporating different PTMs with FeTT model facilitates further performance improvement.","We further elaborate on the discussions of the fine-tune-then-frozen paradigm and the FeTT model from the perspectives of discrepancy in class marginal distributions and feature channels.","Extensive experiments on CL benchmarks validate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2405.11822v1","category":"cs.CV"}
{"created":"2024-05-20 06:28:43","title":"Beyond MLE: Investigating SEARNN for Low-Resourced Neural Machine Translation","abstract":"Structured prediction tasks, like machine translation, involve learning functions that map structured inputs to structured outputs. Recurrent Neural Networks (RNNs) have historically been a popular choice for such tasks, including in natural language processing (NLP) applications. However, training RNNs using Maximum Likelihood Estimation (MLE) has its limitations, including exposure bias and a mismatch between training and testing metrics. SEARNN, based on the learning to search (L2S) framework, has been proposed as an alternative to MLE for RNN training. This project explored the potential of SEARNN to improve machine translation for low-resourced African languages -- a challenging task characterized by limited training data availability and the morphological complexity of the languages. Through experiments conducted on translation for English to Igbo, French to \\ewe, and French to \\ghomala directions, this project evaluated the efficacy of SEARNN over MLE in addressing the unique challenges posed by these languages. With an average BLEU score improvement of $5.4$\\% over the MLE objective, we proved that SEARNN is indeed a viable algorithm to effectively train RNNs on machine translation for low-resourced languages.","sentences":["Structured prediction tasks, like machine translation, involve learning functions that map structured inputs to structured outputs.","Recurrent Neural Networks (RNNs) have historically been a popular choice for such tasks, including in natural language processing (NLP) applications.","However, training RNNs using Maximum Likelihood Estimation (MLE) has its limitations, including exposure bias and a mismatch between training and testing metrics.","SEARNN, based on the learning to search (L2S) framework, has been proposed as an alternative to MLE for RNN training.","This project explored the potential of SEARNN to improve machine translation for low-resourced African languages -- a challenging task characterized by limited training data availability and the morphological complexity of the languages.","Through experiments conducted on translation for English to Igbo, French to \\ewe, and French to \\ghomala directions, this project evaluated the efficacy of SEARNN over MLE in addressing the unique challenges posed by these languages.","With an average BLEU score improvement of $5.4$\\% over the MLE objective, we proved that SEARNN is indeed a viable algorithm to effectively train RNNs on machine translation for low-resourced languages."],"url":"http://arxiv.org/abs/2405.11819v1","category":"cs.CL"}
{"created":"2024-05-20 06:25:39","title":"Systematic Review on Healthcare Systems Engineering utilizing ChatGPT","abstract":"This paper presents an analytical framework for conducting academic reviews in the field of Healthcare Systems Engineering, employing ChatGPT, a state-of-the-art tool among recent language models. We utilized 9,809 abstract paragraphs from conference presentations to systematically review the field. The framework comprises distinct analytical processes, each employing tailored prompts and the systematic use of the ChatGPT API. Through this framework, we organized the target field into 11 topic categories and conducted a comprehensive analysis covering quantitative yearly trends and detailed sub-categories. This effort explores the potential for leveraging ChatGPT to alleviate the burden of academic reviews. Furthermore, it provides valuable insights into the dynamic landscape of Healthcare Systems Engineering research.","sentences":["This paper presents an analytical framework for conducting academic reviews in the field of Healthcare Systems Engineering, employing ChatGPT, a state-of-the-art tool among recent language models.","We utilized 9,809 abstract paragraphs from conference presentations to systematically review the field.","The framework comprises distinct analytical processes, each employing tailored prompts and the systematic use of the ChatGPT API.","Through this framework, we organized the target field into 11 topic categories and conducted a comprehensive analysis covering quantitative yearly trends and detailed sub-categories.","This effort explores the potential for leveraging ChatGPT to alleviate the burden of academic reviews.","Furthermore, it provides valuable insights into the dynamic landscape of Healthcare Systems Engineering research."],"url":"http://arxiv.org/abs/2405.11817v1","category":"cs.ET"}
{"created":"2024-05-20 06:15:25","title":"Nonlinear Lindblad Master Equation and Postselected Skin Effect","abstract":"We introduce a non-linear Lindblad master equation to describe the postselection dynamics of open quantum systems described by the Lindblad master equation, which continuously interpolates between the Lindblad master equation and the dynamical equation governed by an effective non-Hermitian Hamiltonian. Within the framework of the non-linear Lindblad master equation, we study a prototypical model and demonstrate the existence of the postselected skin effect with the distribution of a steady state characterized by the accumulation of particles on one side, as long as some quantum jumping terms are discarded by postselction processes. Moreover, we show that the trajectory-averaged entanglement entropy can reflect the different influences from the environment and postselection, and unveil it exhibiting a special distribution with algebraic growth in the short chain and saturation in the long chain induced by the postselected skin effect.","sentences":["We introduce a non-linear Lindblad master equation to describe the postselection dynamics of open quantum systems described by the Lindblad master equation, which continuously interpolates between the Lindblad master equation and the dynamical equation governed by an effective non-Hermitian Hamiltonian.","Within the framework of the non-linear Lindblad master equation, we study a prototypical model and demonstrate the existence of the postselected skin effect with the distribution of a steady state characterized by the accumulation of particles on one side, as long as some quantum jumping terms are discarded by postselction processes.","Moreover, we show that the trajectory-averaged entanglement entropy can reflect the different influences from the environment and postselection, and unveil it exhibiting a special distribution with algebraic growth in the short chain and saturation in the long chain induced by the postselected skin effect."],"url":"http://arxiv.org/abs/2405.11812v1","category":"quant-ph"}
{"created":"2024-05-20 06:02:13","title":"Online Learning Feedback Control Considering Hysteresis for Musculoskeletal Structures","abstract":"While the musculoskeletal humanoid has various biomimetic benefits, its complex modeling is difficult, and many learning control methods have been developed. However, for the actual robot, the hysteresis of its joint angle tracking is still an obstacle, and realizing target posture quickly and accurately has been difficult. Therefore, we develop a feedback control method considering the hysteresis. To solve the problem in feedback controls caused by the closed-link structure of the musculoskeletal body, we update a neural network representing the relationship between the error of joint angles and the change in target muscle lengths online, and realize target joint angles accurately in a few trials. We compare the performance of several configurations with various network structures and loss definitions, and verify the effectiveness of this study on an actual musculoskeletal humanoid, Musashi.","sentences":["While the musculoskeletal humanoid has various biomimetic benefits, its complex modeling is difficult, and many learning control methods have been developed.","However, for the actual robot, the hysteresis of its joint angle tracking is still an obstacle, and realizing target posture quickly and accurately has been difficult.","Therefore, we develop a feedback control method considering the hysteresis.","To solve the problem in feedback controls caused by the closed-link structure of the musculoskeletal body, we update a neural network representing the relationship between the error of joint angles and the change in target muscle lengths online, and realize target joint angles accurately in a few trials.","We compare the performance of several configurations with various network structures and loss definitions, and verify the effectiveness of this study on an actual musculoskeletal humanoid, Musashi."],"url":"http://arxiv.org/abs/2405.11808v1","category":"cs.RO"}
{"created":"2024-05-20 06:00:12","title":"Dual-sided Peltier Elements for Rapid Thermal Feedback in Wearables","abstract":"This paper introduces a motor-driven Peltier device designed to deliver immediate thermal sensations within extended reality (XR) environments. The system incorporates eight motor-driven Peltier elements, facilitating swift transitions between warm and cool sensations by rotating preheated or cooled elements to opposite sides. A multi-layer structure, comprising aluminum and silicone layers, ensures user comfort and safety while maintaining optimal temperatures for thermal stimuli. Time-temperature characteristic analysis demonstrates the system's ability to provide warm and cool sensations efficiently, with a dual-sided lifetime of up to 206 seconds at a 2V input. Our system design is adaptable to various body parts and can be synchronized with corresponding visual stimuli to enhance the immersive sensation of virtual object interaction and information delivery.","sentences":["This paper introduces a motor-driven Peltier device designed to deliver immediate thermal sensations within extended reality (XR) environments.","The system incorporates eight motor-driven Peltier elements, facilitating swift transitions between warm and cool sensations by rotating preheated or cooled elements to opposite sides.","A multi-layer structure, comprising aluminum and silicone layers, ensures user comfort and safety while maintaining optimal temperatures for thermal stimuli.","Time-temperature characteristic analysis demonstrates the system's ability to provide warm and cool sensations efficiently, with a dual-sided lifetime of up to 206 seconds at a 2V input.","Our system design is adaptable to various body parts and can be synchronized with corresponding visual stimuli to enhance the immersive sensation of virtual object interaction and information delivery."],"url":"http://arxiv.org/abs/2405.11807v1","category":"cs.HC"}
{"created":"2024-05-20 05:56:02","title":"Global stability and period-doubling bifurcations of a discrete Kolmogorov predator-prey model with Ricker-type prey growth","abstract":"In this paper, we study the dynamics of a discrete Kolmogorov predator-prey model with Ricker-type prey growth. We give the sufficient and necessary condition to guarantee the existence and uniqueness of the positive fixed point. Using the center manifold theory, we prove that the period-doubling bifurcations can occur at the positive fixed point. Furthermore, our numerical simulations reveal that the model can exhibit cascades of period-doubling bifurcations leading to chaos, which is a significant difference from the behavior of continuous predator-prey models. Despite the complexities of the model dynamics, we are able to provide a criterion for the global stability of the positive fixed point by using a geometric analysis of the nullclines.","sentences":["In this paper, we study the dynamics of a discrete Kolmogorov predator-prey model with Ricker-type prey growth.","We give the sufficient and necessary condition to guarantee the existence and uniqueness of the positive fixed point.","Using the center manifold theory, we prove that the period-doubling bifurcations can occur at the positive fixed point.","Furthermore, our numerical simulations reveal that the model can exhibit cascades of period-doubling bifurcations leading to chaos, which is a significant difference from the behavior of continuous predator-prey models.","Despite the complexities of the model dynamics, we are able to provide a criterion for the global stability of the positive fixed point by using a geometric analysis of the nullclines."],"url":"http://arxiv.org/abs/2405.11806v1","category":"math.DS"}
{"created":"2024-05-20 05:39:52","title":"Minimal frequently stable is almost automorphic","abstract":"We show that a minimal toplogical dynamical system that is frequently stable if and only if it is almost automorphic.","sentences":["We show that a minimal toplogical dynamical system that is frequently stable if and only if it is almost automorphic."],"url":"http://arxiv.org/abs/2405.11797v1","category":"math.DS"}
{"created":"2024-05-20 05:14:12","title":"The Grandparent Scam: A Systems Perspective Case Study On Elder Fraud And The Concept Of Human Layering","abstract":"In April 2024, an 81-year-old Ohio man was charged with murder, assault, and kidnapping. The man believed that he was protecting his family from scammers threatening harm. What he did not realize was that the 61-year-old Uber driver he killed, was also a victim of the same scammers. This case study examines some common variants of the Grandparent Scam from a systems perspective and how weaponization of conscience is used in these scams. Additionally, this study examines the parallels between layering in money laundering and human layering in the execution of these scams.","sentences":["In April 2024, an 81-year-old Ohio man was charged with murder, assault, and kidnapping.","The man believed that he was protecting his family from scammers threatening harm.","What he did not realize was that the 61-year-old Uber driver he killed, was also a victim of the same scammers.","This case study examines some common variants of the Grandparent Scam from a systems perspective and how weaponization of conscience is used in these scams.","Additionally, this study examines the parallels between layering in money laundering and human layering in the execution of these scams."],"url":"http://arxiv.org/abs/2405.11789v1","category":"cs.CY"}
{"created":"2024-05-20 05:11:02","title":"TinyLLaVA Factory: A Modularized Codebase for Small-scale Large Multimodal Models","abstract":"We present TinyLLaVA Factory, an open-source modular codebase for small-scale large multimodal models (LMMs) with a focus on simplicity of code implementations, extensibility of new features, and reproducibility of training results. Following the design philosophy of the factory pattern in software engineering, TinyLLaVA Factory modularizes the entire system into interchangeable components, with each component integrating a suite of cutting-edge models and methods, meanwhile leaving room for extensions to more features. In addition to allowing users to customize their own LMMs, TinyLLaVA Factory provides popular training recipes to let users pretrain and finetune their models with less coding effort. Empirical experiments validate the effectiveness of our codebase. The goal of TinyLLaVA Factory is to assist researchers and practitioners in exploring the wide landscape of designing and training small-scale LMMs with affordable computational resources.","sentences":["We present TinyLLaVA Factory, an open-source modular codebase for small-scale large multimodal models (LMMs) with a focus on simplicity of code implementations, extensibility of new features, and reproducibility of training results.","Following the design philosophy of the factory pattern in software engineering, TinyLLaVA Factory modularizes the entire system into interchangeable components, with each component integrating a suite of cutting-edge models and methods, meanwhile leaving room for extensions to more features.","In addition to allowing users to customize their own LMMs, TinyLLaVA Factory provides popular training recipes to let users pretrain and finetune their models with less coding effort.","Empirical experiments validate the effectiveness of our codebase.","The goal of TinyLLaVA Factory is to assist researchers and practitioners in exploring the wide landscape of designing and training small-scale LMMs with affordable computational resources."],"url":"http://arxiv.org/abs/2405.11788v1","category":"cs.LG"}
{"created":"2024-05-20 05:09:36","title":"Enhanced dissipation and stability of Poiseuille flow for two-dimensional Boussinesq system","abstract":"We investigate the nonlinear stability problem for the two-dimensional Boussinesq system around the Poiseuille flow in a finite channel. The system has the characteristic of Navier-slip boundary condition for the velocity and Dirichlet boundary condition for the temperature, with a small viscosity $\\nu$ and small thermal diffusion $\\mu,$ respectively. More precisely, we prove that if the initial velocity and initial temperature satisfies$$||u_{0}-(1-y^2,0) ||_{H^{\\frac{7}{2}+}}\\leq c_0\\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{\\frac{2}{3}}$$ and $$ ||\\theta_{0}||_{H^1}+|||D_x|^{\\frac{1}{8}}\\theta_{0}||_{H^1}\\leq c_1\\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{{\\frac{31}{24}}}$$ for some small constants $c_0$ and $c_1$ which are both independent of $\\mu,\\nu$, then we can reach the conclusion that the velocity remains within $O\\left( \\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{\\frac{2}{3}}\\right) $ of the Poiseuille flow; the temperature remains $O\\left( \\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{\\frac{31}{24}}\\right) $ of the constant 0, and approaches to 0 as $t\\rightarrow\\infty.$","sentences":["We investigate the nonlinear stability problem for the two-dimensional Boussinesq system around the Poiseuille flow in a finite channel.","The system has the characteristic of Navier-slip boundary condition for the velocity and Dirichlet boundary condition for the temperature, with a small viscosity $\\nu$ and small thermal diffusion $\\mu,$ respectively.","More precisely, we prove that if the initial velocity and initial temperature satisfies$$||u_{0}-(1-y^2,0) ||_{H^{\\frac{7}{2}+}}\\leq c_0\\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{\\frac{2}{3}}$$ and $$ ||\\theta_{0}||_{H^1}+|||D_x|^{\\frac{1}{8}}\\theta_{0}||_{H^1}\\leq c_1\\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{{\\frac{31}{24}}}$$ for some small constants $c_0$ and $c_1$ which are both independent of $\\mu,\\nu$, then we can reach the conclusion that the velocity remains within $O\\left( \\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{\\frac{2}{3}}\\right) $ of the Poiseuille flow; the temperature remains $O\\left( \\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{\\frac{31}{24}}\\right) $ of the constant 0, and approaches to 0 as $t\\rightarrow\\infty.$"],"url":"http://arxiv.org/abs/2405.11787v1","category":"math.AP"}
{"created":"2024-05-20 04:17:11","title":"Half-dimensional immersions into the para-complex projective space and Ruh-Vilms type theorems","abstract":"In this paper we study isometric immersions $f:M^n \\to {\\mathbb {C}^{\\prime}}\\!P^n$ of an $n$-dimensional pseudo-Riemannian manifold $M^n$ into the $n$-dimensional para-complex projective space ${\\mathbb {C}^{\\prime}}\\!P^n$. We study the immersion $f$ by means of a lift $\\mathfrak f$ of $f$ into a quadric hypersurface in ${S^{2n+1}_{n+1}}$. We find the frame equations and compatibility conditions. We specialize these results to dimension $n = 2$ and a definite metric on $M^2$ in isothermal coordinates and consider the special cases of Lagrangian surface immersions and minimal surface immersions. We characterize surface immersions with special properties in terms of primitive harmonicity of the Gauss maps.","sentences":["In this paper we study isometric immersions $f:M^n \\to {\\mathbb {C}^{\\prime}}\\!P^n$ of an $n$-dimensional pseudo-Riemannian manifold $M^n$ into the $n$-dimensional para-complex projective space ${\\mathbb {C}^{\\prime}}\\!P^n$. We study the immersion $f$ by means of a lift $\\mathfrak f$ of $f$ into a quadric hypersurface in ${","S^{2n+1}_{n+1}}$. We find the frame equations and compatibility conditions.","We specialize these results to dimension $n = 2$ and a definite metric on $M^2$ in isothermal coordinates and consider the special cases of Lagrangian surface immersions and minimal surface immersions.","We characterize surface immersions with special properties in terms of primitive harmonicity of the Gauss maps."],"url":"http://arxiv.org/abs/2405.11771v1","category":"math.DG"}
{"created":"2024-05-20 03:47:12","title":"Modeling User Fatigue for Sequential Recommendation","abstract":"Recommender systems filter out information that meets user interests. However, users may be tired of the recommendations that are too similar to the content they have been exposed to in a short historical period, which is the so-called user fatigue. Despite the significance for a better user experience, user fatigue is seldom explored by existing recommenders. In fact, there are three main challenges to be addressed for modeling user fatigue, including what features support it, how it influences user interests, and how its explicit signals are obtained. In this paper, we propose to model user Fatigue in interest learning for sequential Recommendations (FRec). To address the first challenge, based on a multi-interest framework, we connect the target item with historical items and construct an interest-aware similarity matrix as features to support fatigue modeling. Regarding the second challenge, built upon feature cross, we propose a fatigue-enhanced multi-interest fusion to capture long-term interest. In addition, we develop a fatigue-gated recurrent unit for short-term interest learning, with temporal fatigue representations as important inputs for constructing update and reset gates. For the last challenge, we propose a novel sequence augmentation to obtain explicit fatigue signals for contrastive learning. We conduct extensive experiments on real-world datasets, including two public datasets and one large-scale industrial dataset. Experimental results show that FRec can improve AUC and GAUC up to 0.026 and 0.019 compared with state-of-the-art models, respectively. Moreover, large-scale online experiments demonstrate the effectiveness of FRec for fatigue reduction. Our codes are released at https://github.com/tsinghua-fib-lab/SIGIR24-FRec.","sentences":["Recommender systems filter out information that meets user interests.","However, users may be tired of the recommendations that are too similar to the content they have been exposed to in a short historical period, which is the so-called user fatigue.","Despite the significance for a better user experience, user fatigue is seldom explored by existing recommenders.","In fact, there are three main challenges to be addressed for modeling user fatigue, including what features support it, how it influences user interests, and how its explicit signals are obtained.","In this paper, we propose to model user Fatigue in interest learning for sequential Recommendations (FRec).","To address the first challenge, based on a multi-interest framework, we connect the target item with historical items and construct an interest-aware similarity matrix as features to support fatigue modeling.","Regarding the second challenge, built upon feature cross, we propose a fatigue-enhanced multi-interest fusion to capture long-term interest.","In addition, we develop a fatigue-gated recurrent unit for short-term interest learning, with temporal fatigue representations as important inputs for constructing update and reset gates.","For the last challenge, we propose a novel sequence augmentation to obtain explicit fatigue signals for contrastive learning.","We conduct extensive experiments on real-world datasets, including two public datasets and one large-scale industrial dataset.","Experimental results show that FRec can improve AUC and GAUC up to 0.026 and 0.019 compared with state-of-the-art models, respectively.","Moreover, large-scale online experiments demonstrate the effectiveness of FRec for fatigue reduction.","Our codes are released at https://github.com/tsinghua-fib-lab/SIGIR24-FRec."],"url":"http://arxiv.org/abs/2405.11764v1","category":"cs.IR"}
{"created":"2024-05-20 03:46:42","title":"Uncertainty of interpretability in Landslide Susceptibility Mapping: A Comparative Analysis of Statistical, Machine Learning, and Deep Learning Models","abstract":"Landslide susceptibility mapping (LSM) is crucial for identifying high-risk areas and informing prevention strategies. This study investigates the interpretability of statistical, machine learning (ML), and deep learning (DL) models in predicting landslide susceptibility. This is achieved by incorporating various relevant interpretation methods and two types of input factors: a comprehensive set of 19 contributing factors that are statistically relevant to landslides, as well as a dedicated set of 9 triggering factors directly associated with triggering landslides. Given that model performance is a crucial metric in LSM, our investigations into interpretability naturally involve assessing and comparing LSM accuracy across different models considered. In our investigation, the convolutional neural network model achieved the highest accuracy (0.8447 with 19 factors; 0.8048 with 9 factors), while Extreme Gradient Boosting and Support Vector Machine also demonstrated strong predictive capabilities, outperforming conventional statistical models. These findings indicate that DL and sophisticated ML algorithms can effectively capture the complex relationships between input factors and landslide occurrence. However, the interpretability of predictions varied among different models, particularly when using the broader set of 19 contributing factors. Explanation methods like SHAP, LIME, and DeepLIFT also led to variations in interpretation results. Using a comprehensive set of 19 contributing factors improved prediction accuracy but introduced complexities and inconsistency in model interpretations. Focusing on a dedicated set of 9 triggering factors sacrificed some predictive power but enhanced interpretability, as evidenced by more consistent key factors identified across various models and alignment with the findings of field investigation reports....","sentences":["Landslide susceptibility mapping (LSM) is crucial for identifying high-risk areas and informing prevention strategies.","This study investigates the interpretability of statistical, machine learning (ML), and deep learning (DL) models in predicting landslide susceptibility.","This is achieved by incorporating various relevant interpretation methods and two types of input factors: a comprehensive set of 19 contributing factors that are statistically relevant to landslides, as well as a dedicated set of 9 triggering factors directly associated with triggering landslides.","Given that model performance is a crucial metric in LSM, our investigations into interpretability naturally involve assessing and comparing LSM accuracy across different models considered.","In our investigation, the convolutional neural network model achieved the highest accuracy (0.8447 with 19 factors; 0.8048 with 9 factors), while Extreme Gradient Boosting and Support Vector Machine also demonstrated strong predictive capabilities, outperforming conventional statistical models.","These findings indicate that DL and sophisticated ML algorithms can effectively capture the complex relationships between input factors and landslide occurrence.","However, the interpretability of predictions varied among different models, particularly when using the broader set of 19 contributing factors.","Explanation methods like SHAP, LIME, and DeepLIFT also led to variations in interpretation results.","Using a comprehensive set of 19 contributing factors improved prediction accuracy but introduced complexities and inconsistency in model interpretations.","Focusing on a dedicated set of 9 triggering factors sacrificed some predictive power but enhanced interpretability, as evidenced by more consistent key factors identified across various models and alignment with the findings of field investigation reports...."],"url":"http://arxiv.org/abs/2405.11762v1","category":"cs.LG"}
{"created":"2024-05-20 03:20:30","title":"Keck/KCWI Spectroscopy of Globular Clusters in Local Volume Dwarf Galaxies","abstract":"A number of nearby dwarf galaxies have globular cluster (GC) candidates that require spectroscopic confirmation. Here we present Keck telescope spectra for 15 known GCs and GC candidates that may be associated with a host dwarf galaxy, and an additional 3 GCs in the halo of M31 that are candidates for accretion from a now disrupted dwarf galaxy. We confirm 6 star clusters (of intermediate-to-old age) to be associated with NGC~247. The vast bulk of its GC system remains to be studied spectroscopically. We also confirm the GC candidates in F8D1 and DDO190, finding both to be young star clusters. The 3 M31 halo GCs all have radial velocities consistent with M31, are old and very metal-poor. Their ages and metallicities are consistent with accretion from a low mass satellite galaxy. Finally, three objects are found to be background galaxies -- two are projected near NGC~247 and one (candidate GCC7) is near the IKN dwarf. The IKN dwarf thus has only 5 confirmed GCs but still a remarkable specific frequency of 124.","sentences":["A number of nearby dwarf galaxies have globular cluster (GC) candidates that require spectroscopic confirmation.","Here we present Keck telescope spectra for 15 known GCs and GC candidates that may be associated with a host dwarf galaxy, and an additional 3 GCs in the halo of M31 that are candidates for accretion from a now disrupted dwarf galaxy.","We confirm 6 star clusters (of intermediate-to-old age) to be associated with NGC~247.","The vast bulk of its GC system remains to be studied spectroscopically.","We also confirm the GC candidates in F8D1 and DDO190, finding both to be young star clusters.","The 3 M31 halo GCs all have radial velocities consistent with M31, are old and very metal-poor.","Their ages and metallicities are consistent with accretion from a low mass satellite galaxy.","Finally, three objects are found to be background galaxies -- two are projected near NGC~247 and one (candidate GCC7) is near the IKN dwarf.","The IKN dwarf thus has only 5 confirmed GCs but still a remarkable specific frequency of 124."],"url":"http://arxiv.org/abs/2405.11749v1","category":"astro-ph.GA"}
{"created":"2024-05-20 02:33:42","title":"Finite Field Multiple Access for Sourced Massive Random Access with Finite Blocklength","abstract":"For binary source transmission, this paper proposes an element-pair (EP) coding scheme for supporting sourced massive random access, which is used to solve the finite blocklength (FBL) of multiuser reliability transmission problem. In this paper, we first give the definition of an EP, which is used as a virtual resource. If the Cartesian product of $J$ distinct EPs satisfies the unique sum-pattern mapping (USPM) structural property, the $J$ distinct EPs can form an uniquely-decodable EP (UD-EP) code. Then, we introduce a type of orthogonal EP code $\\Psi_{\\rm o, B}$ constructed over an extension field GF($2^m$). Based on the proposed EP code, we present finite-field multiple-access (FFMA) systems, including both the sparse-form-based and diagonal-form-based forms. Simulation results show that, for the massive random access scenario, the error performance of the proposed FFMA systems over a Gaussian multiple-access channel can provide much better error performance than that of a slotted ALOHA system.","sentences":["For binary source transmission, this paper proposes an element-pair (EP) coding scheme for supporting sourced massive random access, which is used to solve the finite blocklength (FBL) of multiuser reliability transmission problem.","In this paper, we first give the definition of an EP, which is used as a virtual resource.","If the Cartesian product of $J$ distinct EPs satisfies the unique sum-pattern mapping (USPM) structural property, the $J$ distinct EPs can form an uniquely-decodable EP (UD-EP) code.","Then, we introduce a type of orthogonal EP code $\\Psi_{\\rm o, B}$ constructed over an extension field GF($2^m$).","Based on the proposed EP code, we present finite-field multiple-access (FFMA) systems, including both the sparse-form-based and diagonal-form-based forms.","Simulation results show that, for the massive random access scenario, the error performance of the proposed FFMA systems over a Gaussian multiple-access channel can provide much better error performance than that of a slotted ALOHA system."],"url":"http://arxiv.org/abs/2405.11734v1","category":"cs.IT"}
{"created":"2024-05-20 02:06:53","title":"RHAML: Rendezvous-based Hierarchical Architecture for Mutual Localization","abstract":"Mutual localization serves as the foundation for collaborative perception and task assignment in multi-robot systems. Effectively utilizing limited onboard sensors for mutual localization between marker-less robots is a worthwhile goal. However, due to inadequate consideration of large scale variations of the observed robot and localization refinement, previous work has shown limited accuracy when robots are equipped only with RGB cameras. To enhance the precision of localization, this paper proposes a novel rendezvous-based hierarchical architecture for mutual localization (RHAML). Firstly, to learn multi-scale robot features, anisotropic convolutions are introduced into the network, yielding initial localization results. Then, the iterative refinement module with rendering is employed to adjust the observed robot poses. Finally, the pose graph is conducted to globally optimize all localization results, which takes into account multi-frame observations. Therefore, a flexible architecture is provided that allows for the selection of appropriate modules based on requirements. Simulations demonstrate that RHAML effectively addresses the problem of multi-robot mutual localization, achieving translation errors below 2 cm and rotation errors below 0.5 degrees when robots exhibit 5 m of depth variation. Moreover, its practical utility is validated by applying it to map fusion when multi-robots explore unknown environments.","sentences":["Mutual localization serves as the foundation for collaborative perception and task assignment in multi-robot systems.","Effectively utilizing limited onboard sensors for mutual localization between marker-less robots is a worthwhile goal.","However, due to inadequate consideration of large scale variations of the observed robot and localization refinement, previous work has shown limited accuracy when robots are equipped only with RGB cameras.","To enhance the precision of localization, this paper proposes a novel rendezvous-based hierarchical architecture for mutual localization (RHAML).","Firstly, to learn multi-scale robot features, anisotropic convolutions are introduced into the network, yielding initial localization results.","Then, the iterative refinement module with rendering is employed to adjust the observed robot poses.","Finally, the pose graph is conducted to globally optimize all localization results, which takes into account multi-frame observations.","Therefore, a flexible architecture is provided that allows for the selection of appropriate modules based on requirements.","Simulations demonstrate that RHAML effectively addresses the problem of multi-robot mutual localization, achieving translation errors below 2 cm and rotation errors below 0.5 degrees when robots exhibit 5 m of depth variation.","Moreover, its practical utility is validated by applying it to map fusion when multi-robots explore unknown environments."],"url":"http://arxiv.org/abs/2405.11726v1","category":"cs.RO"}
{"created":"2024-05-20 01:44:58","title":"Measurement and Control of Solenoid Stroke using Its Electrical Characteristics","abstract":"In this paper, we describe the algorithm to measure the stroke of solenoid using the electric characteristics of the solenoid, without mechanical attachments. We also describe the experimental results of controlling the solenoid stroke at intermediate position.","sentences":["In this paper, we describe the algorithm to measure the stroke of solenoid using the electric characteristics of the solenoid, without mechanical attachments.","We also describe the experimental results of controlling the solenoid stroke at intermediate position."],"url":"http://arxiv.org/abs/2405.11721v1","category":"eess.SY"}
{"created":"2024-05-20 01:37:08","title":"Can we improve the energy efficiency of EUV lithography?","abstract":"This paper discusses a simple, low-cost, highly efficient two-mirror projector with a simplified illumination system. The EUV source power can be reduced by 1/10 compared to the current six-mirror EUV projector system. The required EUV power is 20 watts for process speed of 100 wafers per hour. The proposed in-line projector achieves 0.2 NA (20 mm field) and 0.3 NA (10 mm field), which can be assembled in a cylindrical tube configuration similar to a DUV projector, providing superior mechanical stability and easier assembly/maintenance. The EUV light is introduced in front of the mask through two narrow cylindrical mirrors located on both side of the diffraction cone, providing average normal illumination and reducing the mask 3D effect. The simplified illumination system provides symmetric quadrupole off-axis illumination, bypassing central obscuration and improving spatial resolution, also realizing K\\\"ohler illumination. The theoretical resolution limit is 24 nm (20 mm field), image reduction factor x5 and object image distance (OID) 2000 mm. With the curved surface mask, the tool height can be reduced to (OID) 1500 mm, which provides resolution 16 nm (10 mm field). It will be suitable for small die size chip production for mobile applications as well as the latest chiplet technology.","sentences":["This paper discusses a simple, low-cost, highly efficient two-mirror projector with a simplified illumination system.","The EUV source power can be reduced by 1/10 compared to the current six-mirror EUV projector system.","The required EUV power is 20 watts for process speed of 100 wafers per hour.","The proposed in-line projector achieves 0.2 NA (20 mm field) and 0.3 NA (10 mm field), which can be assembled in a cylindrical tube configuration similar to a DUV projector, providing superior mechanical stability and easier assembly/maintenance.","The EUV light is introduced in front of the mask through two narrow cylindrical mirrors located on both side of the diffraction cone, providing average normal illumination and reducing the mask 3D effect.","The simplified illumination system provides symmetric quadrupole off-axis illumination, bypassing central obscuration and improving spatial resolution, also realizing K\\\"ohler illumination.","The theoretical resolution limit is 24 nm (20 mm field), image reduction factor x5 and object image distance (OID) 2000 mm.","With the curved surface mask, the tool height can be reduced to (OID) 1500 mm, which provides resolution 16 nm (10 mm field).","It will be suitable for small die size chip production for mobile applications as well as the latest chiplet technology."],"url":"http://arxiv.org/abs/2405.11717v1","category":"physics.optics"}
{"created":"2024-05-20 01:32:51","title":"High-Mobility Carriers in Epitaxial IrO2 Films Grown using Hybrid Molecular Beam Epitaxy","abstract":"Binary rutile oxides of 5d metals such as IrO2, stand out as a paradox due to limited experimental studies despite the rich predicted quantum phenomena. Here, we investigate the electrical transport properties of IrO2 by engineering epitaxial thin films grown via hybrid molecular beam epitaxy. Our findings reveal phonon-limited carrier transport and thickness-dependent anisotropic in-plane resistance in IrO2 (110) films, the latter suggesting a complex relationship between strain relaxation and orbital hybridization. Magneto-transport measurements reveal a previously unobserved non-linear Hall effect. A two-carrier analysis of this effect shows the presence of minority carriers with mobility exceeding 3000 cm2/Vs at 1.8 K. These results point towards emergent properties in 5d metal oxides that can be controlled using dimensionality and epitaxial strain.","sentences":["Binary rutile oxides of 5d metals such as IrO2, stand out as a paradox due to limited experimental studies despite the rich predicted quantum phenomena.","Here, we investigate the electrical transport properties of IrO2 by engineering epitaxial thin films grown via hybrid molecular beam epitaxy.","Our findings reveal phonon-limited carrier transport and thickness-dependent anisotropic in-plane resistance in IrO2 (110) films, the latter suggesting a complex relationship between strain relaxation and orbital hybridization.","Magneto-transport measurements reveal a previously unobserved non-linear Hall effect.","A two-carrier analysis of this effect shows the presence of minority carriers with mobility exceeding 3000 cm2/Vs at 1.8 K. These results point towards emergent properties in 5d metal oxides that can be controlled using dimensionality and epitaxial strain."],"url":"http://arxiv.org/abs/2405.11716v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 01:27:16","title":"Generalized regenerating codes and node repair on graphs","abstract":"We consider regenerating codes in distributed storage systems where connections between the nodes are constrained by a graph. In this problem, the failed node downloads the information stored at a subset of vertices of the graph for the purpose of recovering the lost data. Compared to the standard setting, regenerating codes on graphs address two additional features. The repair information is moved across the network, and the cost of node repair is determined by the graphical distance from the helper nodes to the failed node. Accordingly, the helpers far away from the failed node may be expected to contribute less data for repair than the nodes in the neighborhood of that node. We analyze regenerating codes with nonuniform download for repair on graphs. Moreover, in the process of repair, the information moved from the helpers to the failed node may be combined through intermediate processing, reducing the repair bandwidth. We derive lower bounds for communication complexity of node repair on graphs, including repair schemes with nonuniform download and intermediate processing, and construct codes that attain these bounds.   Additionally, some of the nodes may act as adversaries, introducing errors into the data moved in the network. For repair on graphs in the presence of adversarial nodes, we construct codes that support node repair and error correction in systematic nodes.","sentences":["We consider regenerating codes in distributed storage systems where connections between the nodes are constrained by a graph.","In this problem, the failed node downloads the information stored at a subset of vertices of the graph for the purpose of recovering the lost data.","Compared to the standard setting, regenerating codes on graphs address two additional features.","The repair information is moved across the network, and the cost of node repair is determined by the graphical distance from the helper nodes to the failed node.","Accordingly, the helpers far away from the failed node may be expected to contribute less data for repair than the nodes in the neighborhood of that node.","We analyze regenerating codes with nonuniform download for repair on graphs.","Moreover, in the process of repair, the information moved from the helpers to the failed node may be combined through intermediate processing, reducing the repair bandwidth.","We derive lower bounds for communication complexity of node repair on graphs, including repair schemes with nonuniform download and intermediate processing, and construct codes that attain these bounds.   ","Additionally, some of the nodes may act as adversaries, introducing errors into the data moved in the network.","For repair on graphs in the presence of adversarial nodes, we construct codes that support node repair and error correction in systematic nodes."],"url":"http://arxiv.org/abs/2405.11714v1","category":"cs.IT"}
{"created":"2024-05-20 01:09:22","title":"Ab initio intermolecular interactions mediate thermochemically real-fluid effects that affect system reactivity","abstract":"The properties of supercritical fluids are dictated by intermolecular interactions that involve two or more molecules. Such intermolecular interactions were described via intermolecular potentials in historical supercritical combustion modeling studies, but have been treated empirically and with no consideration of radical interactions or multi-body interactions involving more than two molecules. This approach has been adopted long ago, assuming sufficient characterization of real-fluid effects during supercritical combustion. Here, with data from ab initio multi-body intermolecular potentials, non-empirical Virial Equation of State (EoS), and real-fluid thermochemical and kinetic simulations, we reveal that empirical intermolecular potentials can lead to significant errors in representing supercritical fluids under common combustion situations, which can be impressively described by ab initio intermolecular potentials. These interactions are also found to greatly influence autoignition delay times, a common measure of global reactivity, with significant contributions from radical interactions and multi-body interactions. It is therefore of necessity to incorporate ab initio intermolecular interactions in studying supercritical combustion and various dynamic systems involving supercritical fluids, which has now been enabled through the new framework developed in the present study.","sentences":["The properties of supercritical fluids are dictated by intermolecular interactions that involve two or more molecules.","Such intermolecular interactions were described via intermolecular potentials in historical supercritical combustion modeling studies, but have been treated empirically and with no consideration of radical interactions or multi-body interactions involving more than two molecules.","This approach has been adopted long ago, assuming sufficient characterization of real-fluid effects during supercritical combustion.","Here, with data from ab initio multi-body intermolecular potentials, non-empirical Virial Equation of State (EoS), and real-fluid thermochemical and kinetic simulations, we reveal that empirical intermolecular potentials can lead to significant errors in representing supercritical fluids under common combustion situations, which can be impressively described by ab initio intermolecular potentials.","These interactions are also found to greatly influence autoignition delay times, a common measure of global reactivity, with significant contributions from radical interactions and multi-body interactions.","It is therefore of necessity to incorporate ab initio intermolecular interactions in studying supercritical combustion and various dynamic systems involving supercritical fluids, which has now been enabled through the new framework developed in the present study."],"url":"http://arxiv.org/abs/2405.11710v1","category":"physics.chem-ph"}
{"created":"2024-05-20 00:59:08","title":"Comparison of Coarsening Dynamics for the Cahn--Hilliard and Burgers--Cahn--Hilliard Equations","abstract":"We consider coarsening dynamics associated with a Burgers--Cahn--Hilliard system modeling a two-phase flow in one space dimension. Our emphasis is on the effect that coupling between the phase and fluid dynamics has on coarsening rates, and on the mechanisms driving this effect. We start with a detailed examination of coarsening dynamics for the uncoupled Cahn--Hilliard equation, comparing numerically generated rates with two analytic methods, and then we consider how these dynamics are affected by appropriate coupling with a viscous Burgers equation. In order to keep the analysis as self-contained as possible, we establish the global well-posedness of the system under consideration.","sentences":["We consider coarsening dynamics associated with a Burgers--Cahn--Hilliard system modeling a two-phase flow in one space dimension.","Our emphasis is on the effect that coupling between the phase and fluid dynamics has on coarsening rates, and on the mechanisms driving this effect.","We start with a detailed examination of coarsening dynamics for the uncoupled Cahn--Hilliard equation, comparing numerically generated rates with two analytic methods, and then we consider how these dynamics are affected by appropriate coupling with a viscous Burgers equation.","In order to keep the analysis as self-contained as possible, we establish the global well-posedness of the system under consideration."],"url":"http://arxiv.org/abs/2405.11709v1","category":"math.AP"}
{"created":"2024-05-20 00:58:53","title":"Adaptive Batch Normalization Networks for Adversarial Robustness","abstract":"Deep networks are vulnerable to adversarial examples. Adversarial Training (AT) has been a standard foundation of modern adversarial defense approaches due to its remarkable effectiveness. However, AT is extremely time-consuming, refraining it from wide deployment in practical applications. In this paper, we aim at a non-AT defense: How to design a defense method that gets rid of AT but is still robust against strong adversarial attacks? To answer this question, we resort to adaptive Batch Normalization (BN), inspired by the recent advances in test-time domain adaptation. We propose a novel defense accordingly, referred to as the Adaptive Batch Normalization Network (ABNN). ABNN employs a pre-trained substitute model to generate clean BN statistics and sends them to the target model. The target model is exclusively trained on clean data and learns to align the substitute model's BN statistics. Experimental results show that ABNN consistently improves adversarial robustness against both digital and physically realizable attacks on both image and video datasets. Furthermore, ABNN can achieve higher clean data performance and significantly lower training time complexity compared to AT-based approaches.","sentences":["Deep networks are vulnerable to adversarial examples.","Adversarial Training (AT) has been a standard foundation of modern adversarial defense approaches due to its remarkable effectiveness.","However, AT is extremely time-consuming, refraining it from wide deployment in practical applications.","In this paper, we aim at a non-AT defense: How to design a defense method that gets rid of AT but is still robust against strong adversarial attacks?","To answer this question, we resort to adaptive Batch Normalization (BN), inspired by the recent advances in test-time domain adaptation.","We propose a novel defense accordingly, referred to as the Adaptive Batch Normalization Network (ABNN).","ABNN employs a pre-trained substitute model to generate clean BN statistics and sends them to the target model.","The target model is exclusively trained on clean data and learns to align the substitute model's BN statistics.","Experimental results show that ABNN consistently improves adversarial robustness against both digital and physically realizable attacks on both image and video datasets.","Furthermore, ABNN can achieve higher clean data performance and significantly lower training time complexity compared to AT-based approaches."],"url":"http://arxiv.org/abs/2405.11708v1","category":"cs.LG"}
{"created":"2024-05-19 23:31:04","title":"Fixed-parameter tractability of canonical polyadic decomposition over finite fields","abstract":"We present a simple proof that finding a rank-$R$ canonical polyadic decomposition of 3-dimensional tensors over a finite field $\\mathbb{F}$ is fixed-parameter tractable with respect to $R$ and $\\mathbb{F}$. We also show some more concrete upper bounds on the time complexity of this problem.","sentences":["We present a simple proof that finding a rank-$R$ canonical polyadic decomposition of 3-dimensional tensors over a finite field $\\mathbb{F}$ is fixed-parameter tractable with respect to $R$ and $\\mathbb{F}$. We also show some more concrete upper bounds on the time complexity of this problem."],"url":"http://arxiv.org/abs/2405.11699v1","category":"cs.CC"}
{"created":"2024-05-19 22:47:36","title":"A new class of Carleson measures and integral operators on Bergman spaces","abstract":"Let $n$ be a positive integer and $\\mathbf{g}=(g_0,g_1,\\cdots,g_{n-1})$, with $g_k\\in H(\\mathbb{D})$ for $k=0,1,\\cdots,n-1$. Let $I_{\\mathbf{g}}^{(n)}$ be the generalized Volterra-type operators on $H(\\mathbb{C})$, which is represented as $$ I_{\\mathbf{g}}^{(n)}f=I^n\\left(fg_0+f'g_1+\\cdots+f^{(n-1)}g_{n-1}\\right), $$ where $I$ denotes the integration operator $$(If)(z)=\\int_0^zf(w)dw,$$ and $I^n$ is the $n$th iteration of $I$. This operator is a generalization of the operator that was introduced by Chalmoukis in \\cite{Cn}. In this paper, we study the boundedness and compactness of the operator $I_{\\mathbf{g}}^{(n)}$ acting on Bergman spaces to another. As a consequence of these characterizations, we obtain conditions for certain linear differential equations to have solutions in Bergman spaces. Moreover, we study the boundedness, compactness and Hilbert-Schmidtness of the following sums of generalized weighted composition operators: Let $\\mathbf{u}=(u_0,u_1,\\cdots,u_n)$ with $u_k\\in H(\\mathbb{D})$ for $0\\leq k\\leq n$ and $\\varphi$ be an analytic self-map of $\\mathbb{D}.$ The sums of generalized weighted composition operators is defined by $$L_{\\mathbf{u},\\varphi}^{(n)}=\\sum_{k=0}^nW_{u_k,\\varphi}^{(k)},$$ where $$W_{u_k,\\varphi}^{(k)}f=u_k\\cdot f^{(k)}\\circ\\varphi.$$ Our approach involves the study of new class of Sobolev-Carleson measures for classical Bergman spaces on unit disk which appears in the first main Theorems \\ref{Theorem1.1} and \\ref{Theorem1.2}.","sentences":["Let $n$ be a positive integer and $\\mathbf{g}=(g_0,g_1,\\cdots,g_{n-1})$, with $g_k\\in H(\\mathbb{D})$ for $k=0,1,\\cdots,n-1$. Let $I_{\\mathbf{g}}^{(n)}$ be the generalized Volterra-type operators on $H(\\mathbb{C})$, which is represented as $$ I_{\\mathbf{g}}^{(n)}f=I^n\\left(fg_0+f'g_1+\\cdots+f^{(n-1)}g_{n-1}\\right), $$ where $I$ denotes the integration operator $$(If)(z)=\\int_0^zf(w)dw,$$ and $I^n$ is the $n$th iteration of $I$. This operator is a generalization of the operator that was introduced by Chalmoukis in \\cite{Cn}.","In this paper, we study the boundedness and compactness of the operator $I_{\\mathbf{g}}^{(n)}$ acting on Bergman spaces to another.","As a consequence of these characterizations, we obtain conditions for certain linear differential equations to have solutions in Bergman spaces.","Moreover, we study the boundedness, compactness and Hilbert-Schmidtness of the following sums of generalized weighted composition operators: Let $\\mathbf{u}=(u_0,u_1,\\cdots,u_n)$ with $u_k\\in H(\\mathbb{D})$ for $0\\leq k\\leq n$ and $\\varphi$ be an analytic self-map of $\\mathbb{D}.$ The sums of generalized weighted composition operators is defined by $$L_{\\mathbf{u},\\varphi}^{(n)}=\\sum_{k=0}^nW_{u_k,\\varphi}^{(k)},$$ where $$W_{u_k,\\varphi}^{(k)}f=u_k\\cdot f^{(k)}\\circ\\varphi.$$ Our approach involves the study of new class of Sobolev-Carleson measures for classical Bergman spaces on unit disk which appears in the first main Theorems \\ref{Theorem1.1} and \\ref{Theorem1.2}."],"url":"http://arxiv.org/abs/2405.11692v1","category":"math.CV"}
{"created":"2024-05-19 22:26:33","title":"Investigation of suppression of $\u03a5(nS)$ in relativistic heavy-ion collisions at RHIC and LHC energies","abstract":"The primary purpose of studying quarkonium production in relativistic heavy-ion collisions is to understand the properties of the quark-gluon plasma. At various collision systems, measurements of quarkonium states of different binding energies, such as $\\Upsilon(nS)$, can provide comprehensive information. A model study has been performed to investigate the modification of $\\Upsilon(nS)$ production in Pb-Pb collisions at $\\sqrt{s_{\\mathrm{NN}}}=$ 5.02 TeV and Au-Au collisions at $\\sqrt{s_{\\mathrm{NN}}}=$ 200 GeV. The Monte-Carlo simulation study is performed with a publicly available hydrodynamic simulation package for the quark-gluon plasma medium and a theoretical calculation of temperature-dependent thermal width of $\\Upsilon(nS)$ considering the gluo-dissociation and inelastic parton scattering for dissociation inside the medium. In addition, we perform a systematic study with different descriptions of initial collision geometry and formation time of $\\Upsilon(nS)$ to investigate their impacts on yield modification. The model calculation with a varied parameter set can describe the experimental data of $\\Upsilon(nS)$ in Pb-Pb collisions at 5.02 TeV and $\\Upsilon(2S)$ in Au-Au collisions at 200 GeV but underestimates the modification of $\\Upsilon(1S)$ at the lower collision energy. The nuclear absorption mechanism is explored to understand the discrepancy between the data and simulation.","sentences":["The primary purpose of studying quarkonium production in relativistic heavy-ion collisions is to understand the properties of the quark-gluon plasma.","At various collision systems, measurements of quarkonium states of different binding energies, such as $\\Upsilon(nS)$, can provide comprehensive information.","A model study has been performed to investigate the modification of $\\Upsilon(nS)$ production in Pb-Pb collisions at $\\sqrt{s_{\\mathrm{NN}}}=$ 5.02 TeV and Au-Au collisions at $\\sqrt{s_{\\mathrm{NN}}}=$ 200 GeV. The Monte-Carlo simulation study is performed with a publicly available hydrodynamic simulation package for the quark-gluon plasma medium and a theoretical calculation of temperature-dependent thermal width of $\\Upsilon(nS)$ considering the gluo-dissociation and inelastic parton scattering for dissociation inside the medium.","In addition, we perform a systematic study with different descriptions of initial collision geometry and formation time of $\\Upsilon(nS)$ to investigate their impacts on yield modification.","The model calculation with a varied parameter set can describe the experimental data of $\\Upsilon(nS)$ in Pb-Pb collisions at 5.02 TeV and $\\Upsilon(2S)$ in Au-Au collisions at 200 GeV but underestimates the modification of $\\Upsilon(1S)$ at the lower collision energy.","The nuclear absorption mechanism is explored to understand the discrepancy between the data and simulation."],"url":"http://arxiv.org/abs/2405.11689v1","category":"nucl-th"}
{"created":"2024-05-19 22:25:33","title":"Performance Analysis of Monte Carlo Algorithms in Dense Subgraph Identification","abstract":"The exploration of network structures through the lens of graph theory has become a cornerstone in understanding complex systems across diverse fields. Identifying densely connected subgraphs within larger networks is crucial for uncovering functional modules in biological systems, cohesive groups within social networks, and critical paths in technological infrastructures. The most representative approach, the SM algorithm, cannot locate subgraphs with large sizes, therefore cannot identify dense subgraphs; while the SA algorithm previously used by researchers combines simulated annealing and efficient moves for the Markov chain. However, the global optima cannot be guaranteed to be located by the simulated annealing methods including SA unless a logarithmic cooling schedule is used. To this end, our study introduces and evaluates the performance of the Simulated Annealing Algorithm (SAA), which combines simulated annealing with the stochastic approximation Monte Carlo algorithm. The performance of SAA against two other numerical algorithms-SM and SA, is examined in the context of identifying these critical subgraph structures using simulated graphs with embeded cliques. We have found that SAA outperforms both SA and SM by 1) the number of iterations to find the densest subgraph and 2) the percentage of time the algorithm is able to find a clique after 10,000 iterations, and 3) computation time. The promising result of the SAA algorithm could offer a robust tool for dissecting complex systems and potentially transforming our approach to solving problems in interdisciplinary fields.","sentences":["The exploration of network structures through the lens of graph theory has become a cornerstone in understanding complex systems across diverse fields.","Identifying densely connected subgraphs within larger networks is crucial for uncovering functional modules in biological systems, cohesive groups within social networks, and critical paths in technological infrastructures.","The most representative approach, the SM algorithm, cannot locate subgraphs with large sizes, therefore cannot identify dense subgraphs; while the SA algorithm previously used by researchers combines simulated annealing and efficient moves for the Markov chain.","However, the global optima cannot be guaranteed to be located by the simulated annealing methods including SA unless a logarithmic cooling schedule is used.","To this end, our study introduces and evaluates the performance of the Simulated Annealing Algorithm (SAA), which combines simulated annealing with the stochastic approximation Monte Carlo algorithm.","The performance of SAA against two other numerical algorithms-SM and SA, is examined in the context of identifying these critical subgraph structures using simulated graphs with embeded cliques.","We have found that SAA outperforms both SA and SM by 1) the number of iterations to find the densest subgraph and 2) the percentage of time the algorithm is able to find a clique after 10,000 iterations, and 3) computation time.","The promising result of the SAA algorithm could offer a robust tool for dissecting complex systems and potentially transforming our approach to solving problems in interdisciplinary fields."],"url":"http://arxiv.org/abs/2405.11688v1","category":"stat.CO"}
{"created":"2024-05-19 22:08:15","title":"Exploiting Distributional Value Functions for Financial Market Valuation, Enhanced Feature Creation and Improvement of Trading Algorithms","abstract":"While research of reinforcement learning applied to financial markets predominantly concentrates on finding optimal behaviours, it is worth to realize that the reinforcement learning returns $G_t$ and state value functions themselves are of interest and play a pivotal role in the evaluation of assets. Instead of focussing on the more complex task of finding optimal decision rules, this paper studies and applies the power of distributional state value functions in the context of financial market valuation and machine learning based trading algorithms. Accurate and trustworthy estimates of the distributions of $G_t$ provide a competitive edge leading to better informed decisions and more optimal behaviour. Herein, ideas from predictive knowledge and deep reinforcement learning are combined to introduce a novel family of models called CDG-Model, resulting in a highly flexible framework and intuitive approach with minimal assumptions regarding underlying distributions. The models allow seamless integration of typical financial modelling pitfalls like transaction costs, slippage and other possible costs or benefits into the model calculation. They can be applied to any kind of trading strategy or asset class. The frameworks introduced provide concrete business value through their potential in market valuation of single assets and portfolios, in the comparison of strategies as well as in the improvement of market timing. They can positively impact the performance and enhance the learning process of existing or new trading algorithms. They are of interest from a scientific point-of-view and open up multiple areas of future research. Initial implementations and tests were performed on real market data. While the results are promising, applying a robust statistical framework to evaluate the models in general remains a challenge and further investigations are needed.","sentences":["While research of reinforcement learning applied to financial markets predominantly concentrates on finding optimal behaviours, it is worth to realize that the reinforcement learning returns $G_t$ and state value functions themselves are of interest and play a pivotal role in the evaluation of assets.","Instead of focussing on the more complex task of finding optimal decision rules, this paper studies and applies the power of distributional state value functions in the context of financial market valuation and machine learning based trading algorithms.","Accurate and trustworthy estimates of the distributions of $G_t$ provide a competitive edge leading to better informed decisions and more optimal behaviour.","Herein, ideas from predictive knowledge and deep reinforcement learning are combined to introduce a novel family of models called CDG-Model, resulting in a highly flexible framework and intuitive approach with minimal assumptions regarding underlying distributions.","The models allow seamless integration of typical financial modelling pitfalls like transaction costs, slippage and other possible costs or benefits into the model calculation.","They can be applied to any kind of trading strategy or asset class.","The frameworks introduced provide concrete business value through their potential in market valuation of single assets and portfolios, in the comparison of strategies as well as in the improvement of market timing.","They can positively impact the performance and enhance the learning process of existing or new trading algorithms.","They are of interest from a scientific point-of-view and open up multiple areas of future research.","Initial implementations and tests were performed on real market data.","While the results are promising, applying a robust statistical framework to evaluate the models in general remains a challenge and further investigations are needed."],"url":"http://arxiv.org/abs/2405.11686v1","category":"q-fin.ST"}
{"created":"2024-05-20 17:59:59","title":"CMB-HD as a Probe of Dark Matter on Sub-Galactic Scales","abstract":"We show for the first time that high-resolution CMB lensing observations can probe structure on sub-galactic scales. In particular, a CMB-HD experiment can probe out to k ~ 55 h/Mpc, corresponding to halo masses of about $10^8 M_{\\odot}$. Over the range 0.005 h/Mpc < k < 55 h/Mpc, spanning four orders of magnitude, the total lensing signal-to-noise ratio (SNR) from the temperature, polarization, and lensing power spectra is greater than 1900. CMB-HD gains most of the lensing SNR at small scales from the temperature power spectrum, as opposed to the lensing spectrum. These lensing measurements allow CMB-HD to distinguish between cold dark matter (CDM) and non-CDM models that change the matter power spectrum on sub-galactic scales. We also find that CMB-HD can distinguish between baryonic feedback effects and non-CDM models due to the different way each impacts the lensing signal. The kinetic Sunyaev-Zel'dovich (kSZ) power spectrum further constrains non-CDM models that deviate from CDM on the smallest scales CMB-HD measures. For example, CMB-HD can detect 1 keV warm dark matter (WDM) at 30$\\sigma$, or rule out about 7 keV WDM at 95% CL, in a $\\Lambda$WDM + $N_{\\rm{eff}} + \\sum m_\\nu + m_{\\rm{WDM}} + \\log_{10}T_{\\rm{AGN}} + A_{\\rm{kSZ}} + n_{\\rm{kSZ}}$ model; here $T_{\\rm{AGN}}$ characterizes the strength of the feedback, and $A_{\\rm{kSZ}}$ and $n_{\\rm{kSZ}}$ allow freedom in the amplitude and slope of the kinetic Sunyaev-Zel'dovich power spectrum. We make the CMB-HD Fisher code used here publicly available, and note that it can be modified to use any non-CDM model that changes the matter power spectrum.","sentences":["We show for the first time that high-resolution CMB lensing observations can probe structure on sub-galactic scales.","In particular, a CMB-HD experiment can probe out to k ~ 55 h/Mpc, corresponding to halo masses of about $10^8 M_{\\odot}$. Over the range 0.005 h/Mpc < k < 55 h/Mpc, spanning four orders of magnitude, the total lensing signal-to-noise ratio (SNR) from the temperature, polarization, and lensing power spectra is greater than 1900.","CMB-HD gains most of the lensing SNR at small scales from the temperature power spectrum, as opposed to the lensing spectrum.","These lensing measurements allow CMB-HD to distinguish between cold dark matter (CDM) and non-CDM models that change the matter power spectrum on sub-galactic scales.","We also find that CMB-HD can distinguish between baryonic feedback effects and non-CDM models due to the different way each impacts the lensing signal.","The kinetic Sunyaev-Zel'dovich (kSZ) power spectrum further constrains non-CDM models that deviate from CDM on the smallest scales CMB-HD measures.","For example, CMB-HD can detect 1 keV warm dark matter (WDM) at 30$\\sigma$, or rule out about 7 keV WDM at 95% CL, in a $\\Lambda$WDM + $N_{\\rm{eff}} + \\sum m_\\nu + m_{\\rm{WDM}} + \\log_{10}T_{\\rm{AGN}} + A_{\\rm{kSZ}} + n_{\\rm{kSZ}}$ model; here $T_{\\rm{AGN}}$ characterizes the strength of the feedback, and $A_{\\rm{kSZ}}$ and $n_{\\rm{kSZ}}$ allow freedom in the amplitude and slope of the kinetic Sunyaev-Zel'dovich power spectrum.","We make the CMB-HD Fisher code used here publicly available, and note that it can be modified to use any non-CDM model that changes the matter power spectrum."],"url":"http://arxiv.org/abs/2405.12220v1","category":"astro-ph.CO"}
{"created":"2024-05-20 17:59:17","title":"Radiative transfer of 21-cm line through ionised cavities in an expanding universe","abstract":"The optical depth parameterisation is typically used to study the 21-cm signals associated with the properties of the neutral hydrogen (HI) gas and the ionisation morphology during the Epoch of Reionisation (EoR), without solving the radiative transfer equation. To assess the uncertainties resulting from this simplification, we conduct explicit radiative transfer calculations using the cosmological 21-cm radiative transfer (C21LRT) code and examine the imprints of ionisation structures on the 21-cm spectrum. We consider a globally averaged reionisation history and implement fully ionised cavities (HII bubbles) of diameters $d$ ranging from 0.01 Mpc to 10 Mpc at epochs within the emission and the absorption regimes of the 21-cm global signal. The single-ray C21LRT calculations show that the shape of the imprinted spectral features are primarily determined by $d$ and the 21-cm line profile, which is parametrised by the turbulent velocity of the HI gas. It reveals the spectral features tied to the transition from ionised to neutral regions that calculations based on the optical depth parametrisation were unable to capture. We also present analytical approximations of the calculated spectral features of the HII bubbles. The multiple-ray calculations show that the apparent shape of a HII bubble (of $d=5$ Mpc at $z=8$), because of the finite speed of light, differs depending on whether the bubble's ionisation front is stationary or expanding. Our study shows the necessity of properly accounting for the effects of line-continuum interaction, line broadening and cosmological expansion to correctly predict the EoR 21-cm signals.","sentences":["The optical depth parameterisation is typically used to study the 21-cm signals associated with the properties of the neutral hydrogen (HI) gas and the ionisation morphology during the Epoch of Reionisation (EoR), without solving the radiative transfer equation.","To assess the uncertainties resulting from this simplification, we conduct explicit radiative transfer calculations using the cosmological 21-cm radiative transfer (C21LRT) code and examine the imprints of ionisation structures on the 21-cm spectrum.","We consider a globally averaged reionisation history and implement fully ionised cavities (HII bubbles) of diameters $d$ ranging from 0.01 Mpc to 10 Mpc at epochs within the emission and the absorption regimes of the 21-cm global signal.","The single-ray C21LRT calculations show that the shape of the imprinted spectral features are primarily determined by $d$ and the 21-cm line profile, which is parametrised by the turbulent velocity of the HI gas.","It reveals the spectral features tied to the transition from ionised to neutral regions that calculations based on the optical depth parametrisation were unable to capture.","We also present analytical approximations of the calculated spectral features of the HII bubbles.","The multiple-ray calculations show that the apparent shape of a HII bubble (of $d=5$ Mpc at $z=8$), because of the finite speed of light, differs depending on whether the bubble's ionisation front is stationary or expanding.","Our study shows the necessity of properly accounting for the effects of line-continuum interaction, line broadening and cosmological expansion to correctly predict the EoR 21-cm signals."],"url":"http://arxiv.org/abs/2405.12216v1","category":"astro-ph.CO"}
{"created":"2024-05-20 17:56:22","title":"Forced Measurement of Astronomical Sources at Low Signal to Noise","abstract":"We propose a modified moment matching algorithm to avoid catastrophic failures for sources with a low signal to noise ratio (SNR). The proposed modifications include a method to eliminate non-physical negative pixel values and a forced single iteration with an initial guess derived from co-add measurements when iterative methods are unstable. We correct for all biases in measurements introduced by the method. We find that the proposed modifications allow the algorithm to avoid catastrophic failures in nearly 100\\% of the cases, especially at low signal to noise ratio. Additionally, with a reasonable guess from co-add measurements, the algorithm measures the flux, centroid, size, shape and ellipticity with bias statistically consistent with zero. We show the proposed method allows us to measure sources seven times fainter than traditional methods when applied to images obtained from WIYN-ODI. We also present a scheme to find uncertainties in measurements when using the new method to measure astronomical sources.","sentences":["We propose a modified moment matching algorithm to avoid catastrophic failures for sources with a low signal to noise ratio (SNR).","The proposed modifications include a method to eliminate non-physical negative pixel values and a forced single iteration with an initial guess derived from co-add measurements when iterative methods are unstable.","We correct for all biases in measurements introduced by the method.","We find that the proposed modifications allow the algorithm to avoid catastrophic failures in nearly 100\\% of the cases, especially at low signal to noise ratio.","Additionally, with a reasonable guess from co-add measurements, the algorithm measures the flux, centroid, size, shape and ellipticity with bias statistically consistent with zero.","We show the proposed method allows us to measure sources seven times fainter than traditional methods when applied to images obtained from WIYN-ODI.","We also present a scheme to find uncertainties in measurements when using the new method to measure astronomical sources."],"url":"http://arxiv.org/abs/2405.12212v1","category":"astro-ph.IM"}
{"created":"2024-05-20 17:47:18","title":"Optimistic Query Routing in Clustering-based Approximate Maximum Inner Product Search","abstract":"Clustering-based nearest neighbor search is a simple yet effective method in which data points are partitioned into geometric shards to form an index, and only a few shards are searched during query processing to find an approximate set of top-$k$ vectors. Even though the search efficacy is heavily influenced by the algorithm that identifies the set of shards to probe, it has received little attention in the literature. This work attempts to bridge that gap by studying the problem of routing in clustering-based maximum inner product search (MIPS). We begin by unpacking existing routing protocols and notice the surprising contribution of optimism. We then take a page from the sequential decision making literature and formalize that insight following the principle of ``optimism in the face of uncertainty.'' In particular, we present a new framework that incorporates the moments of the distribution of inner products within each shard to optimistically estimate the maximum inner product. We then present a simple instance of our algorithm that uses only the first two moments to reach the same accuracy as state-of-the-art routers such as \\scann by probing up to $50%$ fewer points on a suite of benchmark MIPS datasets. Our algorithm is also space-efficient: we design a sketch of the second moment whose size is independent of the number of points and in practice requires storing only $O(1)$ additional vectors per shard.","sentences":["Clustering-based nearest neighbor search is a simple yet effective method in which data points are partitioned into geometric shards to form an index, and only a few shards are searched during query processing to find an approximate set of top-$k$ vectors.","Even though the search efficacy is heavily influenced by the algorithm that identifies the set of shards to probe, it has received little attention in the literature.","This work attempts to bridge that gap by studying the problem of routing in clustering-based maximum inner product search (MIPS).","We begin by unpacking existing routing protocols and notice the surprising contribution of optimism.","We then take a page from the sequential decision making literature and formalize that insight following the principle of ``optimism in the face of uncertainty.''","In particular, we present a new framework that incorporates the moments of the distribution of inner products within each shard to optimistically estimate the maximum inner product.","We then present a simple instance of our algorithm that uses only the first two moments to reach the same accuracy as state-of-the-art routers such as \\scann by probing up to $50%$ fewer points on a suite of benchmark MIPS datasets.","Our algorithm is also space-efficient: we design a sketch of the second moment whose size is independent of the number of points and in practice requires storing only $O(1)$ additional vectors per shard."],"url":"http://arxiv.org/abs/2405.12207v1","category":"cs.LG"}
{"created":"2024-05-20 17:45:36","title":"Modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models","abstract":"Scientist learn early on how to cite scientific sources to support their claims. Sometimes, however, scientists have challenges determining where a citation should be situated -- or, even worse, fail to cite a source altogether. Automatically detecting sentences that need a citation (i.e., citation worthiness) could solve both of these issues, leading to more robust and well-constructed scientific arguments. Previous researchers have applied machine learning to this task but have used small datasets and models that do not take advantage of recent algorithmic developments such as attention mechanisms in deep learning. We hypothesize that we can develop significantly accurate deep learning architectures that learn from large supervised datasets constructed from open access publications. In this work, we propose a Bidirectional Long Short-Term Memory (BiLSTM) network with attention mechanism and contextual information to detect sentences that need citations. We also produce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset, which is orders of magnitude larger than previous datasets. Our experiments show that our architecture achieves state of the art performance on the standard ACL-ARC dataset ($F_{1}=0.507$) and exhibits high performance ($F_{1}=0.856$) on the new PMOA-CITE. Moreover, we show that it can transfer learning across these datasets. We further use interpretable models to illuminate how specific language is used to promote and inhibit citations. We discover that sections and surrounding sentences are crucial for our improved predictions. We further examined purported mispredictions of the model, and uncovered systematic human mistakes in citation behavior and source data. This opens the door for our model to check documents during pre-submission and pre-archival procedures. We make this new dataset, the code, and a web-based tool available to the community.","sentences":["Scientist learn early on how to cite scientific sources to support their claims.","Sometimes, however, scientists have challenges determining where a citation should be situated -- or, even worse, fail to cite a source altogether.","Automatically detecting sentences that need a citation (i.e., citation worthiness) could solve both of these issues, leading to more robust and well-constructed scientific arguments.","Previous researchers have applied machine learning to this task but have used small datasets and models that do not take advantage of recent algorithmic developments such as attention mechanisms in deep learning.","We hypothesize that we can develop significantly accurate deep learning architectures that learn from large supervised datasets constructed from open access publications.","In this work, we propose a Bidirectional Long Short-Term Memory (BiLSTM) network with attention mechanism and contextual information to detect sentences that need citations.","We also produce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset, which is orders of magnitude larger than previous datasets.","Our experiments show that our architecture achieves state of the art performance on the standard ACL-ARC dataset ($F_{1}=0.507$) and exhibits high performance ($F_{1}=0.856$) on the new PMOA-CITE.","Moreover, we show that it can transfer learning across these datasets.","We further use interpretable models to illuminate how specific language is used to promote and inhibit citations.","We discover that sections and surrounding sentences are crucial for our improved predictions.","We further examined purported mispredictions of the model, and uncovered systematic human mistakes in citation behavior and source data.","This opens the door for our model to check documents during pre-submission and pre-archival procedures.","We make this new dataset, the code, and a web-based tool available to the community."],"url":"http://arxiv.org/abs/2405.12206v1","category":"cs.CL"}
{"created":"2024-05-20 17:22:33","title":"The sign of scalar curvature on K\u00e4hler blowups","abstract":"We show that if $(M,\\omega)$ is a compact K\\\"ahler manifold with positive/negative scalar curvature, then the blowup of $M$ at any point also furnishes a positive/negative scalar curvature K\\\"ahler metric in classes which make the exceptional divisor small. In the case of K\\\"ahler surfaces with positive scalar curvature, this extends a result of N. Hitchin to surfaces and answers a conjecture of C. LeBrun in the affirmative, as a result completing the classification of such surfaces.","sentences":["We show that if $(M,\\omega)$ is a compact K\\\"ahler manifold with positive/negative scalar curvature, then the blowup of $M$ at any point also furnishes a positive/negative scalar curvature K\\\"ahler metric in classes which make the exceptional divisor small.","In the case of K\\\"ahler surfaces with positive scalar curvature, this extends a result of N. Hitchin to surfaces and answers a conjecture of C. LeBrun in the affirmative, as a result completing the classification of such surfaces."],"url":"http://arxiv.org/abs/2405.12189v1","category":"math.DG"}
{"created":"2024-05-20 16:42:41","title":"Optimal Eigenvalue Rigidity of Random Regular Graphs","abstract":"Consider the normalized adjacency matrices of random $d$-regular graphs on $N$ vertices with fixed degree $d\\geq 3$, and denote the eigenvalues as $\\lambda_1=d/\\sqrt{d-1}\\geq \\lambda_2\\geq\\lambda_3\\cdots\\geq \\lambda_N$. We prove that the optimal (up to an extra $N^{{\\rm o}_N(1)}$ factor, where ${\\rm o}_N(1)$ can be arbitrarily small) eigenvalue rigidity holds. More precisely, denote $\\gamma_i$ as the classical location of the $i$-th eigenvalue under the Kesten-Mckay law in decreasing order. Then with probability $1-N^{-1+{\\rm o}_N(1)}$,   \\begin{align*}   |\\lambda_i-\\gamma_i|\\leq \\frac{N^{{\\rm o}_N(1)}}{N^{2/3} (\\min\\{i,N-i+1\\})^{1/3}},\\quad \\text{ for all } i\\in \\{2,3,\\cdots,N\\}.   \\end{align*}   In particular, the fluctuations of extreme eigenvalues are bounded by $N^{-2/3+{\\rm o}_N(1)}$. This gives the same order of fluctuation as for the eigenvalues of matrices from the Gaussian Orthogonal Ensemble.","sentences":["Consider the normalized adjacency matrices of random $d$-regular graphs on $N$ vertices with fixed degree $d\\geq 3$, and denote the eigenvalues as $\\lambda_1=d/\\sqrt{d-1}\\geq \\lambda_2\\geq\\lambda_3\\cdots\\geq \\lambda_N$. We prove that the optimal (up to an extra $N^{{\\rm o}_N(1)}$ factor, where ${\\rm o}_N(1)$ can be arbitrarily small) eigenvalue rigidity holds.","More precisely, denote $\\gamma_i$ as the classical location of the $i$-th eigenvalue under the Kesten-Mckay law in decreasing order.","Then with probability $1-N^{-1+{\\rm o}_N(1)}$,   \\begin{align*}   |\\lambda_i-\\gamma_i|\\leq \\frac{N^{{\\rm o}_N(1)}}{N^{2/3} (\\min\\{i,N-i+1\\})^{1/3}},\\quad \\text{ for all } i\\in \\{2,3,\\cdots,N\\}.   ","\\end{align*}   In particular, the fluctuations of extreme eigenvalues are bounded by $N^{-2/3+{\\rm o}_N(1)}$.","This gives the same order of fluctuation as for the eigenvalues of matrices from the Gaussian Orthogonal Ensemble."],"url":"http://arxiv.org/abs/2405.12161v1","category":"math.PR"}
{"created":"2024-05-20 16:36:58","title":"Asymmetry models and separability for multi-way contingency tables with ordinal categories","abstract":"In this paper, we propose a model that indicates the asymmetry structure for cell probabilities in multivariate contingency tables with the same ordered categories. The proposed model is the closest to the symmetry model in terms of the $f$-divergence under certain conditions and incorporates various asymmetry models as special cases, including existing models. We elucidate the relationship between the proposed model and conventional models from several aspects of divergence in $f$-divergence. Furthermore, we provide theorems showing that the symmetry model can be decomposed into two or more models, each imposing less restrictive parameter constraints than the symmetry condition. We also discuss the properties of goodness-of-fit statistics, particularly focusing on the likelihood ratio test statistics and Wald test statistics. Finally, we summarize the proposed model and discuss some problems and future work.","sentences":["In this paper, we propose a model that indicates the asymmetry structure for cell probabilities in multivariate contingency tables with the same ordered categories.","The proposed model is the closest to the symmetry model in terms of the $f$-divergence under certain conditions and incorporates various asymmetry models as special cases, including existing models.","We elucidate the relationship between the proposed model and conventional models from several aspects of divergence in $f$-divergence.","Furthermore, we provide theorems showing that the symmetry model can be decomposed into two or more models, each imposing less restrictive parameter constraints than the symmetry condition.","We also discuss the properties of goodness-of-fit statistics, particularly focusing on the likelihood ratio test statistics and Wald test statistics.","Finally, we summarize the proposed model and discuss some problems and future work."],"url":"http://arxiv.org/abs/2405.12157v1","category":"stat.ME"}
{"created":"2024-05-20 16:20:30","title":"A Faber-Krahn inequality for the Laplacian with drift under Robin boundary condition","abstract":"We prove a Faber-Krahn inequality for the Laplacian with drift under Robin boundary condition, provided that the $\\beta$ parameter in the Robin condition is large enough. The proof relies on a compactness argument, on the convergence of Robin eigenvalues to Dirichlet eigenvalues when $\\beta$ goes to infinity, and on a strict Faber-Krahn inequality under Dirichlet boundary condition. We also show the existence and uniqueness of drifts $v$ satisfying some $L^\\infty$ constraints and minimizing or maximizing the principal eigenvalue of $-\\Delta+v\\cdot\\nabla$ in a fixed domain and with a fixed parameter $\\beta>0$ in the Robin condition.","sentences":["We prove a Faber-Krahn inequality for the Laplacian with drift under Robin boundary condition, provided that the $\\beta$ parameter in the Robin condition is large enough.","The proof relies on a compactness argument, on the convergence of Robin eigenvalues to Dirichlet eigenvalues when $\\beta$ goes to infinity, and on a strict Faber-Krahn inequality under Dirichlet boundary condition.","We also show the existence and uniqueness of drifts $v$ satisfying some $L^\\infty$ constraints and minimizing or maximizing the principal eigenvalue of $-\\Delta+v\\cdot\\nabla$ in a fixed domain and with a fixed parameter $\\beta>0$ in the Robin condition."],"url":"http://arxiv.org/abs/2405.12148v1","category":"math.AP"}
{"created":"2024-05-20 15:48:32","title":"MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning","abstract":"Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models. In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.","sentences":["Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models.","In this paper, we analyze the impact of low-rank updating, as implemented in LoRA.","Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge.","Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters.","To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix.","Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA.","We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining.","Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks."],"url":"http://arxiv.org/abs/2405.12130v1","category":"cs.CL"}
{"created":"2024-05-20 15:47:42","title":"Elastic solids under frictionless rigid contact and configurational force","abstract":"A homogeneous elastic solid, bounded by a flat surface in its unstressed configuration, undergoes a finite strain when in frictionless contact against a rigid and rectilinear constraint, ending with a rounded or sharp corner, in a two-dimensional formulation. With a strong analogy to fracture mechanics, it is shown that (i.) a path-independent $J$--integral can be defined for frictionless contact problems, (ii.) which is equal to the energy release rate $G$ associated with an infinitesimal growth in the size of the frictionless constraint, and thus gives the value of the configurational force component along the sliding direction. Furthermore, it is found that (iii.) such a configurational sliding force is the Newtonian force component exerted by the elastic solid on the constraint at the frictionless contact. Assuming the kinematics of an Euler-Bernoulli rod for an elastic body of rectangular shape, the results (i.)--(iii.) lead to a new interpretation from a nonlinear solid mechanics perspective of the configurational forces recently disclosed for one-dimensional structures of variable length. Finally, approximate but closed-form solutions (validated with finite element simulations) are exploited to provide further insight into the effect of configurational forces. In particular, two applications are presented which show that a transverse compression can lead to Eulerian buckling or to longitudinal dynamic motion, both realizing novel examples of soft actuation mechanisms. As an application to biology, our results may provide a mechanical explanation for the observed phenomenon of negative durotaxis, where cells migrate from stiffer to softer environments.","sentences":["A homogeneous elastic solid, bounded by a flat surface in its unstressed configuration, undergoes a finite strain when in frictionless contact against a rigid and rectilinear constraint, ending with a rounded or sharp corner, in a two-dimensional formulation.","With a strong analogy to fracture mechanics, it is shown that (i.) a path-independent $J$--integral can be defined for frictionless contact problems, (ii.)","which is equal to the energy release rate $G$ associated with an infinitesimal growth in the size of the frictionless constraint, and thus gives the value of the configurational force component along the sliding direction.","Furthermore, it is found that (iii.)","such a configurational sliding force is the Newtonian force component exerted by the elastic solid on the constraint at the frictionless contact.","Assuming the kinematics of an Euler-Bernoulli rod for an elastic body of rectangular shape, the results (i.)--(iii.) lead to a new interpretation from a nonlinear solid mechanics perspective of the configurational forces recently disclosed for one-dimensional structures of variable length.","Finally, approximate but closed-form solutions (validated with finite element simulations) are exploited to provide further insight into the effect of configurational forces.","In particular, two applications are presented which show that a transverse compression can lead to Eulerian buckling or to longitudinal dynamic motion, both realizing novel examples of soft actuation mechanisms.","As an application to biology, our results may provide a mechanical explanation for the observed phenomenon of negative durotaxis, where cells migrate from stiffer to softer environments."],"url":"http://arxiv.org/abs/2405.12129v1","category":"cond-mat.soft"}
{"created":"2024-05-20 15:26:59","title":"AGNfitter-rx: Modelling the radio-to-X-ray SEDs of AGNs","abstract":"We present new frontiers in the modelling of the spectral energy distributions (SED) of active galaxies by introducing the radio-to-X-ray fitting capabilities of the publicly available Bayesian code AGNfitter. The new code release, called AGNfitter-rx, models the broad-band photometry covering the radio, infrared (IR), optical, ultraviolet (UV) and X-ray bands consistently, using a combination of theoretical and semi-empirical models of the AGN and host galaxy emission. This framework enables the detailed characterization of four physical components of the active nuclei: the accretion disk, the hot dusty torus, the relativistic jets/core radio emission, and the hot corona; alongside modeling three components within the host galaxy: stellar populations, cold dust, and the radio emission from the star-forming regions. Applying AGNfitter-rx to a diverse sample of 36 AGN SEDs at z<0.7 from the AGN SED ATLAS, we investigate and compare the performance of state-of-the-art torus and accretion disk emission models on fit quality and inferred physical parameters. We find that clumpy torus models that include polar winds and semi-empirical accretion disk templates including emission line features significantly increase the fit quality in 67% of the sources, by effectively reducing by $2\\sigma$ fit residuals in the $1.5-5 \\mu \\rm m$ and $0.7 \\mu \\rm m$ regimes.We demonstrate that, by applying AGNfitter-rx on photometric data, we are able to estimate inclination and opening angles of the torus, consistent with spectroscopic classifications within the AGN unified model, as well as black hole mass estimates in agreement with virial estimates based on H$\\alpha$. The wavelength coverage and the flexibility for the inclusion of state-of-the-art theoretical models make AGNfitter-rx a unique tool for the further development of SED modelling for AGNs in present and future radio-to-X-ray galaxy surveys.","sentences":["We present new frontiers in the modelling of the spectral energy distributions (SED) of active galaxies by introducing the radio-to-X-ray fitting capabilities of the publicly available Bayesian code AGNfitter.","The new code release, called AGNfitter-rx, models the broad-band photometry covering the radio, infrared (IR), optical, ultraviolet (UV) and X-ray bands consistently, using a combination of theoretical and semi-empirical models of the AGN and host galaxy emission.","This framework enables the detailed characterization of four physical components of the active nuclei: the accretion disk, the hot dusty torus, the relativistic jets/core radio emission, and the hot corona; alongside modeling three components within the host galaxy: stellar populations, cold dust, and the radio emission from the star-forming regions.","Applying AGNfitter-rx to a diverse sample of 36 AGN SEDs at z<0.7 from the AGN SED ATLAS, we investigate and compare the performance of state-of-the-art torus and accretion disk emission models on fit quality and inferred physical parameters.","We find that clumpy torus models that include polar winds and semi-empirical accretion disk templates including emission line features significantly increase the fit quality in 67% of the sources, by effectively reducing by $2\\sigma$ fit residuals in the $1.5-5 \\mu \\rm m$ and $0.7 \\mu \\rm m$ regimes.","We demonstrate that, by applying AGNfitter-rx on photometric data, we are able to estimate inclination and opening angles of the torus, consistent with spectroscopic classifications within the AGN unified model, as well as black hole mass estimates in agreement with virial estimates based on H$\\alpha$.","The wavelength coverage and the flexibility for the inclusion of state-of-the-art theoretical models make AGNfitter-rx a unique tool for the further development of SED modelling for AGNs in present and future radio-to-X-ray galaxy surveys."],"url":"http://arxiv.org/abs/2405.12111v1","category":"astro-ph.GA"}
{"created":"2024-05-20 15:25:47","title":"CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization","abstract":"3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D Gaussians to represent a scene. With sparse training views, 3DGS easily suffers from overfitting, negatively impacting the reconstruction quality. This paper introduces a new co-regularization perspective for improving sparse-view 3DGS. When training two 3D Gaussian radiance fields with the same sparse views of a scene, we observe that the two radiance fields exhibit \\textit{point disagreement} and \\textit{rendering disagreement} that can unsupervisedly predict reconstruction quality, stemming from the sampling implementation in densification. We further quantify the point disagreement and rendering disagreement by evaluating the registration between Gaussians' point representations and calculating differences in their rendered pixels. The empirical study demonstrates the negative correlation between the two disagreements and accurate reconstruction, which allows us to identify inaccurate reconstruction without accessing ground-truth information. Based on the study, we propose CoR-GS, which identifies and suppresses inaccurate reconstruction based on the two disagreements: (\\romannumeral1) Co-pruning considers Gaussians that exhibit high point disagreement in inaccurate positions and prunes them. (\\romannumeral2) Pseudo-view co-regularization considers pixels that exhibit high rendering disagreement are inaccurately rendered and suppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and Blender demonstrate that CoR-GS effectively regularizes the scene geometry, reconstructs the compact representations, and achieves state-of-the-art novel view synthesis quality under sparse training views.","sentences":["3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D Gaussians to represent a scene.","With sparse training views, 3DGS easily suffers from overfitting, negatively impacting the reconstruction quality.","This paper introduces a new co-regularization perspective for improving sparse-view 3DGS.","When training two 3D Gaussian radiance fields with the same sparse views of a scene, we observe that the two radiance fields exhibit \\textit{point disagreement} and \\textit{rendering disagreement} that can unsupervisedly predict reconstruction quality, stemming from the sampling implementation in densification.","We further quantify the point disagreement and rendering disagreement by evaluating the registration between Gaussians' point representations and calculating differences in their rendered pixels.","The empirical study demonstrates the negative correlation between the two disagreements and accurate reconstruction, which allows us to identify inaccurate reconstruction without accessing ground-truth information.","Based on the study, we propose CoR-GS, which identifies and suppresses inaccurate reconstruction based on the two disagreements: (\\romannumeral1) Co-pruning considers Gaussians that exhibit high point disagreement in inaccurate positions and prunes them.","(\\romannumeral2) Pseudo-view co-regularization considers pixels that exhibit high rendering disagreement are inaccurately rendered and suppress the disagreement.","Results on LLFF, Mip-NeRF360, DTU, and Blender demonstrate that CoR-GS effectively regularizes the scene geometry, reconstructs the compact representations, and achieves state-of-the-art novel view synthesis quality under sparse training views."],"url":"http://arxiv.org/abs/2405.12110v1","category":"cs.CV"}
{"created":"2024-05-20 15:23:19","title":"Imp: Highly Capable Large Multimodal Models for Mobile Devices","abstract":"By harnessing the capabilities of large language models (LLMs), recent large multimodal models (LMMs) have shown remarkable versatility in open-world multimodal understanding. Nevertheless, they are usually parameter-heavy and computation-intensive, thus hindering their applicability in resource-constrained scenarios. To this end, several lightweight LMMs have been proposed successively to maximize the capabilities under constrained scale (e.g., 3B). Despite the encouraging results achieved by these methods, most of them only focus on one or two aspects of the design space, and the key design choices that influence model capability have not yet been thoroughly investigated. In this paper, we conduct a systematic study for lightweight LMMs from the aspects of model architecture, training strategy, and training data. Based on our findings, we obtain Imp -- a family of highly capable LMMs at the 2B-4B scales. Notably, our Imp-3B model steadily outperforms all the existing lightweight LMMs of similar size, and even surpasses the state-of-the-art LMMs at the 13B scale. With low-bit quantization and resolution reduction techniques, our Imp model can be deployed on a Qualcomm Snapdragon 8Gen3 mobile chip with a high inference speed of about 13 tokens/s.","sentences":["By harnessing the capabilities of large language models (LLMs), recent large multimodal models (LMMs) have shown remarkable versatility in open-world multimodal understanding.","Nevertheless, they are usually parameter-heavy and computation-intensive, thus hindering their applicability in resource-constrained scenarios.","To this end, several lightweight LMMs have been proposed successively to maximize the capabilities under constrained scale (e.g., 3B).","Despite the encouraging results achieved by these methods, most of them only focus on one or two aspects of the design space, and the key design choices that influence model capability have not yet been thoroughly investigated.","In this paper, we conduct a systematic study for lightweight LMMs from the aspects of model architecture, training strategy, and training data.","Based on our findings, we obtain Imp -- a family of highly capable LMMs at the 2B-4B scales.","Notably, our Imp-3B model steadily outperforms all the existing lightweight LMMs of similar size, and even surpasses the state-of-the-art LMMs at the 13B scale.","With low-bit quantization and resolution reduction techniques, our Imp model can be deployed on a Qualcomm Snapdragon 8Gen3 mobile chip with a high inference speed of about 13 tokens/s."],"url":"http://arxiv.org/abs/2405.12107v1","category":"cs.CV"}
{"created":"2024-05-20 15:15:41","title":"SiO maser polarization and magnetic field in evolved cool stars","abstract":"Both magnetic fields and photospheric/atmospheric dynamics can be involved in triggering the important mass loss observed in evolved cool stars. Previous works have revealed that these objects exhibit a magnetic field extending beyond their surface. The origin of this magnetic field is still under debate with mechanisms involving a turbulent dynamo, convection, stellar pulsation, and cool spots. Our goal is to estimate the magnetic field strength in the inner circumstellar envelope of six evolved cool stars (five Miras and one Red Supergiant). Combining this work with previous studies, we tentatively constrain the global magnetic field type observed and shed light on the mechanisms at its origin. Using the XPOL polarimeter installed at the IRAM-30 m telescope, we observed the 28 SiO v = 1, J = 2-1 maser line emission and obtained simultaneous spectroscopic measurements of the four Stokes parameters. Applying a careful calibration method for Stokes Q, U, and V, we derive estimates of the magnetic field strength from the circular and linear polarization fractions considering the saturated and unsaturated maser cases under the Zeeman hypothesis. Magnetic field strengths from several Gauss up to several tens of Gauss are derived. These new and more accurate measurements constraining the field strength in the 2-5 stellar radii region better than previous studies and seem to exclude a global poloidal magnetic field type. A combination of a toroidal and a poloidal field is nevertheless not excluded. A variation of the magnetic field strength over a two-months timescale is observed in one Mira star which suggests a possible link to the stellar phase, i.e. with pulsation/photospheric activity.","sentences":["Both magnetic fields and photospheric/atmospheric dynamics can be involved in triggering the important mass loss observed in evolved cool stars.","Previous works have revealed that these objects exhibit a magnetic field extending beyond their surface.","The origin of this magnetic field is still under debate with mechanisms involving a turbulent dynamo, convection, stellar pulsation, and cool spots.","Our goal is to estimate the magnetic field strength in the inner circumstellar envelope of six evolved cool stars (five Miras and one Red Supergiant).","Combining this work with previous studies, we tentatively constrain the global magnetic field type observed and shed light on the mechanisms at its origin.","Using the XPOL polarimeter installed at the IRAM-30 m telescope, we observed the 28 SiO v = 1, J = 2-1 maser line emission and obtained simultaneous spectroscopic measurements of the four Stokes parameters.","Applying a careful calibration method for Stokes Q, U, and V, we derive estimates of the magnetic field strength from the circular and linear polarization fractions considering the saturated and unsaturated maser cases under the Zeeman hypothesis.","Magnetic field strengths from several Gauss up to several tens of Gauss are derived.","These new and more accurate measurements constraining the field strength in the 2-5 stellar radii region better than previous studies and seem to exclude a global poloidal magnetic field type.","A combination of a toroidal and a poloidal field is nevertheless not excluded.","A variation of the magnetic field strength over a two-months timescale is observed in one Mira star which suggests a possible link to the stellar phase, i.e. with pulsation/photospheric activity."],"url":"http://arxiv.org/abs/2405.12103v1","category":"astro-ph.SR"}
{"created":"2024-05-20 15:05:47","title":"Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?","abstract":"Transformer-based trajectory optimization methods have demonstrated exceptional performance in offline Reinforcement Learning (offline RL), yet it poses challenges due to substantial parameter size and limited scalability, which is particularly critical in sequential decision-making scenarios where resources are constrained such as in robots and drones with limited computational power. Mamba, a promising new linear-time sequence model, offers performance on par with transformers while delivering substantially fewer parameters on long sequences. As it remains unclear whether Mamba is compatible with trajectory optimization, this work aims to conduct comprehensive experiments to explore the potential of Decision Mamba in offline RL (dubbed DeMa) from the aspect of data structures and network architectures with the following insights: (1) Long sequences impose a significant computational burden without contributing to performance improvements due to the fact that DeMa's focus on sequences diminishes approximately exponentially. Consequently, we introduce a Transformer-like DeMa as opposed to an RNN-like DeMa. (2) For the components of DeMa, we identify that the hidden attention mechanism is key to its success, which can also work well with other residual structures and does not require position embedding. Extensive evaluations from eight Atari games demonstrate that our specially designed DeMa is compatible with trajectory optimization and surpasses previous state-of-the-art methods, outdoing Decision Transformer (DT) by 80\\% with 30\\% fewer parameters, and exceeds DT in MuJoCo with only a quarter of the parameters.","sentences":["Transformer-based trajectory optimization methods have demonstrated exceptional performance in offline Reinforcement Learning (offline RL), yet it poses challenges due to substantial parameter size and limited scalability, which is particularly critical in sequential decision-making scenarios where resources are constrained such as in robots and drones with limited computational power.","Mamba, a promising new linear-time sequence model, offers performance on par with transformers while delivering substantially fewer parameters on long sequences.","As it remains unclear whether Mamba is compatible with trajectory optimization, this work aims to conduct comprehensive experiments to explore the potential of Decision Mamba in offline RL (dubbed DeMa) from the aspect of data structures and network architectures with the following insights: (1) Long sequences impose a significant computational burden without contributing to performance improvements due to the fact that DeMa's focus on sequences diminishes approximately exponentially.","Consequently, we introduce a Transformer-like DeMa as opposed to an RNN-like DeMa.","(2) For the components of DeMa, we identify that the hidden attention mechanism is key to its success, which can also work well with other residual structures and does not require position embedding.","Extensive evaluations from eight Atari games demonstrate that our specially designed DeMa is compatible with trajectory optimization and surpasses previous state-of-the-art methods, outdoing Decision Transformer (DT) by 80\\% with 30\\% fewer parameters, and exceeds DT in MuJoCo with only a quarter of the parameters."],"url":"http://arxiv.org/abs/2405.12094v1","category":"cs.LG"}
{"created":"2024-05-20 14:56:06","title":"Pressure-induced nearly perfect rectangular lattice and superconductivity in an organic molecular crystal (DMET-TTF)$_2$AuBr$_2$","abstract":"External pressure and associated changes in lattice structures are key to realizing exotic quantum phases such as high-$T_{\\rm c}$ superconductivity. While applying external pressure is a standard method to induce novel lattice structures, its impact on organic molecular crystals has been less explored. Here we report a unique structural phase transition in (DMET-TTF)$_2$AuBr$_2$ under pressure. By combining advanced high-pressure techniques and $ab$ $initio$ calculations, we elucidate that (DMET-TTF)$_2$AuBr$_2$ undergoes a transition from a quasi-one-dimensional lattice to a nearly perfect rectangular lattice at 0.9 GPa. This transition leads to the realization of an antiferromagnetic Mott insulator with $T_{\\rm N}=66$ K, the highest $T_{\\rm N}$ in low-dimensional molecular crystal solids to date. Upon increasing the pressure, the antiferromagnetic ordering is suppressed, and a superconducting phase with $T_{\\rm c}=4.8$ K emerges around 6 GPa. Our study reveals the significant impact of external pressure on lattice structures of organic molecular crystals and highlights the intricate relationship between geometrical frustration and superconductivity. Our findings also pave the way for realizing functional organic molecular crystals through changes in lattice structures by pressure.","sentences":["External pressure and associated changes in lattice structures are key to realizing exotic quantum phases such as high-$T_{\\rm c}$ superconductivity.","While applying external pressure is a standard method to induce novel lattice structures, its impact on organic molecular crystals has been less explored.","Here we report a unique structural phase transition in (DMET-TTF)$_2$AuBr$_2$ under pressure.","By combining advanced high-pressure techniques and $ab$ $initio$ calculations, we elucidate that (DMET-TTF)$_2$AuBr$_2$ undergoes a transition from a quasi-one-dimensional lattice to a nearly perfect rectangular lattice at 0.9 GPa.","This transition leads to the realization of an antiferromagnetic Mott insulator with $T_{\\rm N}=66$ K, the highest $T_{\\rm N}$ in low-dimensional molecular crystal solids to date.","Upon increasing the pressure, the antiferromagnetic ordering is suppressed, and a superconducting phase with $T_{\\rm c}=4.8$ K emerges around 6 GPa.","Our study reveals the significant impact of external pressure on lattice structures of organic molecular crystals and highlights the intricate relationship between geometrical frustration and superconductivity.","Our findings also pave the way for realizing functional organic molecular crystals through changes in lattice structures by pressure."],"url":"http://arxiv.org/abs/2405.12086v1","category":"cond-mat.str-el"}
{"created":"2024-05-20 14:52:05","title":"Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model","abstract":"To obtain high-quality annotations under limited budget, semi-automatic annotation methods are commonly used, where a portion of the data is annotated by experts and a model is then trained to complete the annotations for the remaining data. However, these methods mainly focus on selecting informative data for expert annotations to improve the model predictive ability (i.e., triage-to-human data), while the rest of the data is indiscriminately assigned to model annotation (i.e., triage-to-model data). This may lead to inefficiencies in budget allocation for annotations, as easy data that the model could accurately annotate may be unnecessarily assigned to the expert, and hard data may be misclassified by the model. As a result, the overall annotation quality may be compromised. To address this issue, we propose a selective annotation framework called SANT. It effectively takes advantage of both the triage-to-human and triage-to-model data through the proposed error-aware triage and bi-weighting mechanisms. As such, informative or hard data is assigned to the expert for annotation, while easy data is handled by the model. Experimental results show that SANT consistently outperforms other baselines, leading to higher-quality annotation through its proper allocation of data to both expert and model workers. We provide pioneering work on data annotation within budget constraints, establishing a landmark for future triage-based annotation studies.","sentences":["To obtain high-quality annotations under limited budget, semi-automatic annotation methods are commonly used, where a portion of the data is annotated by experts and a model is then trained to complete the annotations for the remaining data.","However, these methods mainly focus on selecting informative data for expert annotations to improve the model predictive ability (i.e., triage-to-human data), while the rest of the data is indiscriminately assigned to model annotation (i.e., triage-to-model data).","This may lead to inefficiencies in budget allocation for annotations, as easy data that the model could accurately annotate may be unnecessarily assigned to the expert, and hard data may be misclassified by the model.","As a result, the overall annotation quality may be compromised.","To address this issue, we propose a selective annotation framework called SANT.","It effectively takes advantage of both the triage-to-human and triage-to-model data through the proposed error-aware triage and bi-weighting mechanisms.","As such, informative or hard data is assigned to the expert for annotation, while easy data is handled by the model.","Experimental results show that SANT consistently outperforms other baselines, leading to higher-quality annotation through its proper allocation of data to both expert and model workers.","We provide pioneering work on data annotation within budget constraints, establishing a landmark for future triage-based annotation studies."],"url":"http://arxiv.org/abs/2405.12081v1","category":"cs.CL"}
{"created":"2024-05-20 14:45:55","title":"Inequalities between Dirichlet and Neumann eigenvalues of the magnetic Laplacian","abstract":"We consider the magnetic Laplacian with the homogeneous magnetic field in two and three dimensions. We prove that the $(k+1)$-th magnetic Neumann eigenvalue of a bounded convex planar domain is not larger than its $k$-th magnetic Dirichlet eigenvalue. In three dimensions, we restrict our attention to convex domains, which are invariant under rotation by an angle of $\\pi$ around an axis parallel to the magnetic field. For such domains, we prove that the $(k+2)$-th magnetic Neumann eigenvalue is not larger than the $k$-th magnetic Dirichlet eigenvalue provided that this Dirichlet eigenvalue is simple. The proofs rely on a modification of the strategy due to Levine and Weinberger.","sentences":["We consider the magnetic Laplacian with the homogeneous magnetic field in two and three dimensions.","We prove that the $(k+1)$-th magnetic Neumann eigenvalue of a bounded convex planar domain is not larger than its $k$-th magnetic Dirichlet eigenvalue.","In three dimensions, we restrict our attention to convex domains, which are invariant under rotation by an angle of $\\pi$ around an axis parallel to the magnetic field.","For such domains, we prove that the $(k+2)$-th magnetic Neumann eigenvalue is not larger than the $k$-th magnetic Dirichlet eigenvalue provided that this Dirichlet eigenvalue is simple.","The proofs rely on a modification of the strategy due to Levine and Weinberger."],"url":"http://arxiv.org/abs/2405.12077v1","category":"math.SP"}
{"created":"2024-05-20 14:39:49","title":"Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping","abstract":"By equipping the most recent 3D Gaussian Splatting representation with head 3D morphable models (3DMM), existing methods manage to create head avatars with high fidelity. However, most existing methods only reconstruct a head without the body, substantially limiting their application scenarios. We found that naively applying Gaussians to model the clothed chest and shoulders tends to result in blurry reconstruction and noisy floaters under novel poses. This is because of the fundamental limitation of Gaussians and point clouds -- each Gaussian or point can only have a single directional radiance without spatial variance, therefore an unnecessarily large number of them is required to represent complicated spatially varying texture, even for simple geometry. In contrast, we propose to model the body part with a neural texture that consists of coarse and pose-dependent fine colors. To properly render the body texture for each view and pose without accurate geometry nor UV mapping, we optimize another sparse set of Gaussians as anchors that constrain the neural warping field that maps image plane coordinates to the texture space. We demonstrate that Gaussian Head & Shoulders can fit the high-frequency details on the clothed upper body with high fidelity and potentially improve the accuracy and fidelity of the head region. We evaluate our method with casual phone-captured and internet videos and show our method archives superior reconstruction quality and robustness in both self and cross reenactment tasks. To fully utilize the efficient rendering speed of Gaussian splatting, we additionally propose an accelerated inference method of our trained model without Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of around 130 FPS for any subjects.","sentences":["By equipping the most recent 3D Gaussian Splatting representation with head 3D morphable models (3DMM), existing methods manage to create head avatars with high fidelity.","However, most existing methods only reconstruct a head without the body, substantially limiting their application scenarios.","We found that naively applying Gaussians to model the clothed chest and shoulders tends to result in blurry reconstruction and noisy floaters under novel poses.","This is because of the fundamental limitation of Gaussians and point clouds -- each Gaussian or point can only have a single directional radiance without spatial variance, therefore an unnecessarily large number of them is required to represent complicated spatially varying texture, even for simple geometry.","In contrast, we propose to model the body part with a neural texture that consists of coarse and pose-dependent fine colors.","To properly render the body texture for each view and pose without accurate geometry nor UV mapping, we optimize another sparse set of Gaussians as anchors that constrain the neural warping field that maps image plane coordinates to the texture space.","We demonstrate that Gaussian Head & Shoulders can fit the high-frequency details on the clothed upper body with high fidelity and potentially improve the accuracy and fidelity of the head region.","We evaluate our method with casual phone-captured and internet videos and show our method archives superior reconstruction quality and robustness in both self and cross reenactment tasks.","To fully utilize the efficient rendering speed of Gaussian splatting, we additionally propose an accelerated inference method of our trained model without Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of around 130 FPS for any subjects."],"url":"http://arxiv.org/abs/2405.12069v1","category":"cs.CV"}
{"created":"2024-05-20 14:18:36","title":"Parallelization of the K-Means Algorithm with Applications to Big Data Clustering","abstract":"The K-Means clustering using LLoyd's algorithm is an iterative approach to partition the given dataset into K different clusters. The algorithm assigns each point to the cluster based on the following objective function   \\[\\ \\min \\Sigma_{i=1}^{n}||x_i-\\mu_{x_i}||^2\\] The serial algorithm involves iterative steps where we compute the distance of each datapoint from the centroids and assign the datapoint to the nearest centroid. This approach is essentially known as the expectation-maximization step. Clustering involves extensive computations to calculate distances at each iteration, which increases as the number of data points increases. This provides scope for parallelism. However, we must ensure that in a parallel process, each thread has access to the updated centroid value and no racing condition exists on any centroid values. We will compare two different approaches in this project. The first approach is an OpenMP flat synchronous method where all processes are run in parallel, and we use synchronization to ensure safe updates of clusters. The second approach we adopt is a GPU based parallelization approach using OpenACC wherein we will try to make use of GPU architecture to parallelize chunks of the algorithm to observe decreased computation time. We will analyze metrics such as speed up, efficiency,time taken with varying data points, and number of processes to compare the two approaches and understand the relative performance improvement we can get.","sentences":["The K-Means clustering using LLoyd's algorithm is an iterative approach to partition the given dataset into K different clusters.","The algorithm assigns each point to the cluster based on the following objective function   \\[\\ \\min \\Sigma_{i=1}^{n}||x_i-\\mu_{x_i}||^2\\]","The serial algorithm involves iterative steps where we compute the distance of each datapoint from the centroids and assign the datapoint to the nearest centroid.","This approach is essentially known as the expectation-maximization step.","Clustering involves extensive computations to calculate distances at each iteration, which increases as the number of data points increases.","This provides scope for parallelism.","However, we must ensure that in a parallel process, each thread has access to the updated centroid value and no racing condition exists on any centroid values.","We will compare two different approaches in this project.","The first approach is an OpenMP flat synchronous method where all processes are run in parallel, and we use synchronization to ensure safe updates of clusters.","The second approach we adopt is a GPU based parallelization approach using OpenACC wherein we will try to make use of GPU architecture to parallelize chunks of the algorithm to observe decreased computation time.","We will analyze metrics such as speed up, efficiency,time taken with varying data points, and number of processes to compare the two approaches and understand the relative performance improvement we can get."],"url":"http://arxiv.org/abs/2405.12052v1","category":"cs.DC"}
{"created":"2024-05-20 14:14:48","title":"Pointwise well-posedness results for degenerate It\u00f4-SDEs with locally bounded drifts","abstract":"Building on results developed in https://doi.org/10.48550/arXiv.2404.14902, where It\\^{o}-SDEs with possibly degenerate and discontinuous dispersion coefficient and measurable drift were analyzed with respect to a given (sub-)invariant measure, we develop here additional elliptic regularity results for PDEs and consider the same equations with some further regularity assumptions on the coefficients to provide a pointwise analysis for every starting point in Euclidean space, $d\\ge 2$. Our main result is (weak) well-posedness, i.e. weak existence and uniqueness in law, which we obtain under our main assumption for any locally bounded drift and arbitrary starting point among all solutions that spend zero time at the points of degeneracy of the dispersion coefficient. The points of degeneracy form a $d$-dimensional Lebesgue measure zero set, but may be hit by the weak solutions. Weak existence for arbitrary starting point is obtained under broader assumptions. In particular, in that case the drift does not need to be locally bounded.","sentences":["Building on results developed in https://doi.org/10.48550/arXiv.2404.14902, where It\\^{o}-SDEs with possibly degenerate and discontinuous dispersion coefficient and measurable drift were analyzed with respect to a given (sub-)invariant measure, we develop here additional elliptic regularity results for PDEs and consider the same equations with some further regularity assumptions on the coefficients to provide a pointwise analysis for every starting point in Euclidean space, $d\\ge 2$.","Our main result is (weak) well-posedness, i.e. weak existence and uniqueness in law, which we obtain under our main assumption for any locally bounded drift and arbitrary starting point among all solutions that spend zero time at the points of degeneracy of the dispersion coefficient.","The points of degeneracy form a $d$-dimensional Lebesgue measure zero set, but may be hit by the weak solutions.","Weak existence for arbitrary starting point is obtained under broader assumptions.","In particular, in that case the drift does not need to be locally bounded."],"url":"http://arxiv.org/abs/2405.12048v1","category":"math.PR"}
{"created":"2024-05-20 14:09:28","title":"Attribute-Based Authentication in Secure Group Messaging for Distributed Environments","abstract":"Messaging Layer security (MLS) and its underlying Continuous Group Key Agreement (CGKA) protocol allows a group of users to share a cryptographic secret in a dynamic manner, such that the secret is modified in member insertions and deletions. Although this flexibility makes MLS ideal for implementations in distributed environments, a number of issues need to be overcome. Particularly, the use of digital certificates for authentication in a group goes against the group members' privacy. In this work we provide an alternative method of authentication in which the solicitors, instead of revealing their identity, only need to prove possession of certain attributes, dynamically defined by the group, to become a member. Instead of digital certificates, we employ Attribute-Based Credentials accompanied with Selective Disclosure in order to reveal the minimum required amount of information and to prevent attackers from linking the activity of a user through multiple groups. We formally define a CGKA variant named Attribute-Authenticated Continuous Group Key Agreement (AA-CGKA) and provide security proofs for its properties of Requirement Integrity, Unforgeability and Unlinkability. We also provide guidelines for an integration of our construction in MLS.","sentences":["Messaging Layer security (MLS) and its underlying Continuous Group Key Agreement (CGKA) protocol allows a group of users to share a cryptographic secret in a dynamic manner, such that the secret is modified in member insertions and deletions.","Although this flexibility makes MLS ideal for implementations in distributed environments, a number of issues need to be overcome.","Particularly, the use of digital certificates for authentication in a group goes against the group members' privacy.","In this work we provide an alternative method of authentication in which the solicitors, instead of revealing their identity, only need to prove possession of certain attributes, dynamically defined by the group, to become a member.","Instead of digital certificates, we employ Attribute-Based Credentials accompanied with Selective Disclosure in order to reveal the minimum required amount of information and to prevent attackers from linking the activity of a user through multiple groups.","We formally define a CGKA variant named Attribute-Authenticated Continuous Group Key Agreement (AA-CGKA) and provide security proofs for its properties of Requirement Integrity, Unforgeability and Unlinkability.","We also provide guidelines for an integration of our construction in MLS."],"url":"http://arxiv.org/abs/2405.12042v1","category":"cs.CR"}
{"created":"2024-05-20 13:44:50","title":"Enzymatic cycle-based receivers with high input impedance for approximate maximum a posteriori demodulation of concentration modulated signals","abstract":"Molecular communication is a bio-inspired communication paradigm where molecules are used as the information carrier. This paper considers a molecular communication network where the transmitter uses concentration modulated signals for communication. Our focus is to design receivers that can demodulate these signals. We impose three features on our receivers. We want the receivers to use enzymatic cycles as their building blocks, have high input impedance and can work approximately as a maximum a posteriori (MAP) demodulator. No receivers with all these three features exist in the current molecular communication literature. We consider enzymatic cycles because they are a very common class of chemical reactions that are found in living cells. Since a receiver is to be placed in the communication environment, it should ideally have a high input impedance so that it has minimal impact on the environment and on other receivers. Lastly, a MAP receiver has good statistical performance. In this paper, we show how we can use time-scale separation to make an enzymatic cycle to have high input impedance and how the parameters of the enzymatic cycles can be chosen so that the receiver can approximately implement a MAP demodulator. We use simulation to study the performance of this receiver. In particular, we consider an environment with multiple receivers and show that a receiver has little impact on the bit error ratio of a nearby receiver because they have high input impedance.","sentences":["Molecular communication is a bio-inspired communication paradigm where molecules are used as the information carrier.","This paper considers a molecular communication network where the transmitter uses concentration modulated signals for communication.","Our focus is to design receivers that can demodulate these signals.","We impose three features on our receivers.","We want the receivers to use enzymatic cycles as their building blocks, have high input impedance and can work approximately as a maximum a posteriori (MAP) demodulator.","No receivers with all these three features exist in the current molecular communication literature.","We consider enzymatic cycles because they are a very common class of chemical reactions that are found in living cells.","Since a receiver is to be placed in the communication environment, it should ideally have a high input impedance so that it has minimal impact on the environment and on other receivers.","Lastly, a MAP receiver has good statistical performance.","In this paper, we show how we can use time-scale separation to make an enzymatic cycle to have high input impedance and how the parameters of the enzymatic cycles can be chosen so that the receiver can approximately implement a MAP demodulator.","We use simulation to study the performance of this receiver.","In particular, we consider an environment with multiple receivers and show that a receiver has little impact on the bit error ratio of a nearby receiver because they have high input impedance."],"url":"http://arxiv.org/abs/2405.12026v1","category":"cs.IT"}
{"created":"2024-05-20 13:26:59","title":"DarkDNS: Revisiting the Value of Rapid Zone Update","abstract":"Malicious actors exploit the DNS namespace to launch spam campaigns, phishing attacks, malware, and other harmful activities. Combating these threats requires visibility into domain existence, ownership and nameservice activity that the DNS protocol does not itself provide. To facilitate visibility and security-related study of the expanding gTLD namespace, ICANN introduced the Centralized Zone Data Service (CZDS) that shares daily zone file snapshots of new gTLD zones. However, a remarkably high concentration of malicious activity is associated with domains that do not live long enough make it into these daily snapshots. Using public and private sources of newly observed domains to identify this activity, we discover that even with the best available data there is a considerable visibility gap. We find that the daily snapshots miss at least 1% of newly registered and short-lived domains, which are almost always registered with malicious intent. In reducing this critical visibility gap using public sources of data, we demonstrate how more timely access to TLD zone changes can help better prevent abuse. We hope that this work sparks a discussion in the community on how to effectively and safely revive the concept of sharing Rapid Zone Updates for security research.","sentences":["Malicious actors exploit the DNS namespace to launch spam campaigns, phishing attacks, malware, and other harmful activities.","Combating these threats requires visibility into domain existence, ownership and nameservice activity that the DNS protocol does not itself provide.","To facilitate visibility and security-related study of the expanding gTLD namespace, ICANN introduced the Centralized Zone Data Service (CZDS) that shares daily zone file snapshots of new gTLD zones.","However, a remarkably high concentration of malicious activity is associated with domains that do not live long enough make it into these daily snapshots.","Using public and private sources of newly observed domains to identify this activity, we discover that even with the best available data there is a considerable visibility gap.","We find that the daily snapshots miss at least 1% of newly registered and short-lived domains, which are almost always registered with malicious intent.","In reducing this critical visibility gap using public sources of data, we demonstrate how more timely access to TLD zone changes can help better prevent abuse.","We hope that this work sparks a discussion in the community on how to effectively and safely revive the concept of sharing Rapid Zone Updates for security research."],"url":"http://arxiv.org/abs/2405.12010v1","category":"cs.NI"}
{"created":"2024-05-20 12:44:26","title":"DuckDB-SGX2: The Good, The Bad and The Ugly within Confidential Analytical Query Processing","abstract":"We provide an evaluation of an analytical workload in a confidential computing environment, combining DuckDB with two technologies: modular columnar encryption in Parquet files (data at rest) and the newest version of the Intel SGX Trusted Execution Environment (TEE), providing a hardware enclave where data in flight can be (more) securely decrypted and processed. One finding is that the \"performance tax\" for such confidential analytical processing is acceptable compared to not using these technologies. We eventually manage to run TPC-H SF30 with under 2x overhead compared to non-encrypted, non-enclave execution; we show that, specifically, columnar compression and encryption are a good combination. Our second finding consists of dos and don'ts to tune DuckDB to work effectively in this environment. There are various performance hazards: potentially 5x higher cache miss costs due to memory encryption inside the enclave, NUMA penalties, and highly elevated cost of swapping pages in and out of the enclave -- which is also triggered indirectly by using a non-SGX-aware malloc library.","sentences":["We provide an evaluation of an analytical workload in a confidential computing environment, combining DuckDB with two technologies: modular columnar encryption in Parquet files (data at rest) and the newest version of the Intel SGX Trusted Execution Environment (TEE), providing a hardware enclave where data in flight can be (more) securely decrypted and processed.","One finding is that the \"performance tax\" for such confidential analytical processing is acceptable compared to not using these technologies.","We eventually manage to run TPC-H SF30 with under 2x overhead compared to non-encrypted, non-enclave execution; we show that, specifically, columnar compression and encryption are a good combination.","Our second finding consists of dos and don'ts to tune DuckDB to work effectively in this environment.","There are various performance hazards: potentially 5x higher cache miss costs due to memory encryption inside the enclave, NUMA penalties, and highly elevated cost of swapping pages in and out of the enclave -- which is also triggered indirectly by using a non-SGX-aware malloc library."],"url":"http://arxiv.org/abs/2405.11988v1","category":"cs.DB"}
{"created":"2024-05-20 12:36:20","title":"Scheduling Jobs with Work-Inefficient Parallel Solutions","abstract":"This paper introduces the \\emph{serial-parallel decision problem}. Consider an online scheduler that receives a series of tasks, where each task has both a parallel and a serial implementation. The parallel implementation has the advantage that it can make progress concurrently on multiple processors, but the disadvantage that it is (potentially) work-inefficient. As tasks arrive, the scheduler must decide for each task which implementation to use.   We begin by studying \\emph{total awake time}. We give a simple \\emph{decide-on-arrival} scheduler that achieves a competitive ratio of $3$ for total awake time -- this scheduler makes serial/parallel decisions immediately when jobs arrive. Our second result is an \\emph{parallel-work-oblivious} scheduler that achieves a competitive ratio of $6$ for total awake time -- this scheduler makes all of its decisions based only on the size of each serial job and without needing to know anything about the parallel implementations. Finally, we prove a lower bound showing that, if a scheduler wishes to achieve a competitive ratio of $O(1)$, it can have at most one of the two aforementioned properties (decide-on-arrival or parallel-work-oblivious). We also prove lower bounds of the form $1 + \\Omega(1)$ on the optimal competitive ratio for any scheduler.   Next, we turn our attention to optimizing \\emph{mean response time}. Here, we show that it is possible to achieve an $O(1)$ competitive ratio with $O(1)$ speed augmentation. This is the most technically involved of our results. We also prove that, in this setting, it is not possible for a parallel-work-oblivious scheduler to do well.   In addition to these results, we present tight bounds on the optimal competitive ratio if we allow for arrival dependencies between tasks (e.g., tasks are components of a single parallel program), and we give an in-depth discussion of the remaining open questions.","sentences":["This paper introduces the \\emph{serial-parallel decision problem}.","Consider an online scheduler that receives a series of tasks, where each task has both a parallel and a serial implementation.","The parallel implementation has the advantage that it can make progress concurrently on multiple processors, but the disadvantage that it is (potentially) work-inefficient.","As tasks arrive, the scheduler must decide for each task which implementation to use.   ","We begin by studying \\emph{total awake time}.","We give a simple \\emph{decide-on-arrival} scheduler that achieves a competitive ratio of $3$ for total awake time -- this scheduler makes serial/parallel decisions immediately when jobs arrive.","Our second result is an \\emph{parallel-work-oblivious} scheduler that achieves a competitive ratio of $6$ for total awake time -- this scheduler makes all of its decisions based only on the size of each serial job and without needing to know anything about the parallel implementations.","Finally, we prove a lower bound showing that, if a scheduler wishes to achieve a competitive ratio of $O(1)$, it can have at most one of the two aforementioned properties (decide-on-arrival or parallel-work-oblivious).","We also prove lower bounds of the form $1 + \\Omega(1)$ on the optimal competitive ratio for any scheduler.   ","Next, we turn our attention to optimizing \\emph{mean response time}.","Here, we show that it is possible to achieve an $O(1)$ competitive ratio with $O(1)$ speed augmentation.","This is the most technically involved of our results.","We also prove that, in this setting, it is not possible for a parallel-work-oblivious scheduler to do well.   ","In addition to these results, we present tight bounds on the optimal competitive ratio if we allow for arrival dependencies between tasks (e.g., tasks are components of a single parallel program), and we give an in-depth discussion of the remaining open questions."],"url":"http://arxiv.org/abs/2405.11986v1","category":"cs.DS"}
{"created":"2024-05-20 12:30:50","title":"Khovanov algebras of type B and tensor powers of the natural $\\mathrm{OSp}$-representation","abstract":"We develop the theory of projective endofunctors for modules of Khovanov algebras $K$ of type B. In particular we compute the composition factors and the graded layers of the image of a simple module under such a projective functor. We then study variants of such functors for a subquotient $e\\tilde{K}e$. Via a comparison of two graded lifts of the Brauer algebra we relate the Khovanov algebra to the Brauer algebra and use this to show that projective functors describe translation functors on representations of the orthosymplectic supergroup $\\mathrm{OSp}(r|2n)$. As an application we get a description of the Loewy layers of indecomposable summands in tensor powers of the natural representation of $\\mathrm{OSp}(r|2n)$.","sentences":["We develop the theory of projective endofunctors for modules of Khovanov algebras $K$ of type B.","In particular we compute the composition factors and the graded layers of the image of a simple module under such a projective functor.","We then study variants of such functors for a subquotient $e\\tilde{K}e$. Via a comparison of two graded lifts of the Brauer algebra we relate the Khovanov algebra to the Brauer algebra and use this to show that projective functors describe translation functors on representations of the orthosymplectic supergroup $\\mathrm{OSp}(r|2n)$. As an application we get a description of the Loewy layers of indecomposable summands in tensor powers of the natural representation of $\\mathrm{OSp}(r|2n)$."],"url":"http://arxiv.org/abs/2405.11981v1","category":"math.RT"}
{"created":"2024-05-20 11:37:24","title":"Subspace embedding with random Khatri-Rao products and its application to eigensolvers","abstract":"Various iterative eigenvalue solvers have been developed to compute parts of the spectrum for a large sparse matrix, including the power method, Krylov subspace methods, contour integral methods, and preconditioned solvers such as the so called LOBPCG method. All of these solvers rely on random matrices to determine, e.g., starting vectors that have, with high probability, a non-negligible overlap with the eigenvectors of interest. For this purpose, a safe and common choice are unstructured Gaussian random matrices. In this work, we investigate the use of random Khatri-Rao products in eigenvalue solvers. On the one hand, we establish a novel subspace embedding property that provides theoretical justification for the use of such structured random matrices. On the other hand, we highlight the potential algorithmic benefits when solving eigenvalue problems with Kronecker product structure, as they arise frequently from the discretization of eigenvalue problems for differential operators on tensor product domains. In particular, we consider the use of random Khatri-Rao products within a contour integral method and LOBPCG. Numerical experiments indicate that the gains for the contour integral method strongly depend on the ability to efficiently and accurately solve (shifted) matrix equations with low-rank right-hand side. The flexibility of LOBPCG to directly employ preconditioners makes it easier to benefit from Khatri-Rao product structure, at the expense of having less theoretical justification.","sentences":["Various iterative eigenvalue solvers have been developed to compute parts of the spectrum for a large sparse matrix, including the power method, Krylov subspace methods, contour integral methods, and preconditioned solvers such as the so called LOBPCG method.","All of these solvers rely on random matrices to determine, e.g., starting vectors that have, with high probability, a non-negligible overlap with the eigenvectors of interest.","For this purpose, a safe and common choice are unstructured Gaussian random matrices.","In this work, we investigate the use of random Khatri-Rao products in eigenvalue solvers.","On the one hand, we establish a novel subspace embedding property that provides theoretical justification for the use of such structured random matrices.","On the other hand, we highlight the potential algorithmic benefits when solving eigenvalue problems with Kronecker product structure, as they arise frequently from the discretization of eigenvalue problems for differential operators on tensor product domains.","In particular, we consider the use of random Khatri-Rao products within a contour integral method and LOBPCG.","Numerical experiments indicate that the gains for the contour integral method strongly depend on the ability to efficiently and accurately solve (shifted) matrix equations with low-rank right-hand side.","The flexibility of LOBPCG to directly employ preconditioners makes it easier to benefit from Khatri-Rao product structure, at the expense of having less theoretical justification."],"url":"http://arxiv.org/abs/2405.11962v1","category":"math.NA"}
{"created":"2024-05-20 09:14:04","title":"$b\\to{}s\\ell\\ell$ decays at LHCb","abstract":"Flavour changing neutral currents are suppressed in the Standard Model, making them a prime avenue to search for new physics. Although several measurements of the decay $B^{0}\\to{}K^{\\ast{}0}\\mu^{+}\\mu^{-}$ have shown deviations from the Standard Model expectations, long-distance contributions may be imitating new physics. Here, the first amplitude analysis of $B^{0}\\to{}K^{\\ast{}0}\\mu^{+}\\mu^{-}$ is carried out to try and discern such effects.","sentences":["Flavour changing neutral currents are suppressed in the Standard Model, making them a prime avenue to search for new physics.","Although several measurements of the decay $B^{0}\\to{}K^{\\ast{}0}\\mu^{+}\\mu^{-}$ have shown deviations from the Standard Model expectations, long-distance contributions may be imitating new physics.","Here, the first amplitude analysis of $B^{0}\\to{}K^{\\ast{}0}\\mu^{+}\\mu^{-}$ is carried out to try and discern such effects."],"url":"http://arxiv.org/abs/2405.11890v1","category":"hep-ex"}
{"created":"2024-05-20 08:31:17","title":"A study of the reconnection of antiparallel vortices in the infinitely thin case and in the finite thickness case","abstract":"The simplest case is the reconnection of a pair of antiparallel line vortices, e.g., condensation trails of an aircraft. The vortices first undergo long wave deformation (Crow waves), and then reconnect to form coherent structures. Although the behavior of the vortices before and after the reconnection can be clearly observed, what happens during the reconnection still needs to be explained. One of the challenges is related to the fact that the vortices have finite thickness, and therefore, the time and the point of the reconnection cannot be determined. Moreover, the smallest scale of coherent structures that can be observed also depends on the vortex thickness. In this paper, we consider an infinitely thin vortex approximation to study the reconnection process. We show that, in this case, the behavior after the reconnection is quasi-periodic, with the quasi-period being independent of the angle between the vortices at the time of the reconnection. We also show that, in the Fourier transform of the trajectory of the reconnection point, the frequencies that correspond to squares of integers are dominating in a similar way as in the evolution of a polygonal vortex under the localized induction approximation. At the end, we compare the results with a solution of the Navier-Stokes equations for the reconnection of a pair of antiparallel vortices with finite thickness. We use the fluid impulse to determine the reconnection time, the reconnection point, and the quasi-period for this case.","sentences":["The simplest case is the reconnection of a pair of antiparallel line vortices, e.g., condensation trails of an aircraft.","The vortices first undergo long wave deformation (Crow waves), and then reconnect to form coherent structures.","Although the behavior of the vortices before and after the reconnection can be clearly observed, what happens during the reconnection still needs to be explained.","One of the challenges is related to the fact that the vortices have finite thickness, and therefore, the time and the point of the reconnection cannot be determined.","Moreover, the smallest scale of coherent structures that can be observed also depends on the vortex thickness.","In this paper, we consider an infinitely thin vortex approximation to study the reconnection process.","We show that, in this case, the behavior after the reconnection is quasi-periodic, with the quasi-period being independent of the angle between the vortices at the time of the reconnection.","We also show that, in the Fourier transform of the trajectory of the reconnection point, the frequencies that correspond to squares of integers are dominating in a similar way as in the evolution of a polygonal vortex under the localized induction approximation.","At the end, we compare the results with a solution of the Navier-Stokes equations for the reconnection of a pair of antiparallel vortices with finite thickness.","We use the fluid impulse to determine the reconnection time, the reconnection point, and the quasi-period for this case."],"url":"http://arxiv.org/abs/2405.11875v1","category":"math.AP"}
{"created":"2024-05-20 08:13:46","title":"SEMv3: A Fast and Robust Approach to Table Separation Line Detection","abstract":"Table structure recognition (TSR) aims to parse the inherent structure of a table from its input image. The `\"split-and-merge\" paradigm is a pivotal approach to parse table structure, where the table separation line detection is crucial. However, challenges such as wireless and deformed tables make it demanding. In this paper, we adhere to the \"split-and-merge\" paradigm and propose SEMv3 (SEM: Split, Embed and Merge), a method that is both fast and robust for detecting table separation lines. During the split stage, we introduce a Keypoint Offset Regression (KOR) module, which effectively detects table separation lines by directly regressing the offset of each line relative to its keypoint proposals. Moreover, in the merge stage, we define a series of merge actions to efficiently describe the table structure based on table grids. Extensive ablation studies demonstrate that our proposed KOR module can detect table separation lines quickly and accurately. Furthermore, on public datasets (e.g. WTW, ICDAR-2019 cTDaR Historical and iFLYTAB), SEMv3 achieves state-of-the-art (SOTA) performance. The code is available at https://github.com/Chunchunwumu/SEMv3.","sentences":["Table structure recognition (TSR) aims to parse the inherent structure of a table from its input image.","The `\"split-and-merge\" paradigm is a pivotal approach to parse table structure, where the table separation line detection is crucial.","However, challenges such as wireless and deformed tables make it demanding.","In this paper, we adhere to the \"split-and-merge\" paradigm and propose SEMv3 (SEM: Split, Embed and Merge), a method that is both fast and robust for detecting table separation lines.","During the split stage, we introduce a Keypoint Offset Regression (KOR) module, which effectively detects table separation lines by directly regressing the offset of each line relative to its keypoint proposals.","Moreover, in the merge stage, we define a series of merge actions to efficiently describe the table structure based on table grids.","Extensive ablation studies demonstrate that our proposed KOR module can detect table separation lines quickly and accurately.","Furthermore, on public datasets (e.g. WTW, ICDAR-2019 cTDaR Historical and iFLYTAB), SEMv3 achieves state-of-the-art (SOTA) performance.","The code is available at https://github.com/Chunchunwumu/SEMv3."],"url":"http://arxiv.org/abs/2405.11862v1","category":"cs.CV"}
{"created":"2024-05-20 08:02:11","title":"Magnetar QPOs and neutron star crust elasticity","abstract":"The crust region is a tiny fraction of neutron stars, but it has a variety of physical properties and plays an important role in astronomical observations. One of the properties characterizing the crust is the elasticity. In this review, with the approach of asteroseismology, we systematically examine neutron star oscillations excited by crust elasticity, adopting the Cowling approximation. In particular, by identifying the quasi-periodic oscillations observed in magnetar flares with the torsional oscillations, we make a constraint on the nuclear saturation parameters. In addition, we also discuss how the shear and interface modes depend on the neutron star properties. Once one detects an additional signal associated with neutron star oscillations, one can get a more severe constraint on the saturation parameters and/or neutron star properties, which must be a qualitatively different constraint obtained from terrestrial experiments, and help us to complementarily understand astrophysics and nuclear physics.","sentences":["The crust region is a tiny fraction of neutron stars, but it has a variety of physical properties and plays an important role in astronomical observations.","One of the properties characterizing the crust is the elasticity.","In this review, with the approach of asteroseismology, we systematically examine neutron star oscillations excited by crust elasticity, adopting the Cowling approximation.","In particular, by identifying the quasi-periodic oscillations observed in magnetar flares with the torsional oscillations, we make a constraint on the nuclear saturation parameters.","In addition, we also discuss how the shear and interface modes depend on the neutron star properties.","Once one detects an additional signal associated with neutron star oscillations, one can get a more severe constraint on the saturation parameters and/or neutron star properties, which must be a qualitatively different constraint obtained from terrestrial experiments, and help us to complementarily understand astrophysics and nuclear physics."],"url":"http://arxiv.org/abs/2405.11858v1","category":"astro-ph.HE"}
{"created":"2024-05-20 07:41:04","title":"EPPS: Advanced Polyp Segmentation via Edge Information Injection and Selective Feature Decoupling","abstract":"Accurate segmentation of polyps in colonoscopy images is essential for early-stage diagnosis and management of colorectal cancer. Despite advancements in deep learning for polyp segmentation, enduring limitations persist. The edges of polyps are typically ambiguous, making them difficult to discern from the background, and the model performance is often compromised by the influence of irrelevant or unimportant features. To alleviate these challenges, we propose a novel model named Edge-Prioritized Polyp Segmentation (EPPS). Specifically, we incorporate an Edge Mapping Engine (EME) aimed at accurately extracting the edges of polyps. Subsequently, an Edge Information Injector (EII) is devised to augment the mask prediction by injecting the captured edge information into Decoder blocks. Furthermore, we introduce a component called Selective Feature Decoupler (SFD) to suppress the influence of noise and extraneous features on the model. Extensive experiments on 3 widely used polyp segmentation benchmarks demonstrate the superior performance of our method compared with other state-of-the-art approaches.","sentences":["Accurate segmentation of polyps in colonoscopy images is essential for early-stage diagnosis and management of colorectal cancer.","Despite advancements in deep learning for polyp segmentation, enduring limitations persist.","The edges of polyps are typically ambiguous, making them difficult to discern from the background, and the model performance is often compromised by the influence of irrelevant or unimportant features.","To alleviate these challenges, we propose a novel model named Edge-Prioritized Polyp Segmentation (EPPS).","Specifically, we incorporate an Edge Mapping Engine (EME) aimed at accurately extracting the edges of polyps.","Subsequently, an Edge Information Injector (EII) is devised to augment the mask prediction by injecting the captured edge information into Decoder blocks.","Furthermore, we introduce a component called Selective Feature Decoupler (SFD) to suppress the influence of noise and extraneous features on the model.","Extensive experiments on 3 widely used polyp segmentation benchmarks demonstrate the superior performance of our method compared with other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2405.11846v1","category":"cs.CV"}
{"created":"2024-05-20 07:38:51","title":"Speed of Random Walks in Dirichlet Environment on a Galton-Watson Tree","abstract":"This paper deals with a transient random walk in Dirichlet environment, or equivalently a linearly edge reinforced random walk, on a Galton-Watson tree. We compute the stationary distribution of the environment seen from the particle of an edge reinforced random walk. We obtain a formula for the speed and give a necessary and sufficient condition for the walk to have a positive speed under some moment conditions on the offspring distribution of the tree.","sentences":["This paper deals with a transient random walk in Dirichlet environment, or equivalently a linearly edge reinforced random walk, on a Galton-Watson tree.","We compute the stationary distribution of the environment seen from the particle of an edge reinforced random walk.","We obtain a formula for the speed and give a necessary and sufficient condition for the walk to have a positive speed under some moment conditions on the offspring distribution of the tree."],"url":"http://arxiv.org/abs/2405.11845v1","category":"math.PR"}
{"created":"2024-05-20 07:27:33","title":"Sweedler duality for Hom-algebras and Hom-modules","abstract":"The construction of Sweedler duality is an important tool in the theory of Hopf algebras over a field, which is a right adjoint to the dual algebra functor. In this paper, we study the Sweedler duality of Hom-algebras and their Hom-modules. We delve into the structure of Hom-coalgebras and derive the linear morphisms associated with them. Additionally, as an application, we present the (right) Hom-(co)module morphisms under the Sweedler duality.","sentences":["The construction of Sweedler duality is an important tool in the theory of Hopf algebras over a field, which is a right adjoint to the dual algebra functor.","In this paper, we study the Sweedler duality of Hom-algebras and their Hom-modules.","We delve into the structure of Hom-coalgebras and derive the linear morphisms associated with them.","Additionally, as an application, we present the (right) Hom-(co)module morphisms under the Sweedler duality."],"url":"http://arxiv.org/abs/2405.11838v1","category":"math.RA"}
{"created":"2024-05-20 06:58:39","title":"Fe2+ partitioning in Al-free pyrolite: consequences for seismic velocities and heterogeneities","abstract":"Iron partitioning among the main lower mantle phases, bridgmanite (Bm) and ferropericlase (Fp), has non-monotonic behavior owing to the high-spin to low-spin crossover in ferrous iron (Fe2+) in Fp. Results of previous studies of the iron partitioning coefficient between these phases, $K_D$, still have considerable uncertainty. Here, we investigate the Fe2+ partitioning behavior using well-documented ab initio free energy results plus new updates. Although we focus on Fe2+ only, we describe the effect of this iron spin crossover (ISC) on $K_D$ and of the latter on compositions and seismic velocities in a pyrolitic aggregate. Our results suggest that its velocities are mainly affected by the ISC and less so by the Fe2+ partitioning. In contrast, iron partitioning manifests in thermally induced velocity heterogeneity ratios. Prediction of the seismological parameter $R_{S/P}$ ($\\partial \\ln V_S/\\partial \\ln V_P$) including iron partitioning effects resembles quantitatively $R_{S/P}$'s inferred from several tomographic studies down to 2,400 km depth.","sentences":["Iron partitioning among the main lower mantle phases, bridgmanite (Bm) and ferropericlase (Fp), has non-monotonic behavior owing to the high-spin to low-spin crossover in ferrous iron (Fe2+) in Fp. Results of previous studies of the iron partitioning coefficient between these phases, $K_D$, still have considerable uncertainty.","Here, we investigate the Fe2+ partitioning behavior using well-documented ab initio free energy results plus new updates.","Although we focus on Fe2+ only, we describe the effect of this iron spin crossover (ISC) on $K_D$ and of the latter on compositions and seismic velocities in a pyrolitic aggregate.","Our results suggest that its velocities are mainly affected by the ISC and less so by the Fe2+ partitioning.","In contrast, iron partitioning manifests in thermally induced velocity heterogeneity ratios.","Prediction of the seismological parameter $R_{S/P}$ ($\\partial \\ln V_S/\\partial \\ln V_P$) including iron partitioning effects resembles quantitatively $R_{S/P}$'s inferred from several tomographic studies down to 2,400 km depth."],"url":"http://arxiv.org/abs/2405.11830v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 06:32:57","title":"A Three-Phase Analysis of Synergistic Effects During Co-pyrolysis of Algae and Wood for Biochar Yield Using Machine Learning","abstract":"Pyrolysis techniques have served to be a groundbreaking technique for effectively utilising natural and man-made biomass products like plastics, wood, crop residue, fruit peels etc. Recent advancements have shown a greater yield of essential products like biochar, bio-oil and other non-condensable gases by blending different biomasses in a certain ratio. This synergy effect of combining two pyrolytic raw materials i.e co-pyrolysis of algae and wood biomass has been systematically studied and grouped into 3 phases in this research paper-kinetic analysis of co-pyrolysis, correlation among proximate and ultimate analysis with bio-char yield and lastly grouping of different weight ratios based on biochar yield up to a certain percentage. Different ML and DL algorithms have been utilized for regression and classification techniques to give a comprehensive overview of the effect of the synergy of two different biomass materials on biochar yield. For the first phase, the best prediction of biochar yield was obtained by using a decision tree regressor with a perfect MSE score of 0.00, followed by a gradient-boosting regressor. The second phase was analyzed using both ML and DL techniques. Within ML, SVR proved to be the most convenient model with an accuracy score of 0.972 with DNN employed for deep learning technique. Finally, for the third phase, binary classification was applied to biochar yield with and without heating rate for biochar yield percentage above and below 40%. The best technique for ML was Support Vector followed by Random forest while ANN was the most suitable Deep Learning Technique.","sentences":["Pyrolysis techniques have served to be a groundbreaking technique for effectively utilising natural and man-made biomass products like plastics, wood, crop residue, fruit peels etc.","Recent advancements have shown a greater yield of essential products like biochar, bio-oil and other non-condensable gases by blending different biomasses in a certain ratio.","This synergy effect of combining two pyrolytic raw materials i.e co-pyrolysis of algae and wood biomass has been systematically studied and grouped into 3 phases in this research paper-kinetic analysis of co-pyrolysis, correlation among proximate and ultimate analysis with bio-char yield and lastly grouping of different weight ratios based on biochar yield up to a certain percentage.","Different ML and DL algorithms have been utilized for regression and classification techniques to give a comprehensive overview of the effect of the synergy of two different biomass materials on biochar yield.","For the first phase, the best prediction of biochar yield was obtained by using a decision tree regressor with a perfect MSE score of 0.00, followed by a gradient-boosting regressor.","The second phase was analyzed using both ML and DL techniques.","Within ML, SVR proved to be the most convenient model with an accuracy score of 0.972 with DNN employed for deep learning technique.","Finally, for the third phase, binary classification was applied to biochar yield with and without heating rate for biochar yield percentage above and below 40%.","The best technique for ML was Support Vector followed by Random forest while ANN was the most suitable Deep Learning Technique."],"url":"http://arxiv.org/abs/2405.11821v1","category":"cs.LG"}
{"created":"2024-05-20 06:06:25","title":"Formation of Iron-Helium Compounds under High Pressure","abstract":"While helium is a representative noble gas element characterized by its chemical inertness at ambient conditions, recent experiments and calculations argued that a minor amount of helium is incorporated into molten iron from silicate under high pressures. Here we examined the reaction between iron and helium at 8-42 GPa and ~1000-2820 K and found remarkable volume expansion of the Fe lattice, which is attributed to the formations of fcc and distorted hcp iron-helium compounds with x in FeHex up to 0.13 and 0.42, respectively. Upon releasing pressure under room temperature, these fcc and distorted hcp FeHex were still observed while the former lost some helium. In addition, our first-principles calculations indicate that fcc FeHe0.25, with He atoms in the tetrahedral interstitial sites, is dynamically stable in different magnetic states throughout 0-50 GPa. These results support that the Earth's core can be a large reservoir of primordial 3He.","sentences":["While helium is a representative noble gas element characterized by its chemical inertness at ambient conditions, recent experiments and calculations argued that a minor amount of helium is incorporated into molten iron from silicate under high pressures.","Here we examined the reaction between iron and helium at 8-42 GPa and ~1000-2820 K and found remarkable volume expansion of the Fe lattice, which is attributed to the formations of fcc and distorted hcp iron-helium compounds with x in FeHex up to 0.13 and 0.42, respectively.","Upon releasing pressure under room temperature, these fcc and distorted hcp FeHex were still observed while the former lost some helium.","In addition, our first-principles calculations indicate that fcc FeHe0.25, with He atoms in the tetrahedral interstitial sites, is dynamically stable in different magnetic states throughout 0-50 GPa.","These results support that the Earth's core can be a large reservoir of primordial 3He."],"url":"http://arxiv.org/abs/2405.11810v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 05:50:25","title":"Learning of Balance Controller Considering Changes in Body State for Musculoskeletal Humanoids","abstract":"The musculoskeletal humanoid is difficult to modelize due to the flexibility and redundancy of its body, whose state can change over time, and so balance control of its legs is challenging. There are some cases where ordinary PID controls may cause instability. In this study, to solve these problems, we propose a method of learning a correlation model among the joint angle, muscle tension, and muscle length of the ankle and the zero moment point to perform balance control. In addition, information on the changing body state is embedded in the model using parametric bias, and the model estimates and adapts to the current body state by learning this information online. This makes it possible to adapt to changes in upper body posture that are not directly taken into account in the model, since it is difficult to learn the complete dynamics of the whole body considering the amount of data and computation. The model can also adapt to changes in body state, such as the change in footwear and change in the joint origin due to recalibration. The effectiveness of this method is verified by a simulation and by using an actual musculoskeletal humanoid, Musashi.","sentences":["The musculoskeletal humanoid is difficult to modelize due to the flexibility and redundancy of its body, whose state can change over time, and so balance control of its legs is challenging.","There are some cases where ordinary PID controls may cause instability.","In this study, to solve these problems, we propose a method of learning a correlation model among the joint angle, muscle tension, and muscle length of the ankle and the zero moment point to perform balance control.","In addition, information on the changing body state is embedded in the model using parametric bias, and the model estimates and adapts to the current body state by learning this information online.","This makes it possible to adapt to changes in upper body posture that are not directly taken into account in the model, since it is difficult to learn the complete dynamics of the whole body considering the amount of data and computation.","The model can also adapt to changes in body state, such as the change in footwear and change in the joint origin due to recalibration.","The effectiveness of this method is verified by a simulation and by using an actual musculoskeletal humanoid, Musashi."],"url":"http://arxiv.org/abs/2405.11803v1","category":"cs.RO"}
{"created":"2024-05-20 05:46:12","title":"Regularized Entanglement Entropy of Electron-Positron Scattering with a Witness Photon","abstract":"Regularized quantum information metrics are calculated for the scattering process $e^-e^+ \\rightarrow \\gamma,Z\\rightarrow \\mu^-\\mu^+$ that has a witness photon entangled with the initial electron-positron state. Unitarity implies the correct regularization of divergences that appear in both the final density matrix and von Neumann entanglement entropies. The entropies are found to quantify uncertainty or randomness. The variation of information, entanglement entropy, and correlation between the muon's and witness photon's helicities are found to convey equivalent information. The magnitude of the muon's expected helicity rises (falls) as the helicity entropy falls (rises). Area, or the scattering cross section, is a source of entropy for the muon's helicity entropy and momentum entropy. The muon's differential angular entropy distribution is similar to the differential angular cross section distribution, capturing the forward-backward asymmetry at high center of mass energies.","sentences":["Regularized quantum information metrics are calculated for the scattering process $e^-e^+ \\rightarrow \\gamma,Z\\rightarrow \\mu^-\\mu^+$ that has a witness photon entangled with the initial electron-positron state.","Unitarity implies the correct regularization of divergences that appear in both the final density matrix and von Neumann entanglement entropies.","The entropies are found to quantify uncertainty or randomness.","The variation of information, entanglement entropy, and correlation between the muon's and witness photon's helicities are found to convey equivalent information.","The magnitude of the muon's expected helicity rises (falls) as the helicity entropy falls (rises).","Area, or the scattering cross section, is a source of entropy for the muon's helicity entropy and momentum entropy.","The muon's differential angular entropy distribution is similar to the differential angular cross section distribution, capturing the forward-backward asymmetry at high center of mass energies."],"url":"http://arxiv.org/abs/2405.11799v1","category":"hep-th"}
{"created":"2024-05-20 04:55:32","title":"Formulation and evaluation of ocean dynamics problems as optimization problems for quantum annealing machines","abstract":"Recent advancements in quantum computing suggest the potential to revolutionize computational algorithms across various scientific domains including oceanography and atmospheric science. The field is still relatively young and quantum computation is so different from classical computation that suitable frameworks to represent oceanic and atmospheric dynamics are yet to be explored. Quantum annealing, one of the major paradigms, focuses on combinatorial optimization tasks. In this paper, we solve the classical Stommel problem by quantum annealing (QA) and simulated annealing (SA), a classical counterpart of quantum annealing. We cast the linear partial differential equation into an optimization problem by the least-squares method and discretize the cost function in two ways: finite difference and truncated basis expansion. In either case, SA successfully reproduces the expected solution when appropriate parameters are chosen, demonstrating that annealing has the potential. In contrast, QA using the D-Wave quantum annealing machine fails to obtain good solutions for some cases owing to hardware limitations; in particular, the highly limited connectivity graph of the machine limits the size of the solvable problems, at least under currently available algorithms. Either expanding the connectivity graph or improving the graph embedding algorithms would probably be necessary for quantum annealing machines to be usable for oceanic and atmospheric dynamics problems. While this finding emphasizes the need for hardware improvements and enhancements in graph embedding algorithms for practical applications of quantum annealers, the results from simulated annealing suggest its potential to address practical geophysical dynamics problems. As quantum calculation continues to evolve, addressing these challenges may lead to transformative advancements in ocean and atmosphere modeling.","sentences":["Recent advancements in quantum computing suggest the potential to revolutionize computational algorithms across various scientific domains including oceanography and atmospheric science.","The field is still relatively young and quantum computation is so different from classical computation that suitable frameworks to represent oceanic and atmospheric dynamics are yet to be explored.","Quantum annealing, one of the major paradigms, focuses on combinatorial optimization tasks.","In this paper, we solve the classical Stommel problem by quantum annealing (QA) and simulated annealing (SA), a classical counterpart of quantum annealing.","We cast the linear partial differential equation into an optimization problem by the least-squares method and discretize the cost function in two ways: finite difference and truncated basis expansion.","In either case, SA successfully reproduces the expected solution when appropriate parameters are chosen, demonstrating that annealing has the potential.","In contrast, QA using the D-Wave quantum annealing machine fails to obtain good solutions for some cases owing to hardware limitations; in particular, the highly limited connectivity graph of the machine limits the size of the solvable problems, at least under currently available algorithms.","Either expanding the connectivity graph or improving the graph embedding algorithms would probably be necessary for quantum annealing machines to be usable for oceanic and atmospheric dynamics problems.","While this finding emphasizes the need for hardware improvements and enhancements in graph embedding algorithms for practical applications of quantum annealers, the results from simulated annealing suggest its potential to address practical geophysical dynamics problems.","As quantum calculation continues to evolve, addressing these challenges may lead to transformative advancements in ocean and atmosphere modeling."],"url":"http://arxiv.org/abs/2405.11782v1","category":"quant-ph"}
{"created":"2024-05-20 04:54:20","title":"Structural Nested Mean Models Under Parallel Trends with Interference","abstract":"Despite the common occurrence of interference in Difference-in-Differences (DiD) applications, standard DiD methods rely on an assumption that interference is absent, and comparatively little work has considered how to accommodate and learn about spillover effects within a DiD framework. Here, we extend the so-called `DiD-SNMMs' of Shahn et al (2022) to accommodate interference in a time-varying DiD setting. Doing so enables estimation of a richer set of effects than previous DiD approaches. For example, DiD-SNMMs do not assume the absence of spillover effects after direct exposures and can model how effects of direct or indirect (i.e. spillover) exposures depend on past and concurrent (direct or indirect) exposure and covariate history. We consider both cluster and network interference structures an illustrate the methodology in simulations.","sentences":["Despite the common occurrence of interference in Difference-in-Differences (DiD) applications, standard DiD methods rely on an assumption that interference is absent, and comparatively little work has considered how to accommodate and learn about spillover effects within a DiD framework.","Here, we extend the so-called `DiD-SNMMs' of Shahn et al (2022) to accommodate interference in a time-varying DiD setting.","Doing so enables estimation of a richer set of effects than previous DiD approaches.","For example, DiD-SNMMs do not assume the absence of spillover effects after direct exposures and can model how effects of direct or indirect (i.e. spillover) exposures depend on past and concurrent (direct or indirect) exposure and covariate history.","We consider both cluster and network interference structures an illustrate the methodology in simulations."],"url":"http://arxiv.org/abs/2405.11781v1","category":"stat.ME"}
{"created":"2024-05-20 03:14:32","title":"Expectation values in the random walk theory and the diffusion equation","abstract":"The relation between the expectation values computed in the random walk theory, and the heat kernel method for the diffusion equation is explained concretely. The random walk is also realized by simulations and their statistical uncertainties are analyzed.","sentences":["The relation between the expectation values computed in the random walk theory, and the heat kernel method for the diffusion equation is explained concretely.","The random walk is also realized by simulations and their statistical uncertainties are analyzed."],"url":"http://arxiv.org/abs/2405.11748v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-20 03:08:51","title":"Interior Harnack inequality and H\u00f6lder estimates for linearized Monge-Amp\u00e8re equations in divergence form with drift","abstract":"In this paper, we study interior estimates for solutions to linearized Monge-Amp\\`ere equations in divergence form with drift terms and the right-hand side containing the divergence of a bounded vector field. Equations of this type appear in the study of semigeostrophic equations in meteorology and the solvability of singular Abreu equations in the calculus of variations with a convexity constraint. We prove an interior Harnack inequality and H\\\"older estimates for solutions to equations of this type in two dimensions, and under an integrability assumption on the Hessian matrix of the Monge-Amp\\`ere potential in higher dimensions. Our results extend those of Le (Analysis of Monge-Amp\\`ere equations, Graduate Studies in Mathematics, vol.240, American Mathematical Society, 2024) to equations with drift terms.","sentences":["In this paper, we study interior estimates for solutions to linearized Monge-Amp\\`ere equations in divergence form with drift terms and the right-hand side containing the divergence of a bounded vector field.","Equations of this type appear in the study of semigeostrophic equations in meteorology and the solvability of singular Abreu equations in the calculus of variations with a convexity constraint.","We prove an interior Harnack inequality and H\\\"older estimates for solutions to equations of this type in two dimensions, and under an integrability assumption on the Hessian matrix of the Monge-Amp\\`ere potential in higher dimensions.","Our results extend those of Le (Analysis of Monge-Amp\\`ere equations, Graduate Studies in Mathematics, vol.240, American Mathematical Society, 2024) to equations with drift terms."],"url":"http://arxiv.org/abs/2405.11745v1","category":"math.AP"}
{"created":"2024-05-20 02:33:44","title":"Accurate and efficient protein embedding using multi-teacher distillation learning","abstract":"Motivation: Protein embedding, which represents proteins as numerical vectors, is a crucial step in various learning-based protein annotation/classification problems, including gene ontology prediction, protein-protein interaction prediction, and protein structure prediction. However, existing protein embedding methods are often computationally expensive due to their large number of parameters, which can reach millions or even billions. The growing availability of large-scale protein datasets and the need for efficient analysis tools have created a pressing demand for efficient protein embedding methods.   Results: We propose a novel protein embedding approach based on multi-teacher distillation learning, which leverages the knowledge of multiple pre-trained protein embedding models to learn a compact and informative representation of proteins. Our method achieves comparable performance to state-of-the-art methods while significantly reducing computational costs and resource requirements. Specifically, our approach reduces computational time by ~70\\% and maintains almost the same accuracy as the original large models. This makes our method well-suited for large-scale protein analysis and enables the bioinformatics community to perform protein embedding tasks more efficiently.","sentences":["Motivation: Protein embedding, which represents proteins as numerical vectors, is a crucial step in various learning-based protein annotation/classification problems, including gene ontology prediction, protein-protein interaction prediction, and protein structure prediction.","However, existing protein embedding methods are often computationally expensive due to their large number of parameters, which can reach millions or even billions.","The growing availability of large-scale protein datasets and the need for efficient analysis tools have created a pressing demand for efficient protein embedding methods.   ","Results:","We propose a novel protein embedding approach based on multi-teacher distillation learning, which leverages the knowledge of multiple pre-trained protein embedding models to learn a compact and informative representation of proteins.","Our method achieves comparable performance to state-of-the-art methods while significantly reducing computational costs and resource requirements.","Specifically, our approach reduces computational time by ~70\\% and maintains almost the same accuracy as the original large models.","This makes our method well-suited for large-scale protein analysis and enables the bioinformatics community to perform protein embedding tasks more efficiently."],"url":"http://arxiv.org/abs/2405.11735v1","category":"q-bio.GN"}
{"created":"2024-05-20 02:21:01","title":"Optimization of Worker Scheduling at Logistics Depots Using Genetic Algorithms and Simulated Annealing","abstract":"This paper addresses the optimization of scheduling for workers at a logistics depot using a combination of genetic algorithm and simulated annealing algorithm. The efficient scheduling of permanent and temporary workers is crucial for optimizing the efficiency of the logistics depot while minimizing labor usage. The study begins by establishing a 0-1 integer linear programming model, with decision variables determining the scheduling of permanent and temporary workers for each time slot on a given day. The objective function aims to minimize person-days, while constraints ensure fulfillment of hourly labor requirements, limit workers to one time slot per day, cap consecutive working days for permanent workers, and maintain non-negativity and integer constraints. The model is then solved using genetic algorithms and simulated annealing. Results indicate that, for this problem, genetic algorithms outperform simulated annealing in terms of solution quality. The optimal solution reveals a minimum of 29857 person-days.","sentences":["This paper addresses the optimization of scheduling for workers at a logistics depot using a combination of genetic algorithm and simulated annealing algorithm.","The efficient scheduling of permanent and temporary workers is crucial for optimizing the efficiency of the logistics depot while minimizing labor usage.","The study begins by establishing a 0-1 integer linear programming model, with decision variables determining the scheduling of permanent and temporary workers for each time slot on a given day.","The objective function aims to minimize person-days, while constraints ensure fulfillment of hourly labor requirements, limit workers to one time slot per day, cap consecutive working days for permanent workers, and maintain non-negativity and integer constraints.","The model is then solved using genetic algorithms and simulated annealing.","Results indicate that, for this problem, genetic algorithms outperform simulated annealing in terms of solution quality.","The optimal solution reveals a minimum of 29857 person-days."],"url":"http://arxiv.org/abs/2405.11729v1","category":"cs.NE"}
{"created":"2024-05-20 02:19:06","title":"Tighter Bounds on the Expected Absorbing Time of Ungarian Markov Chains","abstract":"In 2023, Defant and Li defined the Ungarian Markov chain associated to a lattice $L$. This Markov chain has state space $L$, and from any state $x \\in L$ transitions to the meet of $\\{x\\} \\cup T$, where $T$ is a randomly selected subset of the elements of $L$ covered by $x$. They defined $\\mathcal{E}(L)$ to be the expected number of steps until the maximal element $\\hat{1}$ of $L$ transitions into the minimal element $\\hat{0}$ in the Ungarian Markov chain, and investigated its asymptotics when $L$ is the weak order on $S_n$, or the $n$th Tamari lattice $\\textrm{Tam}_n$. This paper resolves a conjecture of Defant and Li and makes partial progress towards another, by proving that $\\mathcal{E}(S_n)$ is linear in $n$, and proving an $n^{1 - o(1)}$ lower bound on $\\mathcal{E}(\\textrm{Tam}_n)$.","sentences":["In 2023, Defant and Li defined the Ungarian Markov chain associated to a lattice $L$. This Markov chain has state space $L$, and from any state $x \\in L$ transitions to the meet of $\\{x\\} \\cup T$, where $T$ is a randomly selected subset of the elements of $L$ covered by $x$.","They defined $\\mathcal{E}(L)$ to be the expected number of steps until the maximal element $\\hat{1}$ of $L$ transitions into the minimal element $\\hat{0}$ in the Ungarian Markov chain, and investigated its asymptotics when $L$ is the weak order on $S_n$, or the $n$th Tamari lattice $\\textrm{Tam}_n$. This paper resolves a conjecture of Defant and Li and makes partial progress towards another, by proving that $\\mathcal{E}(S_n)$ is linear in $n$, and proving an $n^{1 - o(1)}$ lower bound on $\\mathcal{E}(\\textrm{Tam}_n)$."],"url":"http://arxiv.org/abs/2405.11728v1","category":"math.CO"}
{"created":"2024-05-20 00:01:36","title":"QComp: A QSAR-Based Data Completion Framework for Drug Discovery","abstract":"In drug discovery, in vitro and in vivo experiments reveal biochemical activities related to the efficacy and toxicity of compounds. The experimental data accumulate into massive, ever-evolving, and sparse datasets. Quantitative Structure-Activity Relationship (QSAR) models, which predict biochemical activities using only the structural information of compounds, face challenges in integrating the evolving experimental data as studies progress. We develop QSAR-Complete (QComp), a data completion framework to address this issue. Based on pre-existing QSAR models, QComp utilizes the correlation inherent in experimental data to enhance prediction accuracy across various tasks. Moreover, QComp emerges as a promising tool for guiding the optimal sequence of experiments by quantifying the reduction in statistical uncertainty for specific endpoints, thereby aiding in rational decision-making throughout the drug discovery process.","sentences":["In drug discovery, in vitro and in vivo experiments reveal biochemical activities related to the efficacy and toxicity of compounds.","The experimental data accumulate into massive, ever-evolving, and sparse datasets.","Quantitative Structure-Activity Relationship (QSAR) models, which predict biochemical activities using only the structural information of compounds, face challenges in integrating the evolving experimental data as studies progress.","We develop QSAR-Complete (QComp), a data completion framework to address this issue.","Based on pre-existing QSAR models, QComp utilizes the correlation inherent in experimental data to enhance prediction accuracy across various tasks.","Moreover, QComp emerges as a promising tool for guiding the optimal sequence of experiments by quantifying the reduction in statistical uncertainty for specific endpoints, thereby aiding in rational decision-making throughout the drug discovery process."],"url":"http://arxiv.org/abs/2405.11703v1","category":"cs.LG"}
{"created":"2024-05-19 23:48:10","title":"On a weighted Hermite-Hadamard inequality involving convex functional arguments","abstract":"In this paper, we are interested in investigating a weighted variant of Hermite-Hadamard type inequalities involving convex functionals. The approach undertaken makes it possible to refine and reverse certain inequalities already known in the literature. It also allows us to provide new weighted convex functional means and establish some related properties with respect to some standard means.","sentences":["In this paper, we are interested in investigating a weighted variant of Hermite-Hadamard type inequalities involving convex functionals.","The approach undertaken makes it possible to refine and reverse certain inequalities already known in the literature.","It also allows us to provide new weighted convex functional means and establish some related properties with respect to some standard means."],"url":"http://arxiv.org/abs/2405.11702v1","category":"math.FA"}
{"created":"2024-05-19 23:04:09","title":"Approximation and Gradient Descent Training with Neural Networks","abstract":"It is well understood that neural networks with carefully hand-picked weights provide powerful function approximation and that they can be successfully trained in over-parametrized regimes. Since over-parametrization ensures zero training error, these two theories are not immediately compatible. Recent work uses the smoothness that is required for approximation results to extend a neural tangent kernel (NTK) optimization argument to an under-parametrized regime and show direct approximation bounds for networks trained by gradient flow. Since gradient flow is only an idealization of a practical method, this paper establishes analogous results for networks trained by gradient descent.","sentences":["It is well understood that neural networks with carefully hand-picked weights provide powerful function approximation and that they can be successfully trained in over-parametrized regimes.","Since over-parametrization ensures zero training error, these two theories are not immediately compatible.","Recent work uses the smoothness that is required for approximation results to extend a neural tangent kernel (NTK) optimization argument to an under-parametrized regime and show direct approximation bounds for networks trained by gradient flow.","Since gradient flow is only an idealization of a practical method, this paper establishes analogous results for networks trained by gradient descent."],"url":"http://arxiv.org/abs/2405.11696v1","category":"cs.LG"}
{"created":"2024-05-19 21:49:49","title":"Contrasting chaotic and stochastic forcing: tipping windows and attractor crises","abstract":"Nonlinear dynamical systems subjected to a combination of noise and time-varying forcing can exhibit sudden changes, critical transitions or tipping points where large or rapid dynamic effects arise from changes in a parameter that are small or slow. Noise-induced tipping can occur where extremes of the forcing causes the system to leave one attractor and transition to another. If this noise corresponds to unresolved chaotic forcing, there is a limit such that this can be approximated by a stochastic differential equation (SDE) and the statistics of large deviations determine the transitions. Away from this limit it makes sense to consider tipping in the presence of chaotic rather than stochastic forcing. In general we argue that close to a parameter value where there is a bifurcation of the unforced system, there will be a chaotic tipping window outside of which tipping cannot happen, in the limit of asymptotically slow change of that parameter. This window is trivial for a stochastically forced system. Entry into the chaotic tipping window can be seen as a boundary crisis/non-autonomous saddle-node bifurcation and corresponds to an exceptional case of the forcing, typically by an unstable periodic orbit. We discuss an illustrative example of a chaotically forced bistable map that highlight the richness of the geometry and bifurcation structure of the dynamics in this case. If a parameter is changing slowly we note there is a dynamic tipping window that can also be determined in terms of unstable periodic orbits.","sentences":["Nonlinear dynamical systems subjected to a combination of noise and time-varying forcing can exhibit sudden changes, critical transitions or tipping points where large or rapid dynamic effects arise from changes in a parameter that are small or slow.","Noise-induced tipping can occur where extremes of the forcing causes the system to leave one attractor and transition to another.","If this noise corresponds to unresolved chaotic forcing, there is a limit such that this can be approximated by a stochastic differential equation (SDE) and the statistics of large deviations determine the transitions.","Away from this limit it makes sense to consider tipping in the presence of chaotic rather than stochastic forcing.","In general we argue that close to a parameter value where there is a bifurcation of the unforced system, there will be a chaotic tipping window outside of which tipping cannot happen, in the limit of asymptotically slow change of that parameter.","This window is trivial for a stochastically forced system.","Entry into the chaotic tipping window can be seen as a boundary crisis/non-autonomous saddle-node bifurcation and corresponds to an exceptional case of the forcing, typically by an unstable periodic orbit.","We discuss an illustrative example of a chaotically forced bistable map that highlight the richness of the geometry and bifurcation structure of the dynamics in this case.","If a parameter is changing slowly we note there is a dynamic tipping window that can also be determined in terms of unstable periodic orbits."],"url":"http://arxiv.org/abs/2405.11680v1","category":"nlin.CD"}
{"created":"2024-05-19 21:35:12","title":"Advancing 6-DoF Instrument Pose Estimation in Variable X-Ray Imaging Geometries","abstract":"Accurate 6-DoF pose estimation of surgical instruments during minimally invasive surgeries can substantially improve treatment strategies and eventual surgical outcome. Existing deep learning methods have achieved accurate results, but they require custom approaches for each object and laborious setup and training environments often stretching to extensive simulations, whilst lacking real-time computation. We propose a general-purpose approach of data acquisition for 6-DoF pose estimation tasks in X-ray systems, a novel and general purpose YOLOv5-6D pose architecture for accurate and fast object pose estimation and a complete method for surgical screw pose estimation under acquisition geometry consideration from a monocular cone-beam X-ray image. The proposed YOLOv5-6D pose model achieves competitive results on public benchmarks whilst being considerably faster at 42 FPS on GPU. In addition, the method generalizes across varying X-ray acquisition geometry and semantic image complexity to enable accurate pose estimation over different domains. Finally, the proposed approach is tested for bone-screw pose estimation for computer-aided guidance during spine surgeries. The model achieves a 92.41% by the 0.1 ADD-S metric, demonstrating a promising approach for enhancing surgical precision and patient outcomes. The code for YOLOv5-6D is publicly available at https://github.com/cviviers/YOLOv5-6D-Pose","sentences":["Accurate 6-DoF pose estimation of surgical instruments during minimally invasive surgeries can substantially improve treatment strategies and eventual surgical outcome.","Existing deep learning methods have achieved accurate results, but they require custom approaches for each object and laborious setup and training environments often stretching to extensive simulations, whilst lacking real-time computation.","We propose a general-purpose approach of data acquisition for 6-DoF pose estimation tasks in X-ray systems, a novel and general purpose YOLOv5-6D pose architecture for accurate and fast object pose estimation and a complete method for surgical screw pose estimation under acquisition geometry consideration from a monocular cone-beam X-ray image.","The proposed YOLOv5-6D pose model achieves competitive results on public benchmarks whilst being considerably faster at 42 FPS on GPU.","In addition, the method generalizes across varying X-ray acquisition geometry and semantic image complexity to enable accurate pose estimation over different domains.","Finally, the proposed approach is tested for bone-screw pose estimation for computer-aided guidance during spine surgeries.","The model achieves a 92.41% by the 0.1 ADD-S metric, demonstrating a promising approach for enhancing surgical precision and patient outcomes.","The code for YOLOv5-6D is publicly available at https://github.com/cviviers/YOLOv5-6D-Pose"],"url":"http://arxiv.org/abs/2405.11677v1","category":"cs.CV"}
{"created":"2024-05-19 20:39:46","title":"Interpretable Machine Learning Enhances Disease Prognosis: Applications on COVID-19 and Onward","abstract":"In response to the COVID-19 pandemic, the integration of interpretable machine learning techniques has garnered significant attention, offering transparent and understandable insights crucial for informed clinical decision making. This literature review delves into the applications of interpretable machine learning in predicting the prognosis of respiratory diseases, particularly focusing on COVID-19 and its implications for future research and clinical practice. We reviewed various machine learning models that are not only capable of incorporating existing clinical domain knowledge but also have the learning capability to explore new information from the data. These models and experiences not only aid in managing the current crisis but also hold promise for addressing future disease outbreaks. By harnessing interpretable machine learning, healthcare systems can enhance their preparedness and response capabilities, thereby improving patient outcomes and mitigating the impact of respiratory diseases in the years to come.","sentences":["In response to the COVID-19 pandemic, the integration of interpretable machine learning techniques has garnered significant attention, offering transparent and understandable insights crucial for informed clinical decision making.","This literature review delves into the applications of interpretable machine learning in predicting the prognosis of respiratory diseases, particularly focusing on COVID-19 and its implications for future research and clinical practice.","We reviewed various machine learning models that are not only capable of incorporating existing clinical domain knowledge but also have the learning capability to explore new information from the data.","These models and experiences not only aid in managing the current crisis but also hold promise for addressing future disease outbreaks.","By harnessing interpretable machine learning, healthcare systems can enhance their preparedness and response capabilities, thereby improving patient outcomes and mitigating the impact of respiratory diseases in the years to come."],"url":"http://arxiv.org/abs/2405.11672v1","category":"cs.LG"}
{"created":"2024-05-19 20:33:38","title":"On $z$-elements of multiplicative lattices","abstract":"The aim of this paper is to investigate further properties of $z$-elements in multiplicative lattices. We utilize $z$-closure operators to extend several properties of $z$-ideals to $z$-elements and introduce various distinguished subclasses of $z$-elements, such as $z$-prime, $z$-semiprime, $z$-primary, $z$-irreducible, and $z$-strongly irreducible elements, and study their properties. We provide a characterization of multiplicative lattices where $z$-elements are closed under finite products and a representation of $z$-elements in terms of $z$-irreducible elements in $z$-Noetherian multiplicative lattices.","sentences":["The aim of this paper is to investigate further properties of $z$-elements in multiplicative lattices.","We utilize $z$-closure operators to extend several properties of $z$-ideals to $z$-elements and introduce various distinguished subclasses of $z$-elements, such as $z$-prime, $z$-semiprime, $z$-primary, $z$-irreducible, and $z$-strongly irreducible elements, and study their properties.","We provide a characterization of multiplicative lattices where $z$-elements are closed under finite products and a representation of $z$-elements in terms of $z$-irreducible elements in $z$-Noetherian multiplicative lattices."],"url":"http://arxiv.org/abs/2405.11670v1","category":"math.RA"}
{"created":"2024-05-19 20:20:03","title":"The Limits and Potentials of Local SGD for Distributed Heterogeneous Learning with Intermittent Communication","abstract":"Local SGD is a popular optimization method in distributed learning, often outperforming other algorithms in practice, including mini-batch SGD. Despite this success, theoretically proving the dominance of local SGD in settings with reasonable data heterogeneity has been difficult, creating a significant gap between theory and practice. In this paper, we provide new lower bounds for local SGD under existing first-order data heterogeneity assumptions, showing that these assumptions are insufficient to prove the effectiveness of local update steps. Furthermore, under these same assumptions, we demonstrate the min-max optimality of accelerated mini-batch SGD, which fully resolves our understanding of distributed optimization for several problem classes. Our results emphasize the need for better models of data heterogeneity to understand the effectiveness of local SGD in practice. Towards this end, we consider higher-order smoothness and heterogeneity assumptions, providing new upper bounds that imply the dominance of local SGD over mini-batch SGD when data heterogeneity is low.","sentences":["Local SGD is a popular optimization method in distributed learning, often outperforming other algorithms in practice, including mini-batch SGD.","Despite this success, theoretically proving the dominance of local SGD in settings with reasonable data heterogeneity has been difficult, creating a significant gap between theory and practice.","In this paper, we provide new lower bounds for local SGD under existing first-order data heterogeneity assumptions, showing that these assumptions are insufficient to prove the effectiveness of local update steps.","Furthermore, under these same assumptions, we demonstrate the min-max optimality of accelerated mini-batch SGD, which fully resolves our understanding of distributed optimization for several problem classes.","Our results emphasize the need for better models of data heterogeneity to understand the effectiveness of local SGD in practice.","Towards this end, we consider higher-order smoothness and heterogeneity assumptions, providing new upper bounds that imply the dominance of local SGD over mini-batch SGD when data heterogeneity is low."],"url":"http://arxiv.org/abs/2405.11667v1","category":"cs.LG"}
{"created":"2024-05-19 20:11:30","title":"Auto-Platoon : Freight by example","abstract":"The work introduces a bio-inspired leader-follower system based on an innovative mechanism proposed as software latching that aims to improve collaboration and coordination between a leader agent and the associated autonomous followers. The system utilizes software latching to establish real-time communication and synchronization between the leader and followers. A layered architecture is proposed, encompassing perception, decision-making, and control modules. Challenges such as uncertainty, dynamic environments, and communication latency are addressed using Deep learning and real-time data processing pipelines. The follower robot is equipped with sensors and communication modules that enable it to track and trace the agent of interest or avoid obstacles. The followers track the leader and dynamically avoid obstacles while maintaining a safe distance from it. The experimental results demonstrate the proposed system's effectiveness, making it a promising solution for achieving success in tasks that demand multi-robot systems capable of navigating complex dynamic environments.","sentences":["The work introduces a bio-inspired leader-follower system based on an innovative mechanism proposed as software latching that aims to improve collaboration and coordination between a leader agent and the associated autonomous followers.","The system utilizes software latching to establish real-time communication and synchronization between the leader and followers.","A layered architecture is proposed, encompassing perception, decision-making, and control modules.","Challenges such as uncertainty, dynamic environments, and communication latency are addressed using Deep learning and real-time data processing pipelines.","The follower robot is equipped with sensors and communication modules that enable it to track and trace the agent of interest or avoid obstacles.","The followers track the leader and dynamically avoid obstacles while maintaining a safe distance from it.","The experimental results demonstrate the proposed system's effectiveness, making it a promising solution for achieving success in tasks that demand multi-robot systems capable of navigating complex dynamic environments."],"url":"http://arxiv.org/abs/2405.11659v1","category":"cs.RO"}
{"created":"2024-05-19 19:38:39","title":"Direct imaging of asymmetric interfaces and electrostatic potentials inside a hafnia-zirconia ferroelectric nanocapacitor","abstract":"In hafnia-based thin-film ferroelectric devices, chemical phenomena during growth and processing such as oxygen vacancy formation and interfacial reactions appear to strongly affect device performance. However, the nanoscale structure, chemistry, and electrical potentials in these devices are not fully known, making it difficult to understand their influence on device properties. Here, we directly image the composition and electrostatic potential with nanometer resolution in the cross section of a nanocrystalline W / Hf$_{0.5}$Zr$_{0.5}$O$_{2-\\delta}$ (HZO) / W ferroelectric capacitor using multimodal electron microscopy. This reveals a 1.4 nm wide tungsten sub-oxide interfacial layer formed at the bottom interface during fabrication which introduces a potential dip and leads to asymmetric switching fields. Additionally, the measured inner potential in HZO is consistent with the presence of about 20% oxygen vacancies and a negative built-in potential in HZO. These chemical and electrostatic details are important to characterize and tune to achieve high performance ferroelectric devices.","sentences":["In hafnia-based thin-film ferroelectric devices, chemical phenomena during growth and processing such as oxygen vacancy formation and interfacial reactions appear to strongly affect device performance.","However, the nanoscale structure, chemistry, and electrical potentials in these devices are not fully known, making it difficult to understand their influence on device properties.","Here, we directly image the composition and electrostatic potential with nanometer resolution in the cross section of a nanocrystalline W / Hf$_{0.5}$Zr$_{0.5}$O$_{2-\\delta}$ (HZO) /","W ferroelectric capacitor using multimodal electron microscopy.","This reveals a 1.4 nm wide tungsten sub-oxide interfacial layer formed at the bottom interface during fabrication which introduces a potential dip and leads to asymmetric switching fields.","Additionally, the measured inner potential in HZO is consistent with the presence of about 20% oxygen vacancies and a negative built-in potential in HZO.","These chemical and electrostatic details are important to characterize and tune to achieve high performance ferroelectric devices."],"url":"http://arxiv.org/abs/2405.11653v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-19 18:29:53","title":"Time depending magnetization of nanoparticles under radiofrequency fields: relaxation time in water for solid-liquid transition","abstract":"In application as hyperthermia and nanowarming, power dissipation arises when the time-dependent magnetization $M(t)$ of an out-of-equilibrium system of nanoparticles lags behind the applied field $H(t)$. The key parameter governing this process is the relaxation time $\\tau$ of the system, which induces a phase shift $\\phi_n$ between $H(t)$ and every nth harmonic component of $M(t)$. In this work, we present an expression for $M(t)$ in terms of $\\tau$ and the equilibrium magnetization, valid for any magnetic system exhibiting odd equilibrium response. From this calculation, we obtain a method for determining the effective $\\tau$ of a MNPs sample directly from the experimental measurement of $M(t)$. Additionally, we demonstrate that the power dissipation (SAR: Specific Absorption Rate) of any magnetic sample under a sinusoidal field can be obtained from the first harmonic component of $M(t)$. As an illustrative application, we explore the variation of $\\tau$ for magnetic MNPs in aqueous suspension during the melting process of the matrix. In this case, the change in $\\tau$ can be understood as a result of the reorientation of the MNPs in the direction of the applied field as the matrix becomes liquid.","sentences":["In application as hyperthermia and nanowarming, power dissipation arises when the time-dependent magnetization $M(t)$ of an out-of-equilibrium system of nanoparticles lags behind the applied field $H(t)$. The key parameter governing this process is the relaxation time $\\tau$ of the system, which induces a phase shift $\\phi_n$ between $H(t)$ and every nth harmonic component of $M(t)$. In this work, we present an expression for $M(t)$ in terms of $\\tau$ and the equilibrium magnetization, valid for any magnetic system exhibiting odd equilibrium response.","From this calculation, we obtain a method for determining the effective $\\tau$ of a MNPs sample directly from the experimental measurement of $M(t)$. Additionally, we demonstrate that the power dissipation (SAR: Specific Absorption Rate) of any magnetic sample under a sinusoidal field can be obtained from the first harmonic component of $M(t)$. As an illustrative application, we explore the variation of $\\tau$ for magnetic MNPs in aqueous suspension during the melting process of the matrix.","In this case, the change in $\\tau$ can be understood as a result of the reorientation of the MNPs in the direction of the applied field as the matrix becomes liquid."],"url":"http://arxiv.org/abs/2405.11641v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-19 17:53:28","title":"On ergodic properties of geodesic flows on uniform visibility manifolds without conjugate points","abstract":"In this paper, we conduct a comprehensive study on ergodic properties of the geodesic flow on a $C^\\infty$ uniform visibility manifold $M$ without conjugate points. If $M$ is a closed surface of genus at least two without conjugate points, and with continuous Green bundles and bounded asymptote, we study the geometric properties of singular geodesics and show that if all singular geodesics are closed, then there are at most finitely many isolated singular closed geodesics and finitely many generalized strips. In particular, the geodesic flow is ergodic with respect to Liouville measure under the above assumption.   Let $(M,g)$ be a closed uniform visibility manifold without conjugate points and $X$ its universal cover. Under the entropy gap assumption, the geodesic flow has a unique measure of maximal entropy (MME for short) by \\cite[Theorem 1.2]{MR}. We develop a Patterson-Sullivan construction of this unique MME and show that it has local product structure, is fully supported, and has the Bernoulli property.   If we assume further that $M$ has continuous Green bundles and the geodesic flow has a hyperbolic periodic point, using the nonuniform hyperbolic structure on an open dense subset and the symbolic approach developed in \\cite{LP}, we show that for any H\\\"{o}lder continuous function, the equilibrium state is unique under the pressure gap condition.   Under the same conditions above, we apply the mixing properties of the MME to count the number of free-homotopy classes containing a closed geodesic, as well as the volume asymptotics of Riemannian balls in the universal cover. We then obtain some rigidity results involving the Margulis function.   Finally, for a uniform visibility manifold $M=X/\\Gamma$ (not necessarily compact) without conjugate points, we show that if $\\Gamma$ is non-elementary and contains an expansive isometry, then the Hopf-Tsuji-Sullivan dichotomy holds.","sentences":["In this paper, we conduct a comprehensive study on ergodic properties of the geodesic flow on a $C^\\infty$ uniform visibility manifold $M$ without conjugate points.","If $M$ is a closed surface of genus at least two without conjugate points, and with continuous Green bundles and bounded asymptote, we study the geometric properties of singular geodesics and show that if all singular geodesics are closed, then there are at most finitely many isolated singular closed geodesics and finitely many generalized strips.","In particular, the geodesic flow is ergodic with respect to Liouville measure under the above assumption.   ","Let $(M,g)$ be a closed uniform visibility manifold without conjugate points and $X$ its universal cover.","Under the entropy gap assumption, the geodesic flow has a unique measure of maximal entropy (MME for short) by \\cite[Theorem 1.2]{MR}.","We develop a Patterson-Sullivan construction of this unique MME and show that it has local product structure, is fully supported, and has the Bernoulli property.   ","If we assume further that $M$ has continuous Green bundles and the geodesic flow has a hyperbolic periodic point, using the nonuniform hyperbolic structure on an open dense subset and the symbolic approach developed in \\cite{LP}, we show that for any H\\\"{o}lder continuous function, the equilibrium state is unique under the pressure gap condition.   ","Under the same conditions above, we apply the mixing properties of the MME to count the number of free-homotopy classes containing a closed geodesic, as well as the volume asymptotics of Riemannian balls in the universal cover.","We then obtain some rigidity results involving the Margulis function.   ","Finally, for a uniform visibility manifold $M=X/\\Gamma$ (not necessarily compact) without conjugate points, we show that if $\\Gamma$ is non-elementary and contains an expansive isometry, then the Hopf-Tsuji-Sullivan dichotomy holds."],"url":"http://arxiv.org/abs/2405.11635v1","category":"math.DS"}
{"created":"2024-05-19 17:46:40","title":"Attention to Quantum Complexity","abstract":"The imminent era of error-corrected quantum computing urgently demands robust methods to characterize complex quantum states, even from limited and noisy measurements. We introduce the Quantum Attention Network (QuAN), a versatile classical AI framework leveraging the power of attention mechanisms specifically tailored to address the unique challenges of learning quantum complexity. Inspired by large language models, QuAN treats measurement snapshots as tokens while respecting their permutation invariance. Combined with a novel parameter-efficient mini-set self-attention block (MSSAB), such data structure enables QuAN to access high-order moments of the bit-string distribution and preferentially attend to less noisy snapshots. We rigorously test QuAN across three distinct quantum simulation settings: driven hard-core Bose-Hubbard model, random quantum circuits, and the toric code under coherent and incoherent noise. QuAN directly learns the growth in entanglement and state complexity from experimentally obtained computational basis measurements. In particular, it learns the growth in complexity of random circuit data upon increasing depth from noisy experimental data. Taken to a regime inaccessible by existing theory, QuAN unveils the complete phase diagram for noisy toric code data as a function of both noise types. This breakthrough highlights the transformative potential of using purposefully designed AI-driven solutions to assist quantum hardware.","sentences":["The imminent era of error-corrected quantum computing urgently demands robust methods to characterize complex quantum states, even from limited and noisy measurements.","We introduce the Quantum Attention Network (QuAN), a versatile classical AI framework leveraging the power of attention mechanisms specifically tailored to address the unique challenges of learning quantum complexity.","Inspired by large language models, QuAN treats measurement snapshots as tokens while respecting their permutation invariance.","Combined with a novel parameter-efficient mini-set self-attention block (MSSAB), such data structure enables QuAN to access high-order moments of the bit-string distribution and preferentially attend to less noisy snapshots.","We rigorously test QuAN across three distinct quantum simulation settings: driven hard-core Bose-Hubbard model, random quantum circuits, and the toric code under coherent and incoherent noise.","QuAN directly learns the growth in entanglement and state complexity from experimentally obtained computational basis measurements.","In particular, it learns the growth in complexity of random circuit data upon increasing depth from noisy experimental data.","Taken to a regime inaccessible by existing theory, QuAN unveils the complete phase diagram for noisy toric code data as a function of both noise types.","This breakthrough highlights the transformative potential of using purposefully designed AI-driven solutions to assist quantum hardware."],"url":"http://arxiv.org/abs/2405.11632v1","category":"quant-ph"}
{"created":"2024-05-19 17:08:31","title":"Decoding by Contrasting Knowledge: Enhancing LLMs' Confidence on Edited Facts","abstract":"The knowledge within large language models (LLMs) may become outdated quickly. While in-context editing (ICE) is currently the most effective method for knowledge editing (KE), it is constrained by the black-box modeling of LLMs and thus lacks interpretability. Our work aims to elucidate the superior performance of ICE on the KE by analyzing the impacts of in-context new knowledge on token-wise distributions. We observe that despite a significant boost in logits of the new knowledge, the performance of is still hindered by stubborn knowledge. Stubborn knowledge refers to as facts that have gained excessive confidence during pretraining, making it hard to edit effectively. To address this issue and further enhance the performance of ICE, we propose a novel approach termed $\\textbf{De}$coding by $\\textbf{C}$ontrasting $\\textbf{K}$nowledge (DeCK). DeCK derives the distribution of the next token by contrasting the logits obtained from the newly edited knowledge guided by ICE with those from the unedited parametric knowledge. Our experiments consistently demonstrate that DeCK enhances the confidence of LLMs in edited facts. For instance, it improves the performance of LLaMA3-8B-instruct on MQuAKE by up to 219%, demonstrating its capability to strengthen ICE in the editing of stubborn knowledge. Our work paves the way to develop the both effective and accountable KE methods for LLMs. (The source code is available at: $\\href{https://github.com/byronBBL/DeCK}{\\text{this https URL.}}$ )","sentences":["The knowledge within large language models (LLMs) may become outdated quickly.","While in-context editing (ICE) is currently the most effective method for knowledge editing (KE), it is constrained by the black-box modeling of LLMs and thus lacks interpretability.","Our work aims to elucidate the superior performance of ICE on the KE by analyzing the impacts of in-context new knowledge on token-wise distributions.","We observe that despite a significant boost in logits of the new knowledge, the performance of is still hindered by stubborn knowledge.","Stubborn knowledge refers to as facts that have gained excessive confidence during pretraining, making it hard to edit effectively.","To address this issue and further enhance the performance of ICE, we propose a novel approach termed $\\textbf{De}$coding by $\\textbf{C}$ontrasting $\\textbf{K}$nowledge (DeCK).","DeCK derives the distribution of the next token by contrasting the logits obtained from the newly edited knowledge guided by ICE with those from the unedited parametric knowledge.","Our experiments consistently demonstrate that DeCK enhances the confidence of LLMs in edited facts.","For instance, it improves the performance of LLaMA3-8B-instruct on MQuAKE by up to 219%, demonstrating its capability to strengthen ICE in the editing of stubborn knowledge.","Our work paves the way to develop the both effective and accountable KE methods for LLMs.","(The source code is available at: $\\href{https://github.com/byronBBL/DeCK}{\\text{this https URL.}}$ )"],"url":"http://arxiv.org/abs/2405.11613v1","category":"cs.CL"}
{"created":"2024-05-19 15:50:57","title":"Global Convergence of Decentralized Retraction-Free Optimization on the Stiefel Manifold","abstract":"Many classical and modern machine learning algorithms require solving optimization tasks under orthogonal constraints. Solving these tasks often require calculating retraction-based gradient descent updates on the corresponding Riemannian manifold, which can be computationally expensive. Recently Ablin et al. proposed an infeasible retraction-free algorithm, which is significantly more efficient. In this paper, we study the decentralized non-convex optimization task over a network of agents on the Stiefel manifold with retraction-free updates. We propose \\textbf{D}ecentralized \\textbf{R}etraction-\\textbf{F}ree \\textbf{G}radient \\textbf{T}racking (DRFGT) algorithm, and show that DRFGT exhibits ergodic $\\mathcal{O}(1/K)$ convergence rate, the same rate of convergence as the centralized, retraction-based methods. We also provide numerical experiments demonstrating that DRFGT performs on par with the state-of-the-art retraction based methods with substantially reduced computational overhead.","sentences":["Many classical and modern machine learning algorithms require solving optimization tasks under orthogonal constraints.","Solving these tasks often require calculating retraction-based gradient descent updates on the corresponding Riemannian manifold, which can be computationally expensive.","Recently Ablin et al. proposed an infeasible retraction-free algorithm, which is significantly more efficient.","In this paper, we study the decentralized non-convex optimization task over a network of agents on the Stiefel manifold with retraction-free updates.","We propose \\textbf{D}ecentralized \\textbf{R}etraction-\\textbf{F}ree \\textbf{G}radient \\textbf{T}racking (DRFGT) algorithm, and show that DRFGT exhibits ergodic $\\mathcal{O}(1/K)$ convergence rate, the same rate of convergence as the centralized, retraction-based methods.","We also provide numerical experiments demonstrating that DRFGT performs on par with the state-of-the-art retraction based methods with substantially reduced computational overhead."],"url":"http://arxiv.org/abs/2405.11590v1","category":"cs.LG"}
{"created":"2024-05-19 15:45:44","title":"Magnetic field dragging in filamentary molecular clouds","abstract":"Maps of polarized dust emission of molecular clouds reveal the morphology of the magnetic field associated with star-forming regions. In particular, polarization maps of hub-filament systems show the distortion of magnetic field lines induced by gas flows onto and inside filaments. We aim to understand the relation between the curvature of magnetic field lines associated with filaments in hub-filament systems and the properties of the underlying gas flows. We consider steady-state models of gas with finite electrical resistivity flowing across a transverse magnetic field. We derive the relation between the bending of the field lines and the flow parameters represented by the Alfv\\'en Mach number and the magnetic Reynolds number. We find that, on the scale of the filaments, the relevant parameter for a gas of finite electrical resistivity is the magnetic Reynolds number, and we derive the relation between the deflection angle of the field from the initial direction (assumed perpendicular to the filament) and the value of the electrical resistivity, due to either Ohmic dissipation or ambipolar diffusion. Application of this model to specific observations of polarized dust emission in filamentary clouds shows that magnetic Reynolds numbers of a few tens are required to reproduce the data. Despite significant uncertainties in the observations (the flow speed, the geometry and orientation of the filament), and the idealization of the model, the specific cases considered show that ambipolar diffusion can provide the resistivity needed to maintain a steady state flow across magnetic fields of significant strength over realistic time scales.","sentences":["Maps of polarized dust emission of molecular clouds reveal the morphology of the magnetic field associated with star-forming regions.","In particular, polarization maps of hub-filament systems show the distortion of magnetic field lines induced by gas flows onto and inside filaments.","We aim to understand the relation between the curvature of magnetic field lines associated with filaments in hub-filament systems and the properties of the underlying gas flows.","We consider steady-state models of gas with finite electrical resistivity flowing across a transverse magnetic field.","We derive the relation between the bending of the field lines and the flow parameters represented by the Alfv\\'en Mach number and the magnetic Reynolds number.","We find that, on the scale of the filaments, the relevant parameter for a gas of finite electrical resistivity is the magnetic Reynolds number, and we derive the relation between the deflection angle of the field from the initial direction (assumed perpendicular to the filament) and the value of the electrical resistivity, due to either Ohmic dissipation or ambipolar diffusion.","Application of this model to specific observations of polarized dust emission in filamentary clouds shows that magnetic Reynolds numbers of a few tens are required to reproduce the data.","Despite significant uncertainties in the observations (the flow speed, the geometry and orientation of the filament), and the idealization of the model, the specific cases considered show that ambipolar diffusion can provide the resistivity needed to maintain a steady state flow across magnetic fields of significant strength over realistic time scales."],"url":"http://arxiv.org/abs/2405.11589v1","category":"astro-ph.GA"}
{"created":"2024-05-19 15:22:25","title":"SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization","abstract":"Transformers have become foundational architectures for both natural language and computer vision tasks. However, the high computational cost makes it quite challenging to deploy on resource-constraint devices. This paper investigates the computational bottleneck modules of efficient transformer, i.e., normalization layers and attention modules. LayerNorm is commonly used in transformer architectures but is not computational friendly due to statistic calculation during inference. However, replacing LayerNorm with more efficient BatchNorm in transformer often leads to inferior performance and collapse in training. To address this problem, we propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training. Moreover, we propose a simplified linear attention (SLA) module that is simple yet effective to achieve strong performance. Extensive experiments on image classification as well as object detection demonstrate the effectiveness of our proposed method. For example, our SLAB-Swin obtains $83.6\\%$ top-1 accuracy on ImageNet-1K with $16.2$ms latency, which is $2.4$ms less than that of Flatten-Swin with $0.1\\%$ higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower latency.Codes are publicly available at https://github.com/xinghaochen/SLAB and https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB.","sentences":["Transformers have become foundational architectures for both natural language and computer vision tasks.","However, the high computational cost makes it quite challenging to deploy on resource-constraint devices.","This paper investigates the computational bottleneck modules of efficient transformer, i.e., normalization layers and attention modules.","LayerNorm is commonly used in transformer architectures but is not computational friendly due to statistic calculation during inference.","However, replacing LayerNorm with more efficient BatchNorm in transformer often leads to inferior performance and collapse in training.","To address this problem, we propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training.","Moreover, we propose a simplified linear attention (SLA) module that is simple yet effective to achieve strong performance.","Extensive experiments on image classification as well as object detection demonstrate the effectiveness of our proposed method.","For example, our SLAB-Swin obtains $83.6\\%$ top-1 accuracy on ImageNet-1K with $16.2$ms latency, which is $2.4$ms less than that of Flatten-Swin with $0.1\\%$ higher accuracy.","We also evaluated our method for language modeling task and obtain comparable performance and lower latency.","Codes are publicly available at https://github.com/xinghaochen/SLAB and https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB."],"url":"http://arxiv.org/abs/2405.11582v1","category":"cs.CV"}
{"created":"2024-05-19 15:11:22","title":"Random Attention Span","abstract":"In this paper, I introduce a random attention span model (RAS) which uses stopping time to identify decision-makers' behavior under limited attention. Unlike many limited attention models, the RAS identifies preferences using time variation without any need for menu variation. In addition, the RAS allows the consideration set to be correlated with the preference. I also use the revealed preference theory that provides testable implications for observable choice probabilities. Then, I test the model and estimate the preference distribution using data from M-Turk experiments on choice behaviors that involve lotteries; there is general alignment with the distribution results from logit attention model.","sentences":["In this paper, I introduce a random attention span model (RAS) which uses stopping time to identify decision-makers' behavior under limited attention.","Unlike many limited attention models, the RAS identifies preferences using time variation without any need for menu variation.","In addition, the RAS allows the consideration set to be correlated with the preference.","I also use the revealed preference theory that provides testable implications for observable choice probabilities.","Then, I test the model and estimate the preference distribution using data from M-Turk experiments on choice behaviors that involve lotteries; there is general alignment with the distribution results from logit attention model."],"url":"http://arxiv.org/abs/2405.11578v1","category":"econ.TH"}
{"created":"2024-05-19 14:30:57","title":"Uncertainty-Aware PPG-2-ECG for Enhanced Cardiovascular Diagnosis using Diffusion Models","abstract":"Analyzing the cardiovascular system condition via Electrocardiography (ECG) is a common and highly effective approach, and it has been practiced and perfected over many decades. ECG sensing is non-invasive and relatively easy to acquire, and yet it is still cumbersome for holter monitoring tests that may span over hours and even days. A possible alternative in this context is Photoplethysmography (PPG): An optically-based signal that measures blood volume fluctuations, as typically sensed by conventional ``wearable devices''. While PPG presents clear advantages in acquisition, convenience, and cost-effectiveness, ECG provides more comprehensive information, allowing for a more precise detection of heart conditions. This implies that a conversion from PPG to ECG, as recently discussed in the literature, inherently involves an unavoidable level of uncertainty. In this paper we introduce a novel methodology for addressing the PPG-2-ECG conversion, and offer an enhanced classification of cardiovascular conditions using the given PPG, all while taking into account the uncertainties arising from the conversion process. We provide a mathematical justification for our proposed computational approach, and present empirical studies demonstrating its superior performance compared to state-of-the-art baseline methods.","sentences":["Analyzing the cardiovascular system condition via Electrocardiography (ECG) is a common and highly effective approach, and it has been practiced and perfected over many decades.","ECG sensing is non-invasive and relatively easy to acquire, and yet it is still cumbersome for holter monitoring tests that may span over hours and even days.","A possible alternative in this context is Photoplethysmography (PPG): An optically-based signal that measures blood volume fluctuations, as typically sensed by conventional ``wearable devices''.","While PPG presents clear advantages in acquisition, convenience, and cost-effectiveness, ECG provides more comprehensive information, allowing for a more precise detection of heart conditions.","This implies that a conversion from PPG to ECG, as recently discussed in the literature, inherently involves an unavoidable level of uncertainty.","In this paper we introduce a novel methodology for addressing the PPG-2-ECG conversion, and offer an enhanced classification of cardiovascular conditions using the given PPG, all while taking into account the uncertainties arising from the conversion process.","We provide a mathematical justification for our proposed computational approach, and present empirical studies demonstrating its superior performance compared to state-of-the-art baseline methods."],"url":"http://arxiv.org/abs/2405.11566v1","category":"cs.LG"}
{"created":"2024-05-19 14:29:02","title":"User-Centric Association and Feedback Bit Allocation for FDD Cell-Free Massive MIMO","abstract":"In this paper, we introduce a novel approach to user-centric association and feedback bit allocation for the downlink of a cell-free massive MIMO (CF-mMIMO) system, operating under limited feedback constraints. In CF-mMIMO systems employing frequency division duplexing, each access point (AP) relies on channel information provided by its associated user equipments (UEs) for beamforming design. Since the uplink control channel is typically shared among UEs, we take account of each AP's total feedback budget, which is distributed among its associated UEs. By employing the Saleh-Valenzuela multi-resolvable path channel model with different average path gains, we first identify necessary feedback information for each UE, along with an appropriate codebook structure. This structure facilitates adaptive quantization of multiple paths based on their dominance. We then formulate a joint optimization problem addressing user-centric UE-AP association and feedback bit allocation. To address this challenge, we analyze the impact of feedback bit allocation and derive our proposed scheme from the solution of an alternative optimization problem aimed at devising long-term policies, explicitly considering the effects of feedback bit allocation. Numerical results show that our proposed scheme effectively enhances the performance of conventional approaches in CF-mMIMO systems.","sentences":["In this paper, we introduce a novel approach to user-centric association and feedback bit allocation for the downlink of a cell-free massive MIMO (CF-mMIMO) system, operating under limited feedback constraints.","In CF-mMIMO systems employing frequency division duplexing, each access point (AP) relies on channel information provided by its associated user equipments (UEs) for beamforming design.","Since the uplink control channel is typically shared among UEs, we take account of each AP's total feedback budget, which is distributed among its associated UEs.","By employing the Saleh-Valenzuela multi-resolvable path channel model with different average path gains, we first identify necessary feedback information for each UE, along with an appropriate codebook structure.","This structure facilitates adaptive quantization of multiple paths based on their dominance.","We then formulate a joint optimization problem addressing user-centric UE-AP association and feedback bit allocation.","To address this challenge, we analyze the impact of feedback bit allocation and derive our proposed scheme from the solution of an alternative optimization problem aimed at devising long-term policies, explicitly considering the effects of feedback bit allocation.","Numerical results show that our proposed scheme effectively enhances the performance of conventional approaches in CF-mMIMO systems."],"url":"http://arxiv.org/abs/2405.11563v1","category":"cs.IT"}
{"created":"2024-05-19 14:22:21","title":"High Discrimination Ratio, Broadband Circularly Polarized Light Photodetector Using Dielectric Achiral Nanostructures","abstract":"The on-chip measurement of polarization states plays an increasingly crucial role in modern sensing and imaging applications. While high-performance monolithic linearly polarized photodetectors have been extensively studied, integrated circularly polarized light (CPL) photodetectors are still hindered by inadequate discrimination capability. In this study, we employ achiral all-dielectric nanostructures to develop a broadband CPL photodetector with an impressive discrimination ratio of ~107 at the wavelength of 405 nm, significantly surpassing its counterparts by two orders of magnitude. Our device shows outstanding CPL discrimination capability across the visible band without requiring intensity calibration. Its function mechanism is based on the CPL-dependent near-field modes within achiral structures: under left or right CPL illumination, distinct near-field modes are excited, resulting in asymmetric irradiation of the two electrodes and generating a photovoltage with directions determined by the chirality of the incident light field. The proposed design strategy facilitates the realization of ultra-compact CPL detection across diverse materials, structures, and spectral ranges, presenting a novel avenue for achieving high-performance monolithic CPL detection.","sentences":["The on-chip measurement of polarization states plays an increasingly crucial role in modern sensing and imaging applications.","While high-performance monolithic linearly polarized photodetectors have been extensively studied, integrated circularly polarized light (CPL) photodetectors are still hindered by inadequate discrimination capability.","In this study, we employ achiral all-dielectric nanostructures to develop a broadband CPL photodetector with an impressive discrimination ratio of ~107 at the wavelength of 405 nm, significantly surpassing its counterparts by two orders of magnitude.","Our device shows outstanding CPL discrimination capability across the visible band without requiring intensity calibration.","Its function mechanism is based on the CPL-dependent near-field modes within achiral structures: under left or right CPL illumination, distinct near-field modes are excited, resulting in asymmetric irradiation of the two electrodes and generating a photovoltage with directions determined by the chirality of the incident light field.","The proposed design strategy facilitates the realization of ultra-compact CPL detection across diverse materials, structures, and spectral ranges, presenting a novel avenue for achieving high-performance monolithic CPL detection."],"url":"http://arxiv.org/abs/2405.11560v1","category":"physics.optics"}
{"created":"2024-05-19 14:06:24","title":"Perspective: Multi-configurational methods in bio-inorganic chemistry","abstract":"Transition metal ions play crucial roles in the structure and function of numerous proteins, contributing to essential biological processes such as catalysis, electron transfer, and oxygen binding. However, accurately modeling the electronic structure and properties of metalloproteins poses significant challenges due to the complex nature of their electronic configurations and strong correlation effects. Multiconfigurational quantum chemistry methods are, in principle, the most appropriate tools for addressing these challenges, offering the capability to capture the inherent multi-reference character and strong electron correlation present in bio-inorganic systems. Yet their computational cost has long hindered wider adoption, making methods such as Density Functional Theory (DFT) the method of choice. However, advancements over the past decade have substantially alleviated this limitation, rendering multiconfigurational quantum chemistry methods more accessible and applicable to a wider range of bio-inorganic systems. In this perspective, we discuss some of these developments and how they have already been used to answer some of the most important questions in bio-inorganic chemistry. We also comment on ongoing developments in the field and how the future of the field may evolve.","sentences":["Transition metal ions play crucial roles in the structure and function of numerous proteins, contributing to essential biological processes such as catalysis, electron transfer, and oxygen binding.","However, accurately modeling the electronic structure and properties of metalloproteins poses significant challenges due to the complex nature of their electronic configurations and strong correlation effects.","Multiconfigurational quantum chemistry methods are, in principle, the most appropriate tools for addressing these challenges, offering the capability to capture the inherent multi-reference character and strong electron correlation present in bio-inorganic systems.","Yet their computational cost has long hindered wider adoption, making methods such as Density Functional Theory (DFT) the method of choice.","However, advancements over the past decade have substantially alleviated this limitation, rendering multiconfigurational quantum chemistry methods more accessible and applicable to a wider range of bio-inorganic systems.","In this perspective, we discuss some of these developments and how they have already been used to answer some of the most important questions in bio-inorganic chemistry.","We also comment on ongoing developments in the field and how the future of the field may evolve."],"url":"http://arxiv.org/abs/2405.11553v1","category":"physics.chem-ph"}
{"created":"2024-05-19 13:23:05","title":"Certified Robust Accuracy of Neural Networks Are Bounded due to Bayes Errors","abstract":"Adversarial examples pose a security threat to many critical systems built on neural networks. While certified training improves robustness, it also decreases accuracy noticeably. Despite various proposals for addressing this issue, the significant accuracy drop remains. More importantly, it is not clear whether there is a certain fundamental limit on achieving robustness whilst maintaining accuracy. In this work, we offer a novel perspective based on Bayes errors. By adopting Bayes error to robustness analysis, we investigate the limit of certified robust accuracy, taking into account data distribution uncertainties. We first show that the accuracy inevitably decreases in the pursuit of robustness due to changed Bayes error in the altered data distribution. Subsequently, we establish an upper bound for certified robust accuracy, considering the distribution of individual classes and their boundaries. Our theoretical results are empirically evaluated on real-world datasets and are shown to be consistent with the limited success of existing certified training results, \\emph{e.g.}, for CIFAR10, our analysis results in an upper bound (of certified robust accuracy) of 67.49\\%, meanwhile existing approaches are only able to increase it from 53.89\\% in 2017 to 62.84\\% in 2023.","sentences":["Adversarial examples pose a security threat to many critical systems built on neural networks.","While certified training improves robustness, it also decreases accuracy noticeably.","Despite various proposals for addressing this issue, the significant accuracy drop remains.","More importantly, it is not clear whether there is a certain fundamental limit on achieving robustness whilst maintaining accuracy.","In this work, we offer a novel perspective based on Bayes errors.","By adopting Bayes error to robustness analysis, we investigate the limit of certified robust accuracy, taking into account data distribution uncertainties.","We first show that the accuracy inevitably decreases in the pursuit of robustness due to changed Bayes error in the altered data distribution.","Subsequently, we establish an upper bound for certified robust accuracy, considering the distribution of individual classes and their boundaries.","Our theoretical results are empirically evaluated on real-world datasets and are shown to be consistent with the limited success of existing certified training results, \\emph{e.g.}, for CIFAR10, our analysis results in an upper bound (of certified robust accuracy) of 67.49\\%, meanwhile existing approaches are only able to increase it from 53.89\\% in 2017 to 62.84\\% in 2023."],"url":"http://arxiv.org/abs/2405.11547v1","category":"stat.ML"}
{"created":"2024-05-19 13:21:49","title":"Dynamical regimes of CCN activation in adiabatic air parcels","abstract":"Ubiquitous, yet elusive to a complete understanding: Tiny, warm clouds with faint visual signatures play a critical role in Earth's energy balance. These \"twilight clouds\", as they are sometimes called, form under weak updraft conditions. Their constituent particles exist in a precarious state, teetering between hazy wisps and activated droplets. This delicate thermodynamic balance creates a limited reservoir of supersaturation, which activated droplets readily consume. Our research presents a novel approach, solving coupled equations for particle growth (K\\\"ohler's equation) and supersaturation change. This reveals previously unconsidered activation states for these clouds. Additionally, the analysis predicts conditions where particles can exhibit self-sustained oscillations between haze and activated droplet states.","sentences":["Ubiquitous, yet elusive to a complete understanding: Tiny, warm clouds with faint visual signatures play a critical role in Earth's energy balance.","These \"twilight clouds\", as they are sometimes called, form under weak updraft conditions.","Their constituent particles exist in a precarious state, teetering between hazy wisps and activated droplets.","This delicate thermodynamic balance creates a limited reservoir of supersaturation, which activated droplets readily consume.","Our research presents a novel approach, solving coupled equations for particle growth (K\\\"ohler's equation) and supersaturation change.","This reveals previously unconsidered activation states for these clouds.","Additionally, the analysis predicts conditions where particles can exhibit self-sustained oscillations between haze and activated droplet states."],"url":"http://arxiv.org/abs/2405.11545v1","category":"physics.ao-ph"}
{"created":"2024-05-19 12:49:21","title":"RobMOT: Robust 3D Multi-Object Tracking by Observational Noise and State Estimation Drift Mitigation on LiDAR PointCloud","abstract":"This work addresses the inherited limitations in the current state-of-the-art 3D multi-object tracking (MOT) methods that follow the tracking-by-detection paradigm, notably trajectory estimation drift for long-occluded objects in LiDAR point cloud streams acquired by autonomous cars. In addition, the absence of adequate track legitimacy verification results in ghost track accumulation. To tackle these issues, we introduce a two-fold innovation. Firstly, we propose refinement in Kalman filter that enhances trajectory drift noise mitigation, resulting in more robust state estimation for occluded objects. Secondly, we propose a novel online track validity mechanism to distinguish between legitimate and ghost tracks combined with a multi-stage observational gating process for incoming observations. This mechanism substantially reduces ghost tracks by up to 80\\% and improves HOTA by 7\\%. Accordingly, we propose an online 3D MOT framework, RobMOT, that demonstrates superior performance over the top-performing state-of-the-art methods, including deep learning approaches, across various detectors with up to 3.28\\% margin in MOTA and 2.36\\% in HOTA. RobMOT excels under challenging conditions, such as prolonged occlusions and the tracking of distant objects, with up to 59\\% enhancement in processing latency.","sentences":["This work addresses the inherited limitations in the current state-of-the-art 3D multi-object tracking (MOT) methods that follow the tracking-by-detection paradigm, notably trajectory estimation drift for long-occluded objects in LiDAR point cloud streams acquired by autonomous cars.","In addition, the absence of adequate track legitimacy verification results in ghost track accumulation.","To tackle these issues, we introduce a two-fold innovation.","Firstly, we propose refinement in Kalman filter that enhances trajectory drift noise mitigation, resulting in more robust state estimation for occluded objects.","Secondly, we propose a novel online track validity mechanism to distinguish between legitimate and ghost tracks combined with a multi-stage observational gating process for incoming observations.","This mechanism substantially reduces ghost tracks by up to 80\\% and improves HOTA by 7\\%.","Accordingly, we propose an online 3D MOT framework, RobMOT, that demonstrates superior performance over the top-performing state-of-the-art methods, including deep learning approaches, across various detectors with up to 3.28\\% margin in MOTA and 2.36\\% in HOTA.","RobMOT excels under challenging conditions, such as prolonged occlusions and the tracking of distant objects, with up to 59\\% enhancement in processing latency."],"url":"http://arxiv.org/abs/2405.11536v1","category":"cs.CV"}
{"created":"2024-05-19 12:24:30","title":"Hierarchical Selective Classification","abstract":"Deploying deep neural networks for risk-sensitive tasks necessitates an uncertainty estimation mechanism. This paper introduces hierarchical selective classification, extending selective classification to a hierarchical setting. Our approach leverages the inherent structure of class relationships, enabling models to reduce the specificity of their predictions when faced with uncertainty. In this paper, we first formalize hierarchical risk and coverage, and introduce hierarchical risk-coverage curves. Next, we develop algorithms for hierarchical selective classification (which we refer to as \"inference rules\"), and propose an efficient algorithm that guarantees a target accuracy constraint with high probability. Lastly, we conduct extensive empirical studies on over a thousand ImageNet classifiers, revealing that training regimes such as CLIP, pretraining on ImageNet21k and knowledge distillation boost hierarchical selective performance.","sentences":["Deploying deep neural networks for risk-sensitive tasks necessitates an uncertainty estimation mechanism.","This paper introduces hierarchical selective classification, extending selective classification to a hierarchical setting.","Our approach leverages the inherent structure of class relationships, enabling models to reduce the specificity of their predictions when faced with uncertainty.","In this paper, we first formalize hierarchical risk and coverage, and introduce hierarchical risk-coverage curves.","Next, we develop algorithms for hierarchical selective classification (which we refer to as \"inference rules\"), and propose an efficient algorithm that guarantees a target accuracy constraint with high probability.","Lastly, we conduct extensive empirical studies on over a thousand ImageNet classifiers, revealing that training regimes such as CLIP, pretraining on ImageNet21k and knowledge distillation boost hierarchical selective performance."],"url":"http://arxiv.org/abs/2405.11533v1","category":"cs.LG"}
{"created":"2024-05-19 11:24:21","title":"A comparative study of augmented inverse propensity weighted estimators using outcome-adaptive lasso and other penalized regression methods","abstract":"Confounder selection may be efficiently conducted using penalized regression methods when causal effects are estimated from observational data with many variables. An outcome-adaptive lasso was proposed to build a model for the propensity score that can be employed in conjunction with other variable selection methods for the outcome model to apply the augmented inverse propensity weighted (AIPW) estimator. However, researchers may not know which method is optimal to use for outcome model when applying the AIPW estimator with the outcome-adaptive lasso. This study provided hints on readily implementable penalized regression methods that should be adopted for the outcome model as a counterpart of the outcome-adaptive lasso. We evaluated the bias and variance of the AIPW estimators using the propensity score (PS) model and an outcome model based on penalized regression methods under various conditions by analyzing a clinical trial example and numerical experiments; the estimates and standard errors of the AIPW estimators were almost identical in an example with over 5000 participants. The AIPW estimators using penalized regression methods with the oracle property performed well in terms of bias and variance in numerical experiments with smaller sample sizes. Meanwhile, the bias of the AIPW estimator using the ordinary lasso for the PS and outcome models was considerably larger.","sentences":["Confounder selection may be efficiently conducted using penalized regression methods when causal effects are estimated from observational data with many variables.","An outcome-adaptive lasso was proposed to build a model for the propensity score that can be employed in conjunction with other variable selection methods for the outcome model to apply the augmented inverse propensity weighted (AIPW) estimator.","However, researchers may not know which method is optimal to use for outcome model when applying the AIPW estimator with the outcome-adaptive lasso.","This study provided hints on readily implementable penalized regression methods that should be adopted for the outcome model as a counterpart of the outcome-adaptive lasso.","We evaluated the bias and variance of the AIPW estimators using the propensity score (PS) model and an outcome model based on penalized regression methods under various conditions by analyzing a clinical trial example and numerical experiments; the estimates and standard errors of the AIPW estimators were almost identical in an example with over 5000 participants.","The AIPW estimators using penalized regression methods with the oracle property performed well in terms of bias and variance in numerical experiments with smaller sample sizes.","Meanwhile, the bias of the AIPW estimator using the ordinary lasso for the PS and outcome models was considerably larger."],"url":"http://arxiv.org/abs/2405.11522v1","category":"stat.ME"}
{"created":"2024-05-19 11:22:51","title":"On Performance of FAS-aided Wireless Powered NOMA Communication Systems","abstract":"This paper studies the performance of a wireless powered communication network (WPCN) under the non-orthogonal multiple access (NOMA) scheme, where users take advantage of an emerging fluid antenna system (FAS). More precisely, we consider a scenario where a transmitter is powered by a remote power beacon (PB) to send information to the planar NOMA FAS-equipped users through Rayleigh fading channels. After introducing the distribution of the equivalent channel coefficients to the users, we derive compact analytical expressions for the outage probability (OP) in order to evaluate the system performance. Additionally, we present asymptotic OP in the high signal-to-noise ratio (SNR) regime. Eventually, results reveal that deploying the FAS with only one activated port in NOMA users can significantly enhance the WPCN performance compared with using traditional antenna systems (TAS).","sentences":["This paper studies the performance of a wireless powered communication network (WPCN) under the non-orthogonal multiple access (NOMA) scheme, where users take advantage of an emerging fluid antenna system (FAS).","More precisely, we consider a scenario where a transmitter is powered by a remote power beacon (PB) to send information to the planar NOMA FAS-equipped users through Rayleigh fading channels.","After introducing the distribution of the equivalent channel coefficients to the users, we derive compact analytical expressions for the outage probability (OP) in order to evaluate the system performance.","Additionally, we present asymptotic OP in the high signal-to-noise ratio (SNR) regime.","Eventually, results reveal that deploying the FAS with only one activated port in NOMA users can significantly enhance the WPCN performance compared with using traditional antenna systems (TAS)."],"url":"http://arxiv.org/abs/2405.11520v1","category":"cs.IT"}
{"created":"2024-05-19 11:12:10","title":"On the Convergence of No-Regret Dynamics in Information Retrieval Games with Proportional Ranking Functions","abstract":"Publishers who publish their content on the web act strategically, in a behavior that can be modeled within the online learning framework. Regret, a central concept in machine learning, serves as a canonical measure for assessing the performance of learning agents within this framework. We prove that any proportional content ranking function with a concave activation function induces games in which no-regret learning dynamics converge. Moreover, for proportional ranking functions, we prove the equivalence of the concavity of the activation function, the social concavity of the induced games and the concavity of the induced games. We also study the empirical trade-offs between publishers' and users' welfare, under different choices of the activation function, using a state-of-the-art no-regret dynamics algorithm. Furthermore, we demonstrate how the choice of the ranking function and changes in the ecosystem structure affect these welfare measures, as well as the dynamics' convergence rate.","sentences":["Publishers who publish their content on the web act strategically, in a behavior that can be modeled within the online learning framework.","Regret, a central concept in machine learning, serves as a canonical measure for assessing the performance of learning agents within this framework.","We prove that any proportional content ranking function with a concave activation function induces games in which no-regret learning dynamics converge.","Moreover, for proportional ranking functions, we prove the equivalence of the concavity of the activation function, the social concavity of the induced games and the concavity of the induced games.","We also study the empirical trade-offs between publishers' and users' welfare, under different choices of the activation function, using a state-of-the-art no-regret dynamics algorithm.","Furthermore, we demonstrate how the choice of the ranking function and changes in the ecosystem structure affect these welfare measures, as well as the dynamics' convergence rate."],"url":"http://arxiv.org/abs/2405.11517v1","category":"cs.GT"}
{"created":"2024-05-19 10:50:06","title":"Optimizing Underwater IoT Routing with Multi-Criteria Decision Making and Uncertainty Weights","abstract":"Effective data routing is vital in the Internet of Things (IoT) paradigm, especially in underwater mobile sensor networks where inefficiency can lead to significant resource consumption. This article presents an innovative method designed to enhance network performance and reduce resource usage, while also accurately determining component weights in these networks, ensuring quality service. Building upon previous research on multi-criteria decision-making systems in coastal RPL networks, our method involves key adaptations for underwater environments. It integrates comprehensive network features to identify the optimal parent node for each sensor, employing the fuzzy SWARA decision-making approach under uncertain conditions. This method takes into account various factors including hops, energy, ARSSI rate, delay, ETX, link delivery rate, and depth to determine the most effective parent node assignment. Through simulation, our approach demonstrates marked improvements in network performance compared to existing solutions. These advancements are significant, offering a new direction in enhancing underwater IoT communications and suggesting wider applications for IoT systems facing similar challenges.","sentences":["Effective data routing is vital in the Internet of Things (IoT) paradigm, especially in underwater mobile sensor networks where inefficiency can lead to significant resource consumption.","This article presents an innovative method designed to enhance network performance and reduce resource usage, while also accurately determining component weights in these networks, ensuring quality service.","Building upon previous research on multi-criteria decision-making systems in coastal RPL networks, our method involves key adaptations for underwater environments.","It integrates comprehensive network features to identify the optimal parent node for each sensor, employing the fuzzy SWARA decision-making approach under uncertain conditions.","This method takes into account various factors including hops, energy, ARSSI rate, delay, ETX, link delivery rate, and depth to determine the most effective parent node assignment.","Through simulation, our approach demonstrates marked improvements in network performance compared to existing solutions.","These advancements are significant, offering a new direction in enhancing underwater IoT communications and suggesting wider applications for IoT systems facing similar challenges."],"url":"http://arxiv.org/abs/2405.11513v1","category":"cs.NI"}
{"created":"2024-05-19 08:57:51","title":"Measurement of the effective leptonic electroweak mixing angle and Drell-Yan forward-backward asymmetry using pp collisions at $\\sqrt{s}=13$ TeV","abstract":"A measurement of the effective leptonic weak mixing angle $\\sin^2\\theta_\\mathrm{eff}^\\ell$ is presented using the forward-backward asymmetry in Drell-Yan dilepton events produced in pp collisions at $\\sqrt{s}=13$ TeV. The data sample corresponds to 137 fb$^{-1}$ of integrated luminosity and consists of dimuon and dielectron events, including the forward electrons reconstructed beyond the coverage of the CMS tracking detectors. Using the CT18Z set of the parton distribution functions (PDF), which provides the best coverage of the central values extracted with the other global PDFs, we obtain $$\\sin^2\\theta^\\ell_\\mathrm{eff} = 0.23157 \\pm 0.00031,$$ where the total uncertainty includes the statistical uncetainties (0.00010), as well as the correlated experimental (0.00015), theoretical (0.00009), and PDF (0.00027) systematic uncertainties. The measured value agrees well with the Standard Model prediction. This is the most precise measurement at a hadron collider with comparable precision to the two most precise results obtained at LEP and SLD.","sentences":["A measurement of the effective leptonic weak mixing angle $\\sin^2\\theta_\\mathrm{eff}^\\ell$ is presented using the forward-backward asymmetry in Drell-Yan dilepton events produced in pp collisions at $\\sqrt{s}=13$ TeV.","The data sample corresponds to 137 fb$^{-1}$ of integrated luminosity and consists of dimuon and dielectron events, including the forward electrons reconstructed beyond the coverage of the CMS tracking detectors.","Using the CT18Z set of the parton distribution functions (PDF), which provides the best coverage of the central values extracted with the other global PDFs, we obtain $$\\sin^2\\theta^\\ell_\\mathrm{eff} = 0.23157 \\pm 0.00031,$$ where the total uncertainty includes the statistical uncetainties (0.00010), as well as the correlated experimental (0.00015), theoretical (0.00009), and PDF (0.00027) systematic uncertainties.","The measured value agrees well with the Standard Model prediction.","This is the most precise measurement at a hadron collider with comparable precision to the two most precise results obtained at LEP and SLD."],"url":"http://arxiv.org/abs/2405.11484v1","category":"hep-ex"}
{"created":"2024-05-19 08:42:16","title":"Explainable Facial Expression Recognition for People with Intellectual Disabilities","abstract":"Facial expression recognition plays an important role in human behaviour, communication, and interaction. Recent neural networks have demonstrated to perform well at its automatic recognition, with different explainability techniques available to make them more transparent. In this work, we propose a facial expression recognition study for people with intellectual disabilities that would be integrated into a social robot. We train two well-known neural networks with five databases of facial expressions and test them with two databases containing people with and without intellectual disabilities. Finally, we study in which regions the models focus to perceive a particular expression using two different explainability techniques: LIME and RISE, assessing the differences when used on images containing disabled and non-disabled people.","sentences":["Facial expression recognition plays an important role in human behaviour, communication, and interaction.","Recent neural networks have demonstrated to perform well at its automatic recognition, with different explainability techniques available to make them more transparent.","In this work, we propose a facial expression recognition study for people with intellectual disabilities that would be integrated into a social robot.","We train two well-known neural networks with five databases of facial expressions and test them with two databases containing people with and without intellectual disabilities.","Finally, we study in which regions the models focus to perceive a particular expression using two different explainability techniques: LIME and RISE, assessing the differences when used on images containing disabled and non-disabled people."],"url":"http://arxiv.org/abs/2405.11482v1","category":"cs.HC"}
{"created":"2024-05-19 08:03:13","title":"Analyze Additive and Interaction Effects via Collaborative Trees","abstract":"We present Collaborative Trees, a novel tree model designed for regression prediction, along with its bagging version, which aims to analyze complex statistical associations between features and uncover potential patterns inherent in the data. We decompose the mean decrease in impurity from the proposed tree model to analyze the additive and interaction effects of features on the response variable. Additionally, we introduce network diagrams to visually depict how each feature contributes additively to the response and how pairs of features contribute interaction effects. Through a detailed demonstration using an embryo growth dataset, we illustrate how the new statistical tools aid data analysis, both visually and numerically. Moreover, we delve into critical aspects of tree modeling, such as prediction performance, inference stability, and bias in feature importance measures, leveraging real datasets and simulation experiments for comprehensive discussions. On the theory side, we show that Collaborative Trees, built upon a ``sum of trees'' approach with our own innovative tree model regularization, exhibit characteristics akin to matching pursuit, under the assumption of high-dimensional independent binary input features (or one-hot feature groups). This newfound link sheds light on the superior capability of our tree model in estimating additive effects of features, a crucial factor for accurate interaction effect estimation.","sentences":["We present Collaborative Trees, a novel tree model designed for regression prediction, along with its bagging version, which aims to analyze complex statistical associations between features and uncover potential patterns inherent in the data.","We decompose the mean decrease in impurity from the proposed tree model to analyze the additive and interaction effects of features on the response variable.","Additionally, we introduce network diagrams to visually depict how each feature contributes additively to the response and how pairs of features contribute interaction effects.","Through a detailed demonstration using an embryo growth dataset, we illustrate how the new statistical tools aid data analysis, both visually and numerically.","Moreover, we delve into critical aspects of tree modeling, such as prediction performance, inference stability, and bias in feature importance measures, leveraging real datasets and simulation experiments for comprehensive discussions.","On the theory side, we show that Collaborative Trees, built upon a ``sum of trees'' approach with our own innovative tree model regularization, exhibit characteristics akin to matching pursuit, under the assumption of high-dimensional independent binary input features (or one-hot feature groups).","This newfound link sheds light on the superior capability of our tree model in estimating additive effects of features, a crucial factor for accurate interaction effect estimation."],"url":"http://arxiv.org/abs/2405.11477v1","category":"stat.ME"}
{"created":"2024-05-19 07:42:10","title":"CMA-ES with Adaptive Reevaluation for Multiplicative Noise","abstract":"The covariance matrix adaptation evolution strategy (CMA-ES) is a powerful optimization method for continuous black-box optimization problems. Several noise-handling methods have been proposed to bring out the optimization performance of the CMA-ES on noisy objective functions. The adaptations of the population size and the learning rate are two major approaches that perform well under additive Gaussian noise. The reevaluation technique is another technique that evaluates each solution multiple times. In this paper, we discuss the difference between those methods from the perspective of stochastic relaxation that considers the maximization of the expected utility function. We derive that the set of maximizers of the noise-independent utility, which is used in the reevaluation technique, certainly contains the optimal solution, while the noise-dependent utility, which is used in the population size and leaning rate adaptations, does not satisfy it under multiplicative noise. Based on the discussion, we develop the reevaluation adaptation CMA-ES (RA-CMA-ES), which computes two update directions using half of the evaluations and adapts the number of reevaluations based on the estimated correlation of those two update directions. The numerical simulation shows that the RA-CMA-ES outperforms the comparative method under multiplicative noise, maintaining competitive performance under additive noise.","sentences":["The covariance matrix adaptation evolution strategy (CMA-ES) is a powerful optimization method for continuous black-box optimization problems.","Several noise-handling methods have been proposed to bring out the optimization performance of the CMA-ES on noisy objective functions.","The adaptations of the population size and the learning rate are two major approaches that perform well under additive Gaussian noise.","The reevaluation technique is another technique that evaluates each solution multiple times.","In this paper, we discuss the difference between those methods from the perspective of stochastic relaxation that considers the maximization of the expected utility function.","We derive that the set of maximizers of the noise-independent utility, which is used in the reevaluation technique, certainly contains the optimal solution, while the noise-dependent utility, which is used in the population size and leaning rate adaptations, does not satisfy it under multiplicative noise.","Based on the discussion, we develop the reevaluation adaptation CMA-ES (RA-CMA-ES), which computes two update directions using half of the evaluations and adapts the number of reevaluations based on the estimated correlation of those two update directions.","The numerical simulation shows that the RA-CMA-ES outperforms the comparative method under multiplicative noise, maintaining competitive performance under additive noise."],"url":"http://arxiv.org/abs/2405.11471v1","category":"cs.NE"}
{"created":"2024-05-19 05:14:36","title":"Breuer-Major Theorems for Hilbert Space-Valued Random Variables","abstract":"Let $\\{X_k\\}_{k \\in \\mathbb{Z}}$ be a stationary Gaussian process with values in a separable Hilbert space $\\mathcal{H}_1$, and let $G:\\mathcal{H}_1 \\to \\mathcal{H}_2$ be an operator acting on $X_k$. Under suitable conditions on the operator $G$ and the temporal and cross-sectional correlations of $\\{X_k\\}_{k \\in \\mathbb{Z}}$, we derive a central limit theorem (CLT) for the normalized partial sums of $\\{G[X_k]\\}_{k \\in \\mathbb{Z}}$. To prove a CLT for the Hilbert space-valued process $\\{G[X_k]\\}_{k \\in \\mathbb{Z}}$, we employ techniques from the recently developed infinite dimensional Malliavin-Stein framework. In addition, we provide quantitative and continuous time versions of the derived CLT. In a series of examples, we recover and strengthen limit theorems for a wide array of statistics relevant in functional data analysis, and present a novel limit theorem in the framework of neural operators as an application of our result.","sentences":["Let $\\{X_k\\}_{k \\in \\mathbb{Z}}$ be a stationary Gaussian process with values in a separable Hilbert space $\\mathcal{H}_1$, and let $G:\\mathcal{H}_1 \\to \\mathcal{H}_2$ be an operator acting on $X_k$. Under suitable conditions on the operator $G$ and the temporal and cross-sectional correlations of $\\{X_k\\}_{k \\in \\mathbb{Z}}$, we derive a central limit theorem (CLT) for the normalized partial sums of $\\{G[X_k]\\}_{k \\in \\mathbb{Z}}$. To prove a CLT for the Hilbert space-valued process $\\{G[X_k]\\}_{k \\in \\mathbb{Z}}$, we employ techniques from the recently developed infinite dimensional Malliavin-Stein framework.","In addition, we provide quantitative and continuous time versions of the derived CLT.","In a series of examples, we recover and strengthen limit theorems for a wide array of statistics relevant in functional data analysis, and present a novel limit theorem in the framework of neural operators as an application of our result."],"url":"http://arxiv.org/abs/2405.11452v1","category":"math.PR"}
{"created":"2024-05-19 04:55:39","title":"Disturbance Evaluation Circuit in Quantum Measurement","abstract":"According to the uncertainty principle, every quantum measurement accompanies disturbance. In particular, accurate sequential measurements need the accurate control of disturbance. However, the correct role of disturbance in the uncertainty principle has been known only recently. Understanding the disturbance is crucial for understanding the fundamentals of physics, and accurately evaluating the disturbance is important for quantum technologies such as quantum information processing and quantum metrology. Therefore, the experimental evaluation of the disturbance is a significant challenge in those fields. In this study, we propose a novel evaluation method for the quantum root-mean-square (QRMS) disturbance and compare its performance with the existing approaches, known as the three-state method (TSM) and the weak measurement method (WMM). Our method establishes a correspondence between the QRMS disturbance of the measurement and the second-order derivative of the decoherence induced in a newly introduced weak probe system with respect to the coupling strength of the weak interaction at its zero-limit. Furthermore, we demonstrate the effectiveness of our method in comparison with the other two through a simulation and experiment using a quantum computer. The results capture the key features of the TSM, WMM, and our method, providing insights into the strengths and limitations of these methods.","sentences":["According to the uncertainty principle, every quantum measurement accompanies disturbance.","In particular, accurate sequential measurements need the accurate control of disturbance.","However, the correct role of disturbance in the uncertainty principle has been known only recently.","Understanding the disturbance is crucial for understanding the fundamentals of physics, and accurately evaluating the disturbance is important for quantum technologies such as quantum information processing and quantum metrology.","Therefore, the experimental evaluation of the disturbance is a significant challenge in those fields.","In this study, we propose a novel evaluation method for the quantum root-mean-square (QRMS) disturbance and compare its performance with the existing approaches, known as the three-state method (TSM) and the weak measurement method (WMM).","Our method establishes a correspondence between the QRMS disturbance of the measurement and the second-order derivative of the decoherence induced in a newly introduced weak probe system with respect to the coupling strength of the weak interaction at its zero-limit.","Furthermore, we demonstrate the effectiveness of our method in comparison with the other two through a simulation and experiment using a quantum computer.","The results capture the key features of the TSM, WMM, and our method, providing insights into the strengths and limitations of these methods."],"url":"http://arxiv.org/abs/2405.11447v1","category":"quant-ph"}
{"created":"2024-05-19 04:43:54","title":"Adaptive Optimal Market Making Strategies with Inventory Liquidation Cos","abstract":"A novel high-frequency market-making approach in discrete time is proposed that admits closed-form solutions. By taking advantage of demand functions that are linear in the quoted bid and ask spreads with random coefficients, we model the variability of the partial filling of limit orders posted in a limit order book (LOB). As a result, we uncover new patterns as to how the demand's randomness affects the optimal placement strategy. We also allow the price process to follow general dynamics without any Brownian or martingale assumption as is commonly adopted in the literature. The most important feature of our optimal placement strategy is that it can react or adapt to the behavior of market orders online. Using LOB data, we train our model and reproduce the anticipated final profit and loss of the optimal strategy on a given testing date using the actual flow of orders in the LOB. Our adaptive optimal strategies outperform the non-adaptive strategy and those that quote limit orders at a fixed distance from the midprice.","sentences":["A novel high-frequency market-making approach in discrete time is proposed that admits closed-form solutions.","By taking advantage of demand functions that are linear in the quoted bid and ask spreads with random coefficients, we model the variability of the partial filling of limit orders posted in a limit order book (LOB).","As a result, we uncover new patterns as to how the demand's randomness affects the optimal placement strategy.","We also allow the price process to follow general dynamics without any Brownian or martingale assumption as is commonly adopted in the literature.","The most important feature of our optimal placement strategy is that it can react or adapt to the behavior of market orders online.","Using LOB data, we train our model and reproduce the anticipated final profit and loss of the optimal strategy on a given testing date using the actual flow of orders in the LOB.","Our adaptive optimal strategies outperform the non-adaptive strategy and those that quote limit orders at a fixed distance from the midprice."],"url":"http://arxiv.org/abs/2405.11444v1","category":"q-fin.TR"}
{"created":"2024-05-19 04:31:54","title":"EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations","abstract":"Content-based recommendation systems play a crucial role in delivering personalized content to users in the digital world. In this work, we introduce EmbSum, a novel framework that enables offline pre-computations of users and candidate items while capturing the interactions within the user engagement history. By utilizing the pretrained encoder-decoder model and poly-attention layers, EmbSum derives User Poly-Embedding (UPE) and Content Poly-Embedding (CPE) to calculate relevance scores between users and candidate items. EmbSum actively learns the long user engagement histories by generating user-interest summary with supervision from large language model (LLM). The effectiveness of EmbSum is validated on two datasets from different domains, surpassing state-of-the-art (SoTA) methods with higher accuracy and fewer parameters. Additionally, the model's ability to generate summaries of user interests serves as a valuable by-product, enhancing its usefulness for personalized content recommendations.","sentences":["Content-based recommendation systems play a crucial role in delivering personalized content to users in the digital world.","In this work, we introduce EmbSum, a novel framework that enables offline pre-computations of users and candidate items while capturing the interactions within the user engagement history.","By utilizing the pretrained encoder-decoder model and poly-attention layers, EmbSum derives User Poly-Embedding (UPE) and Content Poly-Embedding (CPE) to calculate relevance scores between users and candidate items.","EmbSum actively learns the long user engagement histories by generating user-interest summary with supervision from large language model (LLM).","The effectiveness of EmbSum is validated on two datasets from different domains, surpassing state-of-the-art (SoTA) methods with higher accuracy and fewer parameters.","Additionally, the model's ability to generate summaries of user interests serves as a valuable by-product, enhancing its usefulness for personalized content recommendations."],"url":"http://arxiv.org/abs/2405.11441v1","category":"cs.IR"}
{"created":"2024-05-19 03:55:06","title":"Superconductivity in Dilute Hydrides of Ammonia under Pressure","abstract":"In the last decade, there has been great progress in predicting and synthesizing polyhydrides that exhibit superconductivity when squeezed. Dopants allow these compounds to become metals at pressures lower than those required to metallize elemental hydrogen. Here, we show that by combining the fundamental planetary building blocks of molecular hydrogen and ammonia, conventional superconducting compounds can be formed at high pressure. Through extensive theoretical calculations we predict metallic metastable structures with NH$_n$ ($n=10,11,24$) stoichiometries that are based on NH$_4^+$ superalkali cations and complex hydrogenic lattices. The hydrogen atoms in the molecular cation contribute to the superconducting mechanism, and the estimated superconducting critical temperatures, $T_\\text{c}$s, are comparable to the highest values computed for the alkali metal polyhydrides. The largest calculated (isotropic Eliashberg) $T_\\text{c}$ is 179~K for $Pnma$-NH$_{10}$ at 300~GPa. Our results suggest that other molecular cations can be mixed with hydrogen under pressure yielding superconducting compounds.","sentences":["In the last decade, there has been great progress in predicting and synthesizing polyhydrides that exhibit superconductivity when squeezed.","Dopants allow these compounds to become metals at pressures lower than those required to metallize elemental hydrogen.","Here, we show that by combining the fundamental planetary building blocks of molecular hydrogen and ammonia, conventional superconducting compounds can be formed at high pressure.","Through extensive theoretical calculations we predict metallic metastable structures with NH$_n$ ($n=10,11,24$) stoichiometries that are based on NH$_4^+$ superalkali cations and complex hydrogenic lattices.","The hydrogen atoms in the molecular cation contribute to the superconducting mechanism, and the estimated superconducting critical temperatures, $T_\\text{c}$s, are comparable to the highest values computed for the alkali metal polyhydrides.","The largest calculated (isotropic Eliashberg) $T_\\text{c}$ is 179~K for $Pnma$-NH$_{10}$ at 300~GPa.","Our results suggest that other molecular cations can be mixed with hydrogen under pressure yielding superconducting compounds."],"url":"http://arxiv.org/abs/2405.11438v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-19 02:52:17","title":"Ground state configurations of systems of interacting particles in one dimension under the potential $\u03c6(x)=\\left(1+x^4\\right)^{-1}$","abstract":"We build on a 1985 paper by Nijboer and Ruijgrok, and prove results about ground state configurations under the potential $\\phi(x)=\\left(1+x^4\\right)^{-1}$ at high densities (mean number of particles per unit volume) in one dimension. In particular, we show that for densities $\\rho=\\frac{n}{\\sqrt{2}}$ with $n \\in \\mathbb{N}$, the configuration that places $n$ particles at each point of $\\sqrt{2}\\mathbb{Z}$ minimizes the average potential energy per particle.","sentences":["We build on a 1985 paper by Nijboer and Ruijgrok, and prove results about ground state configurations under the potential $\\phi(x)=\\left(1+x^4\\right)^{-1}$ at high densities (mean number of particles per unit volume) in one dimension.","In particular, we show that for densities $\\rho=\\frac{n}{\\sqrt{2}}$ with $n \\in \\mathbb{N}$, the configuration that places $n$ particles at each point of $\\sqrt{2}\\mathbb{Z}$ minimizes the average potential energy per particle."],"url":"http://arxiv.org/abs/2405.11428v1","category":"math-ph"}
{"created":"2024-05-19 02:09:50","title":"Metric Dimension and Resolvability of Jaccard Spaces","abstract":"A subset of points in a metric space is said to resolve it if each point in the space is uniquely characterized by its distance to each point in the subset. In particular, resolving sets can be used to represent points in abstract metric spaces as Euclidean vectors. Importantly, due to the triangle inequality, points close by in the space are represented as vectors with similar coordinates, which may find applications in classification problems of symbolic objects under suitably chosen metrics. In this manuscript, we address the resolvability of Jaccard spaces, i.e., metric spaces of the form $(2^X,\\text{Jac})$, where $2^X$ is the power set of a finite set $X$, and $\\text{Jac}$ is the Jaccard distance between subsets of $X$. Specifically, for different $a,b\\in 2^X$, $\\text{Jac}(a,b)=\\frac{|a\\Delta b|}{|a\\cup b|}$, where $|\\cdot|$ denotes size (i.e., cardinality) and $\\Delta$ denotes the symmetric difference of sets. We combine probabilistic and linear algebra arguments to construct highly likely but nearly optimal (i.e., of minimal size) resolving sets of $(2^X,\\text{Jac})$. In particular, we show that the metric dimension of $(2^X,\\text{Jac})$, i.e., the minimum size of a resolving set of this space, is $\\Theta(|X|/\\ln|X|)$.","sentences":["A subset of points in a metric space is said to resolve it if each point in the space is uniquely characterized by its distance to each point in the subset.","In particular, resolving sets can be used to represent points in abstract metric spaces as Euclidean vectors.","Importantly, due to the triangle inequality, points close by in the space are represented as vectors with similar coordinates, which may find applications in classification problems of symbolic objects under suitably chosen metrics.","In this manuscript, we address the resolvability of Jaccard spaces, i.e., metric spaces of the form $(2^X,\\text{Jac})$, where $2^X$ is the power set of a finite set $X$, and $\\text{Jac}$ is the Jaccard distance between subsets of $X$. Specifically, for different $a,b\\in 2^X$, $\\text{Jac}(a,b)=\\frac{|a\\Delta b|}{|a\\cup b|}$, where $|\\cdot|$ denotes size (i.e., cardinality) and $\\Delta$ denotes the symmetric difference of sets.","We combine probabilistic and linear algebra arguments to construct highly likely but nearly optimal (i.e., of minimal size) resolving sets of $(2^X,\\text{Jac})$. In particular, we show that the metric dimension of $(2^X,\\text{Jac})$, i.e., the minimum size of a resolving set of this space, is $\\Theta(|X|/\\ln|X|)$."],"url":"http://arxiv.org/abs/2405.11424v1","category":"cs.DM"}
{"created":"2024-05-19 02:05:29","title":"Bioverse: GMT and ELT Direct Imaging and High-Resolution Spectroscopy Assessment $\\unicode{x2013}$ Surveying Exo-Earth O$_{\\mathrm{2}}$ and Testing the Habitable Zone Oxygen Hypothesis","abstract":"Biosignature detection in the atmospheres of Earth-like exoplanets is one of the most significant and ambitious goals for astronomy, astrobiology, and humanity. Molecular oxygen is among the strongest indicators of life on Earth, but it will be extremely difficult to detect via transmission spectroscopy. We used the Bioverse statistical framework to assess the ability to probe Earth-like O$_{\\mathrm{2}}$ levels on hypothetical nearby habitable zone exoplanets (EECs) using direct imaging and high-resolution spectroscopy on the Giant Magellan Telescope (GMT) and the Extremely Large Telescope (ELT). We found that O$_{\\mathrm{2}}$ could be probed on up to $\\sim$5 and $\\sim$15 EECs orbiting bright M dwarfs within 20 pc in a 10-year survey on the GMT and ELT, respectively. Earth-like O$_{\\mathrm{2}}$ levels could be probed on four known super-Earth candidates, including Proxima Centauri b, within about one week on the ELT and a few months on the GMT. We also assessed the ability of the ELT to test the habitable zone oxygen hypothesis $\\unicode{x2013}$ that habitable zone Earth-sized planets are more likely to have O$_{\\mathrm{2}}$ $\\unicode{x2013}$ within a 10-year survey using Bioverse. Testing this hypothesis requires either $\\sim$1/2 of the EECs to have O$_{\\mathrm{2}}$ or $\\sim$1/3 if $\\eta_{\\oplus}$ is large. A northern hemisphere large-aperture telescope, such as the Thirty Meter Telescope (TMT), would expand the target star pool by about 25%, reduce the time to probe biosignatures on individual targets, and provide an additional independent check on potential biosignature detections.","sentences":["Biosignature detection in the atmospheres of Earth-like exoplanets is one of the most significant and ambitious goals for astronomy, astrobiology, and humanity.","Molecular oxygen is among the strongest indicators of life on Earth, but it will be extremely difficult to detect via transmission spectroscopy.","We used the Bioverse statistical framework to assess the ability to probe Earth-like O$_{\\mathrm{2}}$ levels on hypothetical nearby habitable zone exoplanets (EECs) using direct imaging and high-resolution spectroscopy on the Giant Magellan Telescope (GMT) and the Extremely Large Telescope (ELT).","We found that O$_{\\mathrm{2}}$ could be probed on up to $\\sim$5 and $\\sim$15 EECs orbiting bright M dwarfs within 20 pc in a 10-year survey on the GMT and ELT, respectively.","Earth-like O$_{\\mathrm{2}}$ levels could be probed on four known super-Earth candidates, including Proxima Centauri b, within about one week on the ELT and a few months on the GMT.","We also assessed the ability of the ELT to test the habitable zone oxygen hypothesis $\\unicode{x2013}$ that habitable zone Earth-sized planets are more likely to have O$_{\\mathrm{2}}$ $\\unicode{x2013}$ within a 10-year survey using Bioverse.","Testing this hypothesis requires either $\\sim$1/2 of the EECs to have O$_{\\mathrm{2}}$ or $\\sim$1/3 if $\\eta_{\\oplus}$ is large.","A northern hemisphere large-aperture telescope, such as the Thirty Meter Telescope (TMT), would expand the target star pool by about 25%, reduce the time to probe biosignatures on individual targets, and provide an additional independent check on potential biosignature detections."],"url":"http://arxiv.org/abs/2405.11423v1","category":"astro-ph.EP"}
{"created":"2024-05-19 00:19:59","title":"Budgeted Recommendation with Delayed Feedback","abstract":"In a conventional contextual multi-armed bandit problem, the feedback (or reward) is immediately observable after an action. Nevertheless, delayed feedback arises in numerous real-life situations and is particularly crucial in time-sensitive applications. The exploration-exploitation dilemma becomes particularly challenging under such conditions, as it couples with the interplay between delays and limited resources. Besides, a limited budget often aggravates the problem by restricting the exploration potential. A motivating example is the distribution of medical supplies at the early stage of COVID-19. The delayed feedback of testing results, thus insufficient information for learning, degraded the efficiency of resource allocation. Motivated by such applications, we study the effect of delayed feedback on constrained contextual bandits. We develop a decision-making policy, delay-oriented resource allocation with learning (DORAL), to optimize the resource expenditure in a contextual multi-armed bandit problem with arm-dependent delayed feedback.","sentences":["In a conventional contextual multi-armed bandit problem, the feedback (or reward) is immediately observable after an action.","Nevertheless, delayed feedback arises in numerous real-life situations and is particularly crucial in time-sensitive applications.","The exploration-exploitation dilemma becomes particularly challenging under such conditions, as it couples with the interplay between delays and limited resources.","Besides, a limited budget often aggravates the problem by restricting the exploration potential.","A motivating example is the distribution of medical supplies at the early stage of COVID-19.","The delayed feedback of testing results, thus insufficient information for learning, degraded the efficiency of resource allocation.","Motivated by such applications, we study the effect of delayed feedback on constrained contextual bandits.","We develop a decision-making policy, delay-oriented resource allocation with learning (DORAL), to optimize the resource expenditure in a contextual multi-armed bandit problem with arm-dependent delayed feedback."],"url":"http://arxiv.org/abs/2405.11417v1","category":"cs.LG"}
{"created":"2024-05-18 23:03:22","title":"Characterizing the Complexity of Social Robot Navigation Scenarios","abstract":"Social robot navigation algorithms are often demonstrated in overly simplified scenarios, prohibiting the extraction of practical insights about their relevance to real world domains. Our key insight is that an understanding of the inherent complexity of a social robot navigation scenario could help characterize the limitations of existing navigation algorithms and provide actionable directions for improvement. Through an exploration of recent literature, we identify a series of factors contributing to the complexity of a scenario, disambiguating between contextual and robot-related ones. We then conduct a simulation study investigating how manipulations of contextual factors impact the performance of a variety of navigation algorithms. We find that dense and narrow environments correlate most strongly with performance drops, while the heterogeneity of agent policies and directionality of interactions have a less pronounced effect. This motivates a shift towards developing and testing algorithms under higher-complexity settings.","sentences":["Social robot navigation algorithms are often demonstrated in overly simplified scenarios, prohibiting the extraction of practical insights about their relevance to real world domains.","Our key insight is that an understanding of the inherent complexity of a social robot navigation scenario could help characterize the limitations of existing navigation algorithms and provide actionable directions for improvement.","Through an exploration of recent literature, we identify a series of factors contributing to the complexity of a scenario, disambiguating between contextual and robot-related ones.","We then conduct a simulation study investigating how manipulations of contextual factors impact the performance of a variety of navigation algorithms.","We find that dense and narrow environments correlate most strongly with performance drops, while the heterogeneity of agent policies and directionality of interactions have a less pronounced effect.","This motivates a shift towards developing and testing algorithms under higher-complexity settings."],"url":"http://arxiv.org/abs/2405.11410v1","category":"cs.RO"}
{"created":"2024-05-18 21:58:40","title":"On the Convergence of Interior-Point Methods for Bound-Constrained Nonlinear Optimization Problems with Noise","abstract":"We analyze the convergence properties of a modified barrier method for solving bound-constrained optimization problems where evaluations of the objective function and its derivatives are affected by bounded and non-diminishing noise. The only modification compared to a standard barrier method is a relaxation of the Armijo line-search condition. We prove that the algorithm generates iterates at which the size of the barrier function gradient eventually falls below a threshold that converges to zero if the noise level converges to zero. Based on this result, we propose a practical stopping test that does not require estimates of unknown problem parameters and identifies iterations in which the theoretical threshold is reached. We also analyze the local convergence properties of the method when noisy second derivatives are used. Under a strict-complementarity assumption, we show that iterates stay in a neighborhood around the optimal solution once it is entered. The neighborhood is defined in a scaled norm that becomes narrower for variables with active bound constraints as the barrier parameter is decreased. As a consequence, we show that active bound constraints can be identified despite noise. Numerical results demonstrate the effectiveness of the stopping test and illustrate the active-set identification properties of the method.","sentences":["We analyze the convergence properties of a modified barrier method for solving bound-constrained optimization problems where evaluations of the objective function and its derivatives are affected by bounded and non-diminishing noise.","The only modification compared to a standard barrier method is a relaxation of the Armijo line-search condition.","We prove that the algorithm generates iterates at which the size of the barrier function gradient eventually falls below a threshold that converges to zero if the noise level converges to zero.","Based on this result, we propose a practical stopping test that does not require estimates of unknown problem parameters and identifies iterations in which the theoretical threshold is reached.","We also analyze the local convergence properties of the method when noisy second derivatives are used.","Under a strict-complementarity assumption, we show that iterates stay in a neighborhood around the optimal solution once it is entered.","The neighborhood is defined in a scaled norm that becomes narrower for variables with active bound constraints as the barrier parameter is decreased.","As a consequence, we show that active bound constraints can be identified despite noise.","Numerical results demonstrate the effectiveness of the stopping test and illustrate the active-set identification properties of the method."],"url":"http://arxiv.org/abs/2405.11400v1","category":"math.OC"}
{"created":"2024-05-18 21:00:07","title":"Optical materials discovery and design with federated databases and machine learning","abstract":"Combinatorial and guided screening of materials space with density-functional theory and related approaches has provided a wealth of hypothetical inorganic materials, which are increasingly tabulated in open databases. The OPTIMADE API is a standardised format for representing crystal structures, their measured and computed properties, and the methods for querying and filtering them from remote resources. Currently, the OPTIMADE federation spans over 20 data providers, rendering over 30 million structures accessible in this way, many of which are novel and have only recently been suggested by machine learning-based approaches. In this work, we outline our approach to non-exhaustively screen this dynamic trove of structures for the next-generation of optical materials. By applying MODNet, a neural network-based model for property prediction, within a combined active learning and high-throughput computation framework, we isolate particular structures and chemistries that should be most fruitful for further theoretical calculations and for experimental study as high-refractive-index materials. By making explicit use of automated calculations, federated dataset curation and machine learning, and by releasing these publicly, the workflows presented here can be periodically re-assessed as new databases implement OPTIMADE, and new hypothetical materials are suggested.","sentences":["Combinatorial and guided screening of materials space with density-functional theory and related approaches has provided a wealth of hypothetical inorganic materials, which are increasingly tabulated in open databases.","The OPTIMADE API is a standardised format for representing crystal structures, their measured and computed properties, and the methods for querying and filtering them from remote resources.","Currently, the OPTIMADE federation spans over 20 data providers, rendering over 30 million structures accessible in this way, many of which are novel and have only recently been suggested by machine learning-based approaches.","In this work, we outline our approach to non-exhaustively screen this dynamic trove of structures for the next-generation of optical materials.","By applying MODNet, a neural network-based model for property prediction, within a combined active learning and high-throughput computation framework, we isolate particular structures and chemistries that should be most fruitful for further theoretical calculations and for experimental study as high-refractive-index materials.","By making explicit use of automated calculations, federated dataset curation and machine learning, and by releasing these publicly, the workflows presented here can be periodically re-assessed as new databases implement OPTIMADE, and new hypothetical materials are suggested."],"url":"http://arxiv.org/abs/2405.11393v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-18 20:22:22","title":"Liver Fat Quantification Network with Body Shape","abstract":"It is clinically important to detect liver fat content as it is related to cardiac complications and cardiovascular disease mortality. However, existing methods are associated with high cost and/or medical complications (e.g., liver biopsy, medical imaging technology) or only roughly estimate the grades of steatosis. In this paper, we propose a deep neural network to accurately estimate liver fat percentage using only body shapes. The proposed framework is composed of a flexible baseline regression network and a lightweight attention module. The attention module is trained to generate discriminative and diverse features, thus significantly improving performance. To validate our proposed method, we perform extensive tests on medical datasets. The experimental results validate our method and prove the efficacy of designing neural networks to predict liver fat using only body shape. Since body shapes can be acquired using inexpensive and readily available optical scanners, the proposed method promised to make accurate assessment of hepatic steatosis more accessible.","sentences":["It is clinically important to detect liver fat content as it is related to cardiac complications and cardiovascular disease mortality.","However, existing methods are associated with high cost and/or medical complications (e.g., liver biopsy, medical imaging technology) or only roughly estimate the grades of steatosis.","In this paper, we propose a deep neural network to accurately estimate liver fat percentage using only body shapes.","The proposed framework is composed of a flexible baseline regression network and a lightweight attention module.","The attention module is trained to generate discriminative and diverse features, thus significantly improving performance.","To validate our proposed method, we perform extensive tests on medical datasets.","The experimental results validate our method and prove the efficacy of designing neural networks to predict liver fat using only body shape.","Since body shapes can be acquired using inexpensive and readily available optical scanners, the proposed method promised to make accurate assessment of hepatic steatosis more accessible."],"url":"http://arxiv.org/abs/2405.11386v1","category":"eess.IV"}
{"created":"2024-05-18 20:12:46","title":"Uniform Ergodicity of Parallel Tempering With Efficient Local Exploration","abstract":"Non-reversible parallel tempering (NRPT) is an effective algorithm for sampling from target distributions with complex geometry, such as those arising from posterior distributions of weakly identifiable and high-dimensional Bayesian models. In this work we establish the uniform (geometric) ergodicity of NRPT under a model of efficient local exploration. The uniform ergodicity log rates are inversely proportional to an easily-estimable divergence, the global communication barrier (GCB), which was recently introduced in the literature. We obtain analogous ergodicity results for classical reversible parallel tempering, providing new evidence that NRPT dominates its reversible counterpart. Our results are based on an analysis of the hitting time of a continuous-time persistent random walk, which is also of independent interest. The rates that we obtain reflect real experiments well for distributions where global exploration is not possible without parallel tempering.","sentences":["Non-reversible parallel tempering (NRPT) is an effective algorithm for sampling from target distributions with complex geometry, such as those arising from posterior distributions of weakly identifiable and high-dimensional Bayesian models.","In this work we establish the uniform (geometric) ergodicity of NRPT under a model of efficient local exploration.","The uniform ergodicity log rates are inversely proportional to an easily-estimable divergence, the global communication barrier (GCB), which was recently introduced in the literature.","We obtain analogous ergodicity results for classical reversible parallel tempering, providing new evidence that NRPT dominates its reversible counterpart.","Our results are based on an analysis of the hitting time of a continuous-time persistent random walk, which is also of independent interest.","The rates that we obtain reflect real experiments well for distributions where global exploration is not possible without parallel tempering."],"url":"http://arxiv.org/abs/2405.11384v1","category":"stat.CO"}
{"created":"2024-05-18 19:26:37","title":"Symmetrically Threaded SQUIDs As Next Generation Kerr-cat Qubits","abstract":"Kerr-cat qubits are bosonic qubits with autonomous protection against bit-flips. They have been studied widely using driven Superconducting Nonlinear Asymmetric Inductive eLement (SNAIL) oscillators. We theoretically investigate an alternate circuit for the Kerr-cat qubit, namely Symmetrically Threaded SQUIDs (STS). We perform the circuit analysis and derive the Gorini-Kossakowski-Sudarshan-Lindblad (GKLS) master equation for the Kerr-cat qubit attached to a thermal environment. We find that the lifetime time of the coherent states ($T_\\alpha$) of the Kerr-cat qubit is the same in both the STS and SNAIL circuits for weak Kerr nonlinearity. However, the STS Kerr-cat qubits have the additional benefit of being resistant against higher order photon dissipation effects, resulting in significantly longer $T_\\alpha$ even with stronger Kerr nonlinearity on the order of $10{~\\rm MHz}$. We also examine the effects of strong flux driving and asymmetric Josephson junctions on $T_\\alpha$. Unlike the SNAIL design, we find a dip in $T_\\alpha$ of the STS Kerr-cat qubit for weak two-photon drive. However, we show that the dip can be mitigated by applying a suitable drive-dependent detuning. With the proposed design and considering a cat size of 10 photons, we predict $T_\\alpha$ of the order of tens of milliseconds even in the presence of multi-photon heating and dephasing effects. The robustness of the STS Kerr-cat qubit makes it a promising component for fault-tolerant quantum processors.","sentences":["Kerr-cat qubits are bosonic qubits with autonomous protection against bit-flips.","They have been studied widely using driven Superconducting Nonlinear Asymmetric Inductive eLement (SNAIL) oscillators.","We theoretically investigate an alternate circuit for the Kerr-cat qubit, namely Symmetrically Threaded SQUIDs (STS).","We perform the circuit analysis and derive the Gorini-Kossakowski-Sudarshan-Lindblad (GKLS) master equation for the Kerr-cat qubit attached to a thermal environment.","We find that the lifetime time of the coherent states ($T_\\alpha$) of the Kerr-cat qubit is the same in both the STS and SNAIL circuits for weak Kerr nonlinearity.","However, the STS Kerr-cat qubits have the additional benefit of being resistant against higher order photon dissipation effects, resulting in significantly longer $T_\\alpha$ even with stronger Kerr nonlinearity on the order of $10{~\\rm MHz}$. We also examine the effects of strong flux driving and asymmetric Josephson junctions on $T_\\alpha$. Unlike the SNAIL design, we find a dip in $T_\\alpha$ of the STS Kerr-cat qubit for weak two-photon drive.","However, we show that the dip can be mitigated by applying a suitable drive-dependent detuning.","With the proposed design and considering a cat size of 10 photons, we predict $T_\\alpha$ of the order of tens of milliseconds even in the presence of multi-photon heating and dephasing effects.","The robustness of the STS Kerr-cat qubit makes it a promising component for fault-tolerant quantum processors."],"url":"http://arxiv.org/abs/2405.11375v1","category":"quant-ph"}
{"created":"2024-05-18 19:13:49","title":"ReModels: Quantile Regression Averaging models","abstract":"Electricity price forecasts play a crucial role in making key business decisions within the electricity markets. A focal point in this domain are probabilistic predictions, which delineate future price values in a more comprehensive manner than simple point forecasts. The golden standard in probabilistic approaches to predict energy prices is the Quantile Regression Averaging (QRA) method. In this paper, we present a Python package that encompasses the implementation of QRA, along with modifications of this approach that have appeared in the literature over the past few years. The proposed package also facilitates the acquisition and preparation of data related to electricity markets, as well as the evaluation of model predictions.","sentences":["Electricity price forecasts play a crucial role in making key business decisions within the electricity markets.","A focal point in this domain are probabilistic predictions, which delineate future price values in a more comprehensive manner than simple point forecasts.","The golden standard in probabilistic approaches to predict energy prices is the Quantile Regression Averaging (QRA) method.","In this paper, we present a Python package that encompasses the implementation of QRA, along with modifications of this approach that have appeared in the literature over the past few years.","The proposed package also facilitates the acquisition and preparation of data related to electricity markets, as well as the evaluation of model predictions."],"url":"http://arxiv.org/abs/2405.11372v1","category":"cs.LG"}
{"created":"2024-05-20 14:05:35","title":"Adaptive Extraction Network for Multivariate Long Sequence Time-Series Forecasting","abstract":"Models employing CNN architecture have made significant progress in multivariate long sequence time-series forecasting (MLSTF), particularly in modeling local time series characteristics. However, during the MLSTF process, extracting the global time series patterns and understanding the correlations among different variables are highly significant. To address this challenge, we introduce multi-resolution convolution and deformable convolution operations. By enlarging the receptive field using convolution kernels with different dilation factors to capture temporal correlation information across different resolutions, and adaptively adjusting the sampling positions through additional offset vectors, we enhance the network's ability to capture correlated features between variables. Building upon this, we propose ATVCNet, an adaptive temporal-variable convolutional network designed to effectively model the local/global temporal dependencies and inter-variable dependencies of multivariate time series. Specifically, extracting and fusing time series features at different resolutions, captures both local contextual information and global patterns in the time series. The designed inter-variable feature adaptive extraction module captures the correlation among different variables in the time series. We evaluated the performance of ATVCNet across eight real-world datasets. The results indicate that ATVCNet achieved a performance improvement of approximately 63.4% over state-of-the-art MLSTF models.","sentences":["Models employing CNN architecture have made significant progress in multivariate long sequence time-series forecasting (MLSTF), particularly in modeling local time series characteristics.","However, during the MLSTF process, extracting the global time series patterns and understanding the correlations among different variables are highly significant.","To address this challenge, we introduce multi-resolution convolution and deformable convolution operations.","By enlarging the receptive field using convolution kernels with different dilation factors to capture temporal correlation information across different resolutions, and adaptively adjusting the sampling positions through additional offset vectors, we enhance the network's ability to capture correlated features between variables.","Building upon this, we propose ATVCNet, an adaptive temporal-variable convolutional network designed to effectively model the local/global temporal dependencies and inter-variable dependencies of multivariate time series.","Specifically, extracting and fusing time series features at different resolutions, captures both local contextual information and global patterns in the time series.","The designed inter-variable feature adaptive extraction module captures the correlation among different variables in the time series.","We evaluated the performance of ATVCNet across eight real-world datasets.","The results indicate that ATVCNet achieved a performance improvement of approximately 63.4% over state-of-the-art MLSTF models."],"url":"http://arxiv.org/abs/2405.12038v1","category":"cs.LG"}
{"created":"2024-05-20 12:11:41","title":"Position-Guided Prompt Learning for Anomaly Detection in Chest X-Rays","abstract":"Anomaly detection in chest X-rays is a critical task. Most methods mainly model the distribution of normal images, and then regard significant deviation from normal distribution as anomaly. Recently, CLIP-based methods, pre-trained on a large number of medical images, have shown impressive performance on zero/few-shot downstream tasks. In this paper, we aim to explore the potential of CLIP-based methods for anomaly detection in chest X-rays. Considering the discrepancy between the CLIP pre-training data and the task-specific data, we propose a position-guided prompt learning method. Specifically, inspired by the fact that experts diagnose chest X-rays by carefully examining distinct lung regions, we propose learnable position-guided text and image prompts to adapt the task data to the frozen pre-trained CLIP-based model. To enhance the model's discriminative capability, we propose a novel structure-preserving anomaly synthesis method within chest x-rays during the training process. Extensive experiments on three datasets demonstrate that our proposed method outperforms some state-of-the-art methods. The code of our implementation is available at https://github.com/sunzc-sunny/PPAD.","sentences":["Anomaly detection in chest X-rays is a critical task.","Most methods mainly model the distribution of normal images, and then regard significant deviation from normal distribution as anomaly.","Recently, CLIP-based methods, pre-trained on a large number of medical images, have shown impressive performance on zero/few-shot downstream tasks.","In this paper, we aim to explore the potential of CLIP-based methods for anomaly detection in chest X-rays.","Considering the discrepancy between the CLIP pre-training data and the task-specific data, we propose a position-guided prompt learning method.","Specifically, inspired by the fact that experts diagnose chest X-rays by carefully examining distinct lung regions, we propose learnable position-guided text and image prompts to adapt the task data to the frozen pre-trained CLIP-based model.","To enhance the model's discriminative capability, we propose a novel structure-preserving anomaly synthesis method within chest x-rays during the training process.","Extensive experiments on three datasets demonstrate that our proposed method outperforms some state-of-the-art methods.","The code of our implementation is available at https://github.com/sunzc-sunny/PPAD."],"url":"http://arxiv.org/abs/2405.11976v1","category":"cs.CV"}
{"created":"2024-05-20 08:42:56","title":"Automorphism groups of prime models, and invariant measures","abstract":"We adapt the notion of a (relatively) definable subset of Aut(M) when M is a saturated model to the case Aut(M/A) when M is atomic and strongly omega-homogeneous over A. We discuss the existence and uniqueness of invariant measures on the Boolean algebra of definable subsets of Aut(M/A). For example when Th(M) is stable we have existence and uniqueness. We also discuss the compatibility of our definability notions with definable Galois cohomology and differential Galois theory.","sentences":["We adapt the notion of a (relatively) definable subset of Aut(M) when M is a saturated model to the case Aut(M/A) when M is atomic and strongly omega-homogeneous over A. We discuss the existence and uniqueness of invariant measures on the Boolean algebra of definable subsets of Aut(M/A).","For example when Th(M) is stable we have existence and uniqueness.","We also discuss the compatibility of our definability notions with definable Galois cohomology and differential Galois theory."],"url":"http://arxiv.org/abs/2405.11878v1","category":"math.LO"}
{"created":"2024-05-20 06:12:33","title":"FedCAda: Adaptive Client-Side Optimization for Accelerated and Stable Federated Learning","abstract":"Federated learning (FL) has emerged as a prominent approach for collaborative training of machine learning models across distributed clients while preserving data privacy. However, the quest to balance acceleration and stability becomes a significant challenge in FL, especially on the client-side. In this paper, we introduce FedCAda, an innovative federated client adaptive algorithm designed to tackle this challenge. FedCAda leverages the Adam algorithm to adjust the correction process of the first moment estimate $m$ and the second moment estimate $v$ on the client-side and aggregate adaptive algorithm parameters on the server-side, aiming to accelerate convergence speed and communication efficiency while ensuring stability and performance. Additionally, we investigate several algorithms incorporating different adjustment functions. This comparative analysis revealed that due to the limited information contained within client models from other clients during the initial stages of federated learning, more substantial constraints need to be imposed on the parameters of the adaptive algorithm. As federated learning progresses and clients gather more global information, FedCAda gradually diminishes the impact on adaptive parameters. These findings provide insights for enhancing the robustness and efficiency of algorithmic improvements. Through extensive experiments on computer vision (CV) and natural language processing (NLP) datasets, we demonstrate that FedCAda outperforms the state-of-the-art methods in terms of adaptability, convergence, stability, and overall performance. This work contributes to adaptive algorithms for federated learning, encouraging further exploration.","sentences":["Federated learning (FL) has emerged as a prominent approach for collaborative training of machine learning models across distributed clients while preserving data privacy.","However, the quest to balance acceleration and stability becomes a significant challenge in FL, especially on the client-side.","In this paper, we introduce FedCAda, an innovative federated client adaptive algorithm designed to tackle this challenge.","FedCAda leverages the Adam algorithm to adjust the correction process of the first moment estimate $m$ and the second moment estimate $v$ on the client-side and aggregate adaptive algorithm parameters on the server-side, aiming to accelerate convergence speed and communication efficiency while ensuring stability and performance.","Additionally, we investigate several algorithms incorporating different adjustment functions.","This comparative analysis revealed that due to the limited information contained within client models from other clients during the initial stages of federated learning, more substantial constraints need to be imposed on the parameters of the adaptive algorithm.","As federated learning progresses and clients gather more global information, FedCAda gradually diminishes the impact on adaptive parameters.","These findings provide insights for enhancing the robustness and efficiency of algorithmic improvements.","Through extensive experiments on computer vision (CV) and natural language processing (NLP) datasets, we demonstrate that FedCAda outperforms the state-of-the-art methods in terms of adaptability, convergence, stability, and overall performance.","This work contributes to adaptive algorithms for federated learning, encouraging further exploration."],"url":"http://arxiv.org/abs/2405.11811v1","category":"cs.LG"}
{"created":"2024-05-20 05:44:11","title":"Self-Supervised Learning of Visual Servoing for Low-Rigidity Robots Considering Temporal Body Changes","abstract":"In this study, we investigate object grasping by visual servoing in a low-rigidity robot. It is difficult for a low-rigidity robot to handle its own body as intended compared to a rigid robot, and calibration between vision and body takes some time. In addition, the robot must constantly adapt to changes in its body, such as the change in camera position and change in joints due to aging. Therefore, we develop a method for a low-rigidity robot to autonomously learn visual servoing of its body. We also develop a mechanism that can adaptively change its visual servoing according to temporal body changes. We apply our method to a low-rigidity 6-axis arm, MyCobot, and confirm its effectiveness by conducting object grasping experiments based on visual servoing.","sentences":["In this study, we investigate object grasping by visual servoing in a low-rigidity robot.","It is difficult for a low-rigidity robot to handle its own body as intended compared to a rigid robot, and calibration between vision and body takes some time.","In addition, the robot must constantly adapt to changes in its body, such as the change in camera position and change in joints due to aging.","Therefore, we develop a method for a low-rigidity robot to autonomously learn visual servoing of its body.","We also develop a mechanism that can adaptively change its visual servoing according to temporal body changes.","We apply our method to a low-rigidity 6-axis arm, MyCobot, and confirm its effectiveness by conducting object grasping experiments based on visual servoing."],"url":"http://arxiv.org/abs/2405.11798v1","category":"cs.RO"}
{"created":"2024-05-20 03:33:12","title":"Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning","abstract":"Semi-supervised learning (SSL) has witnessed remarkable progress, resulting in the emergence of numerous method variations. However, practitioners often encounter challenges when attempting to deploy these methods due to their subpar performance. In this paper, we present a novel SSL approach named FineSSL that significantly addresses this limitation by adapting pre-trained foundation models. We identify the aggregated biases and cognitive deviation problems inherent in foundation models, and propose a simple yet effective solution by imposing balanced margin softmax and decoupled label smoothing. Through extensive experiments, we demonstrate that FineSSL sets a new state of the art for SSL on multiple benchmark datasets, reduces the training cost by over six times, and can seamlessly integrate various fine-tuning and modern SSL algorithms. The source code is available at https://github.com/Gank0078/FineSSL.","sentences":["Semi-supervised learning (SSL) has witnessed remarkable progress, resulting in the emergence of numerous method variations.","However, practitioners often encounter challenges when attempting to deploy these methods due to their subpar performance.","In this paper, we present a novel SSL approach named FineSSL that significantly addresses this limitation by adapting pre-trained foundation models.","We identify the aggregated biases and cognitive deviation problems inherent in foundation models, and propose a simple yet effective solution by imposing balanced margin softmax and decoupled label smoothing.","Through extensive experiments, we demonstrate that FineSSL sets a new state of the art for SSL on multiple benchmark datasets, reduces the training cost by over six times, and can seamlessly integrate various fine-tuning and modern SSL algorithms.","The source code is available at https://github.com/Gank0078/FineSSL."],"url":"http://arxiv.org/abs/2405.11756v1","category":"cs.LG"}
{"created":"2024-05-19 20:13:04","title":"Hyperbolic enhancement of a quantum battery","abstract":"A quantum system which can store energy, and from which one can extract useful work, is known as a quantum battery. Such a device raises interesting issues surrounding how quantum physics can provide certain advantages in the charging, energy storage or discharging of the quantum battery as compared to their classical equivalents. However, the pernicious effect of dissipation degrades the performance of any realistic battery. Here we show how one can circumvent this problem of energy loss by proposing a quantum battery model which benefits from quantum squeezing. Namely, charging the battery quadratically with a short temporal pulse induces a hyperbolic enhancement in the stored energy, such that the dissipation present becomes essentially negligible in comparison. Furthermore, we show that when the driving is strong enough the useful work which can be extracted from the quantum battery, that is the ergotropy, is exactly equal to the stored energy. These impressive properties imply a highly efficient quantum energetic device with abundant amounts of ergotropy. Our theoretical results suggest a possible route to realizing high-performance quantum batteries, which could be realized in a variety of platforms exploiting quantum continuous variables.","sentences":["A quantum system which can store energy, and from which one can extract useful work, is known as a quantum battery.","Such a device raises interesting issues surrounding how quantum physics can provide certain advantages in the charging, energy storage or discharging of the quantum battery as compared to their classical equivalents.","However, the pernicious effect of dissipation degrades the performance of any realistic battery.","Here we show how one can circumvent this problem of energy loss by proposing a quantum battery model which benefits from quantum squeezing.","Namely, charging the battery quadratically with a short temporal pulse induces a hyperbolic enhancement in the stored energy, such that the dissipation present becomes essentially negligible in comparison.","Furthermore, we show that when the driving is strong enough the useful work which can be extracted from the quantum battery, that is the ergotropy, is exactly equal to the stored energy.","These impressive properties imply a highly efficient quantum energetic device with abundant amounts of ergotropy.","Our theoretical results suggest a possible route to realizing high-performance quantum batteries, which could be realized in a variety of platforms exploiting quantum continuous variables."],"url":"http://arxiv.org/abs/2405.11662v1","category":"quant-ph"}
{"created":"2024-05-19 17:58:26","title":"Zero-Shot Stance Detection using Contextual Data Generation with LLMs","abstract":"Stance detection, the classification of attitudes expressed in a text towards a specific topic, is vital for applications like fake news detection and opinion mining. However, the scarcity of labeled data remains a challenge for this task. To address this problem, we propose Dynamic Model Adaptation with Contextual Data Generation (DyMoAdapt) that combines Few-Shot Learning and Large Language Models. In this approach, we aim to fine-tune an existing model at test time. We achieve this by generating new topic-specific data using GPT-3. This method could enhance performance by allowing the adaptation of the model to new topics. However, the results did not increase as we expected. Furthermore, we introduce the Multi Generated Topic VAST (MGT-VAST) dataset, which extends VAST using GPT-3. In this dataset, each context is associated with multiple topics, allowing the model to understand the relationship between contexts and various potential topics","sentences":["Stance detection, the classification of attitudes expressed in a text towards a specific topic, is vital for applications like fake news detection and opinion mining.","However, the scarcity of labeled data remains a challenge for this task.","To address this problem, we propose Dynamic Model Adaptation with Contextual Data Generation (DyMoAdapt) that combines Few-Shot Learning and Large Language Models.","In this approach, we aim to fine-tune an existing model at test time.","We achieve this by generating new topic-specific data using GPT-3.","This method could enhance performance by allowing the adaptation of the model to new topics.","However, the results did not increase as we expected.","Furthermore, we introduce the Multi Generated Topic VAST (MGT-VAST) dataset, which extends VAST using GPT-3.","In this dataset, each context is associated with multiple topics, allowing the model to understand the relationship between contexts and various potential topics"],"url":"http://arxiv.org/abs/2405.11637v1","category":"cs.CL"}
{"created":"2024-05-19 17:51:53","title":"Spectral theory of infinite dimensional dissipative Hamiltonian systems","abstract":"The spectral theory for operator pencils and operator differential-algebraic equations is studied. Special focus is laid on singular operator pencils and three different concepts of singular operator pencils are introduced. The concepts are analyzed in detail and examples are presented that illustrate the subtle differences. It is investigated how these concepts are related to uniqueness of the underlying algebraic-differential operator equation, showing that, in general, classical results known from the finite dimensional case of matrix pencils and differential-algebraic equations do not prevail. The results are then studied in the setting of structured operator pencils arising in dissipative differential-algebraic equations. Here, unlike in the general infinite-dimensional case, the uniqueness of solutions is closely related to the singularity of the pencil.","sentences":["The spectral theory for operator pencils and operator differential-algebraic equations is studied.","Special focus is laid on singular operator pencils and three different concepts of singular operator pencils are introduced.","The concepts are analyzed in detail and examples are presented that illustrate the subtle differences.","It is investigated how these concepts are related to uniqueness of the underlying algebraic-differential operator equation, showing that, in general, classical results known from the finite dimensional case of matrix pencils and differential-algebraic equations do not prevail.","The results are then studied in the setting of structured operator pencils arising in dissipative differential-algebraic equations.","Here, unlike in the general infinite-dimensional case, the uniqueness of solutions is closely related to the singularity of the pencil."],"url":"http://arxiv.org/abs/2405.11634v1","category":"math.FA"}
{"created":"2024-05-19 15:20:27","title":"DOLLmC: DevOPs for Large Language model Customization","abstract":"The rapid integration of Large Language Models (LLMs) into various industries presents both revolutionary opportunities and unique challenges. This research aims to establish a scalable and efficient framework for LLM customization, exploring how DevOps practices should be adapted to meet the specific demands of LLM customization. By integrating ontologies, knowledge maps, and prompt engineering into the DevOps pipeline, we propose a robust framework that enhances continuous learning, seamless deployment, and rigorous version control of LLMs. This methodology is demonstrated through the development of a domain-specific chatbot for the agricultural sector, utilizing heterogeneous data to deliver actionable insights. The proposed methodology, so called DOLLmC, not only addresses the immediate challenges of LLM customization but also promotes scalability and operational efficiency. However, the methodology's primary limitation lies in the need for extensive testing, validation, and broader adoption across different domains.","sentences":["The rapid integration of Large Language Models (LLMs) into various industries presents both revolutionary opportunities and unique challenges.","This research aims to establish a scalable and efficient framework for LLM customization, exploring how DevOps practices should be adapted to meet the specific demands of LLM customization.","By integrating ontologies, knowledge maps, and prompt engineering into the DevOps pipeline, we propose a robust framework that enhances continuous learning, seamless deployment, and rigorous version control of LLMs.","This methodology is demonstrated through the development of a domain-specific chatbot for the agricultural sector, utilizing heterogeneous data to deliver actionable insights.","The proposed methodology, so called DOLLmC, not only addresses the immediate challenges of LLM customization but also promotes scalability and operational efficiency.","However, the methodology's primary limitation lies in the need for extensive testing, validation, and broader adoption across different domains."],"url":"http://arxiv.org/abs/2405.11581v1","category":"cs.SE"}
{"created":"2024-05-19 14:42:19","title":"Quantile Activation: departing from single point estimation for better generalization across distortions","abstract":"A classifier is, in its essence, a function which takes an input and returns the class of the input and implicitly assumes an underlying distribution. We argue in this article that one has to move away from this basic tenet to obtain generalisation across distributions. Specifically, the class of the sample should depend on the points from its context distribution for better generalisation across distributions.   How does one achieve this? The key idea is to adapt the outputs of each neuron of the network to its context distribution. We propose quantile activation, QACT, which, in simple terms, outputs the relative quantile of the sample in its context distribution, instead of the actual values in traditional networks.   The scope of this article is to validate the proposed activation across several experimental settings, and compare it with conventional techniques. For this, we use the datasets developed to test robustness against distortions CIFAR10C, CIFAR100C, MNISTC, TinyImagenetC, and show that we achieve a significantly higher generalisation across distortions than the conventional classifiers, across different architectures. Although this paper is only a proof of concept, we surprisingly find that this approach outperforms DINOv2(small) at large distortions, even though DINOv2 is trained with a far bigger network on a considerably larger dataset.","sentences":["A classifier is, in its essence, a function which takes an input and returns the class of the input and implicitly assumes an underlying distribution.","We argue in this article that one has to move away from this basic tenet to obtain generalisation across distributions.","Specifically, the class of the sample should depend on the points from its context distribution for better generalisation across distributions.   ","How does one achieve this?","The key idea is to adapt the outputs of each neuron of the network to its context distribution.","We propose quantile activation, QACT, which, in simple terms, outputs the relative quantile of the sample in its context distribution, instead of the actual values in traditional networks.   ","The scope of this article is to validate the proposed activation across several experimental settings, and compare it with conventional techniques.","For this, we use the datasets developed to test robustness against distortions CIFAR10C, CIFAR100C, MNISTC, TinyImagenetC, and show that we achieve a significantly higher generalisation across distortions than the conventional classifiers, across different architectures.","Although this paper is only a proof of concept, we surprisingly find that this approach outperforms DINOv2(small) at large distortions, even though DINOv2 is trained with a far bigger network on a considerably larger dataset."],"url":"http://arxiv.org/abs/2405.11573v1","category":"cs.LG"}
{"created":"2024-05-19 14:31:44","title":"A thermodynamic and analytical description on the quantitative phase-field model with enhanced interface diffusivity","abstract":"Based on the idea of maintaining physical diffuse interface kinetics, enhancing interfacial diffusivity has recently provided a new direction for quantitative phase-field simulation at microstructural length and time scale. Establishing a general relationship between interface diffusivity and width is vital to facilitate the practical application. However, it is still limited by time-consuming numerical corrections, and its relationship with non-dilute thermodynamic properties still needs to be revealed. In this study, we present a new thermodynamic and analytical method for determining interfacial diffusivity enhancement. Unlike previous numerical corrections of partition coefficients and interface temperature, this new method aims to keep several thermodynamic quantities unchanged after enlarging the interface width. These essential quantities are theoretically proven to be diffusion potential jump across the diffuse interface and free energy dissipation by trans-interface diffusion. Since no dilute approximation has been employed in model derivation, the present method is available for binary alloys with arbitrary thermodynamic properties and can be easily extended to describe multicomponent systems. Therefore, the present method is expected to advance the recent quantitative phase-field framework and facilitate its practical applications.","sentences":["Based on the idea of maintaining physical diffuse interface kinetics, enhancing interfacial diffusivity has recently provided a new direction for quantitative phase-field simulation at microstructural length and time scale.","Establishing a general relationship between interface diffusivity and width is vital to facilitate the practical application.","However, it is still limited by time-consuming numerical corrections, and its relationship with non-dilute thermodynamic properties still needs to be revealed.","In this study, we present a new thermodynamic and analytical method for determining interfacial diffusivity enhancement.","Unlike previous numerical corrections of partition coefficients and interface temperature, this new method aims to keep several thermodynamic quantities unchanged after enlarging the interface width.","These essential quantities are theoretically proven to be diffusion potential jump across the diffuse interface and free energy dissipation by trans-interface diffusion.","Since no dilute approximation has been employed in model derivation, the present method is available for binary alloys with arbitrary thermodynamic properties and can be easily extended to describe multicomponent systems.","Therefore, the present method is expected to advance the recent quantitative phase-field framework and facilitate its practical applications."],"url":"http://arxiv.org/abs/2405.11568v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-19 14:31:01","title":"Shortcut to Chemically Accurate Quantum Computing via Density-based Basis-set Correction","abstract":"Quantum computing promises a computational advantage over classical methods in electronic-structure calculations, with expected applications in drug design and materials science. Accessing a quantitative description of chemical systems while minimizing quantum resources, such as the number of qubits, is an essential challenge given the limited capabilities of current quantum processors. We provide a shortcut towards quantum computations at chemical accuracy by approaching the complete-basis-set limit (CBS) through integrating density-functional theory into quantum algorithms via density-based basis-set corrections coupled to basis-sets crafted on-the-fly and specifically adapted to a given system/user-defined qubit budget. The approach self-consistently accelerates the basis-set convergence, improving electronic densities, ground-state energies, and first-order properties such as dipole moments. It can also serve as a classical, \\textit{a posteriori}, energy correction to quantum hardware calculations. The strategy is assessed using GPU-accelerated state-vector emulation up to 32 qubits. We converge the ground-state energies of four systems (He, Be, H$_2$, LiH) within chemical accuracy of the CBS full-configuration-interaction reference, while offering a systematic increase of accuracy beyond a double-zeta quality for various molecules up to the H$_8$ hydrogen chain. We also obtain dissociation curves for H$_2$ and LiH that reach the CBS limit whereas for the challenging simulation of the N$_2$ triple-bond breaking, we achieve a near-triple-zeta quality at the cost of a minimal basis-set. This hybrid strategy allows us to obtain quantitative results that would otherwise require brute-force quantum simulations using far more than 100 logical qubits, thereby opening up opportunities to explore real-world chemistry with reasonable computational resources.","sentences":["Quantum computing promises a computational advantage over classical methods in electronic-structure calculations, with expected applications in drug design and materials science.","Accessing a quantitative description of chemical systems while minimizing quantum resources, such as the number of qubits, is an essential challenge given the limited capabilities of current quantum processors.","We provide a shortcut towards quantum computations at chemical accuracy by approaching the complete-basis-set limit (CBS) through integrating density-functional theory into quantum algorithms via density-based basis-set corrections coupled to basis-sets crafted on-the-fly and specifically adapted to a given system/user-defined qubit budget.","The approach self-consistently accelerates the basis-set convergence, improving electronic densities, ground-state energies, and first-order properties such as dipole moments.","It can also serve as a classical, \\textit{a posteriori}, energy correction to quantum hardware calculations.","The strategy is assessed using GPU-accelerated state-vector emulation up to 32 qubits.","We converge the ground-state energies of four systems (He, Be, H$_2$, LiH) within chemical accuracy of the CBS full-configuration-interaction reference, while offering a systematic increase of accuracy beyond a double-zeta quality for various molecules up to the H$_8$ hydrogen chain.","We also obtain dissociation curves for H$_2$ and LiH that reach the CBS limit whereas for the challenging simulation of the N$_2$ triple-bond breaking, we achieve a near-triple-zeta quality at the cost of a minimal basis-set.","This hybrid strategy allows us to obtain quantitative results that would otherwise require brute-force quantum simulations using far more than 100 logical qubits, thereby opening up opportunities to explore real-world chemistry with reasonable computational resources."],"url":"http://arxiv.org/abs/2405.11567v1","category":"physics.chem-ph"}
{"created":"2024-05-19 13:26:33","title":"Adaptive Online Experimental Design for Causal Discovery","abstract":"Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.","sentences":["Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination.","The majority of existing causal discovery methods are developed assuming infinite interventional data.","We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems.","A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case.","We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history.","Given any desired confidence value, the algorithm determines a termination condition and runs until it is met.","We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples.","Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs.","It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples."],"url":"http://arxiv.org/abs/2405.11548v1","category":"cs.LG"}
{"created":"2024-05-19 13:11:18","title":"Verification technology for finger vein biometric","abstract":"Finger vein biometrics is an approach to identifying individuals based on the unique patterns of blood vessels in their fingers, and the technology is advanced in image capture and processing techniques, which is leading to more efficient, accurate, and reliable systems. This article focuses on a verification system that compares the matrices of an efficient finger vein verification system on different databases to test its strength and efficiency. Contrast Limited Adaptive Histogram Equalization (CLAHE) has been examined as an image enhancement and processing method to improve contrast and render details in an image easier to detect. A random forest classifier is deployed with a comparison between two pretrained systems, VGG16 and ResNet50, which are types of convolutional neural networks. VGG-16 and ResNet-50 models are implemented on three different datasets, and fine-tuning these models enabled the harnessing of their powerful capabilities and achieving superior performance on the specific image classification task.","sentences":["Finger vein biometrics is an approach to identifying individuals based on the unique patterns of blood vessels in their fingers, and the technology is advanced in image capture and processing techniques, which is leading to more efficient, accurate, and reliable systems.","This article focuses on a verification system that compares the matrices of an efficient finger vein verification system on different databases to test its strength and efficiency.","Contrast Limited Adaptive Histogram Equalization (CLAHE) has been examined as an image enhancement and processing method to improve contrast and render details in an image easier to detect.","A random forest classifier is deployed with a comparison between two pretrained systems, VGG16 and ResNet50, which are types of convolutional neural networks.","VGG-16 and ResNet-50 models are implemented on three different datasets, and fine-tuning these models enabled the harnessing of their powerful capabilities and achieving superior performance on the specific image classification task."],"url":"http://arxiv.org/abs/2405.11540v1","category":"eess.IV"}
{"created":"2024-05-19 11:29:52","title":"Diffusion-Based Hierarchical Image Steganography","abstract":"This paper introduces Hierarchical Image Steganography, a novel method that enhances the security and capacity of embedding multiple images into a single container using diffusion models. HIS assigns varying levels of robustness to images based on their importance, ensuring enhanced protection against manipulation. It adaptively exploits the robustness of the Diffusion Model alongside the reversibility of the Flow Model. The integration of Embed-Flow and Enhance-Flow improves embedding efficiency and image recovery quality, respectively, setting HIS apart from conventional multi-image steganography techniques. This innovative structure can autonomously generate a container image, thereby securely and efficiently concealing multiple images and text. Rigorous subjective and objective evaluations underscore our advantage in analytical resistance, robustness, and capacity, illustrating its expansive applicability in content safeguarding and privacy fortification.","sentences":["This paper introduces Hierarchical Image Steganography, a novel method that enhances the security and capacity of embedding multiple images into a single container using diffusion models.","HIS assigns varying levels of robustness to images based on their importance, ensuring enhanced protection against manipulation.","It adaptively exploits the robustness of the Diffusion Model alongside the reversibility of the Flow Model.","The integration of Embed-Flow and Enhance-Flow improves embedding efficiency and image recovery quality, respectively, setting HIS apart from conventional multi-image steganography techniques.","This innovative structure can autonomously generate a container image, thereby securely and efficiently concealing multiple images and text.","Rigorous subjective and objective evaluations underscore our advantage in analytical resistance, robustness, and capacity, illustrating its expansive applicability in content safeguarding and privacy fortification."],"url":"http://arxiv.org/abs/2405.11523v1","category":"cs.CV"}
{"created":"2024-05-19 07:04:05","title":"Emphasizing Crucial Features for Efficient Image Restoration","abstract":"Image restoration is a challenging ill-posed problem which estimates latent sharp image from its degraded counterpart. Although the existing methods have achieved promising performance by designing novelty architecture of module, they ignore the fact that different regions in a corrupted image undergo varying degrees of degradation. In this paper, we propose an efficient and effective framework to adapt to varying degrees of degradation across different regions for image restoration. Specifically, we design a spatial and frequency attention mechanism (SFAM) to emphasize crucial features for restoration. SFAM consists of two modules: the spatial domain attention module (SDAM) and the frequency domain attention module (FDAM). The SFAM discerns the degradation location through spatial selective attention and channel selective attention in the spatial domain, while the FDAM enhances high-frequency signals to amplify the disparities between sharp and degraded image pairs in the spectral domain. Additionally, to capture global range information, we introduce a multi-scale block (MSBlock) that consists of three scale branches, each containing multiple simplified channel attention blocks (SCABlocks) and a multi-scale feed-forward block (MSFBlock). Finally, we propose our ECFNet, which integrates the aforementioned components into a U-shaped backbone for recovering high-quality images. Extensive experimental results demonstrate the effectiveness of ECFNet, outperforming state-of-the-art (SOTA) methods on both synthetic and real-world datasets.","sentences":["Image restoration is a challenging ill-posed problem which estimates latent sharp image from its degraded counterpart.","Although the existing methods have achieved promising performance by designing novelty architecture of module, they ignore the fact that different regions in a corrupted image undergo varying degrees of degradation.","In this paper, we propose an efficient and effective framework to adapt to varying degrees of degradation across different regions for image restoration.","Specifically, we design a spatial and frequency attention mechanism (SFAM) to emphasize crucial features for restoration.","SFAM consists of two modules: the spatial domain attention module (SDAM) and the frequency domain attention module (FDAM).","The SFAM discerns the degradation location through spatial selective attention and channel selective attention in the spatial domain, while the FDAM enhances high-frequency signals to amplify the disparities between sharp and degraded image pairs in the spectral domain.","Additionally, to capture global range information, we introduce a multi-scale block (MSBlock) that consists of three scale branches, each containing multiple simplified channel attention blocks (SCABlocks) and a multi-scale feed-forward block (MSFBlock).","Finally, we propose our ECFNet, which integrates the aforementioned components into a U-shaped backbone for recovering high-quality images.","Extensive experimental results demonstrate the effectiveness of ECFNet, outperforming state-of-the-art (SOTA) methods on both synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2405.11468v1","category":"cs.CV"}
{"created":"2024-05-19 06:54:03","title":"AdaAugment: A Tuning-Free and Adaptive Approach to Enhance Data Augmentation","abstract":"Data augmentation (DA) is widely employed to improve the generalization performance of deep models. However, most existing DA methods use augmentation operations with random magnitudes throughout training. While this fosters diversity, it can also inevitably introduce uncontrolled variability in augmented data, which may cause misalignment with the evolving training status of the target models. Both theoretical and empirical findings suggest that this misalignment increases the risks of underfitting and overfitting. To address these limitations, we propose AdaAugment, an innovative and tuning-free Adaptive Augmentation method that utilizes reinforcement learning to dynamically adjust augmentation magnitudes for individual training samples based on real-time feedback from the target network. Specifically, AdaAugment features a dual-model architecture consisting of a policy network and a target network, which are jointly optimized to effectively adapt augmentation magnitudes. The policy network optimizes the variability within the augmented data, while the target network utilizes the adaptively augmented samples for training. Extensive experiments across benchmark datasets and deep architectures demonstrate that AdaAugment consistently outperforms other state-of-the-art DA methods in effectiveness while maintaining remarkable efficiency.","sentences":["Data augmentation (DA) is widely employed to improve the generalization performance of deep models.","However, most existing DA methods use augmentation operations with random magnitudes throughout training.","While this fosters diversity, it can also inevitably introduce uncontrolled variability in augmented data, which may cause misalignment with the evolving training status of the target models.","Both theoretical and empirical findings suggest that this misalignment increases the risks of underfitting and overfitting.","To address these limitations, we propose AdaAugment, an innovative and tuning-free Adaptive Augmentation method that utilizes reinforcement learning to dynamically adjust augmentation magnitudes for individual training samples based on real-time feedback from the target network.","Specifically, AdaAugment features a dual-model architecture consisting of a policy network and a target network, which are jointly optimized to effectively adapt augmentation magnitudes.","The policy network optimizes the variability within the augmented data, while the target network utilizes the adaptively augmented samples for training.","Extensive experiments across benchmark datasets and deep architectures demonstrate that AdaAugment consistently outperforms other state-of-the-art DA methods in effectiveness while maintaining remarkable efficiency."],"url":"http://arxiv.org/abs/2405.11467v1","category":"cs.CV"}
{"created":"2024-05-19 04:57:17","title":"Cross-Domain Knowledge Distillation for Low-Resolution Human Pose Estimation","abstract":"In practical applications of human pose estimation, low-resolution inputs frequently occur, and existing state-of-the-art models perform poorly with low-resolution images. This work focuses on boosting the performance of low-resolution models by distilling knowledge from a high-resolution model. However, we face the challenge of feature size mismatch and class number mismatch when applying knowledge distillation to networks with different input resolutions. To address this issue, we propose a novel cross-domain knowledge distillation (CDKD) framework. In this framework, we construct a scale-adaptive projector ensemble (SAPE) module to spatially align feature maps between models of varying input resolutions. It adopts a projector ensemble to map low-resolution features into multiple common spaces and adaptively merges them based on multi-scale information to match high-resolution features. Additionally, we construct a cross-class alignment (CCA) module to solve the problem of the mismatch of class numbers. By combining an easy-to-hard training (ETHT) strategy, the CCA module further enhances the distillation performance. The effectiveness and efficiency of our approach are demonstrated by extensive experiments on two common benchmark datasets: MPII and COCO. The code is made available in supplementary material.","sentences":["In practical applications of human pose estimation, low-resolution inputs frequently occur, and existing state-of-the-art models perform poorly with low-resolution images.","This work focuses on boosting the performance of low-resolution models by distilling knowledge from a high-resolution model.","However, we face the challenge of feature size mismatch and class number mismatch when applying knowledge distillation to networks with different input resolutions.","To address this issue, we propose a novel cross-domain knowledge distillation (CDKD) framework.","In this framework, we construct a scale-adaptive projector ensemble (SAPE) module to spatially align feature maps between models of varying input resolutions.","It adopts a projector ensemble to map low-resolution features into multiple common spaces and adaptively merges them based on multi-scale information to match high-resolution features.","Additionally, we construct a cross-class alignment (CCA) module to solve the problem of the mismatch of class numbers.","By combining an easy-to-hard training (ETHT) strategy, the CCA module further enhances the distillation performance.","The effectiveness and efficiency of our approach are demonstrated by extensive experiments on two common benchmark datasets: MPII and COCO.","The code is made available in supplementary material."],"url":"http://arxiv.org/abs/2405.11448v1","category":"cs.CV"}
{"created":"2024-05-19 04:49:42","title":"MAML-en-LLM: Model Agnostic Meta-Training of LLMs for Improved In-Context Learning","abstract":"Adapting large language models (LLMs) to unseen tasks with in-context training samples without fine-tuning remains an important research problem. To learn a robust LLM that adapts well to unseen tasks, multiple meta-training approaches have been proposed such as MetaICL and MetaICT, which involve meta-training pre-trained LLMs on a wide variety of diverse tasks. These meta-training approaches essentially perform in-context multi-task fine-tuning and evaluate on a disjointed test set of tasks. Even though they achieve impressive performance, their goal is never to compute a truly general set of parameters. In this paper, we propose MAML-en-LLM, a novel method for meta-training LLMs, which can learn truly generalizable parameters that not only perform well on disjointed tasks but also adapts to unseen tasks. We see an average increase of 2% on unseen domains in the performance while a massive 4% improvement on adaptation performance. Furthermore, we demonstrate that MAML-en-LLM outperforms baselines in settings with limited amount of training data on both seen and unseen domains by an average of 2%. Finally, we discuss the effects of type of tasks, optimizers and task complexity, an avenue barely explored in meta-training literature. Exhaustive experiments across 7 task settings along with two data settings demonstrate that models trained with MAML-en-LLM outperform SOTA meta-training approaches.","sentences":["Adapting large language models (LLMs) to unseen tasks with in-context training samples without fine-tuning remains an important research problem.","To learn a robust LLM that adapts well to unseen tasks, multiple meta-training approaches have been proposed such as MetaICL and MetaICT, which involve meta-training pre-trained LLMs on a wide variety of diverse tasks.","These meta-training approaches essentially perform in-context multi-task fine-tuning and evaluate on a disjointed test set of tasks.","Even though they achieve impressive performance, their goal is never to compute a truly general set of parameters.","In this paper, we propose MAML-en-LLM, a novel method for meta-training LLMs, which can learn truly generalizable parameters that not only perform well on disjointed tasks but also adapts to unseen tasks.","We see an average increase of 2% on unseen domains in the performance while a massive 4% improvement on adaptation performance.","Furthermore, we demonstrate that MAML-en-LLM outperforms baselines in settings with limited amount of training data on both seen and unseen domains by an average of 2%.","Finally, we discuss the effects of type of tasks, optimizers and task complexity, an avenue barely explored in meta-training literature.","Exhaustive experiments across 7 task settings along with two data settings demonstrate that models trained with MAML-en-LLM outperform SOTA meta-training approaches."],"url":"http://arxiv.org/abs/2405.11446v1","category":"cs.CL"}
{"created":"2024-05-19 04:47:02","title":"The Influence of Extended Interactions on Spin Dynamics in One-dimensional Cuprates","abstract":"Quasi-one-dimensional (1D) materials provide a unique platform for understanding the importance and influence of extended interactions on the physics of strongly correlated systems due to their relative structural simplicity and the existence of powerful theoretical tools well-adapted to one spatial dimension. Recently, this was highlighted by anomalous observations in the single-particle spectral function $A(q,\\omega)$ of 1D cuprate chain compounds, measured by angle-resolved photoemission spectroscopy (ARPES), which were explained by the presence of a long-range attractive interaction. Such an extended interaction should leave its fingerprints on other observables, notably the dynamical spin structure factor $S(q,\\omega)$, measured by neutron scattering or resonant inelastic x-ray scattering (RIXS). Starting from a simple Hubbard Hamiltonian in 1D and using time-dependent density matrix renormalization group (tDMRG) methods, we show that the presence of long-range attractive coupling, directly through an instantaneous Coulomb interaction $V$ or retarded electron-phonon ({\\it el-ph}) coupling, can introduce significant spectral weight redistribution in $S(q,\\omega)$ across a wide range of doping. This underscores the significant impact that extended interactions can have on dynamical correlations among particles, and the importance of properly incorporating this influence in modeling. Our results demonstrate that $S(q,\\omega)$ can provide a sensitive experimental constraint, which complements ARPES measurements, in identifying key interactions in 1D cuprates, beyond the standard Hubbard model.","sentences":["Quasi-one-dimensional (1D) materials provide a unique platform for understanding the importance and influence of extended interactions on the physics of strongly correlated systems due to their relative structural simplicity and the existence of powerful theoretical tools well-adapted to one spatial dimension.","Recently, this was highlighted by anomalous observations in the single-particle spectral function $A(q,\\omega)$ of 1D cuprate chain compounds, measured by angle-resolved photoemission spectroscopy (ARPES), which were explained by the presence of a long-range attractive interaction.","Such an extended interaction should leave its fingerprints on other observables, notably the dynamical spin structure factor $S(q,\\omega)$, measured by neutron scattering or resonant inelastic x-ray scattering (RIXS).","Starting from a simple Hubbard Hamiltonian in 1D and using time-dependent density matrix renormalization group (tDMRG) methods, we show that the presence of long-range attractive coupling, directly through an instantaneous Coulomb interaction $V$ or retarded electron-phonon ({\\it el-ph}) coupling, can introduce significant spectral weight redistribution in $S(q,\\omega)$ across a wide range of doping.","This underscores the significant impact that extended interactions can have on dynamical correlations among particles, and the importance of properly incorporating this influence in modeling.","Our results demonstrate that $S(q,\\omega)$ can provide a sensitive experimental constraint, which complements ARPES measurements, in identifying key interactions in 1D cuprates, beyond the standard Hubbard model."],"url":"http://arxiv.org/abs/2405.11445v1","category":"cond-mat.str-el"}
{"created":"2024-05-19 03:46:11","title":"Quantum sampling algorithms for quantum state preparation and matrix block-encoding","abstract":"The problems of quantum state preparation and matrix block-encoding are ubiquitous in quantum computing: they are crucial parts of various quantum algorithms for the purpose for initial state preparation as well as loading problem relevant data. We first present an algorithm based on QRS that prepares a quantum state $|\\psi_f\\rangle \\propto \\sum^N_{x=1} f(x)|x\\rangle$. When combined with efficient reference states the algorithm reduces the cost of quantum state preparation substantially, if certain criteria on $f$ are met. When the preparation of the reference state is not the dominant cost, and the function $f$ and relevant properties are efficiently computable or provided otherwise with cost $o(N)$, the QRS-based method outperforms the generic state preparation algorithm, which has cost $O(N)$. We demonstrate the detailed performance (in terms of the number of Toffoli gates) of the QRS-based algorithm for quantum states commonly appearing in quantum applications, e.g., those with coefficients that obey power law decay, Gaussian, and hyperbolic tangent, and compare it with other methods. Then, we adapt QRS techniques to the matrix block-encoding problem and introduce a QRS-based algorithm for block-encoding a given matrix $A = \\sum_{ij} A_{ij} |i\\rangle \\langle j|$. We work out rescaling factors for different access models, which encode how the information about the matrix is provided to the quantum computer. We exemplify these results for a particular Toeplitz matrix with elements $A_{{\\mathbf{ij}}}= 1/\\|{\\mathbf{i}}-{\\mathbf{j}}\\|^2$, which appears in quantum chemistry, and PDE applications, e.g., when the Coulomb interaction is involved. Our work unifies, and in certain ways goes beyond, various quantum state preparation and matrix block-encoding methods in the literature, and gives detailed performance analysis of important examples that appear in quantum applications.","sentences":["The problems of quantum state preparation and matrix block-encoding are ubiquitous in quantum computing: they are crucial parts of various quantum algorithms for the purpose for initial state preparation as well as loading problem relevant data.","We first present an algorithm based on QRS that prepares a quantum state $|\\psi_f\\rangle \\propto \\sum^N_{x=1} f(x)|x\\rangle$. When combined with efficient reference states the algorithm reduces the cost of quantum state preparation substantially, if certain criteria on $f$ are met.","When the preparation of the reference state is not the dominant cost, and the function $f$ and relevant properties are efficiently computable or provided otherwise with cost $o(N)$, the QRS-based method outperforms the generic state preparation algorithm, which has cost $O(N)$. We demonstrate the detailed performance (in terms of the number of Toffoli gates) of the QRS-based algorithm for quantum states commonly appearing in quantum applications, e.g., those with coefficients that obey power law decay, Gaussian, and hyperbolic tangent, and compare it with other methods.","Then, we adapt QRS techniques to the matrix block-encoding problem and introduce a QRS-based algorithm for block-encoding a given matrix $A = \\sum_{ij} A_{ij} |i\\rangle \\langle j|$.","We work out rescaling factors for different access models, which encode how the information about the matrix is provided to the quantum computer.","We exemplify these results for a particular Toeplitz matrix with elements $A_{{\\mathbf{ij}}}= 1/\\|{\\mathbf{i}}-{\\mathbf{j}}\\|^2$, which appears in quantum chemistry, and PDE applications, e.g., when the Coulomb interaction is involved.","Our work unifies, and in certain ways goes beyond, various quantum state preparation and matrix block-encoding methods in the literature, and gives detailed performance analysis of important examples that appear in quantum applications."],"url":"http://arxiv.org/abs/2405.11436v1","category":"quant-ph"}
{"created":"2024-05-18 23:16:52","title":"SmartAntenna: Enhancing Wireless Range with Autonomous Orientation","abstract":"The SmartAntenna proposes a novel approach to extend wireless communication, focusing on autonomous orientation to extend range and optimize performance. Through meticulous evaluation, various aspects of its functionality were assessed, revealing both strengths and areas for improvement. Notably, the antenna tracking mechanism exhibited remarkable efficacy. The SmartAntenna demonstrated robust functionality throughout extensive testing, underscoring its reliability even amidst complex operational scenarios. However, challenges emerged during target tracking, particularly evident in 360-degree sweeps, necessitating further refinement to enhance accuracy. Despite reliance on the HC-12 module, LoRa, performance limitations surfaced, prompting concerns regarding its suitability for production systems, especially within noisy frequency bands. Nevertheless, the SmartAntenna's adaptability across various wireless technologies holds promise, opening avenues for extended communication ranges and diverse applications. SmartAntenna research contributes valuable insights into optimizing wireless communication systems, paving the way for enhanced performance and expanded capabilities in diverse operational environments.","sentences":["The SmartAntenna proposes a novel approach to extend wireless communication, focusing on autonomous orientation to extend range and optimize performance.","Through meticulous evaluation, various aspects of its functionality were assessed, revealing both strengths and areas for improvement.","Notably, the antenna tracking mechanism exhibited remarkable efficacy.","The SmartAntenna demonstrated robust functionality throughout extensive testing, underscoring its reliability even amidst complex operational scenarios.","However, challenges emerged during target tracking, particularly evident in 360-degree sweeps, necessitating further refinement to enhance accuracy.","Despite reliance on the HC-12 module, LoRa, performance limitations surfaced, prompting concerns regarding its suitability for production systems, especially within noisy frequency bands.","Nevertheless, the SmartAntenna's adaptability across various wireless technologies holds promise, opening avenues for extended communication ranges and diverse applications.","SmartAntenna research contributes valuable insights into optimizing wireless communication systems, paving the way for enhanced performance and expanded capabilities in diverse operational environments."],"url":"http://arxiv.org/abs/2405.11411v1","category":"eess.SY"}
{"created":"2024-05-18 19:53:07","title":"Centralized Gradient-Based Reconstruction for Wall Modelled Large Eddy Simulations of Hypersonic Boundary Layer Transition","abstract":"In this study, we introduce a robust central Gradient-Based Reconstruction (GBR) scheme for the compressible Navier-Stokes equations. The method leverages transformation to characteristic space, allowing selective treatment of waves from the compressible Euler equations. By averaging left- and right-biased state interpolations, a central scheme is achieved for all but the acoustic waves, which require upwinding for stability. Distinct differences were observed between transformations using either primitive or conservative variables. We evaluated the method's robustness and superiority using benchmark problems, including the two-dimensional shock entropy problem, two-dimensional viscous shock tube, and three-dimensional inviscid Taylor-Green vortex. Subsequently, we assessed the method in the context of Wall Modelled Large Eddy Simulations (WMLES), where coarse grids are used to reduce computational cost but also introduce substantial numerical dissipation. Using WMLES, we simulated oblique shock impingement on a Mach 6 disturbed boundary layer and a Mach 7.7 flow over a $15^{\\circ}$ compression ramp. Our findings reveal that: 1) transformation to characteristic space using conservative variables leads to more accurate results; 2) minimizing numerical dissipation through centralized interpolation is crucial. In the compression ramp case, boundary layer separation was shifted slightly upstream, and there was an over-prediction of wall heating, likely attributable to the equilibrium-assuming wall model. Overall, this work showcases the method's potential in accurately capturing complex flow dynamics with reduced numerical dissipation.","sentences":["In this study, we introduce a robust central Gradient-Based Reconstruction (GBR) scheme for the compressible Navier-Stokes equations.","The method leverages transformation to characteristic space, allowing selective treatment of waves from the compressible Euler equations.","By averaging left- and right-biased state interpolations, a central scheme is achieved for all but the acoustic waves, which require upwinding for stability.","Distinct differences were observed between transformations using either primitive or conservative variables.","We evaluated the method's robustness and superiority using benchmark problems, including the two-dimensional shock entropy problem, two-dimensional viscous shock tube, and three-dimensional inviscid Taylor-Green vortex.","Subsequently, we assessed the method in the context of Wall Modelled Large Eddy Simulations (WMLES), where coarse grids are used to reduce computational cost but also introduce substantial numerical dissipation.","Using WMLES, we simulated oblique shock impingement on a Mach 6 disturbed boundary layer and a Mach 7.7 flow over a $15^{\\circ}$ compression ramp.","Our findings reveal that: 1) transformation to characteristic space using conservative variables leads to more accurate results; 2) minimizing numerical dissipation through centralized interpolation is crucial.","In the compression ramp case, boundary layer separation was shifted slightly upstream, and there was an over-prediction of wall heating, likely attributable to the equilibrium-assuming wall model.","Overall, this work showcases the method's potential in accurately capturing complex flow dynamics with reduced numerical dissipation."],"url":"http://arxiv.org/abs/2405.11376v1","category":"physics.flu-dyn"}
{"created":"2024-05-18 17:38:25","title":"Unlock the Power of Algorithm Features: A Generalization Analysis for Algorithm Selection","abstract":"In the field of algorithm selection research, the discussion surrounding algorithm features has been significantly overshadowed by the emphasis on problem features. Although a few empirical studies have yielded evidence regarding the effectiveness of algorithm features, the potential benefits of incorporating algorithm features into algorithm selection models and their suitability for different scenarios remain unclear. It is evident that relying solely on empirical research cannot adequately elucidate the mechanisms underlying performance variations. In this paper, we address this gap by proposing the first provable guarantee for algorithm selection based on algorithm features, taking a generalization perspective. We analyze the benefits and costs associated with algorithm features and investigate how the generalization error is affected by several factors. Specifically, we examine adaptive and predefined algorithm features under transductive and inductive learning paradigms, respectively, and derive upper bounds for the generalization error based on their model's Rademacher complexity. Our theoretical findings not only provide tight upper bounds, but also offer analytical insights into the impact of various factors, including model complexity, the number of problem instances and candidate algorithms, model parameters and feature values, and distributional differences between the training and test sets. Notably, we demonstrate that algorithm feature-based models outperform traditional models relying solely on problem features in complex multi-algorithm scenarios in terms of generalization, and are particularly well-suited for deployment in scenarios under distribution shifts, where the generalization error exhibits a positive correlation with the chi-square distance between training and test sets.","sentences":["In the field of algorithm selection research, the discussion surrounding algorithm features has been significantly overshadowed by the emphasis on problem features.","Although a few empirical studies have yielded evidence regarding the effectiveness of algorithm features, the potential benefits of incorporating algorithm features into algorithm selection models and their suitability for different scenarios remain unclear.","It is evident that relying solely on empirical research cannot adequately elucidate the mechanisms underlying performance variations.","In this paper, we address this gap by proposing the first provable guarantee for algorithm selection based on algorithm features, taking a generalization perspective.","We analyze the benefits and costs associated with algorithm features and investigate how the generalization error is affected by several factors.","Specifically, we examine adaptive and predefined algorithm features under transductive and inductive learning paradigms, respectively, and derive upper bounds for the generalization error based on their model's Rademacher complexity.","Our theoretical findings not only provide tight upper bounds, but also offer analytical insights into the impact of various factors, including model complexity, the number of problem instances and candidate algorithms, model parameters and feature values, and distributional differences between the training and test sets.","Notably, we demonstrate that algorithm feature-based models outperform traditional models relying solely on problem features in complex multi-algorithm scenarios in terms of generalization, and are particularly well-suited for deployment in scenarios under distribution shifts, where the generalization error exhibits a positive correlation with the chi-square distance between training and test sets."],"url":"http://arxiv.org/abs/2405.11349v1","category":"cs.LG"}
{"created":"2024-05-18 17:10:45","title":"Detection and Prediction of Future Massive Black Hole Mergers with Machine Learning and Truncated Waveforms","abstract":"We present a novel machine learning framework tailored to detect massive black hole binaries observed by spaceborne gravitational wave detectors like the Laser Interferometer Space Antenna (LISA) and predict their future merger times. The detection is performed via convolutional neural networks that analyze time-evolving Time-Delay Interferometry (TDI) spectrograms and utilize variations in signal power to trigger alerts. The prediction of future merger times is accomplished with reinforcement learning. Here, the proposed algorithm dynamically refines time-to-merger predictions by assimilating new data as it becomes available. Deep Q-learning serves as the core technique of the approach, utilizing a neural network to estimate Q-values throughout the observational state space. To enhance robust learning in a noisy environment, we integrate an actor-critic mechanism that segregates action proposals from their evaluation, thus harnessing the advantages of policy-based and value-based learning paradigms. We leverage merger estimation obtained via template matching with truncated waveforms to generate rewards for the reinforcement learning agent. These estimations come with uncertainties, which magnify as the merger event stretches further into the future. The reinforcement learning setup is shown to adapt to these uncertainties by employing a policy fine-tuning approach, ensuring the reliability of predictions despite the varying degrees of template-matching precision. The algorithm denotes a first step toward a low-latency model that provides early warnings for impending transient phenomena. By delivering timely and accurate forecasts of merger events, the framework supports the coordination of gravitational wave observations with accompanied electromagnetic counterparts, thus enhancing the prospects of multi-messenger astronomy. We use the LISA Spritz data challenge for validation.","sentences":["We present a novel machine learning framework tailored to detect massive black hole binaries observed by spaceborne gravitational wave detectors like the Laser Interferometer Space Antenna (LISA) and predict their future merger times.","The detection is performed via convolutional neural networks that analyze time-evolving Time-Delay Interferometry (TDI) spectrograms and utilize variations in signal power to trigger alerts.","The prediction of future merger times is accomplished with reinforcement learning.","Here, the proposed algorithm dynamically refines time-to-merger predictions by assimilating new data as it becomes available.","Deep Q-learning serves as the core technique of the approach, utilizing a neural network to estimate Q-values throughout the observational state space.","To enhance robust learning in a noisy environment, we integrate an actor-critic mechanism that segregates action proposals from their evaluation, thus harnessing the advantages of policy-based and value-based learning paradigms.","We leverage merger estimation obtained via template matching with truncated waveforms to generate rewards for the reinforcement learning agent.","These estimations come with uncertainties, which magnify as the merger event stretches further into the future.","The reinforcement learning setup is shown to adapt to these uncertainties by employing a policy fine-tuning approach, ensuring the reliability of predictions despite the varying degrees of template-matching precision.","The algorithm denotes a first step toward a low-latency model that provides early warnings for impending transient phenomena.","By delivering timely and accurate forecasts of merger events, the framework supports the coordination of gravitational wave observations with accompanied electromagnetic counterparts, thus enhancing the prospects of multi-messenger astronomy.","We use the LISA Spritz data challenge for validation."],"url":"http://arxiv.org/abs/2405.11340v1","category":"astro-ph.IM"}
{"created":"2024-05-18 15:26:41","title":"Neural Randomized Planning for Whole Body Robot Motion","abstract":"Robot motion planning has made vast advances over the past decades, but the challenge remains: robot mobile manipulators struggle to plan long-range whole-body motion in common household environments in real time, because of high-dimensional robot configuration space and complex environment geometry. To tackle the challenge, this paper proposes Neural Randomized Planner (NRP), which combines a global sampling-based motion planning (SBMP) algorithm and a local neural sampler. Intuitively, NRP uses the search structure inside the global planner to stitch together learned local sampling distributions to form a global sampling distribution adaptively. It benefits from both learning and planning. Locally, it tackles high dimensionality by learning to sample in promising regions from data, with a rich neural network representation. Globally, it composes the local sampling distributions through planning and exploits local geometric similarity to scale up to complex environments. Experiments both in simulation and on a real robot show \\NRP yields superior performance compared to some of the best classical and learning-enhanced SBMP algorithms. Further, despite being trained in simulation, NRP demonstrates zero-shot transfer to a real robot operating in novel household environments, without any fine-tuning or manual adaptation.","sentences":["Robot motion planning has made vast advances over the past decades, but the challenge remains: robot mobile manipulators struggle to plan long-range whole-body motion in common household environments in real time, because of high-dimensional robot configuration space and complex environment geometry.","To tackle the challenge, this paper proposes Neural Randomized Planner (NRP), which combines a global sampling-based motion planning (SBMP) algorithm and a local neural sampler.","Intuitively, NRP uses the search structure inside the global planner to stitch together learned local sampling distributions to form a global sampling distribution adaptively.","It benefits from both learning and planning.","Locally, it tackles high dimensionality by learning to sample in promising regions from data, with a rich neural network representation.","Globally, it composes the local sampling distributions through planning and exploits local geometric similarity to scale up to complex environments.","Experiments both in simulation and on a real robot show \\NRP yields superior performance compared to some of the best classical and learning-enhanced SBMP algorithms.","Further, despite being trained in simulation, NRP demonstrates zero-shot transfer to a real robot operating in novel household environments, without any fine-tuning or manual adaptation."],"url":"http://arxiv.org/abs/2405.11317v1","category":"cs.RO"}
{"created":"2024-05-18 14:41:54","title":"Meta Reinforcement Learning for Resource Allocation in Multi-Antenna UAV Network with Rate Splitting Multiple Access","abstract":"Unmanned aerial vehicles (UAVs) with multiple antennas have recently been explored to improve capacity in wireless networks. However, the strict energy constraint of UAVs, given their simultaneous flying and communication tasks, renders the exploration of energy-efficient multi-antenna techniques indispensable for UAVs. Meanwhile, lens antenna subarray (LAS) emerges as a promising energy-efficient solution that has not been previously harnessed for this purpose. In this paper, we propose a LAS-aided multi-antenna UAV to serve ground users in the downlink transmission of the terahertz (THz) band, utilizing rate splitting multiple access (RSMA) for effective beam division multiplexing. We formulate an optimization problem of maximizing the total system spectral efficiency (SE). This involves optimizing the UAV's transmit beamforming and the common rate of RSMA. By recasting the optimization problem into a Markov decision process (MDP), we propose a deep deterministic policy gradient (DDPG)-based resource allocation mechanism tailored to capture problem dynamics and optimize its variables. Moreover, given the UAV's frequent mobility and consequential system reconfigurations, we fortify the trained DDPG model with a meta-learning strategy, enhancing its adaptability to system variations. Numerically, more than 20\\% energy efficiency gain is achieved by our proposed LAS-aided multi-antenna UAV equipped with 4 lenses, compared to a single-lens UAV. Simulations also demonstrate that at a signal-to-noise (SNR) of 10 dB, the incorporation of RSMA results in a 22\\% SE enhancement over conventional orthogonal beam division multiple access. Furthermore, the overall system SE improves by 27\\%, when meta-learning is employed for fine-tuning the conventional DDPG method in literature.","sentences":["Unmanned aerial vehicles (UAVs) with multiple antennas have recently been explored to improve capacity in wireless networks.","However, the strict energy constraint of UAVs, given their simultaneous flying and communication tasks, renders the exploration of energy-efficient multi-antenna techniques indispensable for UAVs.","Meanwhile, lens antenna subarray (LAS) emerges as a promising energy-efficient solution that has not been previously harnessed for this purpose.","In this paper, we propose a LAS-aided multi-antenna UAV to serve ground users in the downlink transmission of the terahertz (THz) band, utilizing rate splitting multiple access (RSMA) for effective beam division multiplexing.","We formulate an optimization problem of maximizing the total system spectral efficiency (SE).","This involves optimizing the UAV's transmit beamforming and the common rate of RSMA.","By recasting the optimization problem into a Markov decision process (MDP), we propose a deep deterministic policy gradient (DDPG)-based resource allocation mechanism tailored to capture problem dynamics and optimize its variables.","Moreover, given the UAV's frequent mobility and consequential system reconfigurations, we fortify the trained DDPG model with a meta-learning strategy, enhancing its adaptability to system variations.","Numerically, more than 20\\% energy efficiency gain is achieved by our proposed LAS-aided multi-antenna UAV equipped with 4 lenses, compared to a single-lens UAV.","Simulations also demonstrate that at a signal-to-noise (SNR) of 10 dB, the incorporation of RSMA results in a 22\\% SE enhancement over conventional orthogonal beam division multiple access.","Furthermore, the overall system SE improves by 27\\%, when meta-learning is employed for fine-tuning the conventional DDPG method in literature."],"url":"http://arxiv.org/abs/2405.11306v1","category":"eess.SP"}
{"created":"2024-05-18 13:28:51","title":"Diffusion Model Driven Test-Time Image Adaptation for Robust Skin Lesion Classification","abstract":"Deep learning-based diagnostic systems have demonstrated potential in skin disease diagnosis. However, their performance can easily degrade on test domains due to distribution shifts caused by input-level corruptions, such as imaging equipment variability, brightness changes, and image blur. This will reduce the reliability of model deployment in real-world scenarios. Most existing solutions focus on adapting the source model through retraining on different target domains. Although effective, this retraining process is sensitive to the amount of data and the hyperparameter configuration for optimization. In this paper, we propose a test-time image adaptation method to enhance the accuracy of the model on test data by simultaneously updating and predicting test images. We modify the target test images by projecting them back to the source domain using a diffusion model. Specifically, we design a structure guidance module that adds refinement operations through low-pass filtering during reverse sampling, regularizing the diffusion to preserve structural information. Additionally, we introduce a self-ensembling scheme automatically adjusts the reliance on adapted and unadapted inputs, enhancing adaptation robustness by rejecting inappropriate generative modeling results. To facilitate this study, we constructed the ISIC2019-C and Dermnet-C corruption robustness evaluation benchmarks. Extensive experiments on the proposed benchmarks demonstrate that our method makes the classifier more robust across various corruptions, architectures, and data regimes. Our datasets and code will be available at \\url{https://github.com/minghu0830/Skin-TTA_Diffusion}.","sentences":["Deep learning-based diagnostic systems have demonstrated potential in skin disease diagnosis.","However, their performance can easily degrade on test domains due to distribution shifts caused by input-level corruptions, such as imaging equipment variability, brightness changes, and image blur.","This will reduce the reliability of model deployment in real-world scenarios.","Most existing solutions focus on adapting the source model through retraining on different target domains.","Although effective, this retraining process is sensitive to the amount of data and the hyperparameter configuration for optimization.","In this paper, we propose a test-time image adaptation method to enhance the accuracy of the model on test data by simultaneously updating and predicting test images.","We modify the target test images by projecting them back to the source domain using a diffusion model.","Specifically, we design a structure guidance module that adds refinement operations through low-pass filtering during reverse sampling, regularizing the diffusion to preserve structural information.","Additionally, we introduce a self-ensembling scheme automatically adjusts the reliance on adapted and unadapted inputs, enhancing adaptation robustness by rejecting inappropriate generative modeling results.","To facilitate this study, we constructed the ISIC2019-C and Dermnet-C corruption robustness evaluation benchmarks.","Extensive experiments on the proposed benchmarks demonstrate that our method makes the classifier more robust across various corruptions, architectures, and data regimes.","Our datasets and code will be available at \\url{https://github.com/minghu0830/Skin-TTA_Diffusion}."],"url":"http://arxiv.org/abs/2405.11289v1","category":"eess.IV"}
{"created":"2024-05-18 11:49:09","title":"HR Human: Modeling Human Avatars with Triangular Mesh and High-Resolution Textures from Videos","abstract":"Recently, implicit neural representation has been widely used to generate animatable human avatars. However, the materials and geometry of those representations are coupled in the neural network and hard to edit, which hinders their application in traditional graphics engines. We present a framework for acquiring human avatars that are attached with high-resolution physically-based material textures and triangular mesh from monocular video. Our method introduces a novel information fusion strategy to combine the information from the monocular video and synthesize virtual multi-view images to tackle the sparsity of the input view. We reconstruct humans as deformable neural implicit surfaces and extract triangle mesh in a well-behaved pose as the initial mesh of the next stage. In addition, we introduce an approach to correct the bias for the boundary and size of the coarse mesh extracted. Finally, we adapt prior knowledge of the latent diffusion model at super-resolution in multi-view to distill the decomposed texture. Experiments show that our approach outperforms previous representations in terms of high fidelity, and this explicit result supports deployment on common renderers.","sentences":["Recently, implicit neural representation has been widely used to generate animatable human avatars.","However, the materials and geometry of those representations are coupled in the neural network and hard to edit, which hinders their application in traditional graphics engines.","We present a framework for acquiring human avatars that are attached with high-resolution physically-based material textures and triangular mesh from monocular video.","Our method introduces a novel information fusion strategy to combine the information from the monocular video and synthesize virtual multi-view images to tackle the sparsity of the input view.","We reconstruct humans as deformable neural implicit surfaces and extract triangle mesh in a well-behaved pose as the initial mesh of the next stage.","In addition, we introduce an approach to correct the bias for the boundary and size of the coarse mesh extracted.","Finally, we adapt prior knowledge of the latent diffusion model at super-resolution in multi-view to distill the decomposed texture.","Experiments show that our approach outperforms previous representations in terms of high fidelity, and this explicit result supports deployment on common renderers."],"url":"http://arxiv.org/abs/2405.11270v1","category":"cs.CV"}
{"created":"2024-05-18 11:26:45","title":"MAMCA -- Optimal on Accuracy and Efficiency for Automatic Modulation Classification with Extended Signal Length","abstract":"With the rapid growth of the Internet of Things ecosystem, Automatic Modulation Classification (AMC) has become increasingly paramount. However, extended signal lengths offer a bounty of information, yet impede the model's adaptability, introduce more noise interference, extend the training and inference time, and increase storage overhead. To bridge the gap between these requisites, we propose a novel AMC framework, designated as the Mamba-based Automatic Modulation ClassificAtion (MAMCA). Our method adeptly addresses the accuracy and efficiency requirements for long-sequence AMC. Specifically, we introduce the Selective State Space Model as the backbone, enhancing the model efficiency by reducing the dimensions of the state matrices and diminishing the frequency of information exchange across GPU memories. We design a denoising-capable unit to elevate the network's performance under low signal-to-noise radio. Rigorous experimental evaluations on the publicly available dataset RML2016.10, along with our synthetic dataset within multiple quadrature amplitude modulations and lengths, affirm that MAMCA delivers superior recognition accuracy while necessitating minimal computational time and memory occupancy. Codes are available on https://github.com/ZhangYezhuo/MAMCA.","sentences":["With the rapid growth of the Internet of Things ecosystem, Automatic Modulation Classification (AMC) has become increasingly paramount.","However, extended signal lengths offer a bounty of information, yet impede the model's adaptability, introduce more noise interference, extend the training and inference time, and increase storage overhead.","To bridge the gap between these requisites, we propose a novel AMC framework, designated as the Mamba-based Automatic Modulation ClassificAtion (MAMCA).","Our method adeptly addresses the accuracy and efficiency requirements for long-sequence AMC.","Specifically, we introduce the Selective State Space Model as the backbone, enhancing the model efficiency by reducing the dimensions of the state matrices and diminishing the frequency of information exchange across GPU memories.","We design a denoising-capable unit to elevate the network's performance under low signal-to-noise radio.","Rigorous experimental evaluations on the publicly available dataset RML2016.10, along with our synthetic dataset within multiple quadrature amplitude modulations and lengths, affirm that MAMCA delivers superior recognition accuracy while necessitating minimal computational time and memory occupancy.","Codes are available on https://github.com/ZhangYezhuo/MAMCA."],"url":"http://arxiv.org/abs/2405.11263v1","category":"eess.SP"}
{"created":"2024-05-18 09:29:00","title":"TriLoRA: Integrating SVD for Advanced Style Personalization in Text-to-Image Generation","abstract":"As deep learning technology continues to advance, image generation models, especially models like Stable Diffusion, are finding increasingly widespread application in visual arts creation. However, these models often face challenges such as overfitting, lack of stability in generated results, and difficulties in accurately capturing the features desired by creators during the fine-tuning process. In response to these challenges, we propose an innovative method that integrates Singular Value Decomposition (SVD) into the Low-Rank Adaptation (LoRA) parameter update strategy, aimed at enhancing the fine-tuning efficiency and output quality of image generation models. By incorporating SVD within the LoRA framework, our method not only effectively reduces the risk of overfitting but also enhances the stability of model outputs, and captures subtle, creator-desired feature adjustments more accurately. We evaluated our method on multiple datasets, and the results show that, compared to traditional fine-tuning methods, our approach significantly improves the model's generalization ability and creative flexibility while maintaining the quality of generation. Moreover, this method maintains LoRA's excellent performance under resource-constrained conditions, allowing for significant improvements in image generation quality without sacrificing the original efficiency and resource advantages.","sentences":["As deep learning technology continues to advance, image generation models, especially models like Stable Diffusion, are finding increasingly widespread application in visual arts creation.","However, these models often face challenges such as overfitting, lack of stability in generated results, and difficulties in accurately capturing the features desired by creators during the fine-tuning process.","In response to these challenges, we propose an innovative method that integrates Singular Value Decomposition (SVD) into the Low-Rank Adaptation (LoRA) parameter update strategy, aimed at enhancing the fine-tuning efficiency and output quality of image generation models.","By incorporating SVD within the LoRA framework, our method not only effectively reduces the risk of overfitting but also enhances the stability of model outputs, and captures subtle, creator-desired feature adjustments more accurately.","We evaluated our method on multiple datasets, and the results show that, compared to traditional fine-tuning methods, our approach significantly improves the model's generalization ability and creative flexibility while maintaining the quality of generation.","Moreover, this method maintains LoRA's excellent performance under resource-constrained conditions, allowing for significant improvements in image generation quality without sacrificing the original efficiency and resource advantages."],"url":"http://arxiv.org/abs/2405.11236v1","category":"cs.CV"}
{"created":"2024-05-20 17:55:53","title":"Blow-up solutions of the \"bad\" Boussinesq equation","abstract":"We study blow-up solutions of the ``bad\" Boussinesq equation, and prove that a wide range of asymptotic scenarios can happen. For example, for each $T>0$, $x_{0}\\in \\mathbb{R}$ and $\\delta \\in (0,1)$, we prove that there exist Schwartz class solutions $u(x,t)$ on $\\mathbb{R} \\times [0,T)$ such that $|u(x,t)| \\leq C \\frac{1+x^{2}}{(x-x_{0})^{2}}$ and $u(x_{0},t)\\asymp (T-t)^{-\\delta}$ as $t\\to T$.   We also prove that for any $q\\in \\mathbb{N}$, $T>0$, $x_{0}\\in \\mathbb{R}$, $\\delta \\in (0,\\frac{1}{2})$, there exist Schwartz class solutions $u(x,t)$ on $\\mathbb{R} \\times [0,T)$ such that (i) $|\\partial_{x}^{q_{1}}\\partial_{t}^{q_{2}}u(x,t)|\\leq C$ for each $q_{1},q_{2}\\in \\mathbb{N}$ such that $q_{1}+2q_{2}\\leq q$, (ii) $|\\partial_{x}^{q_{1}}\\partial_{t}^{q_{2}}u(x,t)| \\leq C \\frac{1+|x|}{|x-x_{0}|}$ for each $q_{1},q_{2}\\in \\mathbb{N}$ such that $q_{1}+2q_{2}= q+1$, (iii) $|\\partial_{x}^{q_{1}}\\partial_{t}^{q_{2}}u(x_{0},t)| \\asymp (T-t)^{-\\delta}$ as $t\\to T$ for each $q_{1},q_{2}\\in \\mathbb{N}$ such that $q_{1}+2q_{2}= q+1$.   In particular, when $q=0$, this result establishes the existence of wave-breaking solutions, i.e. solutions that remain bounded but whose $x$-derivative blows up in finite time.","sentences":["We study blow-up solutions of the ``bad\" Boussinesq equation, and prove that a wide range of asymptotic scenarios can happen.","For example, for each $T>0$, $x_{0}\\in \\mathbb{R}$ and $\\delta \\in (0,1)$, we prove that there exist Schwartz class solutions $u(x,t)$ on $\\mathbb{R} \\times","[0,T)$ such that $|u(x,t)| \\leq C \\frac{1+x^{2}}{(x-x_{0})^{2}}$ and $u(x_{0},t)\\asymp (T-t)^{-\\delta}$ as $t\\to T$.   ","We also prove that for any $q\\in \\mathbb{N}$, $T>0$, $x_{0}\\in \\mathbb{R}$, $\\delta \\in (0,\\frac{1}{2})$, there exist Schwartz class solutions $u(x,t)$ on $\\mathbb{R} \\times","[0,T)$ such that (i) $|\\partial_{x}^{q_{1}}\\partial_{t}^{q_{2}}u(x,t)|\\leq C$ for each $q_{1},q_{2}\\in \\mathbb{N}$ such that $q_{1}+2q_{2}\\leq q$, (ii) $|\\partial_{x}^{q_{1}}\\partial_{t}^{q_{2}}u(x,t)| \\leq C \\frac{1+|x|}{|x-x_{0}|}$ for each $q_{1},q_{2}\\in \\mathbb{N}$ such that $q_{1}+2q_{2}= q+1$, (iii) $|\\partial_{x}^{q_{1}}\\partial_{t}^{q_{2}}u(x_{0},t)| \\asymp (T-t)^{-\\delta}$ as $t\\to T$ for each $q_{1},q_{2}\\in \\mathbb{N}$ such that $q_{1}+2q_{2}= q+1$.   In particular, when $q=0$, this result establishes the existence of wave-breaking solutions, i.e. solutions that remain bounded but whose $x$-derivative blows up in finite time."],"url":"http://arxiv.org/abs/2405.12210v1","category":"math.AP"}
{"created":"2024-05-20 17:32:16","title":"YASTN: Yet another symmetric tensor networks; A Python library for abelian symmetric tensor network calculations","abstract":"We present an open-source tensor network Python library for quantum many-body simulations. At its core is an abelian-symmetric tensor, implemented as a sparse block structure managed by logical layer on top of dense multi-dimensional array backend. This serves as the basis for higher-level tensor networks algorithms, operating on matrix product states and projected entangled pair states, implemented here. Using appropriate backend, such as PyTorch, gives direct access to automatic differentiation (AD) for cost-function gradient calculations and execution on GPUs or other supported accelerators. We show the library performance in simulations with infinite projected entangled-pair states, such as finding the ground states with AD, or simulating thermal states of the Hubbard model via imaginary time evolution. We quantify sources of performance gains in those challenging examples allowed by utilizing symmetries.","sentences":["We present an open-source tensor network Python library for quantum many-body simulations.","At its core is an abelian-symmetric tensor, implemented as a sparse block structure managed by logical layer on top of dense multi-dimensional array backend.","This serves as the basis for higher-level tensor networks algorithms, operating on matrix product states and projected entangled pair states, implemented here.","Using appropriate backend, such as PyTorch, gives direct access to automatic differentiation (AD) for cost-function gradient calculations and execution on GPUs or other supported accelerators.","We show the library performance in simulations with infinite projected entangled-pair states, such as finding the ground states with AD, or simulating thermal states of the Hubbard model via imaginary time evolution.","We quantify sources of performance gains in those challenging examples allowed by utilizing symmetries."],"url":"http://arxiv.org/abs/2405.12196v1","category":"cond-mat.str-el"}
{"created":"2024-05-20 17:17:44","title":"Training Data Attribution via Approximate Unrolled Differentation","abstract":"Many training data attribution (TDA) methods aim to estimate how a model's behavior would change if one or more data points were removed from the training set. Methods based on implicit differentiation, such as influence functions, can be made computationally efficient, but fail to account for underspecification, the implicit bias of the optimization algorithm, or multi-stage training pipelines. By contrast, methods based on unrolling address these issues but face scalability challenges. In this work, we connect the implicit-differentiation-based and unrolling-based approaches and combine their benefits by introducing Source, an approximate unrolling-based TDA method that is computed using an influence-function-like formula. While being computationally efficient compared to unrolling-based approaches, Source is suitable in cases where implicit-differentiation-based approaches struggle, such as in non-converged models and multi-stage training pipelines. Empirically, Source outperforms existing TDA techniques in counterfactual prediction, especially in settings where implicit-differentiation-based approaches fall short.","sentences":["Many training data attribution (TDA) methods aim to estimate how a model's behavior would change if one or more data points were removed from the training set.","Methods based on implicit differentiation, such as influence functions, can be made computationally efficient, but fail to account for underspecification, the implicit bias of the optimization algorithm, or multi-stage training pipelines.","By contrast, methods based on unrolling address these issues but face scalability challenges.","In this work, we connect the implicit-differentiation-based and unrolling-based approaches and combine their benefits by introducing Source, an approximate unrolling-based TDA method that is computed using an influence-function-like formula.","While being computationally efficient compared to unrolling-based approaches, Source is suitable in cases where implicit-differentiation-based approaches struggle, such as in non-converged models and multi-stage training pipelines.","Empirically, Source outperforms existing TDA techniques in counterfactual prediction, especially in settings where implicit-differentiation-based approaches fall short."],"url":"http://arxiv.org/abs/2405.12186v1","category":"cs.LG"}
{"created":"2024-05-20 17:06:52","title":"Regularization by rough Kraichnan noise for the generalised SQG equations","abstract":"We consider the generalised Surface Quasi-Geostrophic (gSQG) equations in $\\mathbb R^2$ with parameter $\\beta\\in (0,1)$, an active scalar model interpolating between SQG ($\\beta=1$) and the 2D Euler equations ($\\beta=0$) in vorticity form. Existence of weak $(L^1\\cap L^p)$-valued solutions in the deterministic setting is known, but their uniqueness is open. We show that the addition of a rough Stratonovich transport noise of Kraichnan type regularizes the PDE, providing strong existence and pathwise uniqueness of solutions for initial data $\\theta_0\\in L^1\\cap L^p$, for suitable values $p\\in[2,\\infty]$ related to the regularity degree $\\alpha$ of the noise and the singularity degree $\\beta$ of the velocity field; in particular, we can cover any $\\beta\\in (0,1)$ for suitable $\\alpha$ and $p$ and we can reach a suitable (\"critical\") threshold. The result also holds in the presence of external forcing $f\\in L^1_t (L^1\\cap L^p)$ and solutions are shown to depend continuously on the data of the problem; furthermore, they are well approximated by vanishing viscosity and regular approximations.","sentences":["We consider the generalised Surface Quasi-Geostrophic (gSQG) equations in $\\mathbb R^2$ with parameter $\\beta\\in (0,1)$, an active scalar model interpolating between SQG ($\\beta=1$) and the 2D Euler equations ($\\beta=0$) in vorticity form.","Existence of weak $(L^1\\cap L^p)$-valued solutions in the deterministic setting is known, but their uniqueness is open.","We show that the addition of a rough Stratonovich transport noise of Kraichnan type regularizes the PDE, providing strong existence and pathwise uniqueness of solutions for initial data $\\theta_0\\in L^1\\cap L^p$, for suitable values $p\\in[2,\\infty]$ related to the regularity degree $\\alpha$ of the noise and the singularity degree $\\beta$ of the velocity field; in particular, we can cover any $\\beta\\in (0,1)$ for suitable $\\alpha$ and $p$ and we can reach a suitable (\"critical\") threshold.","The result also holds in the presence of external forcing $f\\in L^1_t (L^1\\cap L^p)$ and solutions are shown to depend continuously on the data of the problem; furthermore, they are well approximated by vanishing viscosity and regular approximations."],"url":"http://arxiv.org/abs/2405.12181v1","category":"math.PR"}
{"created":"2024-05-20 16:13:07","title":"Alterations of electrocortical activity during hand movements induced by motor cortex glioma","abstract":"Glioma cells can reshape functional neuronal networks by hijacking neuronal synapses, leading to partial or complete neurological dysfunction. These mechanisms have been previously explored for language functions. However, the impact of glioma on sensorimotor functions is still unknown. Therefore, we recruited a control group of patients with unaffected motor cortex and a group of patients with glioma-infiltrated motor cortex, and recorded high-density electrocortical signals during finger movement tasks. The results showed that glioma suppresses task-related synchronization in the high-gamma band and reduces the power across all frequency bands. The resulting atypical motor information transmission model with discrete signaling pathways and delayed responses disrupts the stability of neuronal encoding patterns for finger movement kinematics across various temporal-spatial scales. These findings demonstrate that gliomas functionally invade neural circuits within the motor cortex. This result advances our understanding of motor function processing in chronic disease states, which is important to advance the surgical strategies and neurorehabilitation approaches for patients with malignant gliomas.","sentences":["Glioma cells can reshape functional neuronal networks by hijacking neuronal synapses, leading to partial or complete neurological dysfunction.","These mechanisms have been previously explored for language functions.","However, the impact of glioma on sensorimotor functions is still unknown.","Therefore, we recruited a control group of patients with unaffected motor cortex and a group of patients with glioma-infiltrated motor cortex, and recorded high-density electrocortical signals during finger movement tasks.","The results showed that glioma suppresses task-related synchronization in the high-gamma band and reduces the power across all frequency bands.","The resulting atypical motor information transmission model with discrete signaling pathways and delayed responses disrupts the stability of neuronal encoding patterns for finger movement kinematics across various temporal-spatial scales.","These findings demonstrate that gliomas functionally invade neural circuits within the motor cortex.","This result advances our understanding of motor function processing in chronic disease states, which is important to advance the surgical strategies and neurorehabilitation approaches for patients with malignant gliomas."],"url":"http://arxiv.org/abs/2405.12144v1","category":"q-bio.NC"}
{"created":"2024-05-20 16:00:36","title":"On the equivalence of derivatives for maps between Carnot groups","abstract":"This paper gives an alternate, elementary proof of a result of Magnani: maps between Carnot groups that are continuously differential in horizontal directions in the Euclidean sense are continuously Pansu differentiable. Our proof involves primarily Euclidean arguments. We simultaneously reprove Magnani's mean value estimate for continuously Pansu differentiable maps.","sentences":["This paper gives an alternate, elementary proof of a result of Magnani: maps between Carnot groups that are continuously differential in horizontal directions in the Euclidean sense are continuously Pansu differentiable.","Our proof involves primarily Euclidean arguments.","We simultaneously reprove Magnani's mean value estimate for continuously Pansu differentiable maps."],"url":"http://arxiv.org/abs/2405.12138v1","category":"math.MG"}
{"created":"2024-05-20 15:32:39","title":"Asteroseismic measurement of core and envelope rotation rates for 2006 red giant branch stars","abstract":"Tens of thousands of red giant stars in the Kepler data exhibit solar-like oscillations. Their oscillations enable us to study the internal physics from core to surface, such as differential rotation. However, envelope rotation rates have been measured for only a dozen RGB stars so far. The limited sample hinders the theoretical interpretation of angular momentum transport in post-main-sequence phases. We apply a new approach to calculate the asymptotic frequencies of mixed modes, which accounts for the so-called near-degeneracy effects and leads to more proper measurements of envelope rotation rates. By fitting these asymptotic expressions to the observations, we obtain measurements of the properties of g modes and mean core and envelope rotation rates. Among 2495 stars with clear mixed-mode patterns, we found that 800 show doublets and 1206 show triplets, doubling the size of pre-existing catalogues. This led us to discover an over-density of stars that narrowly distribute around a well-defined ridge in the plane showing core rotation rate versus evolution along the RGB. With this work, we also increase the sample of stars with measured envelope rotation rates by two orders of magnitude. We find a decreasing trend between envelope rotation rates and evolution, implying that the envelopes slow down with expansion, as expected. We find 243 stars whose envelope rotation rates are significantly larger than zero. For these stars, the core-to-envelope rotation ratios are around 20 and show a large spread with evolution. Several stars show extremely mild differential rotations, with core-to-surface ratios between 1 and 2. These stars also have very slow core rotation rates, suggesting that they go through a peculiar rotational evolution. We also discovered more stars located below the degeneracy sequence, which will provide the opportunity to study the history of possible stellar mergers.","sentences":["Tens of thousands of red giant stars in the Kepler data exhibit solar-like oscillations.","Their oscillations enable us to study the internal physics from core to surface, such as differential rotation.","However, envelope rotation rates have been measured for only a dozen RGB stars so far.","The limited sample hinders the theoretical interpretation of angular momentum transport in post-main-sequence phases.","We apply a new approach to calculate the asymptotic frequencies of mixed modes, which accounts for the so-called near-degeneracy effects and leads to more proper measurements of envelope rotation rates.","By fitting these asymptotic expressions to the observations, we obtain measurements of the properties of g modes and mean core and envelope rotation rates.","Among 2495 stars with clear mixed-mode patterns, we found that 800 show doublets and 1206 show triplets, doubling the size of pre-existing catalogues.","This led us to discover an over-density of stars that narrowly distribute around a well-defined ridge in the plane showing core rotation rate versus evolution along the RGB.","With this work, we also increase the sample of stars with measured envelope rotation rates by two orders of magnitude.","We find a decreasing trend between envelope rotation rates and evolution, implying that the envelopes slow down with expansion, as expected.","We find 243 stars whose envelope rotation rates are significantly larger than zero.","For these stars, the core-to-envelope rotation ratios are around 20 and show a large spread with evolution.","Several stars show extremely mild differential rotations, with core-to-surface ratios between 1 and 2.","These stars also have very slow core rotation rates, suggesting that they go through a peculiar rotational evolution.","We also discovered more stars located below the degeneracy sequence, which will provide the opportunity to study the history of possible stellar mergers."],"url":"http://arxiv.org/abs/2405.12116v1","category":"astro-ph.SR"}
{"created":"2024-05-20 15:13:22","title":"DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction","abstract":"Math world problems correction(MWPC) is a novel task dedicated to rectifying reasoning errors in the process of solving mathematical problems. In this paper, leveraging the advancements in large language models (LLMs), we address two key objectives:(1) Distinguishing between mathematical reasoning and error correction; (2) Exploring strategies to enhance the error correction capabilities of LLMs in mathematics to solve MWPC task. We noticed that, in real-time education,assisting students in recognizing their mistakes is more crucial than simply providing correct answers. However, current research tends to prioritize obtaining accurate solutions to math problems rather than correcting potentially incorrect ones. Therefore, we modify the research paradigm, demonstrating that improving mathematical reasoning abilities does not equate to mastery in error correction. Meanwhile, we propose a novel method called diagnostic-oriented promping(DOP) aimed at facilitating LLMs to excel in error correction. In experiments, DOP has shown outstanding performance, highlighting its significant impact. We argue that in mathematical education, the demand for outstanding correctors surpasses that for proficient reasoners. Codes and data are available on https://github.com/ChenhaoEcnuCS/Reason-Correct.","sentences":["Math world problems correction(MWPC) is a novel task dedicated to rectifying reasoning errors in the process of solving mathematical problems.","In this paper, leveraging the advancements in large language models (LLMs), we address two key objectives:(1)","Distinguishing between mathematical reasoning and error correction; (2) Exploring strategies to enhance the error correction capabilities of LLMs in mathematics to solve MWPC task.","We noticed that, in real-time education,assisting students in recognizing their mistakes is more crucial than simply providing correct answers.","However, current research tends to prioritize obtaining accurate solutions to math problems rather than correcting potentially incorrect ones.","Therefore, we modify the research paradigm, demonstrating that improving mathematical reasoning abilities does not equate to mastery in error correction.","Meanwhile, we propose a novel method called diagnostic-oriented promping(DOP) aimed at facilitating LLMs to excel in error correction.","In experiments, DOP has shown outstanding performance, highlighting its significant impact.","We argue that in mathematical education, the demand for outstanding correctors surpasses that for proficient reasoners.","Codes and data are available on https://github.com/ChenhaoEcnuCS/Reason-Correct."],"url":"http://arxiv.org/abs/2405.12100v1","category":"cs.CL"}
{"created":"2024-05-20 14:48:53","title":"A deep-water closure model for surface waves on axisymmetric swirling flows","abstract":"We consider the propagation of linear gravity waves on the free surface of steady, axisymmetric flows with purely azimuthal velocity. We propose a two-dimensional set of governing equations for surface waves valid in the deep-water limit. These equations come from a closure condition at the free surface that reduces the three-dimensional Euler equations in the bulk of the fluid to a set of two-dimensional equations applied only at the free surface. Since the closure condition is not obtained rigorously, it is validated numerically through comparisons with full three-dimensional calculations for vortex flows, including for a Lamb--Oseen vortex. The model presented here overcomes three limitations of existing models, namely: it is not restricted to potential base flows; it does not assume the base flow to have a flat free surface; and it does not require the use of infinite-order differential operators (such as $\\tanh(\\nabla)$) in the governing equations. The model can be applied in the case of rapid swirl (large Froude number) where the base free surface is substantially deformed. Since the model contains only derivatives of finite order, it is readily amenable to standard numerical study.","sentences":["We consider the propagation of linear gravity waves on the free surface of steady, axisymmetric flows with purely azimuthal velocity.","We propose a two-dimensional set of governing equations for surface waves valid in the deep-water limit.","These equations come from a closure condition at the free surface that reduces the three-dimensional Euler equations in the bulk of the fluid to a set of two-dimensional equations applied only at the free surface.","Since the closure condition is not obtained rigorously, it is validated numerically through comparisons with full three-dimensional calculations for vortex flows, including for a Lamb--Oseen vortex.","The model presented here overcomes three limitations of existing models, namely: it is not restricted to potential base flows; it does not assume the base flow to have a flat free surface; and it does not require the use of infinite-order differential operators (such as $\\tanh(\\nabla)$) in the governing equations.","The model can be applied in the case of rapid swirl (large Froude number) where the base free surface is substantially deformed.","Since the model contains only derivatives of finite order, it is readily amenable to standard numerical study."],"url":"http://arxiv.org/abs/2405.12078v1","category":"physics.flu-dyn"}
{"created":"2024-05-20 14:26:07","title":"NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo","abstract":"In this work we present a novel multi-view photometric stereo (PS) method. Like many works in 3D reconstruction we are leveraging neural shape representations and learnt renderers. However, our work differs from the state-of-the-art multi-view PS methods such as PS-NeRF or SuperNormal we explicity leverage per-pixel intensity renderings rather than relying mainly on estimated normals.   We model point light attenuation and explicitly raytrace cast shadows in order to best approximate each points incoming radiance. This is used as input to a fully neural material renderer that uses minimal prior assumptions and it is jointly optimised with the surface. Finally, estimated normal and segmentation maps can also incorporated in order to maximise the surface accuracy.   Our method is among the first to outperform the classical approach of DiLiGenT-MV and achieves average 0.2mm Chamfer distance for objects imaged at approx 1.5m distance away with approximate 400x400 resolution. Moreover, we show robustness to poor normals in low light count scenario, achieving 0.27mm Chamfer distance when pixel rendering is used instead of estimated normals.","sentences":["In this work we present a novel multi-view photometric stereo (PS) method.","Like many works in 3D reconstruction we are leveraging neural shape representations and learnt renderers.","However, our work differs from the state-of-the-art multi-view PS methods such as PS-NeRF or SuperNormal we explicity leverage per-pixel intensity renderings rather than relying mainly on estimated normals.   ","We model point light attenuation and explicitly raytrace cast shadows in order to best approximate each points incoming radiance.","This is used as input to a fully neural material renderer that uses minimal prior assumptions and it is jointly optimised with the surface.","Finally, estimated normal and segmentation maps can also incorporated in order to maximise the surface accuracy.   ","Our method is among the first to outperform the classical approach of DiLiGenT-MV and achieves average 0.2mm Chamfer distance for objects imaged at approx 1.5m distance away with approximate 400x400 resolution.","Moreover, we show robustness to poor normals in low light count scenario, achieving 0.27mm Chamfer distance when pixel rendering is used instead of estimated normals."],"url":"http://arxiv.org/abs/2405.12057v1","category":"cs.CV"}
{"created":"2024-05-20 13:39:58","title":"Strategy-Proof Auctions through Conformal Prediction","abstract":"Auctions are key for maximizing sellers' revenue and ensuring truthful bidding among buyers. Recently, an approach known as differentiable economics based on deep learning shows promise in learning optimal auction mechanisms for multiple items and participants. However, this approach has no guarantee of strategy-proofness at test time. Strategy-proofness is crucial as it ensures that buyers are incentivized to bid their true valuations, leading to optimal and fair auction outcomes without the risk of manipulation. Building on conformal prediction, we introduce a novel approach to achieve strategy-proofness with rigorous statistical guarantees. The key novelties of our method are: (i) the formulation of a regret prediction model, used to quantify at test time violations of strategy-proofness; and (ii) an auction acceptance rule that leverages the predicted regret to ensure that for a new auction, the data-driven mechanism meets the strategy-proofness requirement with high probability (e.g., 99\\%). Numerical experiments demonstrate the necessity for rigorous guarantees, the validity of our theoretical results, and the applicability of our proposed method.","sentences":["Auctions are key for maximizing sellers' revenue and ensuring truthful bidding among buyers.","Recently, an approach known as differentiable economics based on deep learning shows promise in learning optimal auction mechanisms for multiple items and participants.","However, this approach has no guarantee of strategy-proofness at test time.","Strategy-proofness is crucial as it ensures that buyers are incentivized to bid their true valuations, leading to optimal and fair auction outcomes without the risk of manipulation.","Building on conformal prediction, we introduce a novel approach to achieve strategy-proofness with rigorous statistical guarantees.","The key novelties of our method are: (i) the formulation of a regret prediction model, used to quantify at test time violations of strategy-proofness; and (ii) an auction acceptance rule that leverages the predicted regret to ensure that for a new auction, the data-driven mechanism meets the strategy-proofness requirement with high probability (e.g., 99\\%).","Numerical experiments demonstrate the necessity for rigorous guarantees, the validity of our theoretical results, and the applicability of our proposed method."],"url":"http://arxiv.org/abs/2405.12016v1","category":"cs.GT"}
{"created":"2024-05-20 12:34:17","title":"A fully discrete evolving surface finite element method for the Cahn-Hilliard equation with a regular potential","abstract":"We study two fully discrete evolving surface finite element schemes for the Cahn-Hilliard equation on an evolving surface, given a smooth potential with polynomial growth. In particular we establish optimal order error bounds for a (fully implicit) backward Euler time-discretisation, and an implicit-explicit time-discretisation, with isoparametric surface finite elements discretising space.","sentences":["We study two fully discrete evolving surface finite element schemes for the Cahn-Hilliard equation on an evolving surface, given a smooth potential with polynomial growth.","In particular we establish optimal order error bounds for a (fully implicit) backward Euler time-discretisation, and an implicit-explicit time-discretisation, with isoparametric surface finite elements discretising space."],"url":"http://arxiv.org/abs/2405.11984v1","category":"math.NA"}
{"created":"2024-05-20 11:48:13","title":"Sobolev regularity theory for stochastic reaction-diffusion-advection equations with spatially homogeneous colored noises and variable-order nonlocal operators","abstract":"This article investigates the existence, uniqueness, and regularity of solutions to nonlinear stochastic reaction-diffusion-advection equations (SRDAEs) with spatially homogeneous colored noises and variable-order nonlocal operators in mixed norm $L_q(L_p)$-spaces. We introduce a new condition (strongly reinforced Dalang's condition) on colored noise, which facilitates a deeper understanding of the complicated relation between nonlinearities and stochastic forces. Additionally, we establish the space-time H\\\"older type regularity of solutions.","sentences":["This article investigates the existence, uniqueness, and regularity of solutions to nonlinear stochastic reaction-diffusion-advection equations (SRDAEs) with spatially homogeneous colored noises and variable-order nonlocal operators in mixed norm $L_q(L_p)$-spaces.","We introduce a new condition (strongly reinforced Dalang's condition) on colored noise, which facilitates a deeper understanding of the complicated relation between nonlinearities and stochastic forces.","Additionally, we establish the space-time H\\\"older type regularity of solutions."],"url":"http://arxiv.org/abs/2405.11969v1","category":"math.PR"}
{"created":"2024-05-20 11:12:33","title":"A gluing construction of constant scalar curvature K\u00e4hler metrics of Poincar\u00e9 type","abstract":"Given a compact K\\\"ahler manifold with no non-trivial holomorphic vector field, assume it admits a constant scalar curvature K\\\"ahler metric. Fix finitely many points, we show the existence of constant scalar curvature K\\\"ahler metrics of Poincar\\'e type on the complement of these points in the compact manifold.","sentences":["Given a compact K\\\"ahler manifold with no non-trivial holomorphic vector field, assume it admits a constant scalar curvature K\\\"ahler metric.","Fix finitely many points, we show the existence of constant scalar curvature K\\\"ahler metrics of Poincar\\'e type on the complement of these points in the compact manifold."],"url":"http://arxiv.org/abs/2405.11952v1","category":"math.DG"}
{"created":"2024-05-20 10:29:21","title":"On the equivalence of two spinodal decomposition criteria with a case study of Fe${}_{15}$Co${}_{15}$Ni${}_{35}$Cu${}_{35}$ multicomponent alloy","abstract":"Spinodal decomposition in multicomponent alloys has attracted increasing attention due to its beneficial effect on their mechanical and functional properties and potential applications. Both based on the Cahn-Hillard equation, the reference element method (REM) and the projection matrix method (PMM) are the two main methods to predict the occurrence of spinodal decomposition in multicomponent alloys. In this work, it is mathematically proven that the two methods are equivalent, and therefore the advanced results based on one method can be applied to the other. Based on these methods, the $Fe{}_{15}$Co${}_{15}$Ni${}_{35}$Cu${}_{35}$ multicomponent alloy is designed as a case study. Experimental results confirm the spinodal decomposition in the heat-treated alloy, and its strength and ductility are simultaneously enhanced. This work can be the pavement for further theoretical and experimental studies on the spinodal decomposition in multicomponent alloys.","sentences":["Spinodal decomposition in multicomponent alloys has attracted increasing attention due to its beneficial effect on their mechanical and functional properties and potential applications.","Both based on the Cahn-Hillard equation, the reference element method (REM) and the projection matrix method (PMM) are the two main methods to predict the occurrence of spinodal decomposition in multicomponent alloys.","In this work, it is mathematically proven that the two methods are equivalent, and therefore the advanced results based on one method can be applied to the other.","Based on these methods, the $Fe{}_{15}$Co${}_{15}$Ni${}_{35}$Cu${}_{35}$ multicomponent alloy is designed as a case study.","Experimental results confirm the spinodal decomposition in the heat-treated alloy, and its strength and ductility are simultaneously enhanced.","This work can be the pavement for further theoretical and experimental studies on the spinodal decomposition in multicomponent alloys."],"url":"http://arxiv.org/abs/2405.11940v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 10:25:50","title":"Convergence of the area functional on spaces with lower Ricci bounds and applications","abstract":"The goal of this note is to prove convergence results w.r.t. the area functional on metric measure spaces with a lower Ricci curvature bound. The paper can be divided in two parts. In the first part, we show that the heat flow provides good approximation properties for the area functional on $RCD(K,\\infty)$ spaces, implying that in this setting the area formula for functions of bounded variation holds and that the area functional coincides with its relaxation. Moreover, we obtain partial regularity and uniqueness results for functions whose epigraphs are perimeter minimizing. In the second part of the paper, we consider Ricci limit spaces (and also finite dimensional $RCD(K,N)$ spaces for some results) and we show that, thanks to the previously obtained properties, minimizers of the area functional can be approximated with minimizers along the converging sequence of manifolds. As a first application, we show that minimizers of the area functional on non-collapsed Ricci limit spaces are locally Lipschitz and satisfy a-priori gradient estimates. Secondly, we obtain a Bernstein-type result for area minimizers with sublinear growth at infinity.","sentences":["The goal of this note is to prove convergence results w.r.t.","the area functional on metric measure spaces with a lower Ricci curvature bound.","The paper can be divided in two parts.","In the first part, we show that the heat flow provides good approximation properties for the area functional on $RCD(K,\\infty)$ spaces, implying that in this setting the area formula for functions of bounded variation holds and that the area functional coincides with its relaxation.","Moreover, we obtain partial regularity and uniqueness results for functions whose epigraphs are perimeter minimizing.","In the second part of the paper, we consider Ricci limit spaces (and also finite dimensional $RCD(K,N)$ spaces for some results) and we show that, thanks to the previously obtained properties, minimizers of the area functional can be approximated with minimizers along the converging sequence of manifolds.","As a first application, we show that minimizers of the area functional on non-collapsed Ricci limit spaces are locally Lipschitz and satisfy a-priori gradient estimates.","Secondly, we obtain a Bernstein-type result for area minimizers with sublinear growth at infinity."],"url":"http://arxiv.org/abs/2405.11938v1","category":"math.DG"}
{"created":"2024-05-20 10:03:26","title":"The Dirichlet problem for Monge-Amp\u00e8re type equations on Riemannian manifolds","abstract":"In this paper, we study the Dirichlet problem for Monge-Amp\\`ere type equations for $p$-plurisubharmonic functions on Riemannian manifolds. The $a$ $priori$ estimates up to the second order derivatives of solutions are established. The existence of a solution then follows by the continuity method.","sentences":["In this paper, we study the Dirichlet problem for Monge-Amp\\`ere type equations for $p$-plurisubharmonic functions on Riemannian manifolds.","The $a$ $priori$ estimates up to the second order derivatives of solutions are established.","The existence of a solution then follows by the continuity method."],"url":"http://arxiv.org/abs/2405.11925v1","category":"math.AP"}
{"created":"2024-05-20 09:58:03","title":"MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections","abstract":"3D Gaussian Splatting showcases notable advancements in photo-realistic and real-time novel view synthesis. However, it faces challenges in modeling mirror reflections, which exhibit substantial appearance variations from different viewpoints. To tackle this problem, we present MirrorGaussian, the first method for mirror scene reconstruction with real-time rendering based on 3D Gaussian Splatting. The key insight is grounded on the mirror symmetry between the real-world space and the virtual mirror space. We introduce an intuitive dual-rendering strategy that enables differentiable rasterization of both the real-world 3D Gaussians and the mirrored counterpart obtained by reflecting the former about the mirror plane. All 3D Gaussians are jointly optimized with the mirror plane in an end-to-end framework. MirrorGaussian achieves high-quality and real-time rendering in scenes with mirrors, empowering scene editing like adding new mirrors and objects. Comprehensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods, achieving state-of-the-art results. Project page: https://mirror-gaussian.github.io/.","sentences":["3D Gaussian Splatting showcases notable advancements in photo-realistic and real-time novel view synthesis.","However, it faces challenges in modeling mirror reflections, which exhibit substantial appearance variations from different viewpoints.","To tackle this problem, we present MirrorGaussian, the first method for mirror scene reconstruction with real-time rendering based on 3D Gaussian Splatting.","The key insight is grounded on the mirror symmetry between the real-world space and the virtual mirror space.","We introduce an intuitive dual-rendering strategy that enables differentiable rasterization of both the real-world 3D Gaussians and the mirrored counterpart obtained by reflecting the former about the mirror plane.","All 3D Gaussians are jointly optimized with the mirror plane in an end-to-end framework.","MirrorGaussian achieves high-quality and real-time rendering in scenes with mirrors, empowering scene editing like adding new mirrors and objects.","Comprehensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods, achieving state-of-the-art results.","Project page: https://mirror-gaussian.github.io/."],"url":"http://arxiv.org/abs/2405.11921v1","category":"cs.CV"}
{"created":"2024-05-20 09:33:27","title":"A comprehensive overview of deep learning techniques for 3D point cloud classification and semantic segmentation","abstract":"Point cloud analysis has a wide range of applications in many areas such as computer vision, robotic manipulation, and autonomous driving. While deep learning has achieved remarkable success on image-based tasks, there are many unique challenges faced by deep neural networks in processing massive, unordered, irregular and noisy 3D points. To stimulate future research, this paper analyzes recent progress in deep learning methods employed for point cloud processing and presents challenges and potential directions to advance this field. It serves as a comprehensive review on two major tasks in 3D point cloud processing-- namely, 3D shape classification and semantic segmentation.","sentences":["Point cloud analysis has a wide range of applications in many areas such as computer vision, robotic manipulation, and autonomous driving.","While deep learning has achieved remarkable success on image-based tasks, there are many unique challenges faced by deep neural networks in processing massive, unordered, irregular and noisy 3D points.","To stimulate future research, this paper analyzes recent progress in deep learning methods employed for point cloud processing and presents challenges and potential directions to advance this field.","It serves as a comprehensive review on two major tasks in 3D point cloud processing-- namely, 3D shape classification and semantic segmentation."],"url":"http://arxiv.org/abs/2405.11903v1","category":"cs.CV"}
{"created":"2024-05-20 09:31:19","title":"Global-in-time well-posedness of the compressible Navier-Stokes equations with striated density","abstract":"We first show local-in-time well-posedness of the compressible Navier-Stokes equations, assuming striated regularity while no other smoothness or smallness conditions on the initial density. With these local-in-time solutions served as blocks, for \\textit{less} regular initial data where the vacuum is permitted, the global-in-time well-posedness follows from the energy estimates and the propagated striated regularity of the density function, if the bulk viscosity coefficient is large enough in the two dimensional case. The global-in-time well-posedness holds also true in the three dimensional case, provided with large bulk viscosity coefficient together with small initial energy. This solves the density-patch problem in the exterior domain for the compressible model with $W^{2,p}$-Interfaces. Finally, the singular incompressible limit toward the inhomogenous incompressible model when the bulk viscosity coefficient tends to infinity is obtained.","sentences":["We first show local-in-time well-posedness of the compressible Navier-Stokes equations, assuming striated regularity while no other smoothness or smallness conditions on the initial density.","With these local-in-time solutions served as blocks, for \\textit{less} regular initial data where the vacuum is permitted, the global-in-time well-posedness follows from the energy estimates and the propagated striated regularity of the density function, if the bulk viscosity coefficient is large enough in the two dimensional case.","The global-in-time well-posedness holds also true in the three dimensional case, provided with large bulk viscosity coefficient together with small initial energy.","This solves the density-patch problem in the exterior domain for the compressible model with $W^{2,p}$-Interfaces.","Finally, the singular incompressible limit toward the inhomogenous incompressible model when the bulk viscosity coefficient tends to infinity is obtained."],"url":"http://arxiv.org/abs/2405.11900v1","category":"math.AP"}
{"created":"2024-05-20 08:01:47","title":"Godbillon-Vey type functional for almost contact manifolds","abstract":"Many contact metric manifolds are critical points of curvature functionals restricted to spaces of associated metrics. The Godbillon-Vey functional has never been considered in a variational context in contact geometry. Recently we extended this functional from foliations to arbitrary plane fields on a 3-dimensional manifold, so, the following question arises: can one use the Godbillon-Vey functional to find optimal almost contact manifolds? In the paper, we introduce a Godbillon-Vey type functional for a 3-dimensional almost contact manifold and find its Euler-Lagrange equations for all variations preserving the Reeb vector field. We construct critical (for our functional) 3-dimensional almost contact manifolds having a double-twisted product structure, these solutions belong to the class $C_{5}\\oplus C_{12}$ according to Chinea-Gonzalez classification.","sentences":["Many contact metric manifolds are critical points of curvature functionals restricted to spaces of associated metrics.","The Godbillon-Vey functional has never been considered in a variational context in contact geometry.","Recently we extended this functional from foliations to arbitrary plane fields on a 3-dimensional manifold, so, the following question arises: can one use the Godbillon-Vey functional to find optimal almost contact manifolds?","In the paper, we introduce a Godbillon-Vey type functional for a 3-dimensional almost contact manifold and find its Euler-Lagrange equations for all variations preserving the Reeb vector field.","We construct critical (for our functional) 3-dimensional almost contact manifolds having a double-twisted product structure, these solutions belong to the class $C_{5}\\oplus C_{12}$ according to Chinea-Gonzalez classification."],"url":"http://arxiv.org/abs/2405.11857v1","category":"math.DG"}
{"created":"2024-05-20 07:35:21","title":"High energy nucleus-nucleus collisions with accounting for Coulomb interaction","abstract":"The differential elastic cross sections of $^{12}$C - $^{12}$C, $^{16}$O - $^{16}$O and $^{20}$Ne - $^{20}$Ne nuclei scattering are calculated in the complete Glauber theory with the account of the modification due to Coulomb interaction and form factor effects. The role of the Coulomb interaction is shown to be significant mainly in the diffractive minima. The results of the complete Coulomb calculations are noticeably different from those obtained in the Born approximation.","sentences":["The differential elastic cross sections of $^{12}$C - $^{12}$C, $^{16}$O - $^{16}$O and $^{20}$Ne - $^{20}$Ne nuclei scattering are calculated in the complete Glauber theory with the account of the modification due to Coulomb interaction and form factor effects.","The role of the Coulomb interaction is shown to be significant mainly in the diffractive minima.","The results of the complete Coulomb calculations are noticeably different from those obtained in the Born approximation."],"url":"http://arxiv.org/abs/2405.11842v1","category":"nucl-th"}
{"created":"2024-05-20 06:20:53","title":"Blow-up phenomena and the local well-posedness of a Two-Component b-Family Equations in Besov spaces","abstract":"In this paper, we first establish the local well-posednesss of a Two-Component b-Family equations in nonhomogeneous Besov spaces $B^{1+\\frac 1 p}_{p,1}$ with $1\\leq p<+\\infty.$ Then we present a new blow-up result for the Two-Component b-Family equations.","sentences":["In this paper, we first establish the local well-posednesss of a Two-Component b-Family equations in nonhomogeneous Besov spaces $B^{1+\\frac 1 p}_{p,1}$ with $1\\leq p<+\\infty.$","Then we present a new blow-up result for the Two-Component b-Family equations."],"url":"http://arxiv.org/abs/2405.11813v1","category":"math.AP"}
{"created":"2024-05-20 05:46:41","title":"LSEnet: Lorentz Structural Entropy Neural Network for Deep Graph Clustering","abstract":"Graph clustering is a fundamental problem in machine learning. Deep learning methods achieve the state-of-the-art results in recent years, but they still cannot work without predefined cluster numbers. Such limitation motivates us to pose a more challenging problem of graph clustering with unknown cluster number. We propose to address this problem from a fresh perspective of graph information theory (i.e., structural information). In the literature, structural information has not yet been introduced to deep clustering, and its classic definition falls short of discrete formulation and modeling node features. In this work, we first formulate a differentiable structural information (DSI) in the continuous realm, accompanied by several theoretical results. By minimizing DSI, we construct the optimal partitioning tree where densely connected nodes in the graph tend to have the same assignment, revealing the cluster structure. DSI is also theoretically presented as a new graph clustering objective, not requiring the predefined cluster number. Furthermore, we design a neural LSEnet in the Lorentz model of hyperbolic space, where we integrate node features to structural information via manifold-valued graph convolution. Extensive empirical results on real graphs show the superiority of our approach.","sentences":["Graph clustering is a fundamental problem in machine learning.","Deep learning methods achieve the state-of-the-art results in recent years, but they still cannot work without predefined cluster numbers.","Such limitation motivates us to pose a more challenging problem of graph clustering with unknown cluster number.","We propose to address this problem from a fresh perspective of graph information theory (i.e., structural information).","In the literature, structural information has not yet been introduced to deep clustering, and its classic definition falls short of discrete formulation and modeling node features.","In this work, we first formulate a differentiable structural information (DSI) in the continuous realm, accompanied by several theoretical results.","By minimizing DSI, we construct the optimal partitioning tree where densely connected nodes in the graph tend to have the same assignment, revealing the cluster structure.","DSI is also theoretically presented as a new graph clustering objective, not requiring the predefined cluster number.","Furthermore, we design a neural LSEnet in the Lorentz model of hyperbolic space, where we integrate node features to structural information via manifold-valued graph convolution.","Extensive empirical results on real graphs show the superiority of our approach."],"url":"http://arxiv.org/abs/2405.11801v1","category":"cs.LG"}
{"created":"2024-05-20 04:26:05","title":"On critical double phase problems in $\\mathbb{R}^N$ involving variable exponents","abstract":"We establish a Lions-type concentration-compactness principle and its variant at infinity for Musielak-Orlicz-Sobolev spaces associated with a double phase operator with variable exponents. Based on these principles, we demonstrate the existence and concentration of solutions for a class of critical double phase equations of Schr\\\"odinger type in $\\mathbb{R}^N$ involving variable exponents with various types of potentials. Our growth condition is more appropriately suited compared to the existing works.","sentences":["We establish a Lions-type concentration-compactness principle and its variant at infinity for Musielak-Orlicz-Sobolev spaces associated with a double phase operator with variable exponents.","Based on these principles, we demonstrate the existence and concentration of solutions for a class of critical double phase equations of Schr\\\"odinger type in $\\mathbb{R}^N$ involving variable exponents with various types of potentials.","Our growth condition is more appropriately suited compared to the existing works."],"url":"http://arxiv.org/abs/2405.11774v1","category":"math.AP"}
{"created":"2024-05-20 03:46:57","title":"On the asymptotic stability of ground states of the pure power NLS on the line at 3rd and 4th order Fermi Golden Rule","abstract":"Assuming the nonlinear Fermi Golden Rule (FGR) and no resonance at the threshold of the continuous spectrum of the linearization and assuming furthermore as hypotheses the results proved numerically by Chang et al. \\cite{Chang} for the exponent $p\\in (3,5)$, we prove that the ground states of the nonlinear Schr\\\"odinger equation (NLS) with pure power nonlinearity of exponent $p$ in the line are asymptotically stable for a certain set of values of the exponent $p$ where the FGR occurs by means of a discrete mode 3rd or 4th order power interaction with the continuous mode. The argument is similar to our recent result valid for $p$ near 3 contained in \\cite{CM24D1}.","sentences":["Assuming the nonlinear Fermi Golden Rule (FGR) and no resonance at the threshold of the continuous spectrum of the linearization and assuming furthermore as hypotheses the results proved numerically by Chang et al. \\cite{Chang} for the exponent $p\\in (3,5)$, we prove that the ground states of the nonlinear Schr\\\"odinger equation (NLS) with pure power nonlinearity of exponent $p$ in the line are asymptotically stable for a certain set of values of the exponent $p$ where the FGR occurs by means of a discrete mode 3rd or 4th order power interaction with the continuous mode.","The argument is similar to our recent result valid for $p$ near 3 contained in \\cite{CM24D1}."],"url":"http://arxiv.org/abs/2405.11763v1","category":"math.AP"}
{"created":"2024-05-20 03:38:02","title":"Differential-phase-shift QKD with practical Mach-Zehnder interferometer","abstract":"Differential-phase-shift (DPS) quantum key distribution stands as a promising protocol due to its simple implementation, which can be realized with a train of coherent pulses and a passive measurement unit. Besides, this protocol has the advantage of being robust against imperfections in the light source. Unfortunately, however, as for the measurement unit, existing security proofs put unrealistic assumptions on it, which could be security loopholes in actual implementations. In this paper, we enhance the implementation security of the DPS protocol by incorporating a major imperfection in the measurement unit. Specifically, our proof enables us to employ practical beam splitters with a known range of the transmittance rather than the one with exactly $50\\%$, as was assumed in the existing security proofs. Our numerical simulations demonstrate that even with fluctuations of $\\pm0.5\\%$ in the transmittance from the ideal value, the key rate degrades only by a factor of 0.57. This result highlights the feasibility of the DPS protocol with practical measurement setups.","sentences":["Differential-phase-shift (DPS) quantum key distribution stands as a promising protocol due to its simple implementation, which can be realized with a train of coherent pulses and a passive measurement unit.","Besides, this protocol has the advantage of being robust against imperfections in the light source.","Unfortunately, however, as for the measurement unit, existing security proofs put unrealistic assumptions on it, which could be security loopholes in actual implementations.","In this paper, we enhance the implementation security of the DPS protocol by incorporating a major imperfection in the measurement unit.","Specifically, our proof enables us to employ practical beam splitters with a known range of the transmittance rather than the one with exactly $50\\%$, as was assumed in the existing security proofs.","Our numerical simulations demonstrate that even with fluctuations of $\\pm0.5\\%$ in the transmittance from the ideal value, the key rate degrades only by a factor of 0.57.","This result highlights the feasibility of the DPS protocol with practical measurement setups."],"url":"http://arxiv.org/abs/2405.11760v1","category":"quant-ph"}
{"created":"2024-05-20 03:28:40","title":"Analytically controlling laser-induced electron phase in sub-cycle motion","abstract":"Precise control of the electron phase accumulated during its sub-cycle motion within intense laser fields is essential in strong-field physics, yet remains mostly indirect and complicated so far. In this Letter, we develop a novel approach to control this sub-cycle electron phase by tuning a low-frequency electric field applied on a centrosymmetric gaseous target during its interaction with a few-cycle infrared laser pulse. Our method is based on a universal analytical relation between the low-frequency electric field and its induced harmonic frequency shift, derived by the strong-field approximation. This simple relation and its universality are confirmed numerically by directly solving the time-dependent Schr\\\"odinger equation. Moreover, we discuss the benefits of the discovered relation in \\textit{in situ} applications, including continuously and precisely tuning XUV waves and developing a new method of comprehensively sampling THz pulse.","sentences":["Precise control of the electron phase accumulated during its sub-cycle motion within intense laser fields is essential in strong-field physics, yet remains mostly indirect and complicated so far.","In this Letter, we develop a novel approach to control this sub-cycle electron phase by tuning a low-frequency electric field applied on a centrosymmetric gaseous target during its interaction with a few-cycle infrared laser pulse.","Our method is based on a universal analytical relation between the low-frequency electric field and its induced harmonic frequency shift, derived by the strong-field approximation.","This simple relation and its universality are confirmed numerically by directly solving the time-dependent Schr\\\"odinger equation.","Moreover, we discuss the benefits of the discovered relation in \\textit{in situ} applications, including continuously and precisely tuning XUV waves and developing a new method of comprehensively sampling THz pulse."],"url":"http://arxiv.org/abs/2405.11753v1","category":"physics.optics"}
{"created":"2024-05-20 03:13:00","title":"Wolff potentials and nonlocal equations of Lane-Emden type","abstract":"We consider nonlocal equations of the type \\[ (-\\Delta_{p})^{s}u = \\mu \\quad \\text{in }\\Omega, \\] where $\\Omega \\subset \\mathbb{R}^{n}$ is either a bounded domain or the whole $\\mathbb{R}^{n}$, $\\mu$ is a Radon measure on $\\Omega$, $0<s<1$ and $1<p<n/s$. Especially, we extend the existence, regularity and Wolff potential estimates for SOLA (Solutions Obtained as Limits of Approximations), established by Kuusi, Mingione, and Sire (Comm. Math. Phys. 337:1317--1368, 2015), to the strongly singular case $1<p\\le2-s/n$. Moreover, using Wolff potentials and Orlicz capacities, we present both a sufficient and a necessary conditions for the existence of SOLA to nonlocal equations of the type \\[ (-\\Delta_{p})^{s}u = P(u) + \\mu \\quad \\text{in }\\Omega, \\] where $P(\\cdot)$ is either a power function or an exponential function.","sentences":["We consider nonlocal equations of the type \\[ (-\\Delta_{p})^{s}u = \\mu \\quad \\text{in }\\Omega, \\] where $\\Omega \\subset \\mathbb{R}^{n}$ is either a bounded domain or the whole $\\mathbb{R}^{n}$, $\\mu$ is a Radon measure on $\\Omega$, $0<s<1$ and $1<p<n/s$. Especially, we extend the existence, regularity and Wolff potential estimates for SOLA (Solutions Obtained as Limits of Approximations), established by Kuusi, Mingione, and Sire (Comm.","Math.","Phys.","337:1317--1368, 2015), to the strongly singular case $1<p\\le2-s/n$. Moreover, using Wolff potentials and Orlicz capacities, we present both a sufficient and a necessary conditions for the existence of SOLA to nonlocal equations of the type \\[ (-\\Delta_{p})^{s}u = P(u)","+ \\mu \\quad \\text{in }\\Omega, \\] where $P(\\cdot)$ is either a power function or an exponential function."],"url":"http://arxiv.org/abs/2405.11747v1","category":"math.AP"}
{"created":"2024-05-20 00:52:19","title":"Blowup time and blowup rate for a pseudo-parabolic equation with singular potential","abstract":"This paper provides the upper and lower bounds of blowup time and blowup rate as well as the exponential growth estimate of blowup solutions for a pseudo-parabolic equation with singular potential. These results complement the ones obtained in the previous literature.","sentences":["This paper provides the upper and lower bounds of blowup time and blowup rate as well as the exponential growth estimate of blowup solutions for a pseudo-parabolic equation with singular potential.","These results complement the ones obtained in the previous literature."],"url":"http://arxiv.org/abs/2405.11707v1","category":"math.AP"}
{"created":"2024-05-19 22:58:22","title":"The Landis conjecture via Liouville comparison principle and criticality theory","abstract":"We give partial affirmative answers to Landis conjecture in all dimensions for two different types of linear, second order, elliptic operators in a domain $\\Omega\\subset \\mathbb{R}^N$. In particular, we provide a sharp decay criterion that ensures when a solution of a nonnegative Schr\\\"odinger equation in $\\mathbb{R}^N$ with a potential $V\\leq 1$ is trivial. Moreover, we address the analogue of Landis conjecture for quasilinear problems. Our approach relies on the application of Liouville comparison principles and criticality theory.","sentences":["We give partial affirmative answers to Landis conjecture in all dimensions for two different types of linear, second order, elliptic operators in a domain $\\Omega\\subset \\mathbb{R}^N$. In particular, we provide a sharp decay criterion that ensures when a solution of a nonnegative Schr\\\"odinger equation in $\\mathbb{R}^N$ with a potential $V\\leq 1$ is trivial.","Moreover, we address the analogue of Landis conjecture for quasilinear problems.","Our approach relies on the application of Liouville comparison principles and criticality theory."],"url":"http://arxiv.org/abs/2405.11695v1","category":"math.AP"}
{"created":"2024-05-19 22:39:24","title":"Analysis of the Pressure-Velocity boundary conditions for the projection method solution of the incompressible Navier-Stokes Equations","abstract":"The projection method is the standard approach for numerically integrating the incompressible Navier-Stokes equation initial-boundary-value problem. Typical boundary conditions specify either the velocity or the gradient velocity on the boundary. Here, we consider the pressure-tangential-velocity boundary condition in which the tangential components of the velocity and the pressure at the boundary are specified.","sentences":["The projection method is the standard approach for numerically integrating the incompressible Navier-Stokes equation initial-boundary-value problem.","Typical boundary conditions specify either the velocity or the gradient velocity on the boundary.","Here, we consider the pressure-tangential-velocity boundary condition in which the tangential components of the velocity and the pressure at the boundary are specified."],"url":"http://arxiv.org/abs/2405.11691v1","category":"physics.flu-dyn"}
{"created":"2024-05-19 22:04:11","title":"Learning Regularities from Data using Spiking Functions: A Theory","abstract":"Deep neural networks trained in an end-to-end manner are proven to be efficient in a wide range of machine learning tasks. However, there is one drawback of end-to-end learning: The learned features and information are implicitly represented in neural network parameters, which cannot be used as regularities, concepts or knowledge to explicitly represent the data probability distribution. To resolve this issue, we propose in this paper a new machine learning theory, which defines in mathematics what are regularities. Briefly, regularities are concise representations of the non-random features, or 'non-randomness' in the data probability distribution. Combining with information theory, we claim that regularities can also be regarded as a small amount of information encoding a large amount of information. Our theory is based on spiking functions. That is, if a function can react to, or spike on specific data samples more frequently than random noise inputs, we say that such a function discovers non-randomness from the data distribution, and encodes the non-randomness into regularities. Our theory also discusses applying multiple spiking functions to the same data distribution. In this process, we claim that the 'best' regularities, or the optimal spiking functions, are those who can capture the largest amount of information from the data distribution, and then encode the captured information in the most concise way. Theorems and hypotheses are provided to describe in mathematics what are 'best' regularities and optimal spiking functions. Finally, we propose a machine learning approach, which can potentially obtain the optimal spiking functions regarding the given dataset in practice.","sentences":["Deep neural networks trained in an end-to-end manner are proven to be efficient in a wide range of machine learning tasks.","However, there is one drawback of end-to-end learning: The learned features and information are implicitly represented in neural network parameters, which cannot be used as regularities, concepts or knowledge to explicitly represent the data probability distribution.","To resolve this issue, we propose in this paper a new machine learning theory, which defines in mathematics what are regularities.","Briefly, regularities are concise representations of the non-random features, or 'non-randomness' in the data probability distribution.","Combining with information theory, we claim that regularities can also be regarded as a small amount of information encoding a large amount of information.","Our theory is based on spiking functions.","That is, if a function can react to, or spike on specific data samples more frequently than random noise inputs, we say that such a function discovers non-randomness from the data distribution, and encodes the non-randomness into regularities.","Our theory also discusses applying multiple spiking functions to the same data distribution.","In this process, we claim that the 'best' regularities, or the optimal spiking functions, are those who can capture the largest amount of information from the data distribution, and then encode the captured information in the most concise way.","Theorems and hypotheses are provided to describe in mathematics what are 'best' regularities and optimal spiking functions.","Finally, we propose a machine learning approach, which can potentially obtain the optimal spiking functions regarding the given dataset in practice."],"url":"http://arxiv.org/abs/2405.11684v1","category":"cs.LG"}
{"created":"2024-05-19 21:53:36","title":"Conditionally-Conjugate Gaussian Process Factor Analysis for Spike Count Data via Data Augmentation","abstract":"Gaussian process factor analysis (GPFA) is a latent variable modeling technique commonly used to identify smooth, low-dimensional latent trajectories underlying high-dimensional neural recordings. Specifically, researchers model spiking rates as Gaussian observations, resulting in tractable inference. Recently, GPFA has been extended to model spike count data. However, due to the non-conjugacy of the likelihood, the inference becomes intractable. Prior works rely on either black-box inference techniques, numerical integration or polynomial approximations of the likelihood to handle intractability. To overcome this challenge, we propose a conditionally-conjugate Gaussian process factor analysis (ccGPFA) resulting in both analytically and computationally tractable inference for modeling neural activity from spike count data. In particular, we develop a novel data augmentation based method that renders the model conditionally conjugate. Consequently, our model enjoys the advantage of simple closed-form updates using a variational EM algorithm. Furthermore, due to its conditional conjugacy, we show our model can be readily scaled using sparse Gaussian Processes and accelerated inference via natural gradients. To validate our method, we empirically demonstrate its efficacy through experiments.","sentences":["Gaussian process factor analysis (GPFA) is a latent variable modeling technique commonly used to identify smooth, low-dimensional latent trajectories underlying high-dimensional neural recordings.","Specifically, researchers model spiking rates as Gaussian observations, resulting in tractable inference.","Recently, GPFA has been extended to model spike count data.","However, due to the non-conjugacy of the likelihood, the inference becomes intractable.","Prior works rely on either black-box inference techniques, numerical integration or polynomial approximations of the likelihood to handle intractability.","To overcome this challenge, we propose a conditionally-conjugate Gaussian process factor analysis (ccGPFA) resulting in both analytically and computationally tractable inference for modeling neural activity from spike count data.","In particular, we develop a novel data augmentation based method that renders the model conditionally conjugate.","Consequently, our model enjoys the advantage of simple closed-form updates using a variational EM algorithm.","Furthermore, due to its conditional conjugacy, we show our model can be readily scaled using sparse Gaussian Processes and accelerated inference via natural gradients.","To validate our method, we empirically demonstrate its efficacy through experiments."],"url":"http://arxiv.org/abs/2405.11683v1","category":"cs.LG"}
{"created":"2024-05-19 21:18:32","title":"Godunov Loss Functions for Modelling of Hyperbolic Conservation Laws","abstract":"Machine learning techniques are being used as an alternative to traditional numerical discretization methods for solving hyperbolic partial differential equations (PDEs) relevant to fluid flow. Whilst numerical methods are higher fidelity, they are computationally expensive. Machine learning methods on the other hand are lower fidelity but provide significant speed-ups. The emergence of physics-informed neural networks (PINNs) in fluid dynamics has allowed scientists to directly use PDEs for evaluating loss functions in an unsupervised manner. The downfall of this approach is that the differential form of systems is invalid at regions of shock inherent in hyperbolic PDEs such as the compressible Euler equations. To circumvent this problem we propose a modification to PDE-based PINN losses by using a finite volume-based loss function that incorporates the flux of Godunov-type methods. These Godunov-type methods are also known as approximate Riemann solvers and evaluate intercell fluxes in an entropy-satisfying manner, yielding more physically accurate shocks. Our approach increases fidelity compared to using regularized PDE-based PINN losses, as tested on the 2D Riemann problem.","sentences":["Machine learning techniques are being used as an alternative to traditional numerical discretization methods for solving hyperbolic partial differential equations (PDEs) relevant to fluid flow.","Whilst numerical methods are higher fidelity, they are computationally expensive.","Machine learning methods on the other hand are lower fidelity but provide significant speed-ups.","The emergence of physics-informed neural networks (PINNs) in fluid dynamics has allowed scientists to directly use PDEs for evaluating loss functions in an unsupervised manner.","The downfall of this approach is that the differential form of systems is invalid at regions of shock inherent in hyperbolic PDEs such as the compressible Euler equations.","To circumvent this problem we propose a modification to PDE-based PINN losses by using a finite volume-based loss function that incorporates the flux of Godunov-type methods.","These Godunov-type methods are also known as approximate Riemann solvers and evaluate intercell fluxes in an entropy-satisfying manner, yielding more physically accurate shocks.","Our approach increases fidelity compared to using regularized PDE-based PINN losses, as tested on the 2D Riemann problem."],"url":"http://arxiv.org/abs/2405.11674v1","category":"physics.flu-dyn"}
{"created":"2024-05-19 21:12:15","title":"Random walk on sphere packings and Delaunay triangulations in arbitrary dimension","abstract":"We prove that random walks on a family of tilings of d-dimensional Euclidean space, with a canonical choice of conductances, converge to Brownian motion modulo time parameterization. This class of tilings includes Delaunay triangulations (the dual of Voronoi tesselations) and sphere packings. Our regularity assumptions are deterministic and mild. For example, our results apply to Delaunay triangulations with vertices sampled from a d-dimensional Gaussian multiplicative chaos measure. As part of our proof, we establish the uniform convergence of certain finite volume schemes for the Laplace equation, with quantitative bounds on the rate of convergence. In the special case of two dimensions, we give a new, short proof of the main result of Gurel-Gurevich--Jerison--Nachmias (2020).","sentences":["We prove that random walks on a family of tilings of d-dimensional Euclidean space, with a canonical choice of conductances, converge to Brownian motion modulo time parameterization.","This class of tilings includes Delaunay triangulations (the dual of Voronoi tesselations) and sphere packings.","Our regularity assumptions are deterministic and mild.","For example, our results apply to Delaunay triangulations with vertices sampled from a d-dimensional Gaussian multiplicative chaos measure.","As part of our proof, we establish the uniform convergence of certain finite volume schemes for the Laplace equation, with quantitative bounds on the rate of convergence.","In the special case of two dimensions, we give a new, short proof of the main result of Gurel-Gurevich--Jerison--Nachmias (2020)."],"url":"http://arxiv.org/abs/2405.11673v1","category":"math.PR"}
{"created":"2024-05-19 20:24:51","title":"Cyber Risks of Machine Translation Critical Errors : Arabic Mental Health Tweets as a Case Study","abstract":"With the advent of Neural Machine Translation (NMT) systems, the MT output has reached unprecedented accuracy levels which resulted in the ubiquity of MT tools on almost all online platforms with multilingual content. However, NMT systems, like other state-of-the-art AI generative systems, are prone to errors that are deemed machine hallucinations. The problem with NMT hallucinations is that they are remarkably \\textit{fluent} hallucinations. Since they are trained to produce grammatically correct utterances, NMT systems are capable of producing mistranslations that are too fluent to be recognised by both users of the MT tool, as well as by automatic quality metrics that are used to gauge their performance. In this paper, we introduce an authentic dataset of machine translation critical errors to point to the ethical and safety issues involved in the common use of MT. The dataset comprises mistranslations of Arabic mental health postings manually annotated with critical error types. We also show how the commonly used quality metrics do not penalise critical errors and highlight this as a critical issue that merits further attention from researchers.","sentences":["With the advent of Neural Machine Translation (NMT) systems, the MT output has reached unprecedented accuracy levels which resulted in the ubiquity of MT tools on almost all online platforms with multilingual content.","However, NMT systems, like other state-of-the-art AI generative systems, are prone to errors that are deemed machine hallucinations.","The problem with NMT hallucinations is that they are remarkably \\textit{fluent} hallucinations.","Since they are trained to produce grammatically correct utterances, NMT systems are capable of producing mistranslations that are too fluent to be recognised by both users of the MT tool, as well as by automatic quality metrics that are used to gauge their performance.","In this paper, we introduce an authentic dataset of machine translation critical errors to point to the ethical and safety issues involved in the common use of MT.","The dataset comprises mistranslations of Arabic mental health postings manually annotated with critical error types.","We also show how the commonly used quality metrics do not penalise critical errors and highlight this as a critical issue that merits further attention from researchers."],"url":"http://arxiv.org/abs/2405.11668v1","category":"cs.CL"}
{"created":"2024-05-19 20:16:06","title":"Crumpled-to-flat transition of quenched disordered membranes at two-loop order","abstract":"We investigate the effects of quenched elastic disorder on the nature of the crumpling-to-flat transition of $D$-dimensional polymerized membranes using a two-loop computation near the upper critical dimension $D_c=4$. While the pure system undergoes fluctuation-induced first order transitions below $D_c$ and for an embedding dimension $d<d_{c,pure}\\simeq 218.2$, one observes, in presence of disorder, the emergence of various regions of second order governed by a disordered stable fixed point for $d<d_{c1}\\sim d_{c,pure}$. This opens the possibility of a new universality class associated with the crumpling-to-flat transition of disordered membranes in $d=3$","sentences":["We investigate the effects of quenched elastic disorder on the nature of the crumpling-to-flat transition of $D$-dimensional polymerized membranes using a two-loop computation near the upper critical dimension $D_c=4$. While the pure system undergoes fluctuation-induced first order transitions below $D_c$ and for an embedding dimension $d<d_{c,pure}\\simeq 218.2$, one observes, in presence of disorder, the emergence of various regions of second order governed by a disordered stable fixed point for $d<d_{c1}\\sim d_{c,pure}$. This opens the possibility of a new universality class associated with the crumpling-to-flat transition of disordered membranes in $d=3$"],"url":"http://arxiv.org/abs/2405.11663v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-19 17:46:27","title":"Competitive ligand binding kinetics to linear polymers","abstract":"Different types of ligands compete in binding to polymers with different consequences for the physical and chemical properties of the resulting complex. Here, we derive a general kinetic model for the competitive binding kinetics of different types of ligands to a linear polymer, using the McGhee and von Hippel detailed binding site counting procedure. The derived model allows the description of the competitive binding process in terms of the size of the ligand, binding and release rates, and cooperativity parameters. We illustrate the implications of the general theory showing the equations for the competitive binding of two ligands. The size of the ligand, given by the number of monomers occluded, is shown to have a great impact in competitive binding. Ligands requiring a large available gap for binding are strongly inhibited by smaller ligands. Ligand size then has a leading role compared to binding affinity or cooperativity. For ligands that can bind in different modes (i.e., different number of monomers), this implies that they are more effective in covering or passivating the polymer in lower modes, if the different modes have similar binding energies.","sentences":["Different types of ligands compete in binding to polymers with different consequences for the physical and chemical properties of the resulting complex.","Here, we derive a general kinetic model for the competitive binding kinetics of different types of ligands to a linear polymer, using the McGhee and von Hippel detailed binding site counting procedure.","The derived model allows the description of the competitive binding process in terms of the size of the ligand, binding and release rates, and cooperativity parameters.","We illustrate the implications of the general theory showing the equations for the competitive binding of two ligands.","The size of the ligand, given by the number of monomers occluded, is shown to have a great impact in competitive binding.","Ligands requiring a large available gap for binding are strongly inhibited by smaller ligands.","Ligand size then has a leading role compared to binding affinity or cooperativity.","For ligands that can bind in different modes (i.e., different number of monomers), this implies that they are more effective in covering or passivating the polymer in lower modes, if the different modes have similar binding energies."],"url":"http://arxiv.org/abs/2405.11631v1","category":"cond-mat.soft"}
{"created":"2024-05-19 17:41:59","title":"Generalised conditions for rapid-turn inflation","abstract":"Rapid-turn slow-roll inflationary trajectories have been shown to be an attractor in two-field models, provided the turn rate is near constant and larger than the slow-roll parameters. These trajectories can produce primordial spectra consistent with current observations on CMB scales. We present the generalized consistency condition for sustained rapid-turn inflationary trajectory with two fields, arbitrary field-space metric and potential valid for any value of the turn rate. This has to be supplemented by a second condition to ensure slow roll evolution. Both conditions together constitute a tool to identify inflationary trajectories with arbitrary values of the turning rate without having to solve the equations of motion. We present a Python package for the numerical identification of regions in field-space and parameter space that allow for rapid-turn trajectories.","sentences":["Rapid-turn slow-roll inflationary trajectories have been shown to be an attractor in two-field models, provided the turn rate is near constant and larger than the slow-roll parameters.","These trajectories can produce primordial spectra consistent with current observations on CMB scales.","We present the generalized consistency condition for sustained rapid-turn inflationary trajectory with two fields, arbitrary field-space metric and potential valid for any value of the turn rate.","This has to be supplemented by a second condition to ensure slow roll evolution.","Both conditions together constitute a tool to identify inflationary trajectories with arbitrary values of the turning rate without having to solve the equations of motion.","We present a Python package for the numerical identification of regions in field-space and parameter space that allow for rapid-turn trajectories."],"url":"http://arxiv.org/abs/2405.11628v1","category":"astro-ph.CO"}
{"created":"2024-05-19 17:29:34","title":"On Generalized Transmuted Lifetime Distribution","abstract":"This article presents a new class of generalized transmuted lifetime distributions which includes a large number of lifetime distributions as sub-family. Several important mathematical quantities such as density function, distribution function, quantile function, moments, moment generating function, stress-strength reliability function, order statistics, R\\'enyi and q-entropy, residual and reversed residual life function, and cumulative information generating function are obtained. The methods of maximum likelihood, ordinary least square, weighted least square, Cram\\'er-von Mises, Anderson Darling, and Right-tail Anderson Darling are considered to estimate the model parameters in a general way. Further, a well-organized Monte Carlo simulation experiments have been performed to observe the behavior of the estimators. Finally, two real data have also been analyzed to demonstrate the effectiveness of the proposed distribution in real-life modeling.","sentences":["This article presents a new class of generalized transmuted lifetime distributions which includes a large number of lifetime distributions as sub-family.","Several important mathematical quantities such as density function, distribution function, quantile function, moments, moment generating function, stress-strength reliability function, order statistics, R\\'enyi and q-entropy, residual and reversed residual life function, and cumulative information generating function are obtained.","The methods of maximum likelihood, ordinary least square, weighted least square, Cram\\'er-von Mises, Anderson Darling, and","Right-tail Anderson Darling are considered to estimate the model parameters in a general way.","Further, a well-organized Monte Carlo simulation experiments have been performed to observe the behavior of the estimators.","Finally, two real data have also been analyzed to demonstrate the effectiveness of the proposed distribution in real-life modeling."],"url":"http://arxiv.org/abs/2405.11624v1","category":"stat.ME"}
{"created":"2024-05-19 16:21:04","title":"Switched Flow Matching: Eliminating Singularities via Switching ODEs","abstract":"Continuous-time generative models, such as Flow Matching (FM), construct probability paths to transport between one distribution and another through the simulation-free learning of the neural ordinary differential equations (ODEs). During inference, however, the learned model often requires multiple neural network evaluations to accurately integrate the flow, resulting in a slow sampling speed. We attribute the reason to the inherent (joint) heterogeneity of source and/or target distributions, namely the singularity problem, which poses challenges for training the neural ODEs effectively. To address this issue, we propose a more general framework, termed Switched FM (SFM), that eliminates singularities via switching ODEs, as opposed to using a uniform ODE in FM. Importantly, we theoretically show that FM cannot transport between two simple distributions due to the existence and uniqueness of initial value problems of ODEs, while these limitations can be well tackled by SFM. From an orthogonal perspective, our framework can seamlessly integrate with the existing advanced techniques, such as minibatch optimal transport, to further enhance the straightness of the flow, yielding a more efficient sampling process with reduced costs. We demonstrate the effectiveness of the newly proposed SFM through several numerical examples.","sentences":["Continuous-time generative models, such as Flow Matching (FM), construct probability paths to transport between one distribution and another through the simulation-free learning of the neural ordinary differential equations (ODEs).","During inference, however, the learned model often requires multiple neural network evaluations to accurately integrate the flow, resulting in a slow sampling speed.","We attribute the reason to the inherent (joint) heterogeneity of source and/or target distributions, namely the singularity problem, which poses challenges for training the neural ODEs effectively.","To address this issue, we propose a more general framework, termed Switched FM (SFM), that eliminates singularities via switching ODEs, as opposed to using a uniform ODE in FM.","Importantly, we theoretically show that FM cannot transport between two simple distributions due to the existence and uniqueness of initial value problems of ODEs, while these limitations can be well tackled by SFM.","From an orthogonal perspective, our framework can seamlessly integrate with the existing advanced techniques, such as minibatch optimal transport, to further enhance the straightness of the flow, yielding a more efficient sampling process with reduced costs.","We demonstrate the effectiveness of the newly proposed SFM through several numerical examples."],"url":"http://arxiv.org/abs/2405.11605v1","category":"cs.LG"}
{"created":"2024-05-19 16:07:01","title":"Equal Sum and Product Problem III","abstract":"Denote by $N(n)$ the number of integer solutions $(x_1,\\,x_2,\\ldots ,x_n)$ of the equation $x_1+x_2+\\ldots+x_n=x_1x_2\\cdot\\ldots\\cdot x_n$ such that $x_1\\ge x_2\\ge\\ldots\\ge x_n\\ge 1$, $n \\in \\mathbb{Z}^+$. The aim of this paper are is twofold: first we present an asymptotic formula for $\\sum\\limits_{2\\le n\\le x}N(n)$, then we verify that the counting function $N(n)$ takes very large value compared to its average value.","sentences":["Denote by $N(n)$ the number of integer solutions $(x_1,\\,x_2,\\ldots ,x_n)$ of the equation $x_1+x_2+\\ldots+x_n=x_1x_2\\cdot\\ldots\\cdot x_n$ such that $x_1\\ge x_2\\ge\\ldots\\ge x_n\\ge 1$, $n \\in \\mathbb{Z}^+$.","The aim of this paper are is twofold: first we present an asymptotic formula for $\\sum\\limits_{2\\le n\\le x}N(n)$, then we verify that the counting function $N(n)$ takes very large value compared to its average value."],"url":"http://arxiv.org/abs/2405.11600v1","category":"math.NT"}
{"created":"2024-05-19 15:39:50","title":"Efficient absorbing boundary conditions for conservation laws: Application to the piston problem of gas dynamics","abstract":"In this work, we address the imposition of outflow boundary conditions for one-dimensional conservation laws. While a highly accurate numerical solution can be obtained in the interior of the domain, boundary discretization can lead to unphysical reflections. We investigate and implement some classes of relaxation methods and far-field operators to deal with this problem without significantly increasing the size of the computational domain. Relations are established between these techniques, and extensions of them are explored. In particular, we introduce a simple and robust relaxation method with a matrix-valued weight function that selectively absorbs outgoing waves. As a challenging model problem, we consider the Lagrangian formulation of the Euler equations for an isotropic gas with inflow boundary conditions determined by an oscillating piston.","sentences":["In this work, we address the imposition of outflow boundary conditions for one-dimensional conservation laws.","While a highly accurate numerical solution can be obtained in the interior of the domain, boundary discretization can lead to unphysical reflections.","We investigate and implement some classes of relaxation methods and far-field operators to deal with this problem without significantly increasing the size of the computational domain.","Relations are established between these techniques, and extensions of them are explored.","In particular, we introduce a simple and robust relaxation method with a matrix-valued weight function that selectively absorbs outgoing waves.","As a challenging model problem, we consider the Lagrangian formulation of the Euler equations for an isotropic gas with inflow boundary conditions determined by an oscillating piston."],"url":"http://arxiv.org/abs/2405.11588v1","category":"math.NA"}
{"created":"2024-05-19 14:57:01","title":"Deflection of light by wormholes and its shadow due to dark matter within modified symmetric teleparallel gravity formalism","abstract":"We explore the possibility of traversable wormhole formation in the dark matter halos in the context of $f(Q)$ gravity. We obtain the exact wormhole solutions with anisotropic matter source based on the Bose-Einstein condensate, Navarro-Frenk-White, and pseudo-isothermal matter density profiles. Notably, we present a novel wormhole solution supported by these dark matters using the expressions for the density profile and rotational velocity along with the modified field equations to calculate the redshift and shape functions of the wormholes. With a particular set of parameters, we demonstrate that our proposed wormhole solutions fulfill the flare-out condition against an asymptotic background. Additionally, we examine the energy conditions, focusing on the null energy conditions at the wormhole's throat, providing a graphical representation of the feasible and negative regions. Our study also examines the wormhole's shadow in the presence of various dark matter models, revealing that higher central densities result in a shadow closer to the throat, whereas lower values have the opposite effect. Moreover, we explore the deflection of light when it encounters these wormholes, particularly noting that light deflection approaches infinity at the throat, where the gravitational field is extremely strong.","sentences":["We explore the possibility of traversable wormhole formation in the dark matter halos in the context of $f(Q)$ gravity.","We obtain the exact wormhole solutions with anisotropic matter source based on the Bose-Einstein condensate, Navarro-Frenk-White, and pseudo-isothermal matter density profiles.","Notably, we present a novel wormhole solution supported by these dark matters using the expressions for the density profile and rotational velocity along with the modified field equations to calculate the redshift and shape functions of the wormholes.","With a particular set of parameters, we demonstrate that our proposed wormhole solutions fulfill the flare-out condition against an asymptotic background.","Additionally, we examine the energy conditions, focusing on the null energy conditions at the wormhole's throat, providing a graphical representation of the feasible and negative regions.","Our study also examines the wormhole's shadow in the presence of various dark matter models, revealing that higher central densities result in a shadow closer to the throat, whereas lower values have the opposite effect.","Moreover, we explore the deflection of light when it encounters these wormholes, particularly noting that light deflection approaches infinity at the throat, where the gravitational field is extremely strong."],"url":"http://arxiv.org/abs/2405.11576v1","category":"gr-qc"}
{"created":"2024-05-19 14:30:41","title":"Collisional damping of wave modes in ion-electron plasmas","abstract":"To expand on recent work, we introduce collisional terms in the analysis of the warm ion-electron, two-fluid equations for a homogeneous plasma at rest. Consequently, the plasma is now described by six variables: the magnetisation, the ratio of masses over charges, the electron and ion sound speeds, the angle between the wave vector and the magnetic field, and a new parameter describing the electron-ion collision frequency. This additional parameter does not introduce new wave modes compared to the collisionless case, but does result in complex mode frequencies. Both for the backward and forward propagating modes the imaginary components are negative and thus quantify collisional damping. We provide convenient (polynomial) expressions to quantify frequencies and damping rates in all short and long wavelength limits, including the cut-off and resonance limits, whilst the one-fluid magnetohydrodynamic limit is retained with the familiar undamped slow, Alfv\\'en and fast (SAF) waves. As collisions only introduce a damping, the previously introduced labelling of the wave modes S, A, F, M, O and X can be kept and assigned based on their long and short wavelength behaviour. The obtained damping at cut-off and resonance limits is parametrised with the collision frequency, and can be tailored to match known kinetic damping expressions. It is demonstrated that varying the angle can introduce crossings between the wave modes, as was already present in the ideal ion-electron case, but also a collision frequency exceeding a critical collision frequency can lead to crossings at angles where previously only avoided crossings were found.","sentences":["To expand on recent work, we introduce collisional terms in the analysis of the warm ion-electron, two-fluid equations for a homogeneous plasma at rest.","Consequently, the plasma is now described by six variables: the magnetisation, the ratio of masses over charges, the electron and ion sound speeds, the angle between the wave vector and the magnetic field, and a new parameter describing the electron-ion collision frequency.","This additional parameter does not introduce new wave modes compared to the collisionless case, but does result in complex mode frequencies.","Both for the backward and forward propagating modes the imaginary components are negative and thus quantify collisional damping.","We provide convenient (polynomial) expressions to quantify frequencies and damping rates in all short and long wavelength limits, including the cut-off and resonance limits, whilst the one-fluid magnetohydrodynamic limit is retained with the familiar undamped slow, Alfv\\'en and fast (SAF) waves.","As collisions only introduce a damping, the previously introduced labelling of the wave modes S, A, F, M, O and X can be kept and assigned based on their long and short wavelength behaviour.","The obtained damping at cut-off and resonance limits is parametrised with the collision frequency, and can be tailored to match known kinetic damping expressions.","It is demonstrated that varying the angle can introduce crossings between the wave modes, as was already present in the ideal ion-electron case, but also a collision frequency exceeding a critical collision frequency can lead to crossings at angles where previously only avoided crossings were found."],"url":"http://arxiv.org/abs/2405.11565v1","category":"physics.plasm-ph"}
{"created":"2024-05-19 14:29:06","title":"CRF360D: Monocular 360 Depth Estimation via Spherical Fully-Connected CRFs","abstract":"Monocular 360 depth estimation is challenging due to the inherent distortion of the equirectangular projection (ERP). This distortion causes a problem: spherical adjacent points are separated after being projected to the ERP plane, particularly in the polar regions. To tackle this problem, recent methods calculate the spherical neighbors in the tangent domain. However, as the tangent patch and sphere only have one common point, these methods construct neighboring spherical relationships around the common point. In this paper, we propose spherical fully-connected CRFs (SF-CRFs). We begin by evenly partitioning an ERP image with regular windows, where windows at the equator involve broader spherical neighbors than those at the poles. To improve the spherical relationships, our SF-CRFs enjoy two key components. Firstly, to involve sufficient spherical neighbors, we propose a Spherical Window Transform (SWT) module. This module aims to replicate the equator window's spherical relationships to all other windows, leveraging the rotational invariance of the sphere. Remarkably, the transformation process is highly efficient, completing the transformation of all windows in a 512X1024 ERP with 0.038 seconds on CPU. Secondly, we propose a Planar-Spherical Interaction (PSI) module to facilitate the relationships between regular and transformed windows, which not only preserves the local details but also captures global structures. By building a decoder based on the SF-CRFs blocks, we propose CRF360D, a novel 360 depth estimation framework that achieves state-of-the-art performance across diverse datasets. Our CRF360D is compatible with different perspective image-trained backbones (e.g., EfficientNet), serving as the encoder.","sentences":["Monocular 360 depth estimation is challenging due to the inherent distortion of the equirectangular projection (ERP).","This distortion causes a problem: spherical adjacent points are separated after being projected to the ERP plane, particularly in the polar regions.","To tackle this problem, recent methods calculate the spherical neighbors in the tangent domain.","However, as the tangent patch and sphere only have one common point, these methods construct neighboring spherical relationships around the common point.","In this paper, we propose spherical fully-connected CRFs (SF-CRFs).","We begin by evenly partitioning an ERP image with regular windows, where windows at the equator involve broader spherical neighbors than those at the poles.","To improve the spherical relationships, our SF-CRFs enjoy two key components.","Firstly, to involve sufficient spherical neighbors, we propose a Spherical Window Transform (SWT) module.","This module aims to replicate the equator window's spherical relationships to all other windows, leveraging the rotational invariance of the sphere.","Remarkably, the transformation process is highly efficient, completing the transformation of all windows in a 512X1024 ERP with 0.038 seconds on CPU.","Secondly, we propose a Planar-Spherical Interaction (PSI) module to facilitate the relationships between regular and transformed windows, which not only preserves the local details but also captures global structures.","By building a decoder based on the SF-CRFs blocks, we propose CRF360D, a novel 360 depth estimation framework that achieves state-of-the-art performance across diverse datasets.","Our CRF360D is compatible with different perspective image-trained backbones (e.g., EfficientNet), serving as the encoder."],"url":"http://arxiv.org/abs/2405.11564v1","category":"cs.CV"}
{"created":"2024-05-19 14:27:19","title":"On the restriction of various Laplace operators on submanifolds","abstract":"When considering Navier-Stokes equations on Riemannian manifolds one frequently encounters situations where the manifold is embedded in the ambient Euclidean space. In this context it is interesting to investigate what is the precise relationship of the diffusion operator in the ambient space to the diffusion operator on the manifold. The present paper gives a precise characterization of this situation for general surfaces in three dimensional space.","sentences":["When considering Navier-Stokes equations on Riemannian manifolds one frequently encounters situations where the manifold is embedded in the ambient Euclidean space.","In this context it is interesting to investigate what is the precise relationship of the diffusion operator in the ambient space to the diffusion operator on the manifold.","The present paper gives a precise characterization of this situation for general surfaces in three dimensional space."],"url":"http://arxiv.org/abs/2405.11562v1","category":"math.AP"}
{"created":"2024-05-19 14:19:03","title":"Higher $\\varepsilon$-poles and logarithms in the MS-like schemes from the algebraic structure of the renormalization group","abstract":"We investigate the structure of renormalization constants within the MS-like renormalization prescriptions for a version of dimensional regularization in which the dimensionful regularization parameter $\\Lambda$ differs from the renormalization point $\\mu$. Namely, we rewrite the all-loop equations relating coefficients at higher $\\varepsilon$-poles and higher powers of $\\ln\\Lambda/\\mu$ to the coefficients of the renormalization group functions in a simple unified form. It is argued that this form follows from the algebraic structure of the renormalization group.","sentences":["We investigate the structure of renormalization constants within the MS-like renormalization prescriptions for a version of dimensional regularization in which the dimensionful regularization parameter $\\Lambda$ differs from the renormalization point $\\mu$. Namely, we rewrite the all-loop equations relating coefficients at higher $\\varepsilon$-poles and higher powers of $\\ln\\Lambda/\\mu$ to the coefficients of the renormalization group functions in a simple unified form.","It is argued that this form follows from the algebraic structure of the renormalization group."],"url":"http://arxiv.org/abs/2405.11557v1","category":"hep-th"}
{"created":"2024-05-19 13:20:43","title":"The boundary value contact problem of electroelasticity for piecewise-homogeneous piezo-electric plate with elastic inclusion and cut","abstract":"A contact problem of the theory of electroelasticity for piecewise-homogeneous plate of piezo-electric material with infinite cut and elastic finite inclusion of variable bending rigidity is considered. By using methods of the theory of analytic function, the problem is reduced to a system of singular integro-differential equation with fixed singularity. Using an integral transformation a Riemann problem is obtained, the solution of which is presented in explicit form.","sentences":["A contact problem of the theory of electroelasticity for piecewise-homogeneous plate of piezo-electric material with infinite cut and elastic finite inclusion of variable bending rigidity is considered.","By using methods of the theory of analytic function, the problem is reduced to a system of singular integro-differential equation with fixed singularity.","Using an integral transformation a Riemann problem is obtained, the solution of which is presented in explicit form."],"url":"http://arxiv.org/abs/2405.11543v1","category":"math-ph"}
{"created":"2024-05-19 13:15:23","title":"From Fourier to Neural ODEs: Flow matching for modeling complex systems","abstract":"Modeling complex systems using standard neural ordinary differential equations (NODEs) often faces some essential challenges, including high computational costs and susceptibility to local optima. To address these challenges, we propose a simulation-free framework, called Fourier NODEs (FNODEs), that effectively trains NODEs by directly matching the target vector field based on Fourier analysis. Specifically, we employ the Fourier analysis to estimate temporal and potential high-order spatial gradients from noisy observational data. We then incorporate the estimated spatial gradients as additional inputs to a neural network. Furthermore, we utilize the estimated temporal gradient as the optimization objective for the output of the neural network. Later, the trained neural network generates more data points through an ODE solver without participating in the computational graph, facilitating more accurate estimations of gradients based on Fourier analysis. These two steps form a positive feedback loop, enabling accurate dynamics modeling in our framework. Consequently, our approach outperforms state-of-the-art methods in terms of training time, dynamics prediction, and robustness. Finally, we demonstrate the superior performance of our framework using a number of representative complex systems.","sentences":["Modeling complex systems using standard neural ordinary differential equations (NODEs) often faces some essential challenges, including high computational costs and susceptibility to local optima.","To address these challenges, we propose a simulation-free framework, called Fourier NODEs (FNODEs), that effectively trains NODEs by directly matching the target vector field based on Fourier analysis.","Specifically, we employ the Fourier analysis to estimate temporal and potential high-order spatial gradients from noisy observational data.","We then incorporate the estimated spatial gradients as additional inputs to a neural network.","Furthermore, we utilize the estimated temporal gradient as the optimization objective for the output of the neural network.","Later, the trained neural network generates more data points through an ODE solver without participating in the computational graph, facilitating more accurate estimations of gradients based on Fourier analysis.","These two steps form a positive feedback loop, enabling accurate dynamics modeling in our framework.","Consequently, our approach outperforms state-of-the-art methods in terms of training time, dynamics prediction, and robustness.","Finally, we demonstrate the superior performance of our framework using a number of representative complex systems."],"url":"http://arxiv.org/abs/2405.11542v1","category":"cs.LG"}
{"created":"2024-05-19 10:57:13","title":"Polynomial convergence rate for quasiperiodic homogenization of Hamilton-Jacobi equations","abstract":"In this paper, we demonstrate a polynomial convergence rate for quasi-periodic homogenization of Hamilton--Jacobi equations in one-dimensional with $n$-frequency potentials. The proof relies on a connection between optimal control theory, regularity of the effective Hamiltonian, and new quantitative ergodic estimates based on Diophantine approximations.","sentences":["In this paper, we demonstrate a polynomial convergence rate for quasi-periodic homogenization of Hamilton--Jacobi equations in one-dimensional with $n$-frequency potentials.","The proof relies on a connection between optimal control theory, regularity of the effective Hamiltonian, and new quantitative ergodic estimates based on Diophantine approximations."],"url":"http://arxiv.org/abs/2405.11516v1","category":"math.AP"}
{"created":"2024-05-19 10:54:03","title":"Towards Translating Real-World Code with LLMs: A Study of Translating to Rust","abstract":"Large language models (LLMs) show promise in code translation - the task of translating code written in one programming language to another language - due to their ability to write code in most programming languages. However, LLM's effectiveness on translating real-world code remains largely unstudied. In this work, we perform the first substantial study on LLM-based translation to Rust by assessing the ability of five state-of-the-art LLMs, GPT4, Claude 3, Claude 2.1, Gemini Pro, and Mixtral. We conduct our study on code extracted from real-world open source projects. To enable our study, we develop FLOURINE, an end-to-end code translation tool that uses differential fuzzing to check if a Rust translation is I/O equivalent to the original source program, eliminating the need for pre-existing test cases. As part of our investigation, we assess both the LLM's ability to produce an initially successful translation, as well as their capacity to fix a previously generated buggy one. If the original and the translated programs are not I/O equivalent, we apply a set of automated feedback strategies, including feedback to the LLM with counterexamples. Our results show that the most successful LLM can translate 47% of our benchmarks, and also provides insights into next steps for improvements.","sentences":["Large language models (LLMs) show promise in code translation - the task of translating code written in one programming language to another language - due to their ability to write code in most programming languages.","However, LLM's effectiveness on translating real-world code remains largely unstudied.","In this work, we perform the first substantial study on LLM-based translation to Rust by assessing the ability of five state-of-the-art LLMs, GPT4, Claude 3, Claude 2.1, Gemini Pro, and Mixtral.","We conduct our study on code extracted from real-world open source projects.","To enable our study, we develop FLOURINE, an end-to-end code translation tool that uses differential fuzzing to check if a Rust translation is I/O equivalent to the original source program, eliminating the need for pre-existing test cases.","As part of our investigation, we assess both the LLM's ability to produce an initially successful translation, as well as their capacity to fix a previously generated buggy one.","If the original and the translated programs are not I/O equivalent, we apply a set of automated feedback strategies, including feedback to the LLM with counterexamples.","Our results show that the most successful LLM can translate 47% of our benchmarks, and also provides insights into next steps for improvements."],"url":"http://arxiv.org/abs/2405.11514v1","category":"cs.SE"}
{"created":"2024-05-19 10:25:52","title":"On some open problems in reliability theory","abstract":"We study a stochastic scheduling on an unreliable machine with general up-times and general set-up times which is described by a group of partial differential equations with Dirac-delta functions in the boundary and initial conditions. In special case that the random processing rate of job $i,$ the random up-time rate of job $i$ and the random repair rate of job $i$ are constants, we determine the explicit expression of its time-dependent solution and give the asymptotic behavior of its time-dependent solution. Our result implies that $C_0-$semigroup theory is not suitable for this model. In general case, we determine the Laplace transform of its time-dependent solution. Next, we convert the model into an abstract Cauchy problem whose underlying operator is an evolution family. Finally, we leave some open problems.","sentences":["We study a stochastic scheduling on an unreliable machine with general up-times and general set-up times which is described by a group of partial differential equations with Dirac-delta functions in the boundary and initial conditions.","In special case that the random processing rate of job $i,$ the random up-time rate of job $i$ and the random repair rate of job $i$ are constants, we determine the explicit expression of its time-dependent solution and give the asymptotic behavior of its time-dependent solution.","Our result implies that $C_0-$semigroup theory is not suitable for this model.","In general case, we determine the Laplace transform of its time-dependent solution.","Next, we convert the model into an abstract Cauchy problem whose underlying operator is an evolution family.","Finally, we leave some open problems."],"url":"http://arxiv.org/abs/2405.11510v1","category":"math.PR"}
{"created":"2024-05-19 10:00:06","title":"CTGNN: Crystal Transformer Graph Neural Network for Crystal Material Property Prediction","abstract":"The combination of deep learning algorithm and materials science has made significant progress in predicting novel materials and understanding various behaviours of materials. Here, we introduced a new model called as the Crystal Transformer Graph Neural Network (CTGNN), which combines the advantages of Transformer model and graph neural networks to address the complexity of structure-properties relation of material data. Compared to the state-of-the-art models, CTGNN incorporates the graph network structure for capturing local atomic interactions and the dual-Transformer structures to model intra-crystal and inter-atomic relationships comprehensively. The benchmark carried on by the proposed CTGNN indicates that CTGNN significantly outperforms existing models like CGCNN and MEGNET in the prediction of formation energy and bandgap properties. Our work highlights the potential of CTGNN to enhance the performance of properties prediction and accelerates the discovery of new materials, particularly for perovskite materials.","sentences":["The combination of deep learning algorithm and materials science has made significant progress in predicting novel materials and understanding various behaviours of materials.","Here, we introduced a new model called as the Crystal Transformer Graph Neural Network (CTGNN), which combines the advantages of Transformer model and graph neural networks to address the complexity of structure-properties relation of material data.","Compared to the state-of-the-art models, CTGNN incorporates the graph network structure for capturing local atomic interactions and the dual-Transformer structures to model intra-crystal and inter-atomic relationships comprehensively.","The benchmark carried on by the proposed CTGNN indicates that CTGNN significantly outperforms existing models like CGCNN and MEGNET in the prediction of formation energy and bandgap properties.","Our work highlights the potential of CTGNN to enhance the performance of properties prediction and accelerates the discovery of new materials, particularly for perovskite materials."],"url":"http://arxiv.org/abs/2405.11502v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-19 09:19:40","title":"Point Cloud Compression with Implicit Neural Representations: A Unified Framework","abstract":"Point clouds have become increasingly vital across various applications thanks to their ability to realistically depict 3D objects and scenes. Nevertheless, effectively compressing unstructured, high-precision point cloud data remains a significant challenge. In this paper, we present a pioneering point cloud compression framework capable of handling both geometry and attribute components. Unlike traditional approaches and existing learning-based methods, our framework utilizes two coordinate-based neural networks to implicitly represent a voxelized point cloud. The first network generates the occupancy status of a voxel, while the second network determines the attributes of an occupied voxel. To tackle an immense number of voxels within the volumetric space, we partition the space into smaller cubes and focus solely on voxels within non-empty cubes. By feeding the coordinates of these voxels into the respective networks, we reconstruct the geometry and attribute components of the original point cloud. The neural network parameters are further quantized and compressed. Experimental results underscore the superior performance of our proposed method compared to the octree-based approach employed in the latest G-PCC standards. Moreover, our method exhibits high universality when contrasted with existing learning-based techniques.","sentences":["Point clouds have become increasingly vital across various applications thanks to their ability to realistically depict 3D objects and scenes.","Nevertheless, effectively compressing unstructured, high-precision point cloud data remains a significant challenge.","In this paper, we present a pioneering point cloud compression framework capable of handling both geometry and attribute components.","Unlike traditional approaches and existing learning-based methods, our framework utilizes two coordinate-based neural networks to implicitly represent a voxelized point cloud.","The first network generates the occupancy status of a voxel, while the second network determines the attributes of an occupied voxel.","To tackle an immense number of voxels within the volumetric space, we partition the space into smaller cubes and focus solely on voxels within non-empty cubes.","By feeding the coordinates of these voxels into the respective networks, we reconstruct the geometry and attribute components of the original point cloud.","The neural network parameters are further quantized and compressed.","Experimental results underscore the superior performance of our proposed method compared to the octree-based approach employed in the latest G-PCC standards.","Moreover, our method exhibits high universality when contrasted with existing learning-based techniques."],"url":"http://arxiv.org/abs/2405.11493v1","category":"cs.CV"}
{"created":"2024-05-19 09:16:33","title":"Probing double distribution function models in the lattice Boltzmann method for highly compressible flows","abstract":"The double distribution function approach is an efficient route towards extension of kinetic solvers to compressible flows. With a number of realizations available, an overview and comparative study in the context of high speed compressible flows is presented. We discuss the different variants of the energy partition, analyses of hydrodynamic limits and a numerical study of accuracy and performance with the particles on demand realization. Out of three considered energy partition strategies, it is shown that the non-translational energy split requires a higher-order quadrature for proper recovery of the Navier--Stokes--Fourier equations. The internal energy split on the other hand, while recovering the correct hydrodynamic limit with fourth-order quadrature, comes with a non-local --both in space and time-- source term which contributes to higher computational cost and memory overhead. Based on our analysis, the total energy split demonstrates the optimal overall performance.","sentences":["The double distribution function approach is an efficient route towards extension of kinetic solvers to compressible flows.","With a number of realizations available, an overview and comparative study in the context of high speed compressible flows is presented.","We discuss the different variants of the energy partition, analyses of hydrodynamic limits and a numerical study of accuracy and performance with the particles on demand realization.","Out of three considered energy partition strategies, it is shown that the non-translational energy split requires a higher-order quadrature for proper recovery of the Navier--Stokes--Fourier equations.","The internal energy split on the other hand, while recovering the correct hydrodynamic limit with fourth-order quadrature, comes with a non-local --both in space and time-- source term which contributes to higher computational cost and memory overhead.","Based on our analysis, the total energy split demonstrates the optimal overall performance."],"url":"http://arxiv.org/abs/2405.11489v1","category":"physics.flu-dyn"}
{"created":"2024-05-19 09:03:35","title":"Normal traces and applications to continuity equations on bounded domains","abstract":"In this work, we study several properties of the normal Lebesgue trace of vector fields introduced by the second and third author in [18] in the context of the energy conservation for the Euler equations in Onsager-critical classes. Among several properties, we prove that the normal Lebesgue trace satisfies the Gauss-Green identity and, by providing explicit counterexamples, that it is a notion sitting strictly between the distributional one for measure-divergence vector fields and the strong one for $BV$ functions. These results are then applied to the study of the uniqueness of weak solutions for continuity equations on bounded domains, allowing to remove the assumption in [15] of global $BV$ regularity up to the boundary, at least around the portion of the boundary where the characteristics exit the domain or are tangent. The proof relies on an explicit renormalization formula completely characterized by the boundary datum and the positive part of the normal Lebesgue trace. In the case when the characteristics enter the domain, a counterexample shows that achieving the normal trace in the Lebesgue sense is not enough to prevent non-uniqueness, and thus a $BV$ assumption seems to be necessary for the uniqueness of weak solutions.","sentences":["In this work, we study several properties of the normal Lebesgue trace of vector fields introduced by the second and third author in [18] in the context of the energy conservation for the Euler equations in Onsager-critical classes.","Among several properties, we prove that the normal Lebesgue trace satisfies the Gauss-Green identity and, by providing explicit counterexamples, that it is a notion sitting strictly between the distributional one for measure-divergence vector fields and the strong one for $BV$ functions.","These results are then applied to the study of the uniqueness of weak solutions for continuity equations on bounded domains, allowing to remove the assumption in [15] of global $BV$ regularity up to the boundary, at least around the portion of the boundary where the characteristics exit the domain or are tangent.","The proof relies on an explicit renormalization formula completely characterized by the boundary datum and the positive part of the normal Lebesgue trace.","In the case when the characteristics enter the domain, a counterexample shows that achieving the normal trace in the Lebesgue sense is not enough to prevent non-uniqueness, and thus a $BV$ assumption seems to be necessary for the uniqueness of weak solutions."],"url":"http://arxiv.org/abs/2405.11486v1","category":"math.AP"}
{"created":"2024-05-19 05:40:47","title":"Impact Analysis of the Chesa Boudin Administration","abstract":"Claims of soft-handed prosecutorial policies and increases in crime were precipitating factors in the removal of Chesa Boudin as district attorney of the city and county of San Francisco. However, little research has been conducted to empirically investigate the veracity of these indictments on the former district attorney. Using regression discontinuity design (RDD), I find that the Boudin administration led to a 36\\% and 21\\% reduction in monthly prosecutions and convictions respectively for all crimes. Moreover, his tenure increased monthly successful case diversions by 58\\%. When only looking at violent crimes during this period, the SFDA's office saw a 36\\% decrease, 7\\% decrease, and 47\\% increase in monthly prosecutions, convictions, and successful case diversions respectively. Although, the decrease in monthly convictions was not statistically significant for the violent crimes subset. Additionally, I did identify a potentially causal relationship between lower numbers of prosecutions and higher levels of criminal activity, however, such findings did not meet the standard for statistical significance. Finally, I conclude that using machine learning algorithms, such as neural networks and K-nearest neighbors, in place of ordinary least squares regression for the estimation of the reduced form equation possibly may decrease the size of the standard errors of the parameters in the structural equation. However, future research needs to be conducted in this space to corroborate these initially promising findings.","sentences":["Claims of soft-handed prosecutorial policies and increases in crime were precipitating factors in the removal of Chesa Boudin as district attorney of the city and county of San Francisco.","However, little research has been conducted to empirically investigate the veracity of these indictments on the former district attorney.","Using regression discontinuity design (RDD), I find that the Boudin administration led to a 36\\% and 21\\% reduction in monthly prosecutions and convictions respectively for all crimes.","Moreover, his tenure increased monthly successful case diversions by 58\\%.","When only looking at violent crimes during this period, the SFDA's office saw a 36\\% decrease, 7\\% decrease, and 47\\% increase in monthly prosecutions, convictions, and successful case diversions respectively.","Although, the decrease in monthly convictions was not statistically significant for the violent crimes subset.","Additionally, I did identify a potentially causal relationship between lower numbers of prosecutions and higher levels of criminal activity, however, such findings did not meet the standard for statistical significance.","Finally, I conclude that using machine learning algorithms, such as neural networks and K-nearest neighbors, in place of ordinary least squares regression for the estimation of the reduced form equation possibly may decrease the size of the standard errors of the parameters in the structural equation.","However, future research needs to be conducted in this space to corroborate these initially promising findings."],"url":"http://arxiv.org/abs/2405.11455v1","category":"econ.GN"}
{"created":"2024-05-19 05:33:16","title":"Study of relativistic hot accretion flow around Kerr-like Wormhole","abstract":"We investigate the structure of relativistic, low-angular momentum, inviscid advective accretion flow in a stationary axisymmetric Kerr-like wormhole (WH) spacetime, characterized by the spin parameter ($a_{\\rm k}$), the dimensionless parameter ($\\beta$), and the source mass ($M_{\\rm WH}$). In doing so, we self-consistently solve the set of governing equations describing the relativistic accretion flow around a Kerr-like WH in the steady state, and for the first time, we obtain all possible classes of global accretion solutions for transonic as well as subsonic flows. We study the properties of dynamical and thermodynamical flow variables and examine how the nature of the accretion solutions alters due to the change of the model parameters, namely energy ($\\mathcal{E}$), angular momentum ($\\lambda$), $a_{\\rm k}$, and $\\beta$. Further, we separate the parameter space in $\\lambda-\\mathcal{E}$ plane according to the nature of the flow solutions, and study the modification of the parameter space by varying $a_{\\rm k}$ and $\\beta$. Moreover, we retrace the parameter space in $a_{\\rm k}-\\beta$ plane that allows accretion solutions containing multiple critical points. Finally, we calculate the disc luminosity ($L$) considering free-free emissions for transonic solutions as these solutions are astrophysically relevant and discuss the implication of this model formalism in the context of astrophysical applications.","sentences":["We investigate the structure of relativistic, low-angular momentum, inviscid advective accretion flow in a stationary axisymmetric Kerr-like wormhole (WH) spacetime, characterized by the spin parameter ($a_{\\rm k}$), the dimensionless parameter ($\\beta$), and the source mass ($M_{\\rm WH}$).","In doing so, we self-consistently solve the set of governing equations describing the relativistic accretion flow around a Kerr-like WH in the steady state, and for the first time, we obtain all possible classes of global accretion solutions for transonic as well as subsonic flows.","We study the properties of dynamical and thermodynamical flow variables and examine how the nature of the accretion solutions alters due to the change of the model parameters, namely energy ($\\mathcal{E}$), angular momentum ($\\lambda$), $a_{\\rm k}$, and $\\beta$. Further, we separate the parameter space in $\\lambda-\\mathcal{E}$ plane according to the nature of the flow solutions, and study the modification of the parameter space by varying $a_{\\rm k}$ and $\\beta$.","Moreover, we retrace the parameter space in $a_{\\rm k}-\\beta$ plane that allows accretion solutions containing multiple critical points.","Finally, we calculate the disc luminosity ($L$) considering free-free emissions for transonic solutions as these solutions are astrophysically relevant and discuss the implication of this model formalism in the context of astrophysical applications."],"url":"http://arxiv.org/abs/2405.11453v1","category":"astro-ph.HE"}
{"created":"2024-05-19 04:39:37","title":"Model of the effective separable potential in the problem of three one-dimensional quantum particles","abstract":"The goal of this paper is to construct an effective model for studying the asymptotic solution of the scattering problem of three one-dimensional quantum particles with finite (short-range) attractive pair potentials. The asymptotic nature of the solution is defined by the rapid decrease in its discrepancy in the Schr\\\"odinger equation.","sentences":["The goal of this paper is to construct an effective model for studying the asymptotic solution of the scattering problem of three one-dimensional quantum particles with finite (short-range) attractive pair potentials.","The asymptotic nature of the solution is defined by the rapid decrease in its discrepancy in the Schr\\\"odinger equation."],"url":"http://arxiv.org/abs/2405.11443v1","category":"math-ph"}
{"created":"2024-05-19 03:42:37","title":"Generic behavior of differentially positive systems on a globally orderable Riemannian manifold","abstract":"Differentially positive systems are the nonlinear systems whose linearization along trajectories preserves a cone field on a smooth Riemannian manifold. One of the embryonic forms for cone fields in reality is originated from the general relativity. By utilizing the Perron-Frobenius vector fields and the $\\Gamma$-invariance of cone fields, we show that generic (i.e.,``almost all\" in the topological sense) orbits are convergent to certain single equilibrium. This solved a reduced version of Forni-Sepulchre's conjecture in 2016 for globally orderable manifolds.","sentences":["Differentially positive systems are the nonlinear systems whose linearization along trajectories preserves a cone field on a smooth Riemannian manifold.","One of the embryonic forms for cone fields in reality is originated from the general relativity.","By utilizing the Perron-Frobenius vector fields and the $\\Gamma$-invariance of cone fields, we show that generic (i.e.,``almost all\" in the topological sense) orbits are convergent to certain single equilibrium.","This solved a reduced version of Forni-Sepulchre's conjecture in 2016 for globally orderable manifolds."],"url":"http://arxiv.org/abs/2405.11434v1","category":"math.DS"}
{"created":"2024-05-19 03:27:31","title":"On Robust Reinforcement Learning with Lipschitz-Bounded Policy Networks","abstract":"This paper presents a study of robust policy networks in deep reinforcement learning. We investigate the benefits of policy parameterizations that naturally satisfy constraints on their Lipschitz bound, analyzing their empirical performance and robustness on two representative problems: pendulum swing-up and Atari Pong. We illustrate that policy networks with small Lipschitz bounds are significantly more robust to disturbances, random noise, and targeted adversarial attacks than unconstrained policies composed of vanilla multi-layer perceptrons or convolutional neural networks. Moreover, we find that choosing a policy parameterization with a non-conservative Lipschitz bound and an expressive, nonlinear layer architecture gives the user much finer control over the performance-robustness trade-off than existing state-of-the-art methods based on spectral normalization.","sentences":["This paper presents a study of robust policy networks in deep reinforcement learning.","We investigate the benefits of policy parameterizations that naturally satisfy constraints on their Lipschitz bound, analyzing their empirical performance and robustness on two representative problems: pendulum swing-up and Atari Pong.","We illustrate that policy networks with small Lipschitz bounds are significantly more robust to disturbances, random noise, and targeted adversarial attacks than unconstrained policies composed of vanilla multi-layer perceptrons or convolutional neural networks.","Moreover, we find that choosing a policy parameterization with a non-conservative Lipschitz bound and an expressive, nonlinear layer architecture gives the user much finer control over the performance-robustness trade-off than existing state-of-the-art methods based on spectral normalization."],"url":"http://arxiv.org/abs/2405.11432v1","category":"cs.LG"}
{"created":"2024-05-19 03:15:27","title":"Review of deep learning models for crypto price prediction: implementation and evaluation","abstract":"There has been much interest in accurate cryptocurrency price forecast models by investors and researchers. Deep Learning models are prominent machine learning techniques that have transformed various fields and have shown potential for finance and economics. Although various deep learning models have been explored for cryptocurrency price forecasting, it is not clear which models are suitable due to high market volatility. In this study, we review the literature about deep learning for cryptocurrency price forecasting and evaluate novel deep learning models for cryptocurrency stock price prediction. Our deep learning models include variants of long short-term memory (LSTM) recurrent neural networks, variants of convolutional neural networks (CNNs), and the Transformer model. We evaluate univariate and multivariate approaches for multi-step ahead predicting of cryptocurrencies close-price. Our results show that the univariate LSTM model variants perform best for cryptocurrency predictions. We also carry out volatility analysis on the four cryptocurrencies which reveals significant fluctuations in their prices throughout the COVID-19 pandemic. Additionally, we investigate the prediction accuracy of two scenarios identified by different training sets for the models. First, we use the pre-COVID-19 datasets to model cryptocurrency close-price forecasting during the early period of COVID-19. Secondly, we utilise data from the COVID-19 period to predict prices for 2023 to 2024.","sentences":["There has been much interest in accurate cryptocurrency price forecast models by investors and researchers.","Deep Learning models are prominent machine learning techniques that have transformed various fields and have shown potential for finance and economics.","Although various deep learning models have been explored for cryptocurrency price forecasting, it is not clear which models are suitable due to high market volatility.","In this study, we review the literature about deep learning for cryptocurrency price forecasting and evaluate novel deep learning models for cryptocurrency stock price prediction.","Our deep learning models include variants of long short-term memory (LSTM) recurrent neural networks, variants of convolutional neural networks (CNNs), and the Transformer model.","We evaluate univariate and multivariate approaches for multi-step ahead predicting of cryptocurrencies close-price.","Our results show that the univariate LSTM model variants perform best for cryptocurrency predictions.","We also carry out volatility analysis on the four cryptocurrencies which reveals significant fluctuations in their prices throughout the COVID-19 pandemic.","Additionally, we investigate the prediction accuracy of two scenarios identified by different training sets for the models.","First, we use the pre-COVID-19 datasets to model cryptocurrency close-price forecasting during the early period of COVID-19.","Secondly, we utilise data from the COVID-19 period to predict prices for 2023 to 2024."],"url":"http://arxiv.org/abs/2405.11431v1","category":"cs.LG"}
{"created":"2024-05-19 02:54:46","title":"Severi Varieties on Ruled Surfaces over Elliptic Curves","abstract":"We proved that the general members of Severi varieties on an Atiyah ruled surface over a general elliptic curve have nodes and ordinary triple points as singularities.","sentences":["We proved that the general members of Severi varieties on an Atiyah ruled surface over a general elliptic curve have nodes and ordinary triple points as singularities."],"url":"http://arxiv.org/abs/2405.11429v1","category":"math.AG"}
{"created":"2024-05-19 02:18:04","title":"Quantum Neural Networks for Solving Power System Transient Simulation Problem","abstract":"Quantum computing, leveraging principles of quantum mechanics, represents a transformative approach in computational methodologies, offering significant enhancements over traditional classical systems. This study tackles the complex and computationally demanding task of simulating power system transients through solving differential algebraic equations (DAEs). We introduce two novel Quantum Neural Networks (QNNs): the Sinusoidal-Friendly QNN and the Polynomial-Friendly QNN, proposing them as effective alternatives to conventional simulation techniques. Our application of these QNNs successfully simulates two small power systems, demonstrating their potential to achieve good accuracy. We further explore various configurations, including time intervals, training points, and the selection of classical optimizers, to optimize the solving of DAEs using QNNs. This research not only marks a pioneering effort in applying quantum computing to power system simulations but also expands the potential of quantum technologies in addressing intricate engineering challenges.","sentences":["Quantum computing, leveraging principles of quantum mechanics, represents a transformative approach in computational methodologies, offering significant enhancements over traditional classical systems.","This study tackles the complex and computationally demanding task of simulating power system transients through solving differential algebraic equations (DAEs).","We introduce two novel Quantum Neural Networks (QNNs): the Sinusoidal-Friendly QNN and the Polynomial-Friendly QNN, proposing them as effective alternatives to conventional simulation techniques.","Our application of these QNNs successfully simulates two small power systems, demonstrating their potential to achieve good accuracy.","We further explore various configurations, including time intervals, training points, and the selection of classical optimizers, to optimize the solving of DAEs using QNNs.","This research not only marks a pioneering effort in applying quantum computing to power system simulations but also expands the potential of quantum technologies in addressing intricate engineering challenges."],"url":"http://arxiv.org/abs/2405.11427v1","category":"quant-ph"}
{"created":"2024-05-19 02:14:36","title":"On the electromagnetic couplings in superconducting qubit circuits","abstract":"The electromagnetic couplings among resonators and transmission lines are discussed. A single resonator coupled to an N-port microwave network is formulated. The equation of motion of the resonator and the input-output relations of the network are obtained. Methods of extracting the couplings from electromagnetic solutions are also discussed.","sentences":["The electromagnetic couplings among resonators and transmission lines are discussed.","A single resonator coupled to an N-port microwave network is formulated.","The equation of motion of the resonator and the input-output relations of the network are obtained.","Methods of extracting the couplings from electromagnetic solutions are also discussed."],"url":"http://arxiv.org/abs/2405.11426v1","category":"quant-ph"}
{"created":"2024-05-18 22:39:57","title":"FESSNC: Fast Exponentially Stable and Safe Neural Controller","abstract":"In order to stabilize nonlinear systems modeled by stochastic differential equations, we design a Fast Exponentially Stable and Safe Neural Controller (FESSNC) for fast learning controllers. Our framework is parameterized by neural networks, and realizing both rigorous exponential stability and safety guarantees. Concretely, we design heuristic methods to learn the exponentially stable and the safe controllers, respectively, in light of the classic stochastic exponential stability theory and our established theorem on guaranteeing the almost-sure safety for stochastic dynamics. More significantly, to rigorously ensure the stability and the safety guarantees for the learned controllers, we develop a projection operator, projecting to the space of exponentially-stable and safe controllers. To reduce the high computation cost of solving the projection operation, approximate projection operators are delicately proposed with closed forms that map the learned controllers to the target controller space. Furthermore, we employ Hutchinson's trace estimator for a scalable unbiased estimate of the Hessian matrix that is used in the projection operator, which thus allows for computation cost reduction and therefore can accelerate the training and testing processes. More importantly, our approximate projection operations can be applied to the nonparametric control methods to improve their stability and safety performance. We empirically demonstrate the superiority of the FESSNC over the existing methods.","sentences":["In order to stabilize nonlinear systems modeled by stochastic differential equations, we design a Fast Exponentially Stable and Safe Neural Controller (FESSNC) for fast learning controllers.","Our framework is parameterized by neural networks, and realizing both rigorous exponential stability and safety guarantees.","Concretely, we design heuristic methods to learn the exponentially stable and the safe controllers, respectively, in light of the classic stochastic exponential stability theory and our established theorem on guaranteeing the almost-sure safety for stochastic dynamics.","More significantly, to rigorously ensure the stability and the safety guarantees for the learned controllers, we develop a projection operator, projecting to the space of exponentially-stable and safe controllers.","To reduce the high computation cost of solving the projection operation, approximate projection operators are delicately proposed with closed forms that map the learned controllers to the target controller space.","Furthermore, we employ Hutchinson's trace estimator for a scalable unbiased estimate of the Hessian matrix that is used in the projection operator, which thus allows for computation cost reduction and therefore can accelerate the training and testing processes.","More importantly, our approximate projection operations can be applied to the nonparametric control methods to improve their stability and safety performance.","We empirically demonstrate the superiority of the FESSNC over the existing methods."],"url":"http://arxiv.org/abs/2405.11406v1","category":"math.OC"}
{"created":"2024-05-18 21:23:23","title":"Jeans analysis in fractional gravity","abstract":"It has recently been demonstrated [A. Giusti, Phys. Rev. D 101, 124029 (2020)] that characteristic traits of Milgrom's modified Newtonian dynamics (MOND) can be replicated from an entirely distinct framework: a fractional variant of Newtonian mechanics. To further assess its validity, this proposal needs to be tested in relevant astrophysical scenarios. Here, we investigate its implications on Jeans gravitational instability and related phenomena. We examine scenarios involving classical matter confined by gravity and extend our analysis to the quantum domain, through a Schr\\\"odinger-Newton approach. We also derive a generalized Lane-Emden equation associated with fractional gravity. Through comparisons between the derived stability criteria and the observed stability of Bok globules, we establish constraints on the theory's parameters to align with observational data.","sentences":["It has recently been demonstrated [A. Giusti, Phys.","Rev. D 101, 124029 (2020)] that characteristic traits of Milgrom's modified Newtonian dynamics (MOND) can be replicated from an entirely distinct framework: a fractional variant of Newtonian mechanics.","To further assess its validity, this proposal needs to be tested in relevant astrophysical scenarios.","Here, we investigate its implications on Jeans gravitational instability and related phenomena.","We examine scenarios involving classical matter confined by gravity and extend our analysis to the quantum domain, through a Schr\\\"odinger-Newton approach.","We also derive a generalized Lane-Emden equation associated with fractional gravity.","Through comparisons between the derived stability criteria and the observed stability of Bok globules, we establish constraints on the theory's parameters to align with observational data."],"url":"http://arxiv.org/abs/2405.11395v1","category":"gr-qc"}
{"created":"2024-05-18 21:08:50","title":"Online Mental Stress Detection Using Frontal-channel EEG Recordings in a Classroom Scenario","abstract":"Objective: To investigate the effects of different approaches to EEG preprocessing, channel montage selection, and model architecture on the performance of an online-capable stress detection algorithm in a classroom scenario. Methods: This analysis used EEG data from a longitudinal stress and fatigue study conducted among university students. Their self-reported stress ratings during each class session were the basis for classifying EEG recordings into either normal or elevated stress states. We used a data-processing pipeline that combined Artifact Subspace Reconstruction (ASR)and an Independent Component Analysis (ICA)-based method to achieve online artifact removal. We compared the performance of a Linear Discriminant Analysis (LDA) and a 4-layer neural network as classifiers. We opted for accuracy, balanced accuracy, and F1 score as the metrics for assessing performance. We examined the impact of varying numbers of input channels using different channel montages. Additionally, we explored different window lengths and step sizes during online evaluation. Results: Our online artifact removal method achieved performance comparable to the offline ICA method in both offline and online evaluations. A balanced accuracy of 77% and 78% in an imbalanced binary classification were observed when using the 11-frontal-channel LDA model with the proposed artifact removal method. Moreover, the model performance remained intact when changing the channel montage from 30 full-scalp channels to just 11 frontal channels. During the online evaluation, we achieved the highest balanced accuracy (78%) with a window length of 20 seconds and a step size of 1 second. Significance: This study comprehensively investigates the deployment of stress detection in real-world scenarios. The findings of this study provide insight into the development of daily mental stress monitoring.","sentences":["Objective: To investigate the effects of different approaches to EEG preprocessing, channel montage selection, and model architecture on the performance of an online-capable stress detection algorithm in a classroom scenario.","Methods: This analysis used EEG data from a longitudinal stress and fatigue study conducted among university students.","Their self-reported stress ratings during each class session were the basis for classifying EEG recordings into either normal or elevated stress states.","We used a data-processing pipeline that combined Artifact Subspace Reconstruction (ASR)and an Independent Component Analysis (ICA)-based method to achieve online artifact removal.","We compared the performance of a Linear Discriminant Analysis (LDA) and a 4-layer neural network as classifiers.","We opted for accuracy, balanced accuracy, and F1 score as the metrics for assessing performance.","We examined the impact of varying numbers of input channels using different channel montages.","Additionally, we explored different window lengths and step sizes during online evaluation.","Results:","Our online artifact removal method achieved performance comparable to the offline ICA method in both offline and online evaluations.","A balanced accuracy of 77% and 78% in an imbalanced binary classification were observed when using the 11-frontal-channel LDA model with the proposed artifact removal method.","Moreover, the model performance remained intact when changing the channel montage from 30 full-scalp channels to just 11 frontal channels.","During the online evaluation, we achieved the highest balanced accuracy (78%) with a window length of 20 seconds and a step size of 1 second.","Significance: This study comprehensively investigates the deployment of stress detection in real-world scenarios.","The findings of this study provide insight into the development of daily mental stress monitoring."],"url":"http://arxiv.org/abs/2405.11394v1","category":"q-bio.NC"}
{"created":"2024-05-18 20:12:16","title":"Investigating KAN-Based Physics-Informed Neural Networks for EMI/EMC Simulations","abstract":"The main objective of this paper is to investigate the feasibility of employing Physics-Informed Neural Networks (PINNs) techniques, in particular KolmogorovArnold Networks (KANs), for facilitating Electromagnetic Interference (EMI) simulations. It introduces some common EM problem formulations and how they can be solved using AI-driven solutions instead of lengthy and complex full-wave numerical simulations. This research may open new horizons for green EMI simulation workflows with less energy consumption and feasible computational capacity.","sentences":["The main objective of this paper is to investigate the feasibility of employing Physics-Informed Neural Networks (PINNs) techniques, in particular KolmogorovArnold Networks (KANs), for facilitating Electromagnetic Interference (EMI) simulations.","It introduces some common EM problem formulations and how they can be solved using AI-driven solutions instead of lengthy and complex full-wave numerical simulations.","This research may open new horizons for green EMI simulation workflows with less energy consumption and feasible computational capacity."],"url":"http://arxiv.org/abs/2405.11383v1","category":"cs.LG"}
{"created":"2024-05-18 19:03:55","title":"Memory corrections to Markovian Langevin dynamics","abstract":"Analysis of non-Markovian systems and memory induced phenomena poses an everlasting challenge for physics. As a paradigmatic example we consider a classical Brownian particle of mass $M$ subjected to an external force and exposed to correlated thermal fluctuations. We show that the recently developed approach to this system, in which its non-Markovian dynamics given by the Generalized Langevin Equation is approximated by its memoryless counterpart but with the effective particle mass $M^* < M$, can be derived within the Markovian embedding technique. Using this method we calculate the first and the second order memory correction to Markovian dynamics of the Brownian particle for the memory kernel represented as the Prony series. The second one lowers the effective mass of the system further and improves precision of the approximation. Our work opens the door for the derivation of higher order memory corrections to Markovian Langevin dynamics.","sentences":["Analysis of non-Markovian systems and memory induced phenomena poses an everlasting challenge for physics.","As a paradigmatic example we consider a classical Brownian particle of mass $M$ subjected to an external force and exposed to correlated thermal fluctuations.","We show that the recently developed approach to this system, in which its non-Markovian dynamics given by the Generalized Langevin Equation is approximated by its memoryless counterpart but with the effective particle mass $M^* < M$, can be derived within the Markovian embedding technique.","Using this method we calculate the first and the second order memory correction to Markovian dynamics of the Brownian particle for the memory kernel represented as the Prony series.","The second one lowers the effective mass of the system further and improves precision of the approximation.","Our work opens the door for the derivation of higher order memory corrections to Markovian Langevin dynamics."],"url":"http://arxiv.org/abs/2405.11370v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-18 19:00:56","title":"Weak solutions for a singular beam equation","abstract":"This paper deals with a dynamic Gao beam of infinite length subjected to a moving concentrated Dirac mass. Under appropriate regularity assumptions on the initial data, the problem possesses a weak solution which is obtained as the limit of a sequence of solutions of regularized problems.","sentences":["This paper deals with a dynamic Gao beam of infinite length subjected to a moving concentrated Dirac mass.","Under appropriate regularity assumptions on the initial data, the problem possesses a weak solution which is obtained as the limit of a sequence of solutions of regularized problems."],"url":"http://arxiv.org/abs/2405.11369v1","category":"math.AP"}
{"created":"2024-05-18 18:15:12","title":"Optimal recovery of linear operators from information of random functions","abstract":"The paper concerns problems of the recovery of linear operators defined on sets of functions from information of these functions given with stochastic errors. The constructed optimal recovery methods, in general, do not use all the available information. As a consequence, optimal methods are obtained for recovering derivatives of functions from Sobolev classes by the information of their Fourier transforms given with stochastic errors. A similar problem is considered for solutions of the heat equation.","sentences":["The paper concerns problems of the recovery of linear operators defined on sets of functions from information of these functions given with stochastic errors.","The constructed optimal recovery methods, in general, do not use all the available information.","As a consequence, optimal methods are obtained for recovering derivatives of functions from Sobolev classes by the information of their Fourier transforms given with stochastic errors.","A similar problem is considered for solutions of the heat equation."],"url":"http://arxiv.org/abs/2405.11363v1","category":"math.NA"}
{"created":"2024-05-18 18:11:28","title":"Extinctions as a vestige of instability: the geometry of stability and feasibility","abstract":"Species coexistence is a complex, multifaceted problem. At an equilibrium, coexistence requires two conditions: stability under small perturbations; and feasibility, meaning all species abundances are positive. Which of these two conditions is more restrictive has been debated for many years, with many works focusing on statistical arguments for systems with many species. Within the framework of the Lotka-Volterra equations, we examine the geometry of the region of coexistence in the space of interaction strengths, for symmetric competitive interactions and any finite number of species. We consider what happens when starting at a point within the coexistence region, and changing the interaction strengths continuously until one of the two conditions breaks. We find that coexistence generically breaks through the loss of feasibility, as the abundance of one species reaches zero. An exception to this rule - where stability breaks before feasibility - happens only at isolated points, or more generally on a lower dimensional subset of the boundary.   The reason behind this is that as a stability boundary is approached, some of the abundances generally diverge towards minus infinity, and so go extinct at some earlier point, breaking the feasibility condition first. These results define a new sense in which feasibility is a more restrictive condition than stability, and show that these two requirements are closely interrelated. We then show how our results affect the changes in the set of coexisting species when interaction strengths are changed: a system of coexisting species loses a species by its abundance continuously going to zero, and this new fixed point is unique. As parameters are further changed, multiple alternative equilibria may be found. Finally, we discuss the extent to which our results apply to asymmetric interactions.","sentences":["Species coexistence is a complex, multifaceted problem.","At an equilibrium, coexistence requires two conditions: stability under small perturbations; and feasibility, meaning all species abundances are positive.","Which of these two conditions is more restrictive has been debated for many years, with many works focusing on statistical arguments for systems with many species.","Within the framework of the Lotka-Volterra equations, we examine the geometry of the region of coexistence in the space of interaction strengths, for symmetric competitive interactions and any finite number of species.","We consider what happens when starting at a point within the coexistence region, and changing the interaction strengths continuously until one of the two conditions breaks.","We find that coexistence generically breaks through the loss of feasibility, as the abundance of one species reaches zero.","An exception to this rule - where stability breaks before feasibility - happens only at isolated points, or more generally on a lower dimensional subset of the boundary.   ","The reason behind this is that as a stability boundary is approached, some of the abundances generally diverge towards minus infinity, and so go extinct at some earlier point, breaking the feasibility condition first.","These results define a new sense in which feasibility is a more restrictive condition than stability, and show that these two requirements are closely interrelated.","We then show how our results affect the changes in the set of coexisting species when interaction strengths are changed: a system of coexisting species loses a species by its abundance continuously going to zero, and this new fixed point is unique.","As parameters are further changed, multiple alternative equilibria may be found.","Finally, we discuss the extent to which our results apply to asymmetric interactions."],"url":"http://arxiv.org/abs/2405.11360v1","category":"q-bio.PE"}
{"created":"2024-05-18 17:44:02","title":"Hierarchical Reinforcement Learning Empowered Task Offloading in V2I Networks","abstract":"Edge computing plays an essential role in the vehicle-to-infrastructure (V2I) networks, where vehicles offload their intensive computation tasks to the road-side units for saving energy and reduce the latency. This paper designs the optimal task offloading policy to address the concerns involving processing delay, energy consumption and edge computing cost. Each computation task consisting of some interdependent sub-tasks is characterized as a directed acyclic graph (DAG). In such dynamic networks, a novel hierarchical Offloading scheme is proposed by leveraging deep reinforcement learning (DRL). The inter-dependencies among the DAGs of the computation tasks are extracted using a graph neural network with attention mechanism. A parameterized DRL algorithm is developed to deal with the hierarchical action space containing both discrete and continuous actions. Simulation results with a real-world car speed dataset demonstrate that the proposed scheme can effectively reduce the system overhead.","sentences":["Edge computing plays an essential role in the vehicle-to-infrastructure (V2I) networks, where vehicles offload their intensive computation tasks to the road-side units for saving energy and reduce the latency.","This paper designs the optimal task offloading policy to address the concerns involving processing delay, energy consumption and edge computing cost.","Each computation task consisting of some interdependent sub-tasks is characterized as a directed acyclic graph (DAG).","In such dynamic networks, a novel hierarchical Offloading scheme is proposed by leveraging deep reinforcement learning (DRL).","The inter-dependencies among the DAGs of the computation tasks are extracted using a graph neural network with attention mechanism.","A parameterized DRL algorithm is developed to deal with the hierarchical action space containing both discrete and continuous actions.","Simulation results with a real-world car speed dataset demonstrate that the proposed scheme can effectively reduce the system overhead."],"url":"http://arxiv.org/abs/2405.11352v1","category":"cs.NI"}
{"created":"2024-05-18 17:09:57","title":"Lowest-order Nonstandard Finite Element Methods for Time-Fractional Biharmonic Problem","abstract":"In this work, we consider an initial-boundary value problem for a time-fractional biharmonic equation in a bounded convex polygonal domain in $\\mathbb{R}^2$ with clamped boundary conditions. After establishing the well-posedness, we focus on some regularity results of the solution with respect to the regularity of the problem data. The spatially semidiscrete scheme covers several popular lowest-order piecewise-quadratic finite element schemes, namely, Morley, discontinuous Galerkin, and $C^0$ interior penalty methods, and includes both smooth and nonsmooth initial data. Optimal order error bounds with respect to the regularity assumptions on the data are proved for both homogeneous and nonhomogeneous problems. The numerical experiments validate the theoretical convergence rate results.","sentences":["In this work, we consider an initial-boundary value problem for a time-fractional biharmonic equation in a bounded convex polygonal domain in $\\mathbb{R}^2$ with clamped boundary conditions.","After establishing the well-posedness, we focus on some regularity results of the solution with respect to the regularity of the problem data.","The spatially semidiscrete scheme covers several popular lowest-order piecewise-quadratic finite element schemes, namely, Morley, discontinuous Galerkin, and $C^0$ interior penalty methods, and includes both smooth and nonsmooth initial data.","Optimal order error bounds with respect to the regularity assumptions on the data are proved for both homogeneous and nonhomogeneous problems.","The numerical experiments validate the theoretical convergence rate results."],"url":"http://arxiv.org/abs/2405.11339v1","category":"math.NA"}
{"created":"2024-05-18 16:47:21","title":"Detecting Complex Multi-step Attacks with Explainable Graph Neural Network","abstract":"Complex multi-step attacks have caused significant damage to numerous critical infrastructures. To detect such attacks, graph neural network based methods have shown promising results by modeling the system's events as a graph. However, existing methods still face several challenges when deployed in practice. First, there is a lack of sufficient real attack data especially considering the large volume of normal data. Second, the modeling of event graphs is challenging due to their dynamic and heterogeneous nature. Third, the lack of explanation in learning models undermines the trustworthiness of such methods in production environments. To address the above challenges, in this paper, we propose an attack detection method, Trace2Vec. The approach first designs an erosion function to augment rare attack samples, and integrates them into the event graphs. Next, it models the event graphs via a continuous-time dynamic heterogeneous graph neural network. Finally, it employs the Monte Carlo tree search algorithm to identify events with greater contributions to the attack, thus enhancing the explainability of the detection result. We have implemented a prototype for Trace2Vec, and the experimental evaluations demonstrate its superior detection and explanation performance compared to existing methods.","sentences":["Complex multi-step attacks have caused significant damage to numerous critical infrastructures.","To detect such attacks, graph neural network based methods have shown promising results by modeling the system's events as a graph.","However, existing methods still face several challenges when deployed in practice.","First, there is a lack of sufficient real attack data especially considering the large volume of normal data.","Second, the modeling of event graphs is challenging due to their dynamic and heterogeneous nature.","Third, the lack of explanation in learning models undermines the trustworthiness of such methods in production environments.","To address the above challenges, in this paper, we propose an attack detection method, Trace2Vec.","The approach first designs an erosion function to augment rare attack samples, and integrates them into the event graphs.","Next, it models the event graphs via a continuous-time dynamic heterogeneous graph neural network.","Finally, it employs the Monte Carlo tree search algorithm to identify events with greater contributions to the attack, thus enhancing the explainability of the detection result.","We have implemented a prototype for Trace2Vec, and the experimental evaluations demonstrate its superior detection and explanation performance compared to existing methods."],"url":"http://arxiv.org/abs/2405.11335v1","category":"cs.CR"}
{"created":"2024-05-18 16:46:27","title":"Dirac fermions in a spinning conical G\u00f6del-type spacetime","abstract":"In this paper, we determine the relativistic and nonrelativistic energy levels for Dirac fermions in a spinning conical G\\\"odel-type spacetime in $(2+1)$-dimensions, where we work with the curved Dirac equation in polar coordinates and we use the tetrads formalism. Solving a second-order differential equation for the two components of the Dirac spinor, we obtain a generalized Laguerre equation, and the relativistic energy levels of the fermion and antifermion, where such levels are quantized in terms of the radial and total magnetic quantum numbers $n$ and $m_j$, and explicitly depends on the spin parameter $s$ (describes the ``spin''), spinorial parameter $u$ (describes the two components of the spinor), curvature and rotation parameters $\\alpha$ and $\\beta$ (describes the conical curvature and the angular momentum of the spinning cosmic string), and on the vorticity parameter $\\Omega$ (describes the G\\\"odel-type spacetime). In particular, the quantization is a direct result of the existence of $\\Omega$ (i.e., $\\Omega$ acts as a kind of ``external field or potential''). We see that for $m_j>0$, the energy levels do not depend on $s$ and $u$; however, depend on $n$, $m_j$, $\\alpha$, and $\\beta$. In this case, $\\alpha$ breaks the degeneracy of the energy levels and such levels can increase infinitely in the limit $\\frac{4\\Omega\\beta}{\\alpha}\\to 1$. Already for $m_j<0$, we see that the energy levels depends on $s$, $u$ and $n$; however, it no longer depends on $m_j$, $\\alpha$ and $\\beta$. In this case, it is as if the fermion ``lives only in a flat G\\\"odel-type spacetime''. Besides, we also study the low-energy or nonrelativistic limit of the system. In both cases (relativistic and nonrelativistic), we graphically analyze the behavior of energy levels as a function of $\\Omega$, $\\alpha$, and $\\beta$ for three different values of $n$ (ground state and the first two excited states).","sentences":["In this paper, we determine the relativistic and nonrelativistic energy levels for Dirac fermions in a spinning conical G\\\"odel-type spacetime in $(2+1)$-dimensions, where we work with the curved Dirac equation in polar coordinates and we use the tetrads formalism.","Solving a second-order differential equation for the two components of the Dirac spinor, we obtain a generalized Laguerre equation, and the relativistic energy levels of the fermion and antifermion, where such levels are quantized in terms of the radial and total magnetic quantum numbers $n$ and $m_j$, and explicitly depends on the spin parameter $s$ (describes the ``spin''), spinorial parameter $u$ (describes the two components of the spinor), curvature and rotation parameters $\\alpha$ and $\\beta$ (describes the conical curvature and the angular momentum of the spinning cosmic string), and on the vorticity parameter $\\Omega$ (describes the G\\\"odel-type spacetime).","In particular, the quantization is a direct result of the existence of $\\Omega$ (i.e., $\\Omega$ acts as a kind of ``external field or potential'').","We see that for $m_j>0$, the energy levels do not depend on $s$ and $u$; however, depend on $n$, $m_j$, $\\alpha$, and $\\beta$. In this case, $\\alpha$ breaks the degeneracy of the energy levels and such levels can increase infinitely in the limit $\\frac{4\\Omega\\beta}{\\alpha}\\to 1$. Already for $m_j<0$, we see that the energy levels depends on $s$, $u$ and $n$; however, it no longer depends on $m_j$, $\\alpha$ and $\\beta$. In this case, it is as if the fermion ``lives only in a flat G\\\"odel-type spacetime''.","Besides, we also study the low-energy or nonrelativistic limit of the system.","In both cases (relativistic and nonrelativistic), we graphically analyze the behavior of energy levels as a function of $\\Omega$, $\\alpha$, and $\\beta$ for three different values of $n$ (ground state and the first two excited states)."],"url":"http://arxiv.org/abs/2405.11334v1","category":"gr-qc"}
{"created":"2024-05-18 16:34:57","title":"First Order Linear Proportional Difference Equation with Integration Factor the $(s,t)$-Pantograph Function","abstract":"In this paper, we find solutions to first-order linear proportional difference equations via the $(s,t)$-integration factor method. The $(s,t)$-integration factor involves the $(s,t)$-Pantograph function, which is a generalization of the partial Theta function. Other equations are solved including the $(s,t)$-analog of the Bernoulli equation.","sentences":["In this paper, we find solutions to first-order linear proportional difference equations via the $(s,t)$-integration factor method.","The $(s,t)$-integration factor involves the $(s,t)$-Pantograph function, which is a generalization of the partial Theta function.","Other equations are solved including the $(s,t)$-analog of the Bernoulli equation."],"url":"http://arxiv.org/abs/2405.11332v1","category":"math.CA"}
{"created":"2024-05-18 16:17:05","title":"Risk-neutral valuation of options under arithmetic Brownian motions","abstract":"On April 22, 2020, the CME Group switched to Bachelier pricing for a group of oil futures options. The Bachelier model, or more generally the arithmetic Brownian motion (ABM), is not so widely used in finance, though. This paper provides the first comprehensive survey of options pricing under ABM. Using the risk-neutral valuation, we derive formulas for European options for three underlying types, namely an underlying that does not pay dividends, an underlying that pays a continuous dividend yield, and futures. Further, we derive Black-Scholes-Merton-like partial differential equations, which can in principle be utilized to price American options numerically via finite difference.","sentences":["On April 22, 2020, the CME Group switched to Bachelier pricing for a group of oil futures options.","The Bachelier model, or more generally the arithmetic Brownian motion (ABM), is not so widely used in finance, though.","This paper provides the first comprehensive survey of options pricing under ABM.","Using the risk-neutral valuation, we derive formulas for European options for three underlying types, namely an underlying that does not pay dividends, an underlying that pays a continuous dividend yield, and futures.","Further, we derive Black-Scholes-Merton-like partial differential equations, which can in principle be utilized to price American options numerically via finite difference."],"url":"http://arxiv.org/abs/2405.11329v1","category":"q-fin.PR"}
{"created":"2024-05-18 15:59:41","title":"On the Trajectory Regularity of ODE-based Diffusion Sampling","abstract":"Diffusion-based generative models use stochastic differential equations (SDEs) and their equivalent ordinary differential equations (ODEs) to establish a smooth connection between a complex data distribution and a tractable prior distribution. In this paper, we identify several intriguing trajectory properties in the ODE-based sampling process of diffusion models. We characterize an implicit denoising trajectory and discuss its vital role in forming the coupled sampling trajectory with a strong shape regularity, regardless of the generated content. We also describe a dynamic programming-based scheme to make the time schedule in sampling better fit the underlying trajectory structure. This simple strategy requires minimal modification to any given ODE-based numerical solvers and incurs negligible computational cost, while delivering superior performance in image generation, especially in $5\\sim 10$ function evaluations.","sentences":["Diffusion-based generative models use stochastic differential equations (SDEs) and their equivalent ordinary differential equations (ODEs) to establish a smooth connection between a complex data distribution and a tractable prior distribution.","In this paper, we identify several intriguing trajectory properties in the ODE-based sampling process of diffusion models.","We characterize an implicit denoising trajectory and discuss its vital role in forming the coupled sampling trajectory with a strong shape regularity, regardless of the generated content.","We also describe a dynamic programming-based scheme to make the time schedule in sampling better fit the underlying trajectory structure.","This simple strategy requires minimal modification to any given ODE-based numerical solvers and incurs negligible computational cost, while delivering superior performance in image generation, especially in $5\\sim 10$ function evaluations."],"url":"http://arxiv.org/abs/2405.11326v1","category":"cs.LG"}
{"created":"2024-05-18 14:55:34","title":"Boundary homogenization for partially reactive patches","abstract":"A wide variety of physical, chemical, and biological processes involve diffusive particles interacting with surfaces containing reactive patches. The theory of boundary homogenization seeks to encapsulate the effective reactivity of such a patchy surface by a single trapping rate parameter. In this paper, we derive the trapping rate for partially reactive patches occupying a small fraction of a surface. We use matched asymptotic analysis, double perturbation expansions, and homogenization theory to derive formulas for the trapping rate in terms of the far-field behavior of solutions to certain partial differential equations (PDEs). We then develop kinetic Monte Carlo (KMC) algorithms to rapidly compute these far-field behaviors. These KMC algorithms depend on probabilistic representations of PDE solutions, including using the theory of Brownian local time. We confirm our results by comparing to KMC simulations of the full stochastic system. We further compare our results to prior heuristic approximations.","sentences":["A wide variety of physical, chemical, and biological processes involve diffusive particles interacting with surfaces containing reactive patches.","The theory of boundary homogenization seeks to encapsulate the effective reactivity of such a patchy surface by a single trapping rate parameter.","In this paper, we derive the trapping rate for partially reactive patches occupying a small fraction of a surface.","We use matched asymptotic analysis, double perturbation expansions, and homogenization theory to derive formulas for the trapping rate in terms of the far-field behavior of solutions to certain partial differential equations (PDEs).","We then develop kinetic Monte Carlo (KMC) algorithms to rapidly compute these far-field behaviors.","These KMC algorithms depend on probabilistic representations of PDE solutions, including using the theory of Brownian local time.","We confirm our results by comparing to KMC simulations of the full stochastic system.","We further compare our results to prior heuristic approximations."],"url":"http://arxiv.org/abs/2405.11310v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-18 14:35:57","title":"Quantum-Train: Rethinking Hybrid Quantum-Classical Machine Learning in the Model Compression Perspective","abstract":"We introduces the Quantum-Train(QT) framework, a novel approach that integrates quantum computing with classical machine learning algorithms to address significant challenges in data encoding, model compression, and inference hardware requirements. Even with a slight decrease in accuracy, QT achieves remarkable results by employing a quantum neural network alongside a classical mapping model, which significantly reduces the parameter count from $M$ to $O(\\text{polylog} (M))$ during training. Our experiments demonstrate QT's effectiveness in classification tasks, offering insights into its potential to revolutionize machine learning by leveraging quantum computational advantages. This approach not only improves model efficiency but also reduces generalization errors, showcasing QT's potential across various machine learning applications.","sentences":["We introduces the Quantum-Train(QT) framework, a novel approach that integrates quantum computing with classical machine learning algorithms to address significant challenges in data encoding, model compression, and inference hardware requirements.","Even with a slight decrease in accuracy, QT achieves remarkable results by employing a quantum neural network alongside a classical mapping model, which significantly reduces the parameter count from $M$ to $O(\\text{polylog} (M))$ during training.","Our experiments demonstrate QT's effectiveness in classification tasks, offering insights into its potential to revolutionize machine learning by leveraging quantum computational advantages.","This approach not only improves model efficiency but also reduces generalization errors, showcasing QT's potential across various machine learning applications."],"url":"http://arxiv.org/abs/2405.11304v1","category":"quant-ph"}
{"created":"2024-05-20 17:37:10","title":"Multi-View Attentive Contextualization for Multi-View 3D Object Detection","abstract":"We present Multi-View Attentive Contextualization (MvACon), a simple yet effective method for improving 2D-to-3D feature lifting in query-based multi-view 3D (MV3D) object detection. Despite remarkable progress witnessed in the field of query-based MV3D object detection, prior art often suffers from either the lack of exploiting high-resolution 2D features in dense attention-based lifting, due to high computational costs, or from insufficiently dense grounding of 3D queries to multi-scale 2D features in sparse attention-based lifting. Our proposed MvACon hits the two birds with one stone using a representationally dense yet computationally sparse attentive feature contextualization scheme that is agnostic to specific 2D-to-3D feature lifting approaches. In experiments, the proposed MvACon is thoroughly tested on the nuScenes benchmark, using both the BEVFormer and its recent 3D deformable attention (DFA3D) variant, as well as the PETR, showing consistent detection performance improvement, especially in enhancing performance in location, orientation, and velocity prediction. It is also tested on the Waymo-mini benchmark using BEVFormer with similar improvement. We qualitatively and quantitatively show that global cluster-based contexts effectively encode dense scene-level contexts for MV3D object detection. The promising results of our proposed MvACon reinforces the adage in computer vision -- ``(contextualized) feature matters\".","sentences":["We present Multi-View Attentive Contextualization (MvACon), a simple yet effective method for improving 2D-to-3D feature lifting in query-based multi-view 3D (MV3D) object detection.","Despite remarkable progress witnessed in the field of query-based MV3D object detection, prior art often suffers from either the lack of exploiting high-resolution 2D features in dense attention-based lifting, due to high computational costs, or from insufficiently dense grounding of 3D queries to multi-scale 2D features in sparse attention-based lifting.","Our proposed MvACon hits the two birds with one stone using a representationally dense yet computationally sparse attentive feature contextualization scheme that is agnostic to specific 2D-to-3D feature lifting approaches.","In experiments, the proposed MvACon is thoroughly tested on the nuScenes benchmark, using both the BEVFormer and its recent 3D deformable attention (DFA3D) variant, as well as the PETR, showing consistent detection performance improvement, especially in enhancing performance in location, orientation, and velocity prediction.","It is also tested on the Waymo-mini benchmark using BEVFormer with similar improvement.","We qualitatively and quantitatively show that global cluster-based contexts effectively encode dense scene-level contexts for MV3D object detection.","The promising results of our proposed MvACon reinforces the adage in computer vision -- ``(contextualized) feature matters\"."],"url":"http://arxiv.org/abs/2405.12200v1","category":"cs.CV"}
{"created":"2024-05-20 15:48:40","title":"SkyCURTAINs: Model agnostic search for Stellar Streams with Gaia data","abstract":"We present SkyCURTAINs, a data driven and model agnostic method to search for stellar streams in the Milky Way galaxy using data from the Gaia telescope. SkyCURTAINs is a weakly supervised machine learning algorithm that builds a background enriched template in the signal region by leveraging the correlation of the source's characterising features with their proper motion in the sky. This allows for a more representative template of the background in the signal region, and reduces the false positives in the search for stellar streams. The minimal model assumptions in the SkyCURTAINs method allow for a flexible and efficient search for various kinds of anomalies such as streams, globular clusters, or dwarf galaxies directly from the data. We test the performance of SkyCURTAINs on the GD-1 stream and show that it is able to recover the stream with a purity of 75.4% which is an improvement of over 10% over existing machine learning based methods while retaining a signal efficiency of 37.9%.","sentences":["We present SkyCURTAINs, a data driven and model agnostic method to search for stellar streams in the Milky Way galaxy using data from the Gaia telescope.","SkyCURTAINs is a weakly supervised machine learning algorithm that builds a background enriched template in the signal region by leveraging the correlation of the source's characterising features with their proper motion in the sky.","This allows for a more representative template of the background in the signal region, and reduces the false positives in the search for stellar streams.","The minimal model assumptions in the SkyCURTAINs method allow for a flexible and efficient search for various kinds of anomalies such as streams, globular clusters, or dwarf galaxies directly from the data.","We test the performance of SkyCURTAINs on the GD-1 stream and show that it is able to recover the stream with a purity of 75.4% which is an improvement of over 10% over existing machine learning based methods while retaining a signal efficiency of 37.9%."],"url":"http://arxiv.org/abs/2405.12131v1","category":"astro-ph.GA"}
{"created":"2024-05-20 15:08:15","title":"Using Unsupervised Learning to Explore Robot-Pedestrian Interactions in Urban Environments","abstract":"This study identifies a gap in data-driven approaches to robot-centric pedestrian interactions and proposes a corresponding pipeline. The pipeline utilizes unsupervised learning techniques to identify patterns in interaction data of urban environments, specifically focusing on conflict scenarios. Analyzed features include the robot's and pedestrian's speed and contextual parameters such as proximity to intersections. They are extracted and reduced in dimensionality using Principal Component Analysis (PCA). Finally, K-means clustering is employed to uncover underlying patterns in the interaction data. A use case application of the pipeline is presented, utilizing real-world robot mission data from a mid-sized German city. The results indicate the need for enriching interaction representations with contextual information to enable fine-grained analysis and reasoning. Nevertheless, they also highlight the need for expanding the data set and incorporating additional contextual factors to enhance the robots situational awareness and interaction quality.","sentences":["This study identifies a gap in data-driven approaches to robot-centric pedestrian interactions and proposes a corresponding pipeline.","The pipeline utilizes unsupervised learning techniques to identify patterns in interaction data of urban environments, specifically focusing on conflict scenarios.","Analyzed features include the robot's and pedestrian's speed and contextual parameters such as proximity to intersections.","They are extracted and reduced in dimensionality using Principal Component Analysis (PCA).","Finally, K-means clustering is employed to uncover underlying patterns in the interaction data.","A use case application of the pipeline is presented, utilizing real-world robot mission data from a mid-sized German city.","The results indicate the need for enriching interaction representations with contextual information to enable fine-grained analysis and reasoning.","Nevertheless, they also highlight the need for expanding the data set and incorporating additional contextual factors to enhance the robots situational awareness and interaction quality."],"url":"http://arxiv.org/abs/2405.12098v1","category":"cs.RO"}
{"created":"2024-05-20 14:57:16","title":"Channel Balance Interpolation in the Lightning Network via Machine Learning","abstract":"The Bitcoin Lightning Network is a Layer 2 payment protocol that addresses Bitcoin's scalability by facilitating quick and cost effective transactions through payment channels. This research explores the feasibility of using machine learning models to interpolate channel balances within the network, which can be used for optimizing the network's pathfinding algorithms. While there has been much exploration in balance probing and multipath payment protocols, predicting channel balances using solely node and channel features remains an uncharted area. This paper evaluates the performance of several machine learning models against two heuristic baselines and investigates the predictive capabilities of various features. Our model performs favorably in experimental evaluation, outperforming by 10% against an equal split baseline where both edges are assigned half of the channel capacity.","sentences":["The Bitcoin Lightning Network is a Layer 2 payment protocol that addresses Bitcoin's scalability by facilitating quick and cost effective transactions through payment channels.","This research explores the feasibility of using machine learning models to interpolate channel balances within the network, which can be used for optimizing the network's pathfinding algorithms.","While there has been much exploration in balance probing and multipath payment protocols, predicting channel balances using solely node and channel features remains an uncharted area.","This paper evaluates the performance of several machine learning models against two heuristic baselines and investigates the predictive capabilities of various features.","Our model performs favorably in experimental evaluation, outperforming by 10% against an equal split baseline where both edges are assigned half of the channel capacity."],"url":"http://arxiv.org/abs/2405.12087v1","category":"cs.LG"}
{"created":"2024-05-20 12:53:32","title":"TEGLIE: Transformer encoders as strong gravitational lens finders in KiDS","abstract":"We apply a state-of-the-art transformer algorithm to 221 deg$^2$ of the Kilo Degree Survey (KiDS) to search for new strong gravitational lenses (SGL). We test four transformer encoders trained on simulated data from the Strong Lens Finding Challenge on KiDS survey data. The best performing model is fine-tuned on real images of SGL candidates identified in previous searches. To expand the dataset for fine-tuning, data augmentation techniques are employed, including rotation, flipping, transposition, and white noise injection. The network fine-tuned with rotated, flipped, and transposed images exhibited the best performance and is used to hunt for SGL in the overlapping region of the Galaxy And Mass Assembly (GAMA) and KiDS surveys on galaxies up to $z$=0.8. Candidate SGLs are matched with those from other surveys and examined using GAMA data to identify blended spectra resulting from the signal from multiple objects in a fiber. We observe that fine-tuning the transformer encoder to the KiDS data reduces the number of false positives by 70%. Additionally, applying the fine-tuned model to a sample of $\\sim$ 5,000,000 galaxies results in a list of $\\sim$ 51,000 SGL candidates. Upon visual inspection, this list is narrowed down to 231 candidates. Combined with the SGL candidates identified in the model testing, our final sample includes 264 candidates, with 71 high-confidence SGLs of which 44 are new discoveries. We propose fine-tuning via real augmented images as a viable approach to mitigating false positives when transitioning from simulated lenses to real surveys. Additionally, we provide a list of 121 false positives that exhibit features similar to lensed objects, which can benefit the training of future machine learning models in this field.","sentences":["We apply a state-of-the-art transformer algorithm to 221 deg$^2$ of the Kilo Degree Survey (KiDS) to search for new strong gravitational lenses (SGL).","We test four transformer encoders trained on simulated data from the Strong Lens Finding Challenge on KiDS survey data.","The best performing model is fine-tuned on real images of SGL candidates identified in previous searches.","To expand the dataset for fine-tuning, data augmentation techniques are employed, including rotation, flipping, transposition, and white noise injection.","The network fine-tuned with rotated, flipped, and transposed images exhibited the best performance and is used to hunt for SGL in the overlapping region of the Galaxy And Mass Assembly (GAMA) and KiDS surveys on galaxies up to $z$=0.8.","Candidate SGLs are matched with those from other surveys and examined using GAMA data to identify blended spectra resulting from the signal from multiple objects in a fiber.","We observe that fine-tuning the transformer encoder to the KiDS data reduces the number of false positives by 70%.","Additionally, applying the fine-tuned model to a sample of $\\sim$ 5,000,000 galaxies results in a list of $\\sim$ 51,000 SGL candidates.","Upon visual inspection, this list is narrowed down to 231 candidates.","Combined with the SGL candidates identified in the model testing, our final sample includes 264 candidates, with 71 high-confidence SGLs of which 44 are new discoveries.","We propose fine-tuning via real augmented images as a viable approach to mitigating false positives when transitioning from simulated lenses to real surveys.","Additionally, we provide a list of 121 false positives that exhibit features similar to lensed objects, which can benefit the training of future machine learning models in this field."],"url":"http://arxiv.org/abs/2405.11992v1","category":"astro-ph.GA"}
{"created":"2024-05-20 11:28:32","title":"Exploring Commonalities in Explanation Frameworks: A Multi-Domain Survey Analysis","abstract":"This study presents insights gathered from surveys and discussions with specialists in three domains, aiming to find essential elements for a universal explanation framework that could be applied to these and other similar use cases. The insights are incorporated into a software tool that utilizes GP algorithms, known for their interpretability. The applications analyzed include a medical scenario (involving predictive ML), a retail use case (involving prescriptive ML), and an energy use case (also involving predictive ML). We interviewed professionals from each sector, transcribing their conversations for further analysis. Additionally, experts and non-experts in these fields filled out questionnaires designed to probe various dimensions of explanatory methods. The findings indicate a universal preference for sacrificing a degree of accuracy in favor of greater explainability. Additionally, we highlight the significance of feature importance and counterfactual explanations as critical components of such a framework. Our questionnaires are publicly available to facilitate the dissemination of knowledge in the field of XAI.","sentences":["This study presents insights gathered from surveys and discussions with specialists in three domains, aiming to find essential elements for a universal explanation framework that could be applied to these and other similar use cases.","The insights are incorporated into a software tool that utilizes GP algorithms, known for their interpretability.","The applications analyzed include a medical scenario (involving predictive ML), a retail use case (involving prescriptive ML), and an energy use case (also involving predictive ML).","We interviewed professionals from each sector, transcribing their conversations for further analysis.","Additionally, experts and non-experts in these fields filled out questionnaires designed to probe various dimensions of explanatory methods.","The findings indicate a universal preference for sacrificing a degree of accuracy in favor of greater explainability.","Additionally, we highlight the significance of feature importance and counterfactual explanations as critical components of such a framework.","Our questionnaires are publicly available to facilitate the dissemination of knowledge in the field of XAI."],"url":"http://arxiv.org/abs/2405.11958v1","category":"cs.LG"}
{"created":"2024-05-20 11:02:53","title":"Distinguished In Uniform: Self Attention Vs. Virtual Nodes","abstract":"Graph Transformers (GTs) such as SAN and GPS are graph processing models that combine Message-Passing GNNs (MPGNNs) with global Self-Attention. They were shown to be universal function approximators, with two reservations: 1. The initial node features must be augmented with certain positional encodings. 2. The approximation is non-uniform: Graphs of different sizes may require a different approximating network.   We first clarify that this form of universality is not unique to GTs: Using the same positional encodings, also pure MPGNNs and even 2-layer MLPs are non-uniform universal approximators. We then consider uniform expressivity: The target function is to be approximated by a single network for graphs of all sizes. There, we compare GTs to the more efficient MPGNN + Virtual Node architecture. The essential difference between the two model definitions is in their global computation method -- Self-Attention Vs Virtual Node. We prove that none of the models is a uniform-universal approximator, before proving our main result: Neither model's uniform expressivity subsumes the other's. We demonstrate the theory with experiments on synthetic data. We further augment our study with real-world datasets, observing mixed results which indicate no clear ranking in practice as well.","sentences":["Graph Transformers (GTs) such as SAN and GPS are graph processing models that combine Message-Passing GNNs (MPGNNs) with global Self-Attention.","They were shown to be universal function approximators, with two reservations:","1.","The initial node features must be augmented with certain positional encodings.","2.","The approximation is non-uniform: Graphs of different sizes may require a different approximating network.   ","We first clarify that this form of universality is not unique to GTs: Using the same positional encodings, also pure MPGNNs and even 2-layer MLPs are non-uniform universal approximators.","We then consider uniform expressivity: The target function is to be approximated by a single network for graphs of all sizes.","There, we compare GTs to the more efficient MPGNN + Virtual Node architecture.","The essential difference between the two model definitions is in their global computation method -- Self-Attention Vs Virtual Node.","We prove that none of the models is a uniform-universal approximator, before proving our main result: Neither model's uniform expressivity subsumes the other's.","We demonstrate the theory with experiments on synthetic data.","We further augment our study with real-world datasets, observing mixed results which indicate no clear ranking in practice as well."],"url":"http://arxiv.org/abs/2405.11951v1","category":"cs.LG"}
{"created":"2024-05-20 10:53:36","title":"A Fast-cadenced Search for Gamma-Ray Burst Orphan Afterglows with the Deeper, Wider, Faster Programme","abstract":"The relativistic outflows that produce Long GRBs (LGRBs) can be described by a structured jet model where prompt $\\gamma$-ray emission is restricted to a narrow region in the jet's core. Viewing the jet off-axis from the core, a population of afterglows without an associated GRB detection can be predicted. In this work, we conduct an archival search for these `orphan' afterglows (OAs) with minute-cadence, deep ($g\\sim23$) data from the Dark Energy Camera (DECam) taken as part of the Deeper, Wider, Faster programme (DWF). We introduce a method to select fast-evolving OA candidates within DWF data that comprises a machine learning model, based on a realistic synthetic population of OAs. Using this classifier, we recover 51 OA candidates. Of these candidates, 42 are likely flare events from M-class stars. The remaining nine possess quiescent, coincident sources in archival data with angular profiles consistent with a star. Comparing these sources to the expected population of LGRB host galaxies, we conclude that these are likely Galactic events. We calculate an upper limit on the rate of OAs down to $g<22$ AB mag of 2.49 deg$^{-2}$yr$^{-1}$ using our criteria and constrain the parameter space of possible jet structures. We also place an upper limit of the characteristic angle between the $\\gamma$-ray emitting region and the jet's half opening angle, assuming a shallow angular jet structure with a power-law index of 0.8. These values are $58.3^{\\circ}$ and $56.6^{\\circ}$ for a smooth power-law and a power-law with core jet model respectively.","sentences":["The relativistic outflows that produce Long GRBs (LGRBs) can be described by a structured jet model where prompt $\\gamma$-ray emission is restricted to a narrow region in the jet's core.","Viewing the jet off-axis from the core, a population of afterglows without an associated GRB detection can be predicted.","In this work, we conduct an archival search for these `orphan' afterglows (OAs) with minute-cadence, deep ($g\\sim23$) data from the Dark Energy Camera (DECam) taken as part of the Deeper, Wider, Faster programme (DWF).","We introduce a method to select fast-evolving OA candidates within DWF data that comprises a machine learning model, based on a realistic synthetic population of OAs.","Using this classifier, we recover 51 OA candidates.","Of these candidates, 42 are likely flare events from M-class stars.","The remaining nine possess quiescent, coincident sources in archival data with angular profiles consistent with a star.","Comparing these sources to the expected population of LGRB host galaxies, we conclude that these are likely Galactic events.","We calculate an upper limit on the rate of OAs down to $g<22$ AB mag of 2.49 deg$^{-2}$yr$^{-1}$ using our criteria and constrain the parameter space of possible jet structures.","We also place an upper limit of the characteristic angle between the $\\gamma$-ray emitting region and the jet's half opening angle, assuming a shallow angular jet structure with a power-law index of 0.8.","These values are $58.3^{\\circ}$ and $56.6^{\\circ}$ for a smooth power-law and a power-law with core jet model respectively."],"url":"http://arxiv.org/abs/2405.11949v1","category":"astro-ph.HE"}
{"created":"2024-05-20 10:12:23","title":"Data Contamination Calibration for Black-box LLMs","abstract":"The rapid advancements of Large Language Models (LLMs) tightly associate with the expansion of the training data size. However, the unchecked ultra-large-scale training sets introduce a series of potential risks like data contamination, i.e. the benchmark data is used for training. In this work, we propose a holistic method named Polarized Augment Calibration (PAC) along with a new to-be-released dataset to detect the contaminated data and diminish the contamination effect. PAC extends the popular MIA (Membership Inference Attack) -- from machine learning community -- by forming a more global target at detecting training data to Clarify invisible training data. As a pioneering work, PAC is very much plug-and-play that can be integrated with most (if not all) current white- and black-box LLMs. By extensive experiments, PAC outperforms existing methods by at least 4.5%, towards data contamination detection on more 4 dataset formats, with more than 10 base LLMs. Besides, our application in real-world scenarios highlights the prominent presence of contamination and related issues.","sentences":["The rapid advancements of Large Language Models (LLMs) tightly associate with the expansion of the training data size.","However, the unchecked ultra-large-scale training sets introduce a series of potential risks like data contamination, i.e. the benchmark data is used for training.","In this work, we propose a holistic method named Polarized Augment Calibration (PAC) along with a new to-be-released dataset to detect the contaminated data and diminish the contamination effect.","PAC extends the popular MIA (Membership Inference Attack) -- from machine learning community -- by forming a more global target at detecting training data to Clarify invisible training data.","As a pioneering work, PAC is very much plug-and-play that can be integrated with most (if not all) current white- and black-box LLMs.","By extensive experiments, PAC outperforms existing methods by at least 4.5%, towards data contamination detection on more 4 dataset formats, with more than 10 base LLMs.","Besides, our application in real-world scenarios highlights the prominent presence of contamination and related issues."],"url":"http://arxiv.org/abs/2405.11930v1","category":"cs.LG"}
{"created":"2024-05-20 08:57:39","title":"Vertical Federated Learning Hybrid Local Pre-training","abstract":"Vertical Federated Learning (VFL), which has a broad range of real-world applications, has received much attention in both academia and industry. Enterprises aspire to exploit more valuable features of the same users from diverse departments to boost their model prediction skills. VFL addresses this demand and concurrently secures individual parties from exposing their raw data. However, conventional VFL encounters a bottleneck as it only leverages aligned samples, whose size shrinks with more parties involved, resulting in data scarcity and the waste of unaligned data. To address this problem, we propose a novel VFL Hybrid Local Pre-training (VFLHLP) approach. VFLHLP first pre-trains local networks on the local data of participating parties. Then it utilizes these pre-trained networks to adjust the sub-model for the labeled party or enhance representation learning for other parties during downstream federated learning on aligned data, boosting the performance of federated models.","sentences":["Vertical Federated Learning (VFL), which has a broad range of real-world applications, has received much attention in both academia and industry.","Enterprises aspire to exploit more valuable features of the same users from diverse departments to boost their model prediction skills.","VFL addresses this demand and concurrently secures individual parties from exposing their raw data.","However, conventional VFL encounters a bottleneck as it only leverages aligned samples, whose size shrinks with more parties involved, resulting in data scarcity and the waste of unaligned data.","To address this problem, we propose a novel VFL Hybrid Local Pre-training (VFLHLP) approach.","VFLHLP first pre-trains local networks on the local data of participating parties.","Then it utilizes these pre-trained networks to adjust the sub-model for the labeled party or enhance representation learning for other parties during downstream federated learning on aligned data, boosting the performance of federated models."],"url":"http://arxiv.org/abs/2405.11884v1","category":"cs.LG"}
{"created":"2024-05-20 05:18:47","title":"Source Localization by Multidimensional Steered Response Power Mapping with Sparse Bayesian Learning","abstract":"We propose an advance Steered Response Power (SRP) method for localizing multiple sources. While conventional SRP performs well in adverse conditions, it remains to struggle in scenarios with closely neighboring sources, resulting in ambiguous SRP maps. We address this issue by applying sparsity optimization in SRP to obtain high-resolution maps. Our approach represents SRP maps as multidimensional matrices to preserve time-frequency information and further improve performance in unfavorable conditions. We use multi-dictionary Sparse Bayesian Learning to localize sources without needing prior knowledge of their quantity. We validate our method through practical experiments with a 16-channel planar microphone array and compare against three other SRP and sparsity-based methods. Our multidimensional SRP approach outperforms conventional SRP and the current state-of-the-art sparse SRP methods for localizing closely spaced sources in a reverberant room.","sentences":["We propose an advance Steered Response Power (SRP) method for localizing multiple sources.","While conventional SRP performs well in adverse conditions, it remains to struggle in scenarios with closely neighboring sources, resulting in ambiguous SRP maps.","We address this issue by applying sparsity optimization in SRP to obtain high-resolution maps.","Our approach represents SRP maps as multidimensional matrices to preserve time-frequency information and further improve performance in unfavorable conditions.","We use multi-dictionary Sparse Bayesian Learning to localize sources without needing prior knowledge of their quantity.","We validate our method through practical experiments with a 16-channel planar microphone array and compare against three other SRP and sparsity-based methods.","Our multidimensional SRP approach outperforms conventional SRP and the current state-of-the-art sparse SRP methods for localizing closely spaced sources in a reverberant room."],"url":"http://arxiv.org/abs/2405.11792v1","category":"eess.AS"}
{"created":"2024-05-20 04:31:04","title":"Exploring Ordinality in Text Classification: A Comparative Study of Explicit and Implicit Techniques","abstract":"Ordinal Classification (OC) is a widely encountered challenge in Natural Language Processing (NLP), with applications in various domains such as sentiment analysis, rating prediction, and more. Previous approaches to tackle OC have primarily focused on modifying existing or creating novel loss functions that \\textbf{explicitly} account for the ordinal nature of labels. However, with the advent of Pretrained Language Models (PLMs), it became possible to tackle ordinality through the \\textbf{implicit} semantics of the labels as well. This paper provides a comprehensive theoretical and empirical examination of both these approaches. Furthermore, we also offer strategic recommendations regarding the most effective approach to adopt based on specific settings.","sentences":["Ordinal Classification (OC) is a widely encountered challenge in Natural Language Processing (NLP), with applications in various domains such as sentiment analysis, rating prediction, and more.","Previous approaches to tackle OC have primarily focused on modifying existing or creating novel loss functions that \\textbf{explicitly} account for the ordinal nature of labels.","However, with the advent of Pretrained Language Models (PLMs), it became possible to tackle ordinality through the \\textbf{implicit} semantics of the labels as well.","This paper provides a comprehensive theoretical and empirical examination of both these approaches.","Furthermore, we also offer strategic recommendations regarding the most effective approach to adopt based on specific settings."],"url":"http://arxiv.org/abs/2405.11775v1","category":"cs.CL"}
{"created":"2024-05-20 04:15:59","title":"Learning Spatial Similarity Distribution for Few-shot Object Counting","abstract":"Few-shot object counting aims to count the number of objects in a query image that belong to the same class as the given exemplar images. Existing methods compute the similarity between the query image and exemplars in the 2D spatial domain and perform regression to obtain the counting number. However, these methods overlook the rich information about the spatial distribution of similarity on the exemplar images, leading to significant impact on matching accuracy. To address this issue, we propose a network learning Spatial Similarity Distribution (SSD) for few-shot object counting, which preserves the spatial structure of exemplar features and calculates a 4D similarity pyramid point-to-point between the query features and exemplar features, capturing the complete distribution information for each point in the 4D similarity space. We propose a Similarity Learning Module (SLM) which applies the efficient center-pivot 4D convolutions on the similarity pyramid to map different similarity distributions to distinct predicted density values, thereby obtaining accurate count. Furthermore, we also introduce a Feature Cross Enhancement (FCE) module that enhances query and exemplar features mutually to improve the accuracy of feature matching. Our approach outperforms state-of-the-art methods on multiple datasets, including FSC-147 and CARPK. Code is available at https://github.com/CBalance/SSD.","sentences":["Few-shot object counting aims to count the number of objects in a query image that belong to the same class as the given exemplar images.","Existing methods compute the similarity between the query image and exemplars in the 2D spatial domain and perform regression to obtain the counting number.","However, these methods overlook the rich information about the spatial distribution of similarity on the exemplar images, leading to significant impact on matching accuracy.","To address this issue, we propose a network learning Spatial Similarity Distribution (SSD) for few-shot object counting, which preserves the spatial structure of exemplar features and calculates a 4D similarity pyramid point-to-point between the query features and exemplar features, capturing the complete distribution information for each point in the 4D similarity space.","We propose a Similarity Learning Module (SLM) which applies the efficient center-pivot 4D convolutions on the similarity pyramid to map different similarity distributions to distinct predicted density values, thereby obtaining accurate count.","Furthermore, we also introduce a Feature Cross Enhancement (FCE) module that enhances query and exemplar features mutually to improve the accuracy of feature matching.","Our approach outperforms state-of-the-art methods on multiple datasets, including FSC-147 and CARPK.","Code is available at https://github.com/CBalance/SSD."],"url":"http://arxiv.org/abs/2405.11770v1","category":"cs.CV"}
{"created":"2024-05-20 01:37:21","title":"Feasibility Consistent Representation Learning for Safe Reinforcement Learning","abstract":"In the field of safe reinforcement learning (RL), finding a balance between satisfying safety constraints and optimizing reward performance presents a significant challenge. A key obstacle in this endeavor is the estimation of safety constraints, which is typically more difficult than estimating a reward metric due to the sparse nature of the constraint signals. To address this issue, we introduce a novel framework named Feasibility Consistent Safe Reinforcement Learning (FCSRL). This framework combines representation learning with feasibility-oriented objectives to identify and extract safety-related information from the raw state for safe RL. Leveraging self-supervised learning techniques and a more learnable safety metric, our approach enhances the policy learning and constraint estimation. Empirical evaluations across a range of vector-state and image-based tasks demonstrate that our method is capable of learning a better safety-aware embedding and achieving superior performance than previous representation learning baselines.","sentences":["In the field of safe reinforcement learning (RL), finding a balance between satisfying safety constraints and optimizing reward performance presents a significant challenge.","A key obstacle in this endeavor is the estimation of safety constraints, which is typically more difficult than estimating a reward metric due to the sparse nature of the constraint signals.","To address this issue, we introduce a novel framework named Feasibility Consistent Safe Reinforcement Learning (FCSRL).","This framework combines representation learning with feasibility-oriented objectives to identify and extract safety-related information from the raw state for safe RL.","Leveraging self-supervised learning techniques and a more learnable safety metric, our approach enhances the policy learning and constraint estimation.","Empirical evaluations across a range of vector-state and image-based tasks demonstrate that our method is capable of learning a better safety-aware embedding and achieving superior performance than previous representation learning baselines."],"url":"http://arxiv.org/abs/2405.11718v1","category":"cs.LG"}
{"created":"2024-05-19 18:42:36","title":"Morphological Prototyping for Unsupervised Slide Representation Learning in Computational Pathology","abstract":"Representation learning of pathology whole-slide images (WSIs) has been has primarily relied on weak supervision with Multiple Instance Learning (MIL). However, the slide representations resulting from this approach are highly tailored to specific clinical tasks, which limits their expressivity and generalization, particularly in scenarios with limited data. Instead, we hypothesize that morphological redundancy in tissue can be leveraged to build a task-agnostic slide representation in an unsupervised fashion. To this end, we introduce PANTHER, a prototype-based approach rooted in the Gaussian mixture model that summarizes the set of WSI patches into a much smaller set of morphological prototypes. Specifically, each patch is assumed to have been generated from a mixture distribution, where each mixture component represents a morphological exemplar. Utilizing the estimated mixture parameters, we then construct a compact slide representation that can be readily used for a wide range of downstream tasks. By performing an extensive evaluation of PANTHER on subtyping and survival tasks using 13 datasets, we show that 1) PANTHER outperforms or is on par with supervised MIL baselines and 2) the analysis of morphological prototypes brings new qualitative and quantitative insights into model interpretability.","sentences":["Representation learning of pathology whole-slide images (WSIs) has been has primarily relied on weak supervision with Multiple Instance Learning (MIL).","However, the slide representations resulting from this approach are highly tailored to specific clinical tasks, which limits their expressivity and generalization, particularly in scenarios with limited data.","Instead, we hypothesize that morphological redundancy in tissue can be leveraged to build a task-agnostic slide representation in an unsupervised fashion.","To this end, we introduce PANTHER, a prototype-based approach rooted in the Gaussian mixture model that summarizes the set of WSI patches into a much smaller set of morphological prototypes.","Specifically, each patch is assumed to have been generated from a mixture distribution, where each mixture component represents a morphological exemplar.","Utilizing the estimated mixture parameters, we then construct a compact slide representation that can be readily used for a wide range of downstream tasks.","By performing an extensive evaluation of PANTHER on subtyping and survival tasks using 13 datasets, we show that 1) PANTHER outperforms or is on par with supervised MIL baselines and 2) the analysis of morphological prototypes brings new qualitative and quantitative insights into model interpretability."],"url":"http://arxiv.org/abs/2405.11643v1","category":"cs.CV"}
{"created":"2024-05-19 17:49:33","title":"Geometry-Aware Instrumental Variable Regression","abstract":"Instrumental variable (IV) regression can be approached through its formulation in terms of conditional moment restrictions (CMR). Building on variants of the generalized method of moments, most CMR estimators are implicitly based on approximating the population data distribution via reweightings of the empirical sample. While for large sample sizes, in the independent identically distributed (IID) setting, reweightings can provide sufficient flexibility, they might fail to capture the relevant information in presence of corrupted data or data prone to adversarial attacks. To address these shortcomings, we propose the Sinkhorn Method of Moments, an optimal transport-based IV estimator that takes into account the geometry of the data manifold through data-derivative information. We provide a simple plug-and-play implementation of our method that performs on par with related estimators in standard settings but improves robustness against data corruption and adversarial attacks.","sentences":["Instrumental variable (IV) regression can be approached through its formulation in terms of conditional moment restrictions (CMR).","Building on variants of the generalized method of moments, most CMR estimators are implicitly based on approximating the population data distribution via reweightings of the empirical sample.","While for large sample sizes, in the independent identically distributed (IID) setting, reweightings can provide sufficient flexibility, they might fail to capture the relevant information in presence of corrupted data or data prone to adversarial attacks.","To address these shortcomings, we propose the Sinkhorn Method of Moments, an optimal transport-based IV estimator that takes into account the geometry of the data manifold through data-derivative information.","We provide a simple plug-and-play implementation of our method that performs on par with related estimators in standard settings but improves robustness against data corruption and adversarial attacks."],"url":"http://arxiv.org/abs/2405.11633v1","category":"cs.LG"}
{"created":"2024-05-19 17:36:55","title":"Universal spectra of noisy parameterized quantum circuits","abstract":"Random unitaries are an important resource for quantum information processing. While their universal properties have been thoroughly analyzed, it is not known what happens to these properties when the unitaries are sampled on the present-day noisy intermediate-scale quantum (NISQ) computers. We implement parameterized circuits, which have been proposed as a means to generate random unitaries, on a transmon platform and model these implementations as quantum maps. To retrieve the maps, a machine-learning assisted tomography is used. We find the spectrum of a map to be either an annulus or a disk depending on the circuit depth and detect an annulus-disk transition. By their spectral properties, the retrieved maps appear to be very similar to a recently introduced ensemble of random maps, for which spectral densities can be analytically evaluated.","sentences":["Random unitaries are an important resource for quantum information processing.","While their universal properties have been thoroughly analyzed, it is not known what happens to these properties when the unitaries are sampled on the present-day noisy intermediate-scale quantum (NISQ) computers.","We implement parameterized circuits, which have been proposed as a means to generate random unitaries, on a transmon platform and model these implementations as quantum maps.","To retrieve the maps, a machine-learning assisted tomography is used.","We find the spectrum of a map to be either an annulus or a disk depending on the circuit depth and detect an annulus-disk transition.","By their spectral properties, the retrieved maps appear to be very similar to a recently introduced ensemble of random maps, for which spectral densities can be analytically evaluated."],"url":"http://arxiv.org/abs/2405.11625v1","category":"quant-ph"}
{"created":"2024-05-19 17:23:04","title":"Continuous Predictive Modeling of Clinical Notes and ICD Codes in Patient Health Records","abstract":"Electronic Health Records (EHR) serve as a valuable source of patient information, offering insights into medical histories, treatments, and outcomes. Previous research has developed systems for detecting applicable ICD codes that should be assigned while writing a given EHR document, mainly focusing on discharge summaries written at the end of a hospital stay. In this work, we investigate the potential of predicting these codes for the whole patient stay at different time points during their stay, even before they are officially assigned by clinicians. The development of methods to predict diagnoses and treatments earlier in advance could open opportunities for predictive medicine, such as identifying disease risks sooner, suggesting treatments, and optimizing resource allocation. Our experiments show that predictions regarding final ICD codes can be made already two days after admission and we propose a custom model that improves performance on this early prediction task.","sentences":["Electronic Health Records (EHR) serve as a valuable source of patient information, offering insights into medical histories, treatments, and outcomes.","Previous research has developed systems for detecting applicable ICD codes that should be assigned while writing a given EHR document, mainly focusing on discharge summaries written at the end of a hospital stay.","In this work, we investigate the potential of predicting these codes for the whole patient stay at different time points during their stay, even before they are officially assigned by clinicians.","The development of methods to predict diagnoses and treatments earlier in advance could open opportunities for predictive medicine, such as identifying disease risks sooner, suggesting treatments, and optimizing resource allocation.","Our experiments show that predictions regarding final ICD codes can be made already two days after admission and we propose a custom model that improves performance on this early prediction task."],"url":"http://arxiv.org/abs/2405.11622v1","category":"cs.CL"}
{"created":"2024-05-19 17:09:43","title":"Nickel and Diming Your GAN: A Dual-Method Approach to Enhancing GAN Efficiency via Knowledge Distillation","abstract":"In this paper, we address the challenge of compressing generative adversarial networks (GANs) for deployment in resource-constrained environments by proposing two novel methodologies: Distribution Matching for Efficient compression (DiME) and Network Interactive Compression via Knowledge Exchange and Learning (NICKEL). DiME employs foundation models as embedding kernels for efficient distribution matching, leveraging maximum mean discrepancy to facilitate effective knowledge distillation. Simultaneously, NICKEL employs an interactive compression method that enhances the communication between the student generator and discriminator, achieving a balanced and stable compression process. Our comprehensive evaluation on the StyleGAN2 architecture with the FFHQ dataset shows the effectiveness of our approach, with NICKEL & DiME achieving FID scores of 10.45 and 15.93 at compression rates of 95.73% and 98.92%, respectively. Remarkably, our methods sustain generative quality even at an extreme compression rate of 99.69%, surpassing the previous state-of-the-art performance by a large margin. These findings not only demonstrate our methodologies' capacity to significantly lower GANs' computational demands but also pave the way for deploying high-quality GAN models in settings with limited resources. Our code will be released soon.","sentences":["In this paper, we address the challenge of compressing generative adversarial networks (GANs) for deployment in resource-constrained environments by proposing two novel methodologies: Distribution Matching for Efficient compression (DiME) and Network Interactive Compression via Knowledge Exchange and Learning (NICKEL).","DiME employs foundation models as embedding kernels for efficient distribution matching, leveraging maximum mean discrepancy to facilitate effective knowledge distillation.","Simultaneously, NICKEL employs an interactive compression method that enhances the communication between the student generator and discriminator, achieving a balanced and stable compression process.","Our comprehensive evaluation on the StyleGAN2 architecture with the FFHQ dataset shows the effectiveness of our approach, with NICKEL & DiME achieving FID scores of 10.45 and 15.93 at compression rates of 95.73% and 98.92%, respectively.","Remarkably, our methods sustain generative quality even at an extreme compression rate of 99.69%, surpassing the previous state-of-the-art performance by a large margin.","These findings not only demonstrate our methodologies' capacity to significantly lower GANs' computational demands but also pave the way for deploying high-quality GAN models in settings with limited resources.","Our code will be released soon."],"url":"http://arxiv.org/abs/2405.11614v1","category":"cs.CV"}
{"created":"2024-05-19 15:56:03","title":"Speech-dependent Data Augmentation for Own Voice Reconstruction with Hearable Microphones in Noisy Environments","abstract":"Own voice pickup for hearables in noisy environments benefits from using both an outer and an in-ear microphone outside and inside the occluded ear. Due to environmental noise recorded at both microphones, and amplification of the own voice at low frequencies and band-limitation at the in-ear microphone, an own voice reconstruction system is needed to enable communication. A large amount of own voice signals is required to train a supervised deep learning-based own voice reconstruction system. Training data can either be obtained by recording a large amount of own voice signals of different talkers with a specific device, which is costly, or through augmentation of available speech data. Own voice signals can be simulated by assuming a linear time-invariant relative transfer function between hearable microphones for each phoneme, referred to as own voice transfer characteristics. In this paper, we propose data augmentation techniques for training an own voice reconstruction system based on speech-dependent models of own voice transfer characteristics between hearable microphones. The proposed techniques use few recorded own voice signals to estimate transfer characteristics and can then be used to simulate a large amount of own voice signals based on single-channel speech signals. Experimental results show that the proposed speech-dependent individual data augmentation technique leads to better performance compared to other data augmentation techniques or compared to training only on the available recorded own voice signals, and additional fine-tuning on the available recorded signals can improve performance further.","sentences":["Own voice pickup for hearables in noisy environments benefits from using both an outer and an in-ear microphone outside and inside the occluded ear.","Due to environmental noise recorded at both microphones, and amplification of the own voice at low frequencies and band-limitation at the in-ear microphone, an own voice reconstruction system is needed to enable communication.","A large amount of own voice signals is required to train a supervised deep learning-based own voice reconstruction system.","Training data can either be obtained by recording a large amount of own voice signals of different talkers with a specific device, which is costly, or through augmentation of available speech data.","Own voice signals can be simulated by assuming a linear time-invariant relative transfer function between hearable microphones for each phoneme, referred to as own voice transfer characteristics.","In this paper, we propose data augmentation techniques for training an own voice reconstruction system based on speech-dependent models of own voice transfer characteristics between hearable microphones.","The proposed techniques use few recorded own voice signals to estimate transfer characteristics and can then be used to simulate a large amount of own voice signals based on single-channel speech signals.","Experimental results show that the proposed speech-dependent individual data augmentation technique leads to better performance compared to other data augmentation techniques or compared to training only on the available recorded own voice signals, and additional fine-tuning on the available recorded signals can improve performance further."],"url":"http://arxiv.org/abs/2405.11592v1","category":"eess.AS"}
{"created":"2024-05-19 15:53:18","title":"Generative Students: Using LLM-Simulated Student Profiles to Support Question Item Evaluation","abstract":"Evaluating the quality of automatically generated question items has been a long standing challenge. In this paper, we leverage LLMs to simulate student profiles and generate responses to multiple-choice questions (MCQs). The generative students' responses to MCQs can further support question item evaluation. We propose Generative Students, a prompt architecture designed based on the KLI framework. A generative student profile is a function of the list of knowledge components the student has mastered, has confusion about or has no evidence of knowledge of. We instantiate the Generative Students concept on the subject domain of heuristic evaluation. We created 45 generative students using GPT-4 and had them respond to 20 MCQs. We found that the generative students produced logical and believable responses that were aligned with their profiles. We then compared the generative students' responses to real students' responses on the same set of MCQs and found a high correlation. Moreover, there was considerable overlap in the difficult questions identified by generative students and real students. A subsequent case study demonstrated that an instructor could improve question quality based on the signals provided by Generative Students.","sentences":["Evaluating the quality of automatically generated question items has been a long standing challenge.","In this paper, we leverage LLMs to simulate student profiles and generate responses to multiple-choice questions (MCQs).","The generative students' responses to MCQs can further support question item evaluation.","We propose Generative Students, a prompt architecture designed based on the KLI framework.","A generative student profile is a function of the list of knowledge components the student has mastered, has confusion about or has no evidence of knowledge of.","We instantiate the Generative Students concept on the subject domain of heuristic evaluation.","We created 45 generative students using GPT-4 and had them respond to 20 MCQs.","We found that the generative students produced logical and believable responses that were aligned with their profiles.","We then compared the generative students' responses to real students' responses on the same set of MCQs and found a high correlation.","Moreover, there was considerable overlap in the difficult questions identified by generative students and real students.","A subsequent case study demonstrated that an instructor could improve question quality based on the signals provided by Generative Students."],"url":"http://arxiv.org/abs/2405.11591v1","category":"cs.HC"}
{"created":"2024-05-19 11:55:48","title":"Learning More Generalized Experts by Merging Experts in Mixture-of-Experts","abstract":"We observe that incorporating a shared layer in a mixture-of-experts can lead to performance degradation. This leads us to hypothesize that learning shared features poses challenges in deep learning, potentially caused by the same feature being learned as various different features. To address this issue, we track each expert's usage frequency and merge the two most frequently selected experts. We then update the least frequently selected expert using the combination of experts. This approach, combined with the subsequent learning of the router's expert selection, allows the model to determine if the most frequently selected experts have learned the same feature differently. If they have, the combined expert can be further trained to learn a more general feature. Consequently, our algorithm enhances transfer learning and mitigates catastrophic forgetting when applied to multi-domain task incremental learning.","sentences":["We observe that incorporating a shared layer in a mixture-of-experts can lead to performance degradation.","This leads us to hypothesize that learning shared features poses challenges in deep learning, potentially caused by the same feature being learned as various different features.","To address this issue, we track each expert's usage frequency and merge the two most frequently selected experts.","We then update the least frequently selected expert using the combination of experts.","This approach, combined with the subsequent learning of the router's expert selection, allows the model to determine if the most frequently selected experts have learned the same feature differently.","If they have, the combined expert can be further trained to learn a more general feature.","Consequently, our algorithm enhances transfer learning and mitigates catastrophic forgetting when applied to multi-domain task incremental learning."],"url":"http://arxiv.org/abs/2405.11530v1","category":"cs.LG"}
{"created":"2024-05-19 11:33:49","title":"Simple-Sampling and Hard-Mixup with Prototypes to Rebalance Contrastive Learning for Text Classification","abstract":"Text classification is a crucial and fundamental task in natural language processing. Compared with the previous learning paradigm of pre-training and fine-tuning by cross entropy loss, the recently proposed supervised contrastive learning approach has received tremendous attention due to its powerful feature learning capability and robustness. Although several studies have incorporated this technique for text classification, some limitations remain. First, many text datasets are imbalanced, and the learning mechanism of supervised contrastive learning is sensitive to data imbalance, which may harm the model performance. Moreover, these models leverage separate classification branch with cross entropy and supervised contrastive learning branch without explicit mutual guidance. To this end, we propose a novel model named SharpReCL for imbalanced text classification tasks. First, we obtain the prototype vector of each class in the balanced classification branch to act as a representation of each class. Then, by further explicitly leveraging the prototype vectors, we construct a proper and sufficient target sample set with the same size for each class to perform the supervised contrastive learning procedure. The empirical results show the effectiveness of our model, which even outperforms popular large language models across several datasets.","sentences":["Text classification is a crucial and fundamental task in natural language processing.","Compared with the previous learning paradigm of pre-training and fine-tuning by cross entropy loss, the recently proposed supervised contrastive learning approach has received tremendous attention due to its powerful feature learning capability and robustness.","Although several studies have incorporated this technique for text classification, some limitations remain.","First, many text datasets are imbalanced, and the learning mechanism of supervised contrastive learning is sensitive to data imbalance, which may harm the model performance.","Moreover, these models leverage separate classification branch with cross entropy and supervised contrastive learning branch without explicit mutual guidance.","To this end, we propose a novel model named SharpReCL for imbalanced text classification tasks.","First, we obtain the prototype vector of each class in the balanced classification branch to act as a representation of each class.","Then, by further explicitly leveraging the prototype vectors, we construct a proper and sufficient target sample set with the same size for each class to perform the supervised contrastive learning procedure.","The empirical results show the effectiveness of our model, which even outperforms popular large language models across several datasets."],"url":"http://arxiv.org/abs/2405.11524v1","category":"cs.CL"}
{"created":"2024-05-19 11:17:00","title":"MSNER: A Multilingual Speech Dataset for Named Entity Recognition","abstract":"While extensively explored in text-based tasks, Named Entity Recognition (NER) remains largely neglected in spoken language understanding. Existing resources are limited to a single, English-only dataset. This paper addresses this gap by introducing MSNER, a freely available, multilingual speech corpus annotated with named entities. It provides annotations to the VoxPopuli dataset in four languages (Dutch, French, German, and Spanish). We have also releasing an efficient annotation tool that leverages automatic pre-annotations for faster manual refinement. This results in 590 and 15 hours of silver-annotated speech for training and validation, alongside a 17-hour, manually-annotated evaluation set. We further provide an analysis comparing silver and gold annotations. Finally, we present baseline NER models to stimulate further research on this newly available dataset.","sentences":["While extensively explored in text-based tasks, Named Entity Recognition (NER) remains largely neglected in spoken language understanding.","Existing resources are limited to a single, English-only dataset.","This paper addresses this gap by introducing MSNER, a freely available, multilingual speech corpus annotated with named entities.","It provides annotations to the VoxPopuli dataset in four languages (Dutch, French, German, and Spanish).","We have also releasing an efficient annotation tool that leverages automatic pre-annotations for faster manual refinement.","This results in 590 and 15 hours of silver-annotated speech for training and validation, alongside a 17-hour, manually-annotated evaluation set.","We further provide an analysis comparing silver and gold annotations.","Finally, we present baseline NER models to stimulate further research on this newly available dataset."],"url":"http://arxiv.org/abs/2405.11519v1","category":"cs.CL"}
{"created":"2024-05-19 10:38:59","title":"Going into Orbit: Massively Parallelizing Episodic Reinforcement Learning","abstract":"The possibilities of robot control have multiplied across various domains through the application of deep reinforcement learning. To overcome safety and sampling efficiency issues, deep reinforcement learning models can be trained in a simulation environment, allowing for faster iteration cycles. This can be enhanced further by parallelizing the training process using GPUs. NVIDIA's open-source robot learning framework Orbit leverages this potential by wrapping tensor-based reinforcement learning libraries for high parallelism and building upon Isaac Sim for its simulations. We contribute a detailed description of the implementation of a benchmark reinforcement learning task, namely box pushing, using Orbit. Additionally, we benchmark the performance of our implementation in comparison to a CPU-based implementation and report the performance metrics. Finally, we tune the hyper parameters of our implementation and show that we can generate significantly more samples in the same amount of time by using Orbit.","sentences":["The possibilities of robot control have multiplied across various domains through the application of deep reinforcement learning.","To overcome safety and sampling efficiency issues, deep reinforcement learning models can be trained in a simulation environment, allowing for faster iteration cycles.","This can be enhanced further by parallelizing the training process using GPUs.","NVIDIA's open-source robot learning framework Orbit leverages this potential by wrapping tensor-based reinforcement learning libraries for high parallelism and building upon Isaac Sim for its simulations.","We contribute a detailed description of the implementation of a benchmark reinforcement learning task, namely box pushing, using Orbit.","Additionally, we benchmark the performance of our implementation in comparison to a CPU-based implementation and report the performance metrics.","Finally, we tune the hyper parameters of our implementation and show that we can generate significantly more samples in the same amount of time by using Orbit."],"url":"http://arxiv.org/abs/2405.11512v1","category":"cs.RO"}
{"created":"2024-05-19 10:14:55","title":"Enhancing user experience in large language models through human-centered design: Integrating theoretical insights with an experimental study to meet diverse software learning needs with a single document knowledge base","abstract":"This paper begins with a theoretical exploration of the rise of large language models (LLMs) in Human-Computer Interaction (HCI), their impact on user experience (HX), and related challenges. It then discusses the benefits of Human-Centered Design (HCD) principles and the possibility of their application within LLMs, subsequently deriving six specific HCD guidelines for LLMs. Following this, a preliminary experiment is presented as an example to demonstrate how HCD principles can be employed to enhance user experience within GPT by using a single document input to GPT's Knowledge base as a new knowledge resource to control the interactions between GPT and users, aiming to meet the diverse needs of hypothetical software learners as much as possible. The experimental results demonstrate the effect of different elements' forms and organizational methods in the document, as well as GPT's relevant configurations, on the interaction effectiveness between GPT and software learners. A series of trials are conducted to explore better methods to realize text and image displaying, and jump action. Two template documents are compared in the aspects of the performances of the four interaction modes. Through continuous optimization, an improved version of the document was obtained to serve as a template for future use and research.","sentences":["This paper begins with a theoretical exploration of the rise of large language models (LLMs) in Human-Computer Interaction (HCI), their impact on user experience (HX), and related challenges.","It then discusses the benefits of Human-Centered Design (HCD) principles and the possibility of their application within LLMs, subsequently deriving six specific HCD guidelines for LLMs.","Following this, a preliminary experiment is presented as an example to demonstrate how HCD principles can be employed to enhance user experience within GPT by using a single document input to GPT's Knowledge base as a new knowledge resource to control the interactions between GPT and users, aiming to meet the diverse needs of hypothetical software learners as much as possible.","The experimental results demonstrate the effect of different elements' forms and organizational methods in the document, as well as GPT's relevant configurations, on the interaction effectiveness between GPT and software learners.","A series of trials are conducted to explore better methods to realize text and image displaying, and jump action.","Two template documents are compared in the aspects of the performances of the four interaction modes.","Through continuous optimization, an improved version of the document was obtained to serve as a template for future use and research."],"url":"http://arxiv.org/abs/2405.11505v1","category":"cs.HC"}
{"created":"2024-05-19 09:57:34","title":"Interpreting a Semantic Segmentation Model for Coastline Detection","abstract":"We interpret a deep-learning semantic segmentation model used to classify coastline satellite images into land and water. This is to build trust in the model and gain new insight into the process of coastal water body extraction. Specifically, we seek to understand which spectral bands are important for predicting segmentation masks. This is done using a permutation importance approach. Results show that the NIR is the most important spectral band. Permuting this band lead to a decrease in accuracy of 38.12 percentage points. This is followed by Water Vapour, SWIR 1, and Blue bands with 2.58, 0.78 and 0.19 respectively. Water Vapour is not typically used in water indices and these results suggest it may be useful for water body extraction. Permuting, the Coastal Aerosol, Green, Red, RE1, RE2, RE3, RE4, and SWIR 2 bands did not decrease accuracy. This suggests they could be excluded from future model builds reducing complexity and computational requirements.","sentences":["We interpret a deep-learning semantic segmentation model used to classify coastline satellite images into land and water.","This is to build trust in the model and gain new insight into the process of coastal water body extraction.","Specifically, we seek to understand which spectral bands are important for predicting segmentation masks.","This is done using a permutation importance approach.","Results show that the NIR is the most important spectral band.","Permuting this band lead to a decrease in accuracy of 38.12 percentage points.","This is followed by Water Vapour, SWIR 1, and Blue bands with 2.58, 0.78 and 0.19 respectively.","Water Vapour is not typically used in water indices and these results suggest it may be useful for water body extraction.","Permuting, the Coastal Aerosol, Green, Red, RE1, RE2, RE3, RE4, and SWIR 2 bands did not decrease accuracy.","This suggests they could be excluded from future model builds reducing complexity and computational requirements."],"url":"http://arxiv.org/abs/2405.11500v1","category":"cs.LG"}
{"created":"2024-05-19 09:51:10","title":"The Effectiveness of Edge Detection Evaluation Metrics for Automated Coastline Detection","abstract":"We analyse the effectiveness of RMSE, PSNR, SSIM and FOM for evaluating edge detection algorithms used for automated coastline detection. Typically, the accuracy of detected coastlines is assessed visually. This can be impractical on a large scale leading to the need for objective evaluation metrics. Hence, we conduct an experiment to find reliable metrics. We apply Canny edge detection to 95 coastline satellite images across 49 testing locations. We vary the Hysteresis thresholds and compare metric values to a visual analysis of detected edges. We found that FOM was the most reliable metric for selecting the best threshold. It could select a better threshold 92.6% of the time and the best threshold 66.3% of the time. This is compared RMSE, PSNR and SSIM which could select the best threshold 6.3%, 6.3% and 11.6% of the time respectively. We provide a reason for these results by reformulating RMSE, PSNR and SSIM in terms of confusion matrix measures. This suggests these metrics not only fail for this experiment but are not useful for evaluating edge detection in general.","sentences":["We analyse the effectiveness of RMSE, PSNR, SSIM and FOM for evaluating edge detection algorithms used for automated coastline detection.","Typically, the accuracy of detected coastlines is assessed visually.","This can be impractical on a large scale leading to the need for objective evaluation metrics.","Hence, we conduct an experiment to find reliable metrics.","We apply Canny edge detection to 95 coastline satellite images across 49 testing locations.","We vary the Hysteresis thresholds and compare metric values to a visual analysis of detected edges.","We found that FOM was the most reliable metric for selecting the best threshold.","It could select a better threshold 92.6% of the time and the best threshold 66.3% of the time.","This is compared RMSE, PSNR and SSIM which could select the best threshold 6.3%, 6.3% and 11.6% of the time respectively.","We provide a reason for these results by reformulating RMSE, PSNR and SSIM in terms of confusion matrix measures.","This suggests these metrics not only fail for this experiment but are not useful for evaluating edge detection in general."],"url":"http://arxiv.org/abs/2405.11498v1","category":"cs.CV"}
{"created":"2024-05-19 09:38:56","title":"DEMO: A Statistical Perspective for Efficient Image-Text Matching","abstract":"Image-text matching has been a long-standing problem, which seeks to connect vision and language through semantic understanding. Due to the capability to manage large-scale raw data, unsupervised hashing-based approaches have gained prominence recently. They typically construct a semantic similarity structure using the natural distance, which subsequently provides guidance to the model optimization process. However, the similarity structure could be biased at the boundaries of semantic distributions, causing error accumulation during sequential optimization. To tackle this, we introduce a novel hashing approach termed Distribution-based Structure Mining with Consistency Learning (DEMO) for efficient image-text matching. From a statistical view, DEMO characterizes each image using multiple augmented views, which are considered as samples drawn from its intrinsic semantic distribution. Then, we employ a non-parametric distribution divergence to ensure a robust and precise similarity structure. In addition, we introduce collaborative consistency learning which not only preserves the similarity structure in the Hamming space but also encourages consistency between retrieval distribution from different directions in a self-supervised manner. Through extensive experiments on three benchmark image-text matching datasets, we demonstrate that DEMO achieves superior performance compared with many state-of-the-art methods.","sentences":["Image-text matching has been a long-standing problem, which seeks to connect vision and language through semantic understanding.","Due to the capability to manage large-scale raw data, unsupervised hashing-based approaches have gained prominence recently.","They typically construct a semantic similarity structure using the natural distance, which subsequently provides guidance to the model optimization process.","However, the similarity structure could be biased at the boundaries of semantic distributions, causing error accumulation during sequential optimization.","To tackle this, we introduce a novel hashing approach termed Distribution-based Structure Mining with Consistency Learning (DEMO) for efficient image-text matching.","From a statistical view, DEMO characterizes each image using multiple augmented views, which are considered as samples drawn from its intrinsic semantic distribution.","Then, we employ a non-parametric distribution divergence to ensure a robust and precise similarity structure.","In addition, we introduce collaborative consistency learning which not only preserves the similarity structure in the Hamming space but also encourages consistency between retrieval distribution from different directions in a self-supervised manner.","Through extensive experiments on three benchmark image-text matching datasets, we demonstrate that DEMO achieves superior performance compared with many state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.11496v1","category":"cs.CV"}
{"created":"2024-05-19 09:25:55","title":"Automated Coastline Extraction Using Edge Detection Algorithms","abstract":"We analyse the effectiveness of edge detection algorithms for the purpose of automatically extracting coastlines from satellite images. Four algorithms - Canny, Sobel, Scharr and Prewitt are compared visually and using metrics. With an average SSIM of 0.8, Canny detected edges that were closest to the reference edges. However, the algorithm had difficulty distinguishing noisy edges, e.g. due to development, from coastline edges. In addition, histogram equalization and Gaussian blur were shown to improve the effectiveness of the edge detection algorithms by up to 1.5 and 1.6 times respectively.","sentences":["We analyse the effectiveness of edge detection algorithms for the purpose of automatically extracting coastlines from satellite images.","Four algorithms - Canny, Sobel, Scharr and Prewitt are compared visually and using metrics.","With an average SSIM of 0.8, Canny detected edges that were closest to the reference edges.","However, the algorithm had difficulty distinguishing noisy edges, e.g. due to development, from coastline edges.","In addition, histogram equalization and Gaussian blur were shown to improve the effectiveness of the edge detection algorithms by up to 1.5 and 1.6 times respectively."],"url":"http://arxiv.org/abs/2405.11494v1","category":"cs.CV"}
{"created":"2024-05-19 09:19:31","title":"Enhancing Vehicle Aerodynamics with Deep Reinforcement Learning in Voxelised Models","abstract":"Aerodynamic design optimisation plays a crucial role in improving the performance and efficiency of automotive vehicles. This paper presents a novel approach for aerodynamic optimisation in car design using deep reinforcement learning (DRL). Traditional optimisation methods often face challenges in handling the complexity of the design space and capturing non-linear relationships between design parameters and aerodynamic performance metrics. This study addresses these challenges by employing DRL to learn optimal aerodynamic design strategies in a voxelised model representation. The proposed approach utilises voxelised models to discretise the vehicle geometry into a grid of voxels, allowing for a detailed representation of the aerodynamic flow field. The Proximal Policy Optimisation (PPO) algorithm is then employed to train a DRL agent to optimise the design parameters of the vehicle with respect to drag force, kinetic energy, and voxel collision count. Experimental results demonstrate the effectiveness and efficiency of the proposed approach in achieving significant results in aerodynamic performance. The findings highlight the potential of DRL techniques for addressing complex aerodynamic design optimisation problems in automotive engineering, with implications for improving vehicle performance, fuel efficiency, and environmental sustainability.","sentences":["Aerodynamic design optimisation plays a crucial role in improving the performance and efficiency of automotive vehicles.","This paper presents a novel approach for aerodynamic optimisation in car design using deep reinforcement learning (DRL).","Traditional optimisation methods often face challenges in handling the complexity of the design space and capturing non-linear relationships between design parameters and aerodynamic performance metrics.","This study addresses these challenges by employing DRL to learn optimal aerodynamic design strategies in a voxelised model representation.","The proposed approach utilises voxelised models to discretise the vehicle geometry into a grid of voxels, allowing for a detailed representation of the aerodynamic flow field.","The Proximal Policy Optimisation (PPO) algorithm is then employed to train a DRL agent to optimise the design parameters of the vehicle with respect to drag force, kinetic energy, and voxel collision count.","Experimental results demonstrate the effectiveness and efficiency of the proposed approach in achieving significant results in aerodynamic performance.","The findings highlight the potential of DRL techniques for addressing complex aerodynamic design optimisation problems in automotive engineering, with implications for improving vehicle performance, fuel efficiency, and environmental sustainability."],"url":"http://arxiv.org/abs/2405.11492v1","category":"cs.RO"}
{"created":"2024-05-19 08:24:34","title":"Physics-aware Hand-object Interaction Denoising","abstract":"The credibility and practicality of a reconstructed hand-object interaction sequence depend largely on its physical plausibility. However, due to high occlusions during hand-object interaction, physical plausibility remains a challenging criterion for purely vision-based tracking methods. To address this issue and enhance the results of existing hand trackers, this paper proposes a novel physically-aware hand motion de-noising method. Specifically, we introduce two learned loss terms that explicitly capture two crucial aspects of physical plausibility: grasp credibility and manipulation feasibility. These terms are used to train a physically-aware de-noising network. Qualitative and quantitative experiments demonstrate that our approach significantly improves both fine-grained physical plausibility and overall pose accuracy, surpassing current state-of-the-art de-noising methods.","sentences":["The credibility and practicality of a reconstructed hand-object interaction sequence depend largely on its physical plausibility.","However, due to high occlusions during hand-object interaction, physical plausibility remains a challenging criterion for purely vision-based tracking methods.","To address this issue and enhance the results of existing hand trackers, this paper proposes a novel physically-aware hand motion de-noising method.","Specifically, we introduce two learned loss terms that explicitly capture two crucial aspects of physical plausibility: grasp credibility and manipulation feasibility.","These terms are used to train a physically-aware de-noising network.","Qualitative and quantitative experiments demonstrate that our approach significantly improves both fine-grained physical plausibility and overall pose accuracy, surpassing current state-of-the-art de-noising methods."],"url":"http://arxiv.org/abs/2405.11481v1","category":"cs.CV"}
{"created":"2024-05-20 16:27:25","title":"A Nearly Quadratic Improvement for Memory Reallocation","abstract":"In the Memory Reallocation Problem a set of items of various sizes must be dynamically assigned to non-overlapping contiguous chunks of memory. It is guaranteed that the sum of the sizes of all items present at any time is at most a $(1-\\varepsilon)$-fraction of the total size of memory (i.e., the load-factor is at most $1-\\varepsilon$). The allocator receives insert and delete requests online, and can re-arrange existing items to handle the requests, but at a reallocation cost defined to be the sum of the sizes of items moved divided by the size of the item being inserted/deleted.   The folklore algorithm for Memory Reallocation achieves a cost of $O(\\varepsilon^{-1})$ per update. In recent work at FOCS'23, Kuszmaul showed that, in the special case where each item is promised to be smaller than an $\\varepsilon^4$-fraction of memory, it is possible to achieve expected update cost $O(\\log\\varepsilon^{-1})$. Kuszmaul conjectures, however, that for larger items the folklore algorithm is optimal.   In this work we disprove Kuszmaul's conjecture, giving an allocator that achieves expected update cost $O(\\varepsilon^{-1/2} \\operatorname*{polylog} \\varepsilon^{-1})$ on any input sequence. We also give the first non-trivial lower bound for the Memory Reallocation Problem: we demonstrate an input sequence on which any resizable allocator (even offline) must incur amortized update cost at least $\\Omega(\\log\\varepsilon^{-1})$.   Finally, we analyze the Memory Reallocation Problem on a stochastic sequence of inserts and deletes, with random sizes in $[\\delta, 2 \\delta]$ for some $\\delta$. We show that, in this simplified setting, it is possible to achieve $O(\\log\\varepsilon^{-1})$ expected update cost, even in the ``large item'' parameter regime ($\\delta > \\varepsilon^4$).","sentences":["In the Memory Reallocation Problem a set of items of various sizes must be dynamically assigned to non-overlapping contiguous chunks of memory.","It is guaranteed that the sum of the sizes of all items present at any time is at most a $(1-\\varepsilon)$-fraction of the total size of memory (i.e., the load-factor is at most $1-\\varepsilon$).","The allocator receives insert and delete requests online, and can re-arrange existing items to handle the requests, but at a reallocation cost defined to be the sum of the sizes of items moved divided by the size of the item being inserted/deleted.   ","The folklore algorithm for Memory Reallocation achieves a cost of $O(\\varepsilon^{-1})$ per update.","In recent work at FOCS'23, Kuszmaul showed that, in the special case where each item is promised to be smaller than an $\\varepsilon^4$-fraction of memory, it is possible to achieve expected update cost $O(\\log\\varepsilon^{-1})$. Kuszmaul conjectures, however, that for larger items the folklore algorithm is optimal.   ","In this work we disprove Kuszmaul's conjecture, giving an allocator that achieves expected update cost $O(\\varepsilon^{-1/2} \\operatorname*{polylog} \\varepsilon^{-1})$ on any input sequence.","We also give the first non-trivial lower bound for the Memory Reallocation Problem: we demonstrate an input sequence on which any resizable allocator (even offline) must incur amortized update cost at least $\\Omega(\\log\\varepsilon^{-1})$.   Finally, we analyze the Memory Reallocation Problem on a stochastic sequence of inserts and deletes, with random sizes in $[\\delta, 2 \\delta]$ for some $\\delta$. We show that, in this simplified setting, it is possible to achieve $O(\\log\\varepsilon^{-1})$ expected update cost, even in the ``large item'' parameter regime ($\\delta > \\varepsilon^4$)."],"url":"http://arxiv.org/abs/2405.12152v1","category":"cs.DS"}
{"created":"2024-05-20 14:37:12","title":"QuanEstimation.jl: An open-source Julia framework for quantum parameter estimation","abstract":"As the main theoretical support of quantum metrology, quantum parameter estimation must follow the steps of quantum metrology towards the applied science and industry. Hence, optimal scheme design will soon be a crucial and core task for quantum parameter estimation. To efficiently accomplish this task, software packages aimed at computer-aided design are in high demand. In response to this need, we hereby introduce QuanEstimation.jl, an open-source Julia framework for scheme evaluation and design in quantum parameter estimation. It can be used either as an independent package or as the computational core of the recently developed hybrid-language (Python-Julia) package QuanEstimation [Phys. Rev. Res. 4, 043057 (2022)]. Utilizing this framework, the scheme evaluation and design in quantum parameter estimation can be readily performed, especially when quantum noises exist.","sentences":["As the main theoretical support of quantum metrology, quantum parameter estimation must follow the steps of quantum metrology towards the applied science and industry.","Hence, optimal scheme design will soon be a crucial and core task for quantum parameter estimation.","To efficiently accomplish this task, software packages aimed at computer-aided design are in high demand.","In response to this need, we hereby introduce QuanEstimation.jl, an open-source Julia framework for scheme evaluation and design in quantum parameter estimation.","It can be used either as an independent package or as the computational core of the recently developed hybrid-language (Python-Julia) package QuanEstimation [Phys. Rev. Res. 4, 043057 (2022)].","Utilizing this framework, the scheme evaluation and design in quantum parameter estimation can be readily performed, especially when quantum noises exist."],"url":"http://arxiv.org/abs/2405.12066v1","category":"quant-ph"}
{"created":"2024-05-20 14:34:09","title":"Approximating Multi-Dimensional and Multiband Signals","abstract":"We study the problem of representing a discrete tensor that comes from finite uniform samplings of a multi-dimensional and multiband analog signal. Particularly, we consider two typical cases in which the shape of the subbands is cubic or parallelepipedic. For the cubic case, by examining the spectrum of its corresponding time- and band-limited operators, we obtain a low-dimensional optimal dictionary to represent the original tensor. We further prove that the optimal dictionary can be approximated by the famous \\ac{dpss} with certain modulation, leading to an efficient constructing method. For the parallelepipedic case, we show that there also exists a low-dimensional dictionary to represent the original tensor. We present rigorous proof that the numbers of atoms in both dictionaries are approximately equal to the dot of the total number of samplings and the total volume of the subbands. Our derivations are mainly focused on the \\ac{2d} scenarios but can be naturally extended to high dimensions.","sentences":["We study the problem of representing a discrete tensor that comes from finite uniform samplings of a multi-dimensional and multiband analog signal.","Particularly, we consider two typical cases in which the shape of the subbands is cubic or parallelepipedic.","For the cubic case, by examining the spectrum of its corresponding time- and band-limited operators, we obtain a low-dimensional optimal dictionary to represent the original tensor.","We further prove that the optimal dictionary can be approximated by the famous \\ac{dpss} with certain modulation, leading to an efficient constructing method.","For the parallelepipedic case, we show that there also exists a low-dimensional dictionary to represent the original tensor.","We present rigorous proof that the numbers of atoms in both dictionaries are approximately equal to the dot of the total number of samplings and the total volume of the subbands.","Our derivations are mainly focused on the \\ac{2d} scenarios but can be naturally extended to high dimensions."],"url":"http://arxiv.org/abs/2405.12064v1","category":"eess.SP"}
{"created":"2024-05-20 14:06:45","title":"Randomized Gradient Descents on Riemannian Manifolds: Almost Sure Convergence to Global Minima in and beyond Quantum Optimization","abstract":"We analyze the convergence properties of gradient descent algorithms on Riemannian manifolds. We study randomization of the tangent space directions of Riemannian gradient flows for minimizing smooth cost functions (of Morse--Bott type) to obtain convergence to local optima. We prove that through randomly projecting Riemannian gradients according to the Haar measure, convergence to local optima can be obtained almost surely despite the existence of saddle points. As an application we consider ground state preparation through quantum optimization over the unitary group. In this setting one can efficiently approximate the Haar-random projections by implementing unitary 2-designs on quantum computers. We prove that the respective algorithm almost surely converges to the global minimum that corresponds to the ground state of a desired Hamiltonian. Finally, we discuss the time required by the algorithm to pass a saddle point in a simple two-dimensional setting.","sentences":["We analyze the convergence properties of gradient descent algorithms on Riemannian manifolds.","We study randomization of the tangent space directions of Riemannian gradient flows for minimizing smooth cost functions (of Morse--Bott type) to obtain convergence to local optima.","We prove that through randomly projecting Riemannian gradients according to the Haar measure, convergence to local optima can be obtained almost surely despite the existence of saddle points.","As an application we consider ground state preparation through quantum optimization over the unitary group.","In this setting one can efficiently approximate the Haar-random projections by implementing unitary 2-designs on quantum computers.","We prove that the respective algorithm almost surely converges to the global minimum that corresponds to the ground state of a desired Hamiltonian.","Finally, we discuss the time required by the algorithm to pass a saddle point in a simple two-dimensional setting."],"url":"http://arxiv.org/abs/2405.12039v1","category":"math.OC"}
{"created":"2024-05-20 12:22:39","title":"Tutorial on Silicon Photonics Integrated Platform Fiber Edge Coupling","abstract":"Photonic integrated circuits (PICs) play a crucial role in almost every aspect of modern life, such as data storage, telecommunications, medical diagnostics, green energy, autonomous driving, agriculture, and high-performance computing. To fully harness their benefits, an efficient coupling mechanism is required to successfully launch light into waveguides from fibers. This study introduces low-loss coupling strategies and their implementation for a silicon nitride integrated platform. Here we present an overview of coupling technologies, optimized designs, and a tutorial on manufacturing techniques for inverted tapers, which enable effective coupling for both transverse-magnetic and transverse-electric modes. The optimized coupling losses for the UHNA-7 fiber and the inverted taper Si3N4 coupler reached -0.81 dB at 1550 nm per connection for single-mode waveguides with 220x1200 nm cross section. The measured coupling losses in the inverted taper coupler with a standard single-mode fiber were -3.28 dB at 1550 nm per connection for the same platform.","sentences":["Photonic integrated circuits (PICs) play a crucial role in almost every aspect of modern life, such as data storage, telecommunications, medical diagnostics, green energy, autonomous driving, agriculture, and high-performance computing.","To fully harness their benefits, an efficient coupling mechanism is required to successfully launch light into waveguides from fibers.","This study introduces low-loss coupling strategies and their implementation for a silicon nitride integrated platform.","Here we present an overview of coupling technologies, optimized designs, and a tutorial on manufacturing techniques for inverted tapers, which enable effective coupling for both transverse-magnetic and transverse-electric modes.","The optimized coupling losses for the UHNA-7 fiber and the inverted taper Si3N4 coupler reached -0.81 dB at 1550 nm per connection for single-mode waveguides with 220x1200 nm cross section.","The measured coupling losses in the inverted taper coupler with a standard single-mode fiber were -3.28 dB at 1550 nm per connection for the same platform."],"url":"http://arxiv.org/abs/2405.11980v1","category":"physics.optics"}
{"created":"2024-05-20 09:13:47","title":"Lipschitz Continuous Allocations for Optimization Games","abstract":"In cooperative game theory, the primary focus is the equitable allocation of payoffs or costs among agents. However, in the practical applications of cooperative games, accurately representing games is challenging. In such cases, using an allocation method sensitive to small perturbations in the game can lead to various problems, including dissatisfaction among agents and the potential for manipulation by agents seeking to maximize their own benefits. Therefore, the allocation method must be robust against game perturbations.   In this study, we explore optimization games, in which the value of the characteristic function is provided as the optimal value of an optimization problem. To assess the robustness of the allocation methods, we use the Lipschitz constant, which quantifies the extent of change in the allocation vector in response to a unit perturbation in the weight vector of the underlying problem. Thereafter, we provide an algorithm for the matching game that returns an allocation belonging to the $\\left(\\frac{1}{2}-\\epsilon\\right)$-approximate core with Lipschitz constant $O(\\epsilon^{-1})$. Additionally, we provide an algorithm for a minimum spanning tree game that returns an allocation belonging to the $4$-approximate core with a constant Lipschitz constant.   The Shapley value is a popular allocation that satisfies several desirable properties. Therefore, we investigate the robustness of the Shapley value. We demonstrate that the Lipschitz constant of the Shapley value for the minimum spanning tree is constant, whereas that for the matching game is $\\Omega(\\log n)$, where $n$ denotes the number of vertices.","sentences":["In cooperative game theory, the primary focus is the equitable allocation of payoffs or costs among agents.","However, in the practical applications of cooperative games, accurately representing games is challenging.","In such cases, using an allocation method sensitive to small perturbations in the game can lead to various problems, including dissatisfaction among agents and the potential for manipulation by agents seeking to maximize their own benefits.","Therefore, the allocation method must be robust against game perturbations.   ","In this study, we explore optimization games, in which the value of the characteristic function is provided as the optimal value of an optimization problem.","To assess the robustness of the allocation methods, we use the Lipschitz constant, which quantifies the extent of change in the allocation vector in response to a unit perturbation in the weight vector of the underlying problem.","Thereafter, we provide an algorithm for the matching game that returns an allocation belonging to the $\\left(\\frac{1}{2}-\\epsilon\\right)$-approximate core with Lipschitz constant $O(\\epsilon^{-1})$. Additionally, we provide an algorithm for a minimum spanning tree game that returns an allocation belonging to the $4$-approximate core with a constant Lipschitz constant.   ","The Shapley value is a popular allocation that satisfies several desirable properties.","Therefore, we investigate the robustness of the Shapley value.","We demonstrate that the Lipschitz constant of the Shapley value for the minimum spanning tree is constant, whereas that for the matching game is $\\Omega(\\log n)$, where $n$ denotes the number of vertices."],"url":"http://arxiv.org/abs/2405.11889v1","category":"cs.GT"}
{"created":"2024-05-20 07:53:41","title":"Rethinking Overlooked Aspects in Vision-Language Models","abstract":"Recent advancements in large vision-language models (LVLMs), such as GPT4-V and LLaVA, have been substantial. LLaVA's modular architecture, in particular, offers a blend of simplicity and efficiency. Recent works mainly focus on introducing more pre-training and instruction tuning data to improve model's performance. This paper delves into the often-neglected aspects of data efficiency during pre-training and the selection process for instruction tuning datasets. Our research indicates that merely increasing the size of pre-training data does not guarantee improved performance and may, in fact, lead to its degradation. Furthermore, we have established a pipeline to pinpoint the most efficient instruction tuning (SFT) dataset, implying that not all SFT data utilized in existing studies are necessary. The primary objective of this paper is not to introduce a state-of-the-art model, but rather to serve as a roadmap for future research, aiming to optimize data usage during pre-training and fine-tuning processes to enhance the performance of vision-language models.","sentences":["Recent advancements in large vision-language models (LVLMs), such as GPT4-V and LLaVA, have been substantial.","LLaVA's modular architecture, in particular, offers a blend of simplicity and efficiency.","Recent works mainly focus on introducing more pre-training and instruction tuning data to improve model's performance.","This paper delves into the often-neglected aspects of data efficiency during pre-training and the selection process for instruction tuning datasets.","Our research indicates that merely increasing the size of pre-training data does not guarantee improved performance and may, in fact, lead to its degradation.","Furthermore, we have established a pipeline to pinpoint the most efficient instruction tuning (SFT) dataset, implying that not all SFT data utilized in existing studies are necessary.","The primary objective of this paper is not to introduce a state-of-the-art model, but rather to serve as a roadmap for future research, aiming to optimize data usage during pre-training and fine-tuning processes to enhance the performance of vision-language models."],"url":"http://arxiv.org/abs/2405.11850v1","category":"cs.CV"}
{"created":"2024-05-20 04:20:25","title":"CDM-MPC: An Integrated Dynamic Planning and Control Framework for Bipedal Robots Jumping","abstract":"Performing acrobatic maneuvers like dynamic jumping in bipedal robots presents significant challenges in terms of actuation, motion planning, and control. Traditional approaches to these tasks often simplify dynamics to enhance computational efficiency, potentially overlooking critical factors such as the control of centroidal angular momentum (CAM) and the variability of centroidal composite rigid body inertia (CCRBI). This paper introduces a novel integrated dynamic planning and control framework, termed centroidal dynamics model-based model predictive control (CDM-MPC), designed for robust jumping control that fully considers centroidal momentum and non-constant CCRBI. The framework comprises an optimization-based kinodynamic motion planner and an MPC controller for real-time trajectory tracking and replanning. Additionally, a centroidal momentum-based inverse kinematics (IK) solver and a landing heuristic controller are developed to ensure stability during high-impact landings. The efficacy of the CDM-MPC framework is validated through extensive testing on the full-sized humanoid robot KUAVO in both simulations and experiments.","sentences":["Performing acrobatic maneuvers like dynamic jumping in bipedal robots presents significant challenges in terms of actuation, motion planning, and control.","Traditional approaches to these tasks often simplify dynamics to enhance computational efficiency, potentially overlooking critical factors such as the control of centroidal angular momentum (CAM) and the variability of centroidal composite rigid body inertia (CCRBI).","This paper introduces a novel integrated dynamic planning and control framework, termed centroidal dynamics model-based model predictive control (CDM-MPC), designed for robust jumping control that fully considers centroidal momentum and non-constant CCRBI.","The framework comprises an optimization-based kinodynamic motion planner and an MPC controller for real-time trajectory tracking and replanning.","Additionally, a centroidal momentum-based inverse kinematics (IK) solver and a landing heuristic controller are developed to ensure stability during high-impact landings.","The efficacy of the CDM-MPC framework is validated through extensive testing on the full-sized humanoid robot KUAVO in both simulations and experiments."],"url":"http://arxiv.org/abs/2405.11773v1","category":"cs.RO"}
{"created":"2024-05-20 00:21:59","title":"On the principle of squeezing-induced quantum-enhanced multiphase estimation","abstract":"We investigate how squeezing techniques can improve measurement precision in multiphase quantum metrology. While these methods are well-studied and used effectively in single-phase estimations, their use in multiphase situations has not been examined yet. We fill this gap by investigating the mechanism of quantum enhancement in these scenarios. Our analysis provides theoretical and numerical insights into the optimal condition for achieving the quantum Cramer-Rao bound, helping us understand the potential and mechanism for quantum-enhanced multiphase estimations with squeezing. This research opens up new possibilities for advancements in quantum metrology and sensing technologies.","sentences":["We investigate how squeezing techniques can improve measurement precision in multiphase quantum metrology.","While these methods are well-studied and used effectively in single-phase estimations, their use in multiphase situations has not been examined yet.","We fill this gap by investigating the mechanism of quantum enhancement in these scenarios.","Our analysis provides theoretical and numerical insights into the optimal condition for achieving the quantum Cramer-Rao bound, helping us understand the potential and mechanism for quantum-enhanced multiphase estimations with squeezing.","This research opens up new possibilities for advancements in quantum metrology and sensing technologies."],"url":"http://arxiv.org/abs/2405.11705v1","category":"quant-ph"}
{"created":"2024-05-19 23:31:35","title":"On Pompeiu's-Schiffer's Conjectures from Shape Optimization","abstract":"Our aim is to do a come back on Schiffer's and Pompeiu's conjectures with shape optimization tools, maximum principles and Serrin's symmetry method. We propose a way to get affirmative answers in some cases. We propose also sufficient conditions thanks to Riemannian approach of infinite dimension that could be useful for numerical simulations of the shape of domains related to these conjectures.","sentences":["Our aim is to do a come back on Schiffer's and Pompeiu's conjectures with shape optimization tools, maximum principles and Serrin's symmetry method.","We propose a way to get affirmative answers in some cases.","We propose also sufficient conditions thanks to Riemannian approach of infinite dimension that could be useful for numerical simulations of the shape of domains related to these conjectures."],"url":"http://arxiv.org/abs/2405.11700v1","category":"math.AP"}
{"created":"2024-05-19 22:16:29","title":"Crossing The Gap Using Variational Quantum Eigensolver: A Comparative Study","abstract":"Within the evolving domain of quantum computational chemistry, the Variational Quantum Eigensolver (VQE) has been developed to explore not only the ground state but also the excited states of molecules. In this study, we compare the performance of Variational Quantum Deflation (VQD) and Subspace-Search Variational Quantum Eigensolver (SSVQE) methods in determining the low-lying excited states of $LiH$. Our investigation reveals that while VQD exhibits a slight advantage in accuracy, SSVQE stands out for its efficiency, allowing the determination of all low-lying excited states through a single parameter optimization procedure. We further evaluate the effectiveness of optimizers, including Gradient Descent (GD), Quantum Natural Gradient (QNG), and Adam optimizer, in obtaining $LiH$'s first excited state, with the Adam optimizer demonstrating superior efficiency in requiring the fewest iterations. Moreover, we propose a novel approach combining Folded Spectrum VQE (FS-VQE) with either VQD or SSVQE, enabling the exploration of highly excited states. We test the new approaches for finding all three $H_4$'s excited states. Folded Spectrum SSVQE (FS-SSVQE) can find all three highly excited states near $-1.0$ Ha with only one optimizing procedure, but the procedure converges slowly. In contrast, although Folded spectrum VQD (FS-VQD) gets highly excited states with individual optimizing procedures, the optimizing procedure converges faster.","sentences":["Within the evolving domain of quantum computational chemistry, the Variational Quantum Eigensolver (VQE) has been developed to explore not only the ground state but also the excited states of molecules.","In this study, we compare the performance of Variational Quantum Deflation (VQD) and Subspace-Search Variational Quantum Eigensolver (SSVQE) methods in determining the low-lying excited states of $LiH$. Our investigation reveals that while VQD exhibits a slight advantage in accuracy, SSVQE stands out for its efficiency, allowing the determination of all low-lying excited states through a single parameter optimization procedure.","We further evaluate the effectiveness of optimizers, including Gradient Descent (GD), Quantum Natural Gradient (QNG), and Adam optimizer, in obtaining $LiH$'s first excited state, with the Adam optimizer demonstrating superior efficiency in requiring the fewest iterations.","Moreover, we propose a novel approach combining Folded Spectrum VQE (FS-VQE) with either VQD or SSVQE, enabling the exploration of highly excited states.","We test the new approaches for finding all three $H_4$'s excited states.","Folded Spectrum SSVQE (FS-SSVQE) can find all three highly excited states near $-1.0$ Ha with only one optimizing procedure, but the procedure converges slowly.","In contrast, although Folded spectrum VQD (FS-VQD) gets highly excited states with individual optimizing procedures, the optimizing procedure converges faster."],"url":"http://arxiv.org/abs/2405.11687v1","category":"quant-ph"}
{"created":"2024-05-19 21:52:50","title":"FADet: A Multi-sensor 3D Object Detection Network based on Local Featured Attention","abstract":"Camera, LiDAR and radar are common perception sensors for autonomous driving tasks. Robust prediction of 3D object detection is optimally based on the fusion of these sensors. To exploit their abilities wisely remains a challenge because each of these sensors has its own characteristics. In this paper, we propose FADet, a multi-sensor 3D detection network, which specifically studies the characteristics of different sensors based on our local featured attention modules. For camera images, we propose dual-attention-based sub-module. For LiDAR point clouds, triple-attention-based sub-module is utilized while mixed-attention-based sub-module is applied for features of radar points. With local featured attention sub-modules, our FADet has effective detection results in long-tail and complex scenes from camera, LiDAR and radar input. On NuScenes validation dataset, FADet achieves state-of-the-art performance on LiDAR-camera object detection tasks with 71.8% NDS and 69.0% mAP, at the same time, on radar-camera object detection tasks with 51.7% NDS and 40.3% mAP. Code will be released at https://github.com/ZionGo6/FADet.","sentences":["Camera, LiDAR and radar are common perception sensors for autonomous driving tasks.","Robust prediction of 3D object detection is optimally based on the fusion of these sensors.","To exploit their abilities wisely remains a challenge because each of these sensors has its own characteristics.","In this paper, we propose FADet, a multi-sensor 3D detection network, which specifically studies the characteristics of different sensors based on our local featured attention modules.","For camera images, we propose dual-attention-based sub-module.","For LiDAR point clouds, triple-attention-based sub-module is utilized while mixed-attention-based sub-module is applied for features of radar points.","With local featured attention sub-modules, our FADet has effective detection results in long-tail and complex scenes from camera, LiDAR and radar input.","On NuScenes validation dataset, FADet achieves state-of-the-art performance on LiDAR-camera object detection tasks with 71.8% NDS and 69.0% mAP, at the same time, on radar-camera object detection tasks with 51.7% NDS and 40.3% mAP.","Code will be released at https://github.com/ZionGo6/FADet."],"url":"http://arxiv.org/abs/2405.11682v1","category":"cs.CV"}
{"created":"2024-05-19 20:36:49","title":"BYO: A Unified Framework for Benchmarking Large-Scale Graph Containers","abstract":"A fundamental building block in any graph algorithm is a graph container - a data structure used to represent the graph. Ideally, a graph container enables efficient access to the underlying graph, has low space usage, and supports updating the graph efficiently. In this paper, we conduct an extensive empirical evaluation of graph containers designed to support running algorithms on large graphs. To our knowledge, this is the first apples-to-apples comparison of graph containers rather than overall systems, which include confounding factors such as differences in algorithm implementations and infrastructure.   We measure the running time of 10 highly-optimized algorithms across over 20 different containers and 10 graphs. Somewhat surprisingly, we find that the average algorithm running time does not differ much across containers, especially those that support dynamic updates. Specifically, a simple container based on an off-the-shelf B-tree is only 1.22x slower on average than a highly optimized static one. Moreover, we observe that simplifying a graph-container Application Programming Interface (API) to only a few simple functions incurs a mere 1.16x slowdown compared to a complete API. Finally, we also measure batch-insert throughput in dynamic-graph containers for a full picture of their performance.   To perform the benchmarks, we introduce BYO, a unified framework that standardizes evaluations of graph-algorithm performance across different graph containers. BYO extends the Graph Based Benchmark Suite (Dhulipala et al. 18), a state-of-the-art graph algorithm benchmark, to easily plug into different dynamic graph containers and enable fair comparisons between them on a large suite of graph algorithms. While several graph algorithm benchmarks have been developed to date, to the best of our knowledge, BYO is the first system designed to benchmark graph containers","sentences":["A fundamental building block in any graph algorithm is a graph container - a data structure used to represent the graph.","Ideally, a graph container enables efficient access to the underlying graph, has low space usage, and supports updating the graph efficiently.","In this paper, we conduct an extensive empirical evaluation of graph containers designed to support running algorithms on large graphs.","To our knowledge, this is the first apples-to-apples comparison of graph containers rather than overall systems, which include confounding factors such as differences in algorithm implementations and infrastructure.   ","We measure the running time of 10 highly-optimized algorithms across over 20 different containers and 10 graphs.","Somewhat surprisingly, we find that the average algorithm running time does not differ much across containers, especially those that support dynamic updates.","Specifically, a simple container based on an off-the-shelf B-tree is only 1.22x slower on average than a highly optimized static one.","Moreover, we observe that simplifying a graph-container Application Programming Interface (API) to only a few simple functions incurs a mere 1.16x slowdown compared to a complete API.","Finally, we also measure batch-insert throughput in dynamic-graph containers for a full picture of their performance.   ","To perform the benchmarks, we introduce BYO, a unified framework that standardizes evaluations of graph-algorithm performance across different graph containers.","BYO extends the Graph Based Benchmark Suite (Dhulipala et al. 18), a state-of-the-art graph algorithm benchmark, to easily plug into different dynamic graph containers and enable fair comparisons between them on a large suite of graph algorithms.","While several graph algorithm benchmarks have been developed to date, to the best of our knowledge, BYO is the first system designed to benchmark graph containers"],"url":"http://arxiv.org/abs/2405.11671v1","category":"cs.DS"}
{"created":"2024-05-19 17:20:06","title":"Plasma water treatment for PFAS: Study of degradation of perfluorinated substances and their byproducts by using cold atmospheric pressure plasma jet","abstract":"This study evaluates the effectiveness of non-thermal plasma at atmospheric pressure (NTP APPJ) for treating PFAS - contaminated water in different matrices. Successful removal of several perfluoroalkyl carboxylic acids (PFCAs) (C6 to C4), perfluroalkane sulfonic acids (PFSAs) (C8 to C4) and perfluropolyethers (PFPEs) (GenX and ADONA) PFAS compounds was achieved in laboratory scale experiments. Complex matrix effects influence degradation rates. Byproducts from the plasma treatment were investigated, revealing distinct degradation mechanisms for various PFAS compounds. For PFSAs and PFCAs, degradation involved electron transfer, bond breaking and subsequent reactions. Conversely, ADONA and GenX degradation initiated with ether-group cleavage, followed by additional transformation processes. Plasmabased technology shows potential for degradation of PFAS, especially for newer substitute compounds like ADONA and GenX. However, further research is needed to optimize plasma performance for complete mineralization of PFAS. This study also proposes a degradation mechanism for ADONA, marking a novel investigation into ether-group PFAS degradation with potential implications for further research and understanding toxicological implications.","sentences":["This study evaluates the effectiveness of non-thermal plasma at atmospheric pressure (NTP APPJ) for treating PFAS - contaminated water in different matrices.","Successful removal of several perfluoroalkyl carboxylic acids (PFCAs) (C6 to C4), perfluroalkane sulfonic acids (PFSAs) (C8 to C4) and perfluropolyethers (PFPEs) (GenX and ADONA)","PFAS compounds was achieved in laboratory scale experiments.","Complex matrix effects influence degradation rates.","Byproducts from the plasma treatment were investigated, revealing distinct degradation mechanisms for various PFAS compounds.","For PFSAs and PFCAs, degradation involved electron transfer, bond breaking and subsequent reactions.","Conversely, ADONA and GenX degradation initiated with ether-group cleavage, followed by additional transformation processes.","Plasmabased technology shows potential for degradation of PFAS, especially for newer substitute compounds like ADONA and GenX.","However, further research is needed to optimize plasma performance for complete mineralization of PFAS.","This study also proposes a degradation mechanism for ADONA, marking a novel investigation into ether-group PFAS degradation with potential implications for further research and understanding toxicological implications."],"url":"http://arxiv.org/abs/2405.11620v1","category":"physics.plasm-ph"}
{"created":"2024-05-19 15:57:54","title":"First and Second Order Necessary and Sufficient Optimality Conditions of Fritz John Type for Vector Problems over Cones","abstract":"In this paper, we obtain a new proof of Fritz John necessary optimality conditions for vector problems applying Kakutani fixed point theorem and Hadamard directional derivative. We also derive a similar proof of second-order Fritz John necessary optimality conditions. Sufficient conditions for weak global efficiency with generalized convex functions and local efficiency are provided.","sentences":["In this paper, we obtain a new proof of Fritz John necessary optimality conditions for vector problems applying Kakutani fixed point theorem and Hadamard directional derivative.","We also derive a similar proof of second-order Fritz John necessary optimality conditions.","Sufficient conditions for weak global efficiency with generalized convex functions and local efficiency are provided."],"url":"http://arxiv.org/abs/2405.11593v1","category":"math.OC"}
{"created":"2024-05-19 13:48:32","title":"Towards Optimal Beacon Placement for Range-Aided Localization","abstract":"Range-based localization is ubiquitous: global navigation satellite systems (GNSS) power mobile phone-based navigation, and autonomous mobile robots can use range measurements from a variety of modalities including sonar, radar, and even WiFi signals. Many of these localization systems rely on fixed anchors or beacons with known positions acting as transmitters or receivers. In this work, we answer a fundamental question: given a set of positions we would like to localize, how should beacons be placed so as to minimize localization error? Specifically, we present an information theoretic method for optimally selecting an arrangement consisting of a few beacons from a large set of candidate positions. By formulating localization as maximum a posteriori (MAP) estimation, we can cast beacon arrangement as a submodular set function maximization problem. This approach is probabilistically rigorous, simple to implement, and extremely flexible. Furthermore, we prove that the submodular structure of our problem formulation ensures that a greedy algorithm for beacon arrangement has suboptimality guarantees. We compare our method with a number of benchmarks on simulated data and release an open source Python implementation of our algorithm and experiments.","sentences":["Range-based localization is ubiquitous: global navigation satellite systems (GNSS) power mobile phone-based navigation, and autonomous mobile robots can use range measurements from a variety of modalities including sonar, radar, and even WiFi signals.","Many of these localization systems rely on fixed anchors or beacons with known positions acting as transmitters or receivers.","In this work, we answer a fundamental question: given a set of positions we would like to localize, how should beacons be placed so as to minimize localization error?","Specifically, we present an information theoretic method for optimally selecting an arrangement consisting of a few beacons from a large set of candidate positions.","By formulating localization as maximum a posteriori (MAP) estimation, we can cast beacon arrangement as a submodular set function maximization problem.","This approach is probabilistically rigorous, simple to implement, and extremely flexible.","Furthermore, we prove that the submodular structure of our problem formulation ensures that a greedy algorithm for beacon arrangement has suboptimality guarantees.","We compare our method with a number of benchmarks on simulated data and release an open source Python implementation of our algorithm and experiments."],"url":"http://arxiv.org/abs/2405.11550v1","category":"cs.RO"}
{"created":"2024-05-19 10:31:59","title":"Online Action Representation using Change Detection and Symbolic Programming","abstract":"This paper addresses the critical need for online action representation, which is essential for various applications like rehabilitation, surveillance, etc. The task can be defined as representation of actions as soon as they happen in a streaming video without access to video frames in the future. Most of the existing methods use predefined window sizes for video segments, which is a restrictive assumption on the dynamics. The proposed method employs a change detection algorithm to automatically segment action sequences, which form meaningful sub-actions and subsequently fit symbolic generative motion programs to the clipped segments. We determine the start time and end time of segments using change detection followed by a piece-wise linear fit algorithm on joint angle and bone length sequences. Domain-specific symbolic primitives are fit to pose keypoint trajectories of those extracted segments in order to obtain a higher level semantic representation. Since this representation is part-based, it is complementary to the compositional nature of human actions, i.e., a complex activity can be broken down into elementary sub-actions. We show the effectiveness of this representation in the downstream task of class agnostic repetition detection. We propose a repetition counting algorithm based on consecutive similarity matching of primitives, which can do online repetition counting. We also compare the results with a similar but offline repetition counting algorithm. The results of the experiments demonstrate that, despite operating online, the proposed method performs better or on par with the existing method.","sentences":["This paper addresses the critical need for online action representation, which is essential for various applications like rehabilitation, surveillance, etc.","The task can be defined as representation of actions as soon as they happen in a streaming video without access to video frames in the future.","Most of the existing methods use predefined window sizes for video segments, which is a restrictive assumption on the dynamics.","The proposed method employs a change detection algorithm to automatically segment action sequences, which form meaningful sub-actions and subsequently fit symbolic generative motion programs to the clipped segments.","We determine the start time and end time of segments using change detection followed by a piece-wise linear fit algorithm on joint angle and bone length sequences.","Domain-specific symbolic primitives are fit to pose keypoint trajectories of those extracted segments in order to obtain a higher level semantic representation.","Since this representation is part-based, it is complementary to the compositional nature of human actions, i.e., a complex activity can be broken down into elementary sub-actions.","We show the effectiveness of this representation in the downstream task of class agnostic repetition detection.","We propose a repetition counting algorithm based on consecutive similarity matching of primitives, which can do online repetition counting.","We also compare the results with a similar but offline repetition counting algorithm.","The results of the experiments demonstrate that, despite operating online, the proposed method performs better or on par with the existing method."],"url":"http://arxiv.org/abs/2405.11511v1","category":"cs.CV"}
{"created":"2024-05-19 10:17:16","title":"An Investigation into the Thermoelectric Characteristics of Silver-based Chalcopyrites Utilizing a Non-empirical Range-separated Dielectric-dependent Hybrid Approach","abstract":"Our investigation explores the intricate domain of thermoelectric phenomena within silver (Ag)-infused chalcopyrites, focusing on compositions such as AgXTe$_2$ (where X=Ga, In) and the complex quaternary system Ag$_2$ZnSn/GeY$_2$ (with Y=S, Se). Using a sophisticated combination of methodologies, we integrate a non-empirical screened dielectric-dependent hybrid (DDH) functional with semiclassical Boltzmann transport theory. This approach allows us to conduct a detailed analysis of critical thermoelectric properties, including electrical conductivity, Seebeck coefficient, and power factor. Our methodology goes beyond superficial assessments, delving into the intricate interplay of material properties to reveal their true thermoelectric potential. Additionally, we investigate the often-overlooked phenomena of phonon scattering by leveraging both the elastic constant tensor and the deformation potential method. This enables a rigorous examination of electron relaxation time and lattice thermal conductivity, enhancing the robustness of our predictions and demonstrating our commitment to thorough exploration.Through our rigorous investigation, we identify materials with a thermoelectric figure of merit (ZT = $\\sigma S^{2}T/ \\kappa$) exceeding the critical threshold of unity. This significant achievement signals the discovery of materials capable of revolutionizing efficient thermoelectric systems. Our findings delineate a promising trajectory, laying the groundwork for the emergence of a new class of Ag-based chalcopyrites distinguished by their exceptional thermoelectric characteristics. This research not only contributes to the understanding of materials science principles but also catalyzes transformative advancements in thermoelectric technology.","sentences":["Our investigation explores the intricate domain of thermoelectric phenomena within silver (Ag)-infused chalcopyrites, focusing on compositions such as AgXTe$_2$ (where X=Ga, In) and the complex quaternary system Ag$_2$ZnSn/GeY$_2$ (with Y=S, Se).","Using a sophisticated combination of methodologies, we integrate a non-empirical screened dielectric-dependent hybrid (DDH) functional with semiclassical Boltzmann transport theory.","This approach allows us to conduct a detailed analysis of critical thermoelectric properties, including electrical conductivity, Seebeck coefficient, and power factor.","Our methodology goes beyond superficial assessments, delving into the intricate interplay of material properties to reveal their true thermoelectric potential.","Additionally, we investigate the often-overlooked phenomena of phonon scattering by leveraging both the elastic constant tensor and the deformation potential method.","This enables a rigorous examination of electron relaxation time and lattice thermal conductivity, enhancing the robustness of our predictions and demonstrating our commitment to thorough exploration.","Through our rigorous investigation, we identify materials with a thermoelectric figure of merit (ZT = $\\sigma S^{2}T/ \\kappa$) exceeding the critical threshold of unity.","This significant achievement signals the discovery of materials capable of revolutionizing efficient thermoelectric systems.","Our findings delineate a promising trajectory, laying the groundwork for the emergence of a new class of Ag-based chalcopyrites distinguished by their exceptional thermoelectric characteristics.","This research not only contributes to the understanding of materials science principles but also catalyzes transformative advancements in thermoelectric technology."],"url":"http://arxiv.org/abs/2405.11508v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-19 09:52:19","title":"Modes, Bounds, and Synthesis of Optimal Electromagnetic Scatterers","abstract":"This paper presents an optimal synthesis of material distributions in obstacles for maximal extinction, scattering, or absorption. The material synthesis is based on an explicit construction utilizing the current distribution derived from physical bounds excited from the far-field. The bounds are expressed in radiation modes for materials restricted by their resistivity and characteristic modes for materials restricted by the contrast. The results are valid for arbitrary shapes, and analytical expressions are provided for spherical shapes.","sentences":["This paper presents an optimal synthesis of material distributions in obstacles for maximal extinction, scattering, or absorption.","The material synthesis is based on an explicit construction utilizing the current distribution derived from physical bounds excited from the far-field.","The bounds are expressed in radiation modes for materials restricted by their resistivity and characteristic modes for materials restricted by the contrast.","The results are valid for arbitrary shapes, and analytical expressions are provided for spherical shapes."],"url":"http://arxiv.org/abs/2405.11499v1","category":"math-ph"}
{"created":"2024-05-19 05:39:46","title":"Comparisons Are All You Need for Optimizing Smooth Functions","abstract":"When optimizing machine learning models, there are various scenarios where gradient computations are challenging or even infeasible. Furthermore, in reinforcement learning (RL), preference-based RL that only compares between options has wide applications, including reinforcement learning with human feedback in large language models. In this paper, we systematically study optimization of a smooth function $f\\colon\\mathbb{R}^n\\to\\mathbb{R}$ only assuming an oracle that compares function values at two points and tells which is larger. When $f$ is convex, we give two algorithms using $\\tilde{O}(n/\\epsilon)$ and $\\tilde{O}(n^{2})$ comparison queries to find an $\\epsilon$-optimal solution, respectively. When $f$ is nonconvex, our algorithm uses $\\tilde{O}(n/\\epsilon^2)$ comparison queries to find an $\\epsilon$-approximate stationary point. All these results match the best-known zeroth-order algorithms with function evaluation queries in $n$ dependence, thus suggest that \\emph{comparisons are all you need for optimizing smooth functions using derivative-free methods}. In addition, we also give an algorithm for escaping saddle points and reaching an $\\epsilon$-second order stationary point of a nonconvex $f$, using $\\tilde{O}(n^{1.5}/\\epsilon^{2.5})$ comparison queries.","sentences":["When optimizing machine learning models, there are various scenarios where gradient computations are challenging or even infeasible.","Furthermore, in reinforcement learning (RL), preference-based RL that only compares between options has wide applications, including reinforcement learning with human feedback in large language models.","In this paper, we systematically study optimization of a smooth function $f\\colon\\mathbb{R}^n\\to\\mathbb{R}$ only assuming an oracle that compares function values at two points and tells which is larger.","When $f$ is convex, we give two algorithms using $\\tilde{O}(n/\\epsilon)$ and $\\tilde{O}(n^{2})$ comparison queries to find an $\\epsilon$-optimal solution, respectively.","When $f$ is nonconvex, our algorithm uses $\\tilde{O}(n/\\epsilon^2)$ comparison queries to find an $\\epsilon$-approximate stationary point.","All these results match the best-known zeroth-order algorithms with function evaluation queries in $n$ dependence, thus suggest that \\emph{comparisons are all you need for optimizing smooth functions using derivative-free methods}.","In addition, we also give an algorithm for escaping saddle points and reaching an $\\epsilon$-second order stationary point of a nonconvex $f$, using $\\tilde{O}(n^{1.5}/\\epsilon^{2.5})$ comparison queries."],"url":"http://arxiv.org/abs/2405.11454v1","category":"cs.LG"}
{"created":"2024-05-18 22:54:39","title":"Workload Prediction in P4 Programmable Switches: Smart Resource Scheduling","abstract":"The rapid expansion of cloud services and their unpredictable workload demands present significant challenges in resource management. Traditional resource management approaches, primarily based on static rules and thresholds, often fail to ensure cost-effectiveness and optimal resource utilization. This research introduces a predictive model designed to forecast traffic demand, aiming to shift from a reactive to a proactive resource management approach. By integrating advanced predictive analytics with the capabilities of P4 programmable switches, this study seeks to enhance the efficiency of resource utilization and improve system robustness. The goal is to equip organizations with the agility and economic efficiency required to navigate the complexities of dynamic cloud environments effectively. This approach not only promises to refine microservice resource allocation but also supports the broader objective of fostering more resilient and efficient cloud infrastructures.","sentences":["The rapid expansion of cloud services and their unpredictable workload demands present significant challenges in resource management.","Traditional resource management approaches, primarily based on static rules and thresholds, often fail to ensure cost-effectiveness and optimal resource utilization.","This research introduces a predictive model designed to forecast traffic demand, aiming to shift from a reactive to a proactive resource management approach.","By integrating advanced predictive analytics with the capabilities of P4 programmable switches, this study seeks to enhance the efficiency of resource utilization and improve system robustness.","The goal is to equip organizations with the agility and economic efficiency required to navigate the complexities of dynamic cloud environments effectively.","This approach not only promises to refine microservice resource allocation but also supports the broader objective of fostering more resilient and efficient cloud infrastructures."],"url":"http://arxiv.org/abs/2405.11408v1","category":"cs.NI"}
{"created":"2024-05-18 21:58:06","title":"An exact coverage path planning algorithm for UAV-based search and rescue operations","abstract":"Unmanned aerial vehicles (UAVs) are increasingly utilized in global search and rescue efforts, enhancing operational efficiency. In these missions, a coordinated swarm of UAVs is deployed to efficiently cover expansive areas by capturing and analyzing aerial imagery and footage. Rapid coverage is paramount in these scenarios, as swift discovery can mean the difference between life and death for those in peril. This paper focuses on optimizing flight path planning for multiple UAVs in windy conditions to efficiently cover rectangular search areas in minimal time. We address this challenge by dividing the search area into a grid network and formulating it as a mixed-integer program (MIP). Our research introduces a precise lower bound for the objective function and an exact algorithm capable of finding either the optimal solution or a near-optimal solution with a constant absolute gap to optimality. Notably, as the problem complexity increases, our solution exhibits a diminishing relative optimality gap while maintaining negligible computational costs compared to the MIP approach.","sentences":["Unmanned aerial vehicles (UAVs) are increasingly utilized in global search and rescue efforts, enhancing operational efficiency.","In these missions, a coordinated swarm of UAVs is deployed to efficiently cover expansive areas by capturing and analyzing aerial imagery and footage.","Rapid coverage is paramount in these scenarios, as swift discovery can mean the difference between life and death for those in peril.","This paper focuses on optimizing flight path planning for multiple UAVs in windy conditions to efficiently cover rectangular search areas in minimal time.","We address this challenge by dividing the search area into a grid network and formulating it as a mixed-integer program (MIP).","Our research introduces a precise lower bound for the objective function and an exact algorithm capable of finding either the optimal solution or a near-optimal solution with a constant absolute gap to optimality.","Notably, as the problem complexity increases, our solution exhibits a diminishing relative optimality gap while maintaining negligible computational costs compared to the MIP approach."],"url":"http://arxiv.org/abs/2405.11399v1","category":"cs.RO"}
{"created":"2024-05-18 20:58:49","title":"Deep Penalty Methods: A Class of Deep Learning Algorithms for Solving High Dimensional Optimal Stopping Problems","abstract":"We propose a deep learning algorithm for high dimensional optimal stopping problems. Our method is inspired by the penalty method for solving free boundary PDEs. Within our approach, the penalized PDE is approximated using the Deep BSDE framework proposed by \\cite{weinan2017deep}, which leads us to coin the term \"Deep Penalty Method (DPM)\" to refer to our algorithm. We show that the error of the DPM can be bounded by the loss function and $O(\\frac{1}{\\lambda})+O(\\lambda h) +O(\\sqrt{h})$, where $h$ is the step size in time and $\\lambda$ is the penalty parameter. This finding emphasizes the need for careful consideration when selecting the penalization parameter and suggests that the discretization error converges at a rate of order $\\frac{1}{2}$. We validate the efficacy of the DPM through numerical tests conducted on a high-dimensional optimal stopping model in the area of American option pricing. The numerical tests confirm both the accuracy and the computational efficiency of our proposed algorithm.","sentences":["We propose a deep learning algorithm for high dimensional optimal stopping problems.","Our method is inspired by the penalty method for solving free boundary PDEs.","Within our approach, the penalized PDE is approximated using the Deep BSDE framework proposed by \\cite{weinan2017deep}, which leads us to coin the term \"Deep Penalty Method (DPM)\" to refer to our algorithm.","We show that the error of the DPM can be bounded by the loss function and $O(\\frac{1}{\\lambda})+O(\\lambda h)","+O(\\sqrt{h})$, where $h$ is the step size in time and $\\lambda$ is the penalty parameter.","This finding emphasizes the need for careful consideration when selecting the penalization parameter and suggests that the discretization error converges at a rate of order $\\frac{1}{2}$. We validate the efficacy of the DPM through numerical tests conducted on a high-dimensional optimal stopping model in the area of American option pricing.","The numerical tests confirm both the accuracy and the computational efficiency of our proposed algorithm."],"url":"http://arxiv.org/abs/2405.11392v1","category":"q-fin.MF"}
{"created":"2024-05-18 20:53:05","title":"Optimal control barrier functions for RL based safe powertrain control","abstract":"Reinforcement learning (RL) can improve control performance by seeking to learn optimal control policies in the end-use environment for vehicles and other systems. To accomplish this, RL algorithms need to sufficiently explore the state and action spaces. This presents inherent safety risks, and applying RL on safety-critical systems like vehicle powertrain control requires safety enforcement approaches. In this paper, we seek control-barrier function (CBF)-based safety certificates that demarcate safe regions where the RL agent could optimize the control performance. In particular, we derive optimal high-order CBFs that avoid conservatism while ensuring safety for a vehicle in traffic. We demonstrate the workings of the high-order CBF with an RL agent which uses a deep actor-critic architecture to learn to optimize fuel economy and other driver accommodation metrics. We find that the optimized high-order CBF allows the RL-based powertrain control agent to achieve higher total rewards without any crashes in training and evaluation while achieving better accommodation of driver demands compared to previously proposed exponential barrier function filters and model-based baseline controllers.","sentences":["Reinforcement learning (RL) can improve control performance by seeking to learn optimal control policies in the end-use environment for vehicles and other systems.","To accomplish this, RL algorithms need to sufficiently explore the state and action spaces.","This presents inherent safety risks, and applying RL on safety-critical systems like vehicle powertrain control requires safety enforcement approaches.","In this paper, we seek control-barrier function (CBF)-based safety certificates that demarcate safe regions where the RL agent could optimize the control performance.","In particular, we derive optimal high-order CBFs that avoid conservatism while ensuring safety for a vehicle in traffic.","We demonstrate the workings of the high-order CBF with an RL agent which uses a deep actor-critic architecture to learn to optimize fuel economy and other driver accommodation metrics.","We find that the optimized high-order CBF allows the RL-based powertrain control agent to achieve higher total rewards without any crashes in training and evaluation while achieving better accommodation of driver demands compared to previously proposed exponential barrier function filters and model-based baseline controllers."],"url":"http://arxiv.org/abs/2405.11391v1","category":"eess.SY"}
{"created":"2024-05-18 20:24:11","title":"Adjacent Leader Decentralized Stochastic Gradient Descent","abstract":"This work focuses on the decentralized deep learning optimization framework. We propose Adjacent Leader Decentralized Gradient Descent (AL-DSGD), for improving final model performance, accelerating convergence, and reducing the communication overhead of decentralized deep learning optimizers. AL-DSGD relies on two main ideas. Firstly, to increase the influence of the strongest learners on the learning system it assigns weights to different neighbor workers according to both their performance and the degree when averaging among them, and it applies a corrective force on the workers dictated by both the currently best-performing neighbor and the neighbor with the maximal degree. Secondly, to alleviate the problem of the deterioration of the convergence speed and performance of the nodes with lower degrees, AL-DSGD relies on dynamic communication graphs, which effectively allows the workers to communicate with more nodes while keeping the degrees of the nodes low. Experiments demonstrate that AL-DSGD accelerates the convergence of the decentralized state-of-the-art techniques and improves their test performance especially in the communication constrained environments. We also theoretically prove the convergence of the proposed scheme. Finally, we release to the community a highly general and concise PyTorch-based library for distributed training of deep learning models that supports easy implementation of any distributed deep learning approach ((a)synchronous, (de)centralized).","sentences":["This work focuses on the decentralized deep learning optimization framework.","We propose Adjacent Leader Decentralized Gradient Descent (AL-DSGD), for improving final model performance, accelerating convergence, and reducing the communication overhead of decentralized deep learning optimizers.","AL-DSGD relies on two main ideas.","Firstly, to increase the influence of the strongest learners on the learning system it assigns weights to different neighbor workers according to both their performance and the degree when averaging among them, and it applies a corrective force on the workers dictated by both the currently best-performing neighbor and the neighbor with the maximal degree.","Secondly, to alleviate the problem of the deterioration of the convergence speed and performance of the nodes with lower degrees, AL-DSGD relies on dynamic communication graphs, which effectively allows the workers to communicate with more nodes while keeping the degrees of the nodes low.","Experiments demonstrate that AL-DSGD accelerates the convergence of the decentralized state-of-the-art techniques and improves their test performance especially in the communication constrained environments.","We also theoretically prove the convergence of the proposed scheme.","Finally, we release to the community a highly general and concise PyTorch-based library for distributed training of deep learning models that supports easy implementation of any distributed deep learning approach ((a)synchronous, (de)centralized)."],"url":"http://arxiv.org/abs/2405.11389v1","category":"cs.LG"}
{"created":"2024-05-18 20:22:54","title":"Combined film and pulse heating of lithium ion batteries to improve performance in low ambient temperature","abstract":"Low ambient temperatures significantly reduce Lithium ion batteries' (LIBs') charge/discharge power and energy capacity, and cause rapid degradation through lithium plating. These limitations can be addressed by preheating the LIB with an external heat source or by exploiting the internal heat generation through the LIB's internal impedance. Fast external heating generates large temperature gradients across the LIB due to the low thermal conductivity of the cell, while internal impedance heating (usually through AC or pulse charge/discharging) tends to be relatively slow, although it can achieve more uniform temperature distribution. This paper investigates the potential of combining externally sourced resistive film heating with bidirectional pulse heating to achieve fast preheating without causing steep temperature gradients. The LIB is modeled with the Doyle Fuller Newman (DFN) electrochemical model and 1D thermal model, and reinforcement learning (RL) is used to optimize the pulse current amplitude and film voltage concurrently. The results indicate that the optimal policy for maximizing the rate of temperature rise while limiting temperature gradients has the film heating dominate the initial phases and create the ideal conditions for pulse heating to take over. In addition, the pulse component shares the heating load and reduces the energy rating of the auxiliary power source.","sentences":["Low ambient temperatures significantly reduce Lithium ion batteries' (LIBs') charge/discharge power and energy capacity, and cause rapid degradation through lithium plating.","These limitations can be addressed by preheating the LIB with an external heat source or by exploiting the internal heat generation through the LIB's internal impedance.","Fast external heating generates large temperature gradients across the LIB due to the low thermal conductivity of the cell, while internal impedance heating (usually through AC or pulse charge/discharging) tends to be relatively slow, although it can achieve more uniform temperature distribution.","This paper investigates the potential of combining externally sourced resistive film heating with bidirectional pulse heating to achieve fast preheating without causing steep temperature gradients.","The LIB is modeled with the Doyle Fuller Newman (DFN) electrochemical model and 1D thermal model, and reinforcement learning (RL) is used to optimize the pulse current amplitude and film voltage concurrently.","The results indicate that the optimal policy for maximizing the rate of temperature rise while limiting temperature gradients has the film heating dominate the initial phases and create the ideal conditions for pulse heating to take over.","In addition, the pulse component shares the heating load and reduces the energy rating of the auxiliary power source."],"url":"http://arxiv.org/abs/2405.11388v1","category":"eess.SY"}
{"created":"2024-05-18 19:55:09","title":"Symmetry-guided data-driven discovery of native quantum defects in two-dimensional materials","abstract":"Drawing on their atomically thin structure, two-dimensional (2D) materials present a groundbreaking avenue for the precision fabrication and systematic manipulation of quantum defects. Through a method grounded in site-symmetry principles, we devise a comprehensive workflow to pinpoint potential native quantum defects across the entire spectrum of known binary 2D materials. Leveraging both symmetry principles and data-driven approaches markedly enhances the identification of spin defects exhibiting triplet ground states. This advancement is pivotal in discovering NV-like quantum defects in 2D materials, which are instrumental in facilitating a set of quantum functionalities. For discerning the multifaceted functionalities of these quantum defect candidates, their magneto-optical properties are comprehensively estimated using high-throughput computations. Our findings underscore that antisite defects in diverse hosts emerge as prospective quantum defects of significance. Crucially, based on our research, we advocate that the 16 antisites present in post-transition metal monochalcogenides (PTMCs) stand out as a prominent 2D-material-based quantum defect platforms, by their precise defect levels, optimal magneto-optical attributes, and the readily accessible nature of their host materials. This work substantially broadens the repertoire of quantum defects within the 2D material landscape, presenting profound implications for the advancement of quantum information science and technologies.","sentences":["Drawing on their atomically thin structure, two-dimensional (2D) materials present a groundbreaking avenue for the precision fabrication and systematic manipulation of quantum defects.","Through a method grounded in site-symmetry principles, we devise a comprehensive workflow to pinpoint potential native quantum defects across the entire spectrum of known binary 2D materials.","Leveraging both symmetry principles and data-driven approaches markedly enhances the identification of spin defects exhibiting triplet ground states.","This advancement is pivotal in discovering NV-like quantum defects in 2D materials, which are instrumental in facilitating a set of quantum functionalities.","For discerning the multifaceted functionalities of these quantum defect candidates, their magneto-optical properties are comprehensively estimated using high-throughput computations.","Our findings underscore that antisite defects in diverse hosts emerge as prospective quantum defects of significance.","Crucially, based on our research, we advocate that the 16 antisites present in post-transition metal monochalcogenides (PTMCs) stand out as a prominent 2D-material-based quantum defect platforms, by their precise defect levels, optimal magneto-optical attributes, and the readily accessible nature of their host materials.","This work substantially broadens the repertoire of quantum defects within the 2D material landscape, presenting profound implications for the advancement of quantum information science and technologies."],"url":"http://arxiv.org/abs/2405.11379v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-18 19:22:15","title":"Quantum Edge Detection","abstract":"This paper introduces quantum edge detection, aimed at locating boundaries of quantum domains where all particles share the same pure state. Focusing on the 1D scenario of a string of particles, we develop an optimal protocol for quantum edge detection, efficiently computing its success probability through Schur-Weyl duality and semidefinite programming techniques. We analyze the behavior of the success probability as a function of the string length and local dimension, with emphasis in the limit of long strings. We present a protocol based on square root measurement, which proves asymptotically optimal. Additionally, we explore a mixed quantum change point detection scenario where the state of particles transitions from known to unknown, which may find practical applications in detecting malfunctions in quantum devices","sentences":["This paper introduces quantum edge detection, aimed at locating boundaries of quantum domains where all particles share the same pure state.","Focusing on the 1D scenario of a string of particles, we develop an optimal protocol for quantum edge detection, efficiently computing its success probability through Schur-Weyl duality and semidefinite programming techniques.","We analyze the behavior of the success probability as a function of the string length and local dimension, with emphasis in the limit of long strings.","We present a protocol based on square root measurement, which proves asymptotically optimal.","Additionally, we explore a mixed quantum change point detection scenario where the state of particles transitions from known to unknown, which may find practical applications in detecting malfunctions in quantum devices"],"url":"http://arxiv.org/abs/2405.11373v1","category":"quant-ph"}
{"created":"2024-05-18 18:20:06","title":"A Position Allocation Approach to the Scheduling of Battery-Electric Bus Charging","abstract":"Robust charging schedules in a growing market of battery electric bus (BEB) fleets are a critical component to successful adoption. In this paper, a BEB charging scheduling framework that considers spatiotemporal schedule constraints, route schedules, fast and slow charging, and battery dynamics is modeled as a mixed integer linear program (MILP). The MILP is modeled after the Berth Allocation Problem (BAP) in a modified form known as the Position Allocation Problem (PAP). Linear battery dynamics are included to model the charging of buses while at the station. To model the BEB discharges over their respective routes, it is assumed each BEB has an average kWh charge loss while on route. The optimization coordinates BEB charging to ensure that each vehicle remains above a specified state-of-charge (SOC). The model also minimizes the total number of chargers utilized and prioritizes slow charging for battery health. The model validity is demonstrated with a set of routes sampled from the Utah Transit Authority (UTA) for \\A buses and \\N visits to the charging station. The model is also compared to a heuristic algorithm based on charge thresholds referred to as the Qin-Modified method. The results presented show that the slow chargers are more readily selected and the charging and spatiotemporal constraints are met while considering the battery dynamics and minimizing both the charger count and consumption cost.","sentences":["Robust charging schedules in a growing market of battery electric bus (BEB) fleets are a critical component to successful adoption.","In this paper, a BEB charging scheduling framework that considers spatiotemporal schedule constraints, route schedules, fast and slow charging, and battery dynamics is modeled as a mixed integer linear program (MILP).","The MILP is modeled after the Berth Allocation Problem (BAP) in a modified form known as the Position Allocation Problem (PAP).","Linear battery dynamics are included to model the charging of buses while at the station.","To model the BEB discharges over their respective routes, it is assumed each BEB has an average kWh charge loss while on route.","The optimization coordinates BEB charging to ensure that each vehicle remains above a specified state-of-charge (SOC).","The model also minimizes the total number of chargers utilized and prioritizes slow charging for battery health.","The model validity is demonstrated with a set of routes sampled from the Utah Transit Authority (UTA) for \\A buses and \\N visits to the charging station.","The model is also compared to a heuristic algorithm based on charge thresholds referred to as the Qin-Modified method.","The results presented show that the slow chargers are more readily selected and the charging and spatiotemporal constraints are met while considering the battery dynamics and minimizing both the charger count and consumption cost."],"url":"http://arxiv.org/abs/2405.11365v1","category":"math.OC"}
{"created":"2024-05-18 18:14:44","title":"Role of Plasticizers in Film Formation and Deformation-Induced Bonding of Glassy Polymers of Bidisperse Blends","abstract":"Bonding between polymers below the glass transition temperature through molecular-scale dilatation (or densification)-based interdiffusion of macromolecules has recently been introduced. In this mechanism, plastic deformation enables polymer chains to interdiffuse and form entanglements at the interface, facilitating rapid bonding below the glass transition temperature ($T_g$). Here, we are addressing the role of low molecular weight plasticizer in bonding polymer interfaces of bidisperse blends through deformation-induced bonding (DIB) at temperatures well below the surface and bulk glass transition temperatures, $T_g^s$ and $T_g^b$, respectively, by using molecular simulations. These simulations reveal that addition of plasticizer ($\\phi\\le$20\\%) drastically enhances the number of chain-ends at the interfacial region compared to a pure glass sample ($\\phi=0\\%$) during deformation below $T_g^s$, which improves the possibility of opposite side entanglement formation. The changes in stress-strain response of debonded samples correlate with the normalized entanglement density. Likewise, the maximum interfacial fracture energy $G_{I,max}$ of debonded samples is correlated with the plasticizer concentration ($\\phi$), below $T_g^s$. Furthermore, the optimization of material and process conditions for DIB has yielded a notable advancement: achieving a higher bonding strength, approximately one-third of the bulk, all while remaining below $T_g$.","sentences":["Bonding between polymers below the glass transition temperature through molecular-scale dilatation (or densification)-based interdiffusion of macromolecules has recently been introduced.","In this mechanism, plastic deformation enables polymer chains to interdiffuse and form entanglements at the interface, facilitating rapid bonding below the glass transition temperature ($T_g$).","Here, we are addressing the role of low molecular weight plasticizer in bonding polymer interfaces of bidisperse blends through deformation-induced bonding (DIB) at temperatures well below the surface and bulk glass transition temperatures, $T_g^s$ and $T_g^b$, respectively, by using molecular simulations.","These simulations reveal that addition of plasticizer ($\\phi\\le$20\\%) drastically enhances the number of chain-ends at the interfacial region compared to a pure glass sample ($\\phi=0\\%$) during deformation below $T_g^s$, which improves the possibility of opposite side entanglement formation.","The changes in stress-strain response of debonded samples correlate with the normalized entanglement density.","Likewise, the maximum interfacial fracture energy $G_{I,max}$ of debonded samples is correlated with the plasticizer concentration ($\\phi$), below $T_g^s$. Furthermore, the optimization of material and process conditions for DIB has yielded a notable advancement: achieving a higher bonding strength, approximately one-third of the bulk, all while remaining below $T_g$."],"url":"http://arxiv.org/abs/2405.11362v1","category":"cond-mat.soft"}
{"created":"2024-05-18 18:10:53","title":"Optimizing Layerwise Microservice Management in Heterogeneous Wireless Networks","abstract":"Small cells with edge computing are densely deployed in 5G mobile networks to provide high throughput communication and low-latency computation. The flexibility of edge computation is empowered by the deployment of lightweight container-based microservices. In this paper, we take the first step toward optimizing the microservice management in small-cell networks. The prominent feature is that each microservice consists of multiple image layers and different microservices may share some basic layers, thus bringing deep coupling in their placement and service provision. Our objective is to minimize the expected total latency of microservice requests under the storage, communication and computing constraints of the sparsely interconnected small cell nodes. We formulate a binary quadratic program (BQP) with the multi-dimensional strategy of the image layer placement, the access selection and the task assignment. The BQP problem is then transformed into an ILP problem, and is solved by use of a novel sphere-box alternating direction multipliers method (ADMM) with reasonable complexity $O(q^{4})$, where $q$ is the number of variables in the transformed problem. Trace-driven experiments show that the gap between our proposed algorithm and the optimal is reduced by 35$\\%$ compared with benchmark algorithms.","sentences":["Small cells with edge computing are densely deployed in 5G mobile networks to provide high throughput communication and low-latency computation.","The flexibility of edge computation is empowered by the deployment of lightweight container-based microservices.","In this paper, we take the first step toward optimizing the microservice management in small-cell networks.","The prominent feature is that each microservice consists of multiple image layers and different microservices may share some basic layers, thus bringing deep coupling in their placement and service provision.","Our objective is to minimize the expected total latency of microservice requests under the storage, communication and computing constraints of the sparsely interconnected small cell nodes.","We formulate a binary quadratic program (BQP) with the multi-dimensional strategy of the image layer placement, the access selection and the task assignment.","The BQP problem is then transformed into an ILP problem, and is solved by use of a novel sphere-box alternating direction multipliers method (ADMM) with reasonable complexity $O(q^{4})$, where $q$ is the number of variables in the transformed problem.","Trace-driven experiments show that the gap between our proposed algorithm and the optimal is reduced by 35$\\%$ compared with benchmark algorithms."],"url":"http://arxiv.org/abs/2405.11359v1","category":"cs.NI"}
{"created":"2024-05-18 17:58:40","title":"Control-Aware Transmit Power Allocation for 6G In-Factory Subnetwork Control Systems","abstract":"In this paper, we develop a novel power control solution for subnetworks-enabled distributed control systems in factory settings. We propose a channel-independent control-aware (CICA) policy based on the logistic model and learn the parameters using Bayesian optimization with a multi-objective tree-structured Parzen estimator. The objective is to minimize the control cost of the plants, measured as a finite horizon linear quadratic regulator cost. The proposed policy can be executed in a fully distributed manner and does not require cumbersome measurement of channel gain information, hence it is scalable for large-scale deployment of subnetworks for distributed control applications. With extensive numerical simulation and considering different densities of subnetworks, we show that the proposed method can achieve competitive stability performance and high availability for large-scale distributed control plants with limited radio resources.","sentences":["In this paper, we develop a novel power control solution for subnetworks-enabled distributed control systems in factory settings.","We propose a channel-independent control-aware (CICA) policy based on the logistic model and learn the parameters using Bayesian optimization with a multi-objective tree-structured Parzen estimator.","The objective is to minimize the control cost of the plants, measured as a finite horizon linear quadratic regulator cost.","The proposed policy can be executed in a fully distributed manner and does not require cumbersome measurement of channel gain information, hence it is scalable for large-scale deployment of subnetworks for distributed control applications.","With extensive numerical simulation and considering different densities of subnetworks, we show that the proposed method can achieve competitive stability performance and high availability for large-scale distributed control plants with limited radio resources."],"url":"http://arxiv.org/abs/2405.11355v1","category":"eess.SY"}
{"created":"2024-05-18 17:44:17","title":"NTTSuite: Number Theoretic Transform Benchmarks for Accelerating Encrypted Computation","abstract":"Privacy concerns have thrust privacy-preserving computation into the spotlight. Homomorphic encryption (HE) is a cryptographic system that enables computation to occur directly on encrypted data, providing users with strong privacy (and security) guarantees while using the same services they enjoy today unprotected. While promising, HE has seen little adoption due to extremely high computational overheads, rendering it impractical. Homomorphic encryption (HE) is a cryptographic system that enables computation to occur directly on encrypted data. In this paper we develop a benchmark suite, named NTTSuite, to enable researchers to better address these overheads by studying the primary source of HE's slowdown: the number theoretic transform (NTT). NTTSuite constitutes seven unique NTT algorithms with support for CPUs (C++), GPUs (CUDA), and custom hardware (Catapult HLS).In addition, we propose optimizations to improve the performance of NTT running on FPGAs. We find our implementation outperforms the state-of-the-art by 30%.","sentences":["Privacy concerns have thrust privacy-preserving computation into the spotlight.","Homomorphic encryption (HE) is a cryptographic system that enables computation to occur directly on encrypted data, providing users with strong privacy (and security) guarantees while using the same services they enjoy today unprotected.","While promising, HE has seen little adoption due to extremely high computational overheads, rendering it impractical.","Homomorphic encryption (HE) is a cryptographic system that enables computation to occur directly on encrypted data.","In this paper we develop a benchmark suite, named NTTSuite, to enable researchers to better address these overheads by studying the primary source of HE's slowdown: the number theoretic transform (NTT).","NTTSuite constitutes seven unique NTT algorithms with support for CPUs (C++), GPUs (CUDA), and custom hardware (Catapult HLS).In addition, we propose optimizations to improve the performance of NTT running on FPGAs.","We find our implementation outperforms the state-of-the-art by 30%."],"url":"http://arxiv.org/abs/2405.11353v1","category":"cs.CR"}
{"created":"2024-05-18 16:47:36","title":"UPAM: Unified Prompt Attack in Text-to-Image Generation Models Against Both Textual Filters and Visual Checkers","abstract":"Text-to-Image (T2I) models have raised security concerns due to their potential to generate inappropriate or harmful images. In this paper, we propose UPAM, a novel framework that investigates the robustness of T2I models from the attack perspective. Unlike most existing attack methods that focus on deceiving textual defenses, UPAM aims to deceive both textual and visual defenses in T2I models. UPAM enables gradient-based optimization, offering greater effectiveness and efficiency than previous methods. Given that T2I models might not return results due to defense mechanisms, we introduce a Sphere-Probing Learning (SPL) scheme to support gradient optimization even when no results are returned. Additionally, we devise a Semantic-Enhancing Learning (SEL) scheme to finetune UPAM for generating target-aligned images. Our framework also ensures attack stealthiness. Extensive experiments demonstrate UPAM's effectiveness and efficiency.","sentences":["Text-to-Image (T2I) models have raised security concerns due to their potential to generate inappropriate or harmful images.","In this paper, we propose UPAM, a novel framework that investigates the robustness of T2I models from the attack perspective.","Unlike most existing attack methods that focus on deceiving textual defenses, UPAM aims to deceive both textual and visual defenses in T2I models.","UPAM enables gradient-based optimization, offering greater effectiveness and efficiency than previous methods.","Given that T2I models might not return results due to defense mechanisms, we introduce a Sphere-Probing Learning (SPL) scheme to support gradient optimization even when no results are returned.","Additionally, we devise a Semantic-Enhancing Learning (SEL) scheme to finetune UPAM for generating target-aligned images.","Our framework also ensures attack stealthiness.","Extensive experiments demonstrate UPAM's effectiveness and efficiency."],"url":"http://arxiv.org/abs/2405.11336v1","category":"cs.CV"}
{"created":"2024-05-18 15:35:26","title":"High-yield fabrication of bubble-free magic-angle twisted bilayer graphene devices with high twist-angle homogeneity","abstract":"Magic-angle twisted bilayer graphene (MATBG) stands as one of the most versatile materials in condensed-matter physics due to its hosting of a wide variety of exotic phases while also offering convenient tunability. However, the fabrication of MATBG is still manual, and remains to be a challenging and inefficient process, with devices being highly dependent on specific fabrication methods, that often result in inconsistency and variability. In this work, we present an optimized protocol for the fabrication of MATBG samples, for which we use deterministic graphene anchoring to stabilize the twist-angle, and a careful bubble removal techniques to ensure a high twist-angle homogeneity. We use low-temperature transport experiments to extract the average twist-angle between pairs of leads. We find that up to 38 percent of the so fabricated devices show micrometer square sized regions with a twist-angle in the range 1.1 plus/minus 0.1 degrees, and a twist-angle variation of only 0.02 degrees, where in some instances such regions were up to 36 micrometer square large. We are certain that the discussed protocols can be directly transferred to non-graphene materials, and will be useful for the growing field of moire materials.","sentences":["Magic-angle twisted bilayer graphene (MATBG) stands as one of the most versatile materials in condensed-matter physics due to its hosting of a wide variety of exotic phases while also offering convenient tunability.","However, the fabrication of MATBG is still manual, and remains to be a challenging and inefficient process, with devices being highly dependent on specific fabrication methods, that often result in inconsistency and variability.","In this work, we present an optimized protocol for the fabrication of MATBG samples, for which we use deterministic graphene anchoring to stabilize the twist-angle, and a careful bubble removal techniques to ensure a high twist-angle homogeneity.","We use low-temperature transport experiments to extract the average twist-angle between pairs of leads.","We find that up to 38 percent of the so fabricated devices show micrometer square sized regions with a twist-angle in the range 1.1 plus/minus 0.1 degrees, and a twist-angle variation of only 0.02 degrees, where in some instances such regions were up to 36 micrometer square large.","We are certain that the discussed protocols can be directly transferred to non-graphene materials, and will be useful for the growing field of moire materials."],"url":"http://arxiv.org/abs/2405.11323v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-18 15:07:20","title":"Revisiting $O(N)$ $\u03c3$ model at unphysical pion masses and high temperatures. II. The vacuum structure and thermal $\u03c3$ pole trajectory with cross-channel improvements","abstract":"The effective potential of $O(N)$ model at large $N$ limit is reinvestigated for varying pion mass and temperature. For large pion masses and high temperatures, we find the phenomenologically favored vacuum, located on the upper branch of the double-branched effective potential for physical $m_\\pi$, moves to the lower branch and becomes no longer a local minimum but a saddle point. The existence and running of the tachyon pole are also discussed. With the effective coupling constant defined from the effective potential, the possible correspondence relation between the two branches of the effective potential and the two phases of the theory (distinguished by positive or negative coupling) is verified even with nonzero explicit symmetry breaking and at finite temperature. Also, we generalize the $N/D$ modified $O(N)$ model to study the thermal trajectory of the $\\sigma$ pole with the cross-channel contributions considered and find the thermal $\\sigma$ pole trajectory resembles its counterpart with varying pion mass at zero temperature.","sentences":["The effective potential of $O(N)$ model at large $N$ limit is reinvestigated for varying pion mass and temperature.","For large pion masses and high temperatures, we find the phenomenologically favored vacuum, located on the upper branch of the double-branched effective potential for physical $m_\\pi$, moves to the lower branch and becomes no longer a local minimum but a saddle point.","The existence and running of the tachyon pole are also discussed.","With the effective coupling constant defined from the effective potential, the possible correspondence relation between the two branches of the effective potential and the two phases of the theory (distinguished by positive or negative coupling) is verified even with nonzero explicit symmetry breaking and at finite temperature.","Also, we generalize the $N/D$ modified $O(N)$ model to study the thermal trajectory of the $\\sigma$ pole with the cross-channel contributions considered and find the thermal $\\sigma$ pole trajectory resembles its counterpart with varying pion mass at zero temperature."],"url":"http://arxiv.org/abs/2405.11313v1","category":"hep-ph"}
{"created":"2024-05-18 14:00:04","title":"The CAP Principle for LLM Serving","abstract":"We survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale. Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P). Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously. Our survey categorizes existing works within this framework. We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild. We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models. As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges.","sentences":["We survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale.","Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P).","Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously.","Our survey categorizes existing works within this framework.","We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild.","We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models.","As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges."],"url":"http://arxiv.org/abs/2405.11299v1","category":"cs.DB"}
{"created":"2024-05-18 12:19:16","title":"Predicting and Explaining Hearing Aid Usage Using Encoder-Decoder with Attention Mechanism and SHAP","abstract":"It is essential to understand the personal, behavioral, environmental, and other factors that correlate with optimal hearing aid fitting and hearing aid users' experiences in order to improve hearing loss patient satisfaction and quality of life, as well as reduce societal and financial burdens. This work proposes a novel framework that uses Encoder-decoder with attention mechanism (attn-ED) for predicting future hearing aid usage and SHAP to explain the factors contributing to this prediction. It has been demonstrated in experiments that attn-ED performs well at predicting future hearing aid usage, and that SHAP can be utilized to calculate the contribution of different factors affecting hearing aid usage. This framework aims to establish confidence that AI models can be utilized in the medical domain with the use of XAI methods. Moreover, the proposed framework can also assist clinicians in determining the nature of interventions.","sentences":["It is essential to understand the personal, behavioral, environmental, and other factors that correlate with optimal hearing aid fitting and hearing aid users' experiences in order to improve hearing loss patient satisfaction and quality of life, as well as reduce societal and financial burdens.","This work proposes a novel framework that uses Encoder-decoder with attention mechanism (attn-ED) for predicting future hearing aid usage and SHAP to explain the factors contributing to this prediction.","It has been demonstrated in experiments that attn-ED performs well at predicting future hearing aid usage, and that SHAP can be utilized to calculate the contribution of different factors affecting hearing aid usage.","This framework aims to establish confidence that AI models can be utilized in the medical domain with the use of XAI methods.","Moreover, the proposed framework can also assist clinicians in determining the nature of interventions."],"url":"http://arxiv.org/abs/2405.11275v1","category":"cs.LG"}
{"created":"2024-05-18 11:33:08","title":"Stability for Nash Equilibrium Problems","abstract":"This paper is devoted to studying the stability properties of the Karush-Kuhn-Tucker (KKT) solution mapping $S_{\\rm KKT}$ for Nash equilibrium problems (NEPs) with canonical perturbations. Firstly, we obtain an exact characterization of the strong regularity of $S_{\\rm KKT}$ and a sufficient condition that is easy to verify. Secondly, we propose equivalent conditions for the continuously differentiable single-valued localization of $S_{\\rm KKT}$. Thirdly, the isolated calmness of $S_{\\rm KKT}$ is studied based on two conditions: Property A and Property B, and Property B proves to be sufficient for the robustness of both $E(p)$ and $S_{\\rm KKT}$ under the convex assumptions, where $E(p)$ denotes the Nash equilibria at perturbation $p$. Furthermore, we establish that studying the stability properties of the NEP with canonical perturbations is equivalent to studying those of the NEP with only tilt perturbations based on the prior discussions. Finally, we provide detailed characterizations of stability for NEPs whose each individual player solves a quadratic programming (QP) problem.","sentences":["This paper is devoted to studying the stability properties of the Karush-Kuhn-Tucker (KKT) solution mapping $S_{\\rm KKT}$ for Nash equilibrium problems (NEPs) with canonical perturbations.","Firstly, we obtain an exact characterization of the strong regularity of $S_{\\rm KKT}$ and a sufficient condition that is easy to verify.","Secondly, we propose equivalent conditions for the continuously differentiable single-valued localization of $S_{\\rm KKT}$. Thirdly, the isolated calmness of $S_{\\rm KKT}$ is studied based on two conditions: Property A and Property B, and Property B proves to be sufficient for the robustness of both $E(p)$ and $S_{\\rm KKT}$ under the convex assumptions, where $E(p)$ denotes the Nash equilibria at perturbation $p$.","Furthermore, we establish that studying the stability properties of the NEP with canonical perturbations is equivalent to studying those of the NEP with only tilt perturbations based on the prior discussions.","Finally, we provide detailed characterizations of stability for NEPs whose each individual player solves a quadratic programming (QP) problem."],"url":"http://arxiv.org/abs/2405.11266v1","category":"math.OC"}
{"created":"2024-05-18 10:56:45","title":"WisPerMed at \"Discharge Me!\": Advancing Text Generation in Healthcare with Large Language Models, Dynamic Expert Selection, and Priming Techniques on MIMIC-IV","abstract":"This study aims to leverage state of the art language models to automate generating the \"Brief Hospital Course\" and \"Discharge Instructions\" sections of Discharge Summaries from the MIMIC-IV dataset, reducing clinicians' administrative workload. We investigate how automation can improve documentation accuracy, alleviate clinician burnout, and enhance operational efficacy in healthcare facilities. This research was conducted within our participation in the Shared Task Discharge Me! at BioNLP @ ACL 2024. Various strategies were employed, including few-shot learning, instruction tuning, and Dynamic Expert Selection (DES), to develop models capable of generating the required text sections. Notably, utilizing an additional clinical domain-specific dataset demonstrated substantial potential to enhance clinical language processing. The DES method, which optimizes the selection of text outputs from multiple predictions, proved to be especially effective. It achieved the highest overall score of 0.332 in the competition, surpassing single-model outputs. This finding suggests that advanced deep learning methods in combination with DES can effectively automate parts of electronic health record documentation. These advancements could enhance patient care by freeing clinician time for patient interactions. The integration of text selection strategies represents a promising avenue for further research.","sentences":["This study aims to leverage state of the art language models to automate generating the \"Brief Hospital Course\" and \"Discharge Instructions\" sections of Discharge Summaries from the MIMIC-IV dataset, reducing clinicians' administrative workload.","We investigate how automation can improve documentation accuracy, alleviate clinician burnout, and enhance operational efficacy in healthcare facilities.","This research was conducted within our participation in the Shared Task Discharge Me!","at BioNLP @ ACL 2024.","Various strategies were employed, including few-shot learning, instruction tuning, and Dynamic Expert Selection (DES), to develop models capable of generating the required text sections.","Notably, utilizing an additional clinical domain-specific dataset demonstrated substantial potential to enhance clinical language processing.","The DES method, which optimizes the selection of text outputs from multiple predictions, proved to be especially effective.","It achieved the highest overall score of 0.332 in the competition, surpassing single-model outputs.","This finding suggests that advanced deep learning methods in combination with DES can effectively automate parts of electronic health record documentation.","These advancements could enhance patient care by freeing clinician time for patient interactions.","The integration of text selection strategies represents a promising avenue for further research."],"url":"http://arxiv.org/abs/2405.11255v1","category":"cs.CL"}
{"created":"2024-05-18 10:41:57","title":"Dreamer XL: Towards High-Resolution Text-to-3D Generation via Trajectory Score Matching","abstract":"In this work, we propose a novel Trajectory Score Matching (TSM) method that aims to solve the pseudo ground truth inconsistency problem caused by the accumulated error in Interval Score Matching (ISM) when using the Denoising Diffusion Implicit Models (DDIM) inversion process. Unlike ISM which adopts the inversion process of DDIM to calculate on a single path, our TSM method leverages the inversion process of DDIM to generate two paths from the same starting point for calculation. Since both paths start from the same starting point, TSM can reduce the accumulated error compared to ISM, thus alleviating the problem of pseudo ground truth inconsistency. TSM enhances the stability and consistency of the model's generated paths during the distillation process. We demonstrate this experimentally and further show that ISM is a special case of TSM. Furthermore, to optimize the current multi-stage optimization process from high-resolution text to 3D generation, we adopt Stable Diffusion XL for guidance. In response to the issues of abnormal replication and splitting caused by unstable gradients during the 3D Gaussian splatting process when using Stable Diffusion XL, we propose a pixel-by-pixel gradient clipping method. Extensive experiments show that our model significantly surpasses the state-of-the-art models in terms of visual quality and performance. Code: \\url{https://github.com/xingy038/Dreamer-XL}.","sentences":["In this work, we propose a novel Trajectory Score Matching (TSM) method that aims to solve the pseudo ground truth inconsistency problem caused by the accumulated error in Interval Score Matching (ISM) when using the Denoising Diffusion Implicit Models (DDIM) inversion process.","Unlike ISM which adopts the inversion process of DDIM to calculate on a single path, our TSM method leverages the inversion process of DDIM to generate two paths from the same starting point for calculation.","Since both paths start from the same starting point, TSM can reduce the accumulated error compared to ISM, thus alleviating the problem of pseudo ground truth inconsistency.","TSM enhances the stability and consistency of the model's generated paths during the distillation process.","We demonstrate this experimentally and further show that ISM is a special case of TSM.","Furthermore, to optimize the current multi-stage optimization process from high-resolution text to 3D generation, we adopt Stable Diffusion XL for guidance.","In response to the issues of abnormal replication and splitting caused by unstable gradients during the 3D Gaussian splatting process when using Stable Diffusion XL, we propose a pixel-by-pixel gradient clipping method.","Extensive experiments show that our model significantly surpasses the state-of-the-art models in terms of visual quality and performance.","Code: \\url{https://github.com/xingy038/Dreamer-XL}."],"url":"http://arxiv.org/abs/2405.11252v1","category":"cs.CV"}
{"created":"2024-05-18 10:26:00","title":"Generalized extremiles and risk measures of distorted random variables","abstract":"Quantiles, expectiles and extremiles can be seen as concepts defined via an optimization problem, where this optimization problem is driven by two important ingredients: the loss function as well as a distributional weight function. This leads to the formulation of a general class of functionals that contains next to the above concepts many interesting quantities, including also a subclass of distortion risks. The focus of the paper is on developing estimators for such functionals and to establish asymptotic consistency and asymptotic normality of these estimators. The advantage of the general framework is that it allows application to a very broad range of concepts, providing as such estimation tools and tools for statistical inference (for example for construction of confidence intervals) for all involved concepts. After developing the theory for the general functional we apply it to various settings, illustrating the broad applicability. In a real data example the developed tools are used in an analysis of natural disasters.","sentences":["Quantiles, expectiles and extremiles can be seen as concepts defined via an optimization problem, where this optimization problem is driven by two important ingredients: the loss function as well as a distributional weight function.","This leads to the formulation of a general class of functionals that contains next to the above concepts many interesting quantities, including also a subclass of distortion risks.","The focus of the paper is on developing estimators for such functionals and to establish asymptotic consistency and asymptotic normality of these estimators.","The advantage of the general framework is that it allows application to a very broad range of concepts, providing as such estimation tools and tools for statistical inference (for example for construction of confidence intervals) for all involved concepts.","After developing the theory for the general functional we apply it to various settings, illustrating the broad applicability.","In a real data example the developed tools are used in an analysis of natural disasters."],"url":"http://arxiv.org/abs/2405.11248v1","category":"stat.ME"}
{"created":"2024-05-18 10:13:15","title":"On the consistent estimators of the population covariance matrix and its reparameterizations","abstract":"For the high-dimensional covariance estimation problem, when $\\lim_{n\\to \\infty}p/n=c \\in (0,1)$ the orthogonally equivariant estimator of the population covariance matrix proposed by Tsai and Tsai (2024b) enjoys some optimal properties. Under some regularity conditions, they showed that their novel estimators of eigenvalues are consistent for the eigenvalues of the population covariance matrix. In this note, first, we show that their novel estimator is consistent estimator of the population covariance matrix under a high-dimensional asymptotic setup. Moreover, we also show that the novel estimator is the MLE of population covariance matrix when $c \\in (0, 1)$. The novel estimator is incorporated to establish the optimal decomposite $T_{T}^{2}-$test for a high-dimensional statistical hypothesis testing problem.","sentences":["For the high-dimensional covariance estimation problem, when $\\lim_{n\\to \\infty}p/n=c \\in (0,1)$ the orthogonally equivariant estimator of the population covariance matrix proposed by Tsai and Tsai (2024b) enjoys some optimal properties.","Under some regularity conditions, they showed that their novel estimators of eigenvalues are consistent for the eigenvalues of the population covariance matrix.","In this note, first, we show that their novel estimator is consistent estimator of the population covariance matrix under a high-dimensional asymptotic setup.","Moreover, we also show that the novel estimator is the MLE of population covariance matrix when $c \\in (0, 1)$.","The novel estimator is incorporated to establish the optimal decomposite $T_{T}^{2}-$test for a high-dimensional statistical hypothesis testing problem."],"url":"http://arxiv.org/abs/2405.11246v1","category":"math.ST"}
{"created":"2024-05-18 10:05:31","title":"Strided Difference Bound Matrices","abstract":"A wide range of symbolic analysis and optimization problems can be formalized using polyhedra. Sub-classes of polyhedra, also known as sub-polyhedral domains, are sought for their lower space and time complexity. We introduce the Strided Difference Bound Matrix (SDBM) domain, which represents a sweet spot in the context of optimizing compilers. Its expressiveness and efficient algorithms are particularly well suited to the construction of machine learning compilers. We present decision algorithms, abstract domain operators and computational complexity proofs for SDBM. We also conduct an empirical study with the MLIR compiler framework to validate the domain's practical applicability. We characterize a sub-class of SDBMs that frequently occurs in practice, and demonstrate even faster algorithms on this sub-class.","sentences":["A wide range of symbolic analysis and optimization problems can be formalized using polyhedra.","Sub-classes of polyhedra, also known as sub-polyhedral domains, are sought for their lower space and time complexity.","We introduce the Strided Difference Bound Matrix (SDBM) domain, which represents a sweet spot in the context of optimizing compilers.","Its expressiveness and efficient algorithms are particularly well suited to the construction of machine learning compilers.","We present decision algorithms, abstract domain operators and computational complexity proofs for SDBM.","We also conduct an empirical study with the MLIR compiler framework to validate the domain's practical applicability.","We characterize a sub-class of SDBMs that frequently occurs in practice, and demonstrate even faster algorithms on this sub-class."],"url":"http://arxiv.org/abs/2405.11244v1","category":"cs.SC"}
{"created":"2024-05-18 09:39:38","title":"Uncover mortality patterns and hospital effects in COVID-19 heart failure patients: a novel Multilevel logistic cluster-weighted modeling approach","abstract":"Evaluating hospitals' performance and its relation to patients' characteristics is of utmost importance to ensure timely, effective, and optimal treatment. Such a matter is particularly relevant in areas and situations where the healthcare system must contend with an unexpected surge in hospitalizations, such as for heart failure patients in the Lombardy region of Italy during the COVID-19 pandemic. Motivated by this issue, the paper introduces a novel Multilevel Logistic Cluster-Weighted Model (ML-CWMd) for predicting 45-day mortality following hospitalization due to COVID-19. The methodology flexibly accommodates dependence patterns among continuous, categorical, and dichotomous variables; effectively accounting for hospital-specific effects in distinct patient subgroups showing different attributes. A tailored Expectation-Maximization algorithm is developed for parameter estimation, and extensive simulation studies are conducted to evaluate its performance against competing models. The novel approach is applied to administrative data from the Lombardy Region, aiming to profile heart failure patients hospitalized for COVID-19 and investigate the hospital-level impact on their overall mortality. A scenario analysis demonstrates the model's efficacy in managing multiple sources of heterogeneity, thereby yielding promising results in aiding healthcare providers and policy-makers in the identification of patient-specific treatment pathways.","sentences":["Evaluating hospitals' performance and its relation to patients' characteristics is of utmost importance to ensure timely, effective, and optimal treatment.","Such a matter is particularly relevant in areas and situations where the healthcare system must contend with an unexpected surge in hospitalizations, such as for heart failure patients in the Lombardy region of Italy during the COVID-19 pandemic.","Motivated by this issue, the paper introduces a novel Multilevel Logistic Cluster-Weighted Model (ML-CWMd) for predicting 45-day mortality following hospitalization due to COVID-19.","The methodology flexibly accommodates dependence patterns among continuous, categorical, and dichotomous variables; effectively accounting for hospital-specific effects in distinct patient subgroups showing different attributes.","A tailored Expectation-Maximization algorithm is developed for parameter estimation, and extensive simulation studies are conducted to evaluate its performance against competing models.","The novel approach is applied to administrative data from the Lombardy Region, aiming to profile heart failure patients hospitalized for COVID-19 and investigate the hospital-level impact on their overall mortality.","A scenario analysis demonstrates the model's efficacy in managing multiple sources of heterogeneity, thereby yielding promising results in aiding healthcare providers and policy-makers in the identification of patient-specific treatment pathways."],"url":"http://arxiv.org/abs/2405.11239v1","category":"stat.AP"}
{"created":"2024-05-18 08:32:37","title":"BadActs: A Universal Backdoor Defense in the Activation Space","abstract":"Backdoor attacks pose an increasingly severe security threat to Deep Neural Networks (DNNs) during their development stage. In response, backdoor sample purification has emerged as a promising defense mechanism, aiming to eliminate backdoor triggers while preserving the integrity of the clean content in the samples. However, existing approaches have been predominantly focused on the word space, which are ineffective against feature-space triggers and significantly impair performance on clean data. To address this, we introduce a universal backdoor defense that purifies backdoor samples in the activation space by drawing abnormal activations towards optimized minimum clean activation distribution intervals. The advantages of our approach are twofold: (1) By operating in the activation space, our method captures from surface-level information like words to higher-level semantic concepts such as syntax, thus counteracting diverse triggers; (2) the fine-grained continuous nature of the activation space allows for more precise preservation of clean content while removing triggers. Furthermore, we propose a detection module based on statistical information of abnormal activations, to achieve a better trade-off between clean accuracy and defending performance.","sentences":["Backdoor attacks pose an increasingly severe security threat to Deep Neural Networks (DNNs) during their development stage.","In response, backdoor sample purification has emerged as a promising defense mechanism, aiming to eliminate backdoor triggers while preserving the integrity of the clean content in the samples.","However, existing approaches have been predominantly focused on the word space, which are ineffective against feature-space triggers and significantly impair performance on clean data.","To address this, we introduce a universal backdoor defense that purifies backdoor samples in the activation space by drawing abnormal activations towards optimized minimum clean activation distribution intervals.","The advantages of our approach are twofold: (1) By operating in the activation space, our method captures from surface-level information like words to higher-level semantic concepts such as syntax, thus counteracting diverse triggers; (2) the fine-grained continuous nature of the activation space allows for more precise preservation of clean content while removing triggers.","Furthermore, we propose a detection module based on statistical information of abnormal activations, to achieve a better trade-off between clean accuracy and defending performance."],"url":"http://arxiv.org/abs/2405.11227v1","category":"cs.CR"}
{"created":"2024-05-18 08:29:15","title":"The Power of Active Multi-Task Learning in Reinforcement Learning from Human Feedback","abstract":"Reinforcement learning from human feedback (RLHF) has contributed to performance improvements in large language models. To tackle its reliance on substantial amounts of human-labeled data, a successful approach is multi-task representation learning, which involves learning a high-quality, low-dimensional representation from a wide range of source tasks. In this paper, we formulate RLHF as the contextual dueling bandit problem and assume a common linear representation. We demonstrate that the sample complexity of source tasks in multi-task RLHF can be reduced by considering task relevance and allocating different sample sizes to source tasks with varying task relevance. We further propose an algorithm to estimate task relevance by a small number of additional data and then learn a policy. We prove that to achieve $\\varepsilon-$optimal, the sample complexity of the source tasks can be significantly reduced compared to uniform sampling. Additionally, the sample complexity of the target task is only linear in the dimension of the latent space, thanks to representation learning.","sentences":["Reinforcement learning from human feedback (RLHF) has contributed to performance improvements in large language models.","To tackle its reliance on substantial amounts of human-labeled data, a successful approach is multi-task representation learning, which involves learning a high-quality, low-dimensional representation from a wide range of source tasks.","In this paper, we formulate RLHF as the contextual dueling bandit problem and assume a common linear representation.","We demonstrate that the sample complexity of source tasks in multi-task RLHF can be reduced by considering task relevance and allocating different sample sizes to source tasks with varying task relevance.","We further propose an algorithm to estimate task relevance by a small number of additional data and then learn a policy.","We prove that to achieve $\\varepsilon-$optimal, the sample complexity of the source tasks can be significantly reduced compared to uniform sampling.","Additionally, the sample complexity of the target task is only linear in the dimension of the latent space, thanks to representation learning."],"url":"http://arxiv.org/abs/2405.11226v1","category":"cs.LG"}
{"created":"2024-05-20 17:49:11","title":"Modeling Magnetic Flux Emergence in Bipolar Active Regions","abstract":"Active regions (ARs) appear in the solar atmosphere as a consequence of the emergence of magnetic flux-ropes (FR). In this study, we use Bayesian methods to analyze line-of-sight magnetograms of emerging ARs. We employ a FR model consisting of a half-torus field structure based on eight parameters. The goal is to derive constrained physical parameters of the originating FR which are consistent with the observations. Specifically, we aim to obtain a precise estimation of the AR tilt angle and magnetic twist at different stages of the emergence process. To achieve this, we propose four temporal methods that correlate the field parameter evolutions with a single coherent FR. These methods differ from each other in the size of the explored parameter space. We test the methods on four bipolar ARs observed with the Michelson Doppler Imager on board the Solar and Heliospheric Observatory. We find that tilt angles are typically consistent between the temporal methods, improving previous estimations at all stages of the emergence. The twist sign derived from the temporal methods is consistent with previous estimations. The standard errors of all the methods used are similar, indicating that they model the observations equally well. These results indicate that the proposed methods can be used to obtain global magnetic parameters of ARs during their early evolution. The derived parameters contribute to a better understanding of the formation of FRs, and the role of ARs in the magnetic recycling process along the solar cycle.","sentences":["Active regions (ARs) appear in the solar atmosphere as a consequence of the emergence of magnetic flux-ropes (FR).","In this study, we use Bayesian methods to analyze line-of-sight magnetograms of emerging ARs.","We employ a FR model consisting of a half-torus field structure based on eight parameters.","The goal is to derive constrained physical parameters of the originating FR which are consistent with the observations.","Specifically, we aim to obtain a precise estimation of the AR tilt angle and magnetic twist at different stages of the emergence process.","To achieve this, we propose four temporal methods that correlate the field parameter evolutions with a single coherent FR.","These methods differ from each other in the size of the explored parameter space.","We test the methods on four bipolar ARs observed with the Michelson Doppler Imager on board the Solar and Heliospheric Observatory.","We find that tilt angles are typically consistent between the temporal methods, improving previous estimations at all stages of the emergence.","The twist sign derived from the temporal methods is consistent with previous estimations.","The standard errors of all the methods used are similar, indicating that they model the observations equally well.","These results indicate that the proposed methods can be used to obtain global magnetic parameters of ARs during their early evolution.","The derived parameters contribute to a better understanding of the formation of FRs, and the role of ARs in the magnetic recycling process along the solar cycle."],"url":"http://arxiv.org/abs/2405.12208v1","category":"astro-ph.SR"}
{"created":"2024-05-20 17:45:23","title":"Digitization and subduction of $SU(N)$ gauge theories","abstract":"The simulation of lattice gauge theories on quantum computers necessitates digitizing gauge fields. One approach involves substituting the continuous gauge group with a discrete subgroup, but the implications of this approximation still need to be clarified. To gain insights, we investigate the subduction of $ SU(2) $ and $ SU(3)$ to discrete crystal-like subgroups. Using classical lattice calculations, we show that subduction offers valuable information based on subduced direct sums, helping us identify additional terms to incorporate into the lattice action that can mitigate the effects of digitization. Furthermore, we compute the static potentials of all irreducible representations of $ \\Sigma(360 \\times 3) $ at a fixed lattice spacing. Our results reveal a percent-level agreement with the Casimir scaling of \\( SU(3) \\) for irreducible representations that subduce to a single $ \\Sigma(360 \\times 3) $ irreducible representation. This provides a diagnostic measure of approximation quality, as some irreducible representations closely match the expected results while others exhibit significant deviations.","sentences":["The simulation of lattice gauge theories on quantum computers necessitates digitizing gauge fields.","One approach involves substituting the continuous gauge group with a discrete subgroup, but the implications of this approximation still need to be clarified.","To gain insights, we investigate the subduction of $ SU(2) $ and $ SU(3)$ to discrete crystal-like subgroups.","Using classical lattice calculations, we show that subduction offers valuable information based on subduced direct sums, helping us identify additional terms to incorporate into the lattice action that can mitigate the effects of digitization.","Furthermore, we compute the static potentials of all irreducible representations of $ \\Sigma(360 \\times 3) $ at a fixed lattice spacing.","Our results reveal a percent-level agreement with the Casimir scaling of \\( SU(3) \\) for irreducible representations that subduce to a single $ \\Sigma(360 \\times 3) $ irreducible representation.","This provides a diagnostic measure of approximation quality, as some irreducible representations closely match the expected results while others exhibit significant deviations."],"url":"http://arxiv.org/abs/2405.12204v1","category":"hep-lat"}
{"created":"2024-05-20 17:13:46","title":"Decoherence of electron spin qubit during transfer between two semiconductor quantum dots at low magnetic fields","abstract":"Electron shuttling is one of the currently pursued avenues towards the scalability of semiconductor quantum dot-based spin qubits. We theoretically analyze the dephasing of a spin qubit adiabatically transferred between two tunnel-coupled quantum dots. We focus on the regime where the Zeeman splitting is lower than the tunnel coupling, at which interdot tunneling with spin flip is absent, and analyze the sources of errors in spin-coherent electron transfer for Si- and GaAs-based quantum dots. Apart from the obvious effect of fluctuations in spin splitting in each dot (e.g., due to nuclear Overhauser fields) leading to finite $ T_{2}^{*} $ of the stationary spin qubit, we consider effects activated by detuning sweeps aimed at adiabatic qubit transfer between the dots: failure of charge transfer caused by charge noise and phonons, spin relaxation due to enhancement of spin-orbit mixing of levels, and spin dephasing caused by low- and high-frequency noise coupling to the electron's charge in the presence of differences in Zeeman splittings between the two dots. Our results indicate that achieving coherent transfer of electron spin in a $10\\,\\mu$m long dot array necessitates a large and uniform tunnel coupling, with a typical value of $ 2t_c \\gtrsim 60 \\, \\mu$eV.","sentences":["Electron shuttling is one of the currently pursued avenues towards the scalability of semiconductor quantum dot-based spin qubits.","We theoretically analyze the dephasing of a spin qubit adiabatically transferred between two tunnel-coupled quantum dots.","We focus on the regime where the Zeeman splitting is lower than the tunnel coupling, at which interdot tunneling with spin flip is absent, and analyze the sources of errors in spin-coherent electron transfer for Si- and GaAs-based quantum dots.","Apart from the obvious effect of fluctuations in spin splitting in each dot (e.g., due to nuclear Overhauser fields) leading to finite $ T_{2}^{*} $ of the stationary spin qubit, we consider effects activated by detuning sweeps aimed at adiabatic qubit transfer between the dots: failure of charge transfer caused by charge noise and phonons, spin relaxation due to enhancement of spin-orbit mixing of levels, and spin dephasing caused by low- and high-frequency noise coupling to the electron's charge in the presence of differences in Zeeman splittings between the two dots.","Our results indicate that achieving coherent transfer of electron spin in a $10\\,\\mu$m long dot array necessitates a large and uniform tunnel coupling, with a typical value of $ 2t_c \\gtrsim 60 \\, \\mu$eV."],"url":"http://arxiv.org/abs/2405.12185v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 17:01:21","title":"Laboratory Demonstration of Image-Plane Self-Calibration in Interferometry","abstract":"We demonstrate the Shape-Orientation-Size conservation principle for a 3-element interferometer using aperture plane masking at the ALBA visible synchrotron radiation light source. We then use these data to demonstrate Image Plane Self-Calibration.","sentences":["We demonstrate the Shape-Orientation-Size conservation principle for a 3-element interferometer using aperture plane masking at the ALBA visible synchrotron radiation light source.","We then use these data to demonstrate Image Plane Self-Calibration."],"url":"http://arxiv.org/abs/2405.12176v1","category":"physics.optics"}
{"created":"2024-05-20 16:46:21","title":"Probing CPT invariance with top quarks at the LHC","abstract":"The first model-independent sensitivity to CPT violation in the top-quark sector is extracted from ATLAS and CMS measurements of the top and antitop kinematical mass difference. We find that the temporal component of a CPT-violating background field interacting with the top-quark vector current is restricted within the interval $[-0.13,0.29]$ GeV at 95% confidence level.","sentences":["The first model-independent sensitivity to CPT violation in the top-quark sector is extracted from ATLAS and CMS measurements of the top and antitop kinematical mass difference.","We find that the temporal component of a CPT-violating background field interacting with the top-quark vector current is restricted within the interval $","[-0.13,0.29]$ GeV at 95% confidence level."],"url":"http://arxiv.org/abs/2405.12162v1","category":"hep-ph"}
{"created":"2024-05-20 16:36:02","title":"Sobolev regularity of the inverse for minimizers of the neo-Hookean energy satisfying condition INV","abstract":"We study the existence and regularity of minimizers of the neo-Hookean energy in the closure of classes of deformations without cavitation. The exclusion of cavitation is imposed in the form of the divergence identities, which is equivalent to the well-known condition INV with $\\text{Det} = \\det$. We show that the neo-Hookean energy admits minimizers in classes of maps that are one-to-one a.e. with positive Jacobians, provided that these maps are the weak limits of sequences of maps that satisfy the divergence identities. In particular, these classes include the weak closure of diffeomorphisms and the weak closure of homeomorphisms satisfying Lusin's N condition. Moreover, if the minimizers satisfy condition INV, then their inverses have Sobolev regularity. This extends a recent result by Dole\\v{z}alov\\'{a}, Hencl, and Molchanova by showing that the minimizers they obtained enjoy extra regularity properties, and that the existence of minimizers can still be obtained even when their coercivity assumption is relaxed.","sentences":["We study the existence and regularity of minimizers of the neo-Hookean energy in the closure of classes of deformations without cavitation.","The exclusion of cavitation is imposed in the form of the divergence identities, which is equivalent to the well-known condition INV with $\\text{Det} = \\det$. We show that the neo-Hookean energy admits minimizers in classes of maps that are one-to-one a.e. with positive Jacobians, provided that these maps are the weak limits of sequences of maps that satisfy the divergence identities.","In particular, these classes include the weak closure of diffeomorphisms and the weak closure of homeomorphisms satisfying Lusin's N condition.","Moreover, if the minimizers satisfy condition INV, then their inverses have Sobolev regularity.","This extends a recent result by Dole\\v{z}alov\\'{a}, Hencl, and Molchanova by showing that the minimizers they obtained enjoy extra regularity properties, and that the existence of minimizers can still be obtained even when their coercivity assumption is relaxed."],"url":"http://arxiv.org/abs/2405.12156v1","category":"math.AP"}
{"created":"2024-05-20 16:23:52","title":"Beyond Earthly Limits: Protection against Cosmic Radiation through Biological Response Pathways","abstract":"The upcoming phase of space exploration not only includes trips to Mars and beyond, but also holds great promise for human progress. However, the vulnerability of space habitats to cosmic radiation, which consists of Galactic Cosmic Rays and Solar Particle Events, raises important safety concerns for astronauts and other living things that will accompany them. Research exploring the biological effects of cosmic radiation consists of experiments conducted in space itself and in simulated space environments on Earth. Notably, NASA's Space Radiation Laboratory has taken significant steps forward in simulating cosmic radiation by using particle accelerators, marking a notable advancement in this field. Intriguingly, much of the research emphasis thus far has been on understanding how cosmic radiation impacts living organisms, instead of finding ways to help them resist the radiation. In this paper, we briefly talk about current research on the biological effects of cosmic radiation and propose possible protective measures through biological interventions. In our opinion, biological pathways responsible for coping with stressors on Earth offer potential solutions for protection against the stress caused by cosmic radiation. Additionally, we recommend assessing the effectiveness of these pathways through experiments using particle accelerators to simulate the effects of cosmic radiation.","sentences":["The upcoming phase of space exploration not only includes trips to Mars and beyond, but also holds great promise for human progress.","However, the vulnerability of space habitats to cosmic radiation, which consists of Galactic Cosmic Rays and Solar Particle Events, raises important safety concerns for astronauts and other living things that will accompany them.","Research exploring the biological effects of cosmic radiation consists of experiments conducted in space itself and in simulated space environments on Earth.","Notably, NASA's Space Radiation Laboratory has taken significant steps forward in simulating cosmic radiation by using particle accelerators, marking a notable advancement in this field.","Intriguingly, much of the research emphasis thus far has been on understanding how cosmic radiation impacts living organisms, instead of finding ways to help them resist the radiation.","In this paper, we briefly talk about current research on the biological effects of cosmic radiation and propose possible protective measures through biological interventions.","In our opinion, biological pathways responsible for coping with stressors on Earth offer potential solutions for protection against the stress caused by cosmic radiation.","Additionally, we recommend assessing the effectiveness of these pathways through experiments using particle accelerators to simulate the effects of cosmic radiation."],"url":"http://arxiv.org/abs/2405.12151v1","category":"physics.bio-ph"}
{"created":"2024-05-20 16:02:55","title":"Resonant Neutrino Flavor Conversion in the Atmosphere","abstract":"Neutrinos produced in the atmosphere traverse a column density of air before being detected at neutrino observatories like IceCube or KM3NeT. In this work, we extend the neutrino flavor evolution in the {nuSQuIDS} code accounting for the varying height of neutrino production and the variable air density in the atmosphere. These effects can lead to sizeable spectral distortions in standard neutrino oscillations and are crucial to accurately describe some new physics scenarios. As an example, we study a model of quasi-sterile neutrinos that induce resonant flavor conversions at neutrino energies of ${O}(300)\\text{ MeV}$ in matter densities of $1 \\text{ g/cm}^3$. In atmospheric air densities, the same resonance is then realized at neutrino energies of ${O}(300- 700)$~GeV. We find that the new resonance can deplete the $\\nu_\\mu + \\overline{\\nu}_\\mu$ flux at the IceCube Neutrino Observatory by as much as $10\\%$ in the direction of the horizon.","sentences":["Neutrinos produced in the atmosphere traverse a column density of air before being detected at neutrino observatories like IceCube or KM3NeT. In this work, we extend the neutrino flavor evolution in the {nuSQuIDS} code accounting for the varying height of neutrino production and the variable air density in the atmosphere.","These effects can lead to sizeable spectral distortions in standard neutrino oscillations and are crucial to accurately describe some new physics scenarios.","As an example, we study a model of quasi-sterile neutrinos that induce resonant flavor conversions at neutrino energies of ${O}(300)\\text{ MeV}$ in matter densities of $1 \\text{ g/cm}^3$.","In atmospheric air densities, the same resonance is then realized at neutrino energies of ${O}(300- 700)$~GeV. We find that the new resonance can deplete the $\\nu_\\mu + \\overline{\\nu}_\\mu$ flux at the IceCube Neutrino Observatory by as much as $10\\%$ in the direction of the horizon."],"url":"http://arxiv.org/abs/2405.12140v1","category":"hep-ph"}
{"created":"2024-05-20 15:59:31","title":"Naming the SMD Bridge Program--A White Paper","abstract":"NASA's Science Mission Directorate (SMD) has initiated a program to enhance the participation of historically underrepresented institutions and communities in NASA's mission. Currently known as the NASA SMD Bridge Program, its goal is to establish enduring partnerships among these institutions, research-intensive universities, and NASA centers. There are concerns about using \"Bridge\" in the program's name, with stakeholders suggesting that it might stigmatize students and mislead applicants about its focus. In this white paper, we address these concerns and conclude that a name change that better reflects the mission of this SMD effort is necessary to address these concerns.","sentences":["NASA's Science Mission Directorate (SMD) has initiated a program to enhance the participation of historically underrepresented institutions and communities in NASA's mission.","Currently known as the NASA SMD Bridge Program, its goal is to establish enduring partnerships among these institutions, research-intensive universities, and NASA centers.","There are concerns about using \"Bridge\" in the program's name, with stakeholders suggesting that it might stigmatize students and mislead applicants about its focus.","In this white paper, we address these concerns and conclude that a name change that better reflects the mission of this SMD effort is necessary to address these concerns."],"url":"http://arxiv.org/abs/2405.12137v1","category":"astro-ph.IM"}
{"created":"2024-05-20 15:54:06","title":"Absolute reference for microwave polarization experiments -- The COSMOCal project and its proof of concept","abstract":"The cosmic microwave background (CMB), a remnant of the Big Bang, provides unparalleled insights into the primordial universe, its energy content, and the origin of cosmic structures. The success of forthcoming terrestrial and space experiments hinges on meticulously calibrated data. Specifically, the ability to achieve an absolute calibration of the polarization angles with a precision of < 0.1 deg is crucial to identify the signatures of primordial gravitational waves and cosmic birefringence within the CMB polarization. We introduce the COSMOCal project, designed to deploy a polarized source in space for calibrating microwave frequency observations. The project aims to integrate microwave polarization observations from small and large telescopes, ground-based and in space, into a unified scale, enhancing the effectiveness of each observatory and allowing robust combination of data. To demonstrate the feasibility and confirm the observational approach of our project, we developed a prototype instrument that operates in the atmospheric window centered at 260 GHz, specifically tailored for use with the NIKA2 camera at the IRAM 30 m telescope. We present the instrument components and their laboratory characterization. The results of tests performed with the fully assembled prototype using a KIDs-based instrument, similar concept of NIKA2, are also reported. This study paves the way for an observing campaign using the IRAM 30m telescope and contributes to the development of a space-based instrument.","sentences":["The cosmic microwave background (CMB), a remnant of the Big Bang, provides unparalleled insights into the primordial universe, its energy content, and the origin of cosmic structures.","The success of forthcoming terrestrial and space experiments hinges on meticulously calibrated data.","Specifically, the ability to achieve an absolute calibration of the polarization angles with a precision of < 0.1 deg is crucial to identify the signatures of primordial gravitational waves and cosmic birefringence within the CMB polarization.","We introduce the COSMOCal project, designed to deploy a polarized source in space for calibrating microwave frequency observations.","The project aims to integrate microwave polarization observations from small and large telescopes, ground-based and in space, into a unified scale, enhancing the effectiveness of each observatory and allowing robust combination of data.","To demonstrate the feasibility and confirm the observational approach of our project, we developed a prototype instrument that operates in the atmospheric window centered at 260 GHz, specifically tailored for use with the NIKA2 camera at the IRAM 30 m telescope.","We present the instrument components and their laboratory characterization.","The results of tests performed with the fully assembled prototype using a KIDs-based instrument, similar concept of NIKA2, are also reported.","This study paves the way for an observing campaign using the IRAM 30m telescope and contributes to the development of a space-based instrument."],"url":"http://arxiv.org/abs/2405.12135v1","category":"astro-ph.CO"}
{"created":"2024-05-20 15:07:34","title":"Formation of C1 oxygenates by Activation of Methane on B, N Co-doped Graphene Surface Decorated by Oxygen Pre-covered Ir13 Cluster: A First Principles Study","abstract":"We employ density functional theory (DFT) to investigate the adsorption and dehydrogenation of methane on the BNG-Ir13 cluster at both low and high oxygen coverage. The DFT calculations show that the low-oxygen-coverage BNG-Ir13 cluster (BNG-Ir13O cluster) forms methanol and formaldehyde with a lower activation energy barrier compared to the high-oxygen-coverage BNG Ir13 cluster. Furthermore, the results reveal that the BNG-Ir13 cluster with low oxygen coverage has a higher methane adsorption energy and a lower activation energy barrier for methane dissociation compared to the high-oxygen-coverage BNG-Ir13 cluster. Quantitatively, the methane adsorption energy on the low-oxygen-coverage BNG-Ir13 cluster is -0.44 eV, and the second dehydrogenation of methane is the rate-determining step with an energy barrier of 1.24 eV, in both cases lower numbers than those observed for the high-oxygen-coverage BNG-Ir13 cluster.","sentences":["We employ density functional theory (DFT) to investigate the adsorption and dehydrogenation of methane on the BNG-Ir13 cluster at both low and high oxygen coverage.","The DFT calculations show that the low-oxygen-coverage BNG-Ir13 cluster (BNG-Ir13O cluster) forms methanol and formaldehyde with a lower activation energy barrier compared to the high-oxygen-coverage BNG Ir13 cluster.","Furthermore, the results reveal that the BNG-Ir13 cluster with low oxygen coverage has a higher methane adsorption energy and a lower activation energy barrier for methane dissociation compared to the high-oxygen-coverage BNG-Ir13 cluster.","Quantitatively, the methane adsorption energy on the low-oxygen-coverage BNG-Ir13 cluster is -0.44 eV, and the second dehydrogenation of methane is the rate-determining step with an energy barrier of 1.24 eV, in both cases lower numbers than those observed for the high-oxygen-coverage BNG-Ir13 cluster."],"url":"http://arxiv.org/abs/2405.12097v1","category":"physics.app-ph"}
{"created":"2024-05-20 14:51:55","title":"Universal quantum Fisher information and simultaneous occurrence of Landau-class and topological-class transitions in non-Hermitian Jaynes-Cummings models","abstract":"Light-matter interactions provide an ideal testground for interplay of critical phenomena, topological transitions, quantum metrology and non-Hermitian physics. We consider two fundamental non-Hermitian Jaynes-Cummings models which possess real energy spectra in parity-time (PT) symmetry and anti-PT symmetry. We show that the quantum Fisher information is critical around the transitions at the exceptional points and exhibits a super universality with respect to different parameters, all energy levels, both models, symmetric phases and symmetry-broken phases. The transitions are found to be both symmetry-breaking Landau-class transitions (LCTs) and symmetry-protected topological-class of transitions (TCTs), thus realizing a simultaneous occurrence of critical LCTs and TCTs which are conventionally incompatible due to contrary symmetry requirements.","sentences":["Light-matter interactions provide an ideal testground for interplay of critical phenomena, topological transitions, quantum metrology and non-Hermitian physics.","We consider two fundamental non-Hermitian Jaynes-Cummings models which possess real energy spectra in parity-time (PT) symmetry and anti-PT symmetry.","We show that the quantum Fisher information is critical around the transitions at the exceptional points and exhibits a super universality with respect to different parameters, all energy levels, both models, symmetric phases and symmetry-broken phases.","The transitions are found to be both symmetry-breaking Landau-class transitions (LCTs) and symmetry-protected topological-class of transitions (TCTs), thus realizing a simultaneous occurrence of critical LCTs and TCTs which are conventionally incompatible due to contrary symmetry requirements."],"url":"http://arxiv.org/abs/2405.12080v1","category":"quant-ph"}
{"created":"2024-05-20 14:36:52","title":"Investigating stellar activity through eight years of Sun-as-a-star observations","abstract":"Stellar magnetic activity induces both distortions and Doppler-shifts in the absorption line profiles of Sun-like stars. Those effects produce apparent radial velocity (RV) signals which greatly hamper the search for potentially habitable, Earth-like planets. In this work, we investigate these distortions in the Sun using cross-correlation functions (CCFs), derived from intensive monitoring with the high-precision spectrograph HARPS-N. We show that the RV signal arising from line-shape variations on time-scales associated with the solar rotation and activity cycle can be robustly extracted from the data, reducing the RV dispersion by half. Once these have been corrected, activity-induced Doppler-shifts remain, that are modulated at the solar rotation period, and that are most effectively modelled in the time domain, using Gaussian Processes (GPs). Planet signatures are still best retrieved with multi-dimensonal GPs, when activity is jointly modelled from the raw RVs and indicators of the line width or of the Ca II H and K emission. After GP modelling, the residual RVs exhibit a dispersion of 0.6-0.8 m/s, likely to be dominated by signals induced by super-granulation. Finally, we find that the statistical properties of the RVs evolve significantly over time, and that this evolution is primarily driven by sunspots, which control the smoothness of the signal. Such evolution, which reduces the sensitivity to long-period planet signatures, is no longer seen in the activity-induced Doppler-shifts, which is promising for long term RV monitoring surveys such as the Terra Hunting Experiment or the PLATO follow-up campaign.","sentences":["Stellar magnetic activity induces both distortions and Doppler-shifts in the absorption line profiles of Sun-like stars.","Those effects produce apparent radial velocity (RV) signals which greatly hamper the search for potentially habitable, Earth-like planets.","In this work, we investigate these distortions in the Sun using cross-correlation functions (CCFs), derived from intensive monitoring with the high-precision spectrograph HARPS-N. We show that the RV signal arising from line-shape variations on time-scales associated with the solar rotation and activity cycle can be robustly extracted from the data, reducing the RV dispersion by half.","Once these have been corrected, activity-induced Doppler-shifts remain, that are modulated at the solar rotation period, and that are most effectively modelled in the time domain, using Gaussian Processes (GPs).","Planet signatures are still best retrieved with multi-dimensonal GPs, when activity is jointly modelled from the raw RVs and indicators of the line width or of the Ca II H and K emission.","After GP modelling, the residual RVs exhibit a dispersion of 0.6-0.8 m/s, likely to be dominated by signals induced by super-granulation.","Finally, we find that the statistical properties of the RVs evolve significantly over time, and that this evolution is primarily driven by sunspots, which control the smoothness of the signal.","Such evolution, which reduces the sensitivity to long-period planet signatures, is no longer seen in the activity-induced Doppler-shifts, which is promising for long term RV monitoring surveys such as the Terra Hunting Experiment or the PLATO follow-up campaign."],"url":"http://arxiv.org/abs/2405.12065v1","category":"astro-ph.EP"}
{"created":"2024-05-20 14:31:59","title":"The Dynamics of Particle-Particle Correlations and the Ridge Effect in Proton-Proton Collisions","abstract":"In high-energy particle physics, the study of particle-particle correlations in proton-proton and heavy-ion collisions constitutes a pivotal frontier in the effort to understand the fundamental dynamics of the strong force. To the best of our knowledge, we employ for the first time the BFKL dynamics implemented in a Monte Carlo code in momentum space to compute final state correlations in proton-proton collisions. Our present work aims to investigate whether the particular dynamics of the high-energy limit of QCD can contribute to the long-range rapidity correlations and the enigmatic ridge effect in proton-proton collisions.","sentences":["In high-energy particle physics, the study of particle-particle correlations in proton-proton and heavy-ion collisions constitutes a pivotal frontier in the effort to understand the fundamental dynamics of the strong force.","To the best of our knowledge, we employ for the first time the BFKL dynamics implemented in a Monte Carlo code in momentum space to compute final state correlations in proton-proton collisions.","Our present work aims to investigate whether the particular dynamics of the high-energy limit of QCD can contribute to the long-range rapidity correlations and the enigmatic ridge effect in proton-proton collisions."],"url":"http://arxiv.org/abs/2405.12062v1","category":"hep-ph"}
{"created":"2024-05-20 14:29:39","title":"Alkaline earth metal mediated inter-molecular magnetism in perfluorocubane dimers and chains","abstract":"Perfluorocubane ($C_8F_8$) was successfully synthesized and found to accept and store electrons in its internal cubic cavity to form magnetic moments. However their inter-molecule spin-exchange coupling mechanism is yet to be revealed. In this study, we found the inter-molecule magnetic groundstates of $C_8F_8$ dimer and one-dimensional (1D) chain are tunable from antiferromagnetic (AFM) to ferromagnetic (FM) by stacking orders and alkaline earth metals intercalation using first-principle calculations. The inter-molecule couplings are dominated by noncovalent halogen $C-F...C_4$ interactions. Stacking orders of dimers can regulate the relative position of the lone pairs and $\\sigma-holes$ at the molecular interface and thus the magnetic groundstates. Alkaline earth metals M (M = Na, Mg) intercalations could form $C_4-M-C_4$ bonds and lead to FM direct exchange at the inter-molecule region. An unpaired electron donated by the intercalated atoms or electron doping can result in a local magnetic moment in dimers, exhibiting an on-off switching by the odd-even number of electron filling. Novel electronic properties such as spin gapless semiconductor and charge density wave (CDW) states emerge when $C_8F_8$ molecules self-assemble with intercalated atoms to form 1D chains. These findings manifest the roles of stacking and intercalation in modifying intermolecular magnetism and the revealed halogen bond-dominated exchange mechanisms are paramount additions to those previously established non-covalent couplings.","sentences":["Perfluorocubane ($C_8F_8$) was successfully synthesized and found to accept and store electrons in its internal cubic cavity to form magnetic moments.","However their inter-molecule spin-exchange coupling mechanism is yet to be revealed.","In this study, we found the inter-molecule magnetic groundstates of $C_8F_8$ dimer and one-dimensional (1D) chain are tunable from antiferromagnetic (AFM) to ferromagnetic (FM) by stacking orders and alkaline earth metals intercalation using first-principle calculations.","The inter-molecule couplings are dominated by noncovalent halogen $C-F...C_4$ interactions.","Stacking orders of dimers can regulate the relative position of the lone pairs and $\\sigma-holes$ at the molecular interface and thus the magnetic groundstates.","Alkaline earth metals M (M = Na, Mg) intercalations could form $C_4-M-C_4$ bonds and lead to FM direct exchange at the inter-molecule region.","An unpaired electron donated by the intercalated atoms or electron doping can result in a local magnetic moment in dimers, exhibiting an on-off switching by the odd-even number of electron filling.","Novel electronic properties such as spin gapless semiconductor and charge density wave (CDW) states emerge when $C_8F_8$ molecules self-assemble with intercalated atoms to form 1D chains.","These findings manifest the roles of stacking and intercalation in modifying intermolecular magnetism and the revealed halogen bond-dominated exchange mechanisms are paramount additions to those previously established non-covalent couplings."],"url":"http://arxiv.org/abs/2405.12060v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 14:24:47","title":"Rare Beauty and Charm Decays","abstract":"There are two main ways of looking for new physics, direct searches and precision measurements. The latter are sensitive to a broader spectrum of models; they also can be sensitive to higher energy scales than what can be reached through direct searches. We will provide an update of several precision measurements carried out by the LHCb collaboration. In particular, we will show the measurement of $R_{\\phi\\pi}^{(s)}$, the search for $B_c^+ \\to \\pi^+\\mu^+\\mu^-$ and $B_s\\to\\mu^+\\mu^-\\gamma$ and an amplitude analysis of the $\\Lambda_b^0\\to pK^-\\gamma$ decay.","sentences":["There are two main ways of looking for new physics, direct searches and precision measurements.","The latter are sensitive to a broader spectrum of models; they also can be sensitive to higher energy scales than what can be reached through direct searches.","We will provide an update of several precision measurements carried out by the LHCb collaboration.","In particular, we will show the measurement of $R_{\\phi\\pi}^{(s)}$, the search for $B_c^+ \\to \\pi^+\\mu^+\\mu^-$ and $B_s\\to\\mu^+\\mu^-\\gamma$ and an amplitude analysis of the $\\Lambda_b^0\\to pK^-\\gamma$ decay."],"url":"http://arxiv.org/abs/2405.12056v1","category":"hep-ex"}
{"created":"2024-05-20 14:14:10","title":"Non-Formality of $S^2$ via the free loop space","abstract":"We show that the $E_1$-equivalence $C^\\bullet(S^2) \\simeq H^\\bullet(S^2)$ does not intertwine the inclusion of constant loops into the free loop space $S^2 \\to LS^2$. That is, the isomorphism $HH_\\bullet(H^\\bullet(S^2)) \\cong H^\\bullet(LS^2)$ does not preserve the obvious maps to $H^\\bullet(S^2)$ that exist on both sides. We give an explicit computation of the defect in terms of the $E_\\infty$-structure on $C^\\bullet(S^2)$. Finally, we relate our calculation to recent work of Poirier-Tradler on the string topology of $S^2$.","sentences":["We show that the $E_1$-equivalence $C^\\bullet(S^2) \\simeq H^\\bullet(S^2)$ does not intertwine the inclusion of constant loops into the free loop space $S^2 \\to LS^2$.","That is, the isomorphism $HH_\\bullet(H^\\bullet(S^2))","\\cong H^\\bullet(LS^2)$ does not preserve the obvious maps to $H^\\bullet(S^2)$ that exist on both sides.","We give an explicit computation of the defect in terms of the $E_\\infty$-structure on $C^\\bullet(S^2)$. Finally, we relate our calculation to recent work of Poirier-Tradler on the string topology of $S^2$."],"url":"http://arxiv.org/abs/2405.12047v1","category":"math.AT"}
{"created":"2024-05-20 13:39:38","title":"Global Polarization of (Anti-)Hypertriton in Heavy-Ion Collisions","abstract":"Particles of non-zero spin produced in non-central heavy-ion collisions are expected to be polarized along the direction perpendicular to the reaction plane because of their spin-orbit interactions in the produced matter, and this has indeed been observed for many hyperons and vector mesons. Here, we show that the hypertriton ($^3_\\Lambda\\text{H}$), which is the lightest hypernucleus, is also polarized in these collisions. Using the coalescence model based on the kinetic freezeout baryons for light (hyper-)nuclei production, we find that the angular distribution of the decay product of polarized $^3_\\Lambda\\text{H}$ is highly sensitive to the spin configuration of its wavefunction, providing a novel way to determine its spin structure. We also predict the beam energy dependence of $^3_\\Lambda\\text{H}$ and ${^3_{\\bar{\\Lambda}}}\\overline{\\rm H}$ polarizations in heavy-ion collisions from a few GeV to several TeV based on the measured $\\Lambda$ and $\\bar{\\Lambda}$ polarizations. We further discuss the possibility of studying the spin correlations among nucleons and $\\Lambda$ hyperons in the produced hadronic matter from the measured $^3_\\Lambda\\text{H}$ polarization in non-central heavy-ion collisions.","sentences":["Particles of non-zero spin produced in non-central heavy-ion collisions are expected to be polarized along the direction perpendicular to the reaction plane because of their spin-orbit interactions in the produced matter, and this has indeed been observed for many hyperons and vector mesons.","Here, we show that the hypertriton ($^3_\\Lambda\\text{H}$), which is the lightest hypernucleus, is also polarized in these collisions.","Using the coalescence model based on the kinetic freezeout baryons for light (hyper-)nuclei production, we find that the angular distribution of the decay product of polarized $^3_\\Lambda\\text{H}$ is highly sensitive to the spin configuration of its wavefunction, providing a novel way to determine its spin structure.","We also predict the beam energy dependence of $^3_\\Lambda\\text{H}$ and ${^3_{\\bar{\\Lambda}}}\\overline{\\rm H}$ polarizations in heavy-ion collisions from a few GeV to several TeV based on the measured $\\Lambda$ and $\\bar{\\Lambda}$ polarizations.","We further discuss the possibility of studying the spin correlations among nucleons and $\\Lambda$ hyperons in the produced hadronic matter from the measured $^3_\\Lambda\\text{H}$ polarization in non-central heavy-ion collisions."],"url":"http://arxiv.org/abs/2405.12015v1","category":"nucl-th"}
{"created":"2024-05-20 13:15:05","title":"Optical Variability of Blazars in the Tomo-e Gozen Northern Sky Transient Survey","abstract":"We studied the optical variability of 241 BL Lacs and 83 flat-spectrum radio quasars (FSRQ) from the 4LAC catalog using data from the Tomo-e Gozen Northern Sky Transient Survey, with $\\sim$ 50 epochs per blazar on average. We excluded blazars whose optical variability may be underestimated due to the influence of their host galaxy, based on their optical luminosity ($L_O$). FSRQs with $\\gamma$-ray photon index greater than 2.6 exhibit very low optical variability, and their distribution of standard deviation of repeated photometry is significantly different from that of the other FSRQs (KS test P value equal to $5 \\times 10^{-6}$ ). Among a sample of blazars at any particular cosmological epoch, those with lower $\\gamma$-ray luminosity ($L_\\gamma$) tend to have lower optical variability, and those FSRQs with $\\gamma$-ray photon index greater than 2.6 tend to have low $L_\\gamma$. We also measured the structure function of optical variability and found that the amplitude of the structure function for FSRQs is higher than previously measured and higher than that of BL Lacs at multiple time lags. Additionally, the amplitude of the structure function of FSRQs with high $\\gamma$-ray photon index is significantly lower than that of FSRQs with low $\\gamma$-ray photon index. The structure function of FSRQs of high $\\gamma$-ray photon index shows a characteristic timescale of more than 10 days, which may be the variability timescale of the accretion disk. In summary, we infer that the optical component of FSRQs with high $\\gamma$-ray photon index may be dominated by the accretion disk.","sentences":["We studied the optical variability of 241 BL Lacs and 83 flat-spectrum radio quasars (FSRQ) from the 4LAC catalog using data from the Tomo-e Gozen Northern Sky Transient Survey, with $\\sim$ 50 epochs per blazar on average.","We excluded blazars whose optical variability may be underestimated due to the influence of their host galaxy, based on their optical luminosity ($L_O$).","FSRQs with $\\gamma$-ray photon index greater than 2.6 exhibit very low optical variability, and their distribution of standard deviation of repeated photometry is significantly different from that of the other FSRQs (KS test P value equal to $5 \\times 10^{-6}$ ).","Among a sample of blazars at any particular cosmological epoch, those with lower $\\gamma$-ray luminosity ($L_\\gamma$) tend to have lower optical variability, and those FSRQs with $\\gamma$-ray photon index greater than 2.6 tend to have low $L_\\gamma$. We also measured the structure function of optical variability and found that the amplitude of the structure function for FSRQs is higher than previously measured and higher than that of BL Lacs at multiple time lags.","Additionally, the amplitude of the structure function of FSRQs with high $\\gamma$-ray photon index is significantly lower than that of FSRQs with low $\\gamma$-ray photon index.","The structure function of FSRQs of high $\\gamma$-ray photon index shows a characteristic timescale of more than 10 days, which may be the variability timescale of the accretion disk.","In summary, we infer that the optical component of FSRQs with high $\\gamma$-ray photon index may be dominated by the accretion disk."],"url":"http://arxiv.org/abs/2405.12002v1","category":"astro-ph.HE"}
{"created":"2024-05-20 10:13:06","title":"Wilson line-based action for gluodynamics at the quantum level","abstract":"We recently derived a new action for gluodynamics by canonically transforming the Yang-Mills action on light-cone. The transformation elimated triple gluons vertices and replaced the gauge fields with Wilson lines. This greatly reduced the number of diagrams required to compute tree level amplitudes. However, at the quantum level, the action turned out to be incomplete. We present two ways, based on one-loop effective action approach, to systematically develop quantum correction to our action. The first method retains Yang-Mills vertices in the loop, while the second method explicitly incorporates the interaction vertices of our action into the loop. We demonstrate that both approaches are equivalent, although the former appears to be more efficient for computing higher-multiplicity one-loop amplitudes.","sentences":["We recently derived a new action for gluodynamics by canonically transforming the Yang-Mills action on light-cone.","The transformation elimated triple gluons vertices and replaced the gauge fields with Wilson lines.","This greatly reduced the number of diagrams required to compute tree level amplitudes.","However, at the quantum level, the action turned out to be incomplete.","We present two ways, based on one-loop effective action approach, to systematically develop quantum correction to our action.","The first method retains Yang-Mills vertices in the loop, while the second method explicitly incorporates the interaction vertices of our action into the loop.","We demonstrate that both approaches are equivalent, although the former appears to be more efficient for computing higher-multiplicity one-loop amplitudes."],"url":"http://arxiv.org/abs/2405.11931v1","category":"hep-th"}
{"created":"2024-05-20 10:03:41","title":"Energy Window Augmented Plane Waves (EWAPW)","abstract":"In this work we present a new basis set for electronic structure calculations of crystalline solids using Density Functional theory (DFT) methods. In this construction we take advantage of the fact that most DFT calculations use a convergence loop in order to obtain the eigenstates of a final Khon Sham (KS) Hamiltonian matrix whose eigenstates also give the appropriate electron density needed to obtain the KS potential needed for that KS Hamiltonian matrix. Here we propose that for the basis of each step of the iteration we use the previous eigenstate basis in the interstitial region but augmented inside the MT sphere with the solution to the spherically averaged KS Hamiltonian for the energy window of that eigenstate. To reduce the number of times the KS potential needs to be solved inside the MT spheres it is advantageous to use energy windows and solve the KS Hamiltonian inside the MT region only once per window (at some energy inside the window) so that the KS Hamiltonian needs only be solved a small number of times per iteration, for practical applications on the order of 10 to 100 windows. This method combines the energy dependence of methods such as Projected Augmented Wave functions (PAW) with the ability of the basis set to adjust to the solid state (rather then atomic) environment of basis sets such as Linearized Augmented Plane Waves (LAPW).","sentences":["In this work we present a new basis set for electronic structure calculations of crystalline solids using Density Functional theory (DFT) methods.","In this construction we take advantage of the fact that most DFT calculations use a convergence loop in order to obtain the eigenstates of a final Khon Sham (KS) Hamiltonian matrix whose eigenstates also give the appropriate electron density needed to obtain the KS potential needed for that KS Hamiltonian matrix.","Here we propose that for the basis of each step of the iteration we use the previous eigenstate basis in the interstitial region but augmented inside the MT sphere with the solution to the spherically averaged KS Hamiltonian for the energy window of that eigenstate.","To reduce the number of times the KS potential needs to be solved inside the MT spheres it is advantageous to use energy windows and solve the KS Hamiltonian inside the MT region only once per window (at some energy inside the window) so that the KS Hamiltonian needs only be solved a small number of times per iteration, for practical applications on the order of 10 to 100 windows.","This method combines the energy dependence of methods such as Projected Augmented Wave functions (PAW) with the ability of the basis set to adjust to the solid state (rather then atomic) environment of basis sets such as Linearized Augmented Plane Waves (LAPW)."],"url":"http://arxiv.org/abs/2405.11926v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 08:47:01","title":"Probing $\u03c7_{cJ}(J=0,1,2)$ decay into baryon and anti-baryon with SU(3) flavor analysis","abstract":"With the accurate measurements of $\\chi _{cJ}(J=0,1,2)$ charmonium decays, we explore $\\chi _{cJ}\\to \\mathcal{B}_{8}\\bar{\\mathcal{B}}_{8}$ and $\\mathcal{B}_{10}\\bar{\\mathcal{B}}_{10}$ decays based on the SU(3) flavor symmetry model, where $\\mathcal{B}_{8}$ and $\\mathcal{B}_{10}$ are light octet and decuplet baryons, respectively. The decay amplitude relations are determined by an effective interaction Hamiltonian. Then using experimental data and the amplitude relations of $\\chi _{cJ}\\to \\mathcal{B}_{8}\\bar{\\mathcal{B}}_{8}$, $\\mathcal{B}_{10}\\bar{\\mathcal{B}}_{10}$ decays, relative nonperturbative coefficients are constrained, and the branching ratios of unmeasured $\\chi _{cJ}\\to \\mathcal{B}_{8}\\bar{\\mathcal{B}}_{8},\\mathcal{B}_{10}\\bar{\\mathcal{B}}_{10}$, for examples, $\\chi _{cJ}\\to n\\bar{n}$ and $\\chi _{cJ}\\to\\Lambda \\bar{\\Sigma}^{0}+\\bar{\\Lambda} \\Sigma^{0}$ channels, are predicted. Moreover, we discuss the case of adding a mixing angle between $\\Lambda $ and $\\Sigma ^{0}$, which is determined by the quark mass differences. Our results provide valuable insights that will aid in advancing our understanding of the mechanisms and characteristics of $\\chi _{cJ}$ decays.","sentences":["With the accurate measurements of $\\chi _{cJ}(J=0,1,2)$ charmonium decays, we explore $\\chi _{cJ}\\to \\mathcal{B}_{8}\\bar{\\mathcal{B}}_{8}$ and $\\mathcal{B}_{10}\\bar{\\mathcal{B}}_{10}$ decays based on the SU(3) flavor symmetry model, where $\\mathcal{B}_{8}$ and $\\mathcal{B}_{10}$ are light octet and decuplet baryons, respectively.","The decay amplitude relations are determined by an effective interaction Hamiltonian.","Then using experimental data and the amplitude relations of $\\chi _{cJ}\\to \\mathcal{B}_{8}\\bar{\\mathcal{B}}_{8}$, $\\mathcal{B}_{10}\\bar{\\mathcal{B}}_{10}$ decays, relative nonperturbative coefficients are constrained, and the branching ratios of unmeasured $\\chi _{cJ}\\to \\mathcal{B}_{8}\\bar{\\mathcal{B}}_{8},\\mathcal{B}_{10}\\bar{\\mathcal{B}}_{10}$, for examples, $\\chi _{cJ}\\to n\\bar{n}$ and $\\chi _{cJ}\\to\\Lambda \\bar{\\Sigma}^{0}+\\bar{\\Lambda} \\Sigma^{0}$ channels, are predicted.","Moreover, we discuss the case of adding a mixing angle between $\\Lambda $ and $\\Sigma ^{0}$, which is determined by the quark mass differences.","Our results provide valuable insights that will aid in advancing our understanding of the mechanisms and characteristics of $\\chi _{cJ}$ decays."],"url":"http://arxiv.org/abs/2405.11879v1","category":"hep-ph"}
{"created":"2024-05-20 08:24:41","title":"Qualitative properties of solutions to nonlocal infectious SIR epidemic models","abstract":"This paper studies qualitative properties of solutions of nonlocal infectious SIR epidemic models (1.3)-(1.5), with the homogeneous Neumann boundary conditions, Dirichlet boundary conditions and free boundary, respectively. We first use the upper and lower solutions method and the Lyapunov function method to prove the global asymptotically stabilities of the disease-free equilibrium and the unique positive equilibrium of (1.3). Then we use the theory of topological degree in cones to study the positive equilibrium solutions of (1.4), including the necessary and sufficient conditions for the existence, and the uniqueness in some special case. At last, for the free boundary problem (1.5), we study the longtime behaviors of solutions and criteria for spreading and vanishing. The highlights are to overcome failures of the Lyapunov functional method and comparison principle, and difficulties in the maximum principle and Hopf boundary lemma of boundary value problems caused by nonlocal terms.","sentences":["This paper studies qualitative properties of solutions of nonlocal infectious SIR epidemic models (1.3)-(1.5), with the homogeneous Neumann boundary conditions, Dirichlet boundary conditions and free boundary, respectively.","We first use the upper and lower solutions method and the Lyapunov function method to prove the global asymptotically stabilities of the disease-free equilibrium and the unique positive equilibrium of (1.3).","Then we use the theory of topological degree in cones to study the positive equilibrium solutions of (1.4), including the necessary and sufficient conditions for the existence, and the uniqueness in some special case.","At last, for the free boundary problem (1.5), we study the longtime behaviors of solutions and criteria for spreading and vanishing.","The highlights are to overcome failures of the Lyapunov functional method and comparison principle, and difficulties in the maximum principle and Hopf boundary lemma of boundary value problems caused by nonlocal terms."],"url":"http://arxiv.org/abs/2405.11871v1","category":"math.AP"}
{"created":"2024-05-20 08:21:35","title":"DESI constraints on varying electron mass model and axion-like early dark energy","abstract":"Baryon acoustic oscillation (BAO) is one of the important standard ruler in cosmology. The results of the latest BAO measurements by Dark Energy Spectroscopic Instrument (DESI) survey has been reported. Cosmology with the varying electron mass model and the early dark energy (EDE) models are regraded as interesting models to resolve the Hubble tension. We present constraints on the varying electron mass model and EDE models by including new DESI data as well as cosmic microwave background by Planck and the conventional BAO data from 6dF, MGS, and DR12 and supernovae light curve data into analysis. Since new DESI BAO data indicates slightly longer sound horizon $r_dh$ than the other BAO observations, for the varying electron mass model, the larger $H_0 =69.44\\pm 0.84 $ km$/$s$/$Mpc is indicated.","sentences":["Baryon acoustic oscillation (BAO) is one of the important standard ruler in cosmology.","The results of the latest BAO measurements by Dark Energy Spectroscopic Instrument (DESI) survey has been reported.","Cosmology with the varying electron mass model and the early dark energy (EDE) models are regraded as interesting models to resolve the Hubble tension.","We present constraints on the varying electron mass model and EDE models by including new DESI data as well as cosmic microwave background by Planck and the conventional BAO data from 6dF, MGS, and DR12 and supernovae light curve data into analysis.","Since new DESI BAO data indicates slightly longer sound horizon $r_dh$ than the other BAO observations, for the varying electron mass model, the larger $H_0 =69.44\\pm 0.84 $ km$/$s$/$Mpc is indicated."],"url":"http://arxiv.org/abs/2405.11869v1","category":"astro-ph.CO"}
{"created":"2024-05-20 04:35:23","title":"AGN energetics and lifetimes from remnant radio galaxies","abstract":"The energy coupling efficiency of active galactic nucleus (AGN) outbursts is known to differ significantly with factors including the jet kinetic power, duration of the outburst, and properties of the host galaxy cluster. As such, constraints on their jet power and lifetime functions are crucial to quantify the role of kinetic-mode AGN feedback on the evolution of galaxies since $z \\sim 1$. In this work, we address this issue by measuring the energetics of a sample of 79 low-redshift (0.02 $< z <$ 0.2) remnant radio galaxies compiled from large-sky radio surveys - these objects uniquely probe the full duration of an AGN outburst. The jet kinetic power and outburst duration of each remnant are determined using the RAiSE dynamical model based on the surface brightness distribution observed in multi-frequency radio images. We compare the energetics constrained for this sample to those predicted for mock radio source populations - with various intrinsic functions for jet power and lifetime distributions - to correct for sample selection biases imposed on our sample. The intrinsic jet power and lifetime functions that yield a selection-biased mock population most similar to our observed sample are found using Bayesian inference. Our analysis places robust constraints on assumed power-law indices for the intrinsic jet power and lifetime functions: $p(Q)\\propto Q^{-1.49\\pm0.07}$ and $p(t_{\\mathrm{on}})\\propto t_{\\mathrm{on}}^{-0.97\\pm0.12}$ respectively. We discuss the implication of these findings for feedback-regulated accretion and the self-regulating nature of jet activity. The methodology proposed in this work can be extended to active radio galaxies in future studies.","sentences":["The energy coupling efficiency of active galactic nucleus (AGN) outbursts is known to differ significantly with factors including the jet kinetic power, duration of the outburst, and properties of the host galaxy cluster.","As such, constraints on their jet power and lifetime functions are crucial to quantify the role of kinetic-mode AGN feedback on the evolution of galaxies since $z \\sim 1$.","In this work, we address this issue by measuring the energetics of a sample of 79 low-redshift (0.02 $< z <$ 0.2) remnant radio galaxies compiled from large-sky radio surveys - these objects uniquely probe the full duration of an AGN outburst.","The jet kinetic power and outburst duration of each remnant are determined using the RAiSE dynamical model based on the surface brightness distribution observed in multi-frequency radio images.","We compare the energetics constrained for this sample to those predicted for mock radio source populations - with various intrinsic functions for jet power and lifetime distributions - to correct for sample selection biases imposed on our sample.","The intrinsic jet power and lifetime functions that yield a selection-biased mock population most similar to our observed sample are found using Bayesian inference.","Our analysis places robust constraints on assumed power-law indices for the intrinsic jet power and lifetime functions: $p(Q)\\propto Q^{-1.49\\pm0.07}$ and $p(t_{\\mathrm{on}})\\propto t_{\\mathrm{on}}^{-0.97\\pm0.12}$ respectively.","We discuss the implication of these findings for feedback-regulated accretion and the self-regulating nature of jet activity.","The methodology proposed in this work can be extended to active radio galaxies in future studies."],"url":"http://arxiv.org/abs/2405.11777v1","category":"astro-ph.GA"}
{"created":"2024-05-20 03:42:23","title":"Strongly coupled magneto-exciton condensates in large-angle twisted double bilayer graphene","abstract":"Excitons, the bosonic quasiparticle emerging from Coulomb interaction between electrons and holes, will undergo a Bose-Einstein condensation(BEC) and transition into a superfluid state with global phase coherence at low temperatures. An important platform to study such excitonic physics is built on double-layer quantum wells or recent two-dimensional material heterostructures, where two parallel planes of electrons and holes are separated by a thin insulating layer. Lowering this separation distance ($d$) enhances the interlayer Coulomb interaction thereby strengthens the exciton binding energy. However, an exceedingly small $d$ will lead to the undesired interlayer tunneling, which results the annihilation of excitons. Here, we report the observation of a sequences of robust exciton condensates(ECs) in double bilayer graphenes twisted to $\\sim 10^\\circ$ with no insulating mid-layer. The large momentum mismatch between the two graphene layers well suppress the interlayer tunneling, allowing us to reach the separation lower limit $\\sim$ 0.334 nm and investigate ECs in the extreme coupling regime. Carrying out transport measurements on the bulk and edge of the devices, we find incompressible states corresponding to ECs when both layers are half-filled in the $N=0$ and $N=1$ Landau levels (LLs). The comparison between these ECs and theoretical calculations suggest that the low-energy charged excitation of ECs can be meron-antimeron or particle-hole pair, which relies on both LL index and carrier type. Our results establish large-angle twisted bilayers as an experimental platform with extreme coupling strength for studying quantum bosonic phase and its low-energy excitations.","sentences":["Excitons, the bosonic quasiparticle emerging from Coulomb interaction between electrons and holes, will undergo a Bose-Einstein condensation(BEC) and transition into a superfluid state with global phase coherence at low temperatures.","An important platform to study such excitonic physics is built on double-layer quantum wells or recent two-dimensional material heterostructures, where two parallel planes of electrons and holes are separated by a thin insulating layer.","Lowering this separation distance ($d$) enhances the interlayer Coulomb interaction thereby strengthens the exciton binding energy.","However, an exceedingly small $d$ will lead to the undesired interlayer tunneling, which results the annihilation of excitons.","Here, we report the observation of a sequences of robust exciton condensates(ECs) in double bilayer graphenes twisted to $\\sim 10^\\circ$ with no insulating mid-layer.","The large momentum mismatch between the two graphene layers well suppress the interlayer tunneling, allowing us to reach the separation lower limit $\\sim$ 0.334 nm and investigate ECs in the extreme coupling regime.","Carrying out transport measurements on the bulk and edge of the devices, we find incompressible states corresponding to ECs when both layers are half-filled in the $N=0$ and $N=1$ Landau levels (LLs).","The comparison between these ECs and theoretical calculations suggest that the low-energy charged excitation of ECs can be meron-antimeron or particle-hole pair, which relies on both LL index and carrier type.","Our results establish large-angle twisted bilayers as an experimental platform with extreme coupling strength for studying quantum bosonic phase and its low-energy excitations."],"url":"http://arxiv.org/abs/2405.11761v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 01:42:16","title":"Non-Abelian Self-Correcting Quantum Memory","abstract":"We construct a family of infinitely many new candidate non-Abelian self-correcting topological quantum memories in $D\\geq 5+1$ spacetime dimensions without particle excitations using local commuting non-Pauli stabilizer lattice models and field theories of $\\mathbb{Z}_2^3$ higher-form gauge fields with nontrivial topological action. We call such non-Pauli stabilizer models magic stabilizer codes. The family of topological orders have Abelian electric excitations and non-Abelian magnetic excitations that obey Ising-like fusion rules, generalizing the dihedral group $\\mathbb{D}_8$ gauge theory in 2+1d. The simplest example includes a new non-Abelian self-correcting memory in 5+1d with Abelian loop excitations and non-Abelian membrane excitations. We use a Peierls argument to demonstrate the self-correction property and the thermal stability, and devise a probablistic local cellular-automaton decoder.","sentences":["We construct a family of infinitely many new candidate non-Abelian self-correcting topological quantum memories in $D\\geq 5+1$ spacetime dimensions without particle excitations using local commuting non-Pauli stabilizer lattice models and field theories of $\\mathbb{Z}_2^3$ higher-form gauge fields with nontrivial topological action.","We call such non-Pauli stabilizer models magic stabilizer codes.","The family of topological orders have Abelian electric excitations and non-Abelian magnetic excitations that obey Ising-like fusion rules, generalizing the dihedral group $\\mathbb{D}_8$ gauge theory in 2+1d.","The simplest example includes a new non-Abelian self-correcting memory in 5+1d with Abelian loop excitations and non-Abelian membrane excitations.","We use a Peierls argument to demonstrate the self-correction property and the thermal stability, and devise a probablistic local cellular-automaton decoder."],"url":"http://arxiv.org/abs/2405.11719v1","category":"quant-ph"}
{"created":"2024-05-20 01:11:04","title":"Magnetized Accretion onto and Feedback from Supermassive Black Holes in Elliptical Galaxies","abstract":"We present three-dimensional magnetohydrodynamic (MHD) simulations of the fueling of supermassive black holes in elliptical galaxies from a turbulent cooling medium on galactic scales, taking M87* as a typical case. We find that the mass accretion rate is increased by a factor of $\\sim 10$ compared with analogous hydrodynamic simulations. The scaling of $\\dot{M} \\sim r^{1/2}$ roughly holds from $\\sim 10\\,\\mathrm{pc}$ to $\\sim 10^{-3}\\,\\mathrm{pc}$ ($\\sim 10\\, r_\\mathrm{g}$) with the accretion rate through the event horizon being $\\sim 10^{-2}\\, M_\\odot\\,\\mathrm{yr^{-1}}$. The accretion flow on scales $\\sim 0.03-3\\,\\mathrm{kpc}$ takes the form of magnetized filaments. Within $\\sim 30\\,\\mathrm{pc}$, the cold gas circularizes, forming a highly magnetized ($\\beta\\sim 10^{-3}$) thick disk supported by a primarily toroidal magnetic field. The cold disk is truncated and transitions to a turbulent hot accretion flow at $\\sim0.3\\,\\mathrm{pc}$ ($10^3\\,r_\\mathrm{g}$). There are strong outflows towards the poles driven by the magnetic field. The outflow energy flux increases with smaller accretor size, reaching $\\sim 3\\times10^{43}\\,\\mathrm{erg\\,s^{-1}}$ for $r_\\mathrm{in}=8\\,r_\\mathrm{g}$; this corresponds to a nearly constant energy feedback efficiency of $\\eta\\sim0.05-0.1$ independent of accretor size. The feedback energy is enough to balance the total cooling of the M87/Virgo hot halo out to $\\sim 50$ kpc. The accreted magnetic flux at small radii is similar to that in magnetically arrested disk models, consistent with the formation of a powerful jet on horizon scales in M87. Our results motivate a subgrid model for accretion in lower-resolution simulations in which the hot gas accretion rate is suppressed relative to the Bondi rate by $\\sim (10r_\\mathrm{g}/r_\\mathrm{B})^{1/2}$.","sentences":["We present three-dimensional magnetohydrodynamic (MHD) simulations of the fueling of supermassive black holes in elliptical galaxies from a turbulent cooling medium on galactic scales, taking M87* as a typical case.","We find that the mass accretion rate is increased by a factor of $\\sim 10$ compared with analogous hydrodynamic simulations.","The scaling of $\\dot{M} \\sim r^{1/2}$ roughly holds from $\\sim 10\\,\\mathrm{pc}$ to $\\sim 10^{-3}\\,\\mathrm{pc}$ ($\\sim 10\\, r_\\mathrm{g}$) with the accretion rate through the event horizon being $\\sim 10^{-2}\\, M_\\odot\\,\\mathrm{yr^{-1}}$. The accretion flow on scales $\\sim 0.03-3\\,\\mathrm{kpc}$ takes the form of magnetized filaments.","Within $\\sim 30\\,\\mathrm{pc}$, the cold gas circularizes, forming a highly magnetized ($\\beta\\sim 10^{-3}$) thick disk supported by a primarily toroidal magnetic field.","The cold disk is truncated and transitions to a turbulent hot accretion flow at $\\sim0.3\\,\\mathrm{pc}$ ($10^3\\,r_\\mathrm{g}$).","There are strong outflows towards the poles driven by the magnetic field.","The outflow energy flux increases with smaller accretor size, reaching $\\sim 3\\times10^{43}\\,\\mathrm{erg\\,s^{-1}}$ for $r_\\mathrm{in}=8\\,r_\\mathrm{g}$; this corresponds to a nearly constant energy feedback efficiency of $\\eta\\sim0.05-0.1$ independent of accretor size.","The feedback energy is enough to balance the total cooling of the M87/Virgo hot halo out to $\\sim 50$ kpc.","The accreted magnetic flux at small radii is similar to that in magnetically arrested disk models, consistent with the formation of a powerful jet on horizon scales in M87.","Our results motivate a subgrid model for accretion in lower-resolution simulations in which the hot gas accretion rate is suppressed relative to the Bondi rate by $\\sim (10r_\\mathrm{g}/r_\\mathrm{B})^{1/2}$."],"url":"http://arxiv.org/abs/2405.11711v1","category":"astro-ph.HE"}
{"created":"2024-05-19 22:35:02","title":"InterAct: Capture and Modelling of Realistic, Expressive and Interactive Activities between Two Persons in Daily Scenarios","abstract":"We address the problem of accurate capture and expressive modelling of interactive behaviors happening between two persons in daily scenarios. Different from previous works which either only consider one person or focus on conversational gestures, we propose to simultaneously model the activities of two persons, and target objective-driven, dynamic, and coherent interactions which often span long duration. To this end, we capture a new dataset dubbed InterAct, which is composed of 241 motion sequences where two persons perform a realistic scenario over the whole sequence. The audios, body motions, and facial expressions of both persons are all captured in our dataset. We also demonstrate the first diffusion model based approach that directly estimates the interactive motions between two persons from their audios alone. All the data and code will be available for research purposes upon acceptance of the paper.","sentences":["We address the problem of accurate capture and expressive modelling of interactive behaviors happening between two persons in daily scenarios.","Different from previous works which either only consider one person or focus on conversational gestures, we propose to simultaneously model the activities of two persons, and target objective-driven, dynamic, and coherent interactions which often span long duration.","To this end, we capture a new dataset dubbed InterAct, which is composed of 241 motion sequences where two persons perform a realistic scenario over the whole sequence.","The audios, body motions, and facial expressions of both persons are all captured in our dataset.","We also demonstrate the first diffusion model based approach that directly estimates the interactive motions between two persons from their audios alone.","All the data and code will be available for research purposes upon acceptance of the paper."],"url":"http://arxiv.org/abs/2405.11690v1","category":"cs.CV"}
{"created":"2024-05-19 21:52:12","title":"Distributed Tensor Principal Component Analysis","abstract":"As tensors become widespread in modern data analysis, Tucker low-rank Principal Component Analysis (PCA) has become essential for dimensionality reduction and structural discovery in tensor datasets. Motivated by the common scenario where large-scale tensors are distributed across diverse geographic locations, this paper investigates tensor PCA within a distributed framework where direct data pooling is impractical.   We offer a comprehensive analysis of three specific scenarios in distributed Tensor PCA: a homogeneous setting in which tensors at various locations are generated from a single noise-affected model; a heterogeneous setting where tensors at different locations come from distinct models but share some principal components, aiming to improve estimation across all locations; and a targeted heterogeneous setting, designed to boost estimation accuracy at a specific location with limited samples by utilizing transferred knowledge from other sites with ample data.   We introduce novel estimation methods tailored to each scenario, establish statistical guarantees, and develop distributed inference techniques to construct confidence regions. Our theoretical findings demonstrate that these distributed methods achieve sharp rates of accuracy by efficiently aggregating shared information across different tensors, while maintaining reasonable communication costs. Empirical validation through simulations and real-world data applications highlights the advantages of our approaches, particularly in managing heterogeneous tensor data.","sentences":["As tensors become widespread in modern data analysis, Tucker low-rank Principal Component Analysis (PCA) has become essential for dimensionality reduction and structural discovery in tensor datasets.","Motivated by the common scenario where large-scale tensors are distributed across diverse geographic locations, this paper investigates tensor PCA within a distributed framework where direct data pooling is impractical.   ","We offer a comprehensive analysis of three specific scenarios in distributed Tensor PCA: a homogeneous setting in which tensors at various locations are generated from a single noise-affected model; a heterogeneous setting where tensors at different locations come from distinct models but share some principal components, aiming to improve estimation across all locations; and a targeted heterogeneous setting, designed to boost estimation accuracy at a specific location with limited samples by utilizing transferred knowledge from other sites with ample data.   ","We introduce novel estimation methods tailored to each scenario, establish statistical guarantees, and develop distributed inference techniques to construct confidence regions.","Our theoretical findings demonstrate that these distributed methods achieve sharp rates of accuracy by efficiently aggregating shared information across different tensors, while maintaining reasonable communication costs.","Empirical validation through simulations and real-world data applications highlights the advantages of our approaches, particularly in managing heterogeneous tensor data."],"url":"http://arxiv.org/abs/2405.11681v1","category":"stat.ME"}
{"created":"2024-05-19 21:48:09","title":"Spontaneous strain in quasi-two-dimensional Janus CdSe nanoplatelets and its microscopic mechanisms","abstract":"Spontaneous strain and spontaneous folding of thin nanoplatelets are known phenomena whose microscopic mechanisms are still debating. In this work, first-principles calculations are used to study the mechanical stresses that arise in Janus CdSe nanoplatelets and result in their spontaneous strain. Calculations reveal the existence of three microscopic mechanisms of this phenomenon. Two bulk mechanisms are associated with the inverse piezoelectric effect in an electric field created by the difference in electronegativities of ligands and by the depolarizing field resulting from the difference in the potential jumps in electrical double layers on the surfaces of nanoplatelets. These mechanisms account for 5-25% of the observed effect. The third mechanism is associated with the surface strain of nanoplatelets by bridging bonds, and its influence is predominant. It is shown that the latter mechanism cause spontaneous folding of thin CdSe nanoplatelets and, depending on the values of surface stresses and lateral orientation of nanoplatelets, can result in formation of their experimentally observed structures such as scrolls, spirals, and twisted ribbons.","sentences":["Spontaneous strain and spontaneous folding of thin nanoplatelets are known phenomena whose microscopic mechanisms are still debating.","In this work, first-principles calculations are used to study the mechanical stresses that arise in Janus CdSe nanoplatelets and result in their spontaneous strain.","Calculations reveal the existence of three microscopic mechanisms of this phenomenon.","Two bulk mechanisms are associated with the inverse piezoelectric effect in an electric field created by the difference in electronegativities of ligands and by the depolarizing field resulting from the difference in the potential jumps in electrical double layers on the surfaces of nanoplatelets.","These mechanisms account for 5-25% of the observed effect.","The third mechanism is associated with the surface strain of nanoplatelets by bridging bonds, and its influence is predominant.","It is shown that the latter mechanism cause spontaneous folding of thin CdSe nanoplatelets and, depending on the values of surface stresses and lateral orientation of nanoplatelets, can result in formation of their experimentally observed structures such as scrolls, spirals, and twisted ribbons."],"url":"http://arxiv.org/abs/2405.11679v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-19 20:16:18","title":"Limiting absorption principle for contractions","abstract":"We establish limiting absorption principles for contractions on a Hilbert space. Our sufficient conditions are based on positive commutator estimates. We discuss the dynamical implications of this principle to the corresponding discrete-time semigroup and provide several applications. Notably to Toeplitz operators and contractive quantum walks.","sentences":["We establish limiting absorption principles for contractions on a Hilbert space.","Our sufficient conditions are based on positive commutator estimates.","We discuss the dynamical implications of this principle to the corresponding discrete-time semigroup and provide several applications.","Notably to Toeplitz operators and contractive quantum walks."],"url":"http://arxiv.org/abs/2405.11664v1","category":"math-ph"}
{"created":"2024-05-19 20:12:44","title":"A 2.7 $\u03bc$m Backward Wave Optical Parametric Oscillator Source for CO$_2$ Spectroscopy","abstract":"In this study, we demonstrated the direct use of an inherently narrowband backward wave in the mid-infrared for CO$_2$ and H$_2$O vapor spectroscopy in ambient air. This wave is generated in a backward wave optical parametric oscillator (BWOPO) pumped by a multi longitudinal mode laser at 1030 nm, eliminating the need for additional spectral narrowing or wavelength stabilization. A full characterization of the source is presented, revealing a central output at 2712 nm, showcasing temperature tuning of -1.77 GHz/K, and achieving an output pulse energy of 2.3 $\\mu$J. Novel methods are introduced for measuring the linewidth and wavelength stability using lab air. These approaches demonstrate a narrow output of 43 pm and establish an upper limit of stability at 65 MHz, with no active means of stabilization. These findings underscore the potential of BWOPOs as a robust platform for future DIAL systems.","sentences":["In this study, we demonstrated the direct use of an inherently narrowband backward wave in the mid-infrared for CO$_2$ and H$_2$O vapor spectroscopy in ambient air.","This wave is generated in a backward wave optical parametric oscillator (BWOPO) pumped by a multi longitudinal mode laser at 1030 nm, eliminating the need for additional spectral narrowing or wavelength stabilization.","A full characterization of the source is presented, revealing a central output at 2712 nm, showcasing temperature tuning of -1.77 GHz/K, and achieving an output pulse energy of 2.3 $\\mu$J. Novel methods are introduced for measuring the linewidth and wavelength stability using lab air.","These approaches demonstrate a narrow output of 43 pm and establish an upper limit of stability at 65 MHz, with no active means of stabilization.","These findings underscore the potential of BWOPOs as a robust platform for future DIAL systems."],"url":"http://arxiv.org/abs/2405.11661v1","category":"physics.optics"}
{"created":"2024-05-19 19:27:04","title":"On non-detection of Gamma-Ray Bursts in three compact binary merger events detected by LIGO","abstract":"The joint detection of the gravitational wave (GW) event GW170817 and the short-duration gamma-ray burst (SGRB) event GRB 170817A, marked the beginning of GW multi-messenger astronomy and confirmed that binary neutron star mergers are progenitors of at least some SGRBs. An estimated joint detection rate of 0.3 - 1.7 per year between the LIGO-Hanford, LIGO-Livingston and Virgo GW network at design sensitivity, and the Fermi Gamma-ray Burst Monitor was predicted. However, to date, the GW170817/GRB 170817A joint detection has been the only event of its kind so far. Taking into account that SGRBs are narrowly beamed and are emitted perpendicular to the orbital plane of the binary system, we propose that previous mergers involving neutron stars, were orientated such that observation of the emitted SGRB along this narrow jet was not possible. To support this hypothesis we have estimated the inclination of the binary systems for previously detected Binary Neutron Star (BNS) and Black Hole Neutron Star (BHNS) mergers through GW analysis. This analysis was performed using BILBY, a Python based Bayesian inference library, to estimate the inclination of the BNS events GW170817 and GW190425, and the BHNS events GW190917_114630 and GW200115_042309. The results obtained in this study indicate that these binaries may have had inclinations greater than $33^{\\circ}$ with respect to the line of sight from Earth, an upper limit on the viewing angle set from observations of GRB 170817A. This then suggests that the observation of the emitted SGRB from these past mergers might not have been possible.","sentences":["The joint detection of the gravitational wave (GW) event GW170817 and the short-duration gamma-ray burst (SGRB) event GRB 170817A, marked the beginning of GW multi-messenger astronomy and confirmed that binary neutron star mergers are progenitors of at least some SGRBs.","An estimated joint detection rate of 0.3 - 1.7 per year between the LIGO-Hanford, LIGO-Livingston and Virgo GW network at design sensitivity, and the Fermi Gamma-ray Burst Monitor was predicted.","However, to date, the GW170817/GRB 170817A joint detection has been the only event of its kind so far.","Taking into account that SGRBs are narrowly beamed and are emitted perpendicular to the orbital plane of the binary system, we propose that previous mergers involving neutron stars, were orientated such that observation of the emitted SGRB along this narrow jet was not possible.","To support this hypothesis we have estimated the inclination of the binary systems for previously detected Binary Neutron Star (BNS) and Black Hole Neutron Star (BHNS) mergers through GW analysis.","This analysis was performed using BILBY, a Python based Bayesian inference library, to estimate the inclination of the BNS events GW170817 and GW190425, and the BHNS events GW190917_114630 and GW200115_042309.","The results obtained in this study indicate that these binaries may have had inclinations greater than $33^{\\circ}$ with respect to the line of sight from Earth, an upper limit on the viewing angle set from observations of GRB 170817A. This then suggests that the observation of the emitted SGRB from these past mergers might not have been possible."],"url":"http://arxiv.org/abs/2405.11650v1","category":"astro-ph.HE"}
{"created":"2024-05-19 19:00:16","title":"Probing the CIV continuum size luminosity relation in active galactic nuclei with photometric reverberation mapping","abstract":"Reverberation mapping accurately determines virial black hole masses only for redshifts $z <$ 0.2 by utilizing the relationship between the H$\\beta$ broad-line region (BLR) size and the 5100 Angstroms continuum luminosity established with $\\sim 200$ active galactic nuclei (AGN). For quasars at $z \\sim 2-3$ determining the BLR size is time-consuming and limited by seasonal gaps, requiring e.g., $\\sim$ 20 years of monitoring of the CIV emission lines. In this work, we demonstrate that an efficient alternative is to use a continuum size-luminosity relation, which can be obtained up to 150 times faster than BLR sizes using photometric reverberation mapping (PRM). We outline the method and its feasibility based on simulations and propose an observational strategy that can be carried out with meter-class telescopes. In particular, we focus on the ESO La Silla 2.2 meter telescope as it is suitable for an efficient PRM campaign. These observations will provide the scaling factor between the accretion disk and the BLR size (for CIV-1350 Angstroms), which is crucial for estimating the masses of black holes at higher redshifts ($z \\gtrsim 2-3$).","sentences":["Reverberation mapping accurately determines virial black hole masses only for redshifts $z <$ 0.2 by utilizing the relationship between the H$\\beta$ broad-line region (BLR) size and the 5100 Angstroms continuum luminosity established with $\\sim 200$ active galactic nuclei (AGN).","For quasars at $z \\sim 2-3$ determining the BLR size is time-consuming and limited by seasonal gaps, requiring e.g., $\\sim$ 20 years of monitoring of the CIV emission lines.","In this work, we demonstrate that an efficient alternative is to use a continuum size-luminosity relation, which can be obtained up to 150 times faster than BLR sizes using photometric reverberation mapping (PRM).","We outline the method and its feasibility based on simulations and propose an observational strategy that can be carried out with meter-class telescopes.","In particular, we focus on the ESO La Silla 2.2 meter telescope as it is suitable for an efficient PRM campaign.","These observations will provide the scaling factor between the accretion disk and the BLR size (for CIV-1350 Angstroms), which is crucial for estimating the masses of black holes at higher redshifts ($z \\gtrsim 2-3$)."],"url":"http://arxiv.org/abs/2405.11649v1","category":"astro-ph.GA"}
{"created":"2024-05-19 18:38:05","title":"Phase diagram and Coarsening dynamics in dry apolar Active Nematics with Reciprocal local alignment","abstract":"Using the Lebwohl-Lasher interaction for reciprocal local alignment, we present a comprehensive phase diagram for a dry, apolar, active nematic system using its stochastic dynamics. The nematic-isotropic transition in this system is first-order and fluctuation-dominated. Our phase diagram identifies three distinct regions based on activity and orientational noise relative to alignment strength: a homogeneous isotropic phase, a nematic phase with giant density fluctuations, and a coexistence region. Using mean-field analysis and hydrodynamic theory, we demonstrate that reciprocal interactions lead to a density fluctuation-induced first-order transition and derive a phase boundary consistent with numerical results. Quenching from the isotropic to nematic phase reveals coarsening dynamics with distinctive scaling behaviors for density and nematic fields, exhibiting exponents between 2 and 3, typical of non-conserved and conserved dynamics.","sentences":["Using the Lebwohl-Lasher interaction for reciprocal local alignment, we present a comprehensive phase diagram for a dry, apolar, active nematic system using its stochastic dynamics.","The nematic-isotropic transition in this system is first-order and fluctuation-dominated.","Our phase diagram identifies three distinct regions based on activity and orientational noise relative to alignment strength: a homogeneous isotropic phase, a nematic phase with giant density fluctuations, and a coexistence region.","Using mean-field analysis and hydrodynamic theory, we demonstrate that reciprocal interactions lead to a density fluctuation-induced first-order transition and derive a phase boundary consistent with numerical results.","Quenching from the isotropic to nematic phase reveals coarsening dynamics with distinctive scaling behaviors for density and nematic fields, exhibiting exponents between 2 and 3, typical of non-conserved and conserved dynamics."],"url":"http://arxiv.org/abs/2405.11642v1","category":"cond-mat.soft"}
{"created":"2024-05-19 17:55:55","title":"Ultraslow calorimetric studies of the martensitic transformation of NiFeGa alloys: detection and analysis of avalanche phenomena","abstract":"We study the thermal properties of a bulk Ni55Fe19Ga26 Heusler alloy in a conduction calorimeter. At slow heating and cooling rates (1K/h), we compare as-cast and annealed samples. We report a smaller thermal hysteresis after the thermal treatment due to the stabilization of the 14M modulated structure in the martensite phase. In ultraslow experiments (40mK/h), we detect and analyze the calorimetric avalanches associated with the direct and reverse martensitic transformation from cubic to 14M phase. This reveals a distribution of events characterized by a power law with exponential cutoff $p(u) \\propto u^{-\\varepsilon}\\exp(-u/\\xi)$ where $\\varepsilon\\sim 2$ and damping energies $\\xi=370$uJ (direct) and $\\xi=27$uJ (reverse) that characterize the asymmetry of the transformation.","sentences":["We study the thermal properties of a bulk Ni55Fe19Ga26","Heusler alloy in a conduction calorimeter.","At slow heating and cooling rates (1K/h), we compare as-cast and annealed samples.","We report a smaller thermal hysteresis after the thermal treatment due to the stabilization of the 14M modulated structure in the martensite phase.","In ultraslow experiments (40mK/h), we detect and analyze the calorimetric avalanches associated with the direct and reverse martensitic transformation from cubic to 14M phase.","This reveals a distribution of events characterized by a power law with exponential cutoff $p(u) \\propto u^{-\\varepsilon}\\exp(-u/\\xi)$ where $\\varepsilon\\sim 2$ and damping energies $\\xi=370$uJ (direct) and $\\xi=27$uJ (reverse) that characterize the asymmetry of the transformation."],"url":"http://arxiv.org/abs/2405.11636v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-19 17:39:20","title":"Distribution-in-distribution-out Regression","abstract":"Regression analysis with probability measures as input predictors and output response has recently drawn great attention. However, it is challenging to handle multiple input probability measures due to the non-flat Riemannian geometry of the Wasserstein space, hindering the definition of arithmetic operations, hence additive linear structure is not well-defined. In this work, a distribution-in-distribution-out regression model is proposed by introducing parallel transport to achieve provable commutativity and additivity of newly defined arithmetic operations in Wasserstein space. The appealing properties of the DIDO regression model can serve a foundation for model estimation, prediction, and inference. Specifically, the Fr\\'echet least squares estimator is employed to obtain the best linear unbiased estimate, supported by the newly established Fr\\'echet Gauss-Markov Theorem. Furthermore, we investigate a special case when predictors and response are all univariate Gaussian measures, leading to a simple close-form solution of linear model coefficients and $R^2$ metric. A simulation study and real case study in intraoperative cardiac output prediction are performed to evaluate the performance of the proposed method.","sentences":["Regression analysis with probability measures as input predictors and output response has recently drawn great attention.","However, it is challenging to handle multiple input probability measures due to the non-flat Riemannian geometry of the Wasserstein space, hindering the definition of arithmetic operations, hence additive linear structure is not well-defined.","In this work, a distribution-in-distribution-out regression model is proposed by introducing parallel transport to achieve provable commutativity and additivity of newly defined arithmetic operations in Wasserstein space.","The appealing properties of the DIDO regression model can serve a foundation for model estimation, prediction, and inference.","Specifically, the Fr\\'echet least squares estimator is employed to obtain the best linear unbiased estimate, supported by the newly established Fr\\'echet Gauss-Markov Theorem.","Furthermore, we investigate a special case when predictors and response are all univariate Gaussian measures, leading to a simple close-form solution of linear model coefficients and $R^2$ metric.","A simulation study and real case study in intraoperative cardiac output prediction are performed to evaluate the performance of the proposed method."],"url":"http://arxiv.org/abs/2405.11626v1","category":"stat.ME"}
{"created":"2024-05-19 17:24:45","title":"Edge modes of the Helmholtz-Onsager gas in a multiply-connected domain","abstract":"The vortex gas is an approximation used to study 2D flow using statistical mechanics methodologies. We investigate low positive Onsager temperature states for the vortex gas on an annular domain. Using mean field theory, microcanonical sampling of the point gas model, and canonical sampling of a lattice model, we find evidence for edge modes at low energy states.","sentences":["The vortex gas is an approximation used to study 2D flow using statistical mechanics methodologies.","We investigate low positive Onsager temperature states for the vortex gas on an annular domain.","Using mean field theory, microcanonical sampling of the point gas model, and canonical sampling of a lattice model, we find evidence for edge modes at low energy states."],"url":"http://arxiv.org/abs/2405.11623v1","category":"physics.plasm-ph"}
{"created":"2024-05-19 17:15:10","title":"Polyadic Cantor potential of minimum lacunarity: Special case of super periodic generalized unified Cantor potential","abstract":"To bridge the fractal and non-fractal potentials we propose a new generalised version of unified Cantor potential (GUCP), where $\\frac{1}{\\rho^{\\mu+\\nu G}}\\ \\ \\ (\\mu,\\ \\nu $ are real numbers) portion of the potential is removed and is characterised by a integer N which represents the potential count at the stage $S=1$. Notably, the polyadic Cantor potential (PCP) with minimal lacunarity is a specific instance within the GUCP paradigm. Employing the super periodic potential (SPP) formalism, we calculate a closed-form expression for transmission probability $T_{S}(k, N)$ using the $q$-Pochhammer symbol. We further show that GUCP system exhibits sharp transmission resonances, differing from traditional quantum systems. Our analysis reveals saturation in the transmission profile with evolving stages $S$ and establishes a significant scaling relationship between reflection probability and wave-vector ($k$) through analytical derivations.","sentences":["To bridge the fractal and non-fractal potentials we propose a new generalised version of unified Cantor potential (GUCP), where $\\frac{1}{\\rho^{\\mu+\\nu G}}\\ \\ \\ (\\mu,\\ \\nu $ are real numbers) portion of the potential is removed and is characterised by a integer N which represents the potential count at the stage $S=1$. Notably, the polyadic Cantor potential (PCP) with minimal lacunarity is a specific instance within the GUCP paradigm.","Employing the super periodic potential (SPP) formalism, we calculate a closed-form expression for transmission probability $T_{S}(k, N)$ using the $q$-Pochhammer symbol.","We further show that GUCP system exhibits sharp transmission resonances, differing from traditional quantum systems.","Our analysis reveals saturation in the transmission profile with evolving stages $S$ and establishes a significant scaling relationship between reflection probability and wave-vector ($k$) through analytical derivations."],"url":"http://arxiv.org/abs/2405.11617v1","category":"quant-ph"}
{"created":"2024-05-19 16:57:28","title":"Swampland and the Geometry of Marked Moduli Spaces","abstract":"We define the notion of a marked moduli space as the parameter space of a physical theory together with all of its observables. In geometric examples, this coincides with the mathematical notion of Teichm\\\"uller space. We propose two new Swampland principles about the geometry of marked moduli spaces: We conjecture that a marked moduli space is always contractible, and moreover, that there is a unique geodesic connecting any pair of points in it with respect to its physical metric. We provide strong evidence for these conjectures for theories with 8 or more supercharges.","sentences":["We define the notion of a marked moduli space as the parameter space of a physical theory together with all of its observables.","In geometric examples, this coincides with the mathematical notion of Teichm\\\"uller space.","We propose two new Swampland principles about the geometry of marked moduli spaces: We conjecture that a marked moduli space is always contractible, and moreover, that there is a unique geodesic connecting any pair of points in it with respect to its physical metric.","We provide strong evidence for these conjectures for theories with 8 or more supercharges."],"url":"http://arxiv.org/abs/2405.11611v1","category":"hep-th"}
{"created":"2024-05-19 15:58:31","title":"On the Consistency of Rapid-Turn Inflation","abstract":"Recent studies, in the context of consistency conditions for rapid-turn and third order slow-roll inflation in two-field models, raised the question whether this regime can be sustained for more than a few e-folds of expansion. We answer this question in the affirmative by showing that the consistency conditions themselves ensure the longevity of the rapid-turn regime. Furthermore, we prove this for the most general definition of rapid turning (i.e., with a turning rate that is large compared to the slow-roll parameters, but not necessarily large compared to unity), using in the process a generalized consistency condition. We also show that a special class of rapid-turn models, including angular inflation, satisfy a large-(compared to $1$)-turn-rate condition even for non-large turning rates.","sentences":["Recent studies, in the context of consistency conditions for rapid-turn and third order slow-roll inflation in two-field models, raised the question whether this regime can be sustained for more than a few e-folds of expansion.","We answer this question in the affirmative by showing that the consistency conditions themselves ensure the longevity of the rapid-turn regime.","Furthermore, we prove this for the most general definition of rapid turning (i.e., with a turning rate that is large compared to the slow-roll parameters, but not necessarily large compared to unity), using in the process a generalized consistency condition.","We also show that a special class of rapid-turn models, including angular inflation, satisfy a large-(compared to $1$)-turn-rate condition even for non-large turning rates."],"url":"http://arxiv.org/abs/2405.11595v1","category":"hep-th"}
{"created":"2024-05-19 15:38:35","title":"Possible influence of a cosmic repulsion on large-scale jets: geometric viewpoint","abstract":"Cosmic repulsion represented by a small positive value of the cosmological constant changes significantly properties of central gravitational fields at large distances, leading to existence of a static (or turnaround) radius where gravitational attraction of a center is just balanced by cosmic repulsion. Analyzing behavior of radial timelike geodesics in the Schwarzschild-de Sitter spacetime near its static radius we show that the particles with specific energy close to unity have tendency to slow down and cluster just below the static radius, forming clumps which, subsequently, start to expand uniformly due to cosmic repulsion. For central masses of $(10^6$-$10^{11}){\\rm M_{\\odot}}$ and current value of the cosmological constant $1.1\\times 10^{-52}\\,{\\rm m^{-2}}$, this phenomenon takes place at distances of tens to hundreds of kiloparsecs from the center, being comparable with distances in which huge radio-lobes from some active galaxies were observed.","sentences":["Cosmic repulsion represented by a small positive value of the cosmological constant changes significantly properties of central gravitational fields at large distances, leading to existence of a static (or turnaround) radius where gravitational attraction of a center is just balanced by cosmic repulsion.","Analyzing behavior of radial timelike geodesics in the Schwarzschild-de Sitter spacetime near its static radius we show that the particles with specific energy close to unity have tendency to slow down and cluster just below the static radius, forming clumps which, subsequently, start to expand uniformly due to cosmic repulsion.","For central masses of $(10^6$-$10^{11}){\\rm M_{\\odot}}$ and current value of the cosmological constant $1.1\\times 10^{-52}\\,{\\rm m^{-2}}$, this phenomenon takes place at distances of tens to hundreds of kiloparsecs from the center, being comparable with distances in which huge radio-lobes from some active galaxies were observed."],"url":"http://arxiv.org/abs/2405.11587v1","category":"astro-ph.HE"}
{"created":"2024-05-19 15:26:04","title":"Probing Massive Fields with Multi-Band Gravitational-Wave Observations","abstract":"We investigate the prospect of probing massive fields and testing gravitational theories with multi-band observations of gravitational waves emitted from coalescing compact binaries. Focusing on the dipole radiation induced by a massive field, we show that multi-band observations can probe the field with mass ranging from $10^{-16}$ eV to $10^{-15}$ eV, a parameter space that cannot be probed by the milli-Hertz band observations alone. Multi-band observations can also improve the constraints obtained with the LIGO-Virgo-KAGRA binaries by up to 3 orders of magnitude in the mass range. Moreover, we show that multi-band observations can discriminate the spin of the field, which cannot be identified with single-band observations.","sentences":["We investigate the prospect of probing massive fields and testing gravitational theories with multi-band observations of gravitational waves emitted from coalescing compact binaries.","Focusing on the dipole radiation induced by a massive field, we show that multi-band observations can probe the field with mass ranging from $10^{-16}$ eV to $10^{-15}$ eV, a parameter space that cannot be probed by the milli-Hertz band observations alone.","Multi-band observations can also improve the constraints obtained with the LIGO-Virgo-KAGRA binaries by up to 3 orders of magnitude in the mass range.","Moreover, we show that multi-band observations can discriminate the spin of the field, which cannot be identified with single-band observations."],"url":"http://arxiv.org/abs/2405.11583v1","category":"gr-qc"}
{"created":"2024-05-19 14:40:32","title":"Searches for Lepton Flavour Violation at ATLAS and CMS","abstract":"Lepton flavour violation (LFV), and lepton flavour university violation (LFUV), are striking signatures of beyond the Standard Model (BSM) physics. Recent searches for these at the ATLAS and CMS experiments are presented, using proton-proton collisions with a centre of mass energy of 13 TeV. A range of models and signatures are considered, including leptoquarks, heavy neutral leptons, LFV in $\\tau$ lepton decays, and new measurements of $R(K)$ and $R(J/\\Psi)$.","sentences":["Lepton flavour violation (LFV), and lepton flavour university violation (LFUV), are striking signatures of beyond the Standard Model (BSM) physics.","Recent searches for these at the ATLAS and CMS experiments are presented, using proton-proton collisions with a centre of mass energy of 13 TeV. A range of models and signatures are considered, including leptoquarks, heavy neutral leptons, LFV in $\\tau$ lepton decays, and new measurements of $R(K)$ and $R(J/\\Psi)$."],"url":"http://arxiv.org/abs/2405.11572v1","category":"hep-ex"}
{"created":"2024-05-19 14:19:23","title":"Light nuclei production in isobaric $^{96}$Ru + $^{96}$Ru and $^{96}$Ru + $^{96}$Ru collisions at 7.7-200 GeV from a multiphase transport model","abstract":"The production of light nuclei in isobaric $^{96}_{44}$Ru + $^{96}_{44}$Ru and $^{96}_{40}$Zr + $^{96}_{40}$Zr collisions, ranging from $\\sqrt{s_{NN}}$ = 7.7 to 200 GeV, are studied using the string melting version of A Multi Phase Transport (AMPT) model in combination with a coalescence approach to light nuclei production. From the calculated yields, transverse momentum $p_{T}$ spectra, and rapidity dependences of light nuclei $p$, $n$, $d$, $t$, ${}^{3}$He, we find that the Ru+Ru/Zr+Zr ratios for the yields of these particles exceed unity with the inclusion of a quadrupole deformation $\\beta_{ 2 }$ and octupole deformation $\\beta_{ 3 }$ as well as the neutron skins. We also find that heavier particles have a larger deviation from unity. Furthermore, we find that as the collision energy increases, the influence of isospin effects on the production of light nuclei in isobar collisions gradually decreases, while the influence of nuclear structure becomes more significant, particularly evident from the energy dependence of the deuteron ratio, which is unaffected by isospin effects.","sentences":["The production of light nuclei in isobaric $^{96}_{44}$Ru + $^{96}_{44}$Ru and $^{96}_{40}$Zr + $^{96}_{40}$Zr collisions, ranging from $\\sqrt{s_{NN}}$ = 7.7 to 200 GeV, are studied using the string melting version of A Multi Phase Transport (AMPT) model in combination with a coalescence approach to light nuclei production.","From the calculated yields, transverse momentum $p_{T}$ spectra, and rapidity dependences of light nuclei $p$, $n$, $d$, $t$, ${}^{3}$He, we find that the Ru+Ru/Zr+Zr ratios for the yields of these particles exceed unity with the inclusion of a quadrupole deformation $\\beta_{ 2 }$ and octupole deformation $\\beta_{ 3 }$ as well as the neutron skins.","We also find that heavier particles have a larger deviation from unity.","Furthermore, we find that as the collision energy increases, the influence of isospin effects on the production of light nuclei in isobar collisions gradually decreases, while the influence of nuclear structure becomes more significant, particularly evident from the energy dependence of the deuteron ratio, which is unaffected by isospin effects."],"url":"http://arxiv.org/abs/2405.11558v1","category":"nucl-th"}
{"created":"2024-05-19 14:07:42","title":"Strong magnetic field inside degenerate relativistic plasma and the impacts on the neutrino transport in Core-Collapse Supernovae","abstract":"We study the impacts of magnetic field on the neutrino transport inside core-collapse supernovae (CCSNe). Magnetic field quantizes the momentum of electrons and positrons, resulting in the modification of weak-interaction cross sections and the chemical potentials of electrons and positrons. We include these changes in the leakage scheme of neutrino transport and perform 1D CCSN simulations with GR1D, assuming the postbounce magnetic field strength of $10^{16-17}$ G. The results show that the neutrino opacities are enhanced due to the amplified interaction rates, resulting in a larger neutrinosphere. This further reduces the peak value of neutrino luminosities and their decay rates since neutrinos stay longer inside the neutrinosphere. Meanwhile, the neutrino mean energies are smaller shortly after bounce and reach their peak values at later times. As these neutrino properties are crucial in subsequent nucleosynthesis processes, including the $\\nu$p-process, $\\nu$-process, and $r$-process, our findings suggest that the magnetic field may leave discernible marks on the abundance pattern of nucleosynthesis in CCSN.","sentences":["We study the impacts of magnetic field on the neutrino transport inside core-collapse supernovae (CCSNe).","Magnetic field quantizes the momentum of electrons and positrons, resulting in the modification of weak-interaction cross sections and the chemical potentials of electrons and positrons.","We include these changes in the leakage scheme of neutrino transport and perform 1D CCSN simulations with GR1D, assuming the postbounce magnetic field strength of $10^{16-17}$ G.","The results show that the neutrino opacities are enhanced due to the amplified interaction rates, resulting in a larger neutrinosphere.","This further reduces the peak value of neutrino luminosities and their decay rates since neutrinos stay longer inside the neutrinosphere.","Meanwhile, the neutrino mean energies are smaller shortly after bounce and reach their peak values at later times.","As these neutrino properties are crucial in subsequent nucleosynthesis processes, including the $\\nu$p-process, $\\nu$-process, and $r$-process, our findings suggest that the magnetic field may leave discernible marks on the abundance pattern of nucleosynthesis in CCSN."],"url":"http://arxiv.org/abs/2405.11555v1","category":"astro-ph.HE"}
{"created":"2024-05-19 14:04:10","title":"The impact of quark many-body effects on exotic hadrons","abstract":"We investigate the exotic hadrons consisting of two light quarks and two heavy antiquarks, $(q\\bar Q)$-$(q\\bar Q)$. The spin-dependent term between quarks is known to give an attraction to the $ud$ spin-0 component in the isospin-0 $u\\bar c d\\bar c$ system, $T_{cc}$. However, the said component also gets a repulsion from the partial Pauli-blocking. By the dynamical calculation with a simplified quark model, we discuss that the competition of the two effects leads to a shallow bound state for $T_{cc}$, which is preferred from the experiment, and a deep bound state for $T_{bb}$.","sentences":["We investigate the exotic hadrons consisting of two light quarks and two heavy antiquarks, $(q\\bar Q)$-$(q\\bar Q)$.","The spin-dependent term between quarks is known to give an attraction to the $ud$ spin-0 component in the isospin-0 $u\\bar c d\\bar c$ system, $T_{cc}$. However, the said component also gets a repulsion from the partial Pauli-blocking.","By the dynamical calculation with a simplified quark model, we discuss that the competition of the two effects leads to a shallow bound state for $T_{cc}$, which is preferred from the experiment, and a deep bound state for $T_{bb}$."],"url":"http://arxiv.org/abs/2405.11552v1","category":"hep-ph"}
{"created":"2024-05-19 13:22:29","title":"Low-frequency absorption and radio recombination line features of the Galactic Center Lobe","abstract":"The Galactic center lobe (GCL) is a $\\sim 1^\\circ$ object located north of the Galactic center. In the mid-infrared (MIR), the GCL appears as two 8.0-micron filaments that roughly define an ellipse. There is strong 24-micron and radio continuum emission in the interior of the ellipse. Due to its morphology and location in the sky, previous authors have argued that the GCL is created by outflows from star formation in the central molecular zone or by activity of the central black hole Sgr~A$^*$. We present images of the GCL from the GaLactic and Extragalactic All-sky Murchison Widefield Array survey in radio continuum that show thermal absorption against the Galactic center, incompatible with an interpretation of synchrotron self-absorption. Estimates of the cosmic ray emissivity in this direction allow us to place a distance constraint on the GCL. To be consistent with standard emissivity assumptions, the GCL would be located 2kpc away. At a distance of 8kpc, the synchrotron background emissivity is enhanced by $\\sim75$% in the direction of the GCL. We also present radio recombination line data from the Green Bank Telescope that constrains the electron temperature and line widths in this region, which are also more explicable if the GCL lies relatively close.","sentences":["The Galactic center lobe (GCL) is a $\\sim 1^\\circ$ object located north of the Galactic center.","In the mid-infrared (MIR), the GCL appears as two 8.0-micron filaments that roughly define an ellipse.","There is strong 24-micron and radio continuum emission in the interior of the ellipse.","Due to its morphology and location in the sky, previous authors have argued that the GCL is created by outflows from star formation in the central molecular zone or by activity of the central black hole Sgr~A$^*$. We present images of the GCL from the GaLactic and Extragalactic All-sky Murchison Widefield Array survey in radio continuum that show thermal absorption against the Galactic center, incompatible with an interpretation of synchrotron self-absorption.","Estimates of the cosmic ray emissivity in this direction allow us to place a distance constraint on the GCL.","To be consistent with standard emissivity assumptions, the GCL would be located 2kpc away.","At a distance of 8kpc, the synchrotron background emissivity is enhanced by $\\sim75$% in the direction of the GCL.","We also present radio recombination line data from the Green Bank Telescope that constrains the electron temperature and line widths in this region, which are also more explicable if the GCL lies relatively close."],"url":"http://arxiv.org/abs/2405.11546v1","category":"astro-ph.GA"}
