{"created":"2024-06-11 17:59:55","title":"A3VLM: Actionable Articulation-Aware Vision Language Model","abstract":"Vision Language Models (VLMs) have received significant attention in recent years in the robotics community. VLMs are shown to be able to perform complex visual reasoning and scene understanding tasks, which makes them regarded as a potential universal solution for general robotics problems such as manipulation and navigation. However, previous VLMs for robotics such as RT-1, RT-2, and ManipLLM~ have focused on directly learning robot-centric actions. Such approaches require collecting a significant amount of robot interaction data, which is extremely costly in the real world. Thus, we propose A3VLM, an object-centric, actionable, articulation-aware vision language model. A3VLM focuses on the articulation structure and action affordances of objects. Its representation is robot-agnostic and can be translated into robot actions using simple action primitives. Extensive experiments in both simulation benchmarks and real-world settings demonstrate the effectiveness and stability of A3VLM. We release our code and other materials at https://github.com/changhaonan/A3VLM.","sentences":["Vision Language Models (VLMs) have received significant attention in recent years in the robotics community.","VLMs are shown to be able to perform complex visual reasoning and scene understanding tasks, which makes them regarded as a potential universal solution for general robotics problems such as manipulation and navigation.","However, previous VLMs for robotics such as RT-1, RT-2, and ManipLLM~ have focused on directly learning robot-centric actions.","Such approaches require collecting a significant amount of robot interaction data, which is extremely costly in the real world.","Thus, we propose A3VLM, an object-centric, actionable, articulation-aware vision language model.","A3VLM focuses on the articulation structure and action affordances of objects.","Its representation is robot-agnostic and can be translated into robot actions using simple action primitives.","Extensive experiments in both simulation benchmarks and real-world settings demonstrate the effectiveness and stability of A3VLM.","We release our code and other materials at https://github.com/changhaonan/A3VLM."],"url":"http://arxiv.org/abs/2406.07549v1","category":"cs.RO"}
{"created":"2024-06-11 17:59:48","title":"Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?","abstract":"We present a novel task and benchmark for evaluating the ability of text-to-image(T2I) generation models to produce images that fit commonsense in real life, which we call Commonsense-T2I. Given two adversarial text prompts containing an identical set of action words with minor differences, such as \"a lightbulb without electricity\" v.s. \"a lightbulb with electricity\", we evaluate whether T2I models can conduct visual-commonsense reasoning, e.g. produce images that fit \"the lightbulb is unlit\" vs. \"the lightbulb is lit\" correspondingly. Commonsense-T2I presents an adversarial challenge, providing pairwise text prompts along with expected outputs. The dataset is carefully hand-curated by experts and annotated with fine-grained labels, such as commonsense type and likelihood of the expected outputs, to assist analyzing model behavior. We benchmark a variety of state-of-the-art (sota) T2I models and surprisingly find that, there is still a large gap between image synthesis and real life photos--even the DALL-E 3 model could only achieve 48.92% on Commonsense-T2I, and the stable diffusion XL model only achieves 24.92% accuracy. Our experiments show that GPT-enriched prompts cannot solve this challenge, and we include a detailed analysis about possible reasons for such deficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation benchmark for T2I commonsense checking, fostering advancements in real life image generation.","sentences":["We present a novel task and benchmark for evaluating the ability of text-to-image(T2I) generation models to produce images that fit commonsense in real life, which we call Commonsense-T2I. Given two adversarial text prompts containing an identical set of action words with minor differences, such as \"a lightbulb without electricity\" v.s. \"a lightbulb with electricity\", we evaluate whether T2I models can conduct visual-commonsense reasoning, e.g. produce images that fit \"the lightbulb is unlit\" vs. \"the lightbulb is lit\" correspondingly.","Commonsense-T2I presents an adversarial challenge, providing pairwise text prompts along with expected outputs.","The dataset is carefully hand-curated by experts and annotated with fine-grained labels, such as commonsense type and likelihood of the expected outputs, to assist analyzing model behavior.","We benchmark a variety of state-of-the-art (sota) T2I models and surprisingly find that, there is still a large gap between image synthesis and real life photos--even the DALL-E 3 model could only achieve 48.92% on Commonsense-T2I, and the stable diffusion XL model only achieves 24.92% accuracy.","Our experiments show that GPT-enriched prompts cannot solve this challenge, and we include a detailed analysis about possible reasons for such deficiency.","We aim for Commonsense-T2I to serve as a high-quality evaluation benchmark for T2I commonsense checking, fostering advancements in real life image generation."],"url":"http://arxiv.org/abs/2406.07546v1","category":"cs.CV"}
{"created":"2024-06-11 17:59:47","title":"Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena","abstract":"Multiple-choice questions (MCQ) are frequently used to assess large language models (LLMs). Typically, an LLM is given a question and selects the answer deemed most probable after adjustments for factors like length. Unfortunately, LLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to inherent biases of priori unbalanced probabilities, influencing the prediction of answers based on these IDs. Previous research has introduced methods to reduce this ''selection bias'' by simply permutating options on a few test samples and applying to new ones. Another problem of MCQ is the lottery ticket choice by ''random guessing''. The LLM does not learn particular knowledge, but the option is guessed correctly. This situation is especially serious for those small-scale LLMs. To address them, a more thorough approach involves shifting from MCQ to open-style questions, which can fundamentally eliminate selection bias and random guessing issues. However, transitioning causes its own set of challenges in (1) identifying suitable open-style questions and (2) validating the correctness of LLM open-style responses against human-annotated ground-truths. This work aims to tackle these significant difficulties, and establish a new LLM evaluation benchmark through entirely open-style questions. Consequently, we introduce the Open-LLM-Leaderboard to track various LLMs' performance and reflect true capability of them, such as GPT-4o/4/3.5, Claude 3, Gemini, etc. Our code and dataset are available at https://github.com/VILA-Lab/Open-LLM-Leaderboard.","sentences":["Multiple-choice questions (MCQ) are frequently used to assess large language models (LLMs).","Typically, an LLM is given a question and selects the answer deemed most probable after adjustments for factors like length.","Unfortunately, LLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to inherent biases of priori unbalanced probabilities, influencing the prediction of answers based on these IDs.","Previous research has introduced methods to reduce this ''selection bias'' by simply permutating options on a few test samples and applying to new ones.","Another problem of MCQ is the lottery ticket choice by ''random guessing''.","The LLM does not learn particular knowledge, but the option is guessed correctly.","This situation is especially serious for those small-scale LLMs.","To address them, a more thorough approach involves shifting from MCQ to open-style questions, which can fundamentally eliminate selection bias and random guessing issues.","However, transitioning causes its own set of challenges in (1) identifying suitable open-style questions and (2) validating the correctness of LLM open-style responses against human-annotated ground-truths.","This work aims to tackle these significant difficulties, and establish a new LLM evaluation benchmark through entirely open-style questions.","Consequently, we introduce the Open-LLM-Leaderboard to track various LLMs' performance and reflect true capability of them, such as GPT-4o/4/3.5, Claude 3, Gemini, etc.","Our code and dataset are available at https://github.com/VILA-Lab/Open-LLM-Leaderboard."],"url":"http://arxiv.org/abs/2406.07545v1","category":"cs.CL"}
{"created":"2024-06-11 17:59:45","title":"Situational Awareness Matters in 3D Vision Language Reasoning","abstract":"Being able to carry out complicated vision language reasoning tasks in 3D space represents a significant milestone in developing household robots and human-centered embodied AI. In this work, we demonstrate that a critical and distinct challenge in 3D vision language reasoning is situational awareness, which incorporates two key components: (1) The autonomous agent grounds its self-location based on a language prompt. (2) The agent answers open-ended questions from the perspective of its calculated position. To address this challenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D vision language reasoning. We tokenize the 3D scene into sparse voxel representation and propose a language-grounded situation estimator, followed by a situated question answering module. Experiments on the SQA3D and ScanQA datasets show that SIG3D outperforms state-of-the-art models in situation estimation and question answering by a large margin (e.g., an enhancement of over 30% on situation estimation accuracy). Subsequent analysis corroborates our architectural design choices, explores the distinct functions of visual and textual tokens, and highlights the importance of situational awareness in the domain of 3D question answering.","sentences":["Being able to carry out complicated vision language reasoning tasks in 3D space represents a significant milestone in developing household robots and human-centered embodied AI.","In this work, we demonstrate that a critical and distinct challenge in 3D vision language reasoning is situational awareness, which incorporates two key components: (1) The autonomous agent grounds its self-location based on a language prompt.","(2) The agent answers open-ended questions from the perspective of its calculated position.","To address this challenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D vision language reasoning.","We tokenize the 3D scene into sparse voxel representation and propose a language-grounded situation estimator, followed by a situated question answering module.","Experiments on the SQA3D and ScanQA datasets show that SIG3D outperforms state-of-the-art models in situation estimation and question answering by a large margin (e.g., an enhancement of over 30% on situation estimation accuracy).","Subsequent analysis corroborates our architectural design choices, explores the distinct functions of visual and textual tokens, and highlights the importance of situational awareness in the domain of 3D question answering."],"url":"http://arxiv.org/abs/2406.07544v1","category":"cs.CV"}
{"created":"2024-06-11 17:59:31","title":"Cognitive Insights Across Languages: Enhancing Multimodal Interview Analysis","abstract":"Cognitive decline is a natural process that occurs as individuals age. Early diagnosis of anomalous decline is crucial for initiating professional treatment that can enhance the quality of life of those affected. To address this issue, we propose a multimodal model capable of predicting Mild Cognitive Impairment and cognitive scores. The TAUKADIAL dataset is used to conduct the evaluation, which comprises audio recordings of clinical interviews. The proposed model demonstrates the ability to transcribe and differentiate between languages used in the interviews. Subsequently, the model extracts audio and text features, combining them into a multimodal architecture to achieve robust and generalized results. Our approach involves in-depth research to implement various features obtained from the proposed modalities.","sentences":["Cognitive decline is a natural process that occurs as individuals age.","Early diagnosis of anomalous decline is crucial for initiating professional treatment that can enhance the quality of life of those affected.","To address this issue, we propose a multimodal model capable of predicting Mild Cognitive Impairment and cognitive scores.","The TAUKADIAL dataset is used to conduct the evaluation, which comprises audio recordings of clinical interviews.","The proposed model demonstrates the ability to transcribe and differentiate between languages used in the interviews.","Subsequently, the model extracts audio and text features, combining them into a multimodal architecture to achieve robust and generalized results.","Our approach involves in-depth research to implement various features obtained from the proposed modalities."],"url":"http://arxiv.org/abs/2406.07542v1","category":"cs.LG"}
{"created":"2024-06-11 17:56:14","title":"Hearing Anything Anywhere","abstract":"Recent years have seen immense progress in 3D computer vision and computer graphics, with emerging tools that can virtualize real-world 3D environments for numerous Mixed Reality (XR) applications. However, alongside immersive visual experiences, immersive auditory experiences are equally vital to our holistic perception of an environment. In this paper, we aim to reconstruct the spatial acoustic characteristics of an arbitrary environment given only a sparse set of (roughly 12) room impulse response (RIR) recordings and a planar reconstruction of the scene, a setup that is easily achievable by ordinary users. To this end, we introduce DiffRIR, a differentiable RIR rendering framework with interpretable parametric models of salient acoustic features of the scene, including sound source directivity and surface reflectivity. This allows us to synthesize novel auditory experiences through the space with any source audio. To evaluate our method, we collect a dataset of RIR recordings and music in four diverse, real environments. We show that our model outperforms state-ofthe-art baselines on rendering monaural and binaural RIRs and music at unseen locations, and learns physically interpretable parameters characterizing acoustic properties of the sound source and surfaces in the scene.","sentences":["Recent years have seen immense progress in 3D computer vision and computer graphics, with emerging tools that can virtualize real-world 3D environments for numerous Mixed Reality (XR) applications.","However, alongside immersive visual experiences, immersive auditory experiences are equally vital to our holistic perception of an environment.","In this paper, we aim to reconstruct the spatial acoustic characteristics of an arbitrary environment given only a sparse set of (roughly 12) room impulse response (RIR) recordings and a planar reconstruction of the scene, a setup that is easily achievable by ordinary users.","To this end, we introduce DiffRIR, a differentiable RIR rendering framework with interpretable parametric models of salient acoustic features of the scene, including sound source directivity and surface reflectivity.","This allows us to synthesize novel auditory experiences through the space with any source audio.","To evaluate our method, we collect a dataset of RIR recordings and music in four diverse, real environments.","We show that our model outperforms state-ofthe-art baselines on rendering monaural and binaural RIRs and music at unseen locations, and learns physically interpretable parameters characterizing acoustic properties of the sound source and surfaces in the scene."],"url":"http://arxiv.org/abs/2406.07532v1","category":"cs.SD"}
{"created":"2024-06-11 17:53:25","title":"Will Southeast Asia be the next global manufacturing hub? A multiway cointegration, causality, and dynamic connectedness analyses on factors influencing offshore decisions","abstract":"The COVID-19 pandemic has compelled multinational corporations to diversify their global supply chain risk and to relocate their factories to Southeast Asian countries beyond China. Such recent phenomena provide a good opportunity to understand the factors that influenced offshore decisions in the last two decades. We propose a new conceptual framework based on econometric approaches to examine the relationships between these factors. Firstly, the Vector Auto Regression (VAR) for multi-way cointegration analysis by a Johansen test as well as the embedding Granger causality analysis to examine offshore decisions--innovation, technology readiness, infrastructure, foreign direct investment (FDI), and intermediate imports. Secondly, a Quantile Vector Autoregressive (QVAR) model is used to assess the dynamic connectedness among Southeast Asian countries based on the offshore factors. This study explores a system-wide experiment to evaluate the spillover effects of offshore decisions. It reports a comprehensive analysis using time-series data collected from the World Bank. The results of the cointegration, causality, and dynamic connectedness analyses show that a subset of Southeast Asian countries have spillover effects on each other. These countries present a multi-way cointegration and dynamic connectedness relationship. The study contributes to policymaking by providing a data-driven innovative approach through a new conceptual framework.","sentences":["The COVID-19 pandemic has compelled multinational corporations to diversify their global supply chain risk and to relocate their factories to Southeast Asian countries beyond China.","Such recent phenomena provide a good opportunity to understand the factors that influenced offshore decisions in the last two decades.","We propose a new conceptual framework based on econometric approaches to examine the relationships between these factors.","Firstly, the Vector Auto Regression (VAR) for multi-way cointegration analysis by a Johansen test as well as the embedding Granger causality analysis to examine offshore decisions--innovation, technology readiness, infrastructure, foreign direct investment (FDI), and intermediate imports.","Secondly, a Quantile Vector Autoregressive (QVAR) model is used to assess the dynamic connectedness among Southeast Asian countries based on the offshore factors.","This study explores a system-wide experiment to evaluate the spillover effects of offshore decisions.","It reports a comprehensive analysis using time-series data collected from the World Bank.","The results of the cointegration, causality, and dynamic connectedness analyses show that a subset of Southeast Asian countries have spillover effects on each other.","These countries present a multi-way cointegration and dynamic connectedness relationship.","The study contributes to policymaking by providing a data-driven innovative approach through a new conceptual framework."],"url":"http://arxiv.org/abs/2406.07525v1","category":"econ.GN"}
{"created":"2024-06-11 17:51:40","title":"Simple and Effective Masked Diffusion Language Models","abstract":"While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. In this work, we show that simple masked discrete diffusion is more performant than previously thought. We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements. Our objective has a simple form -- it is a mixture of classical masked language modeling losses -- and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model. On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity. We release our code at: https://github.com/kuleshov-group/mdlm","sentences":["While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling.","In this work, we show that simple masked discrete diffusion is more performant than previously thought.","We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements.","Our objective has a simple form -- it is a mixture of classical masked language modeling losses -- and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model.","On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity.","We release our code at: https://github.com/kuleshov-group/mdlm"],"url":"http://arxiv.org/abs/2406.07524v1","category":"cs.CL"}
{"created":"2024-06-11 17:50:15","title":"Neural Gaffer: Relighting Any Object via Diffusion","abstract":"Single-image relighting is a challenging task that involves reasoning about the complex interplay between geometry, materials, and lighting. Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight. Alternatively, some methods explicitly decompose a scene into intrinsic components, such as normals and BRDFs, which can be inaccurate or under-expressive. In this work, we propose a novel end-to-end 2D relighting diffusion model, called Neural Gaffer, that takes a single image of any object and can synthesize an accurate, high-quality relit image under any novel environmental lighting condition, simply by conditioning an image generator on a target environment map, without an explicit scene decomposition. Our method builds on a pre-trained diffusion model, and fine-tunes it on a synthetic relighting dataset, revealing and harnessing the inherent understanding of lighting present in the diffusion model. We evaluate our model on both synthetic and in-the-wild Internet imagery and demonstrate its advantages in terms of generalization and accuracy. Moreover, by combining with other generative methods, our model enables many downstream 2D tasks, such as text-based relighting and object insertion. Our model can also operate as a strong relighting prior for 3D tasks, such as relighting a radiance field.","sentences":["Single-image relighting is a challenging task that involves reasoning about the complex interplay between geometry, materials, and lighting.","Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight.","Alternatively, some methods explicitly decompose a scene into intrinsic components, such as normals and BRDFs, which can be inaccurate or under-expressive.","In this work, we propose a novel end-to-end 2D relighting diffusion model, called Neural Gaffer, that takes a single image of any object and can synthesize an accurate, high-quality relit image under any novel environmental lighting condition, simply by conditioning an image generator on a target environment map, without an explicit scene decomposition.","Our method builds on a pre-trained diffusion model, and fine-tunes it on a synthetic relighting dataset, revealing and harnessing the inherent understanding of lighting present in the diffusion model.","We evaluate our model on both synthetic and in-the-wild Internet imagery and demonstrate its advantages in terms of generalization and accuracy.","Moreover, by combining with other generative methods, our model enables many downstream 2D tasks, such as text-based relighting and object insertion.","Our model can also operate as a strong relighting prior for 3D tasks, such as relighting a radiance field."],"url":"http://arxiv.org/abs/2406.07520v1","category":"cs.CV"}
{"created":"2024-06-11 17:50:04","title":"Physics-guided weak-form discovery of reduced-order models for trapped ultracold hydrodynamics","abstract":"We study the relaxation of a highly collisional, ultracold but nondegenerate gas of polar molecules. Confined within a harmonic trap, the gas is subject to fluid-gaseous coupled dynamics that lead to a breakdown of first-order hydrodynamics. An attempt to treat these higher-order hydrodynamic effects was previously made with a Gaussian ansatz and coarse-graining model parameter [R. R. W. Wang & J. L. Bohn, Phys. Rev. A 108, 013322 (2023)], leading to an approximate set of equations for a few collective observables accessible to experiments. Here we present substantially improved reduced-order models for these same observables, admissible beyond previous parameter regimes, discovered directly from particle simulations using the WSINDy algorithm (Weak-form Sparse Identification of Nonlinear Dynamics). The interpretable nature of the learning algorithm enables estimation of previously unknown physical quantities and discovery of model terms with candidate physical mechanisms, revealing new physics in mixed collisional regimes. Our approach constitutes a general framework for data-driven model identification leveraging known physics.","sentences":["We study the relaxation of a highly collisional, ultracold but nondegenerate gas of polar molecules.","Confined within a harmonic trap, the gas is subject to fluid-gaseous coupled dynamics that lead to a breakdown of first-order hydrodynamics.","An attempt to treat these higher-order hydrodynamic effects was previously made with a Gaussian ansatz and coarse-graining model parameter [R. R. W. Wang & J. L. Bohn, Phys.","Rev. A 108, 013322 (2023)], leading to an approximate set of equations for a few collective observables accessible to experiments.","Here we present substantially improved reduced-order models for these same observables, admissible beyond previous parameter regimes, discovered directly from particle simulations using the WSINDy algorithm (Weak-form Sparse Identification of Nonlinear Dynamics).","The interpretable nature of the learning algorithm enables estimation of previously unknown physical quantities and discovery of model terms with candidate physical mechanisms, revealing new physics in mixed collisional regimes.","Our approach constitutes a general framework for data-driven model identification leveraging known physics."],"url":"http://arxiv.org/abs/2406.07519v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-11 17:46:16","title":"Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement","abstract":"Synthesized data from generative models is increasingly considered as an alternative to human-annotated data for fine-tuning Large Language Models. This raises concerns about model collapse: a drop in performance of models fine-tuned on generated data. Considering that it is easier for both humans and machines to tell between good and bad examples than to generate high-quality samples, we investigate the use of feedback on synthesized data to prevent model collapse. We derive theoretical conditions under which a Gaussian mixture classification model can achieve asymptotically optimal performance when trained on feedback-augmented synthesized data, and provide supporting simulations for finite regimes. We illustrate our theoretical predictions on two practical problems: computing matrix eigenvalues with transformers and news summarization with large language models, which both undergo model collapse when trained on model-generated data. We show that training from feedback-augmented synthesized data, either by pruning incorrect predictions or by selecting the best of several guesses, can prevent model collapse, validating popular approaches like RLHF.","sentences":["Synthesized data from generative models is increasingly considered as an alternative to human-annotated data for fine-tuning Large Language Models.","This raises concerns about model collapse: a drop in performance of models fine-tuned on generated data.","Considering that it is easier for both humans and machines to tell between good and bad examples than to generate high-quality samples, we investigate the use of feedback on synthesized data to prevent model collapse.","We derive theoretical conditions under which a Gaussian mixture classification model can achieve asymptotically optimal performance when trained on feedback-augmented synthesized data, and provide supporting simulations for finite regimes.","We illustrate our theoretical predictions on two practical problems: computing matrix eigenvalues with transformers and news summarization with large language models, which both undergo model collapse when trained on model-generated data.","We show that training from feedback-augmented synthesized data, either by pruning incorrect predictions or by selecting the best of several guesses, can prevent model collapse, validating popular approaches like RLHF."],"url":"http://arxiv.org/abs/2406.07515v1","category":"cs.LG"}
{"created":"2024-06-11 17:40:31","title":"Understanding Visual Concepts Across Models","abstract":"Large multimodal models such as Stable Diffusion can generate, detect, and classify new visual concepts after fine-tuning just a single word embedding. Do models learn similar words for the same concepts (i.e. <orange-cat> = orange + cat)? We conduct a large-scale analysis on three state-of-the-art models in text-to-image generation, open-set object detection, and zero-shot classification, and find that new word embeddings are model-specific and non-transferable. Across 4,800 new embeddings trained for 40 diverse visual concepts on four standard datasets, we find perturbations within an $\\epsilon$-ball to any prior embedding that generate, detect, and classify an arbitrary concept. When these new embeddings are spliced into new models, fine-tuning that targets the original model is lost. We show popular soft prompt-tuning approaches find these perturbative solutions when applied to visual concept learning tasks, and embeddings for visual concepts are not transferable. Code for reproducing our work is available at: https://visual-words.github.io.","sentences":["Large multimodal models such as Stable Diffusion can generate, detect, and classify new visual concepts after fine-tuning just a single word embedding.","Do models learn similar words for the same concepts (i.e. <orange-cat> = orange + cat)?","We conduct a large-scale analysis on three state-of-the-art models in text-to-image generation, open-set object detection, and zero-shot classification, and find that new word embeddings are model-specific and non-transferable.","Across 4,800 new embeddings trained for 40 diverse visual concepts on four standard datasets, we find perturbations within an $\\epsilon$-ball to any prior embedding that generate, detect, and classify an arbitrary concept.","When these new embeddings are spliced into new models, fine-tuning that targets the original model is lost.","We show popular soft prompt-tuning approaches find these perturbative solutions when applied to visual concept learning tasks, and embeddings for visual concepts are not transferable.","Code for reproducing our work is available at: https://visual-words.github.io."],"url":"http://arxiv.org/abs/2406.07506v1","category":"cs.CV"}
{"created":"2024-06-11 17:39:46","title":"Just Because We Camp, Doesn't Mean We Should: The Ethics of Modelling Queer Voices","abstract":"Modern voice cloning models claim to be able to capture a diverse range of voices. We test the ability of a typical pipeline to capture the style known colloquially as \"gay voice\" and notice a homogenisation effect: synthesised speech is rated as sounding significantly \"less gay\" (by LGBTQ+ participants) than its corresponding ground-truth for speakers with \"gay voice\", but ratings actually increase for control speakers. Loss of \"gay voice\" has implications for accessibility. We also find that for speakers with \"gay voice\", loss of \"gay voice\" corresponds to lower similarity ratings.   However, we caution that improving the ability of such models to synthesise ``gay voice'' comes with a great number of risks. We use this pipeline as a starting point for a discussion on the ethics of modelling queer voices more broadly. Collecting \"clean\" queer data has safety and fairness ramifications, and the resulting technology may cause harms from mockery to death.","sentences":["Modern voice cloning models claim to be able to capture a diverse range of voices.","We test the ability of a typical pipeline to capture the style known colloquially as \"gay voice\" and notice a homogenisation effect: synthesised speech is rated as sounding significantly \"less gay\" (by LGBTQ+ participants) than its corresponding ground-truth for speakers with \"gay voice\", but ratings actually increase for control speakers.","Loss of \"gay voice\" has implications for accessibility.","We also find that for speakers with \"gay voice\", loss of \"gay voice\" corresponds to lower similarity ratings.   ","However, we caution that improving the ability of such models to synthesise ``gay voice'' comes with a great number of risks.","We use this pipeline as a starting point for a discussion on the ethics of modelling queer voices more broadly.","Collecting \"clean\" queer data has safety and fairness ramifications, and the resulting technology may cause harms from mockery to death."],"url":"http://arxiv.org/abs/2406.07504v1","category":"cs.CL"}
{"created":"2024-06-11 17:32:28","title":"A pilot protocol and cohort for the investigation of non-pathological variability in speech","abstract":"Background Speech-based biomarkers have potential as a means for regular, objective assessment of symptom severity, remotely and in-clinic in combination with advanced analytical models. However, the complex nature of speech and the often subtle changes associated with health mean that findings are highly dependent on methodological and cohort choices. These are often not reported adequately in studies investigating speech-based health assessment Objective To develop and apply an exemplar protocol to generate a pilot dataset of healthy speech with detailed metadata for the assessment of factors in the speech recording-analysis pipeline, including device choice, speech elicitation task and non-pathological variability. Methods We developed our collection protocol and choice of exemplar speech features based on a thematic literature review. Our protocol includes the elicitation of three different speech types. With a focus towards remote applications, we also choose to collect speech with three different microphone types. We developed a pipeline to extract a set of 14 exemplar speech features. Results We collected speech from 28 individuals three times in one day, repeated at the same times 8-11 weeks later, and from 25 healthy individuals three times in one week. Participant characteristics collected included sex, age, native language status and voice use habits of the participant. A preliminary set of 14 speech features covering timing, prosody, voice quality, articulation and spectral moment characteristics were extracted that provide a resource of normative values. Conclusions There are multiple methodological factors involved in the collection, processing and analysis of speech recordings. Consistent reporting and greater harmonisation of study protocols are urgently required to aid the translation of speech processing into clinical research and practice.","sentences":["Background Speech-based biomarkers have potential as a means for regular, objective assessment of symptom severity, remotely and in-clinic in combination with advanced analytical models.","However, the complex nature of speech and the often subtle changes associated with health mean that findings are highly dependent on methodological and cohort choices.","These are often not reported adequately in studies investigating speech-based health assessment Objective To develop and apply an exemplar protocol to generate a pilot dataset of healthy speech with detailed metadata for the assessment of factors in the speech recording-analysis pipeline, including device choice, speech elicitation task and non-pathological variability.","Methods We developed our collection protocol and choice of exemplar speech features based on a thematic literature review.","Our protocol includes the elicitation of three different speech types.","With a focus towards remote applications, we also choose to collect speech with three different microphone types.","We developed a pipeline to extract a set of 14 exemplar speech features.","Results We collected speech from 28 individuals three times in one day, repeated at the same times 8-11 weeks later, and from 25 healthy individuals three times in one week.","Participant characteristics collected included sex, age, native language status and voice use habits of the participant.","A preliminary set of 14 speech features covering timing, prosody, voice quality, articulation and spectral moment characteristics were extracted that provide a resource of normative values.","Conclusions There are multiple methodological factors involved in the collection, processing and analysis of speech recordings.","Consistent reporting and greater harmonisation of study protocols are urgently required to aid the translation of speech processing into clinical research and practice."],"url":"http://arxiv.org/abs/2406.07497v1","category":"cs.SD"}
{"created":"2024-06-11 17:32:21","title":"TextGrad: Automatic \"Differentiation\" via Text","abstract":"AI is undergoing a paradigm shift, with breakthroughs achieved by systems orchestrating multiple large language models (LLMs) and other complex components. As a result, developing principled and automated optimization methods for compound AI systems is one of the most important new challenges. Neural networks faced a similar challenge in its early days until backpropagation and automatic differentiation transformed the field by making optimization turn-key. Inspired by this, we introduce TextGrad, a powerful framework performing automatic ``differentiation'' via text. TextGrad backpropagates textual feedback provided by LLMs to improve individual components of a compound AI system. In our framework, LLMs provide rich, general, natural language suggestions to optimize variables in computation graphs, ranging from code snippets to molecular structures. TextGrad follows PyTorch's syntax and abstraction and is flexible and easy-to-use. It works out-of-the-box for a variety of tasks, where the users only provide the objective function without tuning components or prompts of the framework. We showcase TextGrad's effectiveness and generality across a diverse range of applications, from question answering and molecule optimization to radiotherapy treatment planning. Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\\%$ to $55\\%$, yields $20\\%$ relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity. TextGrad lays a foundation to accelerate the development of the next-generation of AI systems.","sentences":["AI is undergoing a paradigm shift, with breakthroughs achieved by systems orchestrating multiple large language models (LLMs) and other complex components.","As a result, developing principled and automated optimization methods for compound AI systems is one of the most important new challenges.","Neural networks faced a similar challenge in its early days until backpropagation and automatic differentiation transformed the field by making optimization turn-key.","Inspired by this, we introduce TextGrad, a powerful framework performing automatic ``differentiation'' via text.","TextGrad backpropagates textual feedback provided by LLMs to improve individual components of a compound AI system.","In our framework, LLMs provide rich, general, natural language suggestions to optimize variables in computation graphs, ranging from code snippets to molecular structures.","TextGrad follows PyTorch's syntax and abstraction and is flexible and easy-to-use.","It works out-of-the-box for a variety of tasks, where the users only provide the objective function without tuning components or prompts of the framework.","We showcase TextGrad's effectiveness and generality across a diverse range of applications, from question answering and molecule optimization to radiotherapy treatment planning.","Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\\%$ to $55\\%$, yields $20\\%$ relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity.","TextGrad lays a foundation to accelerate the development of the next-generation of AI systems."],"url":"http://arxiv.org/abs/2406.07496v1","category":"cs.CL"}
{"created":"2024-06-11 17:30:22","title":"CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization","abstract":"Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities. We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement. We observe that only a few datasets span across all subdomains. The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant.","sentences":["Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries.","Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges.","This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases.","We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models.","We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities.","We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement.","We observe that only a few datasets span across all subdomains.","The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines.","Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant."],"url":"http://arxiv.org/abs/2406.07494v2","category":"cs.CL"}
{"created":"2024-06-11 17:26:58","title":"PITCH: Productivity and Mental Well-being Coaching through Daily Conversational Interaction","abstract":"Efficient task planning is essential for productivity and mental well-being, yet individuals often struggle to create realistic plans and reflect upon their productivity. Leveraging the advancement in artificial intelligence (AI), conversational agents have emerged as a promising tool for enhancing productivity. Our work focuses on externalizing plans through conversation, aiming to solidify intentions and foster focused action, thereby positively impacting their productivity and mental well-being. We share our plan of designing a conversational agent to offer insightful questions and reflective prompts for increasing plan adherence by leveraging the social interactivity of natural conversations. Previous studies have shown the effectiveness of such agents, but many interventions remain static, leading to decreased user engagement over time. To address this limitation, we propose a novel rotation and context-aware prompting strategy, providing users with varied interventions daily. Our system, PITCH, utilizes large language models (LLMs) to facilitate externalization and reflection on daily plans. Through this study, we investigate the impact of externalizing tasks with conversational agents on productivity and mental well-being, and the effectiveness of a rotation strategy in maintaining user engagement.","sentences":["Efficient task planning is essential for productivity and mental well-being, yet individuals often struggle to create realistic plans and reflect upon their productivity.","Leveraging the advancement in artificial intelligence (AI), conversational agents have emerged as a promising tool for enhancing productivity.","Our work focuses on externalizing plans through conversation, aiming to solidify intentions and foster focused action, thereby positively impacting their productivity and mental well-being.","We share our plan of designing a conversational agent to offer insightful questions and reflective prompts for increasing plan adherence by leveraging the social interactivity of natural conversations.","Previous studies have shown the effectiveness of such agents, but many interventions remain static, leading to decreased user engagement over time.","To address this limitation, we propose a novel rotation and context-aware prompting strategy, providing users with varied interventions daily.","Our system, PITCH, utilizes large language models (LLMs) to facilitate externalization and reflection on daily plans.","Through this study, we investigate the impact of externalizing tasks with conversational agents on productivity and mental well-being, and the effectiveness of a rotation strategy in maintaining user engagement."],"url":"http://arxiv.org/abs/2406.07485v1","category":"cs.HC"}
{"created":"2024-06-11 17:24:30","title":"The end of multiple choice tests: using AI to enhance assessment","abstract":"Effective teaching relies on knowing what students know-or think they know. Revealing student thinking is challenging. Often used because of their ease of grading, even the best multiple choice (MC) tests, those using research based distractors (wrong answers) are intrinsically limited in the insights they provide due to two factors. When distractors do not reflect student beliefs they can be ignored, increasing the likelihood that the correct answer will be chosen by chance. Moreover, making the correct choice does not guarantee that the student understands why it is correct. To address these limitations, we recommend asking students to explain why they chose their answer, and why \"wrong\" choices are wrong. Using a discipline-trained artificial intelligence-based bot it is possible to analyze their explanations, identifying the concepts and scientific principles that maybe missing or misapplied. The bot also makes suggestions for how instructors can use these data to better guide student thinking. In a small \"proof of concept\" study, we tested this approach using questions from the Biology Concepts Instrument (BCI). The result was rapid, informative, and provided actionable feedback on student thinking. It appears that the use of AI addresses the weaknesses of conventional MC test. It seems likely that incorporating AI-analyzed formative assessments will lead to improved overall learning outcomes.","sentences":["Effective teaching relies on knowing what students know-or think they know.","Revealing student thinking is challenging.","Often used because of their ease of grading, even the best multiple choice (MC) tests, those using research based distractors (wrong answers) are intrinsically limited in the insights they provide due to two factors.","When distractors do not reflect student beliefs they can be ignored, increasing the likelihood that the correct answer will be chosen by chance.","Moreover, making the correct choice does not guarantee that the student understands why it is correct.","To address these limitations, we recommend asking students to explain why they chose their answer, and why \"wrong\" choices are wrong.","Using a discipline-trained artificial intelligence-based bot it is possible to analyze their explanations, identifying the concepts and scientific principles that maybe missing or misapplied.","The bot also makes suggestions for how instructors can use these data to better guide student thinking.","In a small \"proof of concept\" study, we tested this approach using questions from the Biology Concepts Instrument (BCI).","The result was rapid, informative, and provided actionable feedback on student thinking.","It appears that the use of AI addresses the weaknesses of conventional MC test.","It seems likely that incorporating AI-analyzed formative assessments will lead to improved overall learning outcomes."],"url":"http://arxiv.org/abs/2406.07481v1","category":"cs.CY"}
{"created":"2024-06-11 17:23:05","title":"Neutrino magnetic dipole portal with low energy neutrino nucleus scattering data","abstract":"Sterile neutrinos that couple to the Standard Model via the neutrino magnetic dipole portals have been extensively studied at various experiments. In this work, we scrutinize these interactions for sterile neutrinos in the mass range of $\\unit[0.1]{}-\\unit[50]{MeV}$ through the nuclear and electron recoils at various neutrino scattering experiments. For the $e$-flavor specific dipole portal, we demonstrate that Dresden-II can provide leading constraints for $m_N \\lesssim \\unit[0.5]{MeV}$, setting aside currently unresolved theoretical uncertainties. For the $\\mu$-flavor case, we show that the COHERENT experiment can probe a unique parameter region for $m_N$ in the range of $\\unit[10]{}-\\unit[40]{MeV}$ with the full dataset collected by the CsI[Na] scintillation detector, including both the energy and timing structure of the neutrino beam. We also present limits on the parameter regions of the $\\tau$-flavor dipole portal using measurements of the solar neutrino flux from dark matter direct detection experiments.","sentences":["Sterile neutrinos that couple to the Standard Model via the neutrino magnetic dipole portals have been extensively studied at various experiments.","In this work, we scrutinize these interactions for sterile neutrinos in the mass range of $\\unit[0.1]{}-\\unit[50]{MeV}$ through the nuclear and electron recoils at various neutrino scattering experiments.","For the $e$-flavor specific dipole portal, we demonstrate that Dresden-II can provide leading constraints for $m_N \\lesssim \\unit[0.5]{MeV}$, setting aside currently unresolved theoretical uncertainties.","For the $\\mu$-flavor case, we show that the COHERENT experiment can probe a unique parameter region for $m_N$ in the range of $\\unit[10]{}-\\unit[40]{MeV}$ with the full dataset collected by the CsI[Na] scintillation detector, including both the energy and timing structure of the neutrino beam.","We also present limits on the parameter regions of the $\\tau$-flavor dipole portal using measurements of the solar neutrino flux from dark matter direct detection experiments."],"url":"http://arxiv.org/abs/2406.07477v1","category":"hep-ph"}
{"created":"2024-06-11 17:22:23","title":"VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs","abstract":"In this paper, we present the VideoLLaMA 2, a set of Video Large Language Models (Video-LLMs) designed to enhance spatial-temporal modeling and audio understanding in video and audio-oriented tasks. Building upon its predecessor, VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC) connector, which effectively captures the intricate spatial and temporal dynamics of video data. Additionally, we integrate an Audio Branch into the model through joint training, thereby enriching the multimodal understanding capabilities of the model by seamlessly incorporating audio cues. Comprehensive evaluations on multiple-choice video question answering (MC-VQA), open-ended video question answering (OE-VQA), and video captioning (VC) tasks demonstrate that VideoLLaMA 2 consistently achieves competitive results among open-source models and even gets close to some proprietary models on several benchmarks. Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and audio-video question-answering (AQA & OE-AVQA) benchmarks over existing models. These advancements underline VideoLLaMA 2's superior performance in multimodal comprehension, setting a new standard for intelligent video analysis systems. All models are public to facilitate further research.","sentences":["In this paper, we present the VideoLLaMA 2, a set of Video Large Language Models (Video-LLMs) designed to enhance spatial-temporal modeling and audio understanding in video and audio-oriented tasks.","Building upon its predecessor, VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC) connector, which effectively captures the intricate spatial and temporal dynamics of video data.","Additionally, we integrate an Audio Branch into the model through joint training, thereby enriching the multimodal understanding capabilities of the model by seamlessly incorporating audio cues.","Comprehensive evaluations on multiple-choice video question answering (MC-VQA), open-ended video question answering (OE-VQA), and video captioning (VC) tasks demonstrate that VideoLLaMA 2 consistently achieves competitive results among open-source models and even gets close to some proprietary models on several benchmarks.","Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and audio-video question-answering (AQA & OE-AVQA) benchmarks over existing models.","These advancements underline VideoLLaMA 2's superior performance in multimodal comprehension, setting a new standard for intelligent video analysis systems.","All models are public to facilitate further research."],"url":"http://arxiv.org/abs/2406.07476v1","category":"cs.CV"}
{"created":"2024-06-11 17:20:28","title":"Quantifying Local Model Validity using Active Learning","abstract":"Real-world applications of machine learning models are often subject to legal or policy-based regulations. Some of these regulations require ensuring the validity of the model, i.e., the approximation error being smaller than a threshold. A global metric is generally too insensitive to determine the validity of a specific prediction, whereas evaluating local validity is costly since it requires gathering additional data.We propose learning the model error to acquire a local validity estimate while reducing the amount of required data through active learning. Using model validation benchmarks, we provide empirical evidence that the proposed method can lead to an error model with sufficient discriminative properties using a relatively small amount of data. Furthermore, an increased sensitivity to local changes of the validity bounds compared to alternative approaches is demonstrated.","sentences":["Real-world applications of machine learning models are often subject to legal or policy-based regulations.","Some of these regulations require ensuring the validity of the model, i.e., the approximation error being smaller than a threshold.","A global metric is generally too insensitive to determine the validity of a specific prediction, whereas evaluating local validity is costly since it requires gathering additional data.","We propose learning the model error to acquire a local validity estimate while reducing the amount of required data through active learning.","Using model validation benchmarks, we provide empirical evidence that the proposed method can lead to an error model with sufficient discriminative properties using a relatively small amount of data.","Furthermore, an increased sensitivity to local changes of the validity bounds compared to alternative approaches is demonstrated."],"url":"http://arxiv.org/abs/2406.07474v1","category":"stat.ML"}
{"created":"2024-06-11 17:18:11","title":"OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding","abstract":"Surgical scene perception via videos are critical for advancing robotic surgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology. However, the scarcity of diverse and richly annotated video datasets has hindered the development of intelligent systems for surgical workflow analysis. Existing datasets for surgical workflow analysis, which typically face challenges such as small scale, a lack of diversity in surgery and phase categories, and the absence of time-localized annotations, limit the requirements for action understanding and model generalization validation in complex and diverse real-world surgical scenarios. To address this gap, we introduce OphNet, a large-scale, expert-annotated video benchmark for ophthalmic surgical workflow understanding. OphNet features: 1) A diverse collection of 2,278 surgical videos spanning 66 types of cataract, glaucoma, and corneal surgeries, with detailed annotations for 102 unique surgical phases and 150 granular operations; 2) It offers sequential and hierarchical annotations for each surgery, phase, and operation, enabling comprehensive understanding and improved interpretability; 3) Moreover, OphNet provides time-localized annotations, facilitating temporal localization and prediction tasks within surgical workflows. With approximately 205 hours of surgical videos, OphNet is about 20 times larger than the largest existing surgical workflow analysis benchmark. Our dataset and code have been made available at: \\url{https://github.com/minghu0830/OphNet-benchmark}.","sentences":["Surgical scene perception via videos are critical for advancing robotic surgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology.","However, the scarcity of diverse and richly annotated video datasets has hindered the development of intelligent systems for surgical workflow analysis.","Existing datasets for surgical workflow analysis, which typically face challenges such as small scale, a lack of diversity in surgery and phase categories, and the absence of time-localized annotations, limit the requirements for action understanding and model generalization validation in complex and diverse real-world surgical scenarios.","To address this gap, we introduce OphNet, a large-scale, expert-annotated video benchmark for ophthalmic surgical workflow understanding.","OphNet features: 1) A diverse collection of 2,278 surgical videos spanning 66 types of cataract, glaucoma, and corneal surgeries, with detailed annotations for 102 unique surgical phases and 150 granular operations; 2) It offers sequential and hierarchical annotations for each surgery, phase, and operation, enabling comprehensive understanding and improved interpretability; 3) Moreover, OphNet provides time-localized annotations, facilitating temporal localization and prediction tasks within surgical workflows.","With approximately 205 hours of surgical videos, OphNet is about 20 times larger than the largest existing surgical workflow analysis benchmark.","Our dataset and code have been made available at: \\url{https://github.com/minghu0830/OphNet-benchmark}."],"url":"http://arxiv.org/abs/2406.07471v2","category":"cs.CV"}
{"created":"2024-06-11 17:59:53","title":"Image and Video Tokenization with Binary Spherical Quantization","abstract":"We propose a new transformer-based image and video tokenizer with Binary Spherical Quantization (BSQ). BSQ projects the high-dimensional visual embedding to a lower-dimensional hypersphere and then applies binary quantization. BSQ is (1) parameter-efficient without an explicit codebook, (2) scalable to arbitrary token dimensions, and (3) compact: compressing visual data by up to 100$\\times$ with minimal distortion. Our tokenizer uses a transformer encoder and decoder with simple block-wise causal masking to support variable-length videos as input. The resulting BSQ-ViT achieves state-of-the-art visual reconstruction quality on image and video reconstruction benchmarks with 2.4$\\times$ throughput compared to the best prior methods. Furthermore, by learning an autoregressive prior for adaptive arithmetic coding, BSQ-ViT achieves comparable results on video compression with state-of-the-art video compression standards. BSQ-ViT also enables masked language models to achieve competitive image synthesis quality to GAN- and diffusion-based methods.","sentences":["We propose a new transformer-based image and video tokenizer with Binary Spherical Quantization (BSQ).","BSQ projects the high-dimensional visual embedding to a lower-dimensional hypersphere and then applies binary quantization.","BSQ is (1) parameter-efficient without an explicit codebook, (2) scalable to arbitrary token dimensions, and (3) compact: compressing visual data by up to 100$\\times$ with minimal distortion.","Our tokenizer uses a transformer encoder and decoder with simple block-wise causal masking to support variable-length videos as input.","The resulting BSQ-ViT achieves state-of-the-art visual reconstruction quality on image and video reconstruction benchmarks with 2.4$\\times$ throughput compared to the best prior methods.","Furthermore, by learning an autoregressive prior for adaptive arithmetic coding, BSQ-ViT achieves comparable results on video compression with state-of-the-art video compression standards.","BSQ-ViT also enables masked language models to achieve competitive image synthesis quality to GAN- and diffusion-based methods."],"url":"http://arxiv.org/abs/2406.07548v1","category":"cs.CV"}
{"created":"2024-06-11 17:55:25","title":"MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation","abstract":"Model merging has emerged as an effective approach to combine multiple single-task models, fine-tuned from the same pre-trained model, into a multitask model. This process typically involves computing a weighted average of the model parameters without any additional training. Existing model-merging methods focus on enhancing average task accuracy. However, interference and conflicts between the objectives of different tasks can lead to trade-offs during model merging. In real-world applications, a set of solutions with various trade-offs can be more informative, helping practitioners make decisions based on diverse preferences. In this paper, we introduce a novel low-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP identifies a Pareto set of scaling coefficients for merging multiple models to reflect the trade-offs. The core component of MAP is approximating the evaluation metrics of the various tasks using a quadratic approximation surrogate model derived from a pre-selected set of scaling coefficients, enabling amortized inference. Experimental results on vision and natural language processing tasks show that MAP can accurately identify the Pareto front. To further reduce the required computation of MAP, we propose (1) a Bayesian adaptive sampling algorithm and (2) a nested merging scheme with multiple stages.","sentences":["Model merging has emerged as an effective approach to combine multiple single-task models, fine-tuned from the same pre-trained model, into a multitask model.","This process typically involves computing a weighted average of the model parameters without any additional training.","Existing model-merging methods focus on enhancing average task accuracy.","However, interference and conflicts between the objectives of different tasks can lead to trade-offs during model merging.","In real-world applications, a set of solutions with various trade-offs can be more informative, helping practitioners make decisions based on diverse preferences.","In this paper, we introduce a novel low-compute algorithm, Model Merging with Amortized Pareto Front (MAP).","MAP identifies a Pareto set of scaling coefficients for merging multiple models to reflect the trade-offs.","The core component of MAP is approximating the evaluation metrics of the various tasks using a quadratic approximation surrogate model derived from a pre-selected set of scaling coefficients, enabling amortized inference.","Experimental results on vision and natural language processing tasks show that MAP can accurately identify the Pareto front.","To further reduce the required computation of MAP, we propose (1) a Bayesian adaptive sampling algorithm and (2) a nested merging scheme with multiple stages."],"url":"http://arxiv.org/abs/2406.07529v1","category":"cs.LG"}
{"created":"2024-06-11 17:47:27","title":"Instant 3D Human Avatar Generation using Image Diffusion Models","abstract":"We present AvatarPopUp, a method for fast, high quality 3D human avatar generation from different input modalities, such as images and text prompts and with control over the generated pose and shape. The common theme is the use of diffusion-based image generation networks that are specialized for each particular task, followed by a 3D lifting network. We purposefully decouple the generation from the 3D modeling which allow us to leverage powerful image synthesis priors, trained on billions of text-image pairs. We fine-tune latent diffusion networks with additional image conditioning to solve tasks such as image generation and back-view prediction, and to support qualitatively different multiple 3D hypotheses. Our partial fine-tuning approach allows to adapt the networks for each task without inducing catastrophic forgetting. In our experiments, we demonstrate that our method produces accurate, high-quality 3D avatars with diverse appearance that respect the multimodal text, image, and body control signals. Our approach can produce a 3D model in as few as 2 seconds, a four orders of magnitude speedup w.r.t. the vast majority of existing methods, most of which solve only a subset of our tasks, and with fewer controls, thus enabling applications that require the controlled 3D generation of human avatars at scale. The project website can be found at https://www.nikoskolot.com/avatarpopup/.","sentences":["We present AvatarPopUp, a method for fast, high quality 3D human avatar generation from different input modalities, such as images and text prompts and with control over the generated pose and shape.","The common theme is the use of diffusion-based image generation networks that are specialized for each particular task, followed by a 3D lifting network.","We purposefully decouple the generation from the 3D modeling which allow us to leverage powerful image synthesis priors, trained on billions of text-image pairs.","We fine-tune latent diffusion networks with additional image conditioning to solve tasks such as image generation and back-view prediction, and to support qualitatively different multiple 3D hypotheses.","Our partial fine-tuning approach allows to adapt the networks for each task without inducing catastrophic forgetting.","In our experiments, we demonstrate that our method produces accurate, high-quality 3D avatars with diverse appearance that respect the multimodal text, image, and body control signals.","Our approach can produce a 3D model in as few as 2 seconds, a four orders of magnitude speedup w.r.t.","the vast majority of existing methods, most of which solve only a subset of our tasks, and with fewer controls, thus enabling applications that require the controlled 3D generation of human avatars at scale.","The project website can be found at https://www.nikoskolot.com/avatarpopup/."],"url":"http://arxiv.org/abs/2406.07516v1","category":"cs.CV"}
{"created":"2024-06-11 17:29:51","title":"Transition from decaying to decayless kink oscillations of solar coronal loops","abstract":"The transition of an impulsively excited kink oscillation of a solar coronal loop to an oscillation with a stationary amplitude, i.e., the damping pattern, is determined using the low-dimensional self-oscillation model. In the model, the decayless kink oscillations are sustained by the interaction of the oscillating loop with an external quasi-steady flow. The analytical solution is based on the assumption that the combined effect of the effective dissipation, for example, by resonant absorption, and interaction with an external flow, is weak. The effect is characterised by a dimensionless coupling parameter. The damping pattern is found to depend upon the initial amplitude and the coupling parameter. The approximate expression shows a good agreement with a numerical solution of the self-oscillation equation. The plausibility of the established damping pattern is demonstrated by an observational example. Notably, the damping pattern is not exponential, and the characteristic decay time is different from the time determined by the traditionally used exponential damping fit. Implications of this finding for seismology of the solar coronal plasmas are discussed. In particular, it is suggested that a very rapid, in less than the oscillation period, decay of the oscillation to the stationary level, achieved for larger values of the coupling parameter, can explain the relative rareness of the kink oscillation events.","sentences":["The transition of an impulsively excited kink oscillation of a solar coronal loop to an oscillation with a stationary amplitude, i.e., the damping pattern, is determined using the low-dimensional self-oscillation model.","In the model, the decayless kink oscillations are sustained by the interaction of the oscillating loop with an external quasi-steady flow.","The analytical solution is based on the assumption that the combined effect of the effective dissipation, for example, by resonant absorption, and interaction with an external flow, is weak.","The effect is characterised by a dimensionless coupling parameter.","The damping pattern is found to depend upon the initial amplitude and the coupling parameter.","The approximate expression shows a good agreement with a numerical solution of the self-oscillation equation.","The plausibility of the established damping pattern is demonstrated by an observational example.","Notably, the damping pattern is not exponential, and the characteristic decay time is different from the time determined by the traditionally used exponential damping fit.","Implications of this finding for seismology of the solar coronal plasmas are discussed.","In particular, it is suggested that a very rapid, in less than the oscillation period, decay of the oscillation to the stationary level, achieved for larger values of the coupling parameter, can explain the relative rareness of the kink oscillation events."],"url":"http://arxiv.org/abs/2406.07490v1","category":"astro-ph.SR"}
{"created":"2024-06-11 17:27:23","title":"GLAD: Towards Better Reconstruction with Global and Local Adaptive Diffusion Models for Unsupervised Anomaly Detection","abstract":"Diffusion models have shown superior performance on unsupervised anomaly detection tasks. Since trained with normal data only, diffusion models tend to reconstruct normal counterparts of test images with certain noises added. However, these methods treat all potential anomalies equally, which may cause two main problems. From the global perspective, the difficulty of reconstructing images with different anomalies is uneven. Therefore, instead of utilizing the same setting for all samples, we propose to predict a particular denoising step for each sample by evaluating the difference between image contents and the priors extracted from diffusion models. From the local perspective, reconstructing abnormal regions differs from normal areas even in the same image. Theoretically, the diffusion model predicts a noise for each step, typically following a standard Gaussian distribution. However, due to the difference between the anomaly and its potential normal counterpart, the predicted noise in abnormal regions will inevitably deviate from the standard Gaussian distribution. To this end, we propose introducing synthetic abnormal samples in training to encourage the diffusion models to break through the limitation of standard Gaussian distribution, and a spatial-adaptive feature fusion scheme is utilized during inference. With the above modifications, we propose a global and local adaptive diffusion model (abbreviated to GLAD) for unsupervised anomaly detection, which introduces appealing flexibility and achieves anomaly-free reconstruction while retaining as much normal information as possible. Extensive experiments are conducted on three commonly used anomaly detection datasets (MVTec-AD, MPDD, and VisA) and a printed circuit board dataset (PCB-Bank) we integrated, showing the effectiveness of the proposed method.","sentences":["Diffusion models have shown superior performance on unsupervised anomaly detection tasks.","Since trained with normal data only, diffusion models tend to reconstruct normal counterparts of test images with certain noises added.","However, these methods treat all potential anomalies equally, which may cause two main problems.","From the global perspective, the difficulty of reconstructing images with different anomalies is uneven.","Therefore, instead of utilizing the same setting for all samples, we propose to predict a particular denoising step for each sample by evaluating the difference between image contents and the priors extracted from diffusion models.","From the local perspective, reconstructing abnormal regions differs from normal areas even in the same image.","Theoretically, the diffusion model predicts a noise for each step, typically following a standard Gaussian distribution.","However, due to the difference between the anomaly and its potential normal counterpart, the predicted noise in abnormal regions will inevitably deviate from the standard Gaussian distribution.","To this end, we propose introducing synthetic abnormal samples in training to encourage the diffusion models to break through the limitation of standard Gaussian distribution, and a spatial-adaptive feature fusion scheme is utilized during inference.","With the above modifications, we propose a global and local adaptive diffusion model (abbreviated to GLAD) for unsupervised anomaly detection, which introduces appealing flexibility and achieves anomaly-free reconstruction while retaining as much normal information as possible.","Extensive experiments are conducted on three commonly used anomaly detection datasets (MVTec-AD, MPDD, and VisA) and a printed circuit board dataset (PCB-Bank) we integrated, showing the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2406.07487v1","category":"cs.CV"}
{"created":"2024-06-11 17:26:14","title":"Towards Generalized Hydrological Forecasting using Transformer Models for 120-Hour Streamflow Prediction","abstract":"This study explores the efficacy of a Transformer model for 120-hour streamflow prediction across 125 diverse locations in Iowa, US. Utilizing data from the preceding 72 hours, including precipitation, evapotranspiration, and discharge values, we developed a generalized model to predict future streamflow. Our approach contrasts with traditional methods that typically rely on location-specific models. We benchmarked the Transformer model's performance against three deep learning models (LSTM, GRU, and Seq2Seq) and the Persistence approach, employing Nash-Sutcliffe Efficiency (NSE), Kling-Gupta Efficiency (KGE), Pearson's r, and Normalized Root Mean Square Error (NRMSE) as metrics. The study reveals the Transformer model's superior performance, maintaining higher median NSE and KGE scores and exhibiting the lowest NRMSE values. This indicates its capability to accurately simulate and predict streamflow, adapting effectively to varying hydrological conditions and geographical variances. Our findings underscore the Transformer model's potential as an advanced tool in hydrological modeling, offering significant improvements over traditional and contemporary approaches.","sentences":["This study explores the efficacy of a Transformer model for 120-hour streamflow prediction across 125 diverse locations in Iowa, US.","Utilizing data from the preceding 72 hours, including precipitation, evapotranspiration, and discharge values, we developed a generalized model to predict future streamflow.","Our approach contrasts with traditional methods that typically rely on location-specific models.","We benchmarked the Transformer model's performance against three deep learning models (LSTM, GRU, and Seq2Seq) and the Persistence approach, employing Nash-Sutcliffe Efficiency (NSE), Kling-Gupta Efficiency (KGE), Pearson's r, and Normalized Root Mean Square Error (NRMSE) as metrics.","The study reveals the Transformer model's superior performance, maintaining higher median NSE and KGE scores and exhibiting the lowest NRMSE values.","This indicates its capability to accurately simulate and predict streamflow, adapting effectively to varying hydrological conditions and geographical variances.","Our findings underscore the Transformer model's potential as an advanced tool in hydrological modeling, offering significant improvements over traditional and contemporary approaches."],"url":"http://arxiv.org/abs/2406.07484v1","category":"cs.LG"}
{"created":"2024-06-11 17:20:01","title":"Choreographing the Rhythms of Observation: Dynamics for Ranged Observer Bipartite-Unipartite SpatioTemporal (ROBUST) Networks","abstract":"Existing network analysis methods struggle to optimize observer placements in dynamic environments with limited visibility. This dissertation introduces the novel ROBUST (Ranged Observer Bipartite-Unipartite SpatioTemporal) framework, offering a significant advancement in modeling, analyzing, and optimizing observer networks within complex spatiotemporal domains. ROBUST leverages a unique bipartite-unipartite approach, distinguishing between observer and observable entities while incorporating spatial constraints and temporal dynamics.   This research extends spatiotemporal network theory by introducing novel graph-based measures, including myopic degree, spatial closeness centrality, and edge length proportion. These measures, coupled with advanced clustering techniques like Proximal Recurrence, provide insights into network structure, resilience, and the effectiveness of observer placements. The ROBUST framework demonstrates superior resource allocation and strategic responsiveness compared to conventional models. Case studies in oceanographic monitoring, urban safety networks, and multi-agent path planning showcases its practical applicability and adaptability. Results demonstrate significant improvements in coverage, response times, and overall network efficiency.   This work paves the way for future research in incorporating imperfect knowledge, refining temporal pathing methodologies, and expanding the scope of applications. By bridging theoretical advancements with practical solutions, ROBUST stands as a significant contribution to the field, promising to inform and inspire ongoing and future endeavors in network optimization and multi-agent system planning.","sentences":["Existing network analysis methods struggle to optimize observer placements in dynamic environments with limited visibility.","This dissertation introduces the novel ROBUST (Ranged Observer Bipartite-Unipartite SpatioTemporal) framework, offering a significant advancement in modeling, analyzing, and optimizing observer networks within complex spatiotemporal domains.","ROBUST leverages a unique bipartite-unipartite approach, distinguishing between observer and observable entities while incorporating spatial constraints and temporal dynamics.   ","This research extends spatiotemporal network theory by introducing novel graph-based measures, including myopic degree, spatial closeness centrality, and edge length proportion.","These measures, coupled with advanced clustering techniques like Proximal Recurrence, provide insights into network structure, resilience, and the effectiveness of observer placements.","The ROBUST framework demonstrates superior resource allocation and strategic responsiveness compared to conventional models.","Case studies in oceanographic monitoring, urban safety networks, and multi-agent path planning showcases its practical applicability and adaptability.","Results demonstrate significant improvements in coverage, response times, and overall network efficiency.   ","This work paves the way for future research in incorporating imperfect knowledge, refining temporal pathing methodologies, and expanding the scope of applications.","By bridging theoretical advancements with practical solutions, ROBUST stands as a significant contribution to the field, promising to inform and inspire ongoing and future endeavors in network optimization and multi-agent system planning."],"url":"http://arxiv.org/abs/2406.07473v1","category":"cs.MA"}
{"created":"2024-06-11 17:09:31","title":"Reconfigurable Intelligent Surfaces in Dynamic Rich Scattering Environments: BiLSTM-Based Optimization for Accurate User Localization","abstract":"The integration of reconfigurable intelligent surfaces (RIS) in wireless environments offers channel programmability and dynamic control over propagation channels, which is expected to play a crucial role in sixth generation (6G) networks. The majority of RIS-related research has focused on simpler, quasi-free-space conditions, where wireless channels are typically modeled analytically. However, many practical localization scenarios unfold in environments characterized by rich scattering that also change over time. These dynamic and complex conditions pose significant challenges in determining the optimal RIS configuration to maximize localization accuracy. In this paper, we present our approach to overcoming this challenge. This paper introduces a novel approach that leverages a bidirectional long-short term memory (biLSTM) network, trained with a simulator that accurately reflects wave physics, to capture the relationship between wireless channels and the RIS configuration under dynamic, rich-scattering conditions. We use this approach to optimize RIS configurations for enhanced user equipment (UE) localization, measured by mean squared error (MSE). Through extensive simulations, we demonstrate that our approach adapts RIS configurations to significantly improve localization accuracy in such dynamically changing rich scattering environments.","sentences":["The integration of reconfigurable intelligent surfaces (RIS) in wireless environments offers channel programmability and dynamic control over propagation channels, which is expected to play a crucial role in sixth generation (6G) networks.","The majority of RIS-related research has focused on simpler, quasi-free-space conditions, where wireless channels are typically modeled analytically.","However, many practical localization scenarios unfold in environments characterized by rich scattering that also change over time.","These dynamic and complex conditions pose significant challenges in determining the optimal RIS configuration to maximize localization accuracy.","In this paper, we present our approach to overcoming this challenge.","This paper introduces a novel approach that leverages a bidirectional long-short term memory (biLSTM) network, trained with a simulator that accurately reflects wave physics, to capture the relationship between wireless channels and the RIS configuration under dynamic, rich-scattering conditions.","We use this approach to optimize RIS configurations for enhanced user equipment (UE) localization, measured by mean squared error (MSE).","Through extensive simulations, we demonstrate that our approach adapts RIS configurations to significantly improve localization accuracy in such dynamically changing rich scattering environments."],"url":"http://arxiv.org/abs/2406.07463v1","category":"eess.SP"}
{"created":"2024-06-11 17:01:45","title":"fKAN: Fractional Kolmogorov-Arnold Networks with trainable Jacobi basis functions","abstract":"Recent advancements in neural network design have given rise to the development of Kolmogorov-Arnold Networks (KANs), which enhance speed, interpretability, and precision. This paper presents the Fractional Kolmogorov-Arnold Network (fKAN), a novel neural network architecture that incorporates the distinctive attributes of KANs with a trainable adaptive fractional-orthogonal Jacobi function as its basis function. By leveraging the unique mathematical properties of fractional Jacobi functions, including simple derivative formulas, non-polynomial behavior, and activity for both positive and negative input values, this approach ensures efficient learning and enhanced accuracy. The proposed architecture is evaluated across a range of tasks in deep learning and physics-informed deep learning. Precision is tested on synthetic regression data, image classification, image denoising, and sentiment analysis. Additionally, the performance is measured on various differential equations, including ordinary, partial, and fractional delay differential equations. The results demonstrate that integrating fractional Jacobi functions into KANs significantly improves training speed and performance across diverse fields and applications.","sentences":["Recent advancements in neural network design have given rise to the development of Kolmogorov-Arnold Networks (KANs), which enhance speed, interpretability, and precision.","This paper presents the Fractional Kolmogorov-Arnold Network (fKAN), a novel neural network architecture that incorporates the distinctive attributes of KANs with a trainable adaptive fractional-orthogonal Jacobi function as its basis function.","By leveraging the unique mathematical properties of fractional Jacobi functions, including simple derivative formulas, non-polynomial behavior, and activity for both positive and negative input values, this approach ensures efficient learning and enhanced accuracy.","The proposed architecture is evaluated across a range of tasks in deep learning and physics-informed deep learning.","Precision is tested on synthetic regression data, image classification, image denoising, and sentiment analysis.","Additionally, the performance is measured on various differential equations, including ordinary, partial, and fractional delay differential equations.","The results demonstrate that integrating fractional Jacobi functions into KANs significantly improves training speed and performance across diverse fields and applications."],"url":"http://arxiv.org/abs/2406.07456v1","category":"cs.LG"}
{"created":"2024-06-11 17:01:41","title":"Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis","abstract":"In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model. We developed a model-free RLHF best policy identification algorithm, called $\\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM). The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one. $\\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which resembles the result in classic RL, where $c_{\\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size. Moreover, $\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach. Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift.","sentences":["In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model.","We developed a model-free RLHF best policy identification algorithm, called $\\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM).","The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one.","$\\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which resembles the result in classic RL, where $c_{\\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size.","Moreover, $\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach.","Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift."],"url":"http://arxiv.org/abs/2406.07455v1","category":"cs.LG"}
{"created":"2024-06-11 16:34:02","title":"Learning Domain-Invariant Features for Out-of-Context News Detection","abstract":"Multimodal out-of-context news is a common type of misinformation on online media platforms. This involves posting a caption, alongside an invalid out-of-context news image. Reflecting its importance, researchers have developed models to detect such misinformation. However, a common limitation of these models is that they only consider the scenario where pre-labeled data is available for each domain, failing to address the out-of-context news detection on unlabeled domains (e.g., unverified news on new topics or agencies). In this work, we therefore focus on domain adaptive out-of-context news detection. In order to effectively adapt the detection model to unlabeled news topics or agencies, we propose ConDA-TTA (Contrastive Domain Adaptation with Test-Time Adaptation) which applies contrastive learning and maximum mean discrepancy (MMD) to learn the domain-invariant feature. In addition, it leverages target domain statistics during test-time to further assist domain adaptation. Experimental results show that our approach outperforms baselines in 5 out of 7 domain adaptation settings on two public datasets, by as much as 2.93% in F1 and 2.08% in accuracy.","sentences":["Multimodal out-of-context news is a common type of misinformation on online media platforms.","This involves posting a caption, alongside an invalid out-of-context news image.","Reflecting its importance, researchers have developed models to detect such misinformation.","However, a common limitation of these models is that they only consider the scenario where pre-labeled data is available for each domain, failing to address the out-of-context news detection on unlabeled domains (e.g., unverified news on new topics or agencies).","In this work, we therefore focus on domain adaptive out-of-context news detection.","In order to effectively adapt the detection model to unlabeled news topics or agencies, we propose ConDA-TTA (Contrastive Domain Adaptation with Test-Time Adaptation) which applies contrastive learning and maximum mean discrepancy (MMD) to learn the domain-invariant feature.","In addition, it leverages target domain statistics during test-time to further assist domain adaptation.","Experimental results show that our approach outperforms baselines in 5 out of 7 domain adaptation settings on two public datasets, by as much as 2.93% in F1 and 2.08% in accuracy."],"url":"http://arxiv.org/abs/2406.07430v1","category":"cs.CL"}
{"created":"2024-06-11 16:30:30","title":"GemNet: Menu-Based, Strategy-Proof Multi-Bidder Auctions Through Deep Learning","abstract":"Differentiable economics uses deep learning for automated mechanism design. Despite strong progress, it has remained an open problem to learn multi-bidder, general, and fully strategy-proof (SP) auctions. We introduce GEneral Menu-based NETwork (GemNet), which significantly extends the menu-based approach of RochetNet [D\\\"utting et al., 2023] to the multi-bidder setting. The challenge in achieving SP is to learn bidder-independent menus that are feasible, so that the optimal menu choices for each bidder do not over-allocate items when taken together (we call this menu compatibility). GemNet penalizes the failure of menu compatibility during training, and transforms learned menus after training through price changes, by considering a set of discretized bidder values and reasoning about Lipschitz smoothness to guarantee menu compatibility on the entire value space. This approach is general, leaving undisturbed trained menus that already satisfy menu compatibility and reducing to RochetNet for a single bidder. Mixed-integer linear programs are used for menu transforms and through a number of optimizations, including adaptive grids and methods to skip menu elements, we scale to large auction design problems. GemNet learns auctions with better revenue than affine maximization methods, achieves exact SP whereas previous general multi-bidder methods are approximately SP, and offers greatly enhanced interpretability.","sentences":["Differentiable economics uses deep learning for automated mechanism design.","Despite strong progress, it has remained an open problem to learn multi-bidder, general, and fully strategy-proof (SP) auctions.","We introduce GEneral Menu-based NETwork (GemNet), which significantly extends the menu-based approach of RochetNet [D\\\"utting et al., 2023] to the multi-bidder setting.","The challenge in achieving SP is to learn bidder-independent menus that are feasible, so that the optimal menu choices for each bidder do not over-allocate items when taken together (we call this menu compatibility).","GemNet penalizes the failure of menu compatibility during training, and transforms learned menus after training through price changes, by considering a set of discretized bidder values and reasoning about Lipschitz smoothness to guarantee menu compatibility on the entire value space.","This approach is general, leaving undisturbed trained menus that already satisfy menu compatibility and reducing to RochetNet for a single bidder.","Mixed-integer linear programs are used for menu transforms and through a number of optimizations, including adaptive grids and methods to skip menu elements, we scale to large auction design problems.","GemNet learns auctions with better revenue than affine maximization methods, achieves exact SP whereas previous general multi-bidder methods are approximately SP, and offers greatly enhanced interpretability."],"url":"http://arxiv.org/abs/2406.07428v1","category":"cs.GT"}
{"created":"2024-06-11 16:21:57","title":"Graph Reasoning for Explainable Cold Start Recommendation","abstract":"The cold start problem, where new users or items have no interaction history, remains a critical challenge in recommender systems (RS). A common solution involves using Knowledge Graphs (KG) to train entity embeddings or Graph Neural Networks (GNNs). Since KGs incorporate auxiliary data and not just user/item interactions, these methods can make relevant recommendations for cold users or items. Graph Reasoning (GR) methods, however, find paths from users to items to recommend using relations in the KG and, in the context of RS, have been used for interpretability. In this study, we propose GRECS: a framework for adapting GR to cold start recommendations. By utilizing explicit paths starting for users rather than relying only on entity embeddings, GRECS can find items corresponding to users' preferences by navigating the graph, even when limited information about users is available. Our experiments show that GRECS mitigates the cold start problem and outperforms competitive baselines across 5 standard datasets while being explainable. This study highlights the potential of GR for developing explainable recommender systems better suited for managing cold users and items.","sentences":["The cold start problem, where new users or items have no interaction history, remains a critical challenge in recommender systems (RS).","A common solution involves using Knowledge Graphs (KG) to train entity embeddings or Graph Neural Networks (GNNs).","Since KGs incorporate auxiliary data and not just user/item interactions, these methods can make relevant recommendations for cold users or items.","Graph Reasoning (GR) methods, however, find paths from users to items to recommend using relations in the KG and, in the context of RS, have been used for interpretability.","In this study, we propose GRECS: a framework for adapting GR to cold start recommendations.","By utilizing explicit paths starting for users rather than relying only on entity embeddings, GRECS can find items corresponding to users' preferences by navigating the graph, even when limited information about users is available.","Our experiments show that GRECS mitigates the cold start problem and outperforms competitive baselines across 5 standard datasets while being explainable.","This study highlights the potential of GR for developing explainable recommender systems better suited for managing cold users and items."],"url":"http://arxiv.org/abs/2406.07420v1","category":"cs.IR"}
{"created":"2024-06-11 16:21:48","title":"Single and merger soliton dynamics in scalar field dark matter with and without self-interactions","abstract":"(abridged)Scalar field dark matter (SFDM) made of bosons has become a popular alternative to the CDM paradigm, especially for its potential to cure the so-called \"small-scale problems\" of CDM. Cosmological simulations have determined that SFDM halos exhibit a core-envelope structure, but they are computationally expensive. Halo cores have been found to be well approximated by \"solitons\". The study of single soliton and multiple soliton merger dynamics constitutes a more feasible approach to investigate in detail the genuine quantum dynamics of SFDM and its interplay with self-gravity for a multitude of free boson parameters. In this paper, we present dedicated simulations of single solitons and binary soliton mergers, for models without and with a 2-boson, repulsive, weak to intermediate self-interaction (SI), as well as multiple soliton mergers without SI. We adapt the open-source code Pyultralight to simulate solitons with SI. We derive numerical scaling relations between the central density and mass of solitons for several values of SI and find deviations from the monotonic relations known from fuzzy dark matter (no SI), or the strongly repulsive Thomas-Fermi regime. Solitons with SI exemplify larger cores and lower central densities, compared to solitons without SI. Using our simulations, we extract numerical density profiles for solitons and post-merger objects, and fit them to analytic functions of previous literature. We find a mild preference for Gaussian cores for objects with SI, while the envelopes of post-mergers can be fit to NFW profiles albeit with some caution as we discuss. Similar to previous work, we find global, persistent oscillations for solitons as well as post-mergers, confirming that self-gravitating SFDM has very long relaxation times, although objects with SI exhibit oscillations of comparatively smaller amplitude.","sentences":["(abridged)Scalar field dark matter (SFDM) made of bosons has become a popular alternative to the CDM paradigm, especially for its potential to cure the so-called \"small-scale problems\" of CDM.","Cosmological simulations have determined that SFDM halos exhibit a core-envelope structure, but they are computationally expensive.","Halo cores have been found to be well approximated by \"solitons\".","The study of single soliton and multiple soliton merger dynamics constitutes a more feasible approach to investigate in detail the genuine quantum dynamics of SFDM and its interplay with self-gravity for a multitude of free boson parameters.","In this paper, we present dedicated simulations of single solitons and binary soliton mergers, for models without and with a 2-boson, repulsive, weak to intermediate self-interaction (SI), as well as multiple soliton mergers without SI.","We adapt the open-source code Pyultralight to simulate solitons with SI.","We derive numerical scaling relations between the central density and mass of solitons for several values of SI and find deviations from the monotonic relations known from fuzzy dark matter (no SI), or the strongly repulsive Thomas-Fermi regime.","Solitons with SI exemplify larger cores and lower central densities, compared to solitons without SI.","Using our simulations, we extract numerical density profiles for solitons and post-merger objects, and fit them to analytic functions of previous literature.","We find a mild preference for Gaussian cores for objects with SI, while the envelopes of post-mergers can be fit to NFW profiles albeit with some caution as we discuss.","Similar to previous work, we find global, persistent oscillations for solitons as well as post-mergers, confirming that self-gravitating SFDM has very long relaxation times, although objects with SI exhibit oscillations of comparatively smaller amplitude."],"url":"http://arxiv.org/abs/2406.07419v1","category":"astro-ph.CO"}
{"created":"2024-06-11 16:21:33","title":"Enhanced Gene Selection in Single-Cell Genomics: Pre-Filtering Synergy and Reinforced Optimization","abstract":"Recent advancements in single-cell genomics necessitate precision in gene panel selection to interpret complex biological data effectively. Those methods aim to streamline the analysis of scRNA-seq data by focusing on the most informative genes that contribute significantly to the specific analysis task. Traditional selection methods, which often rely on expert domain knowledge, embedded machine learning models, or heuristic-based iterative optimization, are prone to biases and inefficiencies that may obscure critical genomic signals. Recognizing the limitations of traditional methods, we aim to transcend these constraints with a refined strategy. In this study, we introduce an iterative gene panel selection strategy that is applicable to clustering tasks in single-cell genomics. Our method uniquely integrates results from other gene selection algorithms, providing valuable preliminary boundaries or prior knowledge as initial guides in the search space to enhance the efficiency of our framework. Furthermore, we incorporate the stochastic nature of the exploration process in reinforcement learning (RL) and its capability for continuous optimization through reward-based feedback. This combination mitigates the biases inherent in the initial boundaries and harnesses RL's adaptability to refine and target gene panel selection dynamically. To illustrate the effectiveness of our method, we conducted detailed comparative experiments, case studies, and visualization analysis.","sentences":["Recent advancements in single-cell genomics necessitate precision in gene panel selection to interpret complex biological data effectively.","Those methods aim to streamline the analysis of scRNA-seq data by focusing on the most informative genes that contribute significantly to the specific analysis task.","Traditional selection methods, which often rely on expert domain knowledge, embedded machine learning models, or heuristic-based iterative optimization, are prone to biases and inefficiencies that may obscure critical genomic signals.","Recognizing the limitations of traditional methods, we aim to transcend these constraints with a refined strategy.","In this study, we introduce an iterative gene panel selection strategy that is applicable to clustering tasks in single-cell genomics.","Our method uniquely integrates results from other gene selection algorithms, providing valuable preliminary boundaries or prior knowledge as initial guides in the search space to enhance the efficiency of our framework.","Furthermore, we incorporate the stochastic nature of the exploration process in reinforcement learning (RL) and its capability for continuous optimization through reward-based feedback.","This combination mitigates the biases inherent in the initial boundaries and harnesses RL's adaptability to refine and target gene panel selection dynamically.","To illustrate the effectiveness of our method, we conducted detailed comparative experiments, case studies, and visualization analysis."],"url":"http://arxiv.org/abs/2406.07418v1","category":"cs.AI"}
{"created":"2024-06-11 16:10:37","title":"Enhancing Tabular Data Optimization with a Flexible Graph-based Reinforced Exploration Strategy","abstract":"Tabular data optimization methods aim to automatically find an optimal feature transformation process that generates high-value features and improves the performance of downstream machine learning tasks. Current frameworks for automated feature transformation rely on iterative sequence generation tasks, optimizing decision strategies through performance feedback from downstream tasks. However, these approaches fail to effectively utilize historical decision-making experiences and overlook potential relationships among generated features, thus limiting the depth of knowledge extraction. Moreover, the granularity of the decision-making process lacks dynamic backtracking capabilities for individual features, leading to insufficient adaptability when encountering inefficient pathways, adversely affecting overall robustness and exploration efficiency. To address the limitations observed in current automatic feature engineering frameworks, we introduce a novel method that utilizes a feature-state transformation graph to effectively preserve the entire feature transformation journey, where each node represents a specific transformation state. During exploration, three cascading agents iteratively select nodes and idea mathematical operations to generate new transformation states. This strategy leverages the inherent properties of the graph structure, allowing for the preservation and reuse of valuable transformations. It also enables backtracking capabilities through graph pruning techniques, which can rectify inefficient transformation paths. To validate the efficacy and flexibility of our approach, we conducted comprehensive experiments and detailed case studies, demonstrating superior performance in diverse scenarios.","sentences":["Tabular data optimization methods aim to automatically find an optimal feature transformation process that generates high-value features and improves the performance of downstream machine learning tasks.","Current frameworks for automated feature transformation rely on iterative sequence generation tasks, optimizing decision strategies through performance feedback from downstream tasks.","However, these approaches fail to effectively utilize historical decision-making experiences and overlook potential relationships among generated features, thus limiting the depth of knowledge extraction.","Moreover, the granularity of the decision-making process lacks dynamic backtracking capabilities for individual features, leading to insufficient adaptability when encountering inefficient pathways, adversely affecting overall robustness and exploration efficiency.","To address the limitations observed in current automatic feature engineering frameworks, we introduce a novel method that utilizes a feature-state transformation graph to effectively preserve the entire feature transformation journey, where each node represents a specific transformation state.","During exploration, three cascading agents iteratively select nodes and idea mathematical operations to generate new transformation states.","This strategy leverages the inherent properties of the graph structure, allowing for the preservation and reuse of valuable transformations.","It also enables backtracking capabilities through graph pruning techniques, which can rectify inefficient transformation paths.","To validate the efficacy and flexibility of our approach, we conducted comprehensive experiments and detailed case studies, demonstrating superior performance in diverse scenarios."],"url":"http://arxiv.org/abs/2406.07404v1","category":"cs.LG"}
{"created":"2024-06-11 16:07:08","title":"Redefining Automotive Radar Imaging: A Domain-Informed 1D Deep Learning Approach for High-Resolution and Efficient Performance","abstract":"Millimeter-wave (mmWave) radars are indispensable for perception tasks of autonomous vehicles, thanks to their resilience in challenging weather conditions. Yet, their deployment is often limited by insufficient spatial resolution for precise semantic scene interpretation. Classical super-resolution techniques adapted from optical imaging inadequately address the distinct characteristics of radar signal data. In response, our study redefines radar imaging super-resolution as a one-dimensional (1D) signal super-resolution spectra estimation problem by harnessing the radar signal processing domain knowledge, introducing innovative data normalization and a domain-informed signal-to-noise ratio (SNR)-guided loss function. Our tailored deep learning network for automotive radar imaging exhibits remarkable scalability, parameter efficiency and fast inference speed, alongside enhanced performance in terms of radar imaging quality and resolution. Extensive testing confirms that our SR-SPECNet sets a new benchmark in producing high-resolution radar range-azimuth images, outperforming existing methods across varied antenna configurations and dataset sizes. Source code and new radar dataset will be made publicly available online.","sentences":["Millimeter-wave (mmWave) radars are indispensable for perception tasks of autonomous vehicles, thanks to their resilience in challenging weather conditions.","Yet, their deployment is often limited by insufficient spatial resolution for precise semantic scene interpretation.","Classical super-resolution techniques adapted from optical imaging inadequately address the distinct characteristics of radar signal data.","In response, our study redefines radar imaging super-resolution as a one-dimensional (1D) signal super-resolution spectra estimation problem by harnessing the radar signal processing domain knowledge, introducing innovative data normalization and a domain-informed signal-to-noise ratio (SNR)-guided loss function.","Our tailored deep learning network for automotive radar imaging exhibits remarkable scalability, parameter efficiency and fast inference speed, alongside enhanced performance in terms of radar imaging quality and resolution.","Extensive testing confirms that our SR-SPECNet sets a new benchmark in producing high-resolution radar range-azimuth images, outperforming existing methods across varied antenna configurations and dataset sizes.","Source code and new radar dataset will be made publicly available online."],"url":"http://arxiv.org/abs/2406.07399v1","category":"cs.LG"}
{"created":"2024-06-11 15:50:35","title":"Fast Adaptive Meta-Heuristic for Large-Scale Facility Location Problem","abstract":"Facility location problems have been a major research area of interest in the last several decades. In particular, uncapacitated location problems (ULP) have enormous applications. Variations of ULP often appear, especially as large-scale subproblems in more complex combinatorial optimization problems. Although many researchers have studied different versions of ULP (e.g., uncapacitated facility location problem (UCFLP) and p-Median problem), most of these authors have considered small to moderately sized problems. In this paper, we address the ULP and provide a fast adaptive meta-heuristic for large-scale problems. The approach is based on critical event memory tabu search. For the diversification component of the algorithm, we have chosen a procedure based on a sequencing problem commonly used for traveling salesman-type problems. The efficacy of this approach is evaluated across a diverse range of benchmark problems sourced from the Internet, with a comprehensive comparison against four prominent algorithms in the literature. The proposed adaptive critical event tabu search (ACETS) demonstrates remarkable effectiveness for large-scale problems. The algorithm successfully solved all problems optimally within a short computing time. Notably, ACETS discovered three best new solutions for benchmark problems, specifically for Asymmetric 500A-1, Asymmetric 750A-1, and Symmetric 750B-4, underscoring its innovative and robust nature.","sentences":["Facility location problems have been a major research area of interest in the last several decades.","In particular, uncapacitated location problems (ULP) have enormous applications.","Variations of ULP often appear, especially as large-scale subproblems in more complex combinatorial optimization problems.","Although many researchers have studied different versions of ULP (e.g., uncapacitated facility location problem (UCFLP) and p-Median problem), most of these authors have considered small to moderately sized problems.","In this paper, we address the ULP and provide a fast adaptive meta-heuristic for large-scale problems.","The approach is based on critical event memory tabu search.","For the diversification component of the algorithm, we have chosen a procedure based on a sequencing problem commonly used for traveling salesman-type problems.","The efficacy of this approach is evaluated across a diverse range of benchmark problems sourced from the Internet, with a comprehensive comparison against four prominent algorithms in the literature.","The proposed adaptive critical event tabu search (ACETS) demonstrates remarkable effectiveness for large-scale problems.","The algorithm successfully solved all problems optimally within a short computing time.","Notably, ACETS discovered three best new solutions for benchmark problems, specifically for Asymmetric 500A-1, Asymmetric 750A-1, and Symmetric 750B-4, underscoring its innovative and robust nature."],"url":"http://arxiv.org/abs/2406.07382v1","category":"math.OC"}
{"created":"2024-06-11 15:32:39","title":"Fast and accurate evaluation of Biot-Savart integrals over spatial curves","abstract":"The Biot-Savart law is relevant in physical contexts including electromagnetism and fluid dynamics. In the latter case, when the rotation of a fluid is confined to a set of very thin vortex filaments, this law describes the velocity field induced by the spatial arrangement of these objects. The Biot-Savart law is at the core of vortex methods used in the simulation of classical and quantum fluid flows. Naive methods are inefficient when dealing with large numbers of vortex elements, which makes them inadequate for simulating turbulent vortex flows. Here we exploit a direct analogy between the Biot-Savart law and electrostatics to adapt Ewald summation methods, routinely used in molecular dynamics simulations, to vortex filament simulations in three-dimensional periodic domains. In this context, the basic idea is to split the induced velocity onto (i) a coarse-grained velocity generated by a Gaussian-filtered vorticity field, and (ii) a short-range correction accounting for near-singular behaviour near the vortices. The former component can be accurately and efficiently evaluated using the nonuniform fast Fourier transform algorithm. Analytical accuracy estimates are provided as a function of the parameters entering the method. We also discuss how to properly account for the finite vortex core size in kinetic energy estimations. Using numerical experiments, we verify the accuracy and the conservation properties of the proposed approach. Moreover, we demonstrate the $O(N \\log N)$ complexity of the method over a wide range of problem sizes $N$, considerably better than the $O(N^2)$ cost of a naive approach.","sentences":["The Biot-Savart law is relevant in physical contexts including electromagnetism and fluid dynamics.","In the latter case, when the rotation of a fluid is confined to a set of very thin vortex filaments, this law describes the velocity field induced by the spatial arrangement of these objects.","The Biot-Savart law is at the core of vortex methods used in the simulation of classical and quantum fluid flows.","Naive methods are inefficient when dealing with large numbers of vortex elements, which makes them inadequate for simulating turbulent vortex flows.","Here we exploit a direct analogy between the Biot-Savart law and electrostatics to adapt Ewald summation methods, routinely used in molecular dynamics simulations, to vortex filament simulations in three-dimensional periodic domains.","In this context, the basic idea is to split the induced velocity onto (i) a coarse-grained velocity generated by a Gaussian-filtered vorticity field, and (ii) a short-range correction accounting for near-singular behaviour near the vortices.","The former component can be accurately and efficiently evaluated using the nonuniform fast Fourier transform algorithm.","Analytical accuracy estimates are provided as a function of the parameters entering the method.","We also discuss how to properly account for the finite vortex core size in kinetic energy estimations.","Using numerical experiments, we verify the accuracy and the conservation properties of the proposed approach.","Moreover, we demonstrate the $O(N \\log N)$ complexity of the method over a wide range of problem sizes $N$, considerably better than the $O(N^2)$ cost of a naive approach."],"url":"http://arxiv.org/abs/2406.07366v1","category":"physics.comp-ph"}
{"created":"2024-06-11 15:32:32","title":"BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction","abstract":"Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including aspect term, opinion term, aspect category, and sentiment polarity. In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model. Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications. Therefore, we first construct a few-shot ASQP dataset (FSQP) that contains richer categories and is more balanced for the few-shot study. Moreover, recent methods extract quads through a generation paradigm, which involves converting the input sentence into a templated target sequence. However, they primarily focus on the utilization of a single template or the consideration of different template orders, thereby overlooking the correlations among various templates. To tackle this issue, we further propose a Broadview Soft Prompting (BvSP) method that aggregates multiple templates with a broader view by taking into account the correlation between the different templates. Specifically, BvSP uses the pre-trained language model to select the most relevant k templates with Jensen-Shannon divergence. BvSP further introduces soft prompts to guide the pre-trained language model using the selected templates. Then, we aggregate the results of multi-templates by voting mechanism. Empirical results demonstrate that BvSP significantly outperforms the stateof-the-art methods under four few-shot settings and other public datasets. Our code and dataset are available at https://github.com/byinhao/BvSP.","sentences":["Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including aspect term, opinion term, aspect category, and sentiment polarity.","In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model.","Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications.","Therefore, we first construct a few-shot ASQP dataset (FSQP) that contains richer categories and is more balanced for the few-shot study.","Moreover, recent methods extract quads through a generation paradigm, which involves converting the input sentence into a templated target sequence.","However, they primarily focus on the utilization of a single template or the consideration of different template orders, thereby overlooking the correlations among various templates.","To tackle this issue, we further propose a Broadview Soft Prompting (BvSP) method that aggregates multiple templates with a broader view by taking into account the correlation between the different templates.","Specifically, BvSP uses the pre-trained language model to select the most relevant k templates with Jensen-Shannon divergence.","BvSP further introduces soft prompts to guide the pre-trained language model using the selected templates.","Then, we aggregate the results of multi-templates by voting mechanism.","Empirical results demonstrate that BvSP significantly outperforms the stateof-the-art methods under four few-shot settings and other public datasets.","Our code and dataset are available at https://github.com/byinhao/BvSP."],"url":"http://arxiv.org/abs/2406.07365v1","category":"cs.CL"}
{"created":"2024-06-11 15:27:02","title":"A mechanical qubit","abstract":"Strong nonlinear interactions between quantized excitations are an important resource for quantum technologies based on bosonic oscillator modes. However, most electromagnetic and mechanical nonlinearities arising from intrinsic material properties are far too weak compared to dissipation in the system to allow for nonlinear effects to be observed on the single-quantum level. To overcome this limitation, electromagnetic resonators in both the optical and microwave frequency regimes have been coupled to other strongly nonlinear quantum systems such as atoms and superconducting qubits, allowing for the demonstration of effects such as photon blockade and coherent quantum protocols using the Kerr effect. Here, we demonstrate the realization of the single-phonon nonlinear regime in a solid-state mechanical system. The single-phonon anharmonicity in our system exceeds the decoherence rate by a factor of 6.8, allowing us to use the lowest two energy levels of the resonator as a mechanical qubit, for which we show initialization, readout, and a complete set of direct single qubit gates. Our work adds another unique capability to a powerful quantum acoustics platform for quantum simulations, sensing, and information processing.","sentences":["Strong nonlinear interactions between quantized excitations are an important resource for quantum technologies based on bosonic oscillator modes.","However, most electromagnetic and mechanical nonlinearities arising from intrinsic material properties are far too weak compared to dissipation in the system to allow for nonlinear effects to be observed on the single-quantum level.","To overcome this limitation, electromagnetic resonators in both the optical and microwave frequency regimes have been coupled to other strongly nonlinear quantum systems such as atoms and superconducting qubits, allowing for the demonstration of effects such as photon blockade and coherent quantum protocols using the Kerr effect.","Here, we demonstrate the realization of the single-phonon nonlinear regime in a solid-state mechanical system.","The single-phonon anharmonicity in our system exceeds the decoherence rate by a factor of 6.8, allowing us to use the lowest two energy levels of the resonator as a mechanical qubit, for which we show initialization, readout, and a complete set of direct single qubit gates.","Our work adds another unique capability to a powerful quantum acoustics platform for quantum simulations, sensing, and information processing."],"url":"http://arxiv.org/abs/2406.07360v1","category":"quant-ph"}
{"created":"2024-06-11 15:08:14","title":"EdgeTimer: Adaptive Multi-Timescale Scheduling in Mobile Edge Computing with Deep Reinforcement Learning","abstract":"In mobile edge computing (MEC), resource scheduling is crucial to task requests' performance and service providers' cost, involving multi-layer heterogeneous scheduling decisions. Existing schedulers typically adopt static timescales to regularly update scheduling decisions of each layer, without adaptive adjustment of timescales for different layers, resulting in potentially poor performance in practice.   We notice that the adaptive timescales would significantly improve the trade-off between the operation cost and delay performance. Based on this insight, we propose EdgeTimer, the first work to automatically generate adaptive timescales to update multi-layer scheduling decisions using deep reinforcement learning (DRL). First, EdgeTimer uses a three-layer hierarchical DRL framework to decouple the multi-layer decision-making task into a hierarchy of independent sub-tasks for improving learning efficiency. Second, to cope with each sub-task, EdgeTimer adopts a safe multi-agent DRL algorithm for decentralized scheduling while ensuring system reliability. We apply EdgeTimer to a wide range of Kubernetes scheduling rules, and evaluate it using production traces with different workload patterns. Extensive trace-driven experiments demonstrate that EdgeTimer can learn adaptive timescales, irrespective of workload patterns and built-in scheduling rules. It obtains up to 9.1x more profit than existing approaches without sacrificing the delay performance.","sentences":["In mobile edge computing (MEC), resource scheduling is crucial to task requests' performance and service providers' cost, involving multi-layer heterogeneous scheduling decisions.","Existing schedulers typically adopt static timescales to regularly update scheduling decisions of each layer, without adaptive adjustment of timescales for different layers, resulting in potentially poor performance in practice.   ","We notice that the adaptive timescales would significantly improve the trade-off between the operation cost and delay performance.","Based on this insight, we propose EdgeTimer, the first work to automatically generate adaptive timescales to update multi-layer scheduling decisions using deep reinforcement learning (DRL).","First, EdgeTimer uses a three-layer hierarchical DRL framework to decouple the multi-layer decision-making task into a hierarchy of independent sub-tasks for improving learning efficiency.","Second, to cope with each sub-task, EdgeTimer adopts a safe multi-agent DRL algorithm for decentralized scheduling while ensuring system reliability.","We apply EdgeTimer to a wide range of Kubernetes scheduling rules, and evaluate it using production traces with different workload patterns.","Extensive trace-driven experiments demonstrate that EdgeTimer can learn adaptive timescales, irrespective of workload patterns and built-in scheduling rules.","It obtains up to 9.1x more profit than existing approaches without sacrificing the delay performance."],"url":"http://arxiv.org/abs/2406.07342v1","category":"cs.NI"}
{"created":"2024-06-11 15:06:15","title":"Transferring Knowledge from Large Foundation Models to Small Downstream Models","abstract":"How do we transfer the relevant knowledge from ever larger foundation models into small, task-specific downstream models that can run at much lower costs? Standard transfer learning using pre-trained weights as the initialization transfers limited information and commits us to often massive pre-trained architectures. This procedure also precludes combining multiple pre-trained models that learn complementary information. To address these shortcomings, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the smaller downstream model. Rather than indiscriminately compressing all pre-trained features, AFT adaptively transfers pre-trained features that are most useful for performing the downstream task, using a simple regularization that adds minimal overhead. Across multiple vision, language, and multi-modal datasets, AFT achieves significantly better downstream performance compared to alternatives with a similar computational cost. Furthermore, AFT reliably translates improvement in pre-trained models into improvement in downstream performance, even if the downstream model is over $50\\times$ smaller, and can effectively transfer complementary information learned by multiple pre-trained models.","sentences":["How do we transfer the relevant knowledge from ever larger foundation models into small, task-specific downstream models that can run at much lower costs?","Standard transfer learning using pre-trained weights as the initialization transfers limited information and commits us to often massive pre-trained architectures.","This procedure also precludes combining multiple pre-trained models that learn complementary information.","To address these shortcomings, we introduce Adaptive Feature Transfer (AFT).","Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the smaller downstream model.","Rather than indiscriminately compressing all pre-trained features, AFT adaptively transfers pre-trained features that are most useful for performing the downstream task, using a simple regularization that adds minimal overhead.","Across multiple vision, language, and multi-modal datasets, AFT achieves significantly better downstream performance compared to alternatives with a similar computational cost.","Furthermore, AFT reliably translates improvement in pre-trained models into improvement in downstream performance, even if the downstream model is over $50\\times$ smaller, and can effectively transfer complementary information learned by multiple pre-trained models."],"url":"http://arxiv.org/abs/2406.07337v1","category":"cs.LG"}
{"created":"2024-06-11 15:01:20","title":"Minimizing Energy Costs in Deep Learning Model Training: The Gaussian Sampling Approach","abstract":"Computing the loss gradient via backpropagation consumes considerable energy during deep learning (DL) model training. In this paper, we propose a novel approach to efficiently compute DL models' gradients to mitigate the substantial energy overhead associated with backpropagation. Exploiting the over-parameterized nature of DL models and the smoothness of their loss landscapes, we propose a method called {\\em GradSamp} for sampling gradient updates from a Gaussian distribution. Specifically, we update model parameters at a given epoch (chosen periodically or randomly) by perturbing the parameters (element-wise) from the previous epoch with Gaussian ``noise''. The parameters of the Gaussian distribution are estimated using the error between the model parameter values from the two previous epochs. {\\em GradSamp} not only streamlines gradient computation but also enables skipping entire epochs, thereby enhancing overall efficiency. We rigorously validate our hypothesis across a diverse set of standard and non-standard CNN and transformer-based models, spanning various computer vision tasks such as image classification, object detection, and image segmentation. Additionally, we explore its efficacy in out-of-distribution scenarios such as Domain Adaptation (DA), Domain Generalization (DG), and decentralized settings like Federated Learning (FL). Our experimental results affirm the effectiveness of {\\em GradSamp} in achieving notable energy savings without compromising performance, underscoring its versatility and potential impact in practical DL applications.","sentences":["Computing the loss gradient via backpropagation consumes considerable energy during deep learning (DL) model training.","In this paper, we propose a novel approach to efficiently compute DL models' gradients to mitigate the substantial energy overhead associated with backpropagation.","Exploiting the over-parameterized nature of DL models and the smoothness of their loss landscapes, we propose a method called {\\em GradSamp} for sampling gradient updates from a Gaussian distribution.","Specifically, we update model parameters at a given epoch (chosen periodically or randomly) by perturbing the parameters (element-wise) from the previous epoch with Gaussian ``noise''.","The parameters of the Gaussian distribution are estimated using the error between the model parameter values from the two previous epochs.","{\\em GradSamp} not only streamlines gradient computation but also enables skipping entire epochs, thereby enhancing overall efficiency.","We rigorously validate our hypothesis across a diverse set of standard and non-standard CNN and transformer-based models, spanning various computer vision tasks such as image classification, object detection, and image segmentation.","Additionally, we explore its efficacy in out-of-distribution scenarios such as Domain Adaptation (DA), Domain Generalization (DG), and decentralized settings like Federated Learning (FL).","Our experimental results affirm the effectiveness of {\\em GradSamp} in achieving notable energy savings without compromising performance, underscoring its versatility and potential impact in practical DL applications."],"url":"http://arxiv.org/abs/2406.07332v1","category":"cs.CV"}
{"created":"2024-06-11 14:59:18","title":"Beyond Training: Optimizing Reinforcement Learning Based Job Shop Scheduling Through Adaptive Action Sampling","abstract":"Learned construction heuristics for scheduling problems have become increasingly competitive with established solvers and heuristics in recent years. In particular, significant improvements have been observed in solution approaches using deep reinforcement learning (DRL). While much attention has been paid to the design of network architectures and training algorithms to achieve state-of-the-art results, little research has investigated the optimal use of trained DRL agents during inference. Our work is based on the hypothesis that, similar to search algorithms, the utilization of trained DRL agents should be dependent on the acceptable computational budget. We propose a simple yet effective parameterization, called $\\delta$-sampling that manipulates the trained action vector to bias agent behavior towards exploration or exploitation during solution construction. By following this approach, we can achieve a more comprehensive coverage of the search space while still generating an acceptable number of solutions. In addition, we propose an algorithm for obtaining the optimal parameterization for such a given number of solutions and any given trained agent. Experiments extending existing training protocols for job shop scheduling problems with our inference method validate our hypothesis and result in the expected improvements of the generated solutions.","sentences":["Learned construction heuristics for scheduling problems have become increasingly competitive with established solvers and heuristics in recent years.","In particular, significant improvements have been observed in solution approaches using deep reinforcement learning (DRL).","While much attention has been paid to the design of network architectures and training algorithms to achieve state-of-the-art results, little research has investigated the optimal use of trained DRL agents during inference.","Our work is based on the hypothesis that, similar to search algorithms, the utilization of trained DRL agents should be dependent on the acceptable computational budget.","We propose a simple yet effective parameterization, called $\\delta$-sampling that manipulates the trained action vector to bias agent behavior towards exploration or exploitation during solution construction.","By following this approach, we can achieve a more comprehensive coverage of the search space while still generating an acceptable number of solutions.","In addition, we propose an algorithm for obtaining the optimal parameterization for such a given number of solutions and any given trained agent.","Experiments extending existing training protocols for job shop scheduling problems with our inference method validate our hypothesis and result in the expected improvements of the generated solutions."],"url":"http://arxiv.org/abs/2406.07325v1","category":"cs.AI"}
{"created":"2024-06-11 14:15:33","title":"Bilingual Sexism Classification: Fine-Tuned XLM-RoBERTa and GPT-3.5 Few-Shot Learning","abstract":"Sexism in online content is a pervasive issue that necessitates effective classification techniques to mitigate its harmful impact. Online platforms often have sexist comments and posts that create a hostile environment, especially for women and minority groups. This content not only spreads harmful stereotypes but also causes emotional harm. Reliable methods are essential to find and remove sexist content, making online spaces safer and more welcoming. Therefore, the sEXism Identification in Social neTworks (EXIST) challenge addresses this issue at CLEF 2024. This study aims to improve sexism identification in bilingual contexts (English and Spanish) by leveraging natural language processing models. The tasks are to determine whether a text is sexist and what the source intention behind it is. We fine-tuned the XLM-RoBERTa model and separately used GPT-3.5 with few-shot learning prompts to classify sexist content. The XLM-RoBERTa model exhibited robust performance in handling complex linguistic structures, while GPT-3.5's few-shot learning capability allowed for rapid adaptation to new data with minimal labeled examples. Our approach using XLM-RoBERTa achieved 4th place in the soft-soft evaluation of Task 1 (sexism identification). For Task 2 (source intention), we achieved 2nd place in the soft-soft evaluation.","sentences":["Sexism in online content is a pervasive issue that necessitates effective classification techniques to mitigate its harmful impact.","Online platforms often have sexist comments and posts that create a hostile environment, especially for women and minority groups.","This content not only spreads harmful stereotypes but also causes emotional harm.","Reliable methods are essential to find and remove sexist content, making online spaces safer and more welcoming.","Therefore, the sEXism Identification in Social neTworks (EXIST) challenge addresses this issue at CLEF 2024.","This study aims to improve sexism identification in bilingual contexts (English and Spanish) by leveraging natural language processing models.","The tasks are to determine whether a text is sexist and what the source intention behind it is.","We fine-tuned the XLM-RoBERTa model and separately used GPT-3.5 with few-shot learning prompts to classify sexist content.","The XLM-RoBERTa model exhibited robust performance in handling complex linguistic structures, while GPT-3.5's few-shot learning capability allowed for rapid adaptation to new data with minimal labeled examples.","Our approach using XLM-RoBERTa achieved 4th place in the soft-soft evaluation of Task 1 (sexism identification).","For Task 2 (source intention), we achieved 2nd place in the soft-soft evaluation."],"url":"http://arxiv.org/abs/2406.07287v1","category":"cs.CL"}
{"created":"2024-06-11 13:53:27","title":"The geometry of efficient codes: how rate-distortion trade-offs distort the latent representations of generative models","abstract":"Living organisms rely on internal models of the world to act adaptively. These models cannot encode every detail and hence need to compress information. From a cognitive standpoint, information compression can manifest as a distortion of latent representations, resulting in the emergence of representations that may not accurately reflect the external world or its geometry. Rate-distortion theory formalizes the optimal way to compress information, by considering factors such as capacity limitations, the frequency and the utility of stimuli. However, while this theory explains why the above factors distort latent representations, it does not specify which specific distortions they produce. To address this question, here we systematically explore the geometry of the latent representations that emerge in generative models that operate under the principles of rate-distortion theory ($\\beta$-VAEs). Our results highlight that three main classes of distortions of internal representations -- prototypization, specialization, orthogonalization -- emerge as signatures of information compression, under constraints on capacity, data distributions and tasks. These distortions can coexist, giving rise to a rich landscape of latent spaces, whose geometry could differ significantly across generative models subject to different constraints. Our findings contribute to explain how the normative constraints of rate-distortion theory distort the geometry of latent representations of generative models of artificial systems and living organisms.","sentences":["Living organisms rely on internal models of the world to act adaptively.","These models cannot encode every detail and hence need to compress information.","From a cognitive standpoint, information compression can manifest as a distortion of latent representations, resulting in the emergence of representations that may not accurately reflect the external world or its geometry.","Rate-distortion theory formalizes the optimal way to compress information, by considering factors such as capacity limitations, the frequency and the utility of stimuli.","However, while this theory explains why the above factors distort latent representations, it does not specify which specific distortions they produce.","To address this question, here we systematically explore the geometry of the latent representations that emerge in generative models that operate under the principles of rate-distortion theory ($\\beta$-VAEs).","Our results highlight that three main classes of distortions of internal representations -- prototypization, specialization, orthogonalization -- emerge as signatures of information compression, under constraints on capacity, data distributions and tasks.","These distortions can coexist, giving rise to a rich landscape of latent spaces, whose geometry could differ significantly across generative models subject to different constraints.","Our findings contribute to explain how the normative constraints of rate-distortion theory distort the geometry of latent representations of generative models of artificial systems and living organisms."],"url":"http://arxiv.org/abs/2406.07269v1","category":"q-bio.NC"}
{"created":"2024-06-11 13:32:11","title":"Are Protein Language Models Compute Optimal?","abstract":"While protein language models (pLMs) have transformed biological research, the scaling laws governing their improvement remain underexplored. By adapting methodologies from NLP scaling laws, we investigated the optimal ratio between model parameters and training tokens within a fixed compute budget. Our study reveals that pLM sizes scale sublinearly with compute budget, showing diminishing returns in performance as model size increases, and we identify a performance plateau in training loss comparable to the one found in relevant works in the field. Our findings suggest that widely-used pLMs might not be compute-optimal, indicating that larger models could achieve convergence more efficiently. Training a 35M model on a reduced token set, we attained perplexity results comparable to larger models like ESM-2 (15B) and xTrimoPGLM (100B) with a single dataset pass. This work paves the way towards more compute-efficient pLMs, democratizing their training and practical application in computational biology.","sentences":["While protein language models (pLMs) have transformed biological research, the scaling laws governing their improvement remain underexplored.","By adapting methodologies from NLP scaling laws, we investigated the optimal ratio between model parameters and training tokens within a fixed compute budget.","Our study reveals that pLM sizes scale sublinearly with compute budget, showing diminishing returns in performance as model size increases, and we identify a performance plateau in training loss comparable to the one found in relevant works in the field.","Our findings suggest that widely-used pLMs might not be compute-optimal, indicating that larger models could achieve convergence more efficiently.","Training a 35M model on a reduced token set, we attained perplexity results comparable to larger models like ESM-2 (15B) and xTrimoPGLM (100B) with a single dataset pass.","This work paves the way towards more compute-efficient pLMs, democratizing their training and practical application in computational biology."],"url":"http://arxiv.org/abs/2406.07249v1","category":"q-bio.BM"}
{"created":"2024-06-11 13:27:36","title":"Collisionless shock in a relativistically hot unmagnetized electron-positron plasma","abstract":"In this work, we investigate collisionless shocks propagating in a relativistically hot unmagnetized electron-positron plasmas. We estimate the dissipation fraction at shocks in the relativistically hot plasma, showing that it is sufficiently large to explain the observation of gamma-ray bursts even when the shock is not highly relativistic. It is shown by two-dimensional particle in cell simulations that magnetic fields are generated around the shock front by the Weibel instability, as in the cold upstream plasma. However, in contrast to the cold upstream plasma, no particles are accelerated at the shock in the simulation time of $t = 3600 \\omega_p^{-1}$. The decay of the magnetic field in the downstream region is slower for slower shock velocities in the hot plasma cases. Applying the slow decay of the downstream magnetic field, we propose a model that generate magnetic fields in large downstream region, which is required from the standard model of the gamma-ray burst afterglow.","sentences":["In this work, we investigate collisionless shocks propagating in a relativistically hot unmagnetized electron-positron plasmas.","We estimate the dissipation fraction at shocks in the relativistically hot plasma, showing that it is sufficiently large to explain the observation of gamma-ray bursts even when the shock is not highly relativistic.","It is shown by two-dimensional particle in cell simulations that magnetic fields are generated around the shock front by the Weibel instability, as in the cold upstream plasma.","However, in contrast to the cold upstream plasma, no particles are accelerated at the shock in the simulation time of $t = 3600 \\omega_p^{-1}$. The decay of the magnetic field in the downstream region is slower for slower shock velocities in the hot plasma cases.","Applying the slow decay of the downstream magnetic field, we propose a model that generate magnetic fields in large downstream region, which is required from the standard model of the gamma-ray burst afterglow."],"url":"http://arxiv.org/abs/2406.07244v1","category":"astro-ph.HE"}
{"created":"2024-06-11 13:22:52","title":"Variational inequalities and smooth-fit principle for singular stochastic control problems in Hilbert spaces","abstract":"We consider a class of infinite-dimensional singular stochastic control problems. These can be thought of as spatial monotone follower problems and find applications in spatial models of production and climate transition. Let $(D,\\mathcal{M},\\mu)$ be a finite measure space and consider the Hilbert space $H:=L^2(D,\\mathcal{M},\\mu; \\mathbb{R})$. Let then $X$ be an $H$-valued stochastic process on a suitable complete probability space, whose evolution is determined through an SPDE driven by a self-adjoint linear operator $\\mathcal{A}$ and affected by a cylindrical Brownian motion. The evolution of $X$ is controlled linearly via an $H$-valued control consisting of the direction and the intensity of action, a real-valued nondecreasing right-continuous stochastic process, adapted to the underlying filtration. The goal is to minimize a discounted convex cost-functional over an infinite time-horizon. By combining properties of semiconcave functions and techniques from viscosity theory, we first show that the value function of the problem $V$ is a $C^{1,Lip}(H)$-viscosity solution to the corresponding dynamic programming equation, which here takes the form of a variational inequality with gradient constraint. Then, by allowing the decision maker to choose only the intensity of the control and requiring that the given control direction $\\hat{n}$ is an eigenvector of the linear operator $\\mathcal{A}$, we establish that the directional derivative $V_{\\hat{n}}$ is of class $C^1(H)$, hence a second-order smooth-fit principle in the controlled direction holds for $V$. This result is obtained by exploiting a connection to optimal stopping and combining results and techniques from convex analysis and viscosity theory.","sentences":["We consider a class of infinite-dimensional singular stochastic control problems.","These can be thought of as spatial monotone follower problems and find applications in spatial models of production and climate transition.","Let $(D,\\mathcal{M},\\mu)$ be a finite measure space and consider the Hilbert space $H:=L^2(D,\\mathcal{M},\\mu; \\mathbb{R})$. Let then $X$ be an $H$-valued stochastic process on a suitable complete probability space, whose evolution is determined through an SPDE driven by a self-adjoint linear operator $\\mathcal{A}$ and affected by a cylindrical Brownian motion.","The evolution of $X$ is controlled linearly via an $H$-valued control consisting of the direction and the intensity of action, a real-valued nondecreasing right-continuous stochastic process, adapted to the underlying filtration.","The goal is to minimize a discounted convex cost-functional over an infinite time-horizon.","By combining properties of semiconcave functions and techniques from viscosity theory, we first show that the value function of the problem $V$ is a $C^{1,Lip}(H)$-viscosity solution to the corresponding dynamic programming equation, which here takes the form of a variational inequality with gradient constraint.","Then, by allowing the decision maker to choose only the intensity of the control and requiring that the given control direction $\\hat{n}$ is an eigenvector of the linear operator $\\mathcal{A}$, we establish that the directional derivative $V_{\\hat{n}}$ is of class $C^1(H)$, hence a second-order smooth-fit principle in the controlled direction holds for $V$. This result is obtained by exploiting a connection to optimal stopping and combining results and techniques from convex analysis and viscosity theory."],"url":"http://arxiv.org/abs/2406.07242v1","category":"math.OC"}
{"created":"2024-06-11 13:13:56","title":"Non-equilibrium fluctuations of the direct cascade in Surface Quasi Geostrophic turbulence","abstract":"We study the temporal fluctuations of the flux of surface potential energy in Surface Quasi-Geostrophic (SQG) turbulence. By means of high-resolution, direct numerical simulations of the SQG model in the regime of forced and dissipated cascade of temperature variance, we show that the instantaneous imbalance in the energy budget originates a subleading correction to the spectrum of the turbulent cascade. Using a multiple-scale approach combined with a dimensional closure we derive a theoretical prediction for the power-law behavior of the corrections, which holds for a class of turbulent transport equations known as {\\alpha}-turbulence. Further, we develop and apply a method to disentangle the equilibrium and non-equilibrium contribution in the instantaneous spectra, which can be generalized to other turbulent systems.","sentences":["We study the temporal fluctuations of the flux of surface potential energy in Surface Quasi-Geostrophic (SQG) turbulence.","By means of high-resolution, direct numerical simulations of the SQG model in the regime of forced and dissipated cascade of temperature variance, we show that the instantaneous imbalance in the energy budget originates a subleading correction to the spectrum of the turbulent cascade.","Using a multiple-scale approach combined with a dimensional closure we derive a theoretical prediction for the power-law behavior of the corrections, which holds for a class of turbulent transport equations known as {\\alpha}-turbulence.","Further, we develop and apply a method to disentangle the equilibrium and non-equilibrium contribution in the instantaneous spectra, which can be generalized to other turbulent systems."],"url":"http://arxiv.org/abs/2406.07235v1","category":"physics.flu-dyn"}
{"created":"2024-06-11 13:06:28","title":"Haptic Repurposing with GenAI","abstract":"Mixed Reality aims to merge the digital and physical worlds to create immersive human-computer interactions. Despite notable advancements, the absence of realistic haptic feedback often breaks the immersive experience by creating a disconnect between visual and tactile perceptions. This paper introduces Haptic Repurposing with GenAI, an innovative approach to enhance MR interactions by transforming any physical objects into adaptive haptic interfaces for AI-generated virtual assets. Utilizing state-of-the-art generative AI models, this system captures both 2D and 3D features of physical objects and, through user-directed prompts, generates corresponding virtual objects that maintain the physical form of the original objects. Through model-based object tracking, the system dynamically anchors virtual assets to physical props in real time, allowing objects to visually morph into any user-specified virtual object. This paper details the system's development, presents findings from usability studies that validate its effectiveness, and explores its potential to significantly enhance interactive MR environments. The hope is this work can lay a foundation for further research into AI-driven spatial transformation in immersive and haptic technologies.","sentences":["Mixed Reality aims to merge the digital and physical worlds to create immersive human-computer interactions.","Despite notable advancements, the absence of realistic haptic feedback often breaks the immersive experience by creating a disconnect between visual and tactile perceptions.","This paper introduces Haptic Repurposing with GenAI, an innovative approach to enhance MR interactions by transforming any physical objects into adaptive haptic interfaces for AI-generated virtual assets.","Utilizing state-of-the-art generative AI models, this system captures both 2D and 3D features of physical objects and, through user-directed prompts, generates corresponding virtual objects that maintain the physical form of the original objects.","Through model-based object tracking, the system dynamically anchors virtual assets to physical props in real time, allowing objects to visually morph into any user-specified virtual object.","This paper details the system's development, presents findings from usability studies that validate its effectiveness, and explores its potential to significantly enhance interactive MR environments.","The hope is this work can lay a foundation for further research into AI-driven spatial transformation in immersive and haptic technologies."],"url":"http://arxiv.org/abs/2406.07228v1","category":"cs.HC"}
{"created":"2024-06-11 13:04:30","title":"A generic and robust quantum agent inspired by deep meta-reinforcement learning","abstract":"Deep reinforcement learning (deep RL) has enabled human- or superhuman- performances in various applications. Recently, deep RL has also been adopted to improve the performance of quantum control. However, a large volume of data is typically required to train the neural network in deep RL, making it inefficient compared with the traditional optimal quantum control method. Here, we thus develop a new training algorithm inspired by the deep meta-reinforcement learning (deep meta-RL), which requires significantly less training data. The trained neural network is adaptive and robust. In addition, the algorithm proposed by us has been applied to design the Hadamard gate and show that for a wide range of parameters the infidelity of the obtained gate can be made of the order 0.0001. Our algorithm can also automatically adjust the number of pulses required to generate the target gate, which is different from the traditional optimal quantum control method which typically fixes the number of pulses a-priory. The results of this paper can pave the way towards constructing a universally robust quantum agent catering to the different demands in quantum technologies.","sentences":["Deep reinforcement learning (deep RL) has enabled human- or superhuman- performances in various applications.","Recently, deep RL has also been adopted to improve the performance of quantum control.","However, a large volume of data is typically required to train the neural network in deep RL, making it inefficient compared with the traditional optimal quantum control method.","Here, we thus develop a new training algorithm inspired by the deep meta-reinforcement learning (deep meta-RL), which requires significantly less training data.","The trained neural network is adaptive and robust.","In addition, the algorithm proposed by us has been applied to design the Hadamard gate and show that for a wide range of parameters the infidelity of the obtained gate can be made of the order 0.0001.","Our algorithm can also automatically adjust the number of pulses required to generate the target gate, which is different from the traditional optimal quantum control method which typically fixes the number of pulses a-priory.","The results of this paper can pave the way towards constructing a universally robust quantum agent catering to the different demands in quantum technologies."],"url":"http://arxiv.org/abs/2406.07225v1","category":"quant-ph"}
{"created":"2024-06-11 12:32:53","title":"MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance","abstract":"Recent advancements in text-to-image generation models have dramatically enhanced the generation of photorealistic images from textual prompts, leading to an increased interest in personalized text-to-image applications, particularly in multi-subject scenarios. However, these advances are hindered by two main challenges: firstly, the need to accurately maintain the details of each referenced subject in accordance with the textual descriptions; and secondly, the difficulty in achieving a cohesive representation of multiple subjects in a single image without introducing inconsistencies. To address these concerns, our research introduces the MS-Diffusion framework for layout-guided zero-shot image personalization with multi-subjects. This innovative approach integrates grounding tokens with the feature resampler to maintain detail fidelity among subjects. With the layout guidance, MS-Diffusion further improves the cross-attention to adapt to the multi-subject inputs, ensuring that each subject condition acts on specific areas. The proposed multi-subject cross-attention orchestrates harmonious inter-subject compositions while preserving the control of texts. Comprehensive quantitative and qualitative experiments affirm that this method surpasses existing models in both image and text fidelity, promoting the development of personalized text-to-image generation.","sentences":["Recent advancements in text-to-image generation models have dramatically enhanced the generation of photorealistic images from textual prompts, leading to an increased interest in personalized text-to-image applications, particularly in multi-subject scenarios.","However, these advances are hindered by two main challenges: firstly, the need to accurately maintain the details of each referenced subject in accordance with the textual descriptions; and secondly, the difficulty in achieving a cohesive representation of multiple subjects in a single image without introducing inconsistencies.","To address these concerns, our research introduces the MS-Diffusion framework for layout-guided zero-shot image personalization with multi-subjects.","This innovative approach integrates grounding tokens with the feature resampler to maintain detail fidelity among subjects.","With the layout guidance, MS-Diffusion further improves the cross-attention to adapt to the multi-subject inputs, ensuring that each subject condition acts on specific areas.","The proposed multi-subject cross-attention orchestrates harmonious inter-subject compositions while preserving the control of texts.","Comprehensive quantitative and qualitative experiments affirm that this method surpasses existing models in both image and text fidelity, promoting the development of personalized text-to-image generation."],"url":"http://arxiv.org/abs/2406.07209v1","category":"cs.CV"}
{"created":"2024-06-11 12:24:52","title":"Mean-Field Magnetohydrodynamics Models as Scaling Limits of Stochastic Induction Equations","abstract":"We study the asymptotic properties of a stochastic model for the induction equations of the magnetic field in a three dimensional periodic domain. The turbulent velocity field driving the electromotive force on the magnetic field is modeled by a noise white in time. For this model we rigorously take a scaling limit leading to a deterministic model. While in case of isotropic turbulence this produces an additional dissipation in the limit model which influences also the decay rate of the Magnetic field in the stochastic model, the case of turbulence devoloped in a preferential direction allows us to find a dynamo effect.","sentences":["We study the asymptotic properties of a stochastic model for the induction equations of the magnetic field in a three dimensional periodic domain.","The turbulent velocity field driving the electromotive force on the magnetic field is modeled by a noise white in time.","For this model we rigorously take a scaling limit leading to a deterministic model.","While in case of isotropic turbulence this produces an additional dissipation in the limit model which influences also the decay rate of the Magnetic field in the stochastic model, the case of turbulence devoloped in a preferential direction allows us to find a dynamo effect."],"url":"http://arxiv.org/abs/2406.07206v1","category":"math.PR"}
{"created":"2024-06-11 12:23:21","title":"Manifestation of superfluidity in atom-number-imbalanced two-component Bose-Einstein condensates","abstract":"Superfluid and dissipative regimes in the dynamics of a two-component quasi-one-dimensional Bose-Einstein condensate (BEC) with unequal atom numbers in the components have been explored. The system supports localized waves of the symbiotic type owing to the same-species repulsion and cross-species attraction. The minority BEC component moves through the majority component and creates excitations. To quantify the emerging excitations we introduce a time-dependent function called disturbance. Through numerical simulations of the coupled Gross-Pitaevskii equations with periodic boundary conditions, we have revealed a critical velocity of the localized wave, above which a transition from superfluid to dissipative regime occurs, evidenced by a sharp increase in the disturbance function. The factors responsible for the discrepancy between the actual critical velocity and the speed of sound, expected from theoretical arguments, have been discussed.","sentences":["Superfluid and dissipative regimes in the dynamics of a two-component quasi-one-dimensional Bose-Einstein condensate (BEC) with unequal atom numbers in the components have been explored.","The system supports localized waves of the symbiotic type owing to the same-species repulsion and cross-species attraction.","The minority BEC component moves through the majority component and creates excitations.","To quantify the emerging excitations we introduce a time-dependent function called disturbance.","Through numerical simulations of the coupled Gross-Pitaevskii equations with periodic boundary conditions, we have revealed a critical velocity of the localized wave, above which a transition from superfluid to dissipative regime occurs, evidenced by a sharp increase in the disturbance function.","The factors responsible for the discrepancy between the actual critical velocity and the speed of sound, expected from theoretical arguments, have been discussed."],"url":"http://arxiv.org/abs/2406.07204v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-11 12:10:57","title":"Binary asteroid candidates in Gaia DR3 astrometry","abstract":"Asteroids with companions constitute an excellent sample for studying the collisional and dynamical evolution of minor planets. The currently known binary population were discovered by different complementary techniques that produce, for the moment, a strongly biased distribution, especially in a range of intermediate asteroid sizes (approximately 20 to 100 km) where both mutual photometric events and high-resolution adaptive optic imaging are poorly efficient. A totally independent technique of binary asteroid discovery, based on astrometry, can help to reveal new binary systems and populate a range of sizes and separations that remain nearly unexplored. In this work, we describe a dedicated period detection method and its results for the Gaia DR3 data set. This method looks for the presence of a periodic signature in the orbit post-fit residuals. After conservative filtering and validation based on statistical and physical criteria, we are able to present a first sample of astrometric binary candidates, to be confirmed by other observation techniques such as photometric light curves and stellar occultations.","sentences":["Asteroids with companions constitute an excellent sample for studying the collisional and dynamical evolution of minor planets.","The currently known binary population were discovered by different complementary techniques that produce, for the moment, a strongly biased distribution, especially in a range of intermediate asteroid sizes (approximately 20 to 100 km) where both mutual photometric events and high-resolution adaptive optic imaging are poorly efficient.","A totally independent technique of binary asteroid discovery, based on astrometry, can help to reveal new binary systems and populate a range of sizes and separations that remain nearly unexplored.","In this work, we describe a dedicated period detection method and its results for the Gaia DR3 data set.","This method looks for the presence of a periodic signature in the orbit post-fit residuals.","After conservative filtering and validation based on statistical and physical criteria, we are able to present a first sample of astrometric binary candidates, to be confirmed by other observation techniques such as photometric light curves and stellar occultations."],"url":"http://arxiv.org/abs/2406.07195v1","category":"astro-ph.EP"}
{"created":"2024-06-11 12:00:50","title":"Quantum Speedup of the Dispersion and Codebook Design Problems","abstract":"We propose new formulations of max-sum and max-min dispersion problems that enable solutions via the Grover adaptive search (GAS) quantum algorithm, offering quadratic speedup. Dispersion problems are combinatorial optimization problems classified as NP-hard, which appear often in coding theory and wireless communications applications involving optimal codebook design. In turn, GAS is a quantum exhaustive search algorithm that can be used to implement full-fledged maximum-likelihood optimal solutions. In conventional naive formulations however, it is typical to rely on a binary vector spaces, resulting in search space sizes prohibitive even for GAS. To circumvent this challenge, we instead formulate the search of optimal dispersion problem over Dicke states, an equal superposition of binary vectors with equal Hamming weights, which significantly reduces the search space leading to a simplification of the quantum circuit via the elimination of penalty terms. Additionally, we propose a method to replace distance coefficients with their ranks, contributing to the reduction of the number of qubits. Our analysis demonstrates that as a result of the proposed techniques a reduction in query complexity compared to the conventional GAS using Hadamard transform is achieved, enhancing the feasibility of the quantum-based solution of the dispersion problem.","sentences":["We propose new formulations of max-sum and max-min dispersion problems that enable solutions via the Grover adaptive search (GAS) quantum algorithm, offering quadratic speedup.","Dispersion problems are combinatorial optimization problems classified as NP-hard, which appear often in coding theory and wireless communications applications involving optimal codebook design.","In turn, GAS is a quantum exhaustive search algorithm that can be used to implement full-fledged maximum-likelihood optimal solutions.","In conventional naive formulations however, it is typical to rely on a binary vector spaces, resulting in search space sizes prohibitive even for GAS.","To circumvent this challenge, we instead formulate the search of optimal dispersion problem over Dicke states, an equal superposition of binary vectors with equal Hamming weights, which significantly reduces the search space leading to a simplification of the quantum circuit via the elimination of penalty terms.","Additionally, we propose a method to replace distance coefficients with their ranks, contributing to the reduction of the number of qubits.","Our analysis demonstrates that as a result of the proposed techniques a reduction in query complexity compared to the conventional GAS using Hadamard transform is achieved, enhancing the feasibility of the quantum-based solution of the dispersion problem."],"url":"http://arxiv.org/abs/2406.07187v1","category":"quant-ph"}
{"created":"2024-06-11 09:50:42","title":"Solving singular generalized eigenvalue problems. Part III: structure preservation","abstract":"In Parts I and II of this series of papers, three new methods for the computation of eigenvalues of singular pencils were developed: rank-completing perturbations, rank-projections, and augmentation. It was observed that a straightforward structure-preserving adaption for symmetric pencils was not possible and it was left as an open question how to address this challenge. In this Part III, it is shown how the observed issue can be circumvented by using Hermitian perturbations. This leads to structure-preserving analogues of the three techniques from Parts I and II for Hermitian pencils (including real symmetric pencils) as well as for related structures. It is an important feature of these methods that the sign characteristic of the given pencil is preserved. As an application, it is shown that the resulting methods can be used to solve systems of bivariate polynomials.","sentences":["In Parts I and II of this series of papers, three new methods for the computation of eigenvalues of singular pencils were developed: rank-completing perturbations, rank-projections, and augmentation.","It was observed that a straightforward structure-preserving adaption for symmetric pencils was not possible and it was left as an open question how to address this challenge.","In this Part III, it is shown how the observed issue can be circumvented by using Hermitian perturbations.","This leads to structure-preserving analogues of the three techniques from Parts I and II for Hermitian pencils (including real symmetric pencils) as well as for related structures.","It is an important feature of these methods that the sign characteristic of the given pencil is preserved.","As an application, it is shown that the resulting methods can be used to solve systems of bivariate polynomials."],"url":"http://arxiv.org/abs/2406.07109v1","category":"math.NA"}
{"created":"2024-06-11 09:49:47","title":"On the power of adaption and randomization","abstract":"We present bounds between different widths of convex subsets of Banach spaces, including Gelfand and Bernstein widths. Using this, and some relations between widths and minimal errors, we obtain bounds on the maximal gain of adaptive and randomized algorithms over non-adaptive, deterministic ones for approximating linear operators on convex sets. Our results also apply to the approximation of embeddings into the space of bounded functions based on function evaluations, i.e., to sampling recovery in the uniform norm. We conclude with a list of open problems.","sentences":["We present bounds between different widths of convex subsets of Banach spaces, including Gelfand and Bernstein widths.","Using this, and some relations between widths and minimal errors, we obtain bounds on the maximal gain of adaptive and randomized algorithms over non-adaptive, deterministic ones for approximating linear operators on convex sets.","Our results also apply to the approximation of embeddings into the space of bounded functions based on function evaluations, i.e., to sampling recovery in the uniform norm.","We conclude with a list of open problems."],"url":"http://arxiv.org/abs/2406.07108v1","category":"math.NA"}
{"created":"2024-06-11 09:49:00","title":"Agnostic Sharpness-Aware Minimization","abstract":"Sharpness-aware minimization (SAM) has been instrumental in improving deep neural network training by minimizing both the training loss and the sharpness of the loss landscape, leading the model into flatter minima that are associated with better generalization properties. In another aspect, Model-Agnostic Meta-Learning (MAML) is a framework designed to improve the adaptability of models. MAML optimizes a set of meta-models that are specifically tailored for quick adaptation to multiple tasks with minimal fine-tuning steps and can generalize well with limited data. In this work, we explore the connection between SAM and MAML, particularly in terms of enhancing model generalization. We introduce Agnostic-SAM, a novel approach that combines the principles of both SAM and MAML. Agnostic-SAM adapts the core idea of SAM by optimizing the model towards wider local minima using training data, while concurrently maintaining low loss values on validation data. By doing so, it seeks flatter minima that are not only robust to small perturbations but also less vulnerable to data distributional shift problems. Our experimental results demonstrate that Agnostic-SAM significantly improves generalization over baselines across a range of datasets and under challenging conditions such as noisy labels and data limitation.","sentences":["Sharpness-aware minimization (SAM) has been instrumental in improving deep neural network training by minimizing both the training loss and the sharpness of the loss landscape, leading the model into flatter minima that are associated with better generalization properties.","In another aspect, Model-Agnostic Meta-Learning (MAML) is a framework designed to improve the adaptability of models.","MAML optimizes a set of meta-models that are specifically tailored for quick adaptation to multiple tasks with minimal fine-tuning steps and can generalize well with limited data.","In this work, we explore the connection between SAM and MAML, particularly in terms of enhancing model generalization.","We introduce Agnostic-SAM, a novel approach that combines the principles of both SAM and MAML.","Agnostic-SAM adapts the core idea of SAM by optimizing the model towards wider local minima using training data, while concurrently maintaining low loss values on validation data.","By doing so, it seeks flatter minima that are not only robust to small perturbations but also less vulnerable to data distributional shift problems.","Our experimental results demonstrate that Agnostic-SAM significantly improves generalization over baselines across a range of datasets and under challenging conditions such as noisy labels and data limitation."],"url":"http://arxiv.org/abs/2406.07107v2","category":"cs.LG"}
{"created":"2024-06-11 08:59:34","title":"Adaptive Control: Algorithms, Analysis and Applications","abstract":"Adaptive control provides techniques for adjusting control parameters in real time to maintain system performance despite unknown or changing process parameters. These methods use real data to tune controllers and adjust plant models or controller parameters. The field has progressed significantly since the 1970s, helped by digital computers. Early applications offered essential feedback, and theoretical advances solved many basic problems.   This book comprehensively treats adaptive control, guiding readers from basic problems to analytical solutions with practical applications. Presenting a unified view is challenging due to various design steps and applications. However, a coherent presentation of basic techniques is now possible. The book uses a discrete-time approach to reflect the role of digital computers and shares practical experiences and understanding of different control designs.   Mathematical aspects of synthesizing and analyzing algorithms are emphasized, though they alone may not solve practical problems. The book includes applications of control techniques but stresses that a solid mathematical understanding is crucial for creatively applying them to new challenges. Mathematical synthesis and analysis are highlighted, but they must be supplemented with practical problem-solving and algorithm modifications for specific applications.","sentences":["Adaptive control provides techniques for adjusting control parameters in real time to maintain system performance despite unknown or changing process parameters.","These methods use real data to tune controllers and adjust plant models or controller parameters.","The field has progressed significantly since the 1970s, helped by digital computers.","Early applications offered essential feedback, and theoretical advances solved many basic problems.   ","This book comprehensively treats adaptive control, guiding readers from basic problems to analytical solutions with practical applications.","Presenting a unified view is challenging due to various design steps and applications.","However, a coherent presentation of basic techniques is now possible.","The book uses a discrete-time approach to reflect the role of digital computers and shares practical experiences and understanding of different control designs.   ","Mathematical aspects of synthesizing and analyzing algorithms are emphasized, though they alone may not solve practical problems.","The book includes applications of control techniques but stresses that a solid mathematical understanding is crucial for creatively applying them to new challenges.","Mathematical synthesis and analysis are highlighted, but they must be supplemented with practical problem-solving and algorithm modifications for specific applications."],"url":"http://arxiv.org/abs/2406.07073v1","category":"eess.SY"}
{"created":"2024-06-11 08:56:08","title":"Optimal Gait Control for a Tendon-driven Soft Quadruped Robot by Model-based Reinforcement Learning","abstract":"This study presents an innovative approach to optimal gait control for a soft quadruped robot enabled by four Compressible Tendon-driven Soft Actuators (CTSAs). Improving our previous studies of using model-free reinforcement learning for gait control, we employ model-based reinforcement learning (MBRL) to further enhance the performance of the gait controller. Compared to rigid robots, the proposed soft quadruped robot has better safety, less weight, and a simpler mechanism for fabrication and control. However, the primary challenge lies in developing sophisticated control algorithms to attain optimal gait control for fast and stable locomotion. The research employs a multi-stage methodology, including state space restriction, data-driven model training, and reinforcement learning algorithm development. Compared to benchmark methods, the proposed MBRL algorithm, combined with post-training, significantly improves the efficiency and performance of gait control policies. The developed policy is both robust and adaptable to the robot's deformable morphology. The study concludes by highlighting the practical applicability of these findings in real-world scenarios.","sentences":["This study presents an innovative approach to optimal gait control for a soft quadruped robot enabled by four Compressible Tendon-driven Soft Actuators (CTSAs).","Improving our previous studies of using model-free reinforcement learning for gait control, we employ model-based reinforcement learning (MBRL) to further enhance the performance of the gait controller.","Compared to rigid robots, the proposed soft quadruped robot has better safety, less weight, and a simpler mechanism for fabrication and control.","However, the primary challenge lies in developing sophisticated control algorithms to attain optimal gait control for fast and stable locomotion.","The research employs a multi-stage methodology, including state space restriction, data-driven model training, and reinforcement learning algorithm development.","Compared to benchmark methods, the proposed MBRL algorithm, combined with post-training, significantly improves the efficiency and performance of gait control policies.","The developed policy is both robust and adaptable to the robot's deformable morphology.","The study concludes by highlighting the practical applicability of these findings in real-world scenarios."],"url":"http://arxiv.org/abs/2406.07069v1","category":"cs.RO"}
{"created":"2024-06-11 08:47:02","title":"Optimal Gait Design for a Soft Quadruped Robot via Multi-fidelity Bayesian Optimization","abstract":"This study focuses on the locomotion capability improvement in a tendon-driven soft quadruped robot through an online adaptive learning approach. Leveraging the inverse kinematics model of the soft quadruped robot, we employ a central pattern generator to design a parametric gait pattern, and use Bayesian optimization (BO) to find the optimal parameters. Further, to address the challenges of modeling discrepancies, we implement a multi-fidelity BO approach, combining data from both simulation and physical experiments throughout training and optimization. This strategy enables the adaptive refinement of the gait pattern and ensures a smooth transition from simulation to real-world deployment for the controller. Moreover, we integrate a computational task off-loading architecture by edge computing, which reduces the onboard computational and memory overhead, to improve real-time control performance and facilitate an effective online learning process. The proposed approach successfully achieves optimal walking gait design for physical deployment with high efficiency, effectively addressing challenges related to the reality gap in soft robotics.","sentences":["This study focuses on the locomotion capability improvement in a tendon-driven soft quadruped robot through an online adaptive learning approach.","Leveraging the inverse kinematics model of the soft quadruped robot, we employ a central pattern generator to design a parametric gait pattern, and use Bayesian optimization (BO) to find the optimal parameters.","Further, to address the challenges of modeling discrepancies, we implement a multi-fidelity BO approach, combining data from both simulation and physical experiments throughout training and optimization.","This strategy enables the adaptive refinement of the gait pattern and ensures a smooth transition from simulation to real-world deployment for the controller.","Moreover, we integrate a computational task off-loading architecture by edge computing, which reduces the onboard computational and memory overhead, to improve real-time control performance and facilitate an effective online learning process.","The proposed approach successfully achieves optimal walking gait design for physical deployment with high efficiency, effectively addressing challenges related to the reality gap in soft robotics."],"url":"http://arxiv.org/abs/2406.07065v1","category":"cs.RO"}
{"created":"2024-06-11 08:37:31","title":"Adaptive quantum optimization algorithms for programmable atom-cavity systems","abstract":"Developing quantum algorithms adaptive to specific constraints of near-term devices is an essential step towards practical quantum advantage. In a recent work [Phys. Rev. Lett. 131, 103601(2023)], we show cold atoms in an optical cavity can be built as a universal quantum optimizer with programmable all-to-all interactions, and the effective Hamiltonian for atoms directly encodes number partitioning problems (NPPs). Here, we numerically investigate the performance of quantum annealing (QA) and quantum approximate optimization algorithm (QAOA) to find the solution of NPP that is encoded in the ground state of atomic qubits. We find the success probability of the standard QA decays rapidly with the problem size. The optimized annealing path or inhomogeneous driving fields only lead to mild improvement on the success probability. Similarly, the standard QAOA always gets trapped in a false local minimum, and there is no significant performance improvement as we increase the depth of the quantum circuit. Inspired by the counterdiabatic driving, we propose an adaptive ansatz of QAOA which releases the parameter freedom of the NPP Hamiltonian to match higher-order counterdiabatic terms. Through numerical simulations, we find that our adaptive QAOA can achieve the optimal solution within very small circuit depth. It is thus worth paying the extra optimization cost of additional parameters for improving QAOA performance. Therefore, our adaptive QAOA provides a promising choice for programmable atom-cavity systems to demonstrate competitive computational power within its quantum coherence time.","sentences":["Developing quantum algorithms adaptive to specific constraints of near-term devices is an essential step towards practical quantum advantage.","In a recent work [Phys. Rev. Lett.","131, 103601(2023)], we show cold atoms in an optical cavity can be built as a universal quantum optimizer with programmable all-to-all interactions, and the effective Hamiltonian for atoms directly encodes number partitioning problems (NPPs).","Here, we numerically investigate the performance of quantum annealing (QA) and quantum approximate optimization algorithm (QAOA) to find the solution of NPP that is encoded in the ground state of atomic qubits.","We find the success probability of the standard QA decays rapidly with the problem size.","The optimized annealing path or inhomogeneous driving fields only lead to mild improvement on the success probability.","Similarly, the standard QAOA always gets trapped in a false local minimum, and there is no significant performance improvement as we increase the depth of the quantum circuit.","Inspired by the counterdiabatic driving, we propose an adaptive ansatz of QAOA which releases the parameter freedom of the NPP Hamiltonian to match higher-order counterdiabatic terms.","Through numerical simulations, we find that our adaptive QAOA can achieve the optimal solution within very small circuit depth.","It is thus worth paying the extra optimization cost of additional parameters for improving QAOA performance.","Therefore, our adaptive QAOA provides a promising choice for programmable atom-cavity systems to demonstrate competitive computational power within its quantum coherence time."],"url":"http://arxiv.org/abs/2406.07055v1","category":"quant-ph"}
{"created":"2024-06-11 08:34:59","title":"MPSDynamics.jl: Tensor network simulations for finite-temperature (non-Markovian) open quantum system dynamics","abstract":"The MPSDynamics.jl package provides an easy to use interface for performing open quantum systems simulations at zero and finite temperatures. The package has been developed with the aim of studying non-Markovian open system dynamics using the state-of-the-art numerically exact Thermalized-Time Evolving Density operator with Orthonormal Polynomials Algorithm (T-TEDOPA) based on environment chain mapping. The simulations rely on a tensor network representation of the quantum states as matrix product states (MPS) and tree tensor network (TTN) states. Written in the Julia programming language, MPSDynamics.jl is a versatile open-source package providing a choice of several variants of the Time-Dependent Variational Principle (TDVP) method for time evolution (including novel bond-adaptive one-site algorithms). The package also provides strong support for the measurement of single and multi-site observables, as well as the storing and logging of data, which makes it a useful tool for the study of many-body physics. It currently handles long-range interactions, time-dependent Hamiltonians, multiple environments, bosonic and fermionic environments, and joint system-environment observables.","sentences":["The MPSDynamics.jl package provides an easy to use interface for performing open quantum systems simulations at zero and finite temperatures.","The package has been developed with the aim of studying non-Markovian open system dynamics using the state-of-the-art numerically exact Thermalized-Time Evolving Density operator with Orthonormal Polynomials Algorithm (T-TEDOPA) based on environment chain mapping.","The simulations rely on a tensor network representation of the quantum states as matrix product states (MPS) and tree tensor network (TTN) states.","Written in the Julia programming language, MPSDynamics.jl is a versatile open-source package providing a choice of several variants of the Time-Dependent Variational Principle (TDVP) method for time evolution (including novel bond-adaptive one-site algorithms).","The package also provides strong support for the measurement of single and multi-site observables, as well as the storing and logging of data, which makes it a useful tool for the study of many-body physics.","It currently handles long-range interactions, time-dependent Hamiltonians, multiple environments, bosonic and fermionic environments, and joint system-environment observables."],"url":"http://arxiv.org/abs/2406.07052v1","category":"quant-ph"}
{"created":"2024-06-11 08:26:42","title":"DualMamba: A Lightweight Spectral-Spatial Mamba-Convolution Network for Hyperspectral Image Classification","abstract":"The effectiveness and efficiency of modeling complex spectral-spatial relations are both crucial for Hyperspectral image (HSI) classification. Most existing methods based on CNNs and transformers still suffer from heavy computational burdens and have room for improvement in capturing the global-local spectral-spatial feature representation. To this end, we propose a novel lightweight parallel design called lightweight dual-stream Mamba-convolution network (DualMamba) for HSI classification. Specifically, a parallel lightweight Mamba and CNN block are first developed to extract global and local spectral-spatial features. First, the cross-attention spectral-spatial Mamba module is proposed to leverage the global modeling of Mamba at linear complexity. Within this module, dynamic positional embedding is designed to enhance the spatial location information of visual sequences. The lightweight spectral/spatial Mamba blocks comprise an efficient scanning strategy and a lightweight Mamba design to efficiently extract global spectral-spatial features. And the cross-attention spectral-spatial fusion is designed to learn cross-correlation and fuse spectral-spatial features. Second, the lightweight spectral-spatial residual convolution module is proposed with lightweight spectral and spatial branches to extract local spectral-spatial features through residual learning. Finally, the adaptive global-local fusion is proposed to dynamically combine global Mamba features and local convolution features for a global-local spectral-spatial representation. Compared with state-of-the-art HSI classification methods, experimental results demonstrate that DualMamba achieves significant classification accuracy on three public HSI datasets and a superior reduction in model parameters and floating point operations (FLOPs).","sentences":["The effectiveness and efficiency of modeling complex spectral-spatial relations are both crucial for Hyperspectral image (HSI) classification.","Most existing methods based on CNNs and transformers still suffer from heavy computational burdens and have room for improvement in capturing the global-local spectral-spatial feature representation.","To this end, we propose a novel lightweight parallel design called lightweight dual-stream Mamba-convolution network (DualMamba) for HSI classification.","Specifically, a parallel lightweight Mamba and CNN block are first developed to extract global and local spectral-spatial features.","First, the cross-attention spectral-spatial Mamba module is proposed to leverage the global modeling of Mamba at linear complexity.","Within this module, dynamic positional embedding is designed to enhance the spatial location information of visual sequences.","The lightweight spectral/spatial Mamba blocks comprise an efficient scanning strategy and a lightweight Mamba design to efficiently extract global spectral-spatial features.","And the cross-attention spectral-spatial fusion is designed to learn cross-correlation and fuse spectral-spatial features.","Second, the lightweight spectral-spatial residual convolution module is proposed with lightweight spectral and spatial branches to extract local spectral-spatial features through residual learning.","Finally, the adaptive global-local fusion is proposed to dynamically combine global Mamba features and local convolution features for a global-local spectral-spatial representation.","Compared with state-of-the-art HSI classification methods, experimental results demonstrate that DualMamba achieves significant classification accuracy on three public HSI datasets and a superior reduction in model parameters and floating point operations (FLOPs)."],"url":"http://arxiv.org/abs/2406.07050v1","category":"cs.CV"}
{"created":"2024-06-11 07:59:17","title":"Integrating Domain Knowledge for handling Limited Data in Offline RL","abstract":"With the ability to learn from static datasets, Offline Reinforcement Learning (RL) emerges as a compelling avenue for real-world applications. However, state-of-the-art offline RL algorithms perform sub-optimally when confronted with limited data confined to specific regions within the state space. The performance degradation is attributed to the inability of offline RL algorithms to learn appropriate actions for rare or unseen observations. This paper proposes a novel domain knowledge-based regularization technique and adaptively refines the initial domain knowledge to considerably boost performance in limited data with partially omitted states. The key insight is that the regularization term mitigates erroneous actions for sparse samples and unobserved states covered by domain knowledge. Empirical evaluations on standard discrete environment datasets demonstrate a substantial average performance increase of at least 27% compared to existing offline RL algorithms operating on limited data.","sentences":["With the ability to learn from static datasets, Offline Reinforcement Learning (RL) emerges as a compelling avenue for real-world applications.","However, state-of-the-art offline RL algorithms perform sub-optimally when confronted with limited data confined to specific regions within the state space.","The performance degradation is attributed to the inability of offline RL algorithms to learn appropriate actions for rare or unseen observations.","This paper proposes a novel domain knowledge-based regularization technique and adaptively refines the initial domain knowledge to considerably boost performance in limited data with partially omitted states.","The key insight is that the regularization term mitigates erroneous actions for sparse samples and unobserved states covered by domain knowledge.","Empirical evaluations on standard discrete environment datasets demonstrate a substantial average performance increase of at least 27% compared to existing offline RL algorithms operating on limited data."],"url":"http://arxiv.org/abs/2406.07041v1","category":"cs.LG"}
{"created":"2024-06-11 07:37:33","title":"Exoplanets in reflected starlight with dual-field interferometry: A case for shorter wavelengths and a fifth Unit Telescope at VLTI/Paranal","abstract":"The direct observation of cold and temperate planets within 1 to 10 AU would be extremely valuable for uncovering their atmospheric compositions but remains a formidable challenge with current astronomical methods. Ground-based optical interferometry, capable of high angular-resolution imaging, offers a promising avenue for studying these exoplanets, complementing space-based observations. Our objective is to explore the fundamental limits of dual-field interferometry and assess its potential for characterizing exoplanets in reflected light using the Very Large Telescope Interferometer (VLTI). We developed analytical expressions to describe the performance of dual-field interferometry and integrated these with simulations of atmospheric wavefronts corrected by extreme Adaptive Optics. An analytical solution for optimal phase apodization was formulated to enhance starlight rejection when injected into a single-mode fibre. This framework was applied to determine the detectability of known exoplanets in reflected light across various wavelength bands for both the current VLTI and a proposed extended version. Our results indicate that employing shorter wavelengths improves detectability, enabling at least seven Jupiter-mass exoplanets to be observed in the J band with current VLTI's baselines. Adding new baselines with lengths beyond 200 meters significantly enhances VLTI's capabilities, increasing the number of detectable exoplanets and revealing potential habitable zone candidates such as $\\tau$ Ceti e and Proxima Centauri b. To substantially improve the VLTI's exoplanet characterization capabilities, we recommend developing instrumentation at wavelengths shorter than 1$\\,\\mu$m, as well as the addition of a fifth Unit Telescope (UT5).","sentences":["The direct observation of cold and temperate planets within 1 to 10 AU would be extremely valuable for uncovering their atmospheric compositions but remains a formidable challenge with current astronomical methods.","Ground-based optical interferometry, capable of high angular-resolution imaging, offers a promising avenue for studying these exoplanets, complementing space-based observations.","Our objective is to explore the fundamental limits of dual-field interferometry and assess its potential for characterizing exoplanets in reflected light using the Very Large Telescope Interferometer (VLTI).","We developed analytical expressions to describe the performance of dual-field interferometry and integrated these with simulations of atmospheric wavefronts corrected by extreme Adaptive Optics.","An analytical solution for optimal phase apodization was formulated to enhance starlight rejection when injected into a single-mode fibre.","This framework was applied to determine the detectability of known exoplanets in reflected light across various wavelength bands for both the current VLTI and a proposed extended version.","Our results indicate that employing shorter wavelengths improves detectability, enabling at least seven Jupiter-mass exoplanets to be observed in the J band with current VLTI's baselines.","Adding new baselines with lengths beyond 200 meters significantly enhances VLTI's capabilities, increasing the number of detectable exoplanets and revealing potential habitable zone candidates such as $\\tau$ Ceti e and Proxima Centauri b.","To substantially improve the VLTI's exoplanet characterization capabilities, we recommend developing instrumentation at wavelengths shorter than 1$\\,\\mu$m, as well as the addition of a fifth Unit Telescope (UT5)."],"url":"http://arxiv.org/abs/2406.07030v1","category":"astro-ph.EP"}
{"created":"2024-06-11 07:32:25","title":"Heterogeneous Learning Rate Scheduling for Neural Architecture Search on Long-Tailed Datasets","abstract":"In this paper, we attempt to address the challenge of applying Neural Architecture Search (NAS) algorithms, specifically the Differentiable Architecture Search (DARTS), to long-tailed datasets where class distribution is highly imbalanced. We observe that traditional re-sampling and re-weighting techniques, which are effective in standard classification tasks, lead to performance degradation when combined with DARTS. To mitigate this, we propose a novel adaptive learning rate scheduling strategy tailored for the architecture parameters of DARTS when integrated with the Bilateral Branch Network (BBN) for handling imbalanced datasets. Our approach dynamically adjusts the learning rate of the architecture parameters based on the training epoch, preventing the disruption of well-trained representations in the later stages of training. Additionally, we explore the impact of branch mixing factors on the algorithm's performance. Through extensive experiments on the CIFAR-10 dataset with an artificially induced long-tailed distribution, we demonstrate that our method achieves comparable accuracy to using DARTS alone. And the experiment results suggest that re-sampling methods inherently harm the performance of the DARTS algorithm. Our findings highlight the importance of careful data augment when applying DNAS to imbalanced learning scenarios.","sentences":["In this paper, we attempt to address the challenge of applying Neural Architecture Search (NAS) algorithms, specifically the Differentiable Architecture Search (DARTS), to long-tailed datasets where class distribution is highly imbalanced.","We observe that traditional re-sampling and re-weighting techniques, which are effective in standard classification tasks, lead to performance degradation when combined with DARTS.","To mitigate this, we propose a novel adaptive learning rate scheduling strategy tailored for the architecture parameters of DARTS when integrated with the Bilateral Branch Network (BBN) for handling imbalanced datasets.","Our approach dynamically adjusts the learning rate of the architecture parameters based on the training epoch, preventing the disruption of well-trained representations in the later stages of training.","Additionally, we explore the impact of branch mixing factors on the algorithm's performance.","Through extensive experiments on the CIFAR-10 dataset with an artificially induced long-tailed distribution, we demonstrate that our method achieves comparable accuracy to using DARTS alone.","And the experiment results suggest that re-sampling methods inherently harm the performance of the DARTS algorithm.","Our findings highlight the importance of careful data augment when applying DNAS to imbalanced learning scenarios."],"url":"http://arxiv.org/abs/2406.07028v1","category":"cs.LG"}
{"created":"2024-06-11 07:00:08","title":"Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference","abstract":"The customization of large language models (LLMs) for user-specified tasks gets important. However, maintaining all the customized LLMs on cloud servers incurs substantial memory and computational overheads, and uploading user data can also lead to privacy concerns. On-device LLMs can offer a promising solution by mitigating these issues. Yet, the performance of on-device LLMs is inherently constrained by the limitations of small-scaled models. To overcome these restrictions, we first propose Crayon, a novel approach for on-device LLM customization. Crayon begins by constructing a pool of diverse base adapters, and then we instantly blend them into a customized adapter without extra training. In addition, we develop a device-server hybrid inference strategy, which deftly allocates more demanding queries or non-customized tasks to a larger, more capable LLM on a server. This ensures optimal performance without sacrificing the benefits of on-device customization. We carefully craft a novel benchmark from multiple question-answer datasets, and show the efficacy of our method in the LLM customization.","sentences":["The customization of large language models (LLMs) for user-specified tasks gets important.","However, maintaining all the customized LLMs on cloud servers incurs substantial memory and computational overheads, and uploading user data can also lead to privacy concerns.","On-device LLMs can offer a promising solution by mitigating these issues.","Yet, the performance of on-device LLMs is inherently constrained by the limitations of small-scaled models.","To overcome these restrictions, we first propose Crayon, a novel approach for on-device LLM customization.","Crayon begins by constructing a pool of diverse base adapters, and then we instantly blend them into a customized adapter without extra training.","In addition, we develop a device-server hybrid inference strategy, which deftly allocates more demanding queries or non-customized tasks to a larger, more capable LLM on a server.","This ensures optimal performance without sacrificing the benefits of on-device customization.","We carefully craft a novel benchmark from multiple question-answer datasets, and show the efficacy of our method in the LLM customization."],"url":"http://arxiv.org/abs/2406.07007v1","category":"cs.CL"}
{"created":"2024-06-11 06:28:21","title":"On the H\u00f6lder Stability of Multiset and Graph Neural Networks","abstract":"Famously, multiset neural networks based on sum-pooling can separate all distinct multisets, and as a result can be used by message passing neural networks (MPNNs) to separate all pairs of graphs that can be separated by the 1-WL graph isomorphism test. However, the quality of this separation may be very weak, to the extent that the embeddings of \"separable\" multisets and graphs might even be considered identical when using fixed finite precision.   In this work, we propose to fully analyze the separation quality of multiset models and MPNNs via a novel adaptation of Lipschitz and H\\\"{o}lder continuity to parametric functions. We prove that common sum-based models are lower-H\\\"{o}lder continuous, with a H\\\"{o}lder exponent that decays rapidly with the network's depth. Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful MPNNs. To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz continuous. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks.","sentences":["Famously, multiset neural networks based on sum-pooling can separate all distinct multisets, and as a result can be used by message passing neural networks (MPNNs) to separate all pairs of graphs that can be separated by the 1-WL graph isomorphism test.","However, the quality of this separation may be very weak, to the extent that the embeddings of \"separable\" multisets and graphs might even be considered identical when using fixed finite precision.   ","In this work, we propose to fully analyze the separation quality of multiset models and MPNNs via a novel adaptation of Lipschitz and H\\\"{o}lder continuity to parametric functions.","We prove that common sum-based models are lower-H\\\"{o}lder continuous, with a H\\\"{o}lder exponent that decays rapidly with the network's depth.","Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful MPNNs.","To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz continuous.","We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks."],"url":"http://arxiv.org/abs/2406.06984v1","category":"cs.LG"}
{"created":"2024-06-11 05:48:04","title":"Evolving from Single-modal to Multi-modal Facial Deepfake Detection: A Survey","abstract":"This survey addresses the critical challenge of deepfake detection amidst the rapid advancements in artificial intelligence. As AI-generated media, including video, audio and text, become more realistic, the risk of misuse to spread misinformation and commit identity fraud increases. Focused on face-centric deepfakes, this work traces the evolution from traditional single-modality methods to sophisticated multi-modal approaches that handle audio-visual and text-visual scenarios. We provide comprehensive taxonomies of detection techniques, discuss the evolution of generative methods from auto-encoders and GANs to diffusion models, and categorize these technologies by their unique attributes. To our knowledge, this is the first survey of its kind. We also explore the challenges of adapting detection methods to new generative models and enhancing the reliability and robustness of deepfake detectors, proposing directions for future research. This survey offers a detailed roadmap for researchers, supporting the development of technologies to counter the deceptive use of AI in media creation, particularly facial forgery. A curated list of all related papers can be found at \\href{https://github.com/qiqitao77/Comprehensive-Advances-in-Deepfake-Detection-Spanning-Diverse-Modalities}{https://github.com/qiqitao77/Awesome-Comprehensive-Deepfake-Detection}.","sentences":["This survey addresses the critical challenge of deepfake detection amidst the rapid advancements in artificial intelligence.","As AI-generated media, including video, audio and text, become more realistic, the risk of misuse to spread misinformation and commit identity fraud increases.","Focused on face-centric deepfakes, this work traces the evolution from traditional single-modality methods to sophisticated multi-modal approaches that handle audio-visual and text-visual scenarios.","We provide comprehensive taxonomies of detection techniques, discuss the evolution of generative methods from auto-encoders and GANs to diffusion models, and categorize these technologies by their unique attributes.","To our knowledge, this is the first survey of its kind.","We also explore the challenges of adapting detection methods to new generative models and enhancing the reliability and robustness of deepfake detectors, proposing directions for future research.","This survey offers a detailed roadmap for researchers, supporting the development of technologies to counter the deceptive use of AI in media creation, particularly facial forgery.","A curated list of all related papers can be found at \\href{https://github.com/qiqitao77/Comprehensive-Advances-in-Deepfake-Detection-Spanning-Diverse-Modalities}{https://github.com/qiqitao77/Awesome-Comprehensive-Deepfake-Detection}."],"url":"http://arxiv.org/abs/2406.06965v1","category":"cs.CV"}
{"created":"2024-06-11 05:25:25","title":"Stepwise Regression and Pre-trained Edge for Robust Stereo Matching","abstract":"Due to the difficulty in obtaining real samples and ground truth, the generalization performance and the fine-tuned performance are critical for the feasibility of stereo matching methods in real-world applications. However, the presence of substantial disparity distributions and density variations across different datasets presents significant challenges for the generalization and fine-tuning of the model. In this paper, we propose a novel stereo matching method, called SR-Stereo, which mitigates the distributional differences across different datasets by predicting the disparity clips and uses a loss weight related to the regression target scale to improve the accuracy of the disparity clips. Moreover, this stepwise regression architecture can be easily extended to existing iteration-based methods to improve the performance without changing the structure. In addition, to mitigate the edge blurring of the fine-tuned model on sparse ground truth, we propose Domain Adaptation Based on Pre-trained Edges (DAPE). Specifically, we use the predicted disparity and RGB image to estimate the edge map of the target domain image. The edge map is filtered to generate edge map background pseudo-labels, which together with the sparse ground truth disparity on the target domain are used as a supervision to jointly fine-tune the pre-trained stereo matching model. These proposed methods are extensively evaluated on SceneFlow, KITTI, Middbury 2014 and ETH3D. The SR-Stereo achieves competitive disparity estimation performance and state-of-the-art cross-domain generalisation performance. Meanwhile, the proposed DAPE significantly improves the disparity estimation performance of fine-tuned models, especially in the textureless and detail regions.","sentences":["Due to the difficulty in obtaining real samples and ground truth, the generalization performance and the fine-tuned performance are critical for the feasibility of stereo matching methods in real-world applications.","However, the presence of substantial disparity distributions and density variations across different datasets presents significant challenges for the generalization and fine-tuning of the model.","In this paper, we propose a novel stereo matching method, called SR-Stereo, which mitigates the distributional differences across different datasets by predicting the disparity clips and uses a loss weight related to the regression target scale to improve the accuracy of the disparity clips.","Moreover, this stepwise regression architecture can be easily extended to existing iteration-based methods to improve the performance without changing the structure.","In addition, to mitigate the edge blurring of the fine-tuned model on sparse ground truth, we propose Domain Adaptation Based on Pre-trained Edges (DAPE).","Specifically, we use the predicted disparity and RGB image to estimate the edge map of the target domain image.","The edge map is filtered to generate edge map background pseudo-labels, which together with the sparse ground truth disparity on the target domain are used as a supervision to jointly fine-tune the pre-trained stereo matching model.","These proposed methods are extensively evaluated on SceneFlow, KITTI, Middbury 2014 and ETH3D. The SR-Stereo achieves competitive disparity estimation performance and state-of-the-art cross-domain generalisation performance.","Meanwhile, the proposed DAPE significantly improves the disparity estimation performance of fine-tuned models, especially in the textureless and detail regions."],"url":"http://arxiv.org/abs/2406.06953v1","category":"cs.CV"}
{"created":"2024-06-11 03:15:47","title":"Scalability in Workforce Management: Applying Scalability Principles to Foster a Four-Day Work Week","abstract":"The traditional five-day workweek faces mounting challenges, prompting exploration of alternative models like the four-day workweek. This research explores the transformative potential of scalability principles derived from cloud computing and IT in redefining workforce management for a four-day workweek. The study employs a Multivocal Literacy Research methodology, combining grey literature and systematic review approaches. Through a comprehensive review of related work, the challenges, and benefits of transitioning to a four-day workweek are explored. Pilot programs, clear communication, and agility are identified as critical success factors. The synthesis of scalability principles in workforce management serves as a powerful framework for a smooth transition towards a four-day workweek. By prioritizing adaptability, dynamic resource allocation, and data-driven insights, organizations can unlock the full potential of a compressed work schedule. This research contributes valuable insights for organizations seeking to thrive in the evolving landscape of modern work structures and prioritizing employee well-being.","sentences":["The traditional five-day workweek faces mounting challenges, prompting exploration of alternative models like the four-day workweek.","This research explores the transformative potential of scalability principles derived from cloud computing and IT in redefining workforce management for a four-day workweek.","The study employs a Multivocal Literacy Research methodology, combining grey literature and systematic review approaches.","Through a comprehensive review of related work, the challenges, and benefits of transitioning to a four-day workweek are explored.","Pilot programs, clear communication, and agility are identified as critical success factors.","The synthesis of scalability principles in workforce management serves as a powerful framework for a smooth transition towards a four-day workweek.","By prioritizing adaptability, dynamic resource allocation, and data-driven insights, organizations can unlock the full potential of a compressed work schedule.","This research contributes valuable insights for organizations seeking to thrive in the evolving landscape of modern work structures and prioritizing employee well-being."],"url":"http://arxiv.org/abs/2406.06915v1","category":"cs.CY"}
{"created":"2024-06-11 02:33:47","title":"SmartPQ: An Adaptive Concurrent Priority Queue for NUMA Architectures","abstract":"Concurrent priority queues are widely used in important workloads, such as graph applications and discrete event simulations. However, designing scalable concurrent priority queues for NUMA architectures is challenging. Even though several NUMA-oblivious implementations can scale up to a high number of threads, exploiting the potential parallelism of insert operation, NUMA-oblivious implementations scale poorly in deleteMin-dominated workloads. This is because all threads compete for accessing the same memory locations, i.e., the highest-priority element of the queue, thus incurring excessive cache coherence traffic and non-uniform memory accesses between nodes of a NUMA system. In such scenarios, NUMA-aware implementations are typically used to improve system performance on a NUMA system.   In this work, we propose an adaptive priority queue, called SmartPQ. SmartPQ tunes itself by switching between a NUMA-oblivious and a NUMA-aware algorithmic mode to achieve high performance under all various contention scenarios. SmartPQ has two key components. First, it is built on top of NUMA Node Delegation (Nuddle), a generic low-overhead technique to construct efficient NUMA-aware data structures using any arbitrary concurrent NUMA-oblivious implementation as its backbone. Second, SmartPQ integrates a lightweight decision making mechanism to decide when to switch between NUMA-oblivious and NUMA-aware algorithmic modes. Our evaluation shows that, in NUMA systems, SmartPQ performs best in all various contention scenarios with 87.9% success rate, and dynamically adapts between NUMA-aware and NUMA-oblivious algorithmic mode, with negligible performance overheads. SmartPQ improves performance by 1.87x on average over SprayList, the state-of-theart NUMA-oblivious priority queue.","sentences":["Concurrent priority queues are widely used in important workloads, such as graph applications and discrete event simulations.","However, designing scalable concurrent priority queues for NUMA architectures is challenging.","Even though several NUMA-oblivious implementations can scale up to a high number of threads, exploiting the potential parallelism of insert operation, NUMA-oblivious implementations scale poorly in deleteMin-dominated workloads.","This is because all threads compete for accessing the same memory locations, i.e., the highest-priority element of the queue, thus incurring excessive cache coherence traffic and non-uniform memory accesses between nodes of a NUMA system.","In such scenarios, NUMA-aware implementations are typically used to improve system performance on a NUMA system.   ","In this work, we propose an adaptive priority queue, called SmartPQ. SmartPQ tunes itself by switching between a NUMA-oblivious and a NUMA-aware algorithmic mode to achieve high performance under all various contention scenarios.","SmartPQ has two key components.","First, it is built on top of NUMA Node Delegation (Nuddle), a generic low-overhead technique to construct efficient NUMA-aware data structures using any arbitrary concurrent NUMA-oblivious implementation as its backbone.","Second, SmartPQ integrates a lightweight decision making mechanism to decide when to switch between NUMA-oblivious and NUMA-aware algorithmic modes.","Our evaluation shows that, in NUMA systems, SmartPQ performs best in all various contention scenarios with 87.9% success rate, and dynamically adapts between NUMA-aware and NUMA-oblivious algorithmic mode, with negligible performance overheads.","SmartPQ improves performance by 1.87x on average over SprayList, the state-of-theart NUMA-oblivious priority queue."],"url":"http://arxiv.org/abs/2406.06900v1","category":"cs.DC"}
{"created":"2024-06-11 02:07:47","title":"A Subjective Quality Evaluation of 3D Mesh with Dynamic Level of Detail in Virtual Reality","abstract":"3D meshes are one of the main components of Virtual Reality applications. However, many network and computational resources are required to process 3D meshes in real-time. A potential solution to this challenge is to dynamically adapt the Level of Detail (LoD) of a 3D mesh based on the object's position and the user's viewpoint. In this paper, we conduct a subjective study to investigate users' quality perception of 3D meshes with dynamic Level of Detail in a Virtual Reality environment. The subjective experiment is carried out with five 3D meshes of different characteristics, four Levels of Detail, and four distance settings. The results of the experiment show that the impact of the dynamic level of detail depends on both the position of the 3D object in the virtual world and the number of vertices of the original mesh. In addition, we present a quality model that can accurately predict the MOS score of a LoD version of a 3D mesh from the number of vertices and the distance from the viewpoint.","sentences":["3D meshes are one of the main components of Virtual Reality applications.","However, many network and computational resources are required to process 3D meshes in real-time.","A potential solution to this challenge is to dynamically adapt the Level of Detail (LoD) of a 3D mesh based on the object's position and the user's viewpoint.","In this paper, we conduct a subjective study to investigate users' quality perception of 3D meshes with dynamic Level of Detail in a Virtual Reality environment.","The subjective experiment is carried out with five 3D meshes of different characteristics, four Levels of Detail, and four distance settings.","The results of the experiment show that the impact of the dynamic level of detail depends on both the position of the 3D object in the virtual world and the number of vertices of the original mesh.","In addition, we present a quality model that can accurately predict the MOS score of a LoD version of a 3D mesh from the number of vertices and the distance from the viewpoint."],"url":"http://arxiv.org/abs/2406.06888v1","category":"cs.MM"}
{"created":"2024-06-11 01:44:16","title":"Pseudo-Entanglement is Necessary for EFI Pairs","abstract":"Regarding minimal assumptions, most of classical cryptography is known to depend on the existence of One-Way Functions (OWFs). However, recent evidence has shown that this is not the case when considering quantum resources. Besides the well known unconditional security of Quantum Key Distribution, it is now known that computational cryptography may be built on weaker primitives than OWFs, e.g., pseudo-random states [JLS18], one-way state generators [MY23], or EFI pairs of states [BCQ23]. We consider a new quantum resource, pseudo-entanglement, and show that the existence of EFI pairs, one of the current main candidates for the weakest computational assumption for cryptography (necessary for commitments, oblivious transfer, secure multi-party computation, computational zero-knowledge proofs), implies the existence of pseudo-entanglement, as defined by [ABF+24, ABV23] under some reasonable adaptations. We prove this by constructing a new family of pseudo-entangled quantum states given only EFI pairs. Our result has important implications for the field of computational cryptography. It shows that if pseudo-entanglement does not exist, then most of cryptography cannot exist either. Moreover, it establishes pseudo-entanglement as a new minimal assumption for most of computational cryptography, which may pave the way for the unification of other assumptions into a single primitive. Finally, pseudo-entanglement connects physical phenomena and efficient computation, thus, our result strengthens the connection between cryptography and the physical world.","sentences":["Regarding minimal assumptions, most of classical cryptography is known to depend on the existence of One-Way Functions (OWFs).","However, recent evidence has shown that this is not the case when considering quantum resources.","Besides the well known unconditional security of Quantum Key Distribution, it is now known that computational cryptography may be built on weaker primitives than OWFs, e.g., pseudo-random states","[JLS18], one-way state generators [MY23], or EFI pairs of states","[BCQ23].","We consider a new quantum resource, pseudo-entanglement, and show that the existence of EFI pairs, one of the current main candidates for the weakest computational assumption for cryptography (necessary for commitments, oblivious transfer, secure multi-party computation, computational zero-knowledge proofs), implies the existence of pseudo-entanglement, as defined by [ABF+24, ABV23] under some reasonable adaptations.","We prove this by constructing a new family of pseudo-entangled quantum states given only EFI pairs.","Our result has important implications for the field of computational cryptography.","It shows that if pseudo-entanglement does not exist, then most of cryptography cannot exist either.","Moreover, it establishes pseudo-entanglement as a new minimal assumption for most of computational cryptography, which may pave the way for the unification of other assumptions into a single primitive.","Finally, pseudo-entanglement connects physical phenomena and efficient computation, thus, our result strengthens the connection between cryptography and the physical world."],"url":"http://arxiv.org/abs/2406.06881v1","category":"quant-ph"}
{"created":"2024-06-11 01:43:50","title":"Multi-Objective Sizing Optimization Method of Microgrid Considering Cost and Carbon Emissions","abstract":"Microgrid serves as a promising solution to integrate and manage distributed renewable energy resources. In this paper, we establish a stochastic multi-objective sizing optimization (SMOSO) model for microgrid planning, which fully captures the battery degradation characteristics and the total carbon emissions. The microgrid operator aims to simultaneously maximize the economic benefits and minimize carbon emissions, and the degradation of the battery energy storage system (BESS) is modeled as a nonlinear function of power throughput. A self-adaptive multi-objective genetic algorithm (SAMOGA) is proposed to solve the SMOSO model, and this algorithm is enhanced by pre-grouped hierarchical selection and self-adaptive probabilities of crossover and mutation. Several case studies are conducted to determine the microgrid size by analyzing Pareto frontiers, and the simulation results validate that the proposed method has superior performance over other algorithms on the solution quality of optimum and diversity.","sentences":["Microgrid serves as a promising solution to integrate and manage distributed renewable energy resources.","In this paper, we establish a stochastic multi-objective sizing optimization (SMOSO) model for microgrid planning, which fully captures the battery degradation characteristics and the total carbon emissions.","The microgrid operator aims to simultaneously maximize the economic benefits and minimize carbon emissions, and the degradation of the battery energy storage system (BESS) is modeled as a nonlinear function of power throughput.","A self-adaptive multi-objective genetic algorithm (SAMOGA) is proposed to solve the SMOSO model, and this algorithm is enhanced by pre-grouped hierarchical selection and self-adaptive probabilities of crossover and mutation.","Several case studies are conducted to determine the microgrid size by analyzing Pareto frontiers, and the simulation results validate that the proposed method has superior performance over other algorithms on the solution quality of optimum and diversity."],"url":"http://arxiv.org/abs/2406.06880v1","category":"eess.SY"}
{"created":"2024-06-11 01:17:37","title":"Revolutionizing Wireless Networks with Self-Supervised Learning: A Pathway to Intelligent Communications","abstract":"With the rapid proliferation of mobile devices and data, next-generation wireless communication systems face stringent requirements for ultra-low latency, ultra-high reliability, and massive connectivity. Traditional AI-driven wireless network designs, while promising, often suffer from limitations such as dependency on labeled data and poor generalization. To address these challenges, we present an integration of self-supervised learning (SSL) into wireless networks. SSL leverages large volumes of unlabeled data to train models, enhancing scalability, adaptability, and generalization. This paper offers a comprehensive overview of SSL, categorizing its application scenarios in wireless network optimization and presenting a case study on its impact on semantic communication. Our findings highlight the potentials of SSL to significantly improve wireless network performance without extensive labeled data, paving the way for more intelligent and efficient communication systems.","sentences":["With the rapid proliferation of mobile devices and data, next-generation wireless communication systems face stringent requirements for ultra-low latency, ultra-high reliability, and massive connectivity.","Traditional AI-driven wireless network designs, while promising, often suffer from limitations such as dependency on labeled data and poor generalization.","To address these challenges, we present an integration of self-supervised learning (SSL) into wireless networks.","SSL leverages large volumes of unlabeled data to train models, enhancing scalability, adaptability, and generalization.","This paper offers a comprehensive overview of SSL, categorizing its application scenarios in wireless network optimization and presenting a case study on its impact on semantic communication.","Our findings highlight the potentials of SSL to significantly improve wireless network performance without extensive labeled data, paving the way for more intelligent and efficient communication systems."],"url":"http://arxiv.org/abs/2406.06872v1","category":"eess.SP"}
{"created":"2024-06-10 23:36:58","title":"Taxes Are All You Need: Integration of Taxonomical Hierarchy Relationships into the Contrastive Loss","abstract":"In this work, we propose a novel supervised contrastive loss that enables the integration of taxonomic hierarchy information during the representation learning process. A supervised contrastive loss operates by enforcing that images with the same class label (positive samples) project closer to each other than images with differing class labels (negative samples). The advantage of this approach is that it directly penalizes the structure of the representation space itself. This enables greater flexibility with respect to encoding semantic concepts. However, the standard supervised contrastive loss only enforces semantic structure based on the downstream task (i.e. the class label). In reality, the class label is only one level of a \\emph{hierarchy of different semantic relationships known as a taxonomy}. For example, the class label is oftentimes the species of an animal, but between different classes there are higher order relationships such as all animals with wings being ``birds\". We show that by explicitly accounting for these relationships with a weighting penalty in the contrastive loss we can out-perform the supervised contrastive loss. Additionally, we demonstrate the adaptability of the notion of a taxonomy by integrating our loss into medical and noise-based settings that show performance improvements by as much as 7%.","sentences":["In this work, we propose a novel supervised contrastive loss that enables the integration of taxonomic hierarchy information during the representation learning process.","A supervised contrastive loss operates by enforcing that images with the same class label (positive samples) project closer to each other than images with differing class labels (negative samples).","The advantage of this approach is that it directly penalizes the structure of the representation space itself.","This enables greater flexibility with respect to encoding semantic concepts.","However, the standard supervised contrastive loss only enforces semantic structure based on the downstream task (i.e. the class label).","In reality, the class label is only one level of a \\emph{hierarchy of different semantic relationships known as a taxonomy}.","For example, the class label is oftentimes the species of an animal, but between different classes there are higher order relationships such as all animals with wings being ``birds\".","We show that by explicitly accounting for these relationships with a weighting penalty in the contrastive loss we can out-perform the supervised contrastive loss.","Additionally, we demonstrate the adaptability of the notion of a taxonomy by integrating our loss into medical and noise-based settings that show performance improvements by as much as 7%."],"url":"http://arxiv.org/abs/2406.06848v1","category":"cs.CV"}
{"created":"2024-06-10 23:31:36","title":"Generalized W-Net: Arbitrary-style Chinese Character Synthesization","abstract":"Synthesizing Chinese characters with consistent style using few stylized examples is challenging. Existing models struggle to generate arbitrary style characters with limited examples. In this paper, we propose the Generalized W-Net, a novel class of W-shaped architectures that addresses this. By incorporating Adaptive Instance Normalization and introducing multi-content, our approach can synthesize Chinese characters in any desired style, even with limited examples. It handles seen and unseen styles during training and can generate new character contents. Experimental results demonstrate the effectiveness of our approach.","sentences":["Synthesizing Chinese characters with consistent style using few stylized examples is challenging.","Existing models struggle to generate arbitrary style characters with limited examples.","In this paper, we propose the Generalized W-Net, a novel class of W-shaped architectures that addresses this.","By incorporating Adaptive Instance Normalization and introducing multi-content, our approach can synthesize Chinese characters in any desired style, even with limited examples.","It handles seen and unseen styles during training and can generate new character contents.","Experimental results demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2406.06847v1","category":"cs.CV"}
{"created":"2024-06-10 22:07:57","title":"Adapters Strike Back","abstract":"Adapters provide an efficient and lightweight mechanism for adapting trained transformer models to a variety of different tasks. However, they have often been found to be outperformed by other adaptation mechanisms, including low-rank adaptation. In this paper, we provide an in-depth study of adapters, their internal structure, as well as various implementation choices. We uncover pitfalls for using adapters and suggest a concrete, improved adapter architecture, called Adapter+, that not only outperforms previous adapter implementations but surpasses a number of other, more complex adaptation mechanisms in several challenging settings. Despite this, our suggested adapter is highly robust and, unlike previous work, requires little to no manual intervention when addressing a novel scenario. Adapter+ reaches state-of-the-art average accuracy on the VTAB benchmark, even without a per-task hyperparameter optimization.","sentences":["Adapters provide an efficient and lightweight mechanism for adapting trained transformer models to a variety of different tasks.","However, they have often been found to be outperformed by other adaptation mechanisms, including low-rank adaptation.","In this paper, we provide an in-depth study of adapters, their internal structure, as well as various implementation choices.","We uncover pitfalls for using adapters and suggest a concrete, improved adapter architecture, called Adapter+, that not only outperforms previous adapter implementations but surpasses a number of other, more complex adaptation mechanisms in several challenging settings.","Despite this, our suggested adapter is highly robust and, unlike previous work, requires little to no manual intervention when addressing a novel scenario.","Adapter+ reaches state-of-the-art average accuracy on the VTAB benchmark, even without a per-task hyperparameter optimization."],"url":"http://arxiv.org/abs/2406.06820v1","category":"cs.CV"}
{"created":"2024-06-10 21:44:52","title":"Stable Neighbor Denoising for Source-free Domain Adaptive Segmentation","abstract":"We study source-free unsupervised domain adaptation (SFUDA) for semantic segmentation, which aims to adapt a source-trained model to the target domain without accessing the source data. Many works have been proposed to address this challenging problem, among which uncertainty-based self-training is a predominant approach. However, without comprehensive denoising mechanisms, they still largely fall into biased estimates when dealing with different domains and confirmation bias. In this paper, we observe that pseudo-label noise is mainly contained in unstable samples in which the predictions of most pixels undergo significant variations during self-training. Inspired by this, we propose a novel mechanism to denoise unstable samples with stable ones. Specifically, we introduce the Stable Neighbor Denoising (SND) approach, which effectively discovers highly correlated stable and unstable samples by nearest neighbor retrieval and guides the reliable optimization of unstable samples by bi-level learning. Moreover, we compensate for the stable set by object-level object paste, which can further eliminate the bias caused by less learned classes. Our SND enjoys two advantages. First, SND does not require a specific segmentor structure, endowing its universality. Second, SND simultaneously addresses the issues of class, domain, and confirmation biases during adaptation, ensuring its effectiveness. Extensive experiments show that SND consistently outperforms state-of-the-art methods in various SFUDA semantic segmentation settings. In addition, SND can be easily integrated with other approaches, obtaining further improvements.","sentences":["We study source-free unsupervised domain adaptation (SFUDA) for semantic segmentation, which aims to adapt a source-trained model to the target domain without accessing the source data.","Many works have been proposed to address this challenging problem, among which uncertainty-based self-training is a predominant approach.","However, without comprehensive denoising mechanisms, they still largely fall into biased estimates when dealing with different domains and confirmation bias.","In this paper, we observe that pseudo-label noise is mainly contained in unstable samples in which the predictions of most pixels undergo significant variations during self-training.","Inspired by this, we propose a novel mechanism to denoise unstable samples with stable ones.","Specifically, we introduce the Stable Neighbor Denoising (SND) approach, which effectively discovers highly correlated stable and unstable samples by nearest neighbor retrieval and guides the reliable optimization of unstable samples by bi-level learning.","Moreover, we compensate for the stable set by object-level object paste, which can further eliminate the bias caused by less learned classes.","Our SND enjoys two advantages.","First, SND does not require a specific segmentor structure, endowing its universality.","Second, SND simultaneously addresses the issues of class, domain, and confirmation biases during adaptation, ensuring its effectiveness.","Extensive experiments show that SND consistently outperforms state-of-the-art methods in various SFUDA semantic segmentation settings.","In addition, SND can be easily integrated with other approaches, obtaining further improvements."],"url":"http://arxiv.org/abs/2406.06813v1","category":"cs.CV"}
{"created":"2024-06-10 21:33:10","title":"Experimental benchmarking of quantum state overlap estimation strategies with photonic systems","abstract":"Accurately estimating the overlap between quantum states is a fundamental task in quantum information processing. While various strategies using distinct quantum measurements have been proposed for overlap estimation, the lack of experimental benchmarks on estimation precision limits strategy selection in different situations. Here we compare the performance of four practical strategies for overlap estimation, including tomography-tomography, tomography-projection, Schur collective measurement and optical swap test. With a photonic system, the overlap-dependent estimation precision for each strategy is quantified in terms of the average estimation variance over uniformly sampled states, which highlight the different performance of different strategies. We further propose an adaptive strategy with optimized precision in full-range overlap estimation. Our results shed new light on extracting the parameter of interest from quantum systems, prompting the design of efficient quantum protocols.","sentences":["Accurately estimating the overlap between quantum states is a fundamental task in quantum information processing.","While various strategies using distinct quantum measurements have been proposed for overlap estimation, the lack of experimental benchmarks on estimation precision limits strategy selection in different situations.","Here we compare the performance of four practical strategies for overlap estimation, including tomography-tomography, tomography-projection, Schur collective measurement and optical swap test.","With a photonic system, the overlap-dependent estimation precision for each strategy is quantified in terms of the average estimation variance over uniformly sampled states, which highlight the different performance of different strategies.","We further propose an adaptive strategy with optimized precision in full-range overlap estimation.","Our results shed new light on extracting the parameter of interest from quantum systems, prompting the design of efficient quantum protocols."],"url":"http://arxiv.org/abs/2406.06810v2","category":"quant-ph"}
{"created":"2024-06-10 21:10:25","title":"Overcoming Limitations in Artificial Intelligence-based Prostate Cancer Detection through Better Datasets and a Bayesian Approach to Aggregate Panel Predictions","abstract":"Despite considerable progress in developing artificial intelligence (AI) algorithms for prostate cancer detection from whole slide images, the clinical applicability of these models remains limited due to variability in pathological annotations and existing dataset limitations. This article proposes a novel approach to overcome these challenges by leveraging a Bayesian framework to seamlessly integrate new data, and present results as a panel of annotations. The framework is demonstrated by integrating a Bayesian prior with one trained AI model to generate a distribution of Gleason patterns for each pixel of an image. It is shown that using this distribution of Gleason patterns rather than a ground-truth label can improve model applicability, mitigate errors, and highlight areas of interest for pathologists. Additionally, we present a high-quality, hand-curated dataset of prostate histopathological images annotated at the gland level by trained pre-medical students and verified by an expert pathologist. We highlight the potential of this adaptive and uncertainty-aware framework for developing clinically deployable AI tools that can support pathologists in accurate prostate cancer grading, improve diagnostic accuracy, and create positive patient outcomes.","sentences":["Despite considerable progress in developing artificial intelligence (AI) algorithms for prostate cancer detection from whole slide images, the clinical applicability of these models remains limited due to variability in pathological annotations and existing dataset limitations.","This article proposes a novel approach to overcome these challenges by leveraging a Bayesian framework to seamlessly integrate new data, and present results as a panel of annotations.","The framework is demonstrated by integrating a Bayesian prior with one trained AI model to generate a distribution of Gleason patterns for each pixel of an image.","It is shown that using this distribution of Gleason patterns rather than a ground-truth label can improve model applicability, mitigate errors, and highlight areas of interest for pathologists.","Additionally, we present a high-quality, hand-curated dataset of prostate histopathological images annotated at the gland level by trained pre-medical students and verified by an expert pathologist.","We highlight the potential of this adaptive and uncertainty-aware framework for developing clinically deployable AI tools that can support pathologists in accurate prostate cancer grading, improve diagnostic accuracy, and create positive patient outcomes."],"url":"http://arxiv.org/abs/2406.06801v1","category":"q-bio.TO"}
{"created":"2024-06-10 21:02:53","title":"FlexLoc: Conditional Neural Networks for Zero-Shot Sensor Perspective Invariance in Object Localization with Distributed Multimodal Sensors","abstract":"Localization is a critical technology for various applications ranging from navigation and surveillance to assisted living. Localization systems typically fuse information from sensors viewing the scene from different perspectives to estimate the target location while also employing multiple modalities for enhanced robustness and accuracy. Recently, such systems have employed end-to-end deep neural models trained on large datasets due to their superior performance and ability to handle data from diverse sensor modalities. However, such neural models are often trained on data collected from a particular set of sensor poses (i.e., locations and orientations). During real-world deployments, slight deviations from these sensor poses can result in extreme inaccuracies. To address this challenge, we introduce FlexLoc, which employs conditional neural networks to inject node perspective information to adapt the localization pipeline. Specifically, a small subset of model weights are derived from node poses at run time, enabling accurate generalization to unseen perspectives with minimal additional overhead. Our evaluations on a multimodal, multiview indoor tracking dataset showcase that FlexLoc improves the localization accuracy by almost 50% in the zero-shot case (no calibration data available) compared to the baselines. The source code of FlexLoc is available at https://github.com/nesl/FlexLoc.","sentences":["Localization is a critical technology for various applications ranging from navigation and surveillance to assisted living.","Localization systems typically fuse information from sensors viewing the scene from different perspectives to estimate the target location while also employing multiple modalities for enhanced robustness and accuracy.","Recently, such systems have employed end-to-end deep neural models trained on large datasets due to their superior performance and ability to handle data from diverse sensor modalities.","However, such neural models are often trained on data collected from a particular set of sensor poses (i.e., locations and orientations).","During real-world deployments, slight deviations from these sensor poses can result in extreme inaccuracies.","To address this challenge, we introduce FlexLoc, which employs conditional neural networks to inject node perspective information to adapt the localization pipeline.","Specifically, a small subset of model weights are derived from node poses at run time, enabling accurate generalization to unseen perspectives with minimal additional overhead.","Our evaluations on a multimodal, multiview indoor tracking dataset showcase that FlexLoc improves the localization accuracy by almost 50% in the zero-shot case (no calibration data available) compared to the baselines.","The source code of FlexLoc is available at https://github.com/nesl/FlexLoc."],"url":"http://arxiv.org/abs/2406.06796v1","category":"cs.CV"}
{"created":"2024-06-10 20:59:52","title":"Reinforced Compressive Neural Architecture Search for Versatile Adversarial Robustness","abstract":"Prior neural architecture search (NAS) for adversarial robustness works have discovered that a lightweight and adversarially robust neural network architecture could exist in a non-robust large teacher network, generally disclosed by heuristic rules through statistical analysis and neural architecture search, generally disclosed by heuristic rules from neural architecture search. However, heuristic methods cannot uniformly handle different adversarial attacks and \"teacher\" network capacity. To solve this challenge, we propose a Reinforced Compressive Neural Architecture Search (RC-NAS) for Versatile Adversarial Robustness. Specifically, we define task settings that compose datasets, adversarial attacks, and teacher network information. Given diverse tasks, we conduct a novel dual-level training paradigm that consists of a meta-training and a fine-tuning phase to effectively expose the RL agent to diverse attack scenarios (in meta-training), and making it adapt quickly to locate a sub-network (in fine-tuning) for any previously unseen scenarios. Experiments show that our framework could achieve adaptive compression towards different initial teacher networks, datasets, and adversarial attacks, resulting in more lightweight and adversarially robust architectures.","sentences":["Prior neural architecture search (NAS) for adversarial robustness works have discovered that a lightweight and adversarially robust neural network architecture could exist in a non-robust large teacher network, generally disclosed by heuristic rules through statistical analysis and neural architecture search, generally disclosed by heuristic rules from neural architecture search.","However, heuristic methods cannot uniformly handle different adversarial attacks and \"teacher\" network capacity.","To solve this challenge, we propose a Reinforced Compressive Neural Architecture Search (RC-NAS) for Versatile Adversarial Robustness.","Specifically, we define task settings that compose datasets, adversarial attacks, and teacher network information.","Given diverse tasks, we conduct a novel dual-level training paradigm that consists of a meta-training and a fine-tuning phase to effectively expose the RL agent to diverse attack scenarios (in meta-training), and making it adapt quickly to locate a sub-network (in fine-tuning) for any previously unseen scenarios.","Experiments show that our framework could achieve adaptive compression towards different initial teacher networks, datasets, and adversarial attacks, resulting in more lightweight and adversarially robust architectures."],"url":"http://arxiv.org/abs/2406.06792v1","category":"cs.LG"}
{"created":"2024-06-10 19:25:19","title":"Federated Nonparametric Hypothesis Testing with Differential Privacy Constraints: Optimal Rates and Adaptive Tests","abstract":"Federated learning has attracted significant recent attention due to its applicability across a wide range of settings where data is collected and analyzed across disparate locations. In this paper, we study federated nonparametric goodness-of-fit testing in the white-noise-with-drift model under distributed differential privacy (DP) constraints.   We first establish matching lower and upper bounds, up to a logarithmic factor, on the minimax separation rate. This optimal rate serves as a benchmark for the difficulty of the testing problem, factoring in model characteristics such as the number of observations, noise level, and regularity of the signal class, along with the strictness of the $(\\epsilon,\\delta)$-DP requirement. The results demonstrate interesting and novel phase transition phenomena. Furthermore, the results reveal an interesting phenomenon that distributed one-shot protocols with access to shared randomness outperform those without access to shared randomness. We also construct a data-driven testing procedure that possesses the ability to adapt to an unknown regularity parameter over a large collection of function classes with minimal additional cost, all while maintaining adherence to the same set of DP constraints.","sentences":["Federated learning has attracted significant recent attention due to its applicability across a wide range of settings where data is collected and analyzed across disparate locations.","In this paper, we study federated nonparametric goodness-of-fit testing in the white-noise-with-drift model under distributed differential privacy (DP) constraints.   ","We first establish matching lower and upper bounds, up to a logarithmic factor, on the minimax separation rate.","This optimal rate serves as a benchmark for the difficulty of the testing problem, factoring in model characteristics such as the number of observations, noise level, and regularity of the signal class, along with the strictness of the $(\\epsilon,\\delta)$-DP requirement.","The results demonstrate interesting and novel phase transition phenomena.","Furthermore, the results reveal an interesting phenomenon that distributed one-shot protocols with access to shared randomness outperform those without access to shared randomness.","We also construct a data-driven testing procedure that possesses the ability to adapt to an unknown regularity parameter over a large collection of function classes with minimal additional cost, all while maintaining adherence to the same set of DP constraints."],"url":"http://arxiv.org/abs/2406.06749v1","category":"math.ST"}
{"created":"2024-06-10 19:17:09","title":"Multi-Objective Neural Architecture Search for In-Memory Computing","abstract":"In this work, we employ neural architecture search (NAS) to enhance the efficiency of deploying diverse machine learning (ML) tasks on in-memory computing (IMC) architectures. Initially, we design three fundamental components inspired by the convolutional layers found in VGG and ResNet models. Subsequently, we utilize Bayesian optimization to construct a convolutional neural network (CNN) model with adaptable depths, employing these components. Through the Bayesian search algorithm, we explore a vast search space comprising over 640 million network configurations to identify the optimal solution, considering various multi-objective cost functions like accuracy/latency and accuracy/energy. Our evaluation of this NAS approach for IMC architecture deployment spans three distinct image classification datasets, demonstrating the effectiveness of our method in achieving a balanced solution characterized by high accuracy and reduced latency and energy consumption.","sentences":["In this work, we employ neural architecture search (NAS) to enhance the efficiency of deploying diverse machine learning (ML) tasks on in-memory computing (IMC) architectures.","Initially, we design three fundamental components inspired by the convolutional layers found in VGG and ResNet models.","Subsequently, we utilize Bayesian optimization to construct a convolutional neural network (CNN) model with adaptable depths, employing these components.","Through the Bayesian search algorithm, we explore a vast search space comprising over 640 million network configurations to identify the optimal solution, considering various multi-objective cost functions like accuracy/latency and accuracy/energy.","Our evaluation of this NAS approach for IMC architecture deployment spans three distinct image classification datasets, demonstrating the effectiveness of our method in achieving a balanced solution characterized by high accuracy and reduced latency and energy consumption."],"url":"http://arxiv.org/abs/2406.06746v1","category":"cs.LG"}
{"created":"2024-06-10 18:23:03","title":"Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach For Adaptive Brain Stimulation","abstract":"Adaptive brain stimulation can treat neurological conditions such as Parkinson's disease and post-stroke motor deficits by influencing abnormal neural activity. Because of patient heterogeneity, each patient requires a unique stimulation policy to achieve optimal neural responses. Model-free reinforcement learning (MFRL) holds promise in learning effective policies for a variety of similar control tasks, but is limited in domains like brain stimulation by a need for numerous costly environment interactions. In this work we introduce Coprocessor Actor Critic, a novel, model-based reinforcement learning (MBRL) approach for learning neural coprocessor policies for brain stimulation. Our key insight is that coprocessor policy learning is a combination of learning how to act optimally in the world and learning how to induce optimal actions in the world through stimulation of an injured brain. We show that our approach overcomes the limitations of traditional MFRL methods in terms of sample efficiency and task success and outperforms baseline MBRL approaches in a neurologically realistic model of an injured brain.","sentences":["Adaptive brain stimulation can treat neurological conditions such as Parkinson's disease and post-stroke motor deficits by influencing abnormal neural activity.","Because of patient heterogeneity, each patient requires a unique stimulation policy to achieve optimal neural responses.","Model-free reinforcement learning (MFRL) holds promise in learning effective policies for a variety of similar control tasks, but is limited in domains like brain stimulation by a need for numerous costly environment interactions.","In this work we introduce Coprocessor Actor Critic, a novel, model-based reinforcement learning (MBRL) approach for learning neural coprocessor policies for brain stimulation.","Our key insight is that coprocessor policy learning is a combination of learning how to act optimally in the world and learning how to induce optimal actions in the world through stimulation of an injured brain.","We show that our approach overcomes the limitations of traditional MFRL methods in terms of sample efficiency and task success and outperforms baseline MBRL approaches in a neurologically realistic model of an injured brain."],"url":"http://arxiv.org/abs/2406.06714v1","category":"cs.LG"}
{"created":"2024-06-10 18:00:01","title":"Systematic Collapse of the Accretion Disc Across the Supermassive Black Hole Population","abstract":"The structure of the accretion flow onto supermassive black holes (SMBH) is not well understood. Standard disc models match to zeroth order in predicting substantial energy dissipation within optically-thick material producing a characteristic strong blue/UV continuum. However they fail at reproducing more detailed comparisons to the observed spectral shapes along with their observed variability. Based on stellar mass black holes within our galaxy, accretion discs should undergo a transition into an X-ray hot, radiatively inefficient flow, below a (mass scaled) luminosity of $\\sim 0.02\\,L_{\\rm{Edd}}$. While this has been seen in limited samples of nearby low-luminosity active galactic nuclei (AGN) and a few rare changing-look AGN, it is not at all clear whether this transition is present in the wider AGN population across cosmic time. A key issue is the difficulty in disentangling a change in spectral state from increased dust obscuration and/or host galaxy contamination, effectively drowning out the AGN emission. Here we use the new eROSITA eFEDS Survey to identify unobscured AGN from their X-ray emission, matched to excellent optical imaging from Subaru's Hyper Suprime-Cam; allowing the subtraction of the host galaxy contamination. The resulting, uncontaminated, AGN spectra reveal a smooth transition from a strongly disc dominated state in bright AGN, to the collapse of the disc into an inefficient X-ray plasma in the low luminosity AGN, with the transition occurring at $\\sim 0.02\\,L_{\\rm{Edd}}$; revealing fundamental aspects of accretion physics in AGN.","sentences":["The structure of the accretion flow onto supermassive black holes (SMBH) is not well understood.","Standard disc models match to zeroth order in predicting substantial energy dissipation within optically-thick material producing a characteristic strong blue/UV continuum.","However they fail at reproducing more detailed comparisons to the observed spectral shapes along with their observed variability.","Based on stellar mass black holes within our galaxy, accretion discs should undergo a transition into an X-ray hot, radiatively inefficient flow, below a (mass scaled) luminosity of $\\sim 0.02\\,L_{\\rm{Edd}}$.","While this has been seen in limited samples of nearby low-luminosity active galactic nuclei (AGN) and a few rare changing-look AGN, it is not at all clear whether this transition is present in the wider AGN population across cosmic time.","A key issue is the difficulty in disentangling a change in spectral state from increased dust obscuration and/or host galaxy contamination, effectively drowning out the AGN emission.","Here we use the new eROSITA eFEDS","Survey to identify unobscured AGN from their X-ray emission, matched to excellent optical imaging from Subaru's Hyper Suprime-Cam; allowing the subtraction of the host galaxy contamination.","The resulting, uncontaminated, AGN spectra reveal a smooth transition from a strongly disc dominated state in bright AGN, to the collapse of the disc into an inefficient X-ray plasma in the low luminosity AGN, with the transition occurring at $\\sim 0.02\\,L_{\\rm{Edd}}$; revealing fundamental aspects of accretion physics in AGN."],"url":"http://arxiv.org/abs/2406.06674v1","category":"astro-ph.HE"}
{"created":"2024-06-11 17:57:32","title":"Dynamics of the non-radial energy-critical inhomogeneous NLS","abstract":"We consider the focusing inhomogeneous nonlinear Schr\\\"odinger equation \\[ i\\partial_t u + \\Delta u + |x|^{-b}|u|^\\alpha u = 0\\qtq{on}\\R\\times\\R^N, \\] with $\\alpha=\\tfrac{4-2b}{N-2}$, $N=\\{3,4,5\\}$ and $0<b\\leq \\min\\Big\\{\\tfrac{6-N}{2},\\tfrac{4}{N}$\\Big\\}. This paper establishes global well-posedness and scattering for the non-radial energy-critical case in $\\dot{H}^1(\\R^N)$. It extends the previous research by Murphy and the first author \\cite{GM}, which focused on the case $(N,\\alpha,b)=(3,2,1)$. The novelty here, beyond considering higher dimensions, lies in our assumption of the condition $\\sup_{t\\in I}\\|\\nabla u(t)\\|_{L^2}<\\|\\nabla Q\\|_{L^2}$, which is weaker than the condition stated in \\cite{Guzman}. Consequently, if a solution has energy and kinetic energy less than the ground state $Q$ at some point, then the solution is global and scatters. Moreover, we show scattering for the defocusing case. On the other hand, in this work, we also investigate the blow-up issue with nonradial data for $N\\geq 3$ in $H^1(\\mathbb{R}^N)$. This implies that our result holds without classical assumptions such as spherically symmetric data or $|x|u_0 \\in L^2(\\mathbb{R}^N)$.   \\   \\noindent Mathematics Subject Classification. 35A01, 35QA55, 35P25.","sentences":["We consider the focusing inhomogeneous nonlinear Schr\\\"odinger equation \\","[ i\\partial_t u + \\Delta u + |x|^{-b}|u|^\\alpha u = 0\\qtq{on}\\R\\times\\R^N, \\] with $\\alpha=\\tfrac{4-2b}{N-2}$, $N=\\{3,4,5\\}$ and $0<b\\leq \\min\\Big\\{\\tfrac{6-N}{2},\\tfrac{4}{N}$\\Big\\}.","This paper establishes global well-posedness and scattering for the non-radial energy-critical case in $\\dot{H}^1(\\R^N)$. It extends the previous research by Murphy and the first author \\cite{GM}, which focused on the case $(N,\\alpha,b)=(3,2,1)$. The novelty here, beyond considering higher dimensions, lies in our assumption of the condition $\\sup_{t\\in I}\\|\\nabla u(t)\\|_{L^2}<\\|\\nabla Q\\|_{L^2}$, which is weaker than the condition stated in \\cite{Guzman}.","Consequently, if a solution has energy and kinetic energy less than the ground state $Q$ at some point, then the solution is global and scatters.","Moreover, we show scattering for the defocusing case.","On the other hand, in this work, we also investigate the blow-up issue with nonradial data for $N\\geq 3$ in $H^1(\\mathbb{R}^N)$. This implies that our result holds without classical assumptions such as spherically symmetric data or $|x|u_0 \\in L^2(\\mathbb{R}^N)$.   \\   \\noindent Mathematics Subject Classification.","35A01, 35QA55, 35P25."],"url":"http://arxiv.org/abs/2406.07535v1","category":"math.AP"}
{"created":"2024-06-11 17:56:31","title":"Interpreting DESI 2024 BAO: late-time dynamical dark energy or a local effect?","abstract":"We perform fits to DESI, CMB and supernova data to understand the physical origin of the DESI hint for dynamical dark energy. We find that the linear parametrization of the equation of state $w$ may guide to misleading interpretations, such as the hint for a phantom Universe, which are not preferred by the data. Instead, physical quintessence models fit the data well. Model-independently, present observations prefer deviations from the constant dark energy, $w=-1$, only at very low redshifts, $z < \\mathcal{O}(0.1)$. We find that this result is driven by low-$z$ supernova data. Therefore, either the fundamental properties of our Universe, characterised by the equation of state $w$ and the Hubble parameter $H$, underwent dramatic changes very recently or, alternatively, we do not fully understand the systematics of our local Universe in a radius of about $300\\,h^{-1}\\rm Mpc$.","sentences":["We perform fits to DESI, CMB and supernova data to understand the physical origin of the DESI hint for dynamical dark energy.","We find that the linear parametrization of the equation of state $w$ may guide to misleading interpretations, such as the hint for a phantom Universe, which are not preferred by the data.","Instead, physical quintessence models fit the data well.","Model-independently, present observations prefer deviations from the constant dark energy, $w=-1$, only at very low redshifts, $z <","\\mathcal{O}(0.1)$. We find that this result is driven by low-$z$ supernova data.","Therefore, either the fundamental properties of our Universe, characterised by the equation of state $w$ and the Hubble parameter $H$, underwent dramatic changes very recently or, alternatively, we do not fully understand the systematics of our local Universe in a radius of about $300\\,h^{-1}\\rm Mpc$."],"url":"http://arxiv.org/abs/2406.07533v1","category":"astro-ph.CO"}
{"created":"2024-06-11 17:49:35","title":"On constant mean curvature 1-immersions of surfaces into hyperbolic 3-manifolds","abstract":"Motivated by the work of Bryant on constant mean curvature (CMC) $1$-immersions of surfaces into the hyperbolic space H^3 and after the results of Tarantello (2023), we pursue a possible parametrization for the moduli space of (CMC) 1-immersions of a surface S (closed, orientable and of genus >1) into hyperbolic 3-manifolds. Those immersions enter as \"critical\" object in our analysis. In fact, they can be attained only as limits of the (CMC) c-immersions (as c tends to 1), obtained in Huang-Lucia-Tarantello (2022), for |c|<1. However, such passage to the limit can be prevented by possible blow-up phenomena, so that the pullback metrics of the (CMC) c-immersions may yield (at the limit) to a singular metric with conical singularities at finitely many points (the blow-up points). In case of genus g=2, blow up can occur only at a single point, and in Tarantello (2023) it was shown how it could be prevented and the passage to the limit ensured in terms of the Kodaira map. In this note we sharpen this result and for genus g=2, we obtaina condition (we believe sharp) which involves only the Kodaira map on the six Weierstrass points. In addition we tackle the case of higher genus, where multiple blow-up points occur. In this case, we need to identify a suitable replacement of the Kodaira map, now defined on the space of non-zero effective divisors. More importantly, we need to improve in a substantial way the asymptotic analysis of Tarantello (2023) limited to the case of \"blow-up\" with minimal mass. In this direction we give a contribution which best applies to the case of genus g=3, but also provides a relevant step and a convincing indication on what should happen in the general case.","sentences":["Motivated by the work of Bryant on constant mean curvature (CMC) $1$-immersions of surfaces into the hyperbolic space H^3 and after the results of Tarantello (2023), we pursue a possible parametrization for the moduli space of (CMC) 1-immersions of a surface S (closed, orientable and of genus >1) into hyperbolic 3-manifolds.","Those immersions enter as \"critical\" object in our analysis.","In fact, they can be attained only as limits of the (CMC) c-immersions (as c tends to 1), obtained in Huang-Lucia-Tarantello (2022), for |c|<1.","However, such passage to the limit can be prevented by possible blow-up phenomena, so that the pullback metrics of the (CMC) c-immersions may yield (at the limit) to a singular metric with conical singularities at finitely many points (the blow-up points).","In case of genus g=2, blow up can occur only at a single point, and in Tarantello (2023) it was shown how it could be prevented and the passage to the limit ensured in terms of the Kodaira map.","In this note we sharpen this result and for genus g=2, we obtaina condition (we believe sharp) which involves only the Kodaira map on the six Weierstrass points.","In addition we tackle the case of higher genus, where multiple blow-up points occur.","In this case, we need to identify a suitable replacement of the Kodaira map, now defined on the space of non-zero effective divisors.","More importantly, we need to improve in a substantial way the asymptotic analysis of Tarantello (2023) limited to the case of \"blow-up\" with minimal mass.","In this direction we give a contribution which best applies to the case of genus g=3, but also provides a relevant step and a convincing indication on what should happen in the general case."],"url":"http://arxiv.org/abs/2406.07518v1","category":"math.DG"}
{"created":"2024-06-11 17:43:18","title":"Uniqueness on average of large isoperimetric sets in noncompact manifolds with nonnegative Ricci curvature","abstract":"Let $(M^n,g)$ be a complete Riemannian manifold which is not isometric to $\\mathbb{R}^n$, has nonnegative Ricci curvature, Euclidean volume growth, and quadratic Riemann curvature decay. We prove that there is a set $\\mathcal{G}\\subset (0,\\infty)$ with density $1$ at infinity such that for every $V\\in \\mathcal{G}$ there is a unique isoperimetric set of volume $V$ in $M$, and its boundary is strictly volume preserving stable.   The latter result cannot be improved to uniqueness or strict stability for every large volume. Indeed, we construct a complete Riemannian surface that satisfies the previous assumptions, and with the following property: there are arbitrarily large and diverging intervals $I_n\\subset (0,\\infty)$ such that isoperimetric sets with volumes $V\\in I_n$ exist, but they are neither unique nor they have strictly volume preserving stable boundaries.   The proof relies on a set of new ideas, as the present setting goes beyond the range of applicability of the methods based on the implicit function theorem, and no symmetry is assumed.","sentences":["Let $(M^n,g)$ be a complete Riemannian manifold which is not isometric to $\\mathbb{R}^n$, has nonnegative Ricci curvature, Euclidean volume growth, and quadratic Riemann curvature decay.","We prove that there is a set $\\mathcal{G}\\subset (0,\\infty)$ with density $1$ at infinity such that for every $V\\in \\mathcal{G}$ there is a unique isoperimetric set of volume $V$ in $M$, and its boundary is strictly volume preserving stable.   ","The latter result cannot be improved to uniqueness or strict stability for every large volume.","Indeed, we construct a complete Riemannian surface that satisfies the previous assumptions, and with the following property: there are arbitrarily large and diverging intervals $I_n\\subset (0,\\infty)$ such that isoperimetric sets with volumes $V\\in I_n$ exist, but they are neither unique nor they have strictly volume preserving stable boundaries.   ","The proof relies on a set of new ideas, as the present setting goes beyond the range of applicability of the methods based on the implicit function theorem, and no symmetry is assumed."],"url":"http://arxiv.org/abs/2406.07509v1","category":"math.DG"}
{"created":"2024-06-11 17:41:31","title":"$(J/\u03c8, J/\u03c8)$, and $(\u03b7_c, \u03b7_c)$ production through two intermediate photons in electron-positron annihilation at B-factories","abstract":"We study the processes, $e^- e^+ \\rightarrow \\gamma^* \\gamma^* \\rightarrow J/\\psi +J/\\psi$, and $e^- e^+ \\rightarrow \\gamma^* \\gamma^* \\rightarrow \\eta_c+ \\eta_c$ at $\\sqrt{s}=10.6$ GeV in the framework of $4\\times 4$ Bethe-Salpeter equation. For $J/\\psi+J/\\psi$ production, the dominant contribution is through fragmentation process, while for $\\eta_c+\\eta_c$ production, the quark rearrangement diagrams contribute. Our results of cross section for $J/\\psi+J/\\psi$ and $\\psi(2S)+\\psi(2S)$ are compatible with the experimental upper limits set by Belle Collaboration, while in the absence of experimental data for $\\eta_c(1S)+\\eta_c(1S)$, and $\\eta_c(2S)+\\eta_c(2S)$ production, we have given theoretical prediction of their cross sections, and compared with NRQCD prediction.","sentences":["We study the processes, $e^- e^+ \\rightarrow \\gamma^* \\gamma^*","\\rightarrow J/\\psi +J/\\psi$, and $e^- e^+ \\rightarrow \\gamma^*","\\gamma^*","\\rightarrow \\eta_c+ \\eta_c$ at $\\sqrt{s}=10.6$ GeV in the framework of $4\\times 4$ Bethe-Salpeter equation.","For $J/\\psi+J/\\psi$ production, the dominant contribution is through fragmentation process, while for $\\eta_c+\\eta_c$ production, the quark rearrangement diagrams contribute.","Our results of cross section for $J/\\psi+J/\\psi$ and $\\psi(2S)+\\psi(2S)$ are compatible with the experimental upper limits set by Belle Collaboration, while in the absence of experimental data for $\\eta_c(1S)+\\eta_c(1S)$, and $\\eta_c(2S)+\\eta_c(2S)$ production, we have given theoretical prediction of their cross sections, and compared with NRQCD prediction."],"url":"http://arxiv.org/abs/2406.07508v1","category":"hep-ph"}
{"created":"2024-06-11 17:41:26","title":"Flow Map Matching","abstract":"Generative models based on dynamical transport of measure, such as diffusion models, flow matching models, and stochastic interpolants, learn an ordinary or stochastic differential equation whose trajectories push initial conditions from a known base distribution onto the target. While training is cheap, samples are generated via simulation, which is more expensive than one-step models like GANs. To close this gap, we introduce flow map matching -- an algorithm that learns the two-time flow map of an underlying ordinary differential equation. The approach leads to an efficient few-step generative model whose step count can be chosen a-posteriori to smoothly trade off accuracy for computational expense. Leveraging the stochastic interpolant framework, we introduce losses for both direct training of flow maps and distillation from pre-trained (or otherwise known) velocity fields. Theoretically, we show that our approach unifies many existing few-step generative models, including consistency models, consistency trajectory models, progressive distillation, and neural operator approaches, which can be obtained as particular cases of our formalism. With experiments on CIFAR-10 and ImageNet 32x32, we show that flow map matching leads to high-quality samples with significantly reduced sampling cost compared to diffusion or stochastic interpolant methods.","sentences":["Generative models based on dynamical transport of measure, such as diffusion models, flow matching models, and stochastic interpolants, learn an ordinary or stochastic differential equation whose trajectories push initial conditions from a known base distribution onto the target.","While training is cheap, samples are generated via simulation, which is more expensive than one-step models like GANs.","To close this gap, we introduce flow map matching -- an algorithm that learns the two-time flow map of an underlying ordinary differential equation.","The approach leads to an efficient few-step generative model whose step count can be chosen a-posteriori to smoothly trade off accuracy for computational expense.","Leveraging the stochastic interpolant framework, we introduce losses for both direct training of flow maps and distillation from pre-trained (or otherwise known) velocity fields.","Theoretically, we show that our approach unifies many existing few-step generative models, including consistency models, consistency trajectory models, progressive distillation, and neural operator approaches, which can be obtained as particular cases of our formalism.","With experiments on CIFAR-10 and ImageNet 32x32, we show that flow map matching leads to high-quality samples with significantly reduced sampling cost compared to diffusion or stochastic interpolant methods."],"url":"http://arxiv.org/abs/2406.07507v1","category":"cs.LG"}
{"created":"2024-06-11 17:24:02","title":"Image Neural Field Diffusion Models","abstract":"Diffusion models have shown an impressive ability to model complex data distributions, with several key advantages over GANs, such as stable training, better coverage of the training distribution's modes, and the ability to solve inverse problems without extra training. However, most diffusion models learn the distribution of fixed-resolution images. We propose to learn the distribution of continuous images by training diffusion models on image neural fields, which can be rendered at any resolution, and show its advantages over fixed-resolution models. To achieve this, a key challenge is to obtain a latent space that represents photorealistic image neural fields. We propose a simple and effective method, inspired by several recent techniques but with key changes to make the image neural fields photorealistic. Our method can be used to convert existing latent diffusion autoencoders into image neural field autoencoders. We show that image neural field diffusion models can be trained using mixed-resolution image datasets, outperform fixed-resolution diffusion models followed by super-resolution models, and can solve inverse problems with conditions applied at different scales efficiently.","sentences":["Diffusion models have shown an impressive ability to model complex data distributions, with several key advantages over GANs, such as stable training, better coverage of the training distribution's modes, and the ability to solve inverse problems without extra training.","However, most diffusion models learn the distribution of fixed-resolution images.","We propose to learn the distribution of continuous images by training diffusion models on image neural fields, which can be rendered at any resolution, and show its advantages over fixed-resolution models.","To achieve this, a key challenge is to obtain a latent space that represents photorealistic image neural fields.","We propose a simple and effective method, inspired by several recent techniques but with key changes to make the image neural fields photorealistic.","Our method can be used to convert existing latent diffusion autoencoders into image neural field autoencoders.","We show that image neural field diffusion models can be trained using mixed-resolution image datasets, outperform fixed-resolution diffusion models followed by super-resolution models, and can solve inverse problems with conditions applied at different scales efficiently."],"url":"http://arxiv.org/abs/2406.07480v1","category":"cs.CV"}
{"created":"2024-06-11 17:21:15","title":"Partially Observed Trajectory Inference using Optimal Transport and a Dynamics Prior","abstract":"Trajectory inference seeks to recover the temporal dynamics of a population from snapshots of its (uncoupled) temporal marginals, i.e. where observed particles are not tracked over time. Lavenant et al. arXiv:2102.09204 addressed this challenging problem under a stochastic differential equation (SDE) model with a gradient-driven drift in the observed space, introducing a minimum entropy estimator relative to the Wiener measure. Chizat et al. arXiv:2205.07146 then provided a practical grid-free mean-field Langevin (MFL) algorithm using Schr\\\"odinger bridges. Motivated by the overwhelming success of observable state space models in the traditional paired trajectory inference problem (e.g. target tracking), we extend the above framework to a class of latent SDEs in the form of observable state space models. In this setting, we use partial observations to infer trajectories in the latent space under a specified dynamics model (e.g. the constant velocity/acceleration models from target tracking). We introduce PO-MFL to solve this latent trajectory inference problem and provide theoretical guarantees by extending the results of arXiv:2102.09204 to the partially observed setting. We leverage the MFL framework of arXiv:2205.07146, yielding an algorithm based on entropic OT between dynamics-adjusted adjacent time marginals. Experiments validate the robustness of our method and the exponential convergence of the MFL dynamics, and demonstrate significant outperformance over the latent-free method of arXiv:2205.07146 in key scenarios.","sentences":["Trajectory inference seeks to recover the temporal dynamics of a population from snapshots of its (uncoupled) temporal marginals, i.e. where observed particles are not tracked over time.","Lavenant et al. arXiv:2102.09204 addressed this challenging problem under a stochastic differential equation (SDE) model with a gradient-driven drift in the observed space, introducing a minimum entropy estimator relative to the Wiener measure.","Chizat et al.","arXiv:2205.07146 then provided a practical grid-free mean-field Langevin (MFL) algorithm using Schr\\\"odinger bridges.","Motivated by the overwhelming success of observable state space models in the traditional paired trajectory inference problem (e.g. target tracking), we extend the above framework to a class of latent SDEs in the form of observable state space models.","In this setting, we use partial observations to infer trajectories in the latent space under a specified dynamics model (e.g. the constant velocity/acceleration models from target tracking).","We introduce PO-MFL to solve this latent trajectory inference problem and provide theoretical guarantees by extending the results of arXiv:2102.09204 to the partially observed setting.","We leverage the MFL framework of arXiv:2205.07146, yielding an algorithm based on entropic OT between dynamics-adjusted adjacent time marginals.","Experiments validate the robustness of our method and the exponential convergence of the MFL dynamics, and demonstrate significant outperformance over the latent-free method of arXiv:2205.07146 in key scenarios."],"url":"http://arxiv.org/abs/2406.07475v1","category":"cs.LG"}
{"created":"2024-06-11 17:16:58","title":"Exploring non-radial oscillation modes in dark matter admixed neutron stars","abstract":"Because of their extreme densities and consequently, gravitational potential, compact objects such as neutron stars can prove to be excellent captors of dark matter particles. Considering purely gravitational interactions between dark and hadronic matter, we construct dark matter admixed stars composed of two-fluid matter subject to current astrophysical constraints of maximum mass and tidal deformability. We choose a wide range of parameters to construct the dark matter equation of state, and the DDME2 parameterization for the hadronic equation of state. We then examine the effect of dark matter on the stellar structure, tidal deformability and non-radial modes considering the relativistic Cowling approximation. We find the effect on $p$-modes is substantial, with frequencies decreasing up to the typical $f-$mode frequency range for most stars with a dark matter halo. The effects on the $f-$mode frequency are less extreme. Finally, we find the most probable and $1\\sigma$ values of the dark matter parameters used in this study.","sentences":["Because of their extreme densities and consequently, gravitational potential, compact objects such as neutron stars can prove to be excellent captors of dark matter particles.","Considering purely gravitational interactions between dark and hadronic matter, we construct dark matter admixed stars composed of two-fluid matter subject to current astrophysical constraints of maximum mass and tidal deformability.","We choose a wide range of parameters to construct the dark matter equation of state, and the DDME2 parameterization for the hadronic equation of state.","We then examine the effect of dark matter on the stellar structure, tidal deformability and non-radial modes considering the relativistic Cowling approximation.","We find the effect on $p$-modes is substantial, with frequencies decreasing up to the typical $f-$mode frequency range for most stars with a dark matter halo.","The effects on the $f-$mode frequency are less extreme.","Finally, we find the most probable and $1\\sigma$ values of the dark matter parameters used in this study."],"url":"http://arxiv.org/abs/2406.07470v1","category":"astro-ph.HE"}
{"created":"2024-06-11 17:13:18","title":"On functions of low differential uniformity in characteristic 2: A close look (I)","abstract":"We introduce a new concept, the APN-defect, which can be thought of as measuring the distance of a given function $G:\\mathbb{F}_{2^n} \\rightarrow \\mathbb{F}_{2^n}$ to the set of almost perfect nonlinear (APN) functions. This concept is motivated by the detailed analysis of the differential behaviour of non-APN functions (of low differential uniformity) $G$ using the so-called difference squares. We describe the relations between the APN-defect and other recent concepts of similar nature. Upper and lower bounds for the values of APN-defect for several classes of functions of interest, including Dembowski-Ostrom polynomials are given. Its exact values in some cases are also calculated. The difference square corresponding to a modification of the inverse function is determined, its APN-defect depending on $n$ is evaluated and the implications are discussed.   In the forthcoming second part of this work we further examine modifications of the inverse function. We also study modifications of classes of functions of low uniformity over infinitely many extensions of $\\mathbb{F}_{2^n}$. We present quantitative results on their differential behaviour, especially in connection with their APN-defects.","sentences":["We introduce a new concept, the APN-defect, which can be thought of as measuring the distance of a given function $G:\\mathbb{F}_{2^n} \\rightarrow \\mathbb{F}_{2^n}$ to the set of almost perfect nonlinear (APN) functions.","This concept is motivated by the detailed analysis of the differential behaviour of non-APN functions (of low differential uniformity) $G$ using the so-called difference squares.","We describe the relations between the APN-defect and other recent concepts of similar nature.","Upper and lower bounds for the values of APN-defect for several classes of functions of interest, including Dembowski-Ostrom polynomials are given.","Its exact values in some cases are also calculated.","The difference square corresponding to a modification of the inverse function is determined, its APN-defect depending on $n$ is evaluated and the implications are discussed.   ","In the forthcoming second part of this work we further examine modifications of the inverse function.","We also study modifications of classes of functions of low uniformity over infinitely many extensions of $\\mathbb{F}_{2^n}$. We present quantitative results on their differential behaviour, especially in connection with their APN-defects."],"url":"http://arxiv.org/abs/2406.07468v1","category":"cs.IT"}
{"created":"2024-06-11 17:11:08","title":"Microbiomes Through The Looking Glass","abstract":"Bacterial communities are pivotal to maintaining ecological function and preserving the rich tapestry of biological diversity. The rapid development of environmental sequencing technologies, such as metagenomics, has revolutionized our capacity to probe such diversity. However, despite these advances, a theoretical understanding connecting empirical data with ecosystem modelling, in particular in the framework of disordered systems akin to spin glasses, is still in its infancy. Here, we present a comprehensive framework using theories of disordered systems to decode microbiome data, which offers insight into the ecological forces that shape macroecological states. By employing the quenched disordered generalized Lotka-Volterra model, we analyze species abundance data in healthy and diseased human gut microbiomes. Results reveal the emergence of two distinct patterns of species-interaction networks, elucidating the pathways through which dysbiosis may drive microbiome instability. Interaction patterns thus provide a window into the systemic shifts accompanying the transition from health to disease, offering a new perspective on the dynamics of the microbial community. Our findings suggest the potential of disordered systems theory to characterize microbiomes by capturing the essence of ecological interactions and their consequences on stability and functioning, leveraging our understanding of the linkages of dysbiosis and microbial dynamics.","sentences":["Bacterial communities are pivotal to maintaining ecological function and preserving the rich tapestry of biological diversity.","The rapid development of environmental sequencing technologies, such as metagenomics, has revolutionized our capacity to probe such diversity.","However, despite these advances, a theoretical understanding connecting empirical data with ecosystem modelling, in particular in the framework of disordered systems akin to spin glasses, is still in its infancy.","Here, we present a comprehensive framework using theories of disordered systems to decode microbiome data, which offers insight into the ecological forces that shape macroecological states.","By employing the quenched disordered generalized Lotka-Volterra model, we analyze species abundance data in healthy and diseased human gut microbiomes.","Results reveal the emergence of two distinct patterns of species-interaction networks, elucidating the pathways through which dysbiosis may drive microbiome instability.","Interaction patterns thus provide a window into the systemic shifts accompanying the transition from health to disease, offering a new perspective on the dynamics of the microbial community.","Our findings suggest the potential of disordered systems theory to characterize microbiomes by capturing the essence of ecological interactions and their consequences on stability and functioning, leveraging our understanding of the linkages of dysbiosis and microbial dynamics."],"url":"http://arxiv.org/abs/2406.07465v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-11 17:05:44","title":"Existence and asymptotic autonomous robustness of random attractors for three-dimensional stochastic globally modified Navier-Stokes equations on unbounded domains","abstract":"In this article, we discuss the existence and asymptotically autonomous robustness (AAR) (almost surely) of random attractors for 3D stochastic globally modified Navier-Stokes equations (SGMNSE) on Poincar\\'e domains (which may be bounded or unbounded). Our aim is to investigate the existence and AAR of random attractors for 3D SGMNSE when the time-dependent forcing converges to a time-independent function under the perturbation of linear multiplicative noise as well as additive noise. The main approach is to provide a way to justify that, on some uniformly tempered universe, the usual pullback asymptotic compactness of the solution operators is uniform across an infinite time-interval $(-\\infty,\\tau]$. The backward uniform ``tail-smallness'' and ``flattening-property'' of the solutions over $(-\\infty,\\tau]$ have been demonstrated to achieve this goal. To the best of our knowledge, this is the first attempt to establish the existence as well as AAR of random attractors for 3D SGMNSE on unbounded domains.","sentences":["In this article, we discuss the existence and asymptotically autonomous robustness (AAR) (almost surely) of random attractors for 3D stochastic globally modified Navier-Stokes equations (SGMNSE) on Poincar\\'e domains (which may be bounded or unbounded).","Our aim is to investigate the existence and AAR of random attractors for 3D SGMNSE when the time-dependent forcing converges to a time-independent function under the perturbation of linear multiplicative noise as well as additive noise.","The main approach is to provide a way to justify that, on some uniformly tempered universe, the usual pullback asymptotic compactness of the solution operators is uniform across an infinite time-interval $(-\\infty,\\tau]$. The backward uniform ``tail-smallness'' and ``flattening-property'' of the solutions over $(-\\infty,\\tau]$ have been demonstrated to achieve this goal.","To the best of our knowledge, this is the first attempt to establish the existence as well as AAR of random attractors for 3D SGMNSE on unbounded domains."],"url":"http://arxiv.org/abs/2406.07460v1","category":"math.PR"}
{"created":"2024-06-11 17:02:37","title":"Resummation of Multi-Stress Tensors in Higher Dimensions","abstract":"In the context of holographic conformal field theories (CFTs), a system of linear partial differential equations was recently proposed to be the higher-dimensional analogs of the null-state equations in $d=2$ CFTs at large central charge. Solving these equations in a near-lightcone expansion yields solutions that match the minimal-twist multi-stress tensor contributions to a heavy-light four-point correlator (or a thermal two-point correlator) computed using holography, the conformal bootstrap, and other methods. This note explores the exact solutions to these equations. We begin by observing that, in an expansion in terms of the ratio between the heavy operator's dimension and the central charge, the $d=2$ correlator involving the level-two degenerate scalars at each order can be represented as a Bessel function; the resummation yields the Virasoro vacuum block. We next observe a relation between the $d=2$ correlator and the $d=4$ near-lightcone correlator involving light scalars with the same conformal dimension. The resummed $d=4$ correlator takes a simple form in the complex frequency domain. Unlike the Virasoro vacuum block, the resummation in $d=4$ leads to essential singularities. Similar expressions are also obtained when the light scalar's dimension takes other finite values. These CFT results correspond to a holographic computation with a spherical black hole. In addition, using the differential equations, we demonstrate that the correlators can be reconstructed via certain modes. In $d=2$, these modes are related to the Virasoro algebra.","sentences":["In the context of holographic conformal field theories (CFTs), a system of linear partial differential equations was recently proposed to be the higher-dimensional analogs of the null-state equations in $d=2$ CFTs at large central charge.","Solving these equations in a near-lightcone expansion yields solutions that match the minimal-twist multi-stress tensor contributions to a heavy-light four-point correlator (or a thermal two-point correlator) computed using holography, the conformal bootstrap, and other methods.","This note explores the exact solutions to these equations.","We begin by observing that, in an expansion in terms of the ratio between the heavy operator's dimension and the central charge, the $d=2$ correlator involving the level-two degenerate scalars at each order can be represented as a Bessel function; the resummation yields the Virasoro vacuum block.","We next observe a relation between the $d=2$ correlator and the $d=4$ near-lightcone correlator involving light scalars with the same conformal dimension.","The resummed $d=4$ correlator takes a simple form in the complex frequency domain.","Unlike the Virasoro vacuum block, the resummation in $d=4$ leads to essential singularities.","Similar expressions are also obtained when the light scalar's dimension takes other finite values.","These CFT results correspond to a holographic computation with a spherical black hole.","In addition, using the differential equations, we demonstrate that the correlators can be reconstructed via certain modes.","In $d=2$, these modes are related to the Virasoro algebra."],"url":"http://arxiv.org/abs/2406.07458v1","category":"hep-th"}
{"created":"2024-06-11 16:58:00","title":"HTVM: Efficient Neural Network Deployment On Heterogeneous TinyML Platforms","abstract":"Optimal deployment of deep neural networks (DNNs) on state-of-the-art Systems-on-Chips (SoCs) is crucial for tiny machine learning (TinyML) at the edge. The complexity of these SoCs makes deployment non-trivial, as they typically contain multiple heterogeneous compute cores with limited, programmer-managed memory to optimize latency and energy efficiency. We propose HTVM - a compiler that merges TVM with DORY to maximize the utilization of heterogeneous accelerators and minimize data movements. HTVM allows deploying the MLPerf(TM) Tiny suite on DIANA, an SoC with a RISC-V CPU, and digital and analog compute-in-memory AI accelerators, at 120x improved performance over plain TVM deployment.","sentences":["Optimal deployment of deep neural networks (DNNs) on state-of-the-art Systems-on-Chips (SoCs) is crucial for tiny machine learning (TinyML) at the edge.","The complexity of these SoCs makes deployment non-trivial, as they typically contain multiple heterogeneous compute cores with limited, programmer-managed memory to optimize latency and energy efficiency.","We propose HTVM - a compiler that merges TVM with DORY to maximize the utilization of heterogeneous accelerators and minimize data movements.","HTVM allows deploying the MLPerf(TM)","Tiny suite on DIANA, an SoC with a RISC-V CPU, and digital and analog compute-in-memory AI accelerators, at 120x improved performance over plain TVM deployment."],"url":"http://arxiv.org/abs/2406.07453v1","category":"cs.PL"}
{"created":"2024-06-11 16:53:14","title":"Holographic and Gravity-Thermodynamic Approaches in Entropic Cosmology: Bayesian Assessment using late-time Data","abstract":"We investigate the cosmological implications of entropy-based approaches in the context of Holographic Dark Energy (HDE) and Gravity-Thermodynamics (GT) formalisms. We utilise the extended Barrow entropy form, with the index parameter $\\Delta$, representing the fractal dimension of the horizon. We also test implementing different parameter ranges for $\\Delta$, which can be extended to Tsallis' interpretation within the same formal cosmology. We perform a Bayesian analysis to constrain the cosmological parameters using the Pantheon+, more recent DESy5, DESI, and, as a supplement, Quasar datasets. We find that the HDE model within almost all data combinations performs extremely well in comparison to the GT approach, which is usually strongly disfavored. Using the combination of DESy5+DESI alone, we find that the GT approaches are disfavored at $|\\log \\mathcal{B}| \\sim 5.8$ and $|\\log \\mathcal{B}| \\sim 6.2$ for the Barrow and Tsallis limits on $\\Delta$, respectively, wrt $\\Lambda$CDM model. While the HDE approach is statistically equivalent to $\\Lambda$CDM when comparing the Bayesian evidence. We also investigate the evolution of the dark energy equation of state and place limits on the same, consistent with quintessence-like behaviour in the HDE approaches.","sentences":["We investigate the cosmological implications of entropy-based approaches in the context of Holographic Dark Energy (HDE) and Gravity-Thermodynamics (GT) formalisms.","We utilise the extended Barrow entropy form, with the index parameter $\\Delta$, representing the fractal dimension of the horizon.","We also test implementing different parameter ranges for $\\Delta$, which can be extended to Tsallis' interpretation within the same formal cosmology.","We perform a Bayesian analysis to constrain the cosmological parameters using the Pantheon+, more recent DESy5, DESI, and, as a supplement, Quasar datasets.","We find that the HDE model within almost all data combinations performs extremely well in comparison to the GT approach, which is usually strongly disfavored.","Using the combination of DESy5+DESI alone, we find that the GT approaches are disfavored at $|\\log \\mathcal{B}|","\\sim 5.8$ and $|\\log \\mathcal{B}|","\\sim 6.2$ for the Barrow and Tsallis limits on $\\Delta$, respectively, wrt","$\\Lambda$CDM model.","While the HDE approach is statistically equivalent to $\\Lambda$CDM when comparing the Bayesian evidence.","We also investigate the evolution of the dark energy equation of state and place limits on the same, consistent with quintessence-like behaviour in the HDE approaches."],"url":"http://arxiv.org/abs/2406.07446v1","category":"astro-ph.CO"}
{"created":"2024-06-11 16:51:47","title":"Metastability in networks of nonlinear stochastic integrate-and-fire neurons","abstract":"Neurons in the brain continuously process the barrage of sensory inputs they receive from the environment. A wide array of experimental work has shown that the collective activity of neural populations encodes and processes this constant bombardment of information. How these collective patterns of activity depend on single neuron properties is often unclear. Single-neuron recordings have shown that individual neural responses to inputs are nonlinear, which prevents a straightforward extrapolation from single neuron features to emergent collective states. In this work, we use a field theoretic formulation of a stochastic leaky integrate-and-fire model to study the impact of nonlinear intensity functions on macroscopic network activity. We show that the interplay between nonlinear spike emission and membrane potential resets can i) give rise to metastable transitions between active firing rate states, and ii) can enhance or suppress mean firing rates and membrane potentials in opposite directions.","sentences":["Neurons in the brain continuously process the barrage of sensory inputs they receive from the environment.","A wide array of experimental work has shown that the collective activity of neural populations encodes and processes this constant bombardment of information.","How these collective patterns of activity depend on single neuron properties is often unclear.","Single-neuron recordings have shown that individual neural responses to inputs are nonlinear, which prevents a straightforward extrapolation from single neuron features to emergent collective states.","In this work, we use a field theoretic formulation of a stochastic leaky integrate-and-fire model to study the impact of nonlinear intensity functions on macroscopic network activity.","We show that the interplay between nonlinear spike emission and membrane potential resets can i) give rise to metastable transitions between active firing rate states, and ii) can enhance or suppress mean firing rates and membrane potentials in opposite directions."],"url":"http://arxiv.org/abs/2406.07445v1","category":"q-bio.NC"}
{"created":"2024-06-11 16:49:00","title":"GPU Accelerated Implicit Kinetic Meshfree Method based on Modified LU-SGS","abstract":"This report presents the GPU acceleration of implicit kinetic meshfree methods using modified LU-SGS algorithms. The meshfree scheme is based on the least squares kinetic upwind method (LSKUM). In the existing matrix-free LU-SGS approaches for kinetic meshfree methods, the products of split flux Jacobians and increments in conserved vectors are approximated by increments in the split fluxes. In our modified LU-SGS approach, the Jacobian vector products are computed exactly using algorithmic differentiation (AD). The implicit GPU solvers with exact and approximate computation of the Jacobian vector products are applied to the standard test cases for two-dimensional inviscid flows. Numerical results have shown that the GPU solvers with the exact computation of the Jacobian vector products are computationally more efficient and yield better convergence rates than the solvers with approximations to the Jacobian vector products. Benchmarks are presented to assess the performance of implicit GPU solvers compared to the explicit GPU solver and the implicit serial LSKUM solver.","sentences":["This report presents the GPU acceleration of implicit kinetic meshfree methods using modified LU-SGS algorithms.","The meshfree scheme is based on the least squares kinetic upwind method (LSKUM).","In the existing matrix-free LU-SGS approaches for kinetic meshfree methods, the products of split flux Jacobians and increments in conserved vectors are approximated by increments in the split fluxes.","In our modified LU-SGS approach, the Jacobian vector products are computed exactly using algorithmic differentiation (AD).","The implicit GPU solvers with exact and approximate computation of the Jacobian vector products are applied to the standard test cases for two-dimensional inviscid flows.","Numerical results have shown that the GPU solvers with the exact computation of the Jacobian vector products are computationally more efficient and yield better convergence rates than the solvers with approximations to the Jacobian vector products.","Benchmarks are presented to assess the performance of implicit GPU solvers compared to the explicit GPU solver and the implicit serial LSKUM solver."],"url":"http://arxiv.org/abs/2406.07441v1","category":"cs.DC"}
{"created":"2024-06-11 16:45:48","title":"DeformTime: Capturing Variable Dependencies with Deformable Attention for Time Series Forecasting","abstract":"In multivariate time series (MTS) forecasting, existing state-of-the-art deep learning approaches tend to focus on autoregressive formulations and overlook the information within exogenous indicators. To address this limitation, we present DeformTime, a neural network architecture that attempts to capture correlated temporal patterns from the input space, and hence, improve forecasting accuracy. It deploys two core operations performed by deformable attention blocks (DABs): learning dependencies across variables from different time steps (variable DAB), and preserving temporal dependencies in data from previous time steps (temporal DAB). Input data transformation is explicitly designed to enhance learning from the deformed series of information while passing through a DAB. We conduct extensive experiments on 6 MTS data sets, using previously established benchmarks as well as challenging infectious disease modelling tasks with more exogenous variables. The results demonstrate that DeformTime improves accuracy against previous competitive methods across the vast majority of MTS forecasting tasks, reducing the mean absolute error by 10% on average. Notably, performance gains remain consistent across longer forecasting horizons.","sentences":["In multivariate time series (MTS) forecasting, existing state-of-the-art deep learning approaches tend to focus on autoregressive formulations and overlook the information within exogenous indicators.","To address this limitation, we present DeformTime, a neural network architecture that attempts to capture correlated temporal patterns from the input space, and hence, improve forecasting accuracy.","It deploys two core operations performed by deformable attention blocks (DABs): learning dependencies across variables from different time steps (variable DAB), and preserving temporal dependencies in data from previous time steps (temporal DAB).","Input data transformation is explicitly designed to enhance learning from the deformed series of information while passing through a DAB.","We conduct extensive experiments on 6 MTS data sets, using previously established benchmarks as well as challenging infectious disease modelling tasks with more exogenous variables.","The results demonstrate that DeformTime improves accuracy against previous competitive methods across the vast majority of MTS forecasting tasks, reducing the mean absolute error by 10% on average.","Notably, performance gains remain consistent across longer forecasting horizons."],"url":"http://arxiv.org/abs/2406.07438v1","category":"cs.LG"}
{"created":"2024-06-11 17:59:01","title":"Ctrl-X: Controlling Structure and Appearance for Text-To-Image Generation Without Guidance","abstract":"Recent controllable generation approaches such as FreeControl and Diffusion Self-guidance bring fine-grained spatial and appearance control to text-to-image (T2I) diffusion models without training auxiliary modules. However, these methods optimize the latent embedding for each type of score function with longer diffusion steps, making the generation process time-consuming and limiting their flexibility and use. This work presents Ctrl-X, a simple framework for T2I diffusion controlling structure and appearance without additional training or guidance. Ctrl-X designs feed-forward structure control to enable the structure alignment with a structure image and semantic-aware appearance transfer to facilitate the appearance transfer from a user-input image. Extensive qualitative and quantitative experiments illustrate the superior performance of Ctrl-X on various condition inputs and model checkpoints. In particular, Ctrl-X supports novel structure and appearance control with arbitrary condition images of any modality, exhibits superior image quality and appearance transfer compared to existing works, and provides instant plug-and-play functionality to any T2I and text-to-video (T2V) diffusion model. See our project page for an overview of the results: https://genforce.github.io/ctrl-x","sentences":["Recent controllable generation approaches such as FreeControl and Diffusion Self-guidance bring fine-grained spatial and appearance control to text-to-image (T2I) diffusion models without training auxiliary modules.","However, these methods optimize the latent embedding for each type of score function with longer diffusion steps, making the generation process time-consuming and limiting their flexibility and use.","This work presents Ctrl-X, a simple framework for T2I diffusion controlling structure and appearance without additional training or guidance.","Ctrl-X designs feed-forward structure control to enable the structure alignment with a structure image and semantic-aware appearance transfer to facilitate the appearance transfer from a user-input image.","Extensive qualitative and quantitative experiments illustrate the superior performance of Ctrl-X on various condition inputs and model checkpoints.","In particular, Ctrl-X supports novel structure and appearance control with arbitrary condition images of any modality, exhibits superior image quality and appearance transfer compared to existing works, and provides instant plug-and-play functionality to any T2I and text-to-video (T2V) diffusion model.","See our project page for an overview of the results: https://genforce.github.io/ctrl-x"],"url":"http://arxiv.org/abs/2406.07540v1","category":"cs.CV"}
{"created":"2024-06-11 17:51:12","title":"Change of numeraire for weak martingale transport","abstract":"Change of numeraire is a classical tool in mathematical finance. Campi-Laachir-Martini established its applicability to martingale optimal transport. We note that the results of Campi-Laachir-Martini extend to the case of weak martingale transport. We apply this to shadow couplings, continuous time martingale transport problems in the framework of Huesmann-Trevisan and in particular to establish the correspondence between stretched Brownian motion with its geometric counterpart.   Note: We emphasize that we learned about the geometric stretched Brownian motion gSBM (defined in PDE terms) in a presentation of Loeper \\cite{Lo23} before our work on this topic started. We noticed that a change of numeraire transformation in the spirit of \\cite{CaLaMa14} allows for an alternative viewpoint in the weak optimal transport framework. We make our work public following the publication of Backhoff-Loeper-Obloj's work \\cite{BaLoOb24} on arxiv.org. The article \\cite{BaLoOb24} derives gSBM using PDE techniques as well as through an independent probabilistic approach which is close to the one we give in the present article.","sentences":["Change of numeraire is a classical tool in mathematical finance.","Campi-Laachir-Martini established its applicability to martingale optimal transport.","We note that the results of Campi-Laachir-Martini extend to the case of weak martingale transport.","We apply this to shadow couplings, continuous time martingale transport problems in the framework of Huesmann-Trevisan and in particular to establish the correspondence between stretched Brownian motion with its geometric counterpart.   ","Note: We emphasize that we learned about the geometric stretched Brownian motion gSBM (defined in PDE terms) in a presentation of Loeper \\cite{Lo23} before our work on this topic started.","We noticed that a change of numeraire transformation in the spirit of \\cite{CaLaMa14} allows for an alternative viewpoint in the weak optimal transport framework.","We make our work public following the publication of Backhoff-Loeper-Obloj's work \\cite{BaLoOb24} on arxiv.org.","The article \\cite{BaLoOb24} derives gSBM using PDE techniques as well as through an independent probabilistic approach which is close to the one we give in the present article."],"url":"http://arxiv.org/abs/2406.07523v1","category":"math.PR"}
{"created":"2024-06-11 17:50:20","title":"Faster Spectral Density Estimation and Sparsification in the Nuclear Norm","abstract":"We consider the problem of estimating the spectral density of the normalized adjacency matrix of an $n$-node undirected graph. We provide a randomized algorithm that, with $O(n\\epsilon^{-2})$ queries to a degree and neighbor oracle and in $O(n\\epsilon^{-3})$ time, estimates the spectrum up to $\\epsilon$ accuracy in the Wasserstein-1 metric. This improves on previous state-of-the-art methods, including an $O(n\\epsilon^{-7})$ time algorithm from [Braverman et al., STOC 2022] and, for sufficiently small $\\epsilon$, a $2^{O(\\epsilon^{-1})}$ time method from [Cohen-Steiner et al., KDD 2018]. To achieve this result, we introduce a new notion of graph sparsification, which we call nuclear sparsification. We provide an $O(n\\epsilon^{-2})$-query and $O(n\\epsilon^{-2})$-time algorithm for computing $O(n\\epsilon^{-2})$-sparse nuclear sparsifiers. We show that this bound is optimal in both its sparsity and query complexity, and we separate our results from the related notion of additive spectral sparsification. Of independent interest, we show that our sparsification method also yields the first deterministic algorithm for spectral density estimation that scales linearly with $n$ (sublinear in the representation size of the graph).","sentences":["We consider the problem of estimating the spectral density of the normalized adjacency matrix of an $n$-node undirected graph.","We provide a randomized algorithm that, with $O(n\\epsilon^{-2})$ queries to a degree and neighbor oracle and in $O(n\\epsilon^{-3})$ time, estimates the spectrum up to $\\epsilon$ accuracy in the Wasserstein-1 metric.","This improves on previous state-of-the-art methods, including an $O(n\\epsilon^{-7})$ time algorithm from [Braverman et al., STOC 2022] and, for sufficiently small $\\epsilon$, a $2^{O(\\epsilon^{-1})}$ time method from [Cohen-Steiner et al., KDD 2018].","To achieve this result, we introduce a new notion of graph sparsification, which we call nuclear sparsification.","We provide an $O(n\\epsilon^{-2})$-query and $O(n\\epsilon^{-2})$-time algorithm for computing $O(n\\epsilon^{-2})$-sparse nuclear sparsifiers.","We show that this bound is optimal in both its sparsity and query complexity, and we separate our results from the related notion of additive spectral sparsification.","Of independent interest, we show that our sparsification method also yields the first deterministic algorithm for spectral density estimation that scales linearly with $n$ (sublinear in the representation size of the graph)."],"url":"http://arxiv.org/abs/2406.07521v1","category":"cs.DS"}
{"created":"2024-06-11 17:34:46","title":"Trim 3D Gaussian Splatting for Accurate Geometry Representation","abstract":"In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to reconstruct accurate 3D geometry from images. Previous arts for geometry reconstruction from 3D Gaussians mainly focus on exploring strong geometry regularization. Instead, from a fresh perspective, we propose to obtain accurate 3D geometry of a scene by Gaussian trimming, which selectively removes the inaccurate geometry while preserving accurate structures. To achieve this, we analyze the contributions of individual 3D Gaussians and propose a contribution-based trimming strategy to remove the redundant or inaccurate Gaussians. Furthermore, our experimental and theoretical analyses reveal that a relatively small Gaussian scale is a non-negligible factor in representing and optimizing the intricate details. Therefore the proposed TrimGS maintains relatively small Gaussian scales. In addition, TrimGS is also compatible with the effective geometry regularization strategies in previous arts. When combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS consistently yields more accurate geometry and higher perceptual quality. Our project page is https://trimgs.github.io","sentences":["In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to reconstruct accurate 3D geometry from images.","Previous arts for geometry reconstruction from 3D Gaussians mainly focus on exploring strong geometry regularization.","Instead, from a fresh perspective, we propose to obtain accurate 3D geometry of a scene by Gaussian trimming, which selectively removes the inaccurate geometry while preserving accurate structures.","To achieve this, we analyze the contributions of individual 3D Gaussians and propose a contribution-based trimming strategy to remove the redundant or inaccurate Gaussians.","Furthermore, our experimental and theoretical analyses reveal that a relatively small Gaussian scale is a non-negligible factor in representing and optimizing the intricate details.","Therefore the proposed TrimGS maintains relatively small Gaussian scales.","In addition, TrimGS is also compatible with the effective geometry regularization strategies in previous arts.","When combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS consistently yields more accurate geometry and higher perceptual quality.","Our project page is https://trimgs.github.io"],"url":"http://arxiv.org/abs/2406.07499v1","category":"cs.CV"}
{"created":"2024-06-11 17:30:52","title":"Some real Rel trajectories in $\\mathcal{H}(1,1)$ that are not recurrent","abstract":"We study the real Rel orbits of some translation surfaces in the stratum $\\mathcal{H}(1,1)$. Specifically, surfaces that are tremors of the locus of branched double covers of tori. We give necessary and sufficient conditions on tremors of a surface so that the real Rel orbit is recurrent. As a consequence, we are able to provide explicit examples of trajectories of real Rel that are not recurrent.","sentences":["We study the real Rel orbits of some translation surfaces in the stratum $\\mathcal{H}(1,1)$. Specifically, surfaces that are tremors of the locus of branched double covers of tori.","We give necessary and sufficient conditions on tremors of a surface so that the real Rel orbit is recurrent.","As a consequence, we are able to provide explicit examples of trajectories of real Rel that are not recurrent."],"url":"http://arxiv.org/abs/2406.07495v1","category":"math.DS"}
{"created":"2024-06-11 17:28:09","title":"ReduceFormer: Attention with Tensor Reduction by Summation","abstract":"Transformers have excelled in many tasks including vision. However, efficient deployment of transformer models in low-latency or high-throughput applications is hindered by the computation in the attention mechanism which involves expensive operations such as matrix multiplication and Softmax. To address this, we introduce ReduceFormer, a family of models optimized for efficiency with the spirit of attention. ReduceFormer leverages only simple operations such as reduction and element-wise multiplication, leading to greatly simplified architecture and improved inference performance, with up to 37% reduction in latency and 44% improvement in throughput, while maintaining competitive accuracy comparable to other recent methods. The proposed model family is suitable for edge devices where compute resource and memory bandwidth are limited, as well as for cloud computing where high throughput is sought after.","sentences":["Transformers have excelled in many tasks including vision.","However, efficient deployment of transformer models in low-latency or high-throughput applications is hindered by the computation in the attention mechanism which involves expensive operations such as matrix multiplication and Softmax.","To address this, we introduce ReduceFormer, a family of models optimized for efficiency with the spirit of attention.","ReduceFormer leverages only simple operations such as reduction and element-wise multiplication, leading to greatly simplified architecture and improved inference performance, with up to 37% reduction in latency and 44% improvement in throughput, while maintaining competitive accuracy comparable to other recent methods.","The proposed model family is suitable for edge devices where compute resource and memory bandwidth are limited, as well as for cloud computing where high throughput is sought after."],"url":"http://arxiv.org/abs/2406.07488v1","category":"cs.CV"}
{"created":"2024-06-11 17:27:11","title":"Novel Optimized Designs of Modulo $2n+1$ Adder for Quantum Computing","abstract":"Quantum modular adders are one of the most fundamental yet versatile quantum computation operations. They help implement functions of higher complexity, such as subtraction and multiplication, which are used in applications such as quantum cryptanalysis, quantum image processing, and securing communication. To the best of our knowledge, there is no existing design of quantum modulo $(2n+1)$ adder. In this work, we propose four quantum adders targeted specifically for modulo $(2n+1)$ addition. These adders can provide both regular and modulo $(2n+1)$ sum concurrently, enhancing their application in residue number system based arithmetic. Our first design, QMA1, is a novel quantum modulo $(2n+1)$ adder. The second proposed adder, QMA2, optimizes the utilization of quantum gates within the QMA1, resulting in 37.5% reduced CNOT gate count, 46.15% reduced CNOT depth, and 26.5% decrease in both Toffoli gates and depth. We propose a third adder QMA3 that uses zero resets, a dynamic circuits based feature that reuses qubits, leading to 25% savings in qubit count. Our fourth design, QMA4, demonstrates the benefit of incorporating additional zero resets to achieve a purer zero state, reducing quantum state preparation errors. Notably, we conducted experiments using 5-qubit configurations of the proposed modulo $(2n+1)$ adders on the IBM Washington, a 127-qubit quantum computer based on the Eagle R1 architecture, to demonstrate a 28.8% reduction in QMA1's error of which: (i) 18.63% error reduction happens due to gate and depth reduction in QMA2, and (ii) 2.53% drop in error due to qubit reduction in QMA3, and (iii) 7.64% error decreased due to application of additional zero resets in QMA4.","sentences":["Quantum modular adders are one of the most fundamental yet versatile quantum computation operations.","They help implement functions of higher complexity, such as subtraction and multiplication, which are used in applications such as quantum cryptanalysis, quantum image processing, and securing communication.","To the best of our knowledge, there is no existing design of quantum modulo $(2n+1)$ adder.","In this work, we propose four quantum adders targeted specifically for modulo $(2n+1)$ addition.","These adders can provide both regular and modulo $(2n+1)$ sum concurrently, enhancing their application in residue number system based arithmetic.","Our first design, QMA1, is a novel quantum modulo $(2n+1)$ adder.","The second proposed adder, QMA2, optimizes the utilization of quantum gates within the QMA1, resulting in 37.5% reduced CNOT gate count, 46.15% reduced CNOT depth, and 26.5% decrease in both Toffoli gates and depth.","We propose a third adder QMA3 that uses zero resets, a dynamic circuits based feature that reuses qubits, leading to 25% savings in qubit count.","Our fourth design, QMA4, demonstrates the benefit of incorporating additional zero resets to achieve a purer zero state, reducing quantum state preparation errors.","Notably, we conducted experiments using 5-qubit configurations of the proposed modulo $(2n+1)$ adders on the IBM Washington, a 127-qubit quantum computer based on the Eagle R1 architecture, to demonstrate a 28.8% reduction in QMA1's error of which: (i) 18.63% error reduction happens due to gate and depth reduction in QMA2, and (ii) 2.53% drop in error due to qubit reduction in QMA3, and (iii) 7.64% error decreased due to application of additional zero resets in QMA4."],"url":"http://arxiv.org/abs/2406.07486v1","category":"quant-ph"}
{"created":"2024-06-11 17:10:25","title":"Convex ordering for stochastic control: the swing contracts case","abstract":"We investigate propagation of convexity and convex ordering on a typical stochastic optimal control problem, namely the pricing of \\q{\\emph{Take-or-Pay}} swing option, a financial derivative product commonly traded on energy markets. The dynamics of the underlying asset is modelled by an \\emph{ARCH} model with convex coefficients. We prove that the value function associated to the stochastic optimal control problem is a convex function of the underlying asset price. We also introduce a domination criterion offering insights into the monotonicity of the value function with respect to parameters of the underlying \\emph{ARCH} coefficients. We particularly focus on the one-dimensional setting where, by means of Stein's formula and regularization techniques, we show that the convexity assumption for the \\emph{ARCH} coefficients can be relaxed with a semi-convexity assumption. To validate the results presented in this paper, we also conduct numerical illustrations.","sentences":["We investigate propagation of convexity and convex ordering on a typical stochastic optimal control problem, namely the pricing of \\q{\\emph{Take-or-Pay}} swing option, a financial derivative product commonly traded on energy markets.","The dynamics of the underlying asset is modelled by an \\emph{ARCH} model with convex coefficients.","We prove that the value function associated to the stochastic optimal control problem is a convex function of the underlying asset price.","We also introduce a domination criterion offering insights into the monotonicity of the value function with respect to parameters of the underlying \\emph{ARCH} coefficients.","We particularly focus on the one-dimensional setting where, by means of Stein's formula and regularization techniques, we show that the convexity assumption for the \\emph{ARCH} coefficients can be relaxed with a semi-convexity assumption.","To validate the results presented in this paper, we also conduct numerical illustrations."],"url":"http://arxiv.org/abs/2406.07464v1","category":"q-fin.MF"}
{"created":"2024-06-11 17:08:21","title":"Noise-robust Speech Separation with Fast Generative Correction","abstract":"Speech separation, the task of isolating multiple speech sources from a mixed audio signal, remains challenging in noisy environments. In this paper, we propose a generative correction method to enhance the output of a discriminative separator. By leveraging a generative corrector based on a diffusion model, we refine the separation process for single-channel mixture speech by removing noises and perceptually unnatural distortions. Furthermore, we optimize the generative model using a predictive loss to streamline the diffusion model's reverse process into a single step and rectify any associated errors by the reverse process. Our method achieves state-of-the-art performance on the in-domain Libri2Mix noisy dataset, and out-of-domain WSJ with a variety of noises, improving SI-SNR by 22-35% relative to SepFormer, demonstrating robustness and strong generalization capabilities.","sentences":["Speech separation, the task of isolating multiple speech sources from a mixed audio signal, remains challenging in noisy environments.","In this paper, we propose a generative correction method to enhance the output of a discriminative separator.","By leveraging a generative corrector based on a diffusion model, we refine the separation process for single-channel mixture speech by removing noises and perceptually unnatural distortions.","Furthermore, we optimize the generative model using a predictive loss to streamline the diffusion model's reverse process into a single step and rectify any associated errors by the reverse process.","Our method achieves state-of-the-art performance on the in-domain Libri2Mix noisy dataset, and out-of-domain WSJ with a variety of noises, improving SI-SNR by 22-35% relative to SepFormer, demonstrating robustness and strong generalization capabilities."],"url":"http://arxiv.org/abs/2406.07461v1","category":"eess.AS"}
{"created":"2024-06-11 16:57:48","title":"An Optimism-based Approach to Online Evaluation of Generative Models","abstract":"Existing frameworks for evaluating and comparing generative models typically target an offline setting, where the evaluator has access to full batches of data produced by the models. However, in many practical scenarios, the goal is to identify the best model using the fewest generated samples to minimize the costs of querying data from the models. Such an online comparison is challenging with current offline assessment methods. In this work, we propose an online evaluation framework to find the generative model that maximizes a standard assessment score among a group of available models. Our method uses an optimism-based multi-armed bandit framework to identify the model producing data with the highest evaluation score, quantifying the quality and diversity of generated data. Specifically, we study the online assessment of generative models based on the Fr\\'echet Inception Distance (FID) and Inception Score (IS) metrics and propose the FID-UCB and IS-UCB algorithms leveraging the upper confidence bound approach in online learning. We prove sub-linear regret bounds for these algorithms and present numerical results on standard image datasets, demonstrating their effectiveness in identifying the score-maximizing generative model.","sentences":["Existing frameworks for evaluating and comparing generative models typically target an offline setting, where the evaluator has access to full batches of data produced by the models.","However, in many practical scenarios, the goal is to identify the best model using the fewest generated samples to minimize the costs of querying data from the models.","Such an online comparison is challenging with current offline assessment methods.","In this work, we propose an online evaluation framework to find the generative model that maximizes a standard assessment score among a group of available models.","Our method uses an optimism-based multi-armed bandit framework to identify the model producing data with the highest evaluation score, quantifying the quality and diversity of generated data.","Specifically, we study the online assessment of generative models based on the Fr\\'echet Inception Distance (FID) and Inception Score (IS) metrics and propose the FID-UCB and IS-UCB algorithms leveraging the upper confidence bound approach in online learning.","We prove sub-linear regret bounds for these algorithms and present numerical results on standard image datasets, demonstrating their effectiveness in identifying the score-maximizing generative model."],"url":"http://arxiv.org/abs/2406.07451v1","category":"cs.LG"}
{"created":"2024-06-11 16:53:52","title":"Accuracy and limitations of the bond polarizability model in modeling of Raman scattering from molecular dynamics simulations","abstract":"Calculation of Raman scattering from molecular dynamics (MD) simulations requires accurate modeling of the evolution of the electronic polarizability of the system along its MD trajectory. For large systems, this necessitates the use of atomistic models to represent the dependence of electronic polarizability on atomic coordinates. The bond polarizability model (BPM) is the simplest such model and has been used for modeling the Raman spectra of molecular systems but has not been applied to solid-state systems. Here, we systematically investigate the accuracy and limitations of the BPM parameterized from density functional theory (DFT) results for a series of simple molecules such as CO2, SO2, H2S, H2O, NH3, and CH4, the more complex CH2O, CH3OH and CH3CH2OH and thiophene molecules and the BaTiO3 and CsPbBr3 perovskite solids. We find that BPM can reliably reproduce the overall features of the Raman spectra such as shifts of peak positions. However, with the exception of highly symmetric systems, the assumption of non-interacting bonds limits the quantitative accuracy of the BPM; this assumption also leads to qualitatively inaccurate polarizability evolution and Raman spectra for systems where large deviations from the ground state structure are present.","sentences":["Calculation of Raman scattering from molecular dynamics (MD) simulations requires accurate modeling of the evolution of the electronic polarizability of the system along its MD trajectory.","For large systems, this necessitates the use of atomistic models to represent the dependence of electronic polarizability on atomic coordinates.","The bond polarizability model (BPM) is the simplest such model and has been used for modeling the Raman spectra of molecular systems but has not been applied to solid-state systems.","Here, we systematically investigate the accuracy and limitations of the BPM parameterized from density functional theory (DFT) results for a series of simple molecules such as CO2, SO2, H2S, H2O, NH3, and CH4, the more complex CH2O, CH3OH and CH3CH2OH and thiophene molecules and the BaTiO3 and CsPbBr3 perovskite solids.","We find that BPM can reliably reproduce the overall features of the Raman spectra such as shifts of peak positions.","However, with the exception of highly symmetric systems, the assumption of non-interacting bonds limits the quantitative accuracy of the BPM; this assumption also leads to qualitatively inaccurate polarizability evolution and Raman spectra for systems where large deviations from the ground state structure are present."],"url":"http://arxiv.org/abs/2406.07448v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-11 16:14:30","title":"Accelerating Ill-conditioned Hankel Matrix Recovery via Structured Newton-like Descent","abstract":"This paper studies the robust Hankel recovery problem, which simultaneously removes the sparse outliers and fulfills missing entries from the partial observation. We propose a novel non-convex algorithm, coined Hankel Structured Newton-Like Descent (HSNLD), to tackle the robust Hankel recovery problem. HSNLD is highly efficient with linear convergence, and its convergence rate is independent of the condition number of the underlying Hankel matrix. The recovery guarantee has been established under some mild conditions. Numerical experiments on both synthetic and real datasets show the superior performance of HSNLD against state-of-the-art algorithms.","sentences":["This paper studies the robust Hankel recovery problem, which simultaneously removes the sparse outliers and fulfills missing entries from the partial observation.","We propose a novel non-convex algorithm, coined Hankel Structured Newton-Like Descent (HSNLD), to tackle the robust Hankel recovery problem.","HSNLD is highly efficient with linear convergence, and its convergence rate is independent of the condition number of the underlying Hankel matrix.","The recovery guarantee has been established under some mild conditions.","Numerical experiments on both synthetic and real datasets show the superior performance of HSNLD against state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2406.07409v1","category":"stat.ML"}
{"created":"2024-06-11 16:13:11","title":"ADDOPT: An Additive Manufacturing Optimal Control Framework Demonstrated in Minimizing Layer-Level Thermal Variance in Electron Beam Powder Bed Fusion","abstract":"Additive manufacturing (AM) techniques hold promise but face significant challenges in process planning and optimization. The large temporal and spatial variations in temperature that can occur in layer-wise AM lead to thermal excursions, resulting in property variations and defects. These variations cannot always be fully mitigated by simple static parameter search. To address this challenge, we propose a general approach based on modeling AM processes on the part-scale in state-space and framing AM process planning as a numerical optimal control problem. We demonstrate this approach on the problem of minimizing thermal variation in a given layer in the electron beam powder bed fusion (EB-PBF) AM process, and are able to compute globally optimal dynamic process plans. These optimized process plans are then evaluated in simulation, achieving an 87% and 86% reduction in cumulative variance compared to random spot melting and a uniform power field respectively, and are further validated in experiment. This one-shot feedforward planning approach expands the capabilities of AM technology by minimizing the need for experimentation and iteration to achieve process optimization. Further, this work opens the possibility for the application of optimal control theory to part-scale optimization and control in AM.","sentences":["Additive manufacturing (AM) techniques hold promise but face significant challenges in process planning and optimization.","The large temporal and spatial variations in temperature that can occur in layer-wise AM lead to thermal excursions, resulting in property variations and defects.","These variations cannot always be fully mitigated by simple static parameter search.","To address this challenge, we propose a general approach based on modeling AM processes on the part-scale in state-space and framing AM process planning as a numerical optimal control problem.","We demonstrate this approach on the problem of minimizing thermal variation in a given layer in the electron beam powder bed fusion (EB-PBF) AM process, and are able to compute globally optimal dynamic process plans.","These optimized process plans are then evaluated in simulation, achieving an 87% and 86% reduction in cumulative variance compared to random spot melting and a uniform power field respectively, and are further validated in experiment.","This one-shot feedforward planning approach expands the capabilities of AM technology by minimizing the need for experimentation and iteration to achieve process optimization.","Further, this work opens the possibility for the application of optimal control theory to part-scale optimization and control in AM."],"url":"http://arxiv.org/abs/2406.07408v1","category":"eess.SY"}
{"created":"2024-06-11 16:13:09","title":"Private Geometric Median","abstract":"In this paper, we study differentially private (DP) algorithms for computing the geometric median (GM) of a dataset: Given $n$ points, $x_1,\\dots,x_n$ in $\\mathbb{R}^d$, the goal is to find a point $\\theta$ that minimizes the sum of the Euclidean distances to these points, i.e., $\\sum_{i=1}^{n} \\|\\theta - x_i\\|_2$. Off-the-shelf methods, such as DP-GD, require strong a priori knowledge locating the data within a ball of radius $R$, and the excess risk of the algorithm depends linearly on $R$. In this paper, we ask: can we design an efficient and private algorithm with an excess error guarantee that scales with the (unknown) radius containing the majority of the datapoints? Our main contribution is a pair of polynomial-time DP algorithms for the task of private GM with an excess error guarantee that scales with the effective diameter of the datapoints. Additionally, we propose an inefficient algorithm based on the inverse smooth sensitivity mechanism, which satisfies the more restrictive notion of pure DP. We complement our results with a lower bound and demonstrate the optimality of our polynomial-time algorithms in terms of sample complexity.","sentences":["In this paper, we study differentially private (DP) algorithms for computing the geometric median (GM) of a dataset: Given $n$ points, $x_1,\\dots,x_n$ in $\\mathbb{R}^d$, the goal is to find a point $\\theta$ that minimizes the sum of the Euclidean distances to these points, i.e., $\\sum_{i=1}^{n} \\|\\theta - x_i\\|_2$. Off-the-shelf methods, such as DP-GD, require strong a priori knowledge locating the data within a ball of radius $R$, and the excess risk of the algorithm depends linearly on $R$. In this paper, we ask: can we design an efficient and private algorithm with an excess error guarantee that scales with the (unknown) radius containing the majority of the datapoints?","Our main contribution is a pair of polynomial-time DP algorithms for the task of private GM with an excess error guarantee that scales with the effective diameter of the datapoints.","Additionally, we propose an inefficient algorithm based on the inverse smooth sensitivity mechanism, which satisfies the more restrictive notion of pure DP.","We complement our results with a lower bound and demonstrate the optimality of our polynomial-time algorithms in terms of sample complexity."],"url":"http://arxiv.org/abs/2406.07407v1","category":"cs.LG"}
{"created":"2024-06-11 16:09:27","title":"Optimal Marital Strategies: How Couples Develop Successful Interaction Styles","abstract":"The study of marriage dynamics and of strategies to reduce the likelihood of divorce has been an important research area for many years. Gottman's research on successful marriages revealed three matched interaction styles: conflict-avoiding, validating, and volatile. There has, however, been little progress in explaining how couples develop these styles of interaction and why failure to do so leads to failed marriages. In this paper, we show that these interaction styles arise as solutions to an optimal control problem where the couples jointly maximize a common goal. The validating style arises when the benefit from achieving joint happiness is balanced by the emotional cost of adopting a particular style. The ubiquitous conflict-avoider style arises naturally when the couple does not care about the cost. The volatile style is not an optimal solution, but volatile marriages may still be successful for couples with highly positive natural dispositions. The problem of the spouses having different goals in marriage is relevant to marriage repair, and this problem will be studied in the next paper using differential game theory.","sentences":["The study of marriage dynamics and of strategies to reduce the likelihood of divorce has been an important research area for many years.","Gottman's research on successful marriages revealed three matched interaction styles: conflict-avoiding, validating, and volatile.","There has, however, been little progress in explaining how couples develop these styles of interaction and why failure to do so leads to failed marriages.","In this paper, we show that these interaction styles arise as solutions to an optimal control problem where the couples jointly maximize a common goal.","The validating style arises when the benefit from achieving joint happiness is balanced by the emotional cost of adopting a particular style.","The ubiquitous conflict-avoider style arises naturally when the couple does not care about the cost.","The volatile style is not an optimal solution, but volatile marriages may still be successful for couples with highly positive natural dispositions.","The problem of the spouses having different goals in marriage is relevant to marriage repair, and this problem will be studied in the next paper using differential game theory."],"url":"http://arxiv.org/abs/2406.07403v1","category":"math.OC"}
{"created":"2024-06-11 16:01:07","title":"Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B","abstract":"This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.","sentences":["This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks.","Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs.","The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance.","Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench.","The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications."],"url":"http://arxiv.org/abs/2406.07394v1","category":"cs.AI"}
{"created":"2024-06-11 15:57:01","title":"DiffCom: Channel Received Signal is a Natural Condition to Guide Diffusion Posterior Sampling","abstract":"End-to-end visual communication systems typically optimize a trade-off between channel bandwidth costs and signal-level distortion metrics. However, under challenging physical conditions, this traditional discriminative communication paradigm often results in unrealistic reconstructions with perceptible blurring and aliasing artifacts, despite the inclusion of perceptual or adversarial losses for optimizing. This issue primarily stems from the receiver's limited knowledge about the underlying data manifold and the use of deterministic decoding mechanisms. To address these limitations, this paper introduces DiffCom, a novel end-to-end generative communication paradigm that utilizes off-the-shelf generative priors and probabilistic diffusion models for decoding, thereby improving perceptual quality without heavily relying on bandwidth costs and received signal quality. Unlike traditional systems that rely on deterministic decoders optimized solely for distortion metrics, our DiffCom leverages raw channel-received signal as a fine-grained condition to guide stochastic posterior sampling. Our approach ensures that reconstructions remain on the manifold of real data with a novel confirming constraint, enhancing the robustness and reliability of the generated outcomes. Furthermore, DiffCom incorporates a blind posterior sampling technique to address scenarios with unknown forward transmission characteristics. Extensive experimental validations demonstrate that DiffCom not only produces realistic reconstructions with details faithful to the original data but also achieves superior robustness against diverse wireless transmission degradations. Collectively, these advancements establish DiffCom as a new benchmark in designing generative communication systems that offer enhanced robustness and generalization superiorities.","sentences":["End-to-end visual communication systems typically optimize a trade-off between channel bandwidth costs and signal-level distortion metrics.","However, under challenging physical conditions, this traditional discriminative communication paradigm often results in unrealistic reconstructions with perceptible blurring and aliasing artifacts, despite the inclusion of perceptual or adversarial losses for optimizing.","This issue primarily stems from the receiver's limited knowledge about the underlying data manifold and the use of deterministic decoding mechanisms.","To address these limitations, this paper introduces DiffCom, a novel end-to-end generative communication paradigm that utilizes off-the-shelf generative priors and probabilistic diffusion models for decoding, thereby improving perceptual quality without heavily relying on bandwidth costs and received signal quality.","Unlike traditional systems that rely on deterministic decoders optimized solely for distortion metrics, our DiffCom leverages raw channel-received signal as a fine-grained condition to guide stochastic posterior sampling.","Our approach ensures that reconstructions remain on the manifold of real data with a novel confirming constraint, enhancing the robustness and reliability of the generated outcomes.","Furthermore, DiffCom incorporates a blind posterior sampling technique to address scenarios with unknown forward transmission characteristics.","Extensive experimental validations demonstrate that DiffCom not only produces realistic reconstructions with details faithful to the original data but also achieves superior robustness against diverse wireless transmission degradations.","Collectively, these advancements establish DiffCom as a new benchmark in designing generative communication systems that offer enhanced robustness and generalization superiorities."],"url":"http://arxiv.org/abs/2406.07390v1","category":"eess.SP"}
{"created":"2024-06-11 15:51:42","title":"Disrupting Bipartite Trading Networks: Matching for Revenue Maximization","abstract":"We model the role of an online platform disrupting a market with unit-demand buyers and unit-supply sellers. Each seller can transact with a subset of the buyers whom she already knows, as well as with any additional buyers to whom she is introduced by the platform. Given these constraints on trade, prices and transactions are induced by a competitive equilibrium. The platform's revenue is proportional to the total price of all trades between platform-introduced buyers and sellers.   In general, we show that the platform's revenue-maximization problem is computationally intractable. We provide structural results for revenue-optimal matchings and isolate special cases in which the platform can efficiently compute them. Furthermore, in a market where the maximum increase in social welfare that the platform can create is $\\Delta W$, we prove that the platform can attain revenue $\\Omega(\\Delta W/\\log(\\min\\{n,m\\}))$, where $n$ and $m$ are the numbers of buyers and sellers, respectively. When $\\Delta W$ is large compared to welfare without the platform, this gives a polynomial-time algorithm that guarantees a logarithmic approximation of the optimal welfare as revenue. We also show that even when the platform optimizes for revenue, the social welfare is at least an $O(\\log(\\min\\{n,m\\}))$-approximation to the optimal welfare. Finally, we prove significantly stronger bounds for revenue and social welfare in homogeneous-goods markets.","sentences":["We model the role of an online platform disrupting a market with unit-demand buyers and unit-supply sellers.","Each seller can transact with a subset of the buyers whom she already knows, as well as with any additional buyers to whom she is introduced by the platform.","Given these constraints on trade, prices and transactions are induced by a competitive equilibrium.","The platform's revenue is proportional to the total price of all trades between platform-introduced buyers and sellers.   ","In general, we show that the platform's revenue-maximization problem is computationally intractable.","We provide structural results for revenue-optimal matchings and isolate special cases in which the platform can efficiently compute them.","Furthermore, in a market where the maximum increase in social welfare that the platform can create is $\\Delta W$, we prove that the platform can attain revenue $\\Omega(\\Delta W/\\log(\\min\\{n,m\\}))$, where $n$ and $m$ are the numbers of buyers and sellers, respectively.","When $\\Delta W$ is large compared to welfare without the platform, this gives a polynomial-time algorithm that guarantees a logarithmic approximation of the optimal welfare as revenue.","We also show that even when the platform optimizes for revenue, the social welfare is at least an $O(\\log(\\min\\{n,m\\}))$-approximation to the optimal welfare.","Finally, we prove significantly stronger bounds for revenue and social welfare in homogeneous-goods markets."],"url":"http://arxiv.org/abs/2406.07385v1","category":"cs.GT"}
{"created":"2024-06-11 15:50:42","title":"Federated Multi-Agent DRL for Radio Resource Management in Industrial 6G in-X subnetworks","abstract":"Recently, 6G in-X subnetworks have been proposed as low-power short-range radio cells to support localized extreme wireless connectivity inside entities such as industrial robots, vehicles, and the human body. Deployment of in-X subnetworks within these entities may result in rapid changes in interference levels and thus, varying link quality. This paper investigates distributed dynamic channel allocation to mitigate inter-subnetwork interference in dense in-factory deployments of 6G in-X subnetworks. This paper introduces two new techniques, Federated Multi-Agent Double Deep Q-Network (F-MADDQN) and Federated Multi-Agent Deep Proximal Policy Optimization (F-MADPPO), for channel allocation in 6G in-X subnetworks. These techniques are based on a client-to-server horizontal federated reinforcement learning framework. The methods require sharing only local model weights with a centralized gNB for federated aggregation thereby preserving local data privacy and security. Simulations were conducted using a practical indoor factory environment proposed by 5G-ACIA and 3GPP models for in-factory environments. The results showed that the proposed methods achieved slightly better performance than baseline schemes with significantly reduced signaling overhead compared to the baseline solutions. The schemes also showed better robustness and generalization ability to changes in deployment densities and propagation parameters.","sentences":["Recently, 6G in-X subnetworks have been proposed as low-power short-range radio cells to support localized extreme wireless connectivity inside entities such as industrial robots, vehicles, and the human body.","Deployment of in-X subnetworks within these entities may result in rapid changes in interference levels and thus, varying link quality.","This paper investigates distributed dynamic channel allocation to mitigate inter-subnetwork interference in dense in-factory deployments of 6G in-X subnetworks.","This paper introduces two new techniques, Federated Multi-Agent Double Deep Q-Network (F-MADDQN) and Federated Multi-Agent Deep Proximal Policy Optimization (F-MADPPO), for channel allocation in 6G in-X subnetworks.","These techniques are based on a client-to-server horizontal federated reinforcement learning framework.","The methods require sharing only local model weights with a centralized gNB for federated aggregation thereby preserving local data privacy and security.","Simulations were conducted using a practical indoor factory environment proposed by 5G-ACIA and 3GPP models for in-factory environments.","The results showed that the proposed methods achieved slightly better performance than baseline schemes with significantly reduced signaling overhead compared to the baseline solutions.","The schemes also showed better robustness and generalization ability to changes in deployment densities and propagation parameters."],"url":"http://arxiv.org/abs/2406.07383v1","category":"eess.SP"}
{"created":"2024-06-11 15:41:48","title":"Closing the Computational-Query Depth Gap in Parallel Stochastic Convex Optimization","abstract":"We develop a new parallel algorithm for minimizing Lipschitz, convex functions with a stochastic subgradient oracle. The total number of queries made and the query depth, i.e., the number of parallel rounds of queries, match the prior state-of-the-art, [CJJLLST23], while improving upon the computational depth by a polynomial factor for sufficiently small accuracy. When combined with previous state-of-the-art methods our result closes a gap between the best-known query depth and the best-known computational depth of parallel algorithms.   Our method starts with a ball acceleration framework of previous parallel methods, i.e., [CJJJLST20, ACJJS21], which reduce the problem to minimizing a regularized Gaussian convolution of the function constrained to Euclidean balls. By developing and leveraging new stability properties of the Hessian of this induced function, we depart from prior parallel algorithms and reduce these ball-constrained optimization problems to stochastic unconstrained quadratic minimization problems. Although we are unable to prove concentration of the asymmetric matrices that we use to approximate this Hessian, we nevertheless develop an efficient parallel method for solving these quadratics. Interestingly, our algorithms can be improved using fast matrix multiplication and use nearly-linear work if the matrix multiplication exponent is 2.","sentences":["We develop a new parallel algorithm for minimizing Lipschitz, convex functions with a stochastic subgradient oracle.","The total number of queries made and the query depth, i.e., the number of parallel rounds of queries, match the prior state-of-the-art, [CJJLLST23], while improving upon the computational depth by a polynomial factor for sufficiently small accuracy.","When combined with previous state-of-the-art methods our result closes a gap between the best-known query depth and the best-known computational depth of parallel algorithms.   ","Our method starts with a ball acceleration framework of previous parallel methods, i.e., [CJJJLST20, ACJJS21], which reduce the problem to minimizing a regularized Gaussian convolution of the function constrained to Euclidean balls.","By developing and leveraging new stability properties of the Hessian of this induced function, we depart from prior parallel algorithms and reduce these ball-constrained optimization problems to stochastic unconstrained quadratic minimization problems.","Although we are unable to prove concentration of the asymmetric matrices that we use to approximate this Hessian, we nevertheless develop an efficient parallel method for solving these quadratics.","Interestingly, our algorithms can be improved using fast matrix multiplication and use nearly-linear work if the matrix multiplication exponent is 2."],"url":"http://arxiv.org/abs/2406.07373v1","category":"math.OC"}
{"created":"2024-06-11 15:41:48","title":"Movable-Antenna Array Empowered ISAC Systems for Low-Altitude Economy","abstract":"This paper investigates a movable-antenna (MA) array empowered integrated sensing and communications (ISAC) over low-altitude platform (LAP) system to support low-altitude economy (LAE) applications. In the considered system, an unmanned aerial vehicle (UAV) is dispatched to hover in the air, working as the UAV-enabled LAP (ULAP) to provide information transmission and sensing simultaneously for LAE applications. To improve the throughput capacity, we formulate a data rate maximization problem by jointly optimizing the transmit information and sensing beamforming and the antenna positions of the MA array. Since the data rate maximization problem is non-convex with highly coupled variables, we propose an efficient alternation optimization based algorithm, which iteratively optimizes parts of the variables while fixing others. Numerical results show the superiority of the proposed MA array-based scheme in terms of the achievable data rate and beamforming gain compared with two benchmark schemes.","sentences":["This paper investigates a movable-antenna (MA) array empowered integrated sensing and communications (ISAC) over low-altitude platform (LAP) system to support low-altitude economy (LAE) applications.","In the considered system, an unmanned aerial vehicle (UAV) is dispatched to hover in the air, working as the UAV-enabled LAP (ULAP) to provide information transmission and sensing simultaneously for LAE applications.","To improve the throughput capacity, we formulate a data rate maximization problem by jointly optimizing the transmit information and sensing beamforming and the antenna positions of the MA array.","Since the data rate maximization problem is non-convex with highly coupled variables, we propose an efficient alternation optimization based algorithm, which iteratively optimizes parts of the variables while fixing others.","Numerical results show the superiority of the proposed MA array-based scheme in terms of the achievable data rate and beamforming gain compared with two benchmark schemes."],"url":"http://arxiv.org/abs/2406.07374v1","category":"eess.SP"}
{"created":"2024-06-11 15:40:44","title":"iMESA: Incremental Distributed Optimization for Collaborative Simultaneous Localization and Mapping","abstract":"This paper introduces a novel incremental distributed back-end algorithm for Collaborative Simultaneous Localization and Mapping (C-SLAM). For real-world deployments, robotic teams require algorithms to compute a consistent state estimate accurately, within online runtime constraints, and with potentially limited communication. Existing centralized, decentralized, and distributed approaches to solving C-SLAM problems struggle to achieve all of these goals. To address this capability gap, we present Incremental Manifold Edge-based Separable ADMM (iMESA) a fully distributed C-SLAM back-end algorithm that can provide a multi-robot team with accurate state estimates in real-time with only sparse pair-wise communication between robots. Extensive evaluation on real and synthetic data demonstrates that iMESA is able to outperform comparable state-of-the-art C-SLAM back-ends.","sentences":["This paper introduces a novel incremental distributed back-end algorithm for Collaborative Simultaneous Localization and Mapping (C-SLAM).","For real-world deployments, robotic teams require algorithms to compute a consistent state estimate accurately, within online runtime constraints, and with potentially limited communication.","Existing centralized, decentralized, and distributed approaches to solving C-SLAM problems struggle to achieve all of these goals.","To address this capability gap, we present Incremental Manifold Edge-based Separable ADMM (iMESA) a fully distributed C-SLAM back-end algorithm that can provide a multi-robot team with accurate state estimates in real-time with only sparse pair-wise communication between robots.","Extensive evaluation on real and synthetic data demonstrates that iMESA is able to outperform comparable state-of-the-art C-SLAM back-ends."],"url":"http://arxiv.org/abs/2406.07371v1","category":"cs.RO"}
{"created":"2024-06-11 15:28:48","title":"Deep Implicit Optimization for Robust and Flexible Image Registration","abstract":"Deep Learning in Image Registration (DLIR) methods have been tremendously successful in image registration due to their speed and ability to incorporate weak label supervision at training time. However, DLIR methods forego many of the benefits of classical optimization-based methods. The functional nature of deep networks do not guarantee that the predicted transformation is a local minima of the registration objective, the representation of the transformation (displacement/velocity field/affine) is fixed, and the networks are not robust to domain shift. Our method aims to bridge this gap between classical and learning methods by incorporating optimization as a layer in a deep network. A deep network is trained to predict multi-scale dense feature images that are registered using a black box iterative optimization solver. This optimal warp is then used to minimize image and label alignment errors. By implicitly differentiating end-to-end through an iterative optimization solver, our learned features are registration and label-aware, and the warp functions are guaranteed to be local minima of the registration objective in the feature space. Our framework shows excellent performance on in-domain datasets, and is agnostic to domain shift such as anisotropy and varying intensity profiles. For the first time, our method allows switching between arbitrary transformation representations (free-form to diffeomorphic) at test time with zero retraining. End-to-end feature learning also facilitates interpretability of features, and out-of-the-box promptability using additional label-fidelity terms at inference.","sentences":["Deep Learning in Image Registration (DLIR) methods have been tremendously successful in image registration due to their speed and ability to incorporate weak label supervision at training time.","However, DLIR methods forego many of the benefits of classical optimization-based methods.","The functional nature of deep networks do not guarantee that the predicted transformation is a local minima of the registration objective, the representation of the transformation (displacement/velocity field/affine) is fixed, and the networks are not robust to domain shift.","Our method aims to bridge this gap between classical and learning methods by incorporating optimization as a layer in a deep network.","A deep network is trained to predict multi-scale dense feature images that are registered using a black box iterative optimization solver.","This optimal warp is then used to minimize image and label alignment errors.","By implicitly differentiating end-to-end through an iterative optimization solver, our learned features are registration and label-aware, and the warp functions are guaranteed to be local minima of the registration objective in the feature space.","Our framework shows excellent performance on in-domain datasets, and is agnostic to domain shift such as anisotropy and varying intensity profiles.","For the first time, our method allows switching between arbitrary transformation representations (free-form to diffeomorphic) at test time with zero retraining.","End-to-end feature learning also facilitates interpretability of features, and out-of-the-box promptability using additional label-fidelity terms at inference."],"url":"http://arxiv.org/abs/2406.07361v1","category":"cs.CV"}
{"created":"2024-06-11 15:26:20","title":"PSMC: Provable and Scalable Algorithms for Motif Conductance Based Graph Clustering","abstract":"Higher-order graph clustering aims to partition the graph using frequently occurring subgraphs. Motif conductance is one of the most promising higher-order graph clustering models due to its strong interpretability. However, existing motif conductance based graph clustering algorithms are mainly limited by a seminal two-stage reweighting computing framework, needing to enumerate all motif instances to obtain an edge-weighted graph for partitioning. However, such a framework has two-fold vital defects: (1) It can only provide a quadratic bound for the motif with three vertices, and whether there is provable clustering quality for other motifs is still an open question. (2) The enumeration procedure of motif instances incurs prohibitively high costs against large motifs or large dense graphs due to combinatorial explosions. Besides, expensive spectral clustering or local graph diffusion on the edge-weighted graph also makes existing methods unable to handle massive graphs with millions of nodes. To overcome these dilemmas, we propose a Provable and Scalable Motif Conductance algorithm PSMC, which has a fixed and motif-independent approximation ratio for any motif. Specifically, PSMC first defines a new vertex metric Motif Resident based on the given motif, which can be computed locally. Then, it iteratively deletes the vertex with the smallest motif resident value very efficiently using novel dynamic update technologies. Finally, it outputs the locally optimal result during the above iterative process. To further boost efficiency, we propose several effective bounds to estimate the motif resident value of each vertex, which can greatly reduce computational costs. Empirical results show that our proposed algorithms achieve 3.2-32 times speedup and improve the quality by at least 12 times than the baselines.","sentences":["Higher-order graph clustering aims to partition the graph using frequently occurring subgraphs.","Motif conductance is one of the most promising higher-order graph clustering models due to its strong interpretability.","However, existing motif conductance based graph clustering algorithms are mainly limited by a seminal two-stage reweighting computing framework, needing to enumerate all motif instances to obtain an edge-weighted graph for partitioning.","However, such a framework has two-fold vital defects: (1) It can only provide a quadratic bound for the motif with three vertices, and whether there is provable clustering quality for other motifs is still an open question.","(2) The enumeration procedure of motif instances incurs prohibitively high costs against large motifs or large dense graphs due to combinatorial explosions.","Besides, expensive spectral clustering or local graph diffusion on the edge-weighted graph also makes existing methods unable to handle massive graphs with millions of nodes.","To overcome these dilemmas, we propose a Provable and Scalable Motif Conductance algorithm PSMC, which has a fixed and motif-independent approximation ratio for any motif.","Specifically, PSMC first defines a new vertex metric Motif Resident based on the given motif, which can be computed locally.","Then, it iteratively deletes the vertex with the smallest motif resident value very efficiently using novel dynamic update technologies.","Finally, it outputs the locally optimal result during the above iterative process.","To further boost efficiency, we propose several effective bounds to estimate the motif resident value of each vertex, which can greatly reduce computational costs.","Empirical results show that our proposed algorithms achieve 3.2-32 times speedup and improve the quality by at least 12 times than the baselines."],"url":"http://arxiv.org/abs/2406.07357v1","category":"cs.CC"}
{"created":"2024-06-11 15:07:40","title":"Analytical Delta-V Approximation for Nonlinear Programming of Multi-target Rendezvous and Flyby Trajectories","abstract":"This study proposes an analytical Delta-V approximation of short-time transfers based on the linear relative motion and a gradient-based nonlinear programming model of multi-target rendezvous and flyby trajectories. In previous studies, the Lambert's solution is commonly used to evaluate Delta-V of short-duration transfers. In this study, to avoid the iteration process for obtaining the Lambert's solution and its gradient, the linear relative motion equations are applied to form an analytical two-point boundary value model for the near-circular orbit rendezvous problems. Although the relative motion equations are usually applicable when the two orbits are close enough, and the position and velocity errors would become more significant as the orbital differences increase, the errors of the velocity increments were proved acceptable in our simulations. Moreover, the analytical formula facilitates the calculation of the gradients to the start epoch and flight time, which are used to establish a nonlinear programming model for sequence optimization that gradient-based algorithms can easily solve. Simulation results demonstrated that the analytical Delta-V approximation requires much less calculation than the Lambert's solution, and the proposed gradient-based nonlinear programming algorithms can obtain similar results in less time than previous methods.","sentences":["This study proposes an analytical Delta-V approximation of short-time transfers based on the linear relative motion and a gradient-based nonlinear programming model of multi-target rendezvous and flyby trajectories.","In previous studies, the Lambert's solution is commonly used to evaluate Delta-V of short-duration transfers.","In this study, to avoid the iteration process for obtaining the Lambert's solution and its gradient, the linear relative motion equations are applied to form an analytical two-point boundary value model for the near-circular orbit rendezvous problems.","Although the relative motion equations are usually applicable when the two orbits are close enough, and the position and velocity errors would become more significant as the orbital differences increase, the errors of the velocity increments were proved acceptable in our simulations.","Moreover, the analytical formula facilitates the calculation of the gradients to the start epoch and flight time, which are used to establish a nonlinear programming model for sequence optimization that gradient-based algorithms can easily solve.","Simulation results demonstrated that the analytical Delta-V approximation requires much less calculation than the Lambert's solution, and the proposed gradient-based nonlinear programming algorithms can obtain similar results in less time than previous methods."],"url":"http://arxiv.org/abs/2406.07341v1","category":"astro-ph.EP"}
{"created":"2024-06-11 15:06:34","title":"Capacity Credit Evaluation of Generalized Energy Storage Considering Endogenous Uncertainty","abstract":"Generalized energy storage (GES), encompassing both physical and virtual energy storage, can provide remarkable but uncertain adequacy flexibility. When assessing GES's contribution to resource adequacy, the literature typically considers exogenous uncertainties (e.g., failures and stochastic response) but overlooks endogenous uncertainties, such as self-scheduling in liberal markets and decision-dependent uncertainty (DDU). In this regard, this paper proposes a novel capacity credit evaluation framework to accurately quantify GES's contribution to resource adequacy, where a sequential coordinated dispatch method is proposed to capture realistic GES operations by coordinating self-scheduling in the day-ahead energy market and real-time adequacy-oriented dispatch in the capacity market. To incorporate DDU of GES (i.e., responsiveness affected by dispatch decisions and prices in capacity market), we present a chance-constrained optimization approach and tractable solution methodologies for real-time dispatch. We propose a practical adequacy assessment method to quantify the impact of DDU on capacity credit by evaluating the consequence of ignoring DDU. Additionally, a novel capacity credit index called equivalent storage capacity substitution is introduced to quantify the equivalent deterministic storage capacity of the uncertain virtual energy storage. Simulations show that the proposed method yields reliable and accurate capacity credit values by accounting for self-scheduling of GES and managing the risk from DDU. Finally, key impact factors of GES's capacity credit are thoroughly discussed, offering valuable insights for the decision-making of capacity market operators.","sentences":["Generalized energy storage (GES), encompassing both physical and virtual energy storage, can provide remarkable but uncertain adequacy flexibility.","When assessing GES's contribution to resource adequacy, the literature typically considers exogenous uncertainties (e.g., failures and stochastic response) but overlooks endogenous uncertainties, such as self-scheduling in liberal markets and decision-dependent uncertainty (DDU).","In this regard, this paper proposes a novel capacity credit evaluation framework to accurately quantify GES's contribution to resource adequacy, where a sequential coordinated dispatch method is proposed to capture realistic GES operations by coordinating self-scheduling in the day-ahead energy market and real-time adequacy-oriented dispatch in the capacity market.","To incorporate DDU of GES (i.e., responsiveness affected by dispatch decisions and prices in capacity market), we present a chance-constrained optimization approach and tractable solution methodologies for real-time dispatch.","We propose a practical adequacy assessment method to quantify the impact of DDU on capacity credit by evaluating the consequence of ignoring DDU.","Additionally, a novel capacity credit index called equivalent storage capacity substitution is introduced to quantify the equivalent deterministic storage capacity of the uncertain virtual energy storage.","Simulations show that the proposed method yields reliable and accurate capacity credit values by accounting for self-scheduling of GES and managing the risk from DDU.","Finally, key impact factors of GES's capacity credit are thoroughly discussed, offering valuable insights for the decision-making of capacity market operators."],"url":"http://arxiv.org/abs/2406.07338v1","category":"eess.SY"}
{"created":"2024-06-11 14:59:24","title":"3D-Properties: Identifying Challenges in DPO and Charting a Path Forward","abstract":"Aligning large language models (LLMs) with human preference has recently gained tremendous attention, with the canonical yet costly RLHF-PPO and the simple and straightforward Direct Preference Optimization (DPO) as two examples. Despite the efficiency, DPO has rarely be used in the state-of-the-art production-level LLMs, implying its potential pathologies. In this work, we revisit DPO with a comprehensive examination of its empirical efficacy and a systematic comparison with RLHF-PPO. We identify the \\textbf{3D}-properties of DPO's learning outcomes: the \\textbf{D}rastic drop in the likelihood of rejected responses, the \\textbf{D}egradation into LLM unlearning, and the \\textbf{D}ispersion effect on unseen responses through experiments with both a carefully designed toy model and practical LLMs on tasks including mathematical problem-solving and instruction following. These findings inherently connect to some observations made by related works and we additionally contribute a plausible theoretical explanation for them. Accordingly, we propose easy regularization methods to mitigate the issues caused by \\textbf{3D}-properties, improving the training stability and final performance of DPO. Our contributions also include an investigation into how the distribution of the paired preference data impacts the effectiveness of DPO. We hope this work could offer research directions to narrow the gap between reward-free preference learning methods and reward-based ones.","sentences":["Aligning large language models (LLMs) with human preference has recently gained tremendous attention, with the canonical yet costly RLHF-PPO and the simple and straightforward Direct Preference Optimization (DPO) as two examples.","Despite the efficiency, DPO has rarely be used in the state-of-the-art production-level LLMs, implying its potential pathologies.","In this work, we revisit DPO with a comprehensive examination of its empirical efficacy and a systematic comparison with RLHF-PPO.","We identify the \\textbf{3D}-properties of DPO's learning outcomes: the \\textbf{D}rastic drop in the likelihood of rejected responses, the \\textbf{D}egradation into LLM unlearning, and the \\textbf{D}ispersion effect on unseen responses through experiments with both a carefully designed toy model and practical LLMs on tasks including mathematical problem-solving and instruction following.","These findings inherently connect to some observations made by related works and we additionally contribute a plausible theoretical explanation for them.","Accordingly, we propose easy regularization methods to mitigate the issues caused by \\textbf{3D}-properties, improving the training stability and final performance of DPO.","Our contributions also include an investigation into how the distribution of the paired preference data impacts the effectiveness of DPO.","We hope this work could offer research directions to narrow the gap between reward-free preference learning methods and reward-based ones."],"url":"http://arxiv.org/abs/2406.07327v1","category":"cs.AI"}
{"created":"2024-06-11 14:33:31","title":"A directional total variation minimization algorithm for isotropic resolution in digital breast tomosynthesis","abstract":"An optimization-based image reconstruction algorithm is developed for contrast enhanced digital breast tomosynthesis (DBT) using dual-energy scanning. The algorithm minimizes directional total variation (TV) with a data discrepancy and non-negativity constraints. Iodinated contrast agent (ICA) imaging is performed by reconstructing images from dual-energy DBT data followed by weighted subtraction. Physical DBT data is acquired with a Siemens Mammomat scanner of a structured breast phantom with ICA inserts. Results are shown for both directional TV minimization and filtered back-projection for reference. It is seen that directional TV is able to substantially reduce depth blur for the ICA objects.","sentences":["An optimization-based image reconstruction algorithm is developed for contrast enhanced digital breast tomosynthesis (DBT) using dual-energy scanning.","The algorithm minimizes directional total variation (TV) with a data discrepancy and non-negativity constraints.","Iodinated contrast agent (ICA) imaging is performed by reconstructing images from dual-energy DBT data followed by weighted subtraction.","Physical DBT data is acquired with a Siemens Mammomat scanner of a structured breast phantom with ICA inserts.","Results are shown for both directional TV minimization and filtered back-projection for reference.","It is seen that directional TV is able to substantially reduce depth blur for the ICA objects."],"url":"http://arxiv.org/abs/2406.07306v1","category":"physics.med-ph"}
{"created":"2024-06-11 14:29:58","title":"Optimal Scheduling of Battery Storage Systems in the Swedish Multi-FCR Market Incorporating Battery Degradation and Technical Requirements","abstract":"This paper develops a novel mixed-integer linear programming (MILP) model for optimal participation of battery energy storage systems (BESSs) in the Swedish frequency containment reserve (FCR) markets. The developed model aims to maximize the battery owner's potential profit by considering battery degradation and participation in multiple FCR markets, i.e., FCR in normal operation (FCR-N), and FCR in disturbances (FCR-D) for up- and down-regulations. Accordingly, a precise formulation of a detailed battery degradation model and adherence to the technical requirements of the Swedish FCR markets are incorporated into the developed model. To achieve more practical results, simulations are conducted based on one minute time step realistic data for the whole year 2022. The results show a potential profit of 708 thousand Euros for a 1MW/1MWh BESS by participating in multi-FCR market. Analyzing the impact of considering degradation in the optimization problem has shown that the annual battery aging cost could decrease by 5%-29% without a significant effect on profit. The proposed model can be practically used by flexibility asset owners to achieve profitable and sustainable operation strategies that reduce battery degradation.","sentences":["This paper develops a novel mixed-integer linear programming (MILP) model for optimal participation of battery energy storage systems (BESSs) in the Swedish frequency containment reserve (FCR) markets.","The developed model aims to maximize the battery owner's potential profit by considering battery degradation and participation in multiple FCR markets, i.e., FCR in normal operation (FCR-N), and FCR in disturbances (FCR-D) for up- and down-regulations.","Accordingly, a precise formulation of a detailed battery degradation model and adherence to the technical requirements of the Swedish FCR markets are incorporated into the developed model.","To achieve more practical results, simulations are conducted based on one minute time step realistic data for the whole year 2022.","The results show a potential profit of 708 thousand Euros for a 1MW/1MWh BESS by participating in multi-FCR market.","Analyzing the impact of considering degradation in the optimization problem has shown that the annual battery aging cost could decrease by 5%-29% without a significant effect on profit.","The proposed model can be practically used by flexibility asset owners to achieve profitable and sustainable operation strategies that reduce battery degradation."],"url":"http://arxiv.org/abs/2406.07301v1","category":"eess.SY"}
{"created":"2024-06-11 14:27:40","title":"Enhanced In-Flight Connectivity for Urban Air Mobility via LEO Satellite Networks","abstract":"Urban Air Mobility (UAM) is the envisioned future of inter-city aerial transportation. This paper presents a novel, in-flight connectivity link allocation method for UAM, which dynamically switches between terrestrial cellular and Low Earth Orbit (LEO) satellite networks based on real-time conditions. Our approach prefers cellular networks for cost efficiency, switching to LEO satellites under poor cellular conditions to ensure continuous UAM connectivity. By integrating real-time metrics like signal strength, network congestion, and flight trajectory into the selection process, our algorithm effectively balances cost, minimum data rate requirements, and continuity of communication. Numerical results validate minimization of data-loss while ensuring an optimal selection from the set of available above-threshold data rates at every time sample. Furthermore, insights derived from our study emphasize the importance of hybrid connectivity solutions in ensuring seamless, uninterrupted communication for future urban aerial vehicles.","sentences":["Urban Air Mobility (UAM) is the envisioned future of inter-city aerial transportation.","This paper presents a novel, in-flight connectivity link allocation method for UAM, which dynamically switches between terrestrial cellular and Low Earth Orbit (LEO) satellite networks based on real-time conditions.","Our approach prefers cellular networks for cost efficiency, switching to LEO satellites under poor cellular conditions to ensure continuous UAM connectivity.","By integrating real-time metrics like signal strength, network congestion, and flight trajectory into the selection process, our algorithm effectively balances cost, minimum data rate requirements, and continuity of communication.","Numerical results validate minimization of data-loss while ensuring an optimal selection from the set of available above-threshold data rates at every time sample.","Furthermore, insights derived from our study emphasize the importance of hybrid connectivity solutions in ensuring seamless, uninterrupted communication for future urban aerial vehicles."],"url":"http://arxiv.org/abs/2406.07298v1","category":"cs.ET"}
{"created":"2024-06-11 14:24:45","title":"Instruct Large Language Models to Drive like Humans","abstract":"Motion planning in complex scenarios is the core challenge in autonomous driving. Conventional methods apply predefined rules or learn from driving data to plan the future trajectory. Recent methods seek the knowledge preserved in large language models (LLMs) and apply them in the driving scenarios. Despite the promising results, it is still unclear whether the LLM learns the underlying human logic to drive. In this paper, we propose an InstructDriver method to transform LLM into a motion planner with explicit instruction tuning to align its behavior with humans. We derive driving instruction data based on human logic (e.g., do not cause collisions) and traffic rules (e.g., proceed only when green lights). We then employ an interpretable InstructChain module to further reason the final planning reflecting the instructions. Our InstructDriver allows the injection of human rules and learning from driving data, enabling both interpretability and data scalability. Different from existing methods that experimented on closed-loop or simulated settings, we adopt the real-world closed-loop motion planning nuPlan benchmark for better evaluation. InstructDriver demonstrates the effectiveness of the LLM planner in a real-world closed-loop setting. Our code is publicly available at https://github.com/bonbon-rj/InstructDriver.","sentences":["Motion planning in complex scenarios is the core challenge in autonomous driving.","Conventional methods apply predefined rules or learn from driving data to plan the future trajectory.","Recent methods seek the knowledge preserved in large language models (LLMs) and apply them in the driving scenarios.","Despite the promising results, it is still unclear whether the LLM learns the underlying human logic to drive.","In this paper, we propose an InstructDriver method to transform LLM into a motion planner with explicit instruction tuning to align its behavior with humans.","We derive driving instruction data based on human logic (e.g., do not cause collisions) and traffic rules (e.g., proceed only when green lights).","We then employ an interpretable InstructChain module to further reason the final planning reflecting the instructions.","Our InstructDriver allows the injection of human rules and learning from driving data, enabling both interpretability and data scalability.","Different from existing methods that experimented on closed-loop or simulated settings, we adopt the real-world closed-loop motion planning nuPlan benchmark for better evaluation.","InstructDriver demonstrates the effectiveness of the LLM planner in a real-world closed-loop setting.","Our code is publicly available at https://github.com/bonbon-rj/InstructDriver."],"url":"http://arxiv.org/abs/2406.07296v1","category":"cs.RO"}
