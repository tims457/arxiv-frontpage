{"created":"2024-04-22 17:59:36","title":"CrossScore: Towards Multi-View Image Evaluation and Scoring","abstract":"We introduce a novel cross-reference image quality assessment method that effectively fills the gap in the image assessment landscape, complementing the array of established evaluation schemes -- ranging from full-reference metrics like SSIM, no-reference metrics such as NIQE, to general-reference metrics including FID, and Multi-modal-reference metrics, e.g., CLIPScore. Utilising a neural network with the cross-attention mechanism and a unique data collection pipeline from NVS optimisation, our method enables accurate image quality assessment without requiring ground truth references. By comparing a query image against multiple views of the same scene, our method addresses the limitations of existing metrics in novel view synthesis (NVS) and similar tasks where direct reference images are unavailable. Experimental results show that our method is closely correlated to the full-reference metric SSIM, while not requiring ground truth references.","sentences":["We introduce a novel cross-reference image quality assessment method that effectively fills the gap in the image assessment landscape, complementing the array of established evaluation schemes -- ranging from full-reference metrics like SSIM, no-reference metrics such as NIQE, to general-reference metrics including FID, and Multi-modal-reference metrics, e.g., CLIPScore.","Utilising a neural network with the cross-attention mechanism and a unique data collection pipeline from NVS optimisation, our method enables accurate image quality assessment without requiring ground truth references.","By comparing a query image against multiple views of the same scene, our method addresses the limitations of existing metrics in novel view synthesis (NVS) and similar tasks where direct reference images are unavailable.","Experimental results show that our method is closely correlated to the full-reference metric SSIM, while not requiring ground truth references."],"url":"http://arxiv.org/abs/2404.14409v1","category":"cs.CV"}
{"created":"2024-04-22 17:59:29","title":"SpaceByte: Towards Deleting Tokenization from Large Language Modeling","abstract":"Tokenization is widely used in large language models because it significantly improves performance. However, tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character-level modeling performance, and increased modeling complexity. To address these disadvantages without sacrificing performance, we propose SpaceByte, a novel byte-level decoder architecture that closes the performance gap between byte-level and subword autoregressive language modeling. SpaceByte consists of a byte-level Transformer model, but with extra larger transformer blocks inserted in the middle of the layers. We find that performance is significantly improved by applying these larger blocks only after certain bytes, such as space characters, which typically denote word boundaries. Our experiments show that for a fixed training and inference compute budget, SpaceByte outperforms other byte-level architectures and roughly matches the performance of tokenized Transformer architectures.","sentences":["Tokenization is widely used in large language models because it significantly improves performance.","However, tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character-level modeling performance, and increased modeling complexity.","To address these disadvantages without sacrificing performance, we propose SpaceByte, a novel byte-level decoder architecture that closes the performance gap between byte-level and subword autoregressive language modeling.","SpaceByte consists of a byte-level Transformer model, but with extra larger transformer blocks inserted in the middle of the layers.","We find that performance is significantly improved by applying these larger blocks only after certain bytes, such as space characters, which typically denote word boundaries.","Our experiments show that for a fixed training and inference compute budget, SpaceByte outperforms other byte-level architectures and roughly matches the performance of tokenized Transformer architectures."],"url":"http://arxiv.org/abs/2404.14408v1","category":"cs.CL"}
{"created":"2024-04-22 17:55:56","title":"PARAMANU-GANITA: Language Model with Mathematical Capabilities","abstract":"In this paper, we present Paramanu-Ganita, a 208 million parameter novel Auto Regressive (AR) decoder based language model on mathematics. The model is pretrained from scratch at context size of 4096 on our curated mixed mathematical corpus. We evaluate our model on both perplexity metric and GSM8k mathematical benchmark. Paramanu-Ganita despite being 35 times smaller than 7B LLMs, outperformed generalist LLMs such as LLaMa-1 7B by 28.4% points, LLaMa-2 7B by 27.6% points, Falcon 7B by 32.6% points, PaLM 8B by 35.3% points, and math specialised LLMs such as Minerva 8B by 23.2% points, and LLEMMA-7B by 3.0% points in GSM8k test accuracy metric respectively. Paramanu-Ganita also outperformed giant LLMs like PaLM 62B by 6.4% points, Falcon 40B by 19.8% points, LLaMa-1 33B by 3.8% points and Vicuna 13B by 11.8% points respectively. The large significant margin improvement in performance of our math model over the existing LLMs signifies that reasoning capabilities of language model are just not restricted to LLMs with humongous number of parameters. Paramanu-Ganita took 146 hours of A100 training whereas math specialised LLM, LLEMMA 7B, was trained for 23,000 A100 hours of training equivalent. Thus, our approach of pretraining powerful domain specialised language models from scratch for domain adaptation is much more cost-effective than performing continual training of LLMs for domain adaptation. Hence, we conclude that for strong mathematical reasoning abilities of language model, we do not need giant LLMs and immense computing power to our end. In the end, we want to point out that we have only trained Paramanu-Ganita only on a part of our entire mathematical corpus and yet to explore the full potential of our model.","sentences":["In this paper, we present Paramanu-Ganita, a 208 million parameter novel Auto Regressive (AR) decoder based language model on mathematics.","The model is pretrained from scratch at context size of 4096 on our curated mixed mathematical corpus.","We evaluate our model on both perplexity metric and GSM8k mathematical benchmark.","Paramanu-Ganita despite being 35 times smaller than 7B LLMs, outperformed generalist LLMs such as LLaMa-1 7B by 28.4% points, LLaMa-2 7B by 27.6% points, Falcon 7B by 32.6% points, PaLM 8B by 35.3% points, and math specialised LLMs such as Minerva 8B by 23.2% points, and LLEMMA-7B by 3.0% points in GSM8k test accuracy metric respectively.","Paramanu-Ganita also outperformed giant LLMs like PaLM 62B by 6.4% points, Falcon 40B by 19.8% points, LLaMa-1","33B by 3.8% points and Vicuna 13B by 11.8% points respectively.","The large significant margin improvement in performance of our math model over the existing LLMs signifies that reasoning capabilities of language model are just not restricted to LLMs with humongous number of parameters.","Paramanu-Ganita took 146 hours of A100 training whereas math specialised LLM, LLEMMA 7B, was trained for 23,000 A100 hours of training equivalent.","Thus, our approach of pretraining powerful domain specialised language models from scratch for domain adaptation is much more cost-effective than performing continual training of LLMs for domain adaptation.","Hence, we conclude that for strong mathematical reasoning abilities of language model, we do not need giant LLMs and immense computing power to our end.","In the end, we want to point out that we have only trained Paramanu-Ganita only on a part of our entire mathematical corpus and yet to explore the full potential of our model."],"url":"http://arxiv.org/abs/2404.14395v1","category":"cs.CL"}
{"created":"2024-04-22 17:55:11","title":"A Multimodal Automated Interpretability Agent","abstract":"This paper describes MAIA, a Multimodal Automated Interpretability Agent. MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery. It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior. These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results. Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior. We evaluate applications of MAIA to computer vision models. We first characterize MAIA's ability to describe (neuron-level) features in learned representations of images. Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters. We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified.","sentences":["This paper describes MAIA, a Multimodal Automated Interpretability Agent.","MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery.","It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior.","These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results.","Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior.","We evaluate applications of MAIA to computer vision models.","We first characterize MAIA's ability to describe (neuron-level) features in learned representations of images.","Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters.","We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified."],"url":"http://arxiv.org/abs/2404.14394v1","category":"cs.AI"}
{"created":"2024-04-22 17:43:23","title":"A Survey on Self-Evolution of Large Language Models","abstract":"Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work, we present a comprehensive survey of self-evolution approaches in LLMs. We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation. Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module. Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs.","sentences":["Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications.","However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase.","To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing.","This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence.","In this work, we present a comprehensive survey of self-evolution approaches in LLMs.","We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation.","Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module.","Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs."],"url":"http://arxiv.org/abs/2404.14387v1","category":"cs.CL"}
{"created":"2024-04-22 17:28:52","title":"The Life and Legacy of Bui Tuong Phong","abstract":"We examine the life and legacy of pioneering Vietnamese American computer scientist B\\`ui Tuong Phong, whose shading and lighting models turned 50 last year. We trace the trajectory of his life through Vietnam, France, and the United States, and its intersections with global conflicts. Crucially, we present evidence that his name has been cited incorrectly over the last five decades. His family name appears to be B\\`ui, not Phong. By presenting these facts at SIGGRAPH, we hope to collect more information about his life, and ensure that his name is remembered correctly in the future.","sentences":["We examine the life and legacy of pioneering Vietnamese American computer scientist B\\`ui Tuong Phong, whose shading and lighting models turned 50 last year.","We trace the trajectory of his life through Vietnam, France, and the United States, and its intersections with global conflicts.","Crucially, we present evidence that his name has been cited incorrectly over the last five decades.","His family name appears to be B\\`ui, not Phong.","By presenting these facts at SIGGRAPH, we hope to collect more information about his life, and ensure that his name is remembered correctly in the future."],"url":"http://arxiv.org/abs/2404.14376v1","category":"cs.GR"}
{"created":"2024-04-22 17:24:08","title":"On the incidence rate of RR Lyrae stars with non-radial modes","abstract":"Over the recent years, additional low-amplitude non-radial modes were detected in many of the first-overtone RR Lyrae stars. These non-radial modes form a characteristic period ratio with the dominant first-overtone mode of around 0.61. The incidence rate of this phenomenon changes from population to population. It is also strongly dependent on the quality of the analyzed data. Current models explaining these additional signals involve non-radial modes of degrees 8 and 9. Using synthetic horizontal branch populations, we investigate the incidence rate of first-overtone RR Lyrae stars with non-radial modes depending on the population properties, i.e., ages and metallicities. We compare our results with the observed results for globular clusters and the numerous collection of field first-overtone RR Lyrae stars to test the predictions of the models. We used synthetic horizontal branches combined with pulsation models to predict how the incidence rate would depend on the age and metallicity of the population. To test whether the results based on synthetic horizontal branches are realistic, we compared them to incidence rates observed by TESS in first-overtone field RR Lyrae stars, using photometric metallicity values from a newly established calibration for TESS. The analysis of synthetic horizontal branches showed that the incidence rate decreases with decreasing metallicity. We inferred photometric metallicity for RR Lyrae stars observed by TESS and showed that the theoretical predictions are in agreement with the observations. Using the same method, we also conclude that the metallicity distribution of RR Lyrae stars showing an additional mode with a period-ratio around $0.68$ appears to be different from that of both all first-overtone stars and those showing additional non-radial modes.","sentences":["Over the recent years, additional low-amplitude non-radial modes were detected in many of the first-overtone RR Lyrae stars.","These non-radial modes form a characteristic period ratio with the dominant first-overtone mode of around 0.61.","The incidence rate of this phenomenon changes from population to population.","It is also strongly dependent on the quality of the analyzed data.","Current models explaining these additional signals involve non-radial modes of degrees 8 and 9.","Using synthetic horizontal branch populations, we investigate the incidence rate of first-overtone RR Lyrae stars with non-radial modes depending on the population properties, i.e., ages and metallicities.","We compare our results with the observed results for globular clusters and the numerous collection of field first-overtone RR Lyrae stars to test the predictions of the models.","We used synthetic horizontal branches combined with pulsation models to predict how the incidence rate would depend on the age and metallicity of the population.","To test whether the results based on synthetic horizontal branches are realistic, we compared them to incidence rates observed by TESS in first-overtone field RR Lyrae stars, using photometric metallicity values from a newly established calibration for TESS.","The analysis of synthetic horizontal branches showed that the incidence rate decreases with decreasing metallicity.","We inferred photometric metallicity for RR Lyrae stars observed by TESS and showed that the theoretical predictions are in agreement with the observations.","Using the same method, we also conclude that the metallicity distribution of RR Lyrae stars showing an additional mode with a period-ratio around $0.68$ appears to be different from that of both all first-overtone stars and those showing additional non-radial modes."],"url":"http://arxiv.org/abs/2404.14373v1","category":"astro-ph.SR"}
{"created":"2024-04-22 17:22:31","title":"Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph","abstract":"Model scaling is becoming the default choice for many language tasks due to the success of large language models (LLMs). However, it can fall short in specific scenarios where simple customized methods excel. In this paper, we delve into the patent approval pre-diction task and unveil that simple domain-specific graph methods outperform enlarging the model, using the intrinsic dependencies within the patent data. Specifically, we first extend the embedding-based state-of-the-art (SOTA) by scaling up its backbone model with various sizes of open-source LLMs, then explore prompt-based methods to harness proprietary LLMs' potential, but find the best results close to random guessing, underlining the ineffectiveness of model scaling-up. Hence, we propose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulous patent data analyses, capturing the inherent dependencies across segments of the patent text. As it is model-agnostic, we apply cost-effective graph models to our FLAN Graph to obtain representations for approval prediction. Extensive experiments and detailed analyses prove that incorporating FLAN Graph via various graph models consistently outperforms all LLM baselines significantly. We hope that our observations and analyses in this paper can bring more attention to this challenging task and prompt further research into the limitations of LLMs. Our source code and dataset can be obtained from http://github.com/ShangDataLab/FLAN-Graph.","sentences":["Model scaling is becoming the default choice for many language tasks due to the success of large language models (LLMs).","However, it can fall short in specific scenarios where simple customized methods excel.","In this paper, we delve into the patent approval pre-diction task and unveil that simple domain-specific graph methods outperform enlarging the model, using the intrinsic dependencies within the patent data.","Specifically, we first extend the embedding-based state-of-the-art (SOTA) by scaling up its backbone model with various sizes of open-source LLMs, then explore prompt-based methods to harness proprietary LLMs' potential, but find the best results close to random guessing, underlining the ineffectiveness of model scaling-up.","Hence, we propose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulous patent data analyses, capturing the inherent dependencies across segments of the patent text.","As it is model-agnostic, we apply cost-effective graph models to our FLAN Graph to obtain representations for approval prediction.","Extensive experiments and detailed analyses prove that incorporating FLAN Graph via various graph models consistently outperforms all LLM baselines significantly.","We hope that our observations and analyses in this paper can bring more attention to this challenging task and prompt further research into the limitations of LLMs.","Our source code and dataset can be obtained from http://github.com/ShangDataLab/FLAN-Graph."],"url":"http://arxiv.org/abs/2404.14372v1","category":"cs.CL"}
{"created":"2024-04-22 17:21:24","title":"Assessing GPT-4-Vision's Capabilities in UML-Based Code Generation","abstract":"The emergence of advanced neural networks has opened up new ways in automated code generation from conceptual models, promising to enhance software development processes. This paper presents a preliminary evaluation of GPT-4-Vision, a state-of-the-art deep learning model, and its capabilities in transforming Unified Modeling Language (UML) class diagrams into fully operating Java class files. In our study, we used exported images of 18 class diagrams comprising 10 single-class and 8 multi-class diagrams. We used 3 different prompts for each input, and we manually evaluated the results. We created a scoring system in which we scored the occurrence of elements found in the diagram within the source code. On average, the model was able to generate source code for 88% of the elements shown in the diagrams. Our results indicate that GPT-4-Vision exhibits proficiency in handling single-class UML diagrams, successfully transforming them into syntactically correct class files. However, for multi-class UML diagrams, the model's performance is weaker compared to single-class diagrams. In summary, further investigations are necessary to exploit the model's potential completely.","sentences":["The emergence of advanced neural networks has opened up new ways in automated code generation from conceptual models, promising to enhance software development processes.","This paper presents a preliminary evaluation of GPT-4-Vision, a state-of-the-art deep learning model, and its capabilities in transforming Unified Modeling Language (UML) class diagrams into fully operating Java class files.","In our study, we used exported images of 18 class diagrams comprising 10 single-class and 8 multi-class diagrams.","We used 3 different prompts for each input, and we manually evaluated the results.","We created a scoring system in which we scored the occurrence of elements found in the diagram within the source code.","On average, the model was able to generate source code for 88% of the elements shown in the diagrams.","Our results indicate that GPT-4-Vision exhibits proficiency in handling single-class UML diagrams, successfully transforming them into syntactically correct class files.","However, for multi-class UML diagrams, the model's performance is weaker compared to single-class diagrams.","In summary, further investigations are necessary to exploit the model's potential completely."],"url":"http://arxiv.org/abs/2404.14370v1","category":"cs.SE"}
{"created":"2024-04-22 17:20:38","title":"Graphic Design with Large Multimodal Model","abstract":"In the field of graphic design, automating the integration of design elements into a cohesive multi-layered artwork not only boosts productivity but also paves the way for the democratization of graphic design. One existing practice is Graphic Layout Generation (GLG), which aims to layout sequential design elements. It has been constrained by the necessity for a predefined correct sequence of layers, thus limiting creative potential and increasing user workload. In this paper, we present Hierarchical Layout Generation (HLG) as a more flexible and pragmatic setup, which creates graphic composition from unordered sets of design elements. To tackle the HLG task, we introduce Graphist, the first layout generation model based on large multimodal models. Graphist efficiently reframes the HLG as a sequence generation problem, utilizing RGB-A images as input, outputs a JSON draft protocol, indicating the coordinates, size, and order of each element. We develop new evaluation metrics for HLG. Graphist outperforms prior arts and establishes a strong baseline for this field. Project homepage: https://github.com/graphic-design-ai/graphist","sentences":["In the field of graphic design, automating the integration of design elements into a cohesive multi-layered artwork not only boosts productivity but also paves the way for the democratization of graphic design.","One existing practice is Graphic Layout Generation (GLG), which aims to layout sequential design elements.","It has been constrained by the necessity for a predefined correct sequence of layers, thus limiting creative potential and increasing user workload.","In this paper, we present Hierarchical Layout Generation (HLG) as a more flexible and pragmatic setup, which creates graphic composition from unordered sets of design elements.","To tackle the HLG task, we introduce Graphist, the first layout generation model based on large multimodal models.","Graphist efficiently reframes the HLG as a sequence generation problem, utilizing RGB-A images as input, outputs a JSON draft protocol, indicating the coordinates, size, and order of each element.","We develop new evaluation metrics for HLG.","Graphist outperforms prior arts and establishes a strong baseline for this field.","Project homepage: https://github.com/graphic-design-ai/graphist"],"url":"http://arxiv.org/abs/2404.14368v1","category":"cs.CV"}
{"created":"2024-04-22 17:20:18","title":"Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data","abstract":"Learning from preference labels plays a crucial role in fine-tuning large language models. There are several distinct approaches for preference fine-tuning, including supervised learning, on-policy reinforcement learning (RL), and contrastive learning. Different methods come with different implementation tradeoffs and performance differences, and existing empirical findings present different conclusions, for instance, some results show that online RL is quite important to attain good fine-tuning results, while others find (offline) contrastive or even purely supervised methods sufficient. This raises a natural question: what kind of approaches are important for fine-tuning with preference data and why? In this paper, we answer this question by performing a rigorous analysis of a number of fine-tuning techniques on didactic and full-scale LLM problems. Our main finding is that, in general, approaches that use on-policy sampling or attempt to push down the likelihood on certain responses (i.e., employ a \"negative gradient\") outperform offline and maximum likelihood objectives. We conceptualize our insights and unify methods that use on-policy sampling or negative gradient under a notion of mode-seeking objectives for categorical distributions. Mode-seeking objectives are able to alter probability mass on specific bins of a categorical distribution at a fast rate compared to maximum likelihood, allowing them to relocate masses across bins more effectively. Our analysis prescribes actionable insights for preference fine-tuning of LLMs and informs how data should be collected for maximal improvement.","sentences":["Learning from preference labels plays a crucial role in fine-tuning large language models.","There are several distinct approaches for preference fine-tuning, including supervised learning, on-policy reinforcement learning (RL), and contrastive learning.","Different methods come with different implementation tradeoffs and performance differences, and existing empirical findings present different conclusions, for instance, some results show that online RL is quite important to attain good fine-tuning results, while others find (offline) contrastive or even purely supervised methods sufficient.","This raises a natural question: what kind of approaches are important for fine-tuning with preference data and why?","In this paper, we answer this question by performing a rigorous analysis of a number of fine-tuning techniques on didactic and full-scale LLM problems.","Our main finding is that, in general, approaches that use on-policy sampling or attempt to push down the likelihood on certain responses (i.e., employ a \"negative gradient\") outperform offline and maximum likelihood objectives.","We conceptualize our insights and unify methods that use on-policy sampling or negative gradient under a notion of mode-seeking objectives for categorical distributions.","Mode-seeking objectives are able to alter probability mass on specific bins of a categorical distribution at a fast rate compared to maximum likelihood, allowing them to relocate masses across bins more effectively.","Our analysis prescribes actionable insights for preference fine-tuning of LLMs and informs how data should be collected for maximal improvement."],"url":"http://arxiv.org/abs/2404.14367v2","category":"cs.LG"}
{"created":"2024-04-22 17:07:25","title":"Calc-CMU at SemEval-2024 Task 7: Pre-Calc -- Learning to Use the Calculator Improves Numeracy in Language Models","abstract":"Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models. While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders. In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively. We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding. Our code and data are available at https://github.com/calc-cmu/pre-calc.","sentences":["Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models.","While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders.","In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively.","We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding.","Our code and data are available at https://github.com/calc-cmu/pre-calc."],"url":"http://arxiv.org/abs/2404.14355v1","category":"cs.CL"}
{"created":"2024-04-22 17:02:33","title":"Scene Coordinate Reconstruction: Posing of Image Collections via Incremental Learning of a Relocalizer","abstract":"We address the task of estimating camera parameters from a set of images depicting a scene. Popular feature-based structure-from-motion (SfM) tools solve this task by incremental reconstruction: they repeat triangulation of sparse 3D points and registration of more camera views to the sparse point cloud. We re-interpret incremental structure-from-motion as an iterated application and refinement of a visual relocalizer, that is, of a method that registers new views to the current state of the reconstruction. This perspective allows us to investigate alternative visual relocalizers that are not rooted in local feature matching. We show that scene coordinate regression, a learning-based relocalization approach, allows us to build implicit, neural scene representations from unposed images. Different from other learning-based reconstruction methods, we do not require pose priors nor sequential inputs, and we optimize efficiently over thousands of images. Our method, ACE0 (ACE Zero), estimates camera poses to an accuracy comparable to feature-based SfM, as demonstrated by novel view synthesis. Project page: https://nianticlabs.github.io/acezero/","sentences":["We address the task of estimating camera parameters from a set of images depicting a scene.","Popular feature-based structure-from-motion (SfM) tools solve this task by incremental reconstruction: they repeat triangulation of sparse 3D points and registration of more camera views to the sparse point cloud.","We re-interpret incremental structure-from-motion as an iterated application and refinement of a visual relocalizer, that is, of a method that registers new views to the current state of the reconstruction.","This perspective allows us to investigate alternative visual relocalizers that are not rooted in local feature matching.","We show that scene coordinate regression, a learning-based relocalization approach, allows us to build implicit, neural scene representations from unposed images.","Different from other learning-based reconstruction methods, we do not require pose priors nor sequential inputs, and we optimize efficiently over thousands of images.","Our method, ACE0 (ACE Zero), estimates camera poses to an accuracy comparable to feature-based SfM, as demonstrated by novel view synthesis.","Project page: https://nianticlabs.github.io/acezero/"],"url":"http://arxiv.org/abs/2404.14351v1","category":"cs.CV"}
{"created":"2024-04-22 17:00:57","title":"Automatic Discovery of Visual Circuits","abstract":"To date, most discoveries of network subcomponents that implement human-interpretable computations in deep vision models have involved close study of single units and large amounts of human labor. We explore scalable methods for extracting the subgraph of a vision model's computational graph that underlies recognition of a specific visual concept. We introduce a new method for identifying these subgraphs: specifying a visual concept using a few examples, and then tracing the interdependence of neuron activations across layers, or their functional connectivity. We find that our approach extracts circuits that causally affect model output, and that editing these circuits can defend large pretrained models from adversarial attacks.","sentences":["To date, most discoveries of network subcomponents that implement human-interpretable computations in deep vision models have involved close study of single units and large amounts of human labor.","We explore scalable methods for extracting the subgraph of a vision model's computational graph that underlies recognition of a specific visual concept.","We introduce a new method for identifying these subgraphs: specifying a visual concept using a few examples, and then tracing the interdependence of neuron activations across layers, or their functional connectivity.","We find that our approach extracts circuits that causally affect model output, and that editing these circuits can defend large pretrained models from adversarial attacks."],"url":"http://arxiv.org/abs/2404.14349v1","category":"cs.CV"}
{"created":"2024-04-22 17:00:31","title":"Machine Learning in Viscoelastic Fluids via Energy-Based Kernel Embedding","abstract":"The ability to measure differences in collected data is of fundamental importance for quantitative science and machine learning, motivating the establishment of metrics grounded in physical principles. In this study, we focus on the development of such metrics for viscoelastic fluid flows governed by a large class of linear and nonlinear stress models. To do this, we introduce a kernel function corresponding to a given viscoelastic stress model that implicitly embeds flowfield snapshots into a Reproducing Kernel Hilbert Space (RKHS) whose squared norm equals the total mechanical energy. Working implicitly with lifted representations in the RKHS via the kernel function provides natural and unambiguous metrics for distances and angles between flowfields without the need for hyperparameter tuning. Additionally, we present a solution to the preimage problem for our kernels, enabling accurate reconstruction of flowfields from their RKHS representations. Through numerical experiments on an unsteady viscoelastic lid-driven cavity flow, we demonstrate the utility of our kernels for extracting energetically-dominant coherent structures in viscoelastic flows across a range of Reynolds and Weissenberg numbers. Specifically, the features extracted by Kernel Principal Component Analysis (KPCA) of flowfield snapshots using our kernel functions yield reconstructions with superior accuracy in terms of mechanical energy compared to conventional methods such as ordinary Principal Component Analysis (PCA) with na\\\"ively-defined state vectors or KPCA with ad-hoc choices of kernel functions. Our findings underscore the importance of principled choices of metrics in both scientific and machine learning investigations of complex fluid systems.","sentences":["The ability to measure differences in collected data is of fundamental importance for quantitative science and machine learning, motivating the establishment of metrics grounded in physical principles.","In this study, we focus on the development of such metrics for viscoelastic fluid flows governed by a large class of linear and nonlinear stress models.","To do this, we introduce a kernel function corresponding to a given viscoelastic stress model that implicitly embeds flowfield snapshots into a Reproducing Kernel Hilbert Space (RKHS) whose squared norm equals the total mechanical energy.","Working implicitly with lifted representations in the RKHS via the kernel function provides natural and unambiguous metrics for distances and angles between flowfields without the need for hyperparameter tuning.","Additionally, we present a solution to the preimage problem for our kernels, enabling accurate reconstruction of flowfields from their RKHS representations.","Through numerical experiments on an unsteady viscoelastic lid-driven cavity flow, we demonstrate the utility of our kernels for extracting energetically-dominant coherent structures in viscoelastic flows across a range of Reynolds and Weissenberg numbers.","Specifically, the features extracted by Kernel Principal Component Analysis (KPCA) of flowfield snapshots using our kernel functions yield reconstructions with superior accuracy in terms of mechanical energy compared to conventional methods such as ordinary Principal Component Analysis (PCA) with na\\\"ively-defined state vectors or KPCA with ad-hoc choices of kernel functions.","Our findings underscore the importance of principled choices of metrics in both scientific and machine learning investigations of complex fluid systems."],"url":"http://arxiv.org/abs/2404.14347v1","category":"physics.flu-dyn"}
{"created":"2024-04-22 16:47:10","title":"Full Event Particle-Level Unfolding with Variable-Length Latent Variational Diffusion","abstract":"The measurements performed by particle physics experiments must account for the imperfect response of the detectors used to observe the interactions. One approach, unfolding, statistically adjusts the experimental data for detector effects. Recently, generative machine learning models have shown promise for performing unbinned unfolding in a high number of dimensions. However, all current generative approaches are limited to unfolding a fixed set of observables, making them unable to perform full-event unfolding in the variable dimensional environment of collider data. A novel modification to the variational latent diffusion model (VLD) approach to generative unfolding is presented, which allows for unfolding of high- and variable-dimensional feature spaces. The performance of this method is evaluated in the context of semi-leptonic top quark pair production at the Large Hadron Collider.","sentences":["The measurements performed by particle physics experiments must account for the imperfect response of the detectors used to observe the interactions.","One approach, unfolding, statistically adjusts the experimental data for detector effects.","Recently, generative machine learning models have shown promise for performing unbinned unfolding in a high number of dimensions.","However, all current generative approaches are limited to unfolding a fixed set of observables, making them unable to perform full-event unfolding in the variable dimensional environment of collider data.","A novel modification to the variational latent diffusion model (VLD) approach to generative unfolding is presented, which allows for unfolding of high- and variable-dimensional feature spaces.","The performance of this method is evaluated in the context of semi-leptonic top quark pair production at the Large Hadron Collider."],"url":"http://arxiv.org/abs/2404.14332v1","category":"hep-ex"}
{"created":"2024-04-22 16:38:38","title":"Adapting to time: why nature evolved a diverse set of neurons","abstract":"Evolution has yielded a diverse set of neurons with varying morphologies and physiological properties that impact their processing of temporal information. In addition, it is known empirically that spike timing is a significant factor in neural computations. However, despite these two observations, most neural network models deal with spatially structured inputs with synchronous time steps, while restricting variation to parameters like weights and biases. In this study, we investigate the relevance of adapting temporal parameters, like time constants and delays, in feedforward networks that map spatio-temporal spike patterns. In this context, we show that networks with richer potential dynamics are able to more easily and robustly learn tasks with temporal structure. Indeed, when adaptation was restricted to weights, networks were unable to solve most problems. We also show strong interactions between the various parameters and the advantages of adapting temporal parameters when dealing with noise in inputs and weights, which might prove useful in neuromorphic hardware design.","sentences":["Evolution has yielded a diverse set of neurons with varying morphologies and physiological properties that impact their processing of temporal information.","In addition, it is known empirically that spike timing is a significant factor in neural computations.","However, despite these two observations, most neural network models deal with spatially structured inputs with synchronous time steps, while restricting variation to parameters like weights and biases.","In this study, we investigate the relevance of adapting temporal parameters, like time constants and delays, in feedforward networks that map spatio-temporal spike patterns.","In this context, we show that networks with richer potential dynamics are able to more easily and robustly learn tasks with temporal structure.","Indeed, when adaptation was restricted to weights, networks were unable to solve most problems.","We also show strong interactions between the various parameters and the advantages of adapting temporal parameters when dealing with noise in inputs and weights, which might prove useful in neuromorphic hardware design."],"url":"http://arxiv.org/abs/2404.14325v1","category":"cs.NE"}
{"created":"2024-04-22 16:02:48","title":"Explaining Arguments' Strength: Unveiling the Role of Attacks and Supports (Technical Report)","abstract":"Quantitatively explaining the strength of arguments under gradual semantics has recently received increasing attention. Specifically, several works in the literature provide quantitative explanations by computing the attribution scores of arguments. These works disregard the importance of attacks and supports, even though they play an essential role when explaining arguments' strength. In this paper, we propose a novel theory of Relation Attribution Explanations (RAEs), adapting Shapley values from game theory to offer fine-grained insights into the role of attacks and supports in quantitative bipolar argumentation towards obtaining the arguments' strength. We show that RAEs satisfy several desirable properties. We also propose a probabilistic algorithm to approximate RAEs efficiently. Finally, we show the application value of RAEs in fraud detection and large language models case studies.","sentences":["Quantitatively explaining the strength of arguments under gradual semantics has recently received increasing attention.","Specifically, several works in the literature provide quantitative explanations by computing the attribution scores of arguments.","These works disregard the importance of attacks and supports, even though they play an essential role when explaining arguments' strength.","In this paper, we propose a novel theory of Relation Attribution Explanations (RAEs), adapting Shapley values from game theory to offer fine-grained insights into the role of attacks and supports in quantitative bipolar argumentation towards obtaining the arguments' strength.","We show that RAEs satisfy several desirable properties.","We also propose a probabilistic algorithm to approximate RAEs efficiently.","Finally, we show the application value of RAEs in fraud detection and large language models case studies."],"url":"http://arxiv.org/abs/2404.14304v1","category":"cs.AI"}
{"created":"2024-04-22 16:00:46","title":"Marking: Visual Grading with Highlighting Errors and Annotating Missing Bits","abstract":"In this paper, we introduce \"Marking\", a novel grading task that enhances automated grading systems by performing an in-depth analysis of student responses and providing students with visual highlights. Unlike traditional systems that provide binary scores, \"marking\" identifies and categorizes segments of the student response as correct, incorrect, or irrelevant and detects omissions from gold answers. We introduce a new dataset meticulously curated by Subject Matter Experts specifically for this task. We frame \"Marking\" as an extension of the Natural Language Inference (NLI) task, which is extensively explored in the field of Natural Language Processing. The gold answer and the student response play the roles of premise and hypothesis in NLI, respectively. We subsequently train language models to identify entailment, contradiction, and neutrality from student response, akin to NLI, and with the added dimension of identifying omissions from gold answers. Our experimental setup involves the use of transformer models, specifically BERT and RoBERTa, and an intelligent training step using the e-SNLI dataset. We present extensive baseline results highlighting the complexity of the \"Marking\" task, which sets a clear trajectory for the upcoming study. Our work not only opens up new avenues for research in AI-powered educational assessment tools, but also provides a valuable benchmark for the AI in education community to engage with and improve upon in the future. The code and dataset can be found at https://github.com/luffycodes/marking.","sentences":["In this paper, we introduce \"Marking\", a novel grading task that enhances automated grading systems by performing an in-depth analysis of student responses and providing students with visual highlights.","Unlike traditional systems that provide binary scores, \"marking\" identifies and categorizes segments of the student response as correct, incorrect, or irrelevant and detects omissions from gold answers.","We introduce a new dataset meticulously curated by Subject Matter Experts specifically for this task.","We frame \"Marking\" as an extension of the Natural Language Inference (NLI) task, which is extensively explored in the field of Natural Language Processing.","The gold answer and the student response play the roles of premise and hypothesis in NLI, respectively.","We subsequently train language models to identify entailment, contradiction, and neutrality from student response, akin to NLI, and with the added dimension of identifying omissions from gold answers.","Our experimental setup involves the use of transformer models, specifically BERT and RoBERTa, and an intelligent training step using the e-SNLI dataset.","We present extensive baseline results highlighting the complexity of the \"Marking\" task, which sets a clear trajectory for the upcoming study.","Our work not only opens up new avenues for research in AI-powered educational assessment tools, but also provides a valuable benchmark for the AI in education community to engage with and improve upon in the future.","The code and dataset can be found at https://github.com/luffycodes/marking."],"url":"http://arxiv.org/abs/2404.14301v1","category":"cs.CL"}
{"created":"2024-04-22 15:54:53","title":"Does Your Neural Code Completion Model Use My Code? A Membership Inference Approach","abstract":"Recent years have witnessed significant progress in developing deep learning-based models for automated code completion. Although using source code in GitHub has been a common practice for training deep-learning-based models for code completion, it may induce some legal and ethical issues such as copyright infringement. In this paper, we investigate the legal and ethical issues of current neural code completion models by answering the following question: Is my code used to train your neural code completion model? To this end, we tailor a membership inference approach (termed CodeMI) that was originally crafted for classification tasks to a more challenging task of code completion. In particular, since the target code completion models perform as opaque black boxes, preventing access to their training data and parameters, we opt to train multiple shadow models to mimic their behavior. The acquired posteriors from these shadow models are subsequently employed to train a membership classifier. Subsequently, the membership classifier can be effectively employed to deduce the membership status of a given code sample based on the output of a target code completion model. We comprehensively evaluate the effectiveness of this adapted approach across a diverse array of neural code completion models, (i.e., LSTM-based, CodeGPT, CodeGen, and StarCoder). Experimental results reveal that the LSTM-based and CodeGPT models suffer the membership leakage issue, which can be easily detected by our proposed membership inference approach with an accuracy of 0.842, and 0.730, respectively. Interestingly, our experiments also show that the data membership of current large language models of code, e.g., CodeGen and StarCoder, is difficult to detect, leaving amper space for further improvement. Finally, we also try to explain the findings from the perspective of model memorization.","sentences":["Recent years have witnessed significant progress in developing deep learning-based models for automated code completion.","Although using source code in GitHub has been a common practice for training deep-learning-based models for code completion, it may induce some legal and ethical issues such as copyright infringement.","In this paper, we investigate the legal and ethical issues of current neural code completion models by answering the following question: Is my code used to train your neural code completion model?","To this end, we tailor a membership inference approach (termed CodeMI) that was originally crafted for classification tasks to a more challenging task of code completion.","In particular, since the target code completion models perform as opaque black boxes, preventing access to their training data and parameters, we opt to train multiple shadow models to mimic their behavior.","The acquired posteriors from these shadow models are subsequently employed to train a membership classifier.","Subsequently, the membership classifier can be effectively employed to deduce the membership status of a given code sample based on the output of a target code completion model.","We comprehensively evaluate the effectiveness of this adapted approach across a diverse array of neural code completion models, (i.e., LSTM-based, CodeGPT, CodeGen, and StarCoder).","Experimental results reveal that the LSTM-based and CodeGPT models suffer the membership leakage issue, which can be easily detected by our proposed membership inference approach with an accuracy of 0.842, and 0.730, respectively.","Interestingly, our experiments also show that the data membership of current large language models of code, e.g., CodeGen and StarCoder, is difficult to detect, leaving amper space for further improvement.","Finally, we also try to explain the findings from the perspective of model memorization."],"url":"http://arxiv.org/abs/2404.14296v1","category":"cs.SE"}
{"created":"2024-04-22 15:53:08","title":"A Survey on Efficient Inference for Large Language Models","abstract":"Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.","sentences":["Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks.","However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios.","Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference.","This paper presents a comprehensive survey of the existing literature on efficient LLM inference.","We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach.","Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization.","Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights.","Last but not least, we provide some knowledge summary and discuss future research directions."],"url":"http://arxiv.org/abs/2404.14294v1","category":"cs.CL"}
{"created":"2024-04-22 15:42:50","title":"Methodological Reconstruction of Historical Landslide Tsunamis Using Bayesian Inference","abstract":"Indonesia is one of the world's most densely populated regions and lies among the epicenters of Earth's greatest natural hazards. Effectively reducing the disaster potential of these hazards through resource allocation and preparedness first requires an analysis of the risk factors of the region. Since destructive tsunamis present one of the most eminent dangers to coastal communities, understanding their sources and geological history is necessary to determine the potential future risk.   Inspired by results from Cummins et al. 2020, and previous efforts that identified source parameters for earthquake-generated tsunamis, we consider landslide-generated tsunamis. This is done by constructing a probability distribution of potential landslide sources based on anecdotal observations of the 1852 Banda Sea tsunami, using Bayesian inference and scientific computing. After collecting over 100,000 samples (simulating 100,000 landslide induced tsunamis), we conclude that a landslide event provides a reasonable match to the tsunami reported in the anecdotal accounts. However, the most viable landslides may push the boundaries of geological plausibility. Future work creating a joint landslide-earthquake model may compensate for the weaknesses associated with an individual landslide or earthquake source event.","sentences":["Indonesia is one of the world's most densely populated regions and lies among the epicenters of Earth's greatest natural hazards.","Effectively reducing the disaster potential of these hazards through resource allocation and preparedness first requires an analysis of the risk factors of the region.","Since destructive tsunamis present one of the most eminent dangers to coastal communities, understanding their sources and geological history is necessary to determine the potential future risk.   ","Inspired by results from Cummins et al. 2020, and previous efforts that identified source parameters for earthquake-generated tsunamis, we consider landslide-generated tsunamis.","This is done by constructing a probability distribution of potential landslide sources based on anecdotal observations of the 1852 Banda Sea tsunami, using Bayesian inference and scientific computing.","After collecting over 100,000 samples (simulating 100,000 landslide induced tsunamis), we conclude that a landslide event provides a reasonable match to the tsunami reported in the anecdotal accounts.","However, the most viable landslides may push the boundaries of geological plausibility.","Future work creating a joint landslide-earthquake model may compensate for the weaknesses associated with an individual landslide or earthquake source event."],"url":"http://arxiv.org/abs/2404.14288v1","category":"physics.geo-ph"}
{"created":"2024-04-22 15:35:33","title":"LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots","abstract":"Large language models (LLMs) have shown significant potential for robotics applications, particularly task planning, by harnessing their language comprehension and text generation capabilities. However, in applications such as household robotics, a critical gap remains in the personalization of these models to individual user preferences. We introduce LLM-Personalize, a novel framework with an optimization pipeline designed to personalize LLM planners for household robotics. Our LLM-Personalize framework features an LLM planner that performs iterative planning in multi-room, partially-observable household scenarios, making use of a scene graph constructed with local observations. The generated plan consists of a sequence of high-level actions which are subsequently executed by a controller. Central to our approach is the optimization pipeline, which combines imitation learning and iterative self-training to personalize the LLM planner. In particular, the imitation learning phase performs initial LLM alignment from demonstrations, and bootstraps the model to facilitate effective iterative self-training, which further explores and aligns the model to user preferences. We evaluate LLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark for household rearrangements, and show that LLM-Personalize achieves more than a 30 percent increase in success rate over existing LLM planners, showcasing significantly improved alignment with human preferences. Project page: https://donggehan.github.io/projectllmpersonalize/.","sentences":["Large language models (LLMs) have shown significant potential for robotics applications, particularly task planning, by harnessing their language comprehension and text generation capabilities.","However, in applications such as household robotics, a critical gap remains in the personalization of these models to individual user preferences.","We introduce LLM-Personalize, a novel framework with an optimization pipeline designed to personalize LLM planners for household robotics.","Our LLM-Personalize framework features an LLM planner that performs iterative planning in multi-room, partially-observable household scenarios, making use of a scene graph constructed with local observations.","The generated plan consists of a sequence of high-level actions which are subsequently executed by a controller.","Central to our approach is the optimization pipeline, which combines imitation learning and iterative self-training to personalize the LLM planner.","In particular, the imitation learning phase performs initial LLM alignment from demonstrations, and bootstraps the model to facilitate effective iterative self-training, which further explores and aligns the model to user preferences.","We evaluate LLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark for household rearrangements, and show that LLM-Personalize achieves more than a 30 percent increase in success rate over existing LLM planners, showcasing significantly improved alignment with human preferences.","Project page: https://donggehan.github.io/projectllmpersonalize/."],"url":"http://arxiv.org/abs/2404.14285v1","category":"cs.RO"}
{"created":"2024-04-22 15:29:19","title":"RESFM: Robust Equivariant Multiview Structure from Motion","abstract":"Multiview Structure from Motion is a fundamental and challenging computer vision problem. A recent deep-based approach was proposed utilizing matrix equivariant architectures for the simultaneous recovery of camera pose and 3D scene structure from large image collections. This work however made the unrealistic assumption that the point tracks given as input are clean of outliers. Here we propose an architecture suited to dealing with outliers by adding an inlier/outlier classifying module that respects the model equivariance and by adding a robust bundle adjustment step. Experiments demonstrate that our method can be successfully applied in realistic settings that include large image collections and point tracks extracted with common heuristics and include many outliers.","sentences":["Multiview Structure from Motion is a fundamental and challenging computer vision problem.","A recent deep-based approach was proposed utilizing matrix equivariant architectures for the simultaneous recovery of camera pose and 3D scene structure from large image collections.","This work however made the unrealistic assumption that the point tracks given as input are clean of outliers.","Here we propose an architecture suited to dealing with outliers by adding an inlier/outlier classifying module that respects the model equivariance and by adding a robust bundle adjustment step.","Experiments demonstrate that our method can be successfully applied in realistic settings that include large image collections and point tracks extracted with common heuristics and include many outliers."],"url":"http://arxiv.org/abs/2404.14280v1","category":"cs.CV"}
{"created":"2024-04-22 15:26:31","title":"Dipolar order controls dielectric response of glass-forming liquids","abstract":"The dielectric response of liquids reflects both, reorientation of single molecular dipoles and collective modes, i.e., dipolar cross-correlations. A recent theory predicts the latter to produce an additional slow peak in the dielectric loss spectrum. Following this idea we argue that in supercooled liquids the high-frequency power law exponent of the dielectric loss $\\beta$ should be correlated with the degree of dipolar order, i.e., the Kirkwood correlation factor $g_K$. This notion is confirmed for 25 supercooled liquids. While our findings support recent theoretical work the results are shown to violate the earlier Kivelson-Madden theory.","sentences":["The dielectric response of liquids reflects both, reorientation of single molecular dipoles and collective modes, i.e., dipolar cross-correlations.","A recent theory predicts the latter to produce an additional slow peak in the dielectric loss spectrum.","Following this idea we argue that in supercooled liquids the high-frequency power law exponent of the dielectric loss $\\beta$ should be correlated with the degree of dipolar order, i.e., the Kirkwood correlation factor $g_K$. This notion is confirmed for 25 supercooled liquids.","While our findings support recent theoretical work the results are shown to violate the earlier Kivelson-Madden theory."],"url":"http://arxiv.org/abs/2404.14277v1","category":"cond-mat.soft"}
{"created":"2024-04-22 14:57:17","title":"AI-Generated Faces in the Real World: A Large-Scale Case Study of Twitter Profile Images","abstract":"Recent advances in the field of generative artificial intelligence (AI) have blurred the lines between authentic and machine-generated content, making it almost impossible for humans to distinguish between such media. One notable consequence is the use of AI-generated images for fake profiles on social media. While several types of disinformation campaigns and similar incidents have been reported in the past, a systematic analysis has been lacking. In this work, we conduct the first large-scale investigation of the prevalence of AI-generated profile pictures on Twitter. We tackle the challenges of a real-world measurement study by carefully integrating various data sources and designing a multi-stage detection pipeline. Our analysis of nearly 15 million Twitter profile pictures shows that 0.052% were artificially generated, confirming their notable presence on the platform. We comprehensively examine the characteristics of these accounts and their tweet content, and uncover patterns of coordinated inauthentic behavior. The results also reveal several motives, including spamming and political amplification campaigns. Our research reaffirms the need for effective detection and mitigation strategies to cope with the potential negative effects of generative AI in the future.","sentences":["Recent advances in the field of generative artificial intelligence (AI) have blurred the lines between authentic and machine-generated content, making it almost impossible for humans to distinguish between such media.","One notable consequence is the use of AI-generated images for fake profiles on social media.","While several types of disinformation campaigns and similar incidents have been reported in the past, a systematic analysis has been lacking.","In this work, we conduct the first large-scale investigation of the prevalence of AI-generated profile pictures on Twitter.","We tackle the challenges of a real-world measurement study by carefully integrating various data sources and designing a multi-stage detection pipeline.","Our analysis of nearly 15 million Twitter profile pictures shows that 0.052% were artificially generated, confirming their notable presence on the platform.","We comprehensively examine the characteristics of these accounts and their tweet content, and uncover patterns of coordinated inauthentic behavior.","The results also reveal several motives, including spamming and political amplification campaigns.","Our research reaffirms the need for effective detection and mitigation strategies to cope with the potential negative effects of generative AI in the future."],"url":"http://arxiv.org/abs/2404.14244v1","category":"cs.CR"}
{"created":"2024-04-22 14:56:36","title":"Turbo-CF: Matrix Decomposition-Free Graph Filtering for Fast Recommendation","abstract":"A series of graph filtering (GF)-based collaborative filtering (CF) showcases state-of-the-art performance on the recommendation accuracy by using a low-pass filter (LPF) without a training process. However, conventional GF-based CF approaches mostly perform matrix decomposition on the item-item similarity graph to realize the ideal LPF, which results in a non-trivial computational cost and thus makes them less practical in scenarios where rapid recommendations are essential. In this paper, we propose Turbo-CF, a GF-based CF method that is both training-free and matrix decomposition-free. Turbo-CF employs a polynomial graph filter to circumvent the issue of expensive matrix decompositions, enabling us to make full use of modern computer hardware components (i.e., GPU). Specifically, Turbo-CF first constructs an item-item similarity graph whose edge weights are effectively regulated. Then, our own polynomial LPFs are designed to retain only low-frequency signals without explicit matrix decompositions. We demonstrate that Turbo-CF is extremely fast yet accurate, achieving a runtime of less than 1 second on real-world benchmark datasets while achieving recommendation accuracies comparable to best competitors.","sentences":["A series of graph filtering (GF)-based collaborative filtering (CF) showcases state-of-the-art performance on the recommendation accuracy by using a low-pass filter (LPF) without a training process.","However, conventional GF-based CF approaches mostly perform matrix decomposition on the item-item similarity graph to realize the ideal LPF, which results in a non-trivial computational cost and thus makes them less practical in scenarios where rapid recommendations are essential.","In this paper, we propose Turbo-CF, a GF-based CF method that is both training-free and matrix decomposition-free.","Turbo-CF employs a polynomial graph filter to circumvent the issue of expensive matrix decompositions, enabling us to make full use of modern computer hardware components (i.e., GPU).","Specifically, Turbo-CF first constructs an item-item similarity graph whose edge weights are effectively regulated.","Then, our own polynomial LPFs are designed to retain only low-frequency signals without explicit matrix decompositions.","We demonstrate that Turbo-CF is extremely fast yet accurate, achieving a runtime of less than 1 second on real-world benchmark datasets while achieving recommendation accuracies comparable to best competitors."],"url":"http://arxiv.org/abs/2404.14243v1","category":"cs.IR"}
{"created":"2024-04-22 14:53:27","title":"UrbanCross: Enhancing Satellite Image-Text Retrieval with Cross-Domain Adaptation","abstract":"Urbanization challenges underscore the necessity for effective satellite image-text retrieval methods to swiftly access specific information enriched with geographic semantics for urban applications. However, existing methods often overlook significant domain gaps across diverse urban landscapes, primarily focusing on enhancing retrieval performance within single domains. To tackle this issue, we present UrbanCross, a new framework for cross-domain satellite image-text retrieval. UrbanCross leverages a high-quality, cross-domain dataset enriched with extensive geo-tags from three countries to highlight domain diversity. It employs the Large Multimodal Model (LMM) for textual refinement and the Segment Anything Model (SAM) for visual augmentation, achieving a fine-grained alignment of images, segments and texts, yielding a 10% improvement in retrieval performance. Additionally, UrbanCross incorporates an adaptive curriculum-based source sampler and a weighted adversarial cross-domain fine-tuning module, progressively enhancing adaptability across various domains. Extensive experiments confirm UrbanCross's superior efficiency in retrieval and adaptation to new urban environments, demonstrating an average performance increase of 15% over its version without domain adaptation mechanisms, effectively bridging the domain gap.","sentences":["Urbanization challenges underscore the necessity for effective satellite image-text retrieval methods to swiftly access specific information enriched with geographic semantics for urban applications.","However, existing methods often overlook significant domain gaps across diverse urban landscapes, primarily focusing on enhancing retrieval performance within single domains.","To tackle this issue, we present UrbanCross, a new framework for cross-domain satellite image-text retrieval.","UrbanCross leverages a high-quality, cross-domain dataset enriched with extensive geo-tags from three countries to highlight domain diversity.","It employs the Large Multimodal Model (LMM) for textual refinement and the Segment Anything Model (SAM) for visual augmentation, achieving a fine-grained alignment of images, segments and texts, yielding a 10% improvement in retrieval performance.","Additionally, UrbanCross incorporates an adaptive curriculum-based source sampler and a weighted adversarial cross-domain fine-tuning module, progressively enhancing adaptability across various domains.","Extensive experiments confirm UrbanCross's superior efficiency in retrieval and adaptation to new urban environments, demonstrating an average performance increase of 15% over its version without domain adaptation mechanisms, effectively bridging the domain gap."],"url":"http://arxiv.org/abs/2404.14241v1","category":"cs.CV"}
{"created":"2024-04-22 14:49:46","title":"Collaborative Filtering Based on Diffusion Models: Unveiling the Potential of High-Order Connectivity","abstract":"A recent study has shown that diffusion models are well-suited for modeling the generative process of user-item interactions in recommender systems due to their denoising nature. However, existing diffusion model-based recommender systems do not explicitly leverage high-order connectivities that contain crucial collaborative signals for accurate recommendations. Addressing this gap, we propose CF-Diff, a new diffusion model-based collaborative filtering (CF) method, which is capable of making full use of collaborative signals along with multi-hop neighbors. Specifically, the forward-diffusion process adds random noise to user-item interactions, while the reverse-denoising process accommodates our own learning model, named cross-attention-guided multi-hop autoencoder (CAM-AE), to gradually recover the original user-item interactions. CAM-AE consists of two core modules: 1) the attention-aided AE module, responsible for precisely learning latent representations of user-item interactions while preserving the model's complexity at manageable levels, and 2) the multi-hop cross-attention module, which judiciously harnesses high-order connectivity information to capture enhanced collaborative signals. Through comprehensive experiments on three real-world datasets, we demonstrate that CF-Diff is (a) Superior: outperforming benchmark recommendation methods, achieving remarkable gains up to 7.29% compared to the best competitor, (b) Theoretically-validated: reducing computations while ensuring that the embeddings generated by our model closely approximate those from the original cross-attention, and (c) Scalable: proving the computational efficiency that scales linearly with the number of users or items.","sentences":["A recent study has shown that diffusion models are well-suited for modeling the generative process of user-item interactions in recommender systems due to their denoising nature.","However, existing diffusion model-based recommender systems do not explicitly leverage high-order connectivities that contain crucial collaborative signals for accurate recommendations.","Addressing this gap, we propose CF-Diff, a new diffusion model-based collaborative filtering (CF) method, which is capable of making full use of collaborative signals along with multi-hop neighbors.","Specifically, the forward-diffusion process adds random noise to user-item interactions, while the reverse-denoising process accommodates our own learning model, named cross-attention-guided multi-hop autoencoder (CAM-AE), to gradually recover the original user-item interactions.","CAM-AE consists of two core modules: 1) the attention-aided AE module, responsible for precisely learning latent representations of user-item interactions while preserving the model's complexity at manageable levels, and 2) the multi-hop cross-attention module, which judiciously harnesses high-order connectivity information to capture enhanced collaborative signals.","Through comprehensive experiments on three real-world datasets, we demonstrate that CF-Diff is (a) Superior: outperforming benchmark recommendation methods, achieving remarkable gains up to 7.29% compared to the best competitor, (b) Theoretically-validated: reducing computations while ensuring that the embeddings generated by our model closely approximate those from the original cross-attention, and (c) Scalable: proving the computational efficiency that scales linearly with the number of users or items."],"url":"http://arxiv.org/abs/2404.14240v1","category":"cs.IR"}
{"created":"2024-04-22 14:47:42","title":"Beyond the Edge: An Advanced Exploration of Reinforcement Learning for Mobile Edge Computing, its Applications, and Future Research Trajectories","abstract":"Mobile Edge Computing (MEC) broadens the scope of computation and storage beyond the central network, incorporating edge nodes close to end devices. This expansion facilitates the implementation of large-scale \"connected things\" within edge networks. The advent of applications necessitating real-time, high-quality service presents several challenges, such as low latency, high data rate, reliability, efficiency, and security, all of which demand resolution. The incorporation of reinforcement learning (RL) methodologies within MEC networks promotes a deeper understanding of mobile user behaviors and network dynamics, thereby optimizing resource use in computing and communication processes. This paper offers an exhaustive survey of RL applications in MEC networks, initially presenting an overview of RL from its fundamental principles to the latest advanced frameworks. Furthermore, it outlines various RL strategies employed in offloading, caching, and communication within MEC networks. Finally, it explores open issues linked with software and hardware platforms, representation, RL robustness, safe RL, large-scale scheduling, generalization, security, and privacy. The paper proposes specific RL techniques to mitigate these issues and provides insights into their practical applications.","sentences":["Mobile Edge Computing (MEC) broadens the scope of computation and storage beyond the central network, incorporating edge nodes close to end devices.","This expansion facilitates the implementation of large-scale \"connected things\" within edge networks.","The advent of applications necessitating real-time, high-quality service presents several challenges, such as low latency, high data rate, reliability, efficiency, and security, all of which demand resolution.","The incorporation of reinforcement learning (RL) methodologies within MEC networks promotes a deeper understanding of mobile user behaviors and network dynamics, thereby optimizing resource use in computing and communication processes.","This paper offers an exhaustive survey of RL applications in MEC networks, initially presenting an overview of RL from its fundamental principles to the latest advanced frameworks.","Furthermore, it outlines various RL strategies employed in offloading, caching, and communication within MEC networks.","Finally, it explores open issues linked with software and hardware platforms, representation, RL robustness, safe RL, large-scale scheduling, generalization, security, and privacy.","The paper proposes specific RL techniques to mitigate these issues and provides insights into their practical applications."],"url":"http://arxiv.org/abs/2404.14238v1","category":"cs.NI"}
{"created":"2024-04-22 14:46:10","title":"Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback","abstract":"The rapidly developing Large Vision Language Models (LVLMs) have shown notable capabilities on a range of multi-modal tasks, but still face the hallucination phenomena where the generated texts do not align with the given contexts, significantly restricting the usages of LVLMs. Most previous work detects and mitigates hallucination at the coarse-grained level or requires expensive annotation (e.g., labeling by proprietary models or human experts). To address these issues, we propose detecting and mitigating hallucinations in LVLMs via fine-grained AI feedback. The basic idea is that we generate a small-size sentence-level hallucination annotation dataset by proprietary models, whereby we train a hallucination detection model which can perform sentence-level hallucination detection, covering primary hallucination types (i.e., object, attribute, and relationship). Then, we propose a detect-then-rewrite pipeline to automatically construct preference dataset for training hallucination mitigating model. Furthermore, we propose differentiating the severity of hallucinations, and introducing a Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO) for mitigating hallucination in LVLMs by incorporating the severity of hallucinations into preference learning. Extensive experiments demonstrate the effectiveness of our method.","sentences":["The rapidly developing Large Vision Language Models (LVLMs) have shown notable capabilities on a range of multi-modal tasks, but still face the hallucination phenomena where the generated texts do not align with the given contexts, significantly restricting the usages of LVLMs.","Most previous work detects and mitigates hallucination at the coarse-grained level or requires expensive annotation (e.g., labeling by proprietary models or human experts).","To address these issues, we propose detecting and mitigating hallucinations in LVLMs via fine-grained AI feedback.","The basic idea is that we generate a small-size sentence-level hallucination annotation dataset by proprietary models, whereby we train a hallucination detection model which can perform sentence-level hallucination detection, covering primary hallucination types (i.e., object, attribute, and relationship).","Then, we propose a detect-then-rewrite pipeline to automatically construct preference dataset for training hallucination mitigating model.","Furthermore, we propose differentiating the severity of hallucinations, and introducing a Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO) for mitigating hallucination in LVLMs by incorporating the severity of hallucinations into preference learning.","Extensive experiments demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.14233v1","category":"cs.CV"}
{"created":"2024-04-22 14:45:30","title":"Shifting Focus with HCEye: Exploring the Dynamics of Visual Highlighting and Cognitive Load on User Attention and Saliency Prediction","abstract":"Visual highlighting can guide user attention in complex interfaces. However, its effectiveness under limited attentional capacities is underexplored. This paper examines the joint impact of visual highlighting (permanent and dynamic) and dual-task-induced cognitive load on gaze behaviour. Our analysis, using eye-movement data from 27 participants viewing 150 unique webpages reveals that while participants' ability to attend to UI elements decreases with increasing cognitive load, dynamic adaptations (i.e., highlighting) remain attention-grabbing. The presence of these factors significantly alters what people attend to and thus what is salient. Accordingly, we show that state-of-the-art saliency models increase their performance when accounting for different cognitive loads. Our empirical insights, along with our openly available dataset, enhance our understanding of attentional processes in UIs under varying cognitive (and perceptual) loads and open the door for new models that can predict user attention while multitasking.","sentences":["Visual highlighting can guide user attention in complex interfaces.","However, its effectiveness under limited attentional capacities is underexplored.","This paper examines the joint impact of visual highlighting (permanent and dynamic) and dual-task-induced cognitive load on gaze behaviour.","Our analysis, using eye-movement data from 27 participants viewing 150 unique webpages reveals that while participants' ability to attend to UI elements decreases with increasing cognitive load, dynamic adaptations (i.e., highlighting) remain attention-grabbing.","The presence of these factors significantly alters what people attend to and thus what is salient.","Accordingly, we show that state-of-the-art saliency models increase their performance when accounting for different cognitive loads.","Our empirical insights, along with our openly available dataset, enhance our understanding of attentional processes in UIs under varying cognitive (and perceptual) loads and open the door for new models that can predict user attention while multitasking."],"url":"http://arxiv.org/abs/2404.14232v1","category":"cs.HC"}
{"created":"2024-04-22 14:33:16","title":"An Artificial Neuron for Enhanced Problem Solving in Large Language Models","abstract":"Recent advancements in artificial intelligence have propelled the capabilities of Large Language Models, yet their ability to mimic nuanced human reasoning remains limited. This paper introduces a novel conceptual enhancement to LLMs, termed the Artificial Neuron, designed to significantly bolster cognitive processing by integrating external memory systems. This enhancement mimics neurobiological processes, facilitating advanced reasoning and learning through a dynamic feedback loop mechanism. We propose a unique framework wherein each LLM interaction specifically in solving complex math word problems and common sense reasoning tasks is recorded and analyzed. Incorrect responses are refined using a higher capacity LLM or human in the loop corrections, and both the query and the enhanced response are stored in a vector database, structured much like neuronal synaptic connections. This Artificial Neuron thus serves as an external memory aid, allowing the LLM to reference past interactions and apply learned reasoning strategies to new problems. Our experimental setup involves training with the GSM8K dataset for initial model response generation, followed by systematic refinements through feedback loops. Subsequent testing demonstrated a significant improvement in accuracy and efficiency, underscoring the potential of external memory systems to advance LLMs beyond current limitations. This approach not only enhances the LLM's problem solving precision but also reduces computational redundancy, paving the way for more sophisticated applications of artificial intelligence in cognitive tasks. This paper details the methodology, implementation, and implications of the Artificial Neuron model, offering a transformative perspective on enhancing machine intelligence.","sentences":["Recent advancements in artificial intelligence have propelled the capabilities of Large Language Models, yet their ability to mimic nuanced human reasoning remains limited.","This paper introduces a novel conceptual enhancement to LLMs, termed the Artificial Neuron, designed to significantly bolster cognitive processing by integrating external memory systems.","This enhancement mimics neurobiological processes, facilitating advanced reasoning and learning through a dynamic feedback loop mechanism.","We propose a unique framework wherein each LLM interaction specifically in solving complex math word problems and common sense reasoning tasks is recorded and analyzed.","Incorrect responses are refined using a higher capacity LLM or human in the loop corrections, and both the query and the enhanced response are stored in a vector database, structured much like neuronal synaptic connections.","This Artificial Neuron thus serves as an external memory aid, allowing the LLM to reference past interactions and apply learned reasoning strategies to new problems.","Our experimental setup involves training with the GSM8K dataset for initial model response generation, followed by systematic refinements through feedback loops.","Subsequent testing demonstrated a significant improvement in accuracy and efficiency, underscoring the potential of external memory systems to advance LLMs beyond current limitations.","This approach not only enhances the LLM's problem solving precision but also reduces computational redundancy, paving the way for more sophisticated applications of artificial intelligence in cognitive tasks.","This paper details the methodology, implementation, and implications of the Artificial Neuron model, offering a transformative perspective on enhancing machine intelligence."],"url":"http://arxiv.org/abs/2404.14222v1","category":"cs.HC"}
{"created":"2024-04-22 14:32:33","title":"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone","abstract":"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).","sentences":["We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone.","The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data.","The model is also further aligned for robustness, safety, and chat format.","We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench)."],"url":"http://arxiv.org/abs/2404.14219v2","category":"cs.CL"}
{"created":"2024-04-22 14:07:42","title":"BCFPL: Binary classification ConvNet based Fast Parking space recognition with Low resolution image","abstract":"The automobile plays an important role in the economic activities of mankind, especially in the metropolis. Under the circumstances, the demand of quick search for available parking spaces has become a major concern for the automobile drivers. Meanwhile, the public sense of privacy is also awaking, the image-based parking space recognition methods lack the attention of privacy protection. In this paper, we proposed a binary convolutional neural network with lightweight design structure named BCFPL, which can be used to train with low-resolution parking space images and offer a reasonable recognition result. The images of parking space were collected from various complex environments, including different weather, occlusion conditions, and various camera angles. We conducted the training and testing progresses among different datasets and partial subsets. The experimental results show that the accuracy of BCFPL does not decrease compared with the original resolution image directly, and can reach the average level of the existing mainstream method. BCFPL also has low hardware requirements and fast recognition speed while meeting the privacy requirements, so it has application potential in intelligent city construction and automatic driving field.","sentences":["The automobile plays an important role in the economic activities of mankind, especially in the metropolis.","Under the circumstances, the demand of quick search for available parking spaces has become a major concern for the automobile drivers.","Meanwhile, the public sense of privacy is also awaking, the image-based parking space recognition methods lack the attention of privacy protection.","In this paper, we proposed a binary convolutional neural network with lightweight design structure named BCFPL, which can be used to train with low-resolution parking space images and offer a reasonable recognition result.","The images of parking space were collected from various complex environments, including different weather, occlusion conditions, and various camera angles.","We conducted the training and testing progresses among different datasets and partial subsets.","The experimental results show that the accuracy of BCFPL does not decrease compared with the original resolution image directly, and can reach the average level of the existing mainstream method.","BCFPL also has low hardware requirements and fast recognition speed while meeting the privacy requirements, so it has application potential in intelligent city construction and automatic driving field."],"url":"http://arxiv.org/abs/2404.14198v1","category":"cs.CV"}
{"created":"2024-04-22 14:01:24","title":"LLAMP: Assessing Network Latency Tolerance of HPC Applications with Linear Programming","abstract":"The shift towards high-bandwidth networks driven by AI workloads in data centers and HPC clusters has unintentionally aggravated network latency, adversely affecting the performance of communication-intensive HPC applications. As large-scale MPI applications often exhibit significant differences in their network latency tolerance, it is crucial to accurately determine the extent of network latency an application can withstand without significant performance degradation. Current approaches to assessing this metric often rely on specialized hardware or network simulators, which can be inflexible and time-consuming. In response, we introduce LLAMP, a novel toolchain that offers an efficient, analytical approach to evaluating HPC applications' network latency tolerance using the LogGPS model and linear programming. LLAMP equips software developers and network architects with essential insights for optimizing HPC infrastructures and strategically deploying applications to minimize latency impacts. Through our validation on a variety of MPI applications like MILC, LULESH, and LAMMPS, we demonstrate our tool's high accuracy, with relative prediction errors generally below 2%. Additionally, we include a case study of the ICON weather and climate model to illustrate LLAMP's broad applicability in evaluating collective algorithms and network topologies.","sentences":["The shift towards high-bandwidth networks driven by AI workloads in data centers and HPC clusters has unintentionally aggravated network latency, adversely affecting the performance of communication-intensive HPC applications.","As large-scale MPI applications often exhibit significant differences in their network latency tolerance, it is crucial to accurately determine the extent of network latency an application can withstand without significant performance degradation.","Current approaches to assessing this metric often rely on specialized hardware or network simulators, which can be inflexible and time-consuming.","In response, we introduce LLAMP, a novel toolchain that offers an efficient, analytical approach to evaluating HPC applications' network latency tolerance using the LogGPS model and linear programming.","LLAMP equips software developers and network architects with essential insights for optimizing HPC infrastructures and strategically deploying applications to minimize latency impacts.","Through our validation on a variety of MPI applications like MILC, LULESH, and LAMMPS, we demonstrate our tool's high accuracy, with relative prediction errors generally below 2%.","Additionally, we include a case study of the ICON weather and climate model to illustrate LLAMP's broad applicability in evaluating collective algorithms and network topologies."],"url":"http://arxiv.org/abs/2404.14193v1","category":"cs.DC"}
{"created":"2024-04-22 13:58:36","title":"Experimental Validation of Ultrasound Beamforming with End-to-End Deep Learning for Single Plane Wave Imaging","abstract":"Ultrafast ultrasound imaging insonifies a medium with one or a combination of a few plane waves at different beam-steered angles instead of many focused waves. It can achieve much higher frame rates, but often at the cost of reduced image quality. Deep learning approaches have been proposed to mitigate this disadvantage, in particular for single plane wave imaging. Predominantly, image-to-image post-processing networks or fully learned data-to-image neural networks are used. Both construct their mapping purely data-driven and require expressive networks and large amounts of training data to perform well. In contrast, we consider data-to-image networks which incorporate a conventional image formation techniques as differentiable layers in the network architecture. This allows for end-to-end training with small amounts of training data. In this work, using f-k migration as an image formation layer is evaluated in-depth with experimental data. We acquired a data collection designed for benchmarking data-driven plane wave imaging approaches using a realistic breast mimicking phantom and an ultrasound calibration phantom. The evaluation considers global and local image similarity measures and contrast, resolution and lesion detectability analysis. The results show that the proposed network architecture is capable of improving the image quality of single plane wave images on all evaluation metrics. Furthermore, these image quality improvements can be achieved with surprisingly little amounts of training data.","sentences":["Ultrafast ultrasound imaging insonifies a medium with one or a combination of a few plane waves at different beam-steered angles instead of many focused waves.","It can achieve much higher frame rates, but often at the cost of reduced image quality.","Deep learning approaches have been proposed to mitigate this disadvantage, in particular for single plane wave imaging.","Predominantly, image-to-image post-processing networks or fully learned data-to-image neural networks are used.","Both construct their mapping purely data-driven and require expressive networks and large amounts of training data to perform well.","In contrast, we consider data-to-image networks which incorporate a conventional image formation techniques as differentiable layers in the network architecture.","This allows for end-to-end training with small amounts of training data.","In this work, using f-k migration as an image formation layer is evaluated in-depth with experimental data.","We acquired a data collection designed for benchmarking data-driven plane wave imaging approaches using a realistic breast mimicking phantom and an ultrasound calibration phantom.","The evaluation considers global and local image similarity measures and contrast, resolution and lesion detectability analysis.","The results show that the proposed network architecture is capable of improving the image quality of single plane wave images on all evaluation metrics.","Furthermore, these image quality improvements can be achieved with surprisingly little amounts of training data."],"url":"http://arxiv.org/abs/2404.14188v1","category":"eess.IV"}
{"created":"2024-04-22 13:34:50","title":"A multi-robot system for the detection of explosive devices","abstract":"In order to clear the world of the threat posed by landmines and other explosive devices, robotic systems can play an important role. However, the development of such field robots that need to operate in hazardous conditions requires the careful consideration of multiple aspects related to the perception, mobility, and collaboration capabilities of the system. In the framework of a European challenge, the Artificial Intelligence for Detection of Explosive Devices - eXtended (AIDEDeX) project proposes to design a heterogeneous multi-robot system with advanced sensor fusion algorithms. This system is specifically designed to detect and classify improvised explosive devices, explosive ordnances, and landmines. This project integrates specialised sensors, including electromagnetic induction, ground penetrating radar, X-Ray backscatter imaging, Raman spectrometers, and multimodal cameras, to achieve comprehensive threat identification and localisation. The proposed system comprises a fleet of unmanned ground vehicles and unmanned aerial vehicles. This article details the operational phases of the AIDEDeX system, from rapid terrain exploration using unmanned aerial vehicles to specialised detection and classification by unmanned ground vehicles equipped with a robotic manipulator. Initially focusing on a centralised approach, the project will also explore the potential of a decentralised control architecture, taking inspiration from swarm robotics to provide a robust, adaptable, and scalable solution for explosive detection.","sentences":["In order to clear the world of the threat posed by landmines and other explosive devices, robotic systems can play an important role.","However, the development of such field robots that need to operate in hazardous conditions requires the careful consideration of multiple aspects related to the perception, mobility, and collaboration capabilities of the system.","In the framework of a European challenge, the Artificial Intelligence for Detection of Explosive Devices - eXtended (AIDEDeX) project proposes to design a heterogeneous multi-robot system with advanced sensor fusion algorithms.","This system is specifically designed to detect and classify improvised explosive devices, explosive ordnances, and landmines.","This project integrates specialised sensors, including electromagnetic induction, ground penetrating radar, X-Ray backscatter imaging, Raman spectrometers, and multimodal cameras, to achieve comprehensive threat identification and localisation.","The proposed system comprises a fleet of unmanned ground vehicles and unmanned aerial vehicles.","This article details the operational phases of the AIDEDeX system, from rapid terrain exploration using unmanned aerial vehicles to specialised detection and classification by unmanned ground vehicles equipped with a robotic manipulator.","Initially focusing on a centralised approach, the project will also explore the potential of a decentralised control architecture, taking inspiration from swarm robotics to provide a robust, adaptable, and scalable solution for explosive detection."],"url":"http://arxiv.org/abs/2404.14167v1","category":"cs.RO"}
{"created":"2024-04-22 13:20:01","title":"Multidimensional Interpolants","abstract":"In the domain of differential equation-based generative modeling, conventional approaches often rely on single-dimensional scalar values as interpolation coefficients during both training and inference phases. In this work, we introduce, for the first time, a multidimensional interpolant that extends these coefficients into multiple dimensions, leveraging the stochastic interpolant framework. Additionally, we propose a novel path optimization problem tailored to adaptively determine multidimensional inference trajectories, with a predetermined differential equation solver and a fixed number of function evaluations. Our solution involves simulation dynamics coupled with adversarial training to optimize the inference path. Notably, employing a multidimensional interpolant during training improves the model's inference performance, even in the absence of path optimization. When the adaptive, multidimensional path derived from our optimization process is employed, it yields further performance gains, even with fixed solver configurations. The introduction of multidimensional interpolants not only enhances the efficacy of models but also opens up a new domain for exploration in training and inference methodologies, emphasizing the potential of multidimensional paths as an untapped frontier.","sentences":["In the domain of differential equation-based generative modeling, conventional approaches often rely on single-dimensional scalar values as interpolation coefficients during both training and inference phases.","In this work, we introduce, for the first time, a multidimensional interpolant that extends these coefficients into multiple dimensions, leveraging the stochastic interpolant framework.","Additionally, we propose a novel path optimization problem tailored to adaptively determine multidimensional inference trajectories, with a predetermined differential equation solver and a fixed number of function evaluations.","Our solution involves simulation dynamics coupled with adversarial training to optimize the inference path.","Notably, employing a multidimensional interpolant during training improves the model's inference performance, even in the absence of path optimization.","When the adaptive, multidimensional path derived from our optimization process is employed, it yields further performance gains, even with fixed solver configurations.","The introduction of multidimensional interpolants not only enhances the efficacy of models but also opens up a new domain for exploration in training and inference methodologies, emphasizing the potential of multidimensional paths as an untapped frontier."],"url":"http://arxiv.org/abs/2404.14161v1","category":"cs.LG"}
{"created":"2024-04-22 12:45:40","title":"Generative Artificial Intelligence Assisted Wireless Sensing: Human Flow Detection in Practical Communication Environments","abstract":"Groundbreaking applications such as ChatGPT have heightened research interest in generative artificial intelligence (GAI). Essentially, GAI excels not only in content generation but also in signal processing, offering support for wireless sensing. Hence, we introduce a novel GAI-assisted human flow detection system (G-HFD). Rigorously, G-HFD first uses channel state information (CSI) to estimate the velocity and acceleration of propagation path length change of the human-induced reflection (HIR). Then, given the strong inference ability of the diffusion model, we propose a unified weighted conditional diffusion model (UW-CDM) to denoise the estimation results, enabling the detection of the number of targets. Next, we use the CSI obtained by a uniform linear array with wavelength spacing to estimate the HIR's time of flight and direction of arrival (DoA). In this process, UW-CDM solves the problem of ambiguous DoA spectrum, ensuring accurate DoA estimation. Finally, through clustering, G-HFD determines the number of subflows and the number of targets in each subflow, i.e., the subflow size. The evaluation based on practical downlink communication signals shows G-HFD's accuracy of subflow size detection can reach 91%. This validates its effectiveness and underscores the significant potential of GAI in the context of wireless sensing.","sentences":["Groundbreaking applications such as ChatGPT have heightened research interest in generative artificial intelligence (GAI).","Essentially, GAI excels not only in content generation but also in signal processing, offering support for wireless sensing.","Hence, we introduce a novel GAI-assisted human flow detection system (G-HFD).","Rigorously, G-HFD first uses channel state information (CSI) to estimate the velocity and acceleration of propagation path length change of the human-induced reflection (HIR).","Then, given the strong inference ability of the diffusion model, we propose a unified weighted conditional diffusion model (UW-CDM) to denoise the estimation results, enabling the detection of the number of targets.","Next, we use the CSI obtained by a uniform linear array with wavelength spacing to estimate the HIR's time of flight and direction of arrival (DoA).","In this process, UW-CDM solves the problem of ambiguous DoA spectrum, ensuring accurate DoA estimation.","Finally, through clustering, G-HFD determines the number of subflows and the number of targets in each subflow, i.e., the subflow size.","The evaluation based on practical downlink communication signals shows G-HFD's accuracy of subflow size detection can reach 91%.","This validates its effectiveness and underscores the significant potential of GAI in the context of wireless sensing."],"url":"http://arxiv.org/abs/2404.14140v1","category":"eess.SP"}
{"created":"2024-04-22 12:34:53","title":"Quantum Convolutional Neural Networks for the detection of Gamma-Ray Bursts in the AGILE space mission data","abstract":"Quantum computing represents a cutting-edge frontier in artificial intelligence. It makes use of hybrid quantum-classical computation which tries to leverage quantum mechanic principles that allow us to use a different approach to deep learning classification problems. The work presented here falls within the context of the AGILE space mission, launched in 2007 by the Italian Space Agency. We implement different Quantum Convolutional Neural Networks (QCNN) that analyze data acquired by the instruments onboard AGILE to detect Gamma-Ray Bursts from sky maps or light curves. We use several frameworks such as TensorFlow-Quantum, Qiskit and PennyLane to simulate a quantum computer. We achieved an accuracy of 95.1% on sky maps with QCNNs, while the classical counterpart achieved 98.8% on the same data, using however hundreds of thousands more parameters.","sentences":["Quantum computing represents a cutting-edge frontier in artificial intelligence.","It makes use of hybrid quantum-classical computation which tries to leverage quantum mechanic principles that allow us to use a different approach to deep learning classification problems.","The work presented here falls within the context of the AGILE space mission, launched in 2007 by the Italian Space Agency.","We implement different Quantum Convolutional Neural Networks (QCNN) that analyze data acquired by the instruments onboard AGILE to detect Gamma-Ray Bursts from sky maps or light curves.","We use several frameworks such as TensorFlow-Quantum, Qiskit and PennyLane to simulate a quantum computer.","We achieved an accuracy of 95.1% on sky maps with QCNNs, while the classical counterpart achieved 98.8% on the same data, using however hundreds of thousands more parameters."],"url":"http://arxiv.org/abs/2404.14133v1","category":"astro-ph.HE"}
{"created":"2024-04-22 12:25:17","title":"Gaia DR3 detectability of unresolved binary systems","abstract":"Gaia can not individually resolve very close binary systems, however, the collected data can still be used to identify them. A powerful indicator of stellar multiplicity is the sources reported Renormalized Unit Weight Error (ruwe), which effectively captures the astrometric deviations from single-source solutions. We aim to characterise the imprints left on ruwe caused by binarity. By flagging potential binary systems based on ruwe, we aim to characterise which of their properties will contribute the most to their detectability. We develop a model to estimate ruwe values for observations of Gaia sources, based on the biases to the single-source astrometric track arising from the presence of an unseen companion. Then, using the recipes from previous GaiaUnlimited selection functions, we estimate the selection probability of sources with high ruwe, and discuss what binary properties contribute to increasing the sources ruwe. We compute the maximum ruwe value which is compatible with single-source solutions as a function of their location on-sky. We see that binary systems selected as sources with a ruwe higher than this sky-varying threshold have a strong detectability window in their orbital period distribution, which peaks at periods equal to the Gaia observation time baseline. We demonstrate how our sky-varying ruwe threshold provides a more complete sample of binary systems when compared to single sky-averaged values by studying the unresolved binary population in the Gaia Catalogue of Nearby Stars. We provide the code and tools used in this study, as well as the sky-varying ruwe threshold through the GaiaUnlimited Python package","sentences":["Gaia can not individually resolve very close binary systems, however, the collected data can still be used to identify them.","A powerful indicator of stellar multiplicity is the sources reported Renormalized Unit Weight Error (ruwe), which effectively captures the astrometric deviations from single-source solutions.","We aim to characterise the imprints left on ruwe caused by binarity.","By flagging potential binary systems based on ruwe, we aim to characterise which of their properties will contribute the most to their detectability.","We develop a model to estimate ruwe values for observations of Gaia sources, based on the biases to the single-source astrometric track arising from the presence of an unseen companion.","Then, using the recipes from previous GaiaUnlimited selection functions, we estimate the selection probability of sources with high ruwe, and discuss what binary properties contribute to increasing the sources ruwe.","We compute the maximum ruwe value which is compatible with single-source solutions as a function of their location on-sky.","We see that binary systems selected as sources with a ruwe higher than this sky-varying threshold have a strong detectability window in their orbital period distribution, which peaks at periods equal to the Gaia observation time baseline.","We demonstrate how our sky-varying ruwe threshold provides a more complete sample of binary systems when compared to single sky-averaged values by studying the unresolved binary population in the Gaia Catalogue of Nearby Stars.","We provide the code and tools used in this study, as well as the sky-varying ruwe threshold through the GaiaUnlimited Python package"],"url":"http://arxiv.org/abs/2404.14127v1","category":"astro-ph.GA"}
{"created":"2024-04-22 12:24:49","title":"Gaussian distributional structural equation models: A framework for modeling latent heteroscedasticity","abstract":"Accounting for the complexity of psychological theories requires methods that can predict not only changes in the means of latent variables -- such as personality factors, creativity, or intelligence -- but also changes in their variances. Structural equation modeling (SEM) is the framework of choice for analyzing complex relationships among latent variables, but current methods do not allow modeling latent variances as a function of other latent variables. In this paper, we develop a Bayesian framework for Gaussian distributional SEM which overcomes this limitation. We validate our framework using extensive simulations, which demonstrate that the new models produce reliable statistical inference and can be computed with sufficient efficiency for practical everyday use. We illustrate our framework's applicability in a real-world case study that addresses a substantive hypothesis from personality psychology.","sentences":["Accounting for the complexity of psychological theories requires methods that can predict not only changes in the means of latent variables -- such as personality factors, creativity, or intelligence -- but also changes in their variances.","Structural equation modeling (SEM) is the framework of choice for analyzing complex relationships among latent variables, but current methods do not allow modeling latent variances as a function of other latent variables.","In this paper, we develop a Bayesian framework for Gaussian distributional SEM which overcomes this limitation.","We validate our framework using extensive simulations, which demonstrate that the new models produce reliable statistical inference and can be computed with sufficient efficiency for practical everyday use.","We illustrate our framework's applicability in a real-world case study that addresses a substantive hypothesis from personality psychology."],"url":"http://arxiv.org/abs/2404.14124v1","category":"stat.ME"}
{"created":"2024-04-22 12:07:10","title":"Hierarchical localization with panoramic views and triplet loss functions","abstract":"The main objective of this paper is to address the mobile robot localization problem with Triplet Convolutional Neural Networks and test their robustness against changes of the lighting conditions. We have used omnidirectional images from real indoor environments captured in dynamic conditions that have been converted to panoramic format. Two approaches are proposed to address localization by means of triplet neural networks. First, hierarchical localization, which consists in estimating the robot position in two stages: a coarse localization, which involves a room retrieval task, and a fine localization is addressed by means of image retrieval in the previously selected room. Second, global localization, which consists in estimating the position of the robot inside the entire map in a unique step. Besides, an exhaustive study of the loss function influence on the network learning process has been made. The experimental section proves that triplet neural networks are an efficient and robust tool to address the localization of mobile robots in indoor environments, considering real operation conditions.","sentences":["The main objective of this paper is to address the mobile robot localization problem with Triplet Convolutional Neural Networks and test their robustness against changes of the lighting conditions.","We have used omnidirectional images from real indoor environments captured in dynamic conditions that have been converted to panoramic format.","Two approaches are proposed to address localization by means of triplet neural networks.","First, hierarchical localization, which consists in estimating the robot position in two stages: a coarse localization, which involves a room retrieval task, and a fine localization is addressed by means of image retrieval in the previously selected room.","Second, global localization, which consists in estimating the position of the robot inside the entire map in a unique step.","Besides, an exhaustive study of the loss function influence on the network learning process has been made.","The experimental section proves that triplet neural networks are an efficient and robust tool to address the localization of mobile robots in indoor environments, considering real operation conditions."],"url":"http://arxiv.org/abs/2404.14117v1","category":"cs.RO"}
{"created":"2024-04-22 11:28:34","title":"Immersive Rover Control and Obstacle Detection based on Extended Reality and Artificial Intelligence","abstract":"Lunar exploration has become a key focus, driving scientific and technological advances. Ongoing missions are deploying rovers to the surface of the Moon, targeting the far side and south pole. However, these terrains pose challenges, emphasizing the need for precise obstacles and resource detection to avoid mission risks. This work proposes a novel system that integrates eXtended Reality (XR) and Artificial Intelligence (AI) to teleoperate lunar rovers. It is capable of autonomously detecting rocks and recreating an immersive 3D virtual environment of the location of the robot. This system has been validated in a lunar laboratory to observe its advantages over traditional 2D-based teleoperation approaches","sentences":["Lunar exploration has become a key focus, driving scientific and technological advances.","Ongoing missions are deploying rovers to the surface of the Moon, targeting the far side and south pole.","However, these terrains pose challenges, emphasizing the need for precise obstacles and resource detection to avoid mission risks.","This work proposes a novel system that integrates eXtended Reality (XR) and Artificial Intelligence (AI) to teleoperate lunar rovers.","It is capable of autonomously detecting rocks and recreating an immersive 3D virtual environment of the location of the robot.","This system has been validated in a lunar laboratory to observe its advantages over traditional 2D-based teleoperation approaches"],"url":"http://arxiv.org/abs/2404.14095v1","category":"cs.RO"}
{"created":"2024-04-22 11:25:37","title":"Multi-agent Reinforcement Learning-based Joint Precoding and Phase Shift Optimization for RIS-aided Cell-Free Massive MIMO Systems","abstract":"Cell-free (CF) massive multiple-input multiple-output (mMIMO) is a promising technique for achieving high spectral efficiency (SE) using multiple distributed access points (APs). However, harsh propagation environments often lead to significant communication performance degradation due to high penetration loss. To overcome this issue, we introduce the reconfigurable intelligent surface (RIS) into the CF mMIMO system as a low-cost and power-efficient solution. In this paper, we focus on optimizing the joint precoding design of the RIS-aided CF mMIMO system to maximize the sum SE. This involves optimizing the precoding matrix at the APs and the reflection coefficients at the RIS. To tackle this problem, we propose a fully distributed multi-agent reinforcement learning (MARL) algorithm that incorporates fuzzy logic (FL). Unlike conventional approaches that rely on alternating optimization techniques, our FL-based MARL algorithm only requires local channel state information, which reduces the need for high backhaul capacity. Simulation results demonstrate that our proposed FL-MARL algorithm effectively reduces computational complexity while achieving similar performance as conventional MARL methods.","sentences":["Cell-free (CF) massive multiple-input multiple-output (mMIMO) is a promising technique for achieving high spectral efficiency (SE) using multiple distributed access points (APs).","However, harsh propagation environments often lead to significant communication performance degradation due to high penetration loss.","To overcome this issue, we introduce the reconfigurable intelligent surface (RIS) into the CF mMIMO system as a low-cost and power-efficient solution.","In this paper, we focus on optimizing the joint precoding design of the RIS-aided CF mMIMO system to maximize the sum SE.","This involves optimizing the precoding matrix at the APs and the reflection coefficients at the RIS.","To tackle this problem, we propose a fully distributed multi-agent reinforcement learning (MARL) algorithm that incorporates fuzzy logic (FL).","Unlike conventional approaches that rely on alternating optimization techniques, our FL-based MARL algorithm only requires local channel state information, which reduces the need for high backhaul capacity.","Simulation results demonstrate that our proposed FL-MARL algorithm effectively reduces computational complexity while achieving similar performance as conventional MARL methods."],"url":"http://arxiv.org/abs/2404.14092v1","category":"cs.IT"}
{"created":"2024-04-22 11:01:51","title":"Mechanistic Interpretability for AI Safety -- A Review","abstract":"Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.","sentences":["Understanding AI systems' inner workings is critical for ensuring value alignment and safety.","This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding.","We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation.","We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety.","We investigate challenges surrounding scalability, automation, and comprehensive interpretation.","We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning.","Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable."],"url":"http://arxiv.org/abs/2404.14082v1","category":"cs.AI"}
{"created":"2024-04-22 10:34:58","title":"Towards Robust Trajectory Representations: Isolating Environmental Confounders with Causal Learning","abstract":"Trajectory modeling refers to characterizing human movement behavior, serving as a pivotal step in understanding mobility patterns. Nevertheless, existing studies typically ignore the confounding effects of geospatial context, leading to the acquisition of spurious correlations and limited generalization capabilities. To bridge this gap, we initially formulate a Structural Causal Model (SCM) to decipher the trajectory representation learning process from a causal perspective. Building upon the SCM, we further present a Trajectory modeling framework (TrajCL) based on Causal Learning, which leverages the backdoor adjustment theory as an intervention tool to eliminate the spurious correlations between geospatial context and trajectories. Extensive experiments on two real-world datasets verify that TrajCL markedly enhances performance in trajectory classification tasks while showcasing superior generalization and interpretability.","sentences":["Trajectory modeling refers to characterizing human movement behavior, serving as a pivotal step in understanding mobility patterns.","Nevertheless, existing studies typically ignore the confounding effects of geospatial context, leading to the acquisition of spurious correlations and limited generalization capabilities.","To bridge this gap, we initially formulate a Structural Causal Model (SCM) to decipher the trajectory representation learning process from a causal perspective.","Building upon the SCM, we further present a Trajectory modeling framework (TrajCL) based on Causal Learning, which leverages the backdoor adjustment theory as an intervention tool to eliminate the spurious correlations between geospatial context and trajectories.","Extensive experiments on two real-world datasets verify that TrajCL markedly enhances performance in trajectory classification tasks while showcasing superior generalization and interpretability."],"url":"http://arxiv.org/abs/2404.14073v1","category":"cs.LG"}
{"created":"2024-04-22 10:26:49","title":"Holistic Safety and Responsibility Evaluations of Advanced AI Models","abstract":"Safety and responsibility evaluations of advanced AI models are a critical but developing field of research and practice. In the development of Google DeepMind's advanced AI models, we innovated on and applied a broad set of approaches to safety evaluation. In this report, we summarise and share elements of our evolving approach as well as lessons learned for a broad audience. Key lessons learned include: First, theoretical underpinnings and frameworks are invaluable to organise the breadth of risk domains, modalities, forms, metrics, and goals. Second, theory and practice of safety evaluation development each benefit from collaboration to clarify goals, methods and challenges, and facilitate the transfer of insights between different stakeholders and disciplines. Third, similar key methods, lessons, and institutions apply across the range of concerns in responsibility and safety - including established and emerging harms. For this reason it is important that a wide range of actors working on safety evaluation and safety research communities work together to develop, refine and implement novel evaluation approaches and best practices, rather than operating in silos. The report concludes with outlining the clear need to rapidly advance the science of evaluations, to integrate new evaluations into the development and governance of AI, to establish scientifically-grounded norms and standards, and to promote a robust evaluation ecosystem.","sentences":["Safety and responsibility evaluations of advanced AI models are a critical but developing field of research and practice.","In the development of Google DeepMind's advanced AI models, we innovated on and applied a broad set of approaches to safety evaluation.","In this report, we summarise and share elements of our evolving approach as well as lessons learned for a broad audience.","Key lessons learned include: First, theoretical underpinnings and frameworks are invaluable to organise the breadth of risk domains, modalities, forms, metrics, and goals.","Second, theory and practice of safety evaluation development each benefit from collaboration to clarify goals, methods and challenges, and facilitate the transfer of insights between different stakeholders and disciplines.","Third, similar key methods, lessons, and institutions apply across the range of concerns in responsibility and safety - including established and emerging harms.","For this reason it is important that a wide range of actors working on safety evaluation and safety research communities work together to develop, refine and implement novel evaluation approaches and best practices, rather than operating in silos.","The report concludes with outlining the clear need to rapidly advance the science of evaluations, to integrate new evaluations into the development and governance of AI, to establish scientifically-grounded norms and standards, and to promote a robust evaluation ecosystem."],"url":"http://arxiv.org/abs/2404.14068v1","category":"cs.AI"}
{"created":"2024-04-22 10:19:02","title":"FedTAD: Topology-aware Data-free Knowledge Distillation for Subgraph Federated Learning","abstract":"Subgraph federated learning (subgraph-FL) is a new distributed paradigm that facilitates the collaborative training of graph neural networks (GNNs) by multi-client subgraphs. Unfortunately, a significant challenge of subgraph-FL arises from subgraph heterogeneity, which stems from node and topology variation, causing the impaired performance of the global GNN. Despite various studies, they have not yet thoroughly investigated the impact mechanism of subgraph heterogeneity. To this end, we decouple node and topology variation, revealing that they correspond to differences in label distribution and structure homophily. Remarkably, these variations lead to significant differences in the class-wise knowledge reliability of multiple local GNNs, misguiding the model aggregation with varying degrees. Building on this insight, we propose topology-aware data-free knowledge distillation technology (FedTAD), enhancing reliable knowledge transfer from the local model to the global model. Extensive experiments on six public datasets consistently demonstrate the superiority of FedTAD over state-of-the-art baselines.","sentences":["Subgraph federated learning (subgraph-FL) is a new distributed paradigm that facilitates the collaborative training of graph neural networks (GNNs) by multi-client subgraphs.","Unfortunately, a significant challenge of subgraph-FL arises from subgraph heterogeneity, which stems from node and topology variation, causing the impaired performance of the global GNN.","Despite various studies, they have not yet thoroughly investigated the impact mechanism of subgraph heterogeneity.","To this end, we decouple node and topology variation, revealing that they correspond to differences in label distribution and structure homophily.","Remarkably, these variations lead to significant differences in the class-wise knowledge reliability of multiple local GNNs, misguiding the model aggregation with varying degrees.","Building on this insight, we propose topology-aware data-free knowledge distillation technology (FedTAD), enhancing reliable knowledge transfer from the local model to the global model.","Extensive experiments on six public datasets consistently demonstrate the superiority of FedTAD over state-of-the-art baselines."],"url":"http://arxiv.org/abs/2404.14061v1","category":"cs.LG"}
{"created":"2024-04-22 10:16:02","title":"Bored to Death: Artificial Intelligence Research Reveals the Role of Boredom in Suicide Behavior","abstract":"Background: Recent advancements in Artificial Intelligence (AI) contributed significantly to suicide assessment, however, our theoretical understanding of this complex behavior is still limited. Objective: This study aimed to harness AI methodologies to uncover hidden risk factors that trigger or aggravate suicide behaviors. Method: The primary dataset included 228,052 Facebook postings by 1,006 users who completed the gold-standard Columbia Suicide Severity Rating Scale. This dataset was analyzed using a bottom-up research pipeline without a-priory hypotheses and its findings were validated using a top-down analysis of a new dataset. This secondary dataset included responses by 1,062 participants to the same suicide scale as well as to well-validated scales measuring depression and boredom. Results: An almost fully automated, AI-guided research pipeline resulted in four Facebook topics that predicted the risk of suicide, of which the strongest predictor was boredom. A comprehensive literature review using APA PsycInfo revealed that boredom is rarely perceived as a unique risk factor of suicide. A complementing top-down path analysis of the secondary dataset uncovered an indirect relationship between boredom and suicide, which was mediated by depression. An equivalent mediated relationship was observed in the primary Facebook dataset as well. However, here, a direct relationship between boredom and suicide risk was also observed. Conclusions: Integrating AI methods allowed the discovery of an under-researched risk factor of suicide. The study signals boredom as a maladaptive 'ingredient' that might trigger suicide behaviors, regardless of depression. Further studies are recommended to direct clinicians' attention to this burdening, and sometimes existential experience.","sentences":["Background: Recent advancements in Artificial Intelligence (AI) contributed significantly to suicide assessment, however, our theoretical understanding of this complex behavior is still limited.","Objective:","This study aimed to harness AI methodologies to uncover hidden risk factors that trigger or aggravate suicide behaviors.","Method: The primary dataset included 228,052 Facebook postings by 1,006 users who completed the gold-standard Columbia Suicide Severity Rating Scale.","This dataset was analyzed using a bottom-up research pipeline without a-priory hypotheses and its findings were validated using a top-down analysis of a new dataset.","This secondary dataset included responses by 1,062 participants to the same suicide scale as well as to well-validated scales measuring depression and boredom.","Results:","An almost fully automated, AI-guided research pipeline resulted in four Facebook topics that predicted the risk of suicide, of which the strongest predictor was boredom.","A comprehensive literature review using APA PsycInfo revealed that boredom is rarely perceived as a unique risk factor of suicide.","A complementing top-down path analysis of the secondary dataset uncovered an indirect relationship between boredom and suicide, which was mediated by depression.","An equivalent mediated relationship was observed in the primary Facebook dataset as well.","However, here, a direct relationship between boredom and suicide risk was also observed.","Conclusions: Integrating AI methods allowed the discovery of an under-researched risk factor of suicide.","The study signals boredom as a maladaptive 'ingredient' that might trigger suicide behaviors, regardless of depression.","Further studies are recommended to direct clinicians' attention to this burdening, and sometimes existential experience."],"url":"http://arxiv.org/abs/2404.14057v1","category":"cs.CL"}
{"created":"2024-04-22 10:06:17","title":"Unlawful Proxy Discrimination: A Framework for Challenging Inherently Discriminatory Algorithms","abstract":"Emerging scholarship suggests that the EU legal concept of direct discrimination - where a person is given different treatment on grounds of a protected characteristic - may apply to various algorithmic decision-making contexts. This has important implications: unlike indirect discrimination, there is generally no 'objective justification' stage in the direct discrimination framework, which means that the deployment of directly discriminatory algorithms will usually be unlawful per se. In this paper, we focus on the most likely candidate for direct discrimination in the algorithmic context, termed inherent direct discrimination, where a proxy is inextricably linked to a protected characteristic. We draw on computer science literature to suggest that, in the algorithmic context, 'treatment on the grounds of' needs to be understood in terms of two steps: proxy capacity and proxy use. Only where both elements can be made out can direct discrimination be said to be `on grounds of' a protected characteristic. We analyse the legal conditions of our proposed proxy capacity and proxy use tests. Based on this analysis, we discuss technical approaches and metrics that could be developed or applied to identify inherent direct discrimination in algorithmic decision-making.","sentences":["Emerging scholarship suggests that the EU legal concept of direct discrimination - where a person is given different treatment on grounds of a protected characteristic - may apply to various algorithmic decision-making contexts.","This has important implications: unlike indirect discrimination, there is generally no 'objective justification' stage in the direct discrimination framework, which means that the deployment of directly discriminatory algorithms will usually be unlawful per se.","In this paper, we focus on the most likely candidate for direct discrimination in the algorithmic context, termed inherent direct discrimination, where a proxy is inextricably linked to a protected characteristic.","We draw on computer science literature to suggest that, in the algorithmic context, 'treatment on the grounds of' needs to be understood in terms of two steps: proxy capacity and proxy use.","Only where both elements can be made out can direct discrimination be said to be `on grounds of' a protected characteristic.","We analyse the legal conditions of our proposed proxy capacity and proxy use tests.","Based on this analysis, we discuss technical approaches and metrics that could be developed or applied to identify inherent direct discrimination in algorithmic decision-making."],"url":"http://arxiv.org/abs/2404.14050v1","category":"cs.AI"}
{"created":"2024-04-22 09:50:11","title":"Apodotiko: Enabling Efficient Serverless Federated Learning in Heterogeneous Environments","abstract":"Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, current serverless FL systems still suffer from the presence of stragglers, i.e., slow clients that impede the collaborative training process. While strategies aimed at mitigating stragglers in these systems have been proposed, they overlook the diverse hardware resource configurations among FL clients. To this end, we present Apodotiko, a novel asynchronous training strategy designed for serverless FL. Our strategy incorporates a scoring mechanism that evaluates each client's hardware capacity and dataset size to intelligently prioritize and select clients for each training round, thereby minimizing the effects of stragglers on system performance. We comprehensively evaluate Apodotiko across diverse datasets, considering a mix of CPU and GPU clients, and compare its performance against five other FL training strategies. Results from our experiments demonstrate that Apodotiko outperforms other FL training strategies, achieving an average speedup of 2.75x and a maximum speedup of 7.03x. Furthermore, our strategy significantly reduces cold starts by a factor of four on average, demonstrating suitability in serverless environments.","sentences":["Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized.","Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders.","However, current serverless FL systems still suffer from the presence of stragglers, i.e., slow clients that impede the collaborative training process.","While strategies aimed at mitigating stragglers in these systems have been proposed, they overlook the diverse hardware resource configurations among FL clients.","To this end, we present Apodotiko, a novel asynchronous training strategy designed for serverless FL.","Our strategy incorporates a scoring mechanism that evaluates each client's hardware capacity and dataset size to intelligently prioritize and select clients for each training round, thereby minimizing the effects of stragglers on system performance.","We comprehensively evaluate Apodotiko across diverse datasets, considering a mix of CPU and GPU clients, and compare its performance against five other FL training strategies.","Results from our experiments demonstrate that Apodotiko outperforms other FL training strategies, achieving an average speedup of 2.75x and a maximum speedup of 7.03x.","Furthermore, our strategy significantly reduces cold starts by a factor of four on average, demonstrating suitability in serverless environments."],"url":"http://arxiv.org/abs/2404.14033v1","category":"cs.DC"}
{"created":"2024-04-22 09:32:38","title":"Hybrid Ensemble-Based Travel Mode Prediction","abstract":"Travel mode choice (TMC) prediction, which can be formulated as a classification task, helps in understanding what makes citizens choose different modes of transport for individual trips. This is also a major step towards fostering sustainable transportation. As behaviour may evolve over time, we also face the question of detecting concept drift in the data. This necessitates using appropriate methods to address potential concept drift. In particular, it is necessary to decide whether batch or stream mining methods should be used to develop periodically updated TMC models. To address the challenge of the development of TMC models, we propose the novel Incremental Ensemble of Batch and Stream Models (IEBSM) method aimed at adapting travel mode choice classifiers to concept drift possibly occurring in the data. It relies on the combination of drift detectors with batch learning and stream mining models. We compare it against batch and incremental learners, including methods relying on active drift detection. Experiments with varied travel mode data sets representing both city and country levels show that the IEBSM method both detects drift in travel mode data and successfully adapts the models to evolving travel mode choice data. The method has a higher rank than batch and stream learners.","sentences":["Travel mode choice (TMC) prediction, which can be formulated as a classification task, helps in understanding what makes citizens choose different modes of transport for individual trips.","This is also a major step towards fostering sustainable transportation.","As behaviour may evolve over time, we also face the question of detecting concept drift in the data.","This necessitates using appropriate methods to address potential concept drift.","In particular, it is necessary to decide whether batch or stream mining methods should be used to develop periodically updated TMC models.","To address the challenge of the development of TMC models, we propose the novel Incremental Ensemble of Batch and Stream Models (IEBSM) method aimed at adapting travel mode choice classifiers to concept drift possibly occurring in the data.","It relies on the combination of drift detectors with batch learning and stream mining models.","We compare it against batch and incremental learners, including methods relying on active drift detection.","Experiments with varied travel mode data sets representing both city and country levels show that the IEBSM method both detects drift in travel mode data and successfully adapts the models to evolving travel mode choice data.","The method has a higher rank than batch and stream learners."],"url":"http://arxiv.org/abs/2404.14017v1","category":"cs.LG"}
{"created":"2024-04-22 09:16:25","title":"Infusion: Preventing Customized Text-to-Image Diffusion from Overfitting","abstract":"Text-to-image (T2I) customization aims to create images that embody specific visual concepts delineated in textual descriptions. However, existing works still face a main challenge, concept overfitting. To tackle this challenge, we first analyze overfitting, categorizing it into concept-agnostic overfitting, which undermines non-customized concept knowledge, and concept-specific overfitting, which is confined to customize on limited modalities, i.e, backgrounds, layouts, styles. To evaluate the overfitting degree, we further introduce two metrics, i.e, Latent Fisher divergence and Wasserstein metric to measure the distribution changes of non-customized and customized concept respectively. Drawing from the analysis, we propose Infusion, a T2I customization method that enables the learning of target concepts to avoid being constrained by limited training modalities, while preserving non-customized knowledge. Remarkably, Infusion achieves this feat with remarkable efficiency, requiring a mere 11KB of trained parameters. Extensive experiments also demonstrate that our approach outperforms state-of-the-art methods in both single and multi-concept customized generation.","sentences":["Text-to-image (T2I) customization aims to create images that embody specific visual concepts delineated in textual descriptions.","However, existing works still face a main challenge, concept overfitting.","To tackle this challenge, we first analyze overfitting, categorizing it into concept-agnostic overfitting, which undermines non-customized concept knowledge, and concept-specific overfitting, which is confined to customize on limited modalities, i.e, backgrounds, layouts, styles.","To evaluate the overfitting degree, we further introduce two metrics, i.e, Latent Fisher divergence and Wasserstein metric to measure the distribution changes of non-customized and customized concept respectively.","Drawing from the analysis, we propose Infusion, a T2I customization method that enables the learning of target concepts to avoid being constrained by limited training modalities, while preserving non-customized knowledge.","Remarkably, Infusion achieves this feat with remarkable efficiency, requiring a mere 11KB of trained parameters.","Extensive experiments also demonstrate that our approach outperforms state-of-the-art methods in both single and multi-concept customized generation."],"url":"http://arxiv.org/abs/2404.14007v1","category":"cs.CV"}
{"created":"2024-04-22 08:43:39","title":"Special Issue on Modified Gravity Approaches to the Tensions of $\u039b$CDM: Goals and Highlights","abstract":"The Special Issue on \"Modified Gravity Approaches to the Tensions of $\\Lambda$CDM\"} in the Universe journal tackles significant challenges faced by the $\\Lambda$CDM model, including discrepancies in the Hubble constant, growth rate of structures, and cosmological anisotropies. These issues suggest foundational cracks in the model, raising questions about the validity of General Relativity, dark energy, and cosmological principles at large scales. This collection brings together leading researchers to delve into Modified Gravity theories as potential solutions. Covering approaches from Scalar-Tensor theories to $f(R,T)$ gravity and beyond, each contribution presents innovative research aimed at addressing the limitations of the $\\Lambda$CDM model. This Special Issue not only highlights the theoretical and empirical strengths of Modified Gravity models but also opens avenues for future investigations, emphasizing the synergy between theoretical advancements and observational evidence to deepen our cosmological understanding.","sentences":["The Special Issue on \"Modified Gravity Approaches to the Tensions of $\\Lambda$CDM\"} in the Universe journal tackles significant challenges faced by the $\\Lambda$CDM model, including discrepancies in the Hubble constant, growth rate of structures, and cosmological anisotropies.","These issues suggest foundational cracks in the model, raising questions about the validity of General Relativity, dark energy, and cosmological principles at large scales.","This collection brings together leading researchers to delve into Modified Gravity theories as potential solutions.","Covering approaches from Scalar-Tensor theories to $f(R,T)$ gravity and beyond, each contribution presents innovative research aimed at addressing the limitations of the $\\Lambda$CDM model.","This Special Issue not only highlights the theoretical and empirical strengths of Modified Gravity models but also opens avenues for future investigations, emphasizing the synergy between theoretical advancements and observational evidence to deepen our cosmological understanding."],"url":"http://arxiv.org/abs/2404.13981v1","category":"gr-qc"}
{"created":"2024-04-22 08:41:43","title":"Modelling Technique for GDPR-compliance: Toward a Comprehensive Solution","abstract":"Data-driven applications and services have been increasingly deployed in all aspects of life including healthcare and medical services in which a huge amount of personal data is collected, aggregated, and processed in a centralised server from various sources. As a consequence, preserving the data privacy and security of these applications is of paramount importance. Since May 2018, the new data protection legislation in the EU/UK, namely the General Data Protection Regulation (GDPR), has come into force and this has called for a critical need for modelling compliance with the GDPR's sophisticated requirements. Existing threat modelling techniques are not designed to model GDPR compliance, particularly in a complex system where personal data is collected, processed, manipulated, and shared with third parties. In this paper, we present a novel comprehensive solution for developing a threat modelling technique to address threats of non-compliance and mitigate them by taking GDPR requirements as the baseline and combining them with the existing security and privacy modelling techniques (i.e., \\textit{STRIDE} and \\textit{LINDDUN}, respectively). For this purpose, we propose a new data flow diagram integrated with the GDPR principles, develop a knowledge base for the non-compliance threats, and leverage an inference engine for reasoning the GDPR non-compliance threats over the knowledge base. Finally, we demonstrate our solution for threats of non-compliance with legal basis and accountability in a telehealth system to show the feasibility and effectiveness of the proposed solution.","sentences":["Data-driven applications and services have been increasingly deployed in all aspects of life including healthcare and medical services in which a huge amount of personal data is collected, aggregated, and processed in a centralised server from various sources.","As a consequence, preserving the data privacy and security of these applications is of paramount importance.","Since May 2018, the new data protection legislation in the EU/UK, namely the General Data Protection Regulation (GDPR), has come into force and this has called for a critical need for modelling compliance with the GDPR's sophisticated requirements.","Existing threat modelling techniques are not designed to model GDPR compliance, particularly in a complex system where personal data is collected, processed, manipulated, and shared with third parties.","In this paper, we present a novel comprehensive solution for developing a threat modelling technique to address threats of non-compliance and mitigate them by taking GDPR requirements as the baseline and combining them with the existing security and privacy modelling techniques (i.e., \\textit{STRIDE} and \\textit{LINDDUN}, respectively).","For this purpose, we propose a new data flow diagram integrated with the GDPR principles, develop a knowledge base for the non-compliance threats, and leverage an inference engine for reasoning the GDPR non-compliance threats over the knowledge base.","Finally, we demonstrate our solution for threats of non-compliance with legal basis and accountability in a telehealth system to show the feasibility and effectiveness of the proposed solution."],"url":"http://arxiv.org/abs/2404.13979v1","category":"cs.CR"}
{"created":"2024-04-22 08:29:00","title":"DEQ-MCL: Discrete-Event Queue-based Monte-Carlo Localization","abstract":"Spatial cognition in hippocampal formation is posited to play a crucial role in the development of self-localization techniques for robots. In this paper, we propose a self-localization approach, DEQ-MCL, based on the discrete event queue hypothesis associated with phase precession within the hippocampal formation. Our method effectively estimates the posterior distribution of states, encompassing both past, present, and future states that are organized as a queue. This approach enables the smoothing of the posterior distribution of past states using current observations and the weighting of the joint distribution by considering the feasibility of future states. Our findings indicate that the proposed method holds promise for augmenting self-localization performance in indoor environments.","sentences":["Spatial cognition in hippocampal formation is posited to play a crucial role in the development of self-localization techniques for robots.","In this paper, we propose a self-localization approach, DEQ-MCL, based on the discrete event queue hypothesis associated with phase precession within the hippocampal formation.","Our method effectively estimates the posterior distribution of states, encompassing both past, present, and future states that are organized as a queue.","This approach enables the smoothing of the posterior distribution of past states using current observations and the weighting of the joint distribution by considering the feasibility of future states.","Our findings indicate that the proposed method holds promise for augmenting self-localization performance in indoor environments."],"url":"http://arxiv.org/abs/2404.13973v1","category":"cs.AI"}
{"created":"2024-04-22 08:28:41","title":"Non-Uniform Exposure Imaging via Neuromorphic Shutter Control","abstract":"By leveraging the blur-noise trade-off, imaging with non-uniform exposures largely extends the image acquisition flexibility in harsh environments. However, the limitation of conventional cameras in perceiving intra-frame dynamic information prevents existing methods from being implemented in the real-world frame acquisition for real-time adaptive camera shutter control. To address this challenge, we propose a novel Neuromorphic Shutter Control (NSC) system to avoid motion blurs and alleviate instant noises, where the extremely low latency of events is leveraged to monitor the real-time motion and facilitate the scene-adaptive exposure. Furthermore, to stabilize the inconsistent Signal-to-Noise Ratio (SNR) caused by the non-uniform exposure times, we propose an event-based image denoising network within a self-supervised learning paradigm, i.e., SEID, exploring the statistics of image noises and inter-frame motion information of events to obtain artificial supervision signals for high-quality imaging in real-world scenes. To illustrate the effectiveness of the proposed NSC, we implement it in hardware by building a hybrid-camera imaging prototype system, with which we collect a real-world dataset containing well-synchronized frames and events in diverse scenarios with different target scenes and motion patterns. Experiments on the synthetic and real-world datasets demonstrate the superiority of our method over state-of-the-art approaches.","sentences":["By leveraging the blur-noise trade-off, imaging with non-uniform exposures largely extends the image acquisition flexibility in harsh environments.","However, the limitation of conventional cameras in perceiving intra-frame dynamic information prevents existing methods from being implemented in the real-world frame acquisition for real-time adaptive camera shutter control.","To address this challenge, we propose a novel Neuromorphic Shutter Control (NSC) system to avoid motion blurs and alleviate instant noises, where the extremely low latency of events is leveraged to monitor the real-time motion and facilitate the scene-adaptive exposure.","Furthermore, to stabilize the inconsistent Signal-to-Noise Ratio (SNR) caused by the non-uniform exposure times, we propose an event-based image denoising network within a self-supervised learning paradigm, i.e., SEID, exploring the statistics of image noises and inter-frame motion information of events to obtain artificial supervision signals for high-quality imaging in real-world scenes.","To illustrate the effectiveness of the proposed NSC, we implement it in hardware by building a hybrid-camera imaging prototype system, with which we collect a real-world dataset containing well-synchronized frames and events in diverse scenarios with different target scenes and motion patterns.","Experiments on the synthetic and real-world datasets demonstrate the superiority of our method over state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.13972v1","category":"cs.CV"}
{"created":"2024-04-22 08:16:07","title":"Protecting Your LLMs with Information Bottleneck","abstract":"The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content. Despite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts. To address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions. The IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor, preserving only essential information for the target LLMs to respond with the expected answer. Moreover, we further consider a situation where the gradient is not visible to be compatible with any LLM. Our empirical evaluations show that IBProtector outperforms current defense methods in mitigating jailbreak attempts, without overly affecting response quality or inference speed. Its effectiveness and adaptability across various attack methods and target LLMs underscore the potential of IBProtector as a novel, transferable defense that bolsters the security of LLMs without requiring modifications to the underlying models.","sentences":["The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content.","Despite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts.","To address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions.","The IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor, preserving only essential information for the target LLMs to respond with the expected answer.","Moreover, we further consider a situation where the gradient is not visible to be compatible with any LLM.","Our empirical evaluations show that IBProtector outperforms current defense methods in mitigating jailbreak attempts, without overly affecting response quality or inference speed.","Its effectiveness and adaptability across various attack methods and target LLMs underscore the potential of IBProtector as a novel, transferable defense that bolsters the security of LLMs without requiring modifications to the underlying models."],"url":"http://arxiv.org/abs/2404.13968v1","category":"cs.CL"}
{"created":"2024-04-22 08:12:21","title":"Banded totally positive matrices and normality for mixed multiple orthogonal polynomials","abstract":"This paper serves as an introduction to banded totally positive matrices, exploring various characterizations and associated properties. A significant result within is the demonstration that the collection of such matrices forms a semigroup, notably including a subset permitting positive bidiagonal factorization. Moreover, the paper applies this concept to investigate step line normality concerning the degrees of associated recursion polynomials. It presents a spectral Favard theorem, ensuring the existence of measures, thereby guaranteeing that these recursion polynomials represent mixed multiple orthogonal polynomials that maintain normality on the step line indices.","sentences":["This paper serves as an introduction to banded totally positive matrices, exploring various characterizations and associated properties.","A significant result within is the demonstration that the collection of such matrices forms a semigroup, notably including a subset permitting positive bidiagonal factorization.","Moreover, the paper applies this concept to investigate step line normality concerning the degrees of associated recursion polynomials.","It presents a spectral Favard theorem, ensuring the existence of measures, thereby guaranteeing that these recursion polynomials represent mixed multiple orthogonal polynomials that maintain normality on the step line indices."],"url":"http://arxiv.org/abs/2404.13965v1","category":"math.CA"}
{"created":"2024-04-22 08:10:38","title":"An Economic Solution to Copyright Challenges of Generative AI","abstract":"Generative artificial intelligence (AI) systems are trained on large data corpora to generate new pieces of text, images, videos, and other media. There is growing concern that such systems may infringe on the copyright interests of training data contributors. To address the copyright challenges of generative AI, we propose a framework that compensates copyright owners proportionally to their contributions to the creation of AI-generated content. The metric for contributions is quantitatively determined by leveraging the probabilistic nature of modern generative AI models and using techniques from cooperative game theory in economics. This framework enables a platform where AI developers benefit from access to high-quality training data, thus improving model performance. Meanwhile, copyright owners receive fair compensation, driving the continued provision of relevant data for generative model training. Experiments demonstrate that our framework successfully identifies the most relevant data sources used in artwork generation, ensuring a fair and interpretable distribution of revenues among copyright owners.","sentences":["Generative artificial intelligence (AI) systems are trained on large data corpora to generate new pieces of text, images, videos, and other media.","There is growing concern that such systems may infringe on the copyright interests of training data contributors.","To address the copyright challenges of generative AI, we propose a framework that compensates copyright owners proportionally to their contributions to the creation of AI-generated content.","The metric for contributions is quantitatively determined by leveraging the probabilistic nature of modern generative AI models and using techniques from cooperative game theory in economics.","This framework enables a platform where AI developers benefit from access to high-quality training data, thus improving model performance.","Meanwhile, copyright owners receive fair compensation, driving the continued provision of relevant data for generative model training.","Experiments demonstrate that our framework successfully identifies the most relevant data sources used in artwork generation, ensuring a fair and interpretable distribution of revenues among copyright owners."],"url":"http://arxiv.org/abs/2404.13964v2","category":"cs.LG"}
{"created":"2024-04-22 07:54:56","title":"A survey of air combat behavior modeling using machine learning","abstract":"With the recent advances in machine learning, creating agents that behave realistically in simulated air combat has become a growing field of interest. This survey explores the application of machine learning techniques for modeling air combat behavior, motivated by the potential to enhance simulation-based pilot training. Current simulated entities tend to lack realistic behavior, and traditional behavior modeling is labor-intensive and prone to loss of essential domain knowledge between development steps. Advancements in reinforcement learning and imitation learning algorithms have demonstrated that agents may learn complex behavior from data, which could be faster and more scalable than manual methods. Yet, making adaptive agents capable of performing tactical maneuvers and operating weapons and sensors still poses a significant challenge. The survey examines applications, behavior model types, prevalent machine learning methods, and the technical and human challenges in developing adaptive and realistically behaving agents. Another challenge is the transfer of agents from learning environments to military simulation systems and the consequent demand for standardization. Four primary recommendations are presented regarding increased emphasis on beyond-visual-range scenarios, multi-agent machine learning and cooperation, utilization of hierarchical behavior models, and initiatives for standardization and research collaboration. These recommendations aim to address current issues and guide the development of more comprehensive, adaptable, and realistic machine learning-based behavior models for air combat applications.","sentences":["With the recent advances in machine learning, creating agents that behave realistically in simulated air combat has become a growing field of interest.","This survey explores the application of machine learning techniques for modeling air combat behavior, motivated by the potential to enhance simulation-based pilot training.","Current simulated entities tend to lack realistic behavior, and traditional behavior modeling is labor-intensive and prone to loss of essential domain knowledge between development steps.","Advancements in reinforcement learning and imitation learning algorithms have demonstrated that agents may learn complex behavior from data, which could be faster and more scalable than manual methods.","Yet, making adaptive agents capable of performing tactical maneuvers and operating weapons and sensors still poses a significant challenge.","The survey examines applications, behavior model types, prevalent machine learning methods, and the technical and human challenges in developing adaptive and realistically behaving agents.","Another challenge is the transfer of agents from learning environments to military simulation systems and the consequent demand for standardization.","Four primary recommendations are presented regarding increased emphasis on beyond-visual-range scenarios, multi-agent machine learning and cooperation, utilization of hierarchical behavior models, and initiatives for standardization and research collaboration.","These recommendations aim to address current issues and guide the development of more comprehensive, adaptable, and realistic machine learning-based behavior models for air combat applications."],"url":"http://arxiv.org/abs/2404.13954v1","category":"cs.LG"}
{"created":"2024-04-22 07:34:28","title":"Autoencoder-assisted Feature Ensemble Net for Incipient Faults","abstract":"Deep learning has shown the great power in the field of fault detection. However, for incipient faults with tiny amplitude, the detection performance of the current deep learning networks (DLNs) is not satisfactory. Even if prior information about the faults is utilized, DLNs can't successfully detect faults 3, 9 and 15 in Tennessee Eastman process (TEP). These faults are notoriously difficult to detect, lacking effective detection technologies in the field of fault detection. In this work, we propose Autoencoder-assisted Feature Ensemble Net (AE-FENet): a deep feature ensemble framework that uses the unsupervised autoencoder to conduct the feature transformation. Compared with the principle component analysis (PCA) technique adopted in the original Feature Ensemble Net (FENet), autoencoder can mine more exact features on incipient faults, which results in the better detection performance of AE-FENet. With same kinds of basic detectors, AE-FENet achieves a state-of-the-art average accuracy over 96% on faults 3, 9 and 15 in TEP, which represents a significant enhancement in performance compared to other methods. Plenty of experiments have been done to extend our framework, proving that DLNs can be utilized efficiently within this architecture.","sentences":["Deep learning has shown the great power in the field of fault detection.","However, for incipient faults with tiny amplitude, the detection performance of the current deep learning networks (DLNs) is not satisfactory.","Even if prior information about the faults is utilized, DLNs can't successfully detect faults 3, 9 and 15 in Tennessee Eastman process (TEP).","These faults are notoriously difficult to detect, lacking effective detection technologies in the field of fault detection.","In this work, we propose Autoencoder-assisted Feature Ensemble Net (AE-FENet): a deep feature ensemble framework that uses the unsupervised autoencoder to conduct the feature transformation.","Compared with the principle component analysis (PCA) technique adopted in the original Feature Ensemble Net (FENet), autoencoder can mine more exact features on incipient faults, which results in the better detection performance of AE-FENet.","With same kinds of basic detectors, AE-FENet achieves a state-of-the-art average accuracy over 96% on faults 3, 9 and 15 in TEP, which represents a significant enhancement in performance compared to other methods.","Plenty of experiments have been done to extend our framework, proving that DLNs can be utilized efficiently within this architecture."],"url":"http://arxiv.org/abs/2404.13941v1","category":"eess.SY"}
{"created":"2024-04-22 07:32:03","title":"A User-Centric Benchmark for Evaluating Large Language Models","abstract":"Large Language Models (LLMs) are essential tools to collaborate with users on different tasks. Evaluating their performance to serve users' needs in real-world scenarios is important. While many benchmarks have been created, they mainly focus on specific predefined model abilities. Few have covered the intended utilization of LLMs by real users. To address this oversight, we propose benchmarking LLMs from a user perspective in both dataset construction and evaluation designs. We first collect 1846 real-world use cases with 15 LLMs from a user study with 712 participants from 23 countries. These self-reported cases form the User Reported Scenarios(URS) dataset with a categorization of 7 user intents. Secondly, on this authentic multi-cultural dataset, we benchmark 10 LLM services on their efficacy in satisfying user needs. Thirdly, we show that our benchmark scores align well with user-reported experience in LLM interactions across diverse intents, both of which emphasize the overlook of subjective scenarios. In conclusion, our study proposes to benchmark LLMs from a user-centric perspective, aiming to facilitate evaluations that better reflect real user needs. The benchmark dataset and code are available at https://github.com/Alice1998/URS.","sentences":["Large Language Models (LLMs) are essential tools to collaborate with users on different tasks.","Evaluating their performance to serve users' needs in real-world scenarios is important.","While many benchmarks have been created, they mainly focus on specific predefined model abilities.","Few have covered the intended utilization of LLMs by real users.","To address this oversight, we propose benchmarking LLMs from a user perspective in both dataset construction and evaluation designs.","We first collect 1846 real-world use cases with 15 LLMs from a user study with 712 participants from 23 countries.","These self-reported cases form the User Reported Scenarios(URS) dataset with a categorization of 7 user intents.","Secondly, on this authentic multi-cultural dataset, we benchmark 10 LLM services on their efficacy in satisfying user needs.","Thirdly, we show that our benchmark scores align well with user-reported experience in LLM interactions across diverse intents, both of which emphasize the overlook of subjective scenarios.","In conclusion, our study proposes to benchmark LLMs from a user-centric perspective, aiming to facilitate evaluations that better reflect real user needs.","The benchmark dataset and code are available at https://github.com/Alice1998/URS."],"url":"http://arxiv.org/abs/2404.13940v2","category":"cs.CL"}
{"created":"2024-04-22 07:27:07","title":"Data-Based System Representation and Synchronization for Multiagent Systems","abstract":"This paper presents novel solutions of the data-based synchronization problem for continuous-time multiagent systems. We consider the cases of homogeneous and heterogeneous systems. First, a data-based representation of the synchronization error dynamics is obtained for homogeneous systems, using input-state data collected from the agents. Then, we show how to extend existing data-based stabilization results to the multiagent case to stabilize the obtained synchronization errors. The proposed method relies on the solution of a set of linear matrix inequalities that are shown to be feasible. Then, we solve the synchronization problem for heterogeneous systems by means of dynamic controllers. Different from existing results, we do not require model knowledge for the followers and the leader. The theoretical results are finally validated using numerical simulations.","sentences":["This paper presents novel solutions of the data-based synchronization problem for continuous-time multiagent systems.","We consider the cases of homogeneous and heterogeneous systems.","First, a data-based representation of the synchronization error dynamics is obtained for homogeneous systems, using input-state data collected from the agents.","Then, we show how to extend existing data-based stabilization results to the multiagent case to stabilize the obtained synchronization errors.","The proposed method relies on the solution of a set of linear matrix inequalities that are shown to be feasible.","Then, we solve the synchronization problem for heterogeneous systems by means of dynamic controllers.","Different from existing results, we do not require model knowledge for the followers and the leader.","The theoretical results are finally validated using numerical simulations."],"url":"http://arxiv.org/abs/2404.13937v1","category":"eess.SY"}
{"created":"2024-04-22 07:01:19","title":"ActSonic: Everyday Activity Recognition on Smart Glasses using Active Acoustic Sensing","abstract":"In this paper, we introduce ActSonic, an intelligent, low-power active acoustic sensing system integrated into eyeglasses. ActSonic is designed to recognize 27 different everyday activities (e.g., eating, drinking, toothbrushing). It only needs a pair of miniature speakers and microphones mounted on each hinge of eyeglasses to emit ultrasonic waves to create an acoustic aura around the body. Based on the position and motion of various body parts, the acoustic signals are reflected with unique patterns captured by the microphone and analyzed by a customized self-supervised deep learning framework to infer the performed activities. ActSonic was deployed in a user study with 19 participants across 19 households to evaluate its efficacy. Without requiring any training data from a new user (leave-one-participant-out evaluation), ActSonic was able to detect 27 activities with an inference resolution of 1 second, achieving an average F1-score of 86.6% in an unconstrained setting and 93.4% in a prompted setting.","sentences":["In this paper, we introduce ActSonic, an intelligent, low-power active acoustic sensing system integrated into eyeglasses.","ActSonic is designed to recognize 27 different everyday activities (e.g., eating, drinking, toothbrushing).","It only needs a pair of miniature speakers and microphones mounted on each hinge of eyeglasses to emit ultrasonic waves to create an acoustic aura around the body.","Based on the position and motion of various body parts, the acoustic signals are reflected with unique patterns captured by the microphone and analyzed by a customized self-supervised deep learning framework to infer the performed activities.","ActSonic was deployed in a user study with 19 participants across 19 households to evaluate its efficacy.","Without requiring any training data from a new user (leave-one-participant-out evaluation), ActSonic was able to detect 27 activities with an inference resolution of 1 second, achieving an average F1-score of 86.6% in an unconstrained setting and 93.4% in a prompted setting."],"url":"http://arxiv.org/abs/2404.13924v1","category":"cs.HC"}
{"created":"2024-04-22 07:00:17","title":"MaterialSeg3D: Segmenting Dense Materials from 2D Priors for 3D Assets","abstract":"Driven by powerful image diffusion models, recent research has achieved the automatic creation of 3D objects from textual or visual guidance. By performing score distillation sampling (SDS) iteratively across different views, these methods succeed in lifting 2D generative prior to the 3D space. However, such a 2D generative image prior bakes the effect of illumination and shadow into the texture. As a result, material maps optimized by SDS inevitably involve spurious correlated components. The absence of precise material definition makes it infeasible to relight the generated assets reasonably in novel scenes, which limits their application in downstream scenarios. In contrast, humans can effortlessly circumvent this ambiguity by deducing the material of the object from its appearance and semantics. Motivated by this insight, we propose MaterialSeg3D, a 3D asset material generation framework to infer underlying material from the 2D semantic prior. Based on such a prior model, we devise a mechanism to parse material in 3D space. We maintain a UV stack, each map of which is unprojected from a specific viewpoint. After traversing all viewpoints, we fuse the stack through a weighted voting scheme and then employ region unification to ensure the coherence of the object parts. To fuel the learning of semantics prior, we collect a material dataset, named Materialized Individual Objects (MIO), which features abundant images, diverse categories, and accurate annotations. Extensive quantitative and qualitative experiments demonstrate the effectiveness of our method.","sentences":["Driven by powerful image diffusion models, recent research has achieved the automatic creation of 3D objects from textual or visual guidance.","By performing score distillation sampling (SDS) iteratively across different views, these methods succeed in lifting 2D generative prior to the 3D space.","However, such a 2D generative image prior bakes the effect of illumination and shadow into the texture.","As a result, material maps optimized by SDS inevitably involve spurious correlated components.","The absence of precise material definition makes it infeasible to relight the generated assets reasonably in novel scenes, which limits their application in downstream scenarios.","In contrast, humans can effortlessly circumvent this ambiguity by deducing the material of the object from its appearance and semantics.","Motivated by this insight, we propose MaterialSeg3D, a 3D asset material generation framework to infer underlying material from the 2D semantic prior.","Based on such a prior model, we devise a mechanism to parse material in 3D space.","We maintain a UV stack, each map of which is unprojected from a specific viewpoint.","After traversing all viewpoints, we fuse the stack through a weighted voting scheme and then employ region unification to ensure the coherence of the object parts.","To fuel the learning of semantics prior, we collect a material dataset, named Materialized Individual Objects (MIO), which features abundant images, diverse categories, and accurate annotations.","Extensive quantitative and qualitative experiments demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.13923v1","category":"cs.CV"}
{"created":"2024-04-22 06:58:22","title":"Open Datasets for Satellite Radio Resource Control","abstract":"In Non-Terrestrial Networks (NTN), achieving effective radio resource allocation across multi-satellite system, encompassing efficient channel and bandwidth allocation, effective beam management, power control and interference mitigation, poses significant challenges due to the varying satellite links and highly dynamic nature of user traffic. This calls for the development of an intelligent decision-making controller using Artificial Intelligence (AI) to efficiently manage resources in this complex environment. In this context, open datasets can play a crucial role in driving new advancement and facilitating research. Recognizing the significance, this paper aims to contribute the satellite communication research community by providing various open datasets that incorporate realistic traffic flow enabling a variety of uses cases. The primary objective of sharing these datasets is to facilitate the development and benchmarking of advanced resource management solutions, thereby improving the overall satellite communication systems. Furthermore, an application example focused on beam placement optimization via terminal clustering is provided. This assists in optimizing beam allocation task, enabling adaptive beamforming to effectively meet spatiotemporally varying user traffic demands and optimize resource utilization.","sentences":["In Non-Terrestrial Networks (NTN), achieving effective radio resource allocation across multi-satellite system, encompassing efficient channel and bandwidth allocation, effective beam management, power control and interference mitigation, poses significant challenges due to the varying satellite links and highly dynamic nature of user traffic.","This calls for the development of an intelligent decision-making controller using Artificial Intelligence (AI) to efficiently manage resources in this complex environment.","In this context, open datasets can play a crucial role in driving new advancement and facilitating research.","Recognizing the significance, this paper aims to contribute the satellite communication research community by providing various open datasets that incorporate realistic traffic flow enabling a variety of uses cases.","The primary objective of sharing these datasets is to facilitate the development and benchmarking of advanced resource management solutions, thereby improving the overall satellite communication systems.","Furthermore, an application example focused on beam placement optimization via terminal clustering is provided.","This assists in optimizing beam allocation task, enabling adaptive beamforming to effectively meet spatiotemporally varying user traffic demands and optimize resource utilization."],"url":"http://arxiv.org/abs/2404.13920v1","category":"eess.SP"}
{"created":"2024-04-22 06:57:43","title":"Navigating the Path of Writing: Outline-guided Text Generation with Large Language Models","abstract":"Large Language Models (LLMs) have significantly impacted the writing process, enabling collaborative content creation and enhancing productivity. However, generating high-quality, user-aligned text remains challenging. In this paper, we propose Writing Path, a framework that uses explicit outlines to guide LLMs in generating goal-oriented, high-quality pieces of writing. Our approach draws inspiration from structured writing planning and reasoning paths, focusing on capturing and reflecting user intentions throughout the writing process. We construct a diverse dataset from unstructured blog posts to benchmark writing performance and introduce a comprehensive evaluation framework assessing the quality of outlines and generated texts. Our evaluations with GPT-3.5-turbo, GPT-4, and HyperCLOVA X demonstrate that the Writing Path approach significantly enhances text quality according to both LLMs and human evaluations. This study highlights the potential of integrating writing-specific techniques into LLMs to enhance their ability to meet the diverse writing needs of users.","sentences":["Large Language Models (LLMs) have significantly impacted the writing process, enabling collaborative content creation and enhancing productivity.","However, generating high-quality, user-aligned text remains challenging.","In this paper, we propose Writing Path, a framework that uses explicit outlines to guide LLMs in generating goal-oriented, high-quality pieces of writing.","Our approach draws inspiration from structured writing planning and reasoning paths, focusing on capturing and reflecting user intentions throughout the writing process.","We construct a diverse dataset from unstructured blog posts to benchmark writing performance and introduce a comprehensive evaluation framework assessing the quality of outlines and generated texts.","Our evaluations with GPT-3.5-turbo, GPT-4, and HyperCLOVA X demonstrate that the Writing Path approach significantly enhances text quality according to both LLMs and human evaluations.","This study highlights the potential of integrating writing-specific techniques into LLMs to enhance their ability to meet the diverse writing needs of users."],"url":"http://arxiv.org/abs/2404.13919v1","category":"cs.CL"}
{"created":"2024-04-22 06:42:21","title":"Integrated Gradient Correlation: a Dataset-wise Attribution Method","abstract":"Attribution methods are primarily designed to study the distribution of input component contributions to individual model predictions. However, some research applications require a summary of attribution patterns across the entire dataset to facilitate the interpretability of the scrutinized models. In this paper, we present a new method called Integrated Gradient Correlation (IGC) that relates dataset-wise attributions to a model prediction score and enables region-specific analysis by a direct summation over associated components. We demonstrate our method on scalar predictions with the study of image feature representation in the brain from fMRI neural signals and the estimation of neural population receptive fields (NSD dataset), as well as on categorical predictions with the investigation of handwritten digit recognition (MNIST dataset). The resulting IGC attributions show selective patterns, revealing underlying model strategies coherent with their respective objectives.","sentences":["Attribution methods are primarily designed to study the distribution of input component contributions to individual model predictions.","However, some research applications require a summary of attribution patterns across the entire dataset to facilitate the interpretability of the scrutinized models.","In this paper, we present a new method called Integrated Gradient Correlation (IGC) that relates dataset-wise attributions to a model prediction score and enables region-specific analysis by a direct summation over associated components.","We demonstrate our method on scalar predictions with the study of image feature representation in the brain from fMRI neural signals and the estimation of neural population receptive fields (NSD dataset), as well as on categorical predictions with the investigation of handwritten digit recognition (MNIST dataset).","The resulting IGC attributions show selective patterns, revealing underlying model strategies coherent with their respective objectives."],"url":"http://arxiv.org/abs/2404.13910v1","category":"cs.LG"}
{"created":"2024-04-22 06:33:28","title":"Generating Attractive and Authentic Copywriting from Customer Reviews","abstract":"The goal of product copywriting is to capture the interest of potential buyers by emphasizing the features of products through text descriptions. As e-commerce platforms offer a wide range of services, it's becoming essential to dynamically adjust the styles of these auto-generated descriptions. Typical approaches to copywriting generation often rely solely on specified product attributes, which may result in dull and repetitive content. To tackle this issue, we propose to generate copywriting based on customer reviews, as they provide firsthand practical experiences with products, offering a richer source of information than just product attributes. We have developed a sequence-to-sequence framework, enhanced with reinforcement learning, to produce copywriting that is attractive, authentic, and rich in information. Our framework outperforms all existing baseline and zero-shot large language models, including LLaMA-2-chat-7B and GPT-3.5, in terms of both attractiveness and faithfulness. Furthermore, this work features the use of LLMs for aspect-based summaries collection and argument allure assessment. Experiments demonstrate the effectiveness of using LLMs for marketing domain corpus construction. The code and the dataset is publicly available at: https://github.com/YuXiangLin1234/Copywriting-Generation.","sentences":["The goal of product copywriting is to capture the interest of potential buyers by emphasizing the features of products through text descriptions.","As e-commerce platforms offer a wide range of services, it's becoming essential to dynamically adjust the styles of these auto-generated descriptions.","Typical approaches to copywriting generation often rely solely on specified product attributes, which may result in dull and repetitive content.","To tackle this issue, we propose to generate copywriting based on customer reviews, as they provide firsthand practical experiences with products, offering a richer source of information than just product attributes.","We have developed a sequence-to-sequence framework, enhanced with reinforcement learning, to produce copywriting that is attractive, authentic, and rich in information.","Our framework outperforms all existing baseline and zero-shot large language models, including LLaMA-2-chat-7B and GPT-3.5, in terms of both attractiveness and faithfulness.","Furthermore, this work features the use of LLMs for aspect-based summaries collection and argument allure assessment.","Experiments demonstrate the effectiveness of using LLMs for marketing domain corpus construction.","The code and the dataset is publicly available at: https://github.com/YuXiangLin1234/Copywriting-Generation."],"url":"http://arxiv.org/abs/2404.13906v1","category":"cs.CL"}
{"created":"2024-04-22 06:18:37","title":"Towards Better Text-to-Image Generation Alignment via Attention Modulation","abstract":"In text-to-image generation tasks, the advancements of diffusion models have facilitated the fidelity of generated results. However, these models encounter challenges when processing text prompts containing multiple entities and attributes. The uneven distribution of attention results in the issues of entity leakage and attribute misalignment. Training from scratch to address this issue requires numerous labeled data and is resource-consuming. Motivated by this, we propose an attribution-focusing mechanism, a training-free phase-wise mechanism by modulation of attention for diffusion model. One of our core ideas is to guide the model to concentrate on the corresponding syntactic components of the prompt at distinct timesteps. To achieve this, we incorporate a temperature control mechanism within the early phases of the self-attention modules to mitigate entity leakage issues. An object-focused masking scheme and a phase-wise dynamic weight control mechanism are integrated into the cross-attention modules, enabling the model to discern the affiliation of semantic information between entities more effectively. The experimental results in various alignment scenarios demonstrate that our model attain better image-text alignment with minimal additional computational cost.","sentences":["In text-to-image generation tasks, the advancements of diffusion models have facilitated the fidelity of generated results.","However, these models encounter challenges when processing text prompts containing multiple entities and attributes.","The uneven distribution of attention results in the issues of entity leakage and attribute misalignment.","Training from scratch to address this issue requires numerous labeled data and is resource-consuming.","Motivated by this, we propose an attribution-focusing mechanism, a training-free phase-wise mechanism by modulation of attention for diffusion model.","One of our core ideas is to guide the model to concentrate on the corresponding syntactic components of the prompt at distinct timesteps.","To achieve this, we incorporate a temperature control mechanism within the early phases of the self-attention modules to mitigate entity leakage issues.","An object-focused masking scheme and a phase-wise dynamic weight control mechanism are integrated into the cross-attention modules, enabling the model to discern the affiliation of semantic information between entities more effectively.","The experimental results in various alignment scenarios demonstrate that our model attain better image-text alignment with minimal additional computational cost."],"url":"http://arxiv.org/abs/2404.13899v1","category":"cs.CL"}
{"created":"2024-04-22 06:05:35","title":"Optimal Design for Human Feedback","abstract":"Learning of preference models from human feedback has been central to recent advances in artificial intelligence. Motivated by this progress, and the cost of obtaining high-quality human annotations, we study the problem of data collection for learning preference models. The key idea in our work is to generalize optimal designs, a tool for computing efficient data logging policies, to ranked lists. To show the generality of our ideas, we study both absolute and relative feedback on items in the list. We design efficient algorithms for both settings and analyze them. We prove that our preference model estimators improve with more data and so does the ranking error under the estimators. Finally, we experiment with several synthetic and real-world datasets to show the statistical efficiency of our algorithms.","sentences":["Learning of preference models from human feedback has been central to recent advances in artificial intelligence.","Motivated by this progress, and the cost of obtaining high-quality human annotations, we study the problem of data collection for learning preference models.","The key idea in our work is to generalize optimal designs, a tool for computing efficient data logging policies, to ranked lists.","To show the generality of our ideas, we study both absolute and relative feedback on items in the list.","We design efficient algorithms for both settings and analyze them.","We prove that our preference model estimators improve with more data and so does the ranking error under the estimators.","Finally, we experiment with several synthetic and real-world datasets to show the statistical efficiency of our algorithms."],"url":"http://arxiv.org/abs/2404.13895v1","category":"cs.LG"}
{"created":"2024-04-22 05:46:40","title":"Retrieval-Augmented Audio Deepfake Detection","abstract":"With recent advances in speech synthesis including text-to-speech (TTS) and voice conversion (VC) systems enabling the generation of ultra-realistic audio deepfakes, there is growing concern about their potential misuse. However, most deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a single model, resulting in performance bottlenecks and transparency issues. Inspired by retrieval-augmented generation (RAG), we propose a retrieval-augmented detection (RAD) framework that augments test samples with similar retrieved samples for enhanced detection. We also extend the multi-fusion attentive classifier to integrate it with our proposed RAD framework. Extensive experiments show the superior performance of the proposed RAD framework over baseline methods, achieving state-of-the-art results on the ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets. Further sample analysis indicates that the retriever consistently retrieves samples mostly from the same speaker with acoustic characteristics highly consistent with the query audio, thereby improving detection performance.","sentences":["With recent advances in speech synthesis including text-to-speech (TTS) and voice conversion (VC) systems enabling the generation of ultra-realistic audio deepfakes, there is growing concern about their potential misuse.","However, most deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a single model, resulting in performance bottlenecks and transparency issues.","Inspired by retrieval-augmented generation (RAG), we propose a retrieval-augmented detection (RAD) framework that augments test samples with similar retrieved samples for enhanced detection.","We also extend the multi-fusion attentive classifier to integrate it with our proposed RAD framework.","Extensive experiments show the superior performance of the proposed RAD framework over baseline methods, achieving state-of-the-art results on the ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets.","Further sample analysis indicates that the retriever consistently retrieves samples mostly from the same speaker with acoustic characteristics highly consistent with the query audio, thereby improving detection performance."],"url":"http://arxiv.org/abs/2404.13892v2","category":"cs.SD"}
{"created":"2024-04-22 05:37:22","title":"Minimizing Weighted Counterfactual Regret with Optimistic Online Mirror Descent","abstract":"Counterfactual regret minimization (CFR) is a family of algorithms for effectively solving imperfect-information games. It decomposes the total regret into counterfactual regrets, utilizing local regret minimization algorithms, such as Regret Matching (RM) or RM+, to minimize them. Recent research establishes a connection between Online Mirror Descent (OMD) and RM+, paving the way for an optimistic variant PRM+ and its extension PCFR+. However, PCFR+ assigns uniform weights for each iteration when determining regrets, leading to substantial regrets when facing dominated actions. This work explores minimizing weighted counterfactual regret with optimistic OMD, resulting in a novel CFR variant PDCFR+. It integrates PCFR+ and Discounted CFR (DCFR) in a principled manner, swiftly mitigating negative effects of dominated actions and consistently leveraging predictions to accelerate convergence. Theoretical analyses prove that PDCFR+ converges to a Nash equilibrium, particularly under distinct weighting schemes for regrets and average strategies. Experimental results demonstrate PDCFR+'s fast convergence in common imperfect-information games. The code is available at https://github.com/rpSebastian/PDCFRPlus.","sentences":["Counterfactual regret minimization (CFR) is a family of algorithms for effectively solving imperfect-information games.","It decomposes the total regret into counterfactual regrets, utilizing local regret minimization algorithms, such as Regret Matching (RM) or RM+, to minimize them.","Recent research establishes a connection between Online Mirror Descent (OMD) and RM+, paving the way for an optimistic variant PRM+ and its extension PCFR+.","However, PCFR+ assigns uniform weights for each iteration when determining regrets, leading to substantial regrets when facing dominated actions.","This work explores minimizing weighted counterfactual regret with optimistic OMD, resulting in a novel CFR variant PDCFR+.","It integrates PCFR+ and Discounted CFR (DCFR) in a principled manner, swiftly mitigating negative effects of dominated actions and consistently leveraging predictions to accelerate convergence.","Theoretical analyses prove that PDCFR+ converges to a Nash equilibrium, particularly under distinct weighting schemes for regrets and average strategies.","Experimental results demonstrate PDCFR+'s fast convergence in common imperfect-information games.","The code is available at https://github.com/rpSebastian/PDCFRPlus."],"url":"http://arxiv.org/abs/2404.13891v1","category":"cs.LG"}
{"created":"2024-04-22 17:59:57","title":"AutoAD III: The Prequel -- Back to the Pixels","abstract":"Generating Audio Description (AD) for movies is a challenging task that requires fine-grained visual understanding and an awareness of the characters and their names. Currently, visual language models for AD generation are limited by a lack of suitable training data, and also their evaluation is hampered by using performance measures not specialized to the AD domain. In this paper, we make three contributions: (i) We propose two approaches for constructing AD datasets with aligned video data, and build training and evaluation datasets using these. These datasets will be publicly released; (ii) We develop a Q-former-based architecture which ingests raw video and generates AD, using frozen pre-trained visual encoders and large language models; and (iii) We provide new evaluation metrics to benchmark AD quality that are well-matched to human performance. Taken together, we improve the state of the art on AD generation.","sentences":["Generating Audio Description (AD) for movies is a challenging task that requires fine-grained visual understanding and an awareness of the characters and their names.","Currently, visual language models for AD generation are limited by a lack of suitable training data, and also their evaluation is hampered by using performance measures not specialized to the AD domain.","In this paper, we make three contributions: (i) We propose two approaches for constructing AD datasets with aligned video data, and build training and evaluation datasets using these.","These datasets will be publicly released; (ii) We develop a Q-former-based architecture which ingests raw video and generates AD, using frozen pre-trained visual encoders and large language models; and (iii) We provide new evaluation metrics to benchmark AD quality that are well-matched to human performance.","Taken together, we improve the state of the art on AD generation."],"url":"http://arxiv.org/abs/2404.14412v1","category":"cs.CV"}
{"created":"2024-04-22 17:05:29","title":"A Genetic Algorithm For Convex Hull Optimisation","abstract":"Computationally efficient and automated generation of convex hulls is desirable for high throughput materials discovery of thermodynamically stable multi-species crystal structures. A convex hull genetic algorithm is proposed that uses methodology adapted from multi-objective optimisation techniques to optimise the convex hull itself as an object, enabling efficient discovery of convex hulls for N >= 2 species. This method, when tested on a LiSi system utilising pre-trained machine learned potentials, was found to be able to efficiently discover reported structures as well as new potential LiSi candidate structures.","sentences":["Computationally efficient and automated generation of convex hulls is desirable for high throughput materials discovery of thermodynamically stable multi-species crystal structures.","A convex hull genetic algorithm is proposed that uses methodology adapted from multi-objective optimisation techniques to optimise the convex hull itself as an object, enabling efficient discovery of convex hulls for N >= 2 species.","This method, when tested on a LiSi system utilising pre-trained machine learned potentials, was found to be able to efficiently discover reported structures as well as new potential LiSi candidate structures."],"url":"http://arxiv.org/abs/2404.14354v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 16:58:37","title":"Heterogeneous Face Recognition Using Domain Invariant Units","abstract":"Heterogeneous Face Recognition (HFR) aims to expand the applicability of Face Recognition (FR) systems to challenging scenarios, enabling the matching of face images across different domains, such as matching thermal images to visible spectra. However, the development of HFR systems is challenging because of the significant domain gap between modalities and the lack of availability of large-scale paired multi-channel data. In this work, we leverage a pretrained face recognition model as a teacher network to learn domaininvariant network layers called Domain-Invariant Units (DIU) to reduce the domain gap. The proposed DIU can be trained effectively even with a limited amount of paired training data, in a contrastive distillation framework. This proposed approach has the potential to enhance pretrained models, making them more adaptable to a wider range of variations in data. We extensively evaluate our approach on multiple challenging benchmarks, demonstrating superior performance compared to state-of-the-art methods.","sentences":["Heterogeneous Face Recognition (HFR) aims to expand the applicability of Face Recognition (FR) systems to challenging scenarios, enabling the matching of face images across different domains, such as matching thermal images to visible spectra.","However, the development of HFR systems is challenging because of the significant domain gap between modalities and the lack of availability of large-scale paired multi-channel data.","In this work, we leverage a pretrained face recognition model as a teacher network to learn domaininvariant network layers called Domain-Invariant Units (DIU) to reduce the domain gap.","The proposed DIU can be trained effectively even with a limited amount of paired training data, in a contrastive distillation framework.","This proposed approach has the potential to enhance pretrained models, making them more adaptable to a wider range of variations in data.","We extensively evaluate our approach on multiple challenging benchmarks, demonstrating superior performance compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.14343v1","category":"cs.CV"}
{"created":"2024-04-22 16:56:43","title":"Zero-shot Cross-lingual Stance Detection via Adversarial Language Adaptation","abstract":"Stance detection has been widely studied as the task of determining if a social media post is positive, negative or neutral towards a specific issue, such as support towards vaccines. Research in stance detection has however often been limited to a single language and, where more than one language has been studied, research has focused on few-shot settings, overlooking the challenges of developing a zero-shot cross-lingual stance detection model. This paper makes the first such effort by introducing a novel approach to zero-shot cross-lingual stance detection, Multilingual Translation-Augmented BERT (MTAB), aiming to enhance the performance of a cross-lingual classifier in the absence of explicit training data for target languages. Our technique employs translation augmentation to improve zero-shot performance and pairs it with adversarial learning to further boost model efficacy. Through experiments on datasets labeled for stance towards vaccines in four languages English, German, French, Italian. We demonstrate the effectiveness of our proposed approach, showcasing improved results in comparison to a strong baseline model as well as ablated versions of our model. Our experiments demonstrate the effectiveness of model components, not least the translation-augmented data as well as the adversarial learning component, to the improved performance of the model. We have made our source code accessible on GitHub.","sentences":["Stance detection has been widely studied as the task of determining if a social media post is positive, negative or neutral towards a specific issue, such as support towards vaccines.","Research in stance detection has however often been limited to a single language and, where more than one language has been studied, research has focused on few-shot settings, overlooking the challenges of developing a zero-shot cross-lingual stance detection model.","This paper makes the first such effort by introducing a novel approach to zero-shot cross-lingual stance detection, Multilingual Translation-Augmented BERT (MTAB), aiming to enhance the performance of a cross-lingual classifier in the absence of explicit training data for target languages.","Our technique employs translation augmentation to improve zero-shot performance and pairs it with adversarial learning to further boost model efficacy.","Through experiments on datasets labeled for stance towards vaccines in four languages English, German, French, Italian.","We demonstrate the effectiveness of our proposed approach, showcasing improved results in comparison to a strong baseline model as well as ablated versions of our model.","Our experiments demonstrate the effectiveness of model components, not least the translation-augmented data as well as the adversarial learning component, to the improved performance of the model.","We have made our source code accessible on GitHub."],"url":"http://arxiv.org/abs/2404.14339v1","category":"cs.CL"}
{"created":"2024-04-22 16:41:14","title":"Comparison of Empirical Models of Ionospheric Heating to Global Simulations","abstract":"Intense currents produced during geomagnetic storms dissipate energy in the ionosphere through Joule heating. This dissipation has significant space weather effects, and thus it is important to determine the ability of physics-based simulations to replicate real events quantitatively. Several empirical models estimate Joule heating based on ionospheric currents using the AE index. In this study, we select 11 magnetic storm simulations from the CCMC database and compare the integrated Joule heating in the simulations with the results of empirical models. We also use the SWMF global magnetohydrodynamic simulations for 12 storms to reproduce the correlation between the simulated AE index and simulated Joule heating. We find that the scale factors in the empirical models are half what is predicted by the SWMF simulations.","sentences":["Intense currents produced during geomagnetic storms dissipate energy in the ionosphere through Joule heating.","This dissipation has significant space weather effects, and thus it is important to determine the ability of physics-based simulations to replicate real events quantitatively.","Several empirical models estimate Joule heating based on ionospheric currents using the AE index.","In this study, we select 11 magnetic storm simulations from the CCMC database and compare the integrated Joule heating in the simulations with the results of empirical models.","We also use the SWMF global magnetohydrodynamic simulations for 12 storms to reproduce the correlation between the simulated AE index and simulated Joule heating.","We find that the scale factors in the empirical models are half what is predicted by the SWMF simulations."],"url":"http://arxiv.org/abs/2404.14330v1","category":"astro-ph.EP"}
{"created":"2024-04-22 16:40:11","title":"X-Ray: A Sequential 3D Representation for Generation","abstract":"In this paper, we introduce X-Ray, an innovative approach to 3D generation that employs a new sequential representation, drawing inspiration from the depth-revealing capabilities of X-Ray scans to meticulously capture both the external and internal features of objects. Central to our method is the utilization of ray casting techniques originating from the camera's viewpoint, meticulously recording the geometric and textural details encountered across all intersected surfaces. This process efficiently condenses complete objects or scenes into a multi-frame format, just like videos. Such a structure ensures the 3D representation is composed solely of critical surface information. Highlighting the practicality and adaptability of our X-Ray representation, we showcase its utility in synthesizing 3D objects, employing a network architecture akin to that used in video diffusion models. The outcomes reveal our representation's superior performance in enhancing both the accuracy and efficiency of 3D synthesis, heralding new directions for ongoing research and practical implementations in the field.","sentences":["In this paper, we introduce X-Ray, an innovative approach to 3D generation that employs a new sequential representation, drawing inspiration from the depth-revealing capabilities of X-Ray scans to meticulously capture both the external and internal features of objects.","Central to our method is the utilization of ray casting techniques originating from the camera's viewpoint, meticulously recording the geometric and textural details encountered across all intersected surfaces.","This process efficiently condenses complete objects or scenes into a multi-frame format, just like videos.","Such a structure ensures the 3D representation is composed solely of critical surface information.","Highlighting the practicality and adaptability of our X-Ray representation, we showcase its utility in synthesizing 3D objects, employing a network architecture akin to that used in video diffusion models.","The outcomes reveal our representation's superior performance in enhancing both the accuracy and efficiency of 3D synthesis, heralding new directions for ongoing research and practical implementations in the field."],"url":"http://arxiv.org/abs/2404.14329v1","category":"cs.CV"}
{"created":"2024-04-22 16:11:12","title":"Cryogenic sapphire optical reference cavity with crystalline coatings at $\\mathrm{ 1 \\times 10^{-16}}$ fractional instability","abstract":"The frequency stability of a laser locked to an optical reference cavity is fundamentally limited by thermal noise in the cavity length. These fluctuations are linked to material dissipation, which depends both on the temperature of the optical components and the material properties. Here, the design and experimental characterization of a sapphire optical cavity operated at 10 K with crystalline coatings at 1069 nm is presented. Theoretical estimates of the thermo-mechanical noise indicate a thermal noise floor below $\\mathrm{4.5\\times10^{-18}}$. Major technical noise contributions including vibrations, temperature fluctuations, and residual amplitude modulation are characterized in detail. The short-term performance is measured via a three-cornered hat analysis with two other cavity-stabilized lasers, yielding a noise floor of $1\\times10^{-16}$. The long-term performance is measured against an optical lattice clock, indicating cavity stability at the level of $2\\times10^{-15}$ for averaging times up to 10,000 s.","sentences":["The frequency stability of a laser locked to an optical reference cavity is fundamentally limited by thermal noise in the cavity length.","These fluctuations are linked to material dissipation, which depends both on the temperature of the optical components and the material properties.","Here, the design and experimental characterization of a sapphire optical cavity operated at 10 K with crystalline coatings at 1069 nm is presented.","Theoretical estimates of the thermo-mechanical noise indicate a thermal noise floor below $\\mathrm{4.5\\times10^{-18}}$. Major technical noise contributions including vibrations, temperature fluctuations, and residual amplitude modulation are characterized in detail.","The short-term performance is measured via a three-cornered hat analysis with two other cavity-stabilized lasers, yielding a noise floor of $1\\times10^{-16}$. The long-term performance is measured against an optical lattice clock, indicating cavity stability at the level of $2\\times10^{-15}$ for averaging times up to 10,000 s."],"url":"http://arxiv.org/abs/2404.14310v1","category":"physics.optics"}
{"created":"2024-04-22 15:26:59","title":"Mass-radius relationships and contraction of condensed planets by cooling or despinning","abstract":"Condensed planets contract or expand as their temperature changes. With the exception of the effect of phase changes, this phenomenon is generally interpreted as being solely related to the thermal expansivity of the planet's components. However, changes in density affect pressure and gravity and, consequently, the planet's compressibility. A planet's radius is also linked to its rate of rotation. Here again, changes in pressure, gravity and compressibility are coupled. In this article we clarify how the radius of a condensed planet changes with temperature and rotation, using a simple and rigorous thermodynamic model. We consider condensed materials to obey a simple equation of state which generalizes a polytopic EoS as temperature varies. Using this equation, we build simple models of condensed planet's interiors including exoplanets, derive their mass-radius relationships, and study the dependence of their radius with temperature and rotation rate. We show that it depends crucially on the value of $\\rho_s g R/K_s$ ($\\rho_s$ being surface density, $g$ gravity, $R$ radius, $K_s$ surface incompressibility). This non-dimensional number is also the ratio of the dissipation number which appears in compressible convection and the Grune\\\"isen mineralogic parameter. While the radius of small planets depends on temperature, this is not the case for large planets with large dissipation numbers; Earth and a super-Earth like CoRoT-7b are in something of an intermediate state, with a moderately temperature-dependent radius. Similarly, while the radius of these two planets are functions of their rotation rates, this is not the case for smaller or larger planets.","sentences":["Condensed planets contract or expand as their temperature changes.","With the exception of the effect of phase changes, this phenomenon is generally interpreted as being solely related to the thermal expansivity of the planet's components.","However, changes in density affect pressure and gravity and, consequently, the planet's compressibility.","A planet's radius is also linked to its rate of rotation.","Here again, changes in pressure, gravity and compressibility are coupled.","In this article we clarify how the radius of a condensed planet changes with temperature and rotation, using a simple and rigorous thermodynamic model.","We consider condensed materials to obey a simple equation of state which generalizes a polytopic EoS as temperature varies.","Using this equation, we build simple models of condensed planet's interiors including exoplanets, derive their mass-radius relationships, and study the dependence of their radius with temperature and rotation rate.","We show that it depends crucially on the value of $\\rho_s g R/K_s$ ($\\rho_s$ being surface density, $g$ gravity, $R$ radius, $K_s$ surface incompressibility).","This non-dimensional number is also the ratio of the dissipation number which appears in compressible convection and the Grune\\\"isen mineralogic parameter.","While the radius of small planets depends on temperature, this is not the case for large planets with large dissipation numbers; Earth and a super-Earth like CoRoT-7b are in something of an intermediate state, with a moderately temperature-dependent radius.","Similarly, while the radius of these two planets are functions of their rotation rates, this is not the case for smaller or larger planets."],"url":"http://arxiv.org/abs/2404.14278v1","category":"astro-ph.EP"}
{"created":"2024-04-22 15:23:30","title":"Maximally informative feature selection using Information Imbalance: Application to COVID-19 severity prediction","abstract":"Clinical databases typically include, for each patient, many heterogeneous features, for example blood exams, the clinical history before the onset of the disease, the evolution of the symptoms, the results of imaging exams, and many others. We here propose to exploit a recently developed statistical approach, the Information Imbalance, to compare different subsets of patient features, and automatically select the set of features which is maximally informative for a given clinical purpose, especially in minority classes. We adapt the Information Imbalance approach to work in a clinical framework, where patient features are often categorical and are generally available only for a fraction of the patients. We apply this algorithm to a data set of ~ 1,300 patients treated for COVID-19 in Udine hospital before October 2021. Using this approach, we find combinations of features which, if used in combination, are maximally informative of the clinical fate and of the severity of the disease. The optimal number of features, which is determined automatically, turns out to be between 10 and 15. These features can be measured at admission. The approach can be used also if the features are available only for a fraction of the patients, does not require imputation and, importantly, is able to automatically select features with small inter-feature correlation. Clinical insights deriving from this study are also discussed.","sentences":["Clinical databases typically include, for each patient, many heterogeneous features, for example blood exams, the clinical history before the onset of the disease, the evolution of the symptoms, the results of imaging exams, and many others.","We here propose to exploit a recently developed statistical approach, the Information Imbalance, to compare different subsets of patient features, and automatically select the set of features which is maximally informative for a given clinical purpose, especially in minority classes.","We adapt the Information Imbalance approach to work in a clinical framework, where patient features are often categorical and are generally available only for a fraction of the patients.","We apply this algorithm to a data set of ~ 1,300 patients treated for COVID-19 in Udine hospital before October 2021.","Using this approach, we find combinations of features which, if used in combination, are maximally informative of the clinical fate and of the severity of the disease.","The optimal number of features, which is determined automatically, turns out to be between 10 and 15.","These features can be measured at admission.","The approach can be used also if the features are available only for a fraction of the patients, does not require imputation and, importantly, is able to automatically select features with small inter-feature correlation.","Clinical insights deriving from this study are also discussed."],"url":"http://arxiv.org/abs/2404.14275v1","category":"stat.ME"}
{"created":"2024-04-22 15:00:51","title":"From Modalities to Styles: Rethinking the Domain Gap in Heterogeneous Face Recognition","abstract":"Heterogeneous Face Recognition (HFR) focuses on matching faces from different domains, for instance, thermal to visible images, making Face Recognition (FR) systems more versatile for challenging scenarios. However, the domain gap between these domains and the limited large-scale datasets in the target HFR modalities make it challenging to develop robust HFR models from scratch. In our work, we view different modalities as distinct styles and propose a method to modulate feature maps of the target modality to address the domain gap. We present a new Conditional Adaptive Instance Modulation (CAIM ) module that seamlessly fits into existing FR networks, turning them into HFR-ready systems. The CAIM block modulates intermediate feature maps, efficiently adapting to the style of the source modality and bridging the domain gap. Our method enables end-to-end training using a small set of paired samples. We extensively evaluate the proposed approach on various challenging HFR benchmarks, showing that it outperforms state-of-the-art methods. The source code and protocols for reproducing the findings will be made publicly available","sentences":["Heterogeneous Face Recognition (HFR) focuses on matching faces from different domains, for instance, thermal to visible images, making Face Recognition (FR) systems more versatile for challenging scenarios.","However, the domain gap between these domains and the limited large-scale datasets in the target HFR modalities make it challenging to develop robust HFR models from scratch.","In our work, we view different modalities as distinct styles and propose a method to modulate feature maps of the target modality to address the domain gap.","We present a new Conditional Adaptive Instance Modulation (CAIM ) module that seamlessly fits into existing FR networks, turning them into HFR-ready systems.","The CAIM block modulates intermediate feature maps, efficiently adapting to the style of the source modality and bridging the domain gap.","Our method enables end-to-end training using a small set of paired samples.","We extensively evaluate the proposed approach on various challenging HFR benchmarks, showing that it outperforms state-of-the-art methods.","The source code and protocols for reproducing the findings will be made publicly available"],"url":"http://arxiv.org/abs/2404.14247v1","category":"cs.CV"}
{"created":"2024-04-22 14:32:46","title":"Robust electrothermal switching of optical phase change materials through computer-aided adaptive pulse optimization","abstract":"Electrically tunable optical devices present diverse functionalities for manipulating electromagnetic waves by leveraging elements capable of reversibly switching between different optical states. This adaptability in adjusting their responses to electromagnetic waves after fabrication is crucial for developing more efficient and compact optical systems for a broad range of applications including sensing, imaging, telecommunications, and data storage. Chalcogenide-based phase change materials (PCMs) have shown great promise due to their stable, non-volatile phase transition between amorphous and crystalline states. Nonetheless, optimizing the switching parameters of PCM devices and maintaining their stable operation over thousands of cycles with minimal variation can be challenging. In this paper, we report on the critical role of PCM pattern as well as electrical pulse form in achieving reliable and stable switching, extending the operational lifetime of the device beyond 13,000 switching events. To achieve this, we have developed a computer-aided algorithm that monitors optical changes in the device and adjusts the applied voltage in accordance with the phase transformation process, thereby significantly enhancing the lifetime of these reconfigurable devices. Our findings reveal that patterned PCM structures show significantly higher endurance compared to blanket PCM thin films.","sentences":["Electrically tunable optical devices present diverse functionalities for manipulating electromagnetic waves by leveraging elements capable of reversibly switching between different optical states.","This adaptability in adjusting their responses to electromagnetic waves after fabrication is crucial for developing more efficient and compact optical systems for a broad range of applications including sensing, imaging, telecommunications, and data storage.","Chalcogenide-based phase change materials (PCMs) have shown great promise due to their stable, non-volatile phase transition between amorphous and crystalline states.","Nonetheless, optimizing the switching parameters of PCM devices and maintaining their stable operation over thousands of cycles with minimal variation can be challenging.","In this paper, we report on the critical role of PCM pattern as well as electrical pulse form in achieving reliable and stable switching, extending the operational lifetime of the device beyond 13,000 switching events.","To achieve this, we have developed a computer-aided algorithm that monitors optical changes in the device and adjusts the applied voltage in accordance with the phase transformation process, thereby significantly enhancing the lifetime of these reconfigurable devices.","Our findings reveal that patterned PCM structures show significantly higher endurance compared to blanket PCM thin films."],"url":"http://arxiv.org/abs/2404.14220v1","category":"physics.optics"}
{"created":"2024-04-22 14:31:55","title":"General protocols for the efficient distillation of indistinguishable photons","abstract":"Highly pure and indistinguishable photons are a prerequisite for use in quantum information processing. We introduce protocols for the distillation of indistinguishable photons that offer a significant improvement over previous work, reducing distinguishability error rates by a factor of $n$, with resource requirements scaling linearly in $n$. We present the protocols, based on the discrete Fourier transform and Hadamard (Sylvester) matrices, then give both analytical and numerical results regarding their performance. We observe that the same symmetry properties governing suppression laws are instrumental in understanding the behavior of these distillation protocols. We also prove, adapting a result from the Hadamard case, that for the $n$-photon discrete Fourier transform with $n$ a prime power, the suppression laws are exactly characterized by the well-known Zero Transmission Law based on permutation symmetry.","sentences":["Highly pure and indistinguishable photons are a prerequisite for use in quantum information processing.","We introduce protocols for the distillation of indistinguishable photons that offer a significant improvement over previous work, reducing distinguishability error rates by a factor of $n$, with resource requirements scaling linearly in $n$. We present the protocols, based on the discrete Fourier transform and Hadamard (Sylvester) matrices, then give both analytical and numerical results regarding their performance.","We observe that the same symmetry properties governing suppression laws are instrumental in understanding the behavior of these distillation protocols.","We also prove, adapting a result from the Hadamard case, that for the $n$-photon discrete Fourier transform with $n$ a prime power, the suppression laws are exactly characterized by the well-known Zero Transmission Law based on permutation symmetry."],"url":"http://arxiv.org/abs/2404.14217v1","category":"quant-ph"}
{"created":"2024-04-22 14:11:54","title":"Rotting Infinitely Many-armed Bandits beyond the Worst-case Rotting: An Adaptive Approach","abstract":"In this study, we consider the infinitely many armed bandit problems in rotting environments, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged. We explore two scenarios capturing problem-dependent characteristics regarding the decay of rewards: one in which the cumulative amount of rotting is bounded by $V_T$, referred to as the slow-rotting scenario, and the other in which the number of rotting instances is bounded by $S_T$, referred to as the abrupt-rotting scenario. To address the challenge posed by rotting rewards, we introduce an algorithm that utilizes UCB with an adaptive sliding window, designed to manage the bias and variance trade-off arising due to rotting rewards. Our proposed algorithm achieves tight regret bounds for both slow and abrupt rotting scenarios. Lastly, we demonstrate the performance of our algorithms using synthetic datasets.","sentences":["In this study, we consider the infinitely many armed bandit problems in rotting environments, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged.","We explore two scenarios capturing problem-dependent characteristics regarding the decay of rewards: one in which the cumulative amount of rotting is bounded by $V_T$, referred to as the slow-rotting scenario, and the other in which the number of rotting instances is bounded by $S_T$, referred to as the abrupt-rotting scenario.","To address the challenge posed by rotting rewards, we introduce an algorithm that utilizes UCB with an adaptive sliding window, designed to manage the bias and variance trade-off arising due to rotting rewards.","Our proposed algorithm achieves tight regret bounds for both slow and abrupt rotting scenarios.","Lastly, we demonstrate the performance of our algorithms using synthetic datasets."],"url":"http://arxiv.org/abs/2404.14202v1","category":"cs.LG"}
{"created":"2024-04-22 14:06:27","title":"Universal formal asymptotics for localized oscillation of a discrete mass-spring-damper system of time-varying properties, embedded into a one-dimensional medium described by the telegraph equation with variable coefficients","abstract":"We consider a quite general problem concerning a linear free oscillation of a discrete mass-spring-damper system. This discrete sub-system is embedded into a one-dimensional continuum medium described by the linear telegraph equation. In a particular case, the discrete sub-system can move along the continuum one at a sub-critical speed. Provided that the dissipation in both discrete and continuum sub-systems is absent, if parameters of the sub-systems are constants, under certain conditions (the localization conditions), a non-vanishing oscillation localized near the discrete sub-system can be possible. In the paper we assume that the dissipation in the damper and the medium is small, and all discrete-continuum system parameters are slowly varying functions in time and in space (when applicable), such that the localization condition is fulfilled for the instantaneous values of the parameters in a certain neighbourhood of the discrete sub-system position. This general statement can describe a number of mechanical systems of various nature. We derive the expression for the leading-order term of a universal asymptotics, which describes a localized oscillation of the discrete sub-system. In the non-dissipative case, the leading-order term of the expansion for the amplitude is found in the form of an algebraic expression, which involves the instantaneous values of the system parameters. In the dissipative case, the leading-order term for the amplitude, generally, is found in quadratures in the form of a functional, which depends on the history of the system parameters, though in some exceptional cases the result can be obtained as a function of time and the instantaneous limiting values of the system parameters. Finally, we have justified the universal asymptotics by numerical calculations for some particular cases.","sentences":["We consider a quite general problem concerning a linear free oscillation of a discrete mass-spring-damper system.","This discrete sub-system is embedded into a one-dimensional continuum medium described by the linear telegraph equation.","In a particular case, the discrete sub-system can move along the continuum one at a sub-critical speed.","Provided that the dissipation in both discrete and continuum sub-systems is absent, if parameters of the sub-systems are constants, under certain conditions (the localization conditions), a non-vanishing oscillation localized near the discrete sub-system can be possible.","In the paper we assume that the dissipation in the damper and the medium is small, and all discrete-continuum system parameters are slowly varying functions in time and in space (when applicable), such that the localization condition is fulfilled for the instantaneous values of the parameters in a certain neighbourhood of the discrete sub-system position.","This general statement can describe a number of mechanical systems of various nature.","We derive the expression for the leading-order term of a universal asymptotics, which describes a localized oscillation of the discrete sub-system.","In the non-dissipative case, the leading-order term of the expansion for the amplitude is found in the form of an algebraic expression, which involves the instantaneous values of the system parameters.","In the dissipative case, the leading-order term for the amplitude, generally, is found in quadratures in the form of a functional, which depends on the history of the system parameters, though in some exceptional cases the result can be obtained as a function of time and the instantaneous limiting values of the system parameters.","Finally, we have justified the universal asymptotics by numerical calculations for some particular cases."],"url":"http://arxiv.org/abs/2404.14196v1","category":"physics.class-ph"}
{"created":"2024-04-22 13:49:42","title":"Face2Face: Label-driven Facial Retouching Restoration","abstract":"With the popularity of social media platforms such as Instagram and TikTok, and the widespread availability and convenience of retouching tools, an increasing number of individuals are utilizing these tools to beautify their facial photographs. This poses challenges for fields that place high demands on the authenticity of photographs, such as identity verification and social media. By altering facial images, users can easily create deceptive images, leading to the dissemination of false information. This may pose challenges to the reliability of identity verification systems and social media, and even lead to online fraud. To address this issue, some work has proposed makeup removal methods, but they still lack the ability to restore images involving geometric deformations caused by retouching. To tackle the problem of facial retouching restoration, we propose a framework, dubbed Face2Face, which consists of three components: a facial retouching detector, an image restoration model named FaceR, and a color correction module called Hierarchical Adaptive Instance Normalization (H-AdaIN). Firstly, the facial retouching detector predicts a retouching label containing three integers, indicating the retouching methods and their corresponding degrees. Then FaceR restores the retouched image based on the predicted retouching label. Finally, H-AdaIN is applied to address the issue of color shift arising from diffusion models. Extensive experiments demonstrate the effectiveness of our framework and each module.","sentences":["With the popularity of social media platforms such as Instagram and TikTok, and the widespread availability and convenience of retouching tools, an increasing number of individuals are utilizing these tools to beautify their facial photographs.","This poses challenges for fields that place high demands on the authenticity of photographs, such as identity verification and social media.","By altering facial images, users can easily create deceptive images, leading to the dissemination of false information.","This may pose challenges to the reliability of identity verification systems and social media, and even lead to online fraud.","To address this issue, some work has proposed makeup removal methods, but they still lack the ability to restore images involving geometric deformations caused by retouching.","To tackle the problem of facial retouching restoration, we propose a framework, dubbed Face2Face, which consists of three components: a facial retouching detector, an image restoration model named FaceR, and a color correction module called Hierarchical Adaptive Instance Normalization (H-AdaIN).","Firstly, the facial retouching detector predicts a retouching label containing three integers, indicating the retouching methods and their corresponding degrees.","Then FaceR restores the retouched image based on the predicted retouching label.","Finally, H-AdaIN is applied to address the issue of color shift arising from diffusion models.","Extensive experiments demonstrate the effectiveness of our framework and each module."],"url":"http://arxiv.org/abs/2404.14177v1","category":"cs.CV"}
{"created":"2024-04-22 13:41:03","title":"The effects of turbulence modeling on dynamic stall","abstract":"A numerical investigation of the flow evolution over a pitching NACA 0012 airfoil incurring in deep dynamic stall phenomena is presented. The experimental data at Reynolds number Re = 135 000 and reduced frequency k = 0.1, provided by Lee and Gerontakos, are compared to numerical simulations using different turbulence models. After a preliminary space and time convergence study, two- and three-dimensional URANS with different turbulence models are explored, highlighting the advantages and the drawbacks. Then, the turbulence-resolving capabilities of hybrid RANS/LES strategies are exploited to recover and better represent the dynamic stall vortex. In detail, Scale-Adaptive Simulations (SAS) and Stress-Blended Eddy Simulations (SBES) are adopted. Furthermore, the LES resolved portion allows a spectral analysis of the force and moment coefficients to investigate the contribution of frequency lower than the pitching one. Finally, a comparison of the proposed approaches with other numerical simulations is given.","sentences":["A numerical investigation of the flow evolution over a pitching NACA 0012 airfoil incurring in deep dynamic stall phenomena is presented.","The experimental data at Reynolds number Re = 135 000 and reduced frequency k = 0.1, provided by Lee and Gerontakos, are compared to numerical simulations using different turbulence models.","After a preliminary space and time convergence study, two- and three-dimensional URANS with different turbulence models are explored, highlighting the advantages and the drawbacks.","Then, the turbulence-resolving capabilities of hybrid RANS/LES strategies are exploited to recover and better represent the dynamic stall vortex.","In detail, Scale-Adaptive Simulations (SAS) and Stress-Blended Eddy Simulations (SBES) are adopted.","Furthermore, the LES resolved portion allows a spectral analysis of the force and moment coefficients to investigate the contribution of frequency lower than the pitching one.","Finally, a comparison of the proposed approaches with other numerical simulations is given."],"url":"http://arxiv.org/abs/2404.14172v1","category":"physics.flu-dyn"}
{"created":"2024-04-22 13:36:25","title":"Optimal frequency for undulatory motion in granular media","abstract":"Sand is a highly dissipative system, where the local spatial arrangements and densities depend strongly on the applied forces, resulting in fluid-like or solid-like behaviour. This makes sand swimming challenging and intriguing, raising questions about the nature of the motion and how to optimize the design of artificial swimmers able to swim in sand. Recent experiments suggest that lateral undulatory motion enables efficient locomotion, with a non-monotonic dependence of the swimming speed on the undulatory frequency and the height of the sediment bed. Here, we propose a quasi-2D granular model, where the effect of the bed height is modeled by a coarse-grained frictional force with the substrate. We show that the optimal frequency coincides with the second vibrational mode of the swimmer and explain the underlying mechanism through a characterization of the rheology of the medium. Potential implications in the design of artificial swimmers are discussed.","sentences":["Sand is a highly dissipative system, where the local spatial arrangements and densities depend strongly on the applied forces, resulting in fluid-like or solid-like behaviour.","This makes sand swimming challenging and intriguing, raising questions about the nature of the motion and how to optimize the design of artificial swimmers able to swim in sand.","Recent experiments suggest that lateral undulatory motion enables efficient locomotion, with a non-monotonic dependence of the swimming speed on the undulatory frequency and the height of the sediment bed.","Here, we propose a quasi-2D granular model, where the effect of the bed height is modeled by a coarse-grained frictional force with the substrate.","We show that the optimal frequency coincides with the second vibrational mode of the swimmer and explain the underlying mechanism through a characterization of the rheology of the medium.","Potential implications in the design of artificial swimmers are discussed."],"url":"http://arxiv.org/abs/2404.14168v1","category":"cond-mat.soft"}
{"created":"2024-04-22 12:55:21","title":"Travelling waves in an ensemble of excitable oscillators: the interplay of memristive coupling and noise","abstract":"Using methods of numerical simulation, we demonstrate the constructive role of memristive coupling in the context of the travelling waves formation and robustness in an ensemble of excitable oscillators described by the FitzHugh-Nagumo neuron model. First, the revealed aspects of the memristive coupling action are shown on an example of the deterministic model where the memristive properties of the coupling elements provide for achieving travelling waves at lower coupling strength as compared to non-adaptive diffusive coupling. In the presence of noise, the positive role of memristive coupling is manifested as significant increasing a noise intensity critical value corresponding to the noise-induced destruction of travelling waves as compared to classical diffusive interaction. In addition, we point out the second constructive factor, the L{\\'e}vy noise whose properties provide for inducing travelling waves.","sentences":["Using methods of numerical simulation, we demonstrate the constructive role of memristive coupling in the context of the travelling waves formation and robustness in an ensemble of excitable oscillators described by the FitzHugh-Nagumo neuron model.","First, the revealed aspects of the memristive coupling action are shown on an example of the deterministic model where the memristive properties of the coupling elements provide for achieving travelling waves at lower coupling strength as compared to non-adaptive diffusive coupling.","In the presence of noise, the positive role of memristive coupling is manifested as significant increasing a noise intensity critical value corresponding to the noise-induced destruction of travelling waves as compared to classical diffusive interaction.","In addition, we point out the second constructive factor, the L{\\'e}vy noise whose properties provide for inducing travelling waves."],"url":"http://arxiv.org/abs/2404.14147v1","category":"nlin.AO"}
{"created":"2024-04-22 12:05:34","title":"Anomalous dispersion via dissipative coupling in a quantum well exciton-polariton microcavity","abstract":"According to the principles of quantum mechanics the Hamiltonian describing a closed system's energies must be Hermitian. This leads to an avoided crossing on resonance, as coupling between states causes the energy levels to repel. This concept lies at the heart of exciton-polariton physics, where coherent exciton-photon interaction causes polariton branches to repel in momentum dispersion. However, non-Hermitian physics predicts an opposite effect: level attraction, which occurs when significant energy dissipation is present in the system. Here, we show a manifestation of dissipative coupling in a high-quality AlGaAs-based polariton microcavity, where two polariton branches attract, resulting in an anomalous, inverted dispersion of the lower branch in momentum dispersion. We observe the evolution of the level attraction with exciton-photon detuning, leading to changes in anomalous dispersion shape within a single sample. The dissipative coupling is explained by the interaction with an indirect exciton, acting as a highly dissipative channel in our system, and the observed dispersions are well captured within a phenomenological model. Our results present a new mechanism of dissipative coupling in light-matter systems and offer a tunable and well-controlled AlGaAs-based platform for engineering the non-Hermitian and negative mass effects in polariton systems.","sentences":["According to the principles of quantum mechanics the Hamiltonian describing a closed system's energies must be Hermitian.","This leads to an avoided crossing on resonance, as coupling between states causes the energy levels to repel.","This concept lies at the heart of exciton-polariton physics, where coherent exciton-photon interaction causes polariton branches to repel in momentum dispersion.","However, non-Hermitian physics predicts an opposite effect: level attraction, which occurs when significant energy dissipation is present in the system.","Here, we show a manifestation of dissipative coupling in a high-quality AlGaAs-based polariton microcavity, where two polariton branches attract, resulting in an anomalous, inverted dispersion of the lower branch in momentum dispersion.","We observe the evolution of the level attraction with exciton-photon detuning, leading to changes in anomalous dispersion shape within a single sample.","The dissipative coupling is explained by the interaction with an indirect exciton, acting as a highly dissipative channel in our system, and the observed dispersions are well captured within a phenomenological model.","Our results present a new mechanism of dissipative coupling in light-matter systems and offer a tunable and well-controlled AlGaAs-based platform for engineering the non-Hermitian and negative mass effects in polariton systems."],"url":"http://arxiv.org/abs/2404.14116v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-22 11:06:06","title":"Evidence and concerns about a latent, embryonic phase tectonic evolution and the existence of the young subsurface ocean on Mimas","abstract":"New models challenge the long-standing conclusion about Mimas, an icy satellite of Saturn, being an inactive snowball, suggesting the existence of a young stealth ocean. Unfortunately, no observable evidence has been found yet implying tectonic activity and the theoretical subsurface ocean. Here, we present the first structural geological map of the icy satellite, with the signs of various tectonic features, along with a simple crosscutting chronology of lineaments formation. In accordance with the supposedly young age of the stealth ocean, the observed phenomena are described as putative lineaments, ridges, and troughs. Simple tectonic features are identified as young compared to complex structures. The pattern of the linear features seems to overlap with the allocation of various modeled global nonlinear tidal dissipation patterns. In such a way, it may provide the first observed evidence for the existence of the theoretical subsurface stealth ocean. However, the overlapping and crosscutting relation between craters and the observed features may raise concerns about the recent formation of such linear features, indicating possibly long-time dormant or already stopped tectonic processes at the early embryonic phase of lineament formation billions of years ago.","sentences":["New models challenge the long-standing conclusion about Mimas, an icy satellite of Saturn, being an inactive snowball, suggesting the existence of a young stealth ocean.","Unfortunately, no observable evidence has been found yet implying tectonic activity and the theoretical subsurface ocean.","Here, we present the first structural geological map of the icy satellite, with the signs of various tectonic features, along with a simple crosscutting chronology of lineaments formation.","In accordance with the supposedly young age of the stealth ocean, the observed phenomena are described as putative lineaments, ridges, and troughs.","Simple tectonic features are identified as young compared to complex structures.","The pattern of the linear features seems to overlap with the allocation of various modeled global nonlinear tidal dissipation patterns.","In such a way, it may provide the first observed evidence for the existence of the theoretical subsurface stealth ocean.","However, the overlapping and crosscutting relation between craters and the observed features may raise concerns about the recent formation of such linear features, indicating possibly long-time dormant or already stopped tectonic processes at the early embryonic phase of lineament formation billions of years ago."],"url":"http://arxiv.org/abs/2404.14084v1","category":"astro-ph.EP"}
{"created":"2024-04-22 10:52:52","title":"Dynamical scaling and Planckian dissipation due to heavy-fermion quantum criticality","abstract":"We study dynamical scaling associated with a Kondo-breakdown quantum critical point (KB-QCP) of the periodic Anderson model, treated by two-site cellular dynamical mean-field theory (2CDMFT). In the quantum critical region, the staggered spin exhibits SYK-like slow dynamics and its dynamical susceptibility shows $\\omega/T$ scaling. We propose a scaling Ansatz that describes this behavior. It also implies Planckian dissipation for the longest-lived excitations. The current susceptibility follows the same scaling ansatz, leading to strange-metal scaling. This demonstrates that the KB-QCP described by 2CDMFT is an intrinsic (i.e., disorder-free) strange-metal fixed point. Surprisingly, the SYK-like dynamics and scaling are driven by strong vertex contributions to the susceptibilities. Our results for the optical conductivity match experimental observations on YbRh${}_2$Si${}_2$ and CeCoIn${}_5$.","sentences":["We study dynamical scaling associated with a Kondo-breakdown quantum critical point (KB-QCP) of the periodic Anderson model, treated by two-site cellular dynamical mean-field theory (2CDMFT).","In the quantum critical region, the staggered spin exhibits SYK-like slow dynamics and its dynamical susceptibility shows $\\omega/T$ scaling.","We propose a scaling Ansatz that describes this behavior.","It also implies Planckian dissipation for the longest-lived excitations.","The current susceptibility follows the same scaling ansatz, leading to strange-metal scaling.","This demonstrates that the KB-QCP described by 2CDMFT is an intrinsic (i.e., disorder-free) strange-metal fixed point.","Surprisingly, the SYK-like dynamics and scaling are driven by strong vertex contributions to the susceptibilities.","Our results for the optical conductivity match experimental observations on YbRh${}_2$Si${}_2$ and CeCoIn${}_5$."],"url":"http://arxiv.org/abs/2404.14079v1","category":"cond-mat.str-el"}
{"created":"2024-04-22 10:19:16","title":"GatedLexiconNet: A Comprehensive End-to-End Handwritten Paragraph Text Recognition System","abstract":"The Handwritten Text Recognition problem has been a challenge for researchers for the last few decades, especially in the domain of computer vision, a subdomain of pattern recognition. Variability of texts amongst writers, cursiveness, and different font styles of handwritten texts with degradation of historical text images make it a challenging problem. Recognizing scanned document images in neural network-based systems typically involves a two-step approach: segmentation and recognition. However, this method has several drawbacks. These shortcomings encompass challenges in identifying text regions, analyzing layout diversity within pages, and establishing accurate ground truth segmentation. Consequently, these processes are prone to errors, leading to bottlenecks in achieving high recognition accuracies. Thus, in this study, we present an end-to-end paragraph recognition system that incorporates internal line segmentation and gated convolutional layers based encoder. The gating is a mechanism that controls the flow of information and allows to adaptively selection of the more relevant features in handwritten text recognition models. The attention module plays an important role in performing internal line segmentation, allowing the page to be processed line-by-line. During the decoding step, we have integrated a connectionist temporal classification-based word beam search decoder as a post-processing step. In this work, we have extended existing LexiconNet by carefully applying and utilizing gated convolutional layers in the existing deep neural network. Our results at line and page levels also favour our new GatedLexiconNet. This study reported character error rates of 2.27% on IAM, 0.9% on RIMES, and 2.13% on READ-16, and word error rates of 5.73% on IAM, 2.76% on RIMES, and 6.52% on READ-2016 datasets.","sentences":["The Handwritten Text Recognition problem has been a challenge for researchers for the last few decades, especially in the domain of computer vision, a subdomain of pattern recognition.","Variability of texts amongst writers, cursiveness, and different font styles of handwritten texts with degradation of historical text images make it a challenging problem.","Recognizing scanned document images in neural network-based systems typically involves a two-step approach: segmentation and recognition.","However, this method has several drawbacks.","These shortcomings encompass challenges in identifying text regions, analyzing layout diversity within pages, and establishing accurate ground truth segmentation.","Consequently, these processes are prone to errors, leading to bottlenecks in achieving high recognition accuracies.","Thus, in this study, we present an end-to-end paragraph recognition system that incorporates internal line segmentation and gated convolutional layers based encoder.","The gating is a mechanism that controls the flow of information and allows to adaptively selection of the more relevant features in handwritten text recognition models.","The attention module plays an important role in performing internal line segmentation, allowing the page to be processed line-by-line.","During the decoding step, we have integrated a connectionist temporal classification-based word beam search decoder as a post-processing step.","In this work, we have extended existing LexiconNet by carefully applying and utilizing gated convolutional layers in the existing deep neural network.","Our results at line and page levels also favour our new GatedLexiconNet.","This study reported character error rates of 2.27% on IAM, 0.9% on RIMES, and 2.13% on READ-16, and word error rates of 5.73% on IAM, 2.76% on RIMES, and 6.52% on READ-2016 datasets."],"url":"http://arxiv.org/abs/2404.14062v1","category":"cs.CV"}
{"created":"2024-04-22 09:57:53","title":"HashPoint: Accelerated Point Searching and Sampling for Neural Rendering","abstract":"In this paper, we address the problem of efficient point searching and sampling for volume neural rendering. Within this realm, two typical approaches are employed: rasterization and ray tracing. The rasterization-based methods enable real-time rendering at the cost of increased memory and lower fidelity. In contrast, the ray-tracing-based methods yield superior quality but demand longer rendering time. We solve this problem by our HashPoint method combining these two strategies, leveraging rasterization for efficient point searching and sampling, and ray marching for rendering. Our method optimizes point searching by rasterizing points within the camera's view, organizing them in a hash table, and facilitating rapid searches. Notably, we accelerate the rendering process by adaptive sampling on the primary surface encountered by the ray. Our approach yields substantial speed-up for a range of state-of-the-art ray-tracing-based methods, maintaining equivalent or superior accuracy across synthetic and real test datasets. The code will be available at https://jiahao-ma.github.io/hashpoint/.","sentences":["In this paper, we address the problem of efficient point searching and sampling for volume neural rendering.","Within this realm, two typical approaches are employed: rasterization and ray tracing.","The rasterization-based methods enable real-time rendering at the cost of increased memory and lower fidelity.","In contrast, the ray-tracing-based methods yield superior quality but demand longer rendering time.","We solve this problem by our HashPoint method combining these two strategies, leveraging rasterization for efficient point searching and sampling, and ray marching for rendering.","Our method optimizes point searching by rasterizing points within the camera's view, organizing them in a hash table, and facilitating rapid searches.","Notably, we accelerate the rendering process by adaptive sampling on the primary surface encountered by the ray.","Our approach yields substantial speed-up for a range of state-of-the-art ray-tracing-based methods, maintaining equivalent or superior accuracy across synthetic and real test datasets.","The code will be available at https://jiahao-ma.github.io/hashpoint/."],"url":"http://arxiv.org/abs/2404.14044v1","category":"cs.CV"}
{"created":"2024-04-22 09:40:07","title":"Exploring neural oscillations during speech perception via surrogate gradient spiking neural networks","abstract":"Understanding cognitive processes in the brain demands sophisticated models capable of replicating neural dynamics at large scales. We present a physiologically inspired speech recognition architecture, compatible and scalable with deep learning frameworks, and demonstrate that end-to-end gradient descent training leads to the emergence of neural oscillations in the central spiking neural network. Significant cross-frequency couplings, indicative of these oscillations, are measured within and across network layers during speech processing, whereas no such interactions are observed when handling background noise inputs. Furthermore, our findings highlight the crucial inhibitory role of feedback mechanisms, such as spike frequency adaptation and recurrent connections, in regulating and synchronising neural activity to improve recognition performance. Overall, on top of developing our understanding of synchronisation phenomena notably observed in the human auditory pathway, our architecture exhibits dynamic and efficient information processing, with relevance to neuromorphic technology.","sentences":["Understanding cognitive processes in the brain demands sophisticated models capable of replicating neural dynamics at large scales.","We present a physiologically inspired speech recognition architecture, compatible and scalable with deep learning frameworks, and demonstrate that end-to-end gradient descent training leads to the emergence of neural oscillations in the central spiking neural network.","Significant cross-frequency couplings, indicative of these oscillations, are measured within and across network layers during speech processing, whereas no such interactions are observed when handling background noise inputs.","Furthermore, our findings highlight the crucial inhibitory role of feedback mechanisms, such as spike frequency adaptation and recurrent connections, in regulating and synchronising neural activity to improve recognition performance.","Overall, on top of developing our understanding of synchronisation phenomena notably observed in the human auditory pathway, our architecture exhibits dynamic and efficient information processing, with relevance to neuromorphic technology."],"url":"http://arxiv.org/abs/2404.14024v1","category":"cs.CL"}
{"created":"2024-04-22 09:35:48","title":"Physics-Informed Neural Networks and Beyond: Enforcing Physical Constraints in Quantum Dissipative Dynamics","abstract":"Neural networks (NNs) accelerate simulations of quantum dissipative dynamics. Ensuring that these simulations adhere to fundamental physical laws is crucial, but has been largely ignored in the state-of-the-art NN approaches. We show that this may lead to implausible results measured by violation of the trace conservation. To recover the correct physical behavior, we develop physics-informed NNs that mitigate the violations to a good extend. Beyond that, we introduce an approach enforcing the perfect trace conservation by design.","sentences":["Neural networks (NNs) accelerate simulations of quantum dissipative dynamics.","Ensuring that these simulations adhere to fundamental physical laws is crucial, but has been largely ignored in the state-of-the-art NN approaches.","We show that this may lead to implausible results measured by violation of the trace conservation.","To recover the correct physical behavior, we develop physics-informed NNs that mitigate the violations to a good extend.","Beyond that, we introduce an approach enforcing the perfect trace conservation by design."],"url":"http://arxiv.org/abs/2404.14021v1","category":"physics.chem-ph"}
{"created":"2024-04-22 09:09:09","title":"Temporal genomics help in deciphering neutral and adaptive patterns in the contemporary evolution of kelp populations","abstract":"The impact of climate change on populations will be contingent upon their contemporary adaptive evolution. In this study, we investigated the contemporary evolution of four populations of the cold-water kelp Laminaria digitata by analysing their spatial and temporal genomic variation using ddRAD-sequencing. These populations were sampled from the center to the southern margin of its north-eastern Atlantic distribution at two-time points, spanning at least two generations. Through genome scans for local adaptation at a single time point, we identified candidate loci that showed clinal variation correlated with changes in sea surface temperature (SST) along latitudinal gradients. This finding suggests that SST may drive the adaptive response of these kelp populations, although factors such as species' demographic history should also be considered. Additionally, we performed a simulation approach to distinguish the effect of selection from genetic drift in allele frequency changes over time. This enabled the detection of loci in the southernmost population that exhibited temporal differentiation beyond what would be expected from genetic drift alone: these are candidate loci which could have evolved under selection over time. In contrast, we did not detect any outlier locus based on temporal differentiation in the population from the North Sea, which also displayed low and decreasing levels of genetic diversity. The diverse evolutionary scenarios observed among populations can be attributed to variations in the prevalence of selection relative to genetic drift across different environments. Therefore, our study highlights the potential of temporal genomics to offer valuable insights into the contemporary evolution of marine foundation species facing climate change.","sentences":["The impact of climate change on populations will be contingent upon their contemporary adaptive evolution.","In this study, we investigated the contemporary evolution of four populations of the cold-water kelp Laminaria digitata by analysing their spatial and temporal genomic variation using ddRAD-sequencing.","These populations were sampled from the center to the southern margin of its north-eastern Atlantic distribution at two-time points, spanning at least two generations.","Through genome scans for local adaptation at a single time point, we identified candidate loci that showed clinal variation correlated with changes in sea surface temperature (SST) along latitudinal gradients.","This finding suggests that SST may drive the adaptive response of these kelp populations, although factors such as species' demographic history should also be considered.","Additionally, we performed a simulation approach to distinguish the effect of selection from genetic drift in allele frequency changes over time.","This enabled the detection of loci in the southernmost population that exhibited temporal differentiation beyond what would be expected from genetic drift alone: these are candidate loci which could have evolved under selection over time.","In contrast, we did not detect any outlier locus based on temporal differentiation in the population from the North Sea, which also displayed low and decreasing levels of genetic diversity.","The diverse evolutionary scenarios observed among populations can be attributed to variations in the prevalence of selection relative to genetic drift across different environments.","Therefore, our study highlights the potential of temporal genomics to offer valuable insights into the contemporary evolution of marine foundation species facing climate change."],"url":"http://arxiv.org/abs/2404.14003v1","category":"q-bio.PE"}
{"created":"2024-04-22 08:57:46","title":"QCore: Data-Efficient, On-Device Continual Calibration for Quantized Models -- Extended Version","abstract":"We are witnessing an increasing availability of streaming data that may contain valuable information on the underlying processes. It is thus attractive to be able to deploy machine learning models on edge devices near sensors such that decisions can be made instantaneously, rather than first having to transmit incoming data to servers. To enable deployment on edge devices with limited storage and computational capabilities, the full-precision parameters in standard models can be quantized to use fewer bits. The resulting quantized models are then calibrated using back-propagation and full training data to ensure accuracy. This one-time calibration works for deployments in static environments. However, model deployment in dynamic edge environments call for continual calibration to adaptively adjust quantized models to fit new incoming data, which may have different distributions. The first difficulty in enabling continual calibration on the edge is that the full training data may be too large and thus not always available on edge devices. The second difficulty is that the use of back-propagation on the edge for repeated calibration is too expensive. We propose QCore to enable continual calibration on the edge. First, it compresses the full training data into a small subset to enable effective calibration of quantized models with different bit-widths. We also propose means of updating the subset when new streaming data arrives to reflect changes in the environment, while not forgetting earlier training data. Second, we propose a small bit-flipping network that works with the subset to update quantized model parameters, thus enabling efficient continual calibration without back-propagation. An experimental study, conducted with real-world data in a continual learning setting, offers insight into the properties of QCore and shows that it is capable of outperforming strong baseline methods.","sentences":["We are witnessing an increasing availability of streaming data that may contain valuable information on the underlying processes.","It is thus attractive to be able to deploy machine learning models on edge devices near sensors such that decisions can be made instantaneously, rather than first having to transmit incoming data to servers.","To enable deployment on edge devices with limited storage and computational capabilities, the full-precision parameters in standard models can be quantized to use fewer bits.","The resulting quantized models are then calibrated using back-propagation and full training data to ensure accuracy.","This one-time calibration works for deployments in static environments.","However, model deployment in dynamic edge environments call for continual calibration to adaptively adjust quantized models to fit new incoming data, which may have different distributions.","The first difficulty in enabling continual calibration on the edge is that the full training data may be too large and thus not always available on edge devices.","The second difficulty is that the use of back-propagation on the edge for repeated calibration is too expensive.","We propose QCore to enable continual calibration on the edge.","First, it compresses the full training data into a small subset to enable effective calibration of quantized models with different bit-widths.","We also propose means of updating the subset when new streaming data arrives to reflect changes in the environment, while not forgetting earlier training data.","Second, we propose a small bit-flipping network that works with the subset to update quantized model parameters, thus enabling efficient continual calibration without back-propagation.","An experimental study, conducted with real-world data in a continual learning setting, offers insight into the properties of QCore and shows that it is capable of outperforming strong baseline methods."],"url":"http://arxiv.org/abs/2404.13990v1","category":"cs.LG"}
{"created":"2024-04-22 08:44:10","title":"Structure-Aware Human Body Reshaping with Adaptive Affinity-Graph Network","abstract":"Given a source portrait, the automatic human body reshaping task aims at editing it to an aesthetic body shape. As the technology has been widely used in media, several methods have been proposed mainly focusing on generating optical flow to warp the body shape. However, those previous works only consider the local transformation of different body parts (arms, torso, and legs), ignoring the global affinity, and limiting the capacity to ensure consistency and quality across the entire body. In this paper, we propose a novel Adaptive Affinity-Graph Network (AAGN), which extracts the global affinity between different body parts to enhance the quality of the generated optical flow. Specifically, our AAGN primarily introduces the following designs: (1) we propose an Adaptive Affinity-Graph (AAG) Block that leverages the characteristic of a fully connected graph. AAG represents different body parts as nodes in an adaptive fully connected graph and captures all the affinities between nodes to obtain a global affinity map. The design could better improve the consistency between body parts. (2) Besides, for high-frequency details are crucial for photo aesthetics, a Body Shape Discriminator (BSD) is designed to extract information from both high-frequency and spatial domain. Particularly, an SRM filter is utilized to extract high-frequency details, which are combined with spatial features as input to the BSD. With this design, BSD guides the Flow Generator (FG) to pay attention to various fine details rather than rigid pixel-level fitting. Extensive experiments conducted on the BR-5K dataset demonstrate that our framework significantly enhances the aesthetic appeal of reshaped photos, marginally surpassing all previous work to achieve state-of-the-art in all evaluation metrics.","sentences":["Given a source portrait, the automatic human body reshaping task aims at editing it to an aesthetic body shape.","As the technology has been widely used in media, several methods have been proposed mainly focusing on generating optical flow to warp the body shape.","However, those previous works only consider the local transformation of different body parts (arms, torso, and legs), ignoring the global affinity, and limiting the capacity to ensure consistency and quality across the entire body.","In this paper, we propose a novel Adaptive Affinity-Graph Network (AAGN), which extracts the global affinity between different body parts to enhance the quality of the generated optical flow.","Specifically, our AAGN primarily introduces the following designs: (1) we propose an Adaptive Affinity-Graph (AAG) Block that leverages the characteristic of a fully connected graph.","AAG represents different body parts as nodes in an adaptive fully connected graph and captures all the affinities between nodes to obtain a global affinity map.","The design could better improve the consistency between body parts.","(2) Besides, for high-frequency details are crucial for photo aesthetics, a Body Shape Discriminator (BSD) is designed to extract information from both high-frequency and spatial domain.","Particularly, an SRM filter is utilized to extract high-frequency details, which are combined with spatial features as input to the BSD.","With this design, BSD guides the Flow Generator (FG) to pay attention to various fine details rather than rigid pixel-level fitting.","Extensive experiments conducted on the BR-5K dataset demonstrate that our framework significantly enhances the aesthetic appeal of reshaped photos, marginally surpassing all previous work to achieve state-of-the-art in all evaluation metrics."],"url":"http://arxiv.org/abs/2404.13983v1","category":"cs.CV"}
{"created":"2024-04-22 07:55:03","title":"GNSS Measurement-Based Context Recognition for Vehicle Navigation using Gated Recurrent Unit","abstract":"Recent years, people have put forward higher and higher requirements for context-adaptive navigation (CAN). CAN system realizes seamless navigation in complex environments by recognizing the ambient surroundings of vehicles, and it is crucial to develop a fast, reliable, and robust navigational context recognition (NCR) method to enable CAN systems to operate effectively. Environmental context recognition based on Global Navigation Satellite System (GNSS) measurements has attracted widespread attention due to its low cost because it does not require additional infrastructure. The performance and application value of NCR methods depend on three main factors: context categorization, feature extraction, and classification models. In this paper, a fine-grained context categorization framework comprising seven environment categories (open sky, tree-lined avenue, semi-outdoor, urban canyon, viaduct-down, shallow indoor, and deep indoor) is proposed, which currently represents the most elaborate context categorization framework known in this research domain. To improve discrimination between categories, a new feature called the C/N0-weighted azimuth distribution factor, is designed. Then, to ensure real-time performance, a lightweight gated recurrent unit (GRU) network is adopted for its excellent sequence data processing capabilities. A dataset containing 59,996 samples is created and made publicly available to researchers in the NCR community on Github. Extensive experiments have been conducted on the dataset, and the results show that the proposed method achieves an overall recognition accuracy of 99.41\\% for isolated scenarios and 94.95\\% for transition scenarios, with an average transition delay of 2.14 seconds.","sentences":["Recent years, people have put forward higher and higher requirements for context-adaptive navigation (CAN).","CAN system realizes seamless navigation in complex environments by recognizing the ambient surroundings of vehicles, and it is crucial to develop a fast, reliable, and robust navigational context recognition (NCR) method to enable CAN systems to operate effectively.","Environmental context recognition based on Global Navigation Satellite System (GNSS) measurements has attracted widespread attention due to its low cost because it does not require additional infrastructure.","The performance and application value of NCR methods depend on three main factors: context categorization, feature extraction, and classification models.","In this paper, a fine-grained context categorization framework comprising seven environment categories (open sky, tree-lined avenue, semi-outdoor, urban canyon, viaduct-down, shallow indoor, and deep indoor) is proposed, which currently represents the most elaborate context categorization framework known in this research domain.","To improve discrimination between categories, a new feature called the C/N0-weighted azimuth distribution factor, is designed.","Then, to ensure real-time performance, a lightweight gated recurrent unit (GRU) network is adopted for its excellent sequence data processing capabilities.","A dataset containing 59,996 samples is created and made publicly available to researchers in the NCR community on Github.","Extensive experiments have been conducted on the dataset, and the results show that the proposed method achieves an overall recognition accuracy of 99.41\\% for isolated scenarios and 94.95\\% for transition scenarios, with an average transition delay of 2.14 seconds."],"url":"http://arxiv.org/abs/2404.13955v1","category":"eess.SP"}
{"created":"2024-04-22 07:51:13","title":"SPLATE: Sparse Late Interaction Retrieval","abstract":"The late interaction paradigm introduced with ColBERT stands out in the neural Information Retrieval space, offering a compelling effectiveness-efficiency trade-off across many benchmarks. Efficient late interaction retrieval is based on an optimized multi-step strategy, where an approximate search first identifies a set of candidate documents to re-rank exactly. In this work, we introduce SPLATE, a simple and lightweight adaptation of the ColBERTv2 model which learns an ``MLM adapter'', mapping its frozen token embeddings to a sparse vocabulary space with a partially learned SPLADE module. This allows us to perform the candidate generation step in late interaction pipelines with traditional sparse retrieval techniques, making it particularly appealing for running ColBERT in CPU environments. Our SPLATE ColBERTv2 pipeline achieves the same effectiveness as the PLAID ColBERTv2 engine by re-ranking 50 documents that can be retrieved under 10ms.","sentences":["The late interaction paradigm introduced with ColBERT stands out in the neural Information Retrieval space, offering a compelling effectiveness-efficiency trade-off across many benchmarks.","Efficient late interaction retrieval is based on an optimized multi-step strategy, where an approximate search first identifies a set of candidate documents to re-rank exactly.","In this work, we introduce SPLATE, a simple and lightweight adaptation of the ColBERTv2 model which learns an ``MLM adapter'', mapping its frozen token embeddings to a sparse vocabulary space with a partially learned SPLADE module.","This allows us to perform the candidate generation step in late interaction pipelines with traditional sparse retrieval techniques, making it particularly appealing for running ColBERT in CPU environments.","Our SPLATE ColBERTv2 pipeline achieves the same effectiveness as the PLAID ColBERTv2 engine by re-ranking 50 documents that can be retrieved under 10ms."],"url":"http://arxiv.org/abs/2404.13950v1","category":"cs.IR"}
{"created":"2024-04-22 07:03:44","title":"MARIO Eval: Evaluate Your Math LLM with your Math LLM--A mathematical dataset evaluation toolkit","abstract":"Large language models (LLMs) have been explored in a variety of reasoning tasks including solving of mathematical problems. Each math dataset typically includes its own specially designed evaluation script, which, while suitable for its intended use, lacks generalizability across different datasets. Consequently, updates and adaptations to these evaluation tools tend to occur without being systematically reported, leading to inconsistencies and obstacles to fair comparison across studies. To bridge this gap, we introduce a comprehensive mathematical evaluation toolkit that not only utilizes a python computer algebra system (CAS) for its numerical accuracy, but also integrates an optional LLM, known for its considerable natural language processing capabilities. To validate the effectiveness of our toolkit, we manually annotated two distinct datasets. Our experiments demonstrate that the toolkit yields more robust evaluation results compared to prior works, even without an LLM. Furthermore, when an LLM is incorporated, there is a notable enhancement. The code for our method will be made available at \\url{https://github.com/MARIO-Math-Reasoning/math_evaluation}.","sentences":["Large language models (LLMs) have been explored in a variety of reasoning tasks including solving of mathematical problems.","Each math dataset typically includes its own specially designed evaluation script, which, while suitable for its intended use, lacks generalizability across different datasets.","Consequently, updates and adaptations to these evaluation tools tend to occur without being systematically reported, leading to inconsistencies and obstacles to fair comparison across studies.","To bridge this gap, we introduce a comprehensive mathematical evaluation toolkit that not only utilizes a python computer algebra system (CAS) for its numerical accuracy, but also integrates an optional LLM, known for its considerable natural language processing capabilities.","To validate the effectiveness of our toolkit, we manually annotated two distinct datasets.","Our experiments demonstrate that the toolkit yields more robust evaluation results compared to prior works, even without an LLM.","Furthermore, when an LLM is incorporated, there is a notable enhancement.","The code for our method will be made available at \\url{https://github.com/MARIO-Math-Reasoning/math_evaluation}."],"url":"http://arxiv.org/abs/2404.13925v1","category":"cs.CL"}
{"created":"2024-04-22 06:59:03","title":"NeRF-DetS: Enhancing Multi-View 3D Object Detection with Sampling-adaptive Network of Continuous NeRF-based Representation","abstract":"As a preliminary work, NeRF-Det unifies the tasks of novel view synthesis and 3D perception, demonstrating that perceptual tasks can benefit from novel view synthesis methods like NeRF, significantly improving the performance of indoor multi-view 3D object detection. Using the geometry MLP of NeRF to direct the attention of detection head to crucial parts and incorporating self-supervised loss from novel view rendering contribute to the achieved improvement. To better leverage the notable advantages of the continuous representation through neural rendering in space, we introduce a novel 3D perception network structure, NeRF-DetS. The key component of NeRF-DetS is the Multi-level Sampling-Adaptive Network, making the sampling process adaptively from coarse to fine. Also, we propose a superior multi-view information fusion method, known as Multi-head Weighted Fusion. This fusion approach efficiently addresses the challenge of losing multi-view information when using arithmetic mean, while keeping low computational costs. NeRF-DetS outperforms competitive NeRF-Det on the ScanNetV2 dataset, by achieving +5.02% and +5.92% improvement in mAP@.25 and mAP@.50, respectively.","sentences":["As a preliminary work, NeRF-Det unifies the tasks of novel view synthesis and 3D perception, demonstrating that perceptual tasks can benefit from novel view synthesis methods like NeRF, significantly improving the performance of indoor multi-view 3D object detection.","Using the geometry MLP of NeRF to direct the attention of detection head to crucial parts and incorporating self-supervised loss from novel view rendering contribute to the achieved improvement.","To better leverage the notable advantages of the continuous representation through neural rendering in space, we introduce a novel 3D perception network structure, NeRF-DetS.","The key component of NeRF-DetS is the Multi-level Sampling-Adaptive Network, making the sampling process adaptively from coarse to fine.","Also, we propose a superior multi-view information fusion method, known as Multi-head Weighted Fusion.","This fusion approach efficiently addresses the challenge of losing multi-view information when using arithmetic mean, while keeping low computational costs.","NeRF-DetS outperforms competitive NeRF-Det on the ScanNetV2 dataset, by achieving +5.02% and +5.92% improvement in mAP@.25 and mAP@.50, respectively."],"url":"http://arxiv.org/abs/2404.13921v1","category":"cs.CV"}
{"created":"2024-04-22 05:52:33","title":"Ultralow Dissipation Nanomechanical Devices from Monocrystalline Silicon Carbide","abstract":"Due to their low mass and long coherence times, nanomechanical resonators have many applications, from biomolecule mass sensing to hybrid quantum interfaces. In many instances the performance is limited by internal material damping. Crystalline materials promise lower material dissipation, however due to fabrication challenges, amorphous materials are more commonly utilized. Crystalline silicon carbide (SiC) is particularly appealing due to its exquisite mechanical, electrical and optical properties, but to-date exhibits higher nanomechanical dissipation than both amorphous and other crystalline materials. To address this, we fabricate nanomechanical resonators thinned from bulk monocrystalline 4H-SiC. Characterization of multiple resonators of different sizes and thicknesses, allows us to discern the surface and volumetric contributions to dissipation. We measure mechanical dissipation rates as low as 2.7 mHz, more than an order-of-magnitude lower than any previous crystalline SiC resonator, yielding quality factors as high as 20 million at room temperature. We also quantify the nonlinear dissipation of SiC nanomechanical resonators for the first time, finding that it is lower than other materials. This promises higher sensitivity in applications such as mass sensing. By achieving exceptionally low dissipation in SiC resonators, our work provides a path towards improved performance in sensing and other applications.","sentences":["Due to their low mass and long coherence times, nanomechanical resonators have many applications, from biomolecule mass sensing to hybrid quantum interfaces.","In many instances the performance is limited by internal material damping.","Crystalline materials promise lower material dissipation, however due to fabrication challenges, amorphous materials are more commonly utilized.","Crystalline silicon carbide (SiC) is particularly appealing due to its exquisite mechanical, electrical and optical properties, but to-date exhibits higher nanomechanical dissipation than both amorphous and other crystalline materials.","To address this, we fabricate nanomechanical resonators thinned from bulk monocrystalline 4H-SiC. Characterization of multiple resonators of different sizes and thicknesses, allows us to discern the surface and volumetric contributions to dissipation.","We measure mechanical dissipation rates as low as 2.7 mHz, more than an order-of-magnitude lower than any previous crystalline SiC resonator, yielding quality factors as high as 20 million at room temperature.","We also quantify the nonlinear dissipation of SiC nanomechanical resonators for the first time, finding that it is lower than other materials.","This promises higher sensitivity in applications such as mass sensing.","By achieving exceptionally low dissipation in SiC resonators, our work provides a path towards improved performance in sensing and other applications."],"url":"http://arxiv.org/abs/2404.13893v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-22 04:41:42","title":"FreqBlender: Enhancing DeepFake Detection by Blending Frequency Knowledge","abstract":"Generating synthetic fake faces, known as pseudo-fake faces, is an effective way to improve the generalization of DeepFake detection. Existing methods typically generate these faces by blending real or fake faces in color space. While these methods have shown promise, they overlook the simulation of frequency distribution in pseudo-fake faces, limiting the learning of generic forgery traces in-depth. To address this, this paper introduces {\\em FreqBlender}, a new method that can generate pseudo-fake faces by blending frequency knowledge. Specifically, we investigate the major frequency components and propose a Frequency Parsing Network to adaptively partition frequency components related to forgery traces. Then we blend this frequency knowledge from fake faces into real faces to generate pseudo-fake faces. Since there is no ground truth for frequency components, we describe a dedicated training strategy by leveraging the inner correlations among different frequency knowledge to instruct the learning process. Experimental results demonstrate the effectiveness of our method in enhancing DeepFake detection, making it a potential plug-and-play strategy for other methods.","sentences":["Generating synthetic fake faces, known as pseudo-fake faces, is an effective way to improve the generalization of DeepFake detection.","Existing methods typically generate these faces by blending real or fake faces in color space.","While these methods have shown promise, they overlook the simulation of frequency distribution in pseudo-fake faces, limiting the learning of generic forgery traces in-depth.","To address this, this paper introduces {\\em FreqBlender}, a new method that can generate pseudo-fake faces by blending frequency knowledge.","Specifically, we investigate the major frequency components and propose a Frequency Parsing Network to adaptively partition frequency components related to forgery traces.","Then we blend this frequency knowledge from fake faces into real faces to generate pseudo-fake faces.","Since there is no ground truth for frequency components, we describe a dedicated training strategy by leveraging the inner correlations among different frequency knowledge to instruct the learning process.","Experimental results demonstrate the effectiveness of our method in enhancing DeepFake detection, making it a potential plug-and-play strategy for other methods."],"url":"http://arxiv.org/abs/2404.13872v1","category":"cs.CV"}
{"created":"2024-04-22 03:39:03","title":"Self-Supervised Monocular Depth Estimation in the Dark: Towards Data Distribution Compensation","abstract":"Nighttime self-supervised monocular depth estimation has received increasing attention in recent years. However, using night images for self-supervision is unreliable because the photometric consistency assumption is usually violated in the videos taken under complex lighting conditions. Even with domain adaptation or photometric loss repair, performance is still limited by the poor supervision of night images on trainable networks. In this paper, we propose a self-supervised nighttime monocular depth estimation method that does not use any night images during training. Our framework utilizes day images as a stable source for self-supervision and applies physical priors (e.g., wave optics, reflection model and read-shot noise model) to compensate for some key day-night differences. With day-to-night data distribution compensation, our framework can be trained in an efficient one-stage self-supervised manner. Though no nighttime images are considered during training, qualitative and quantitative results demonstrate that our method achieves SoTA depth estimating results on the challenging nuScenes-Night and RobotCar-Night compared with existing methods.","sentences":["Nighttime self-supervised monocular depth estimation has received increasing attention in recent years.","However, using night images for self-supervision is unreliable because the photometric consistency assumption is usually violated in the videos taken under complex lighting conditions.","Even with domain adaptation or photometric loss repair, performance is still limited by the poor supervision of night images on trainable networks.","In this paper, we propose a self-supervised nighttime monocular depth estimation method that does not use any night images during training.","Our framework utilizes day images as a stable source for self-supervision and applies physical priors (e.g., wave optics, reflection model and read-shot noise model) to compensate for some key day-night differences.","With day-to-night data distribution compensation, our framework can be trained in an efficient one-stage self-supervised manner.","Though no nighttime images are considered during training, qualitative and quantitative results demonstrate that our method achieves SoTA depth estimating results on the challenging nuScenes-Night and RobotCar-Night compared with existing methods."],"url":"http://arxiv.org/abs/2404.13854v1","category":"cs.CV"}
{"created":"2024-04-22 03:35:19","title":"ICST-DNET: An Interpretable Causal Spatio-Temporal Diffusion Network for Traffic Speed Prediction","abstract":"Traffic speed prediction is significant for intelligent navigation and congestion alleviation. However, making accurate predictions is challenging due to three factors: 1) traffic diffusion, i.e., the spatial and temporal causality existing between the traffic conditions of multiple neighboring roads, 2) the poor interpretability of traffic data with complicated spatio-temporal correlations, and 3) the latent pattern of traffic speed fluctuations over time, such as morning and evening rush. Jointly considering these factors, in this paper, we present a novel architecture for traffic speed prediction, called Interpretable Causal Spatio-Temporal Diffusion Network (ICST-DNET). Specifically, ICST-DENT consists of three parts, namely the Spatio-Temporal Causality Learning (STCL), Causal Graph Generation (CGG), and Speed Fluctuation Pattern Recognition (SFPR) modules. First, to model the traffic diffusion within road networks, an STCL module is proposed to capture both the temporal causality on each individual road and the spatial causality in each road pair. The CGG module is then developed based on STCL to enhance the interpretability of the traffic diffusion procedure from the temporal and spatial perspectives. Specifically, a time causality matrix is generated to explain the temporal causality between each road's historical and future traffic conditions. For spatial causality, we utilize causal graphs to visualize the diffusion process in road pairs. Finally, to adapt to traffic speed fluctuations in different scenarios, we design a personalized SFPR module to select the historical timesteps with strong influences for learning the pattern of traffic speed fluctuations. Extensive experimental results prove that ICST-DNET can outperform all existing baselines, as evidenced by the higher prediction accuracy, ability to explain causality, and adaptability to different scenarios.","sentences":["Traffic speed prediction is significant for intelligent navigation and congestion alleviation.","However, making accurate predictions is challenging due to three factors: 1) traffic diffusion, i.e., the spatial and temporal causality existing between the traffic conditions of multiple neighboring roads, 2) the poor interpretability of traffic data with complicated spatio-temporal correlations, and 3) the latent pattern of traffic speed fluctuations over time, such as morning and evening rush.","Jointly considering these factors, in this paper, we present a novel architecture for traffic speed prediction, called Interpretable Causal Spatio-Temporal Diffusion Network (ICST-DNET).","Specifically, ICST-DENT consists of three parts, namely the Spatio-Temporal Causality Learning (STCL), Causal Graph Generation (CGG), and Speed Fluctuation Pattern Recognition (SFPR) modules.","First, to model the traffic diffusion within road networks, an STCL module is proposed to capture both the temporal causality on each individual road and the spatial causality in each road pair.","The CGG module is then developed based on STCL to enhance the interpretability of the traffic diffusion procedure from the temporal and spatial perspectives.","Specifically, a time causality matrix is generated to explain the temporal causality between each road's historical and future traffic conditions.","For spatial causality, we utilize causal graphs to visualize the diffusion process in road pairs.","Finally, to adapt to traffic speed fluctuations in different scenarios, we design a personalized SFPR module to select the historical timesteps with strong influences for learning the pattern of traffic speed fluctuations.","Extensive experimental results prove that ICST-DNET can outperform all existing baselines, as evidenced by the higher prediction accuracy, ability to explain causality, and adaptability to different scenarios."],"url":"http://arxiv.org/abs/2404.13853v1","category":"cs.LG"}
{"created":"2024-04-22 03:31:34","title":"Toward Robust LiDAR based 3D Object Detection via Density-Aware Adaptive Thresholding","abstract":"Robust 3D object detection is a core challenge for autonomous mobile systems in field robotics. To tackle this issue, many researchers have demonstrated improvements in 3D object detection performance in datasets. However, real-world urban scenarios with unstructured and dynamic situations can still lead to numerous false positives, posing a challenge for robust 3D object detection models. This paper presents a post-processing algorithm that dynamically adjusts object detection thresholds based on the distance from the ego-vehicle. 3D object detection models usually perform well in detecting nearby objects but may exhibit suboptimal performance for distant ones. While conventional perception algorithms typically employ a single threshold in post-processing, the proposed algorithm addresses this issue by employing adaptive thresholds based on the distance from the ego-vehicle, minimizing false negatives and reducing false positives in urban scenarios. The results show performance enhancements in 3D object detection models across a range of scenarios, not only in dynamic urban road conditions but also in scenarios involving adverse weather conditions.","sentences":["Robust 3D object detection is a core challenge for autonomous mobile systems in field robotics.","To tackle this issue, many researchers have demonstrated improvements in 3D object detection performance in datasets.","However, real-world urban scenarios with unstructured and dynamic situations can still lead to numerous false positives, posing a challenge for robust 3D object detection models.","This paper presents a post-processing algorithm that dynamically adjusts object detection thresholds based on the distance from the ego-vehicle.","3D object detection models usually perform well in detecting nearby objects but may exhibit suboptimal performance for distant ones.","While conventional perception algorithms typically employ a single threshold in post-processing, the proposed algorithm addresses this issue by employing adaptive thresholds based on the distance from the ego-vehicle, minimizing false negatives and reducing false positives in urban scenarios.","The results show performance enhancements in 3D object detection models across a range of scenarios, not only in dynamic urban road conditions but also in scenarios involving adverse weather conditions."],"url":"http://arxiv.org/abs/2404.13852v1","category":"cs.RO"}
{"created":"2024-04-22 03:15:42","title":"DSDRNet: Disentangling Representation and Reconstruct Network for Domain Generalization","abstract":"Domain generalization faces challenges due to the distribution shift between training and testing sets, and the presence of unseen target domains. Common solutions include domain alignment, meta-learning, data augmentation, or ensemble learning, all of which rely on domain labels or domain adversarial techniques. In this paper, we propose a Dual-Stream Separation and Reconstruction Network, dubbed DSDRNet. It is a disentanglement-reconstruction approach that integrates features of both inter-instance and intra-instance through dual-stream fusion. The method introduces novel supervised signals by combining inter-instance semantic distance and intra-instance similarity. Incorporating Adaptive Instance Normalization (AdaIN) into a two-stage cyclic reconstruction process enhances self-disentangled reconstruction signals to facilitate model convergence. Extensive experiments on four benchmark datasets demonstrate that DSDRNet outperforms other popular methods in terms of domain generalization capabilities.","sentences":["Domain generalization faces challenges due to the distribution shift between training and testing sets, and the presence of unseen target domains.","Common solutions include domain alignment, meta-learning, data augmentation, or ensemble learning, all of which rely on domain labels or domain adversarial techniques.","In this paper, we propose a Dual-Stream Separation and Reconstruction Network, dubbed DSDRNet.","It is a disentanglement-reconstruction approach that integrates features of both inter-instance and intra-instance through dual-stream fusion.","The method introduces novel supervised signals by combining inter-instance semantic distance and intra-instance similarity.","Incorporating Adaptive Instance Normalization (AdaIN) into a two-stage cyclic reconstruction process enhances self-disentangled reconstruction signals to facilitate model convergence.","Extensive experiments on four benchmark datasets demonstrate that DSDRNet outperforms other popular methods in terms of domain generalization capabilities."],"url":"http://arxiv.org/abs/2404.13848v1","category":"cs.CV"}
{"created":"2024-04-22 03:05:32","title":"EventLens: Leveraging Event-Aware Pretraining and Cross-modal Linking Enhances Visual Commonsense Reasoning","abstract":"Visual Commonsense Reasoning (VCR) is a cognitive task, challenging models to answer visual questions requiring human commonsense, and to provide rationales explaining why the answers are correct. With emergence of Large Language Models (LLMs), it is natural and imperative to explore their applicability to VCR. However, VCR task demands more external knowledge to tackle its challenging questions, necessitating special designs to activate LLMs' commonsense reasoning abilities. Also, most existing Multimodal LLMs adopted an abstraction of entire input image, which makes it difficult to comprehend VCR's unique co-reference tags between image regions and text, posing challenges for fine-grained alignment. To address these issues, we propose EventLens that leverages Event-Aware Pretraining and Cross-modal Linking and EnhanceS VCR. First, by emulating the cognitive process of human reasoning, an Event-Aware Pretraining auxiliary task is introduced to better activate LLM's global comprehension of intricate scenarios. Second, during fine-tuning, we further utilize reference tags to bridge RoI features with texts, while preserving both modality semantics. Finally, we use instruct-style prompts to narrow the gap between pretraining and fine-tuning, and task-specific adapters to better integrate LLM's inherent knowledge with new commonsense. Experimental results show the effectiveness of our proposed auxiliary task and fine-grained linking strategy.","sentences":["Visual Commonsense Reasoning (VCR) is a cognitive task, challenging models to answer visual questions requiring human commonsense, and to provide rationales explaining why the answers are correct.","With emergence of Large Language Models (LLMs), it is natural and imperative to explore their applicability to VCR.","However, VCR task demands more external knowledge to tackle its challenging questions, necessitating special designs to activate LLMs' commonsense reasoning abilities.","Also, most existing Multimodal LLMs adopted an abstraction of entire input image, which makes it difficult to comprehend VCR's unique co-reference tags between image regions and text, posing challenges for fine-grained alignment.","To address these issues, we propose EventLens that leverages Event-Aware Pretraining and Cross-modal Linking and EnhanceS VCR.","First, by emulating the cognitive process of human reasoning, an Event-Aware Pretraining auxiliary task is introduced to better activate LLM's global comprehension of intricate scenarios.","Second, during fine-tuning, we further utilize reference tags to bridge RoI features with texts, while preserving both modality semantics.","Finally, we use instruct-style prompts to narrow the gap between pretraining and fine-tuning, and task-specific adapters to better integrate LLM's inherent knowledge with new commonsense.","Experimental results show the effectiveness of our proposed auxiliary task and fine-grained linking strategy."],"url":"http://arxiv.org/abs/2404.13847v1","category":"cs.CV"}
{"created":"2024-04-22 02:52:54","title":"ColA: Collaborative Adaptation with Gradient Learning","abstract":"A primary function of back-propagation is to compute both the gradient of hidden representations and parameters for optimization with gradient descent. Training large models requires high computational costs due to their vast parameter sizes. While Parameter-Efficient Fine-Tuning (PEFT) methods aim to train smaller auxiliary models to save computational space, they still present computational overheads, especially in Fine-Tuning as a Service (FTaaS) for numerous users. We introduce Collaborative Adaptation (ColA) with Gradient Learning (GL), a parameter-free, model-agnostic fine-tuning approach that decouples the computation of the gradient of hidden representations and parameters. In comparison to PEFT methods, ColA facilitates more cost-effective FTaaS by offloading the computation of the gradient to low-cost devices. We also provide a theoretical analysis of ColA and experimentally demonstrate that ColA can perform on par or better than existing PEFT methods on various benchmarks.","sentences":["A primary function of back-propagation is to compute both the gradient of hidden representations and parameters for optimization with gradient descent.","Training large models requires high computational costs due to their vast parameter sizes.","While Parameter-Efficient Fine-Tuning (PEFT) methods aim to train smaller auxiliary models to save computational space, they still present computational overheads, especially in Fine-Tuning as a Service (FTaaS) for numerous users.","We introduce Collaborative Adaptation (ColA) with Gradient Learning (GL), a parameter-free, model-agnostic fine-tuning approach that decouples the computation of the gradient of hidden representations and parameters.","In comparison to PEFT methods, ColA facilitates more cost-effective FTaaS by offloading the computation of the gradient to low-cost devices.","We also provide a theoretical analysis of ColA and experimentally demonstrate that ColA can perform on par or better than existing PEFT methods on various benchmarks."],"url":"http://arxiv.org/abs/2404.13844v1","category":"cs.LG"}
{"created":"2024-04-22 02:04:34","title":"GazeIntent: Adapting dwell-time selection in VR interaction with real-time intent modeling","abstract":"The use of ML models to predict a user's cognitive state from behavioral data has been studied for various applications which includes predicting the intent to perform selections in VR. We developed a novel technique that uses gaze-based intent models to adapt dwell-time thresholds to aid gaze-only selection. A dataset of users performing selection in arithmetic tasks was used to develop intent prediction models (F1 = 0.94). We developed GazeIntent to adapt selection dwell times based on intent model outputs and conducted an end-user study with returning and new users performing additional tasks with varied selection frequencies. Personalized models for returning users effectively accounted for prior experience and were preferred by 63% of users. Our work provides the field with methods to adapt dwell-based selection to users, account for experience over time, and consider tasks that vary by selection frequency","sentences":["The use of ML models to predict a user's cognitive state from behavioral data has been studied for various applications which includes predicting the intent to perform selections in VR.","We developed a novel technique that uses gaze-based intent models to adapt dwell-time thresholds to aid gaze-only selection.","A dataset of users performing selection in arithmetic tasks was used to develop intent prediction models (F1 = 0.94).","We developed GazeIntent to adapt selection dwell times based on intent model outputs and conducted an end-user study with returning and new users performing additional tasks with varied selection frequencies.","Personalized models for returning users effectively accounted for prior experience and were preferred by 63% of users.","Our work provides the field with methods to adapt dwell-based selection to users, account for experience over time, and consider tasks that vary by selection frequency"],"url":"http://arxiv.org/abs/2404.13829v1","category":"cs.HC"}
{"created":"2024-04-22 01:40:37","title":"Joint Liability Model with Adaptation to Climate Change","abstract":"This paper extends the application of ESG score assessment methodologies from large corporations to individual farmers' production, within the context of climate change. Our proposal involves the integration of crucial agricultural sustainability variables into conventional personal credit evaluation frameworks, culminating in the formulation of a holistic sustainable credit rating referred to as the Environmental, Social, Economics (ESE) score. This ESE score is integrated into theoretical joint liability models, to gain valuable insights into optimal group sizes and individual-ESE score relationships. Additionally, we adopt a mean-variance utility function for farmers to effectively capture the risk associated with anticipated profits. Through a set of simulation exercises, the paper investigates the implications of incorporating ESE scores into credit evaluation systems, offering a nuanced comprehension of the repercussions under various climatic conditions.","sentences":["This paper extends the application of ESG score assessment methodologies from large corporations to individual farmers' production, within the context of climate change.","Our proposal involves the integration of crucial agricultural sustainability variables into conventional personal credit evaluation frameworks, culminating in the formulation of a holistic sustainable credit rating referred to as the Environmental, Social, Economics (ESE) score.","This ESE score is integrated into theoretical joint liability models, to gain valuable insights into optimal group sizes and individual-ESE score relationships.","Additionally, we adopt a mean-variance utility function for farmers to effectively capture the risk associated with anticipated profits.","Through a set of simulation exercises, the paper investigates the implications of incorporating ESE scores into credit evaluation systems, offering a nuanced comprehension of the repercussions under various climatic conditions."],"url":"http://arxiv.org/abs/2404.13818v1","category":"q-fin.GN"}
{"created":"2024-04-22 00:16:18","title":"Adaptive Heterogeneous Client Sampling for Federated Learning over Wireless Networks","abstract":"Federated learning (FL) algorithms usually sample a fraction of clients in each round (partial participation) when the number of participants is large and the server's communication bandwidth is limited. Recent works on the convergence analysis of FL have focused on unbiased client sampling, e.g., sampling uniformly at random, which suffers from slow wall-clock time for convergence due to high degrees of system heterogeneity and statistical heterogeneity. This paper aims to design an adaptive client sampling algorithm for FL over wireless networks that tackles both system and statistical heterogeneity to minimize the wall-clock convergence time. We obtain a new tractable convergence bound for FL algorithms with arbitrary client sampling probability. Based on the bound, we analytically establish the relationship between the total learning time and sampling probability with an adaptive bandwidth allocation scheme, which results in a non-convex optimization problem. We design an efficient algorithm for learning the unknown parameters in the convergence bound and develop a low-complexity algorithm to approximately solve the non-convex problem. Our solution reveals the impact of system and statistical heterogeneity parameters on the optimal client sampling design. Moreover, our solution shows that as the number of sampled clients increases, the total convergence time first decreases and then increases because a larger sampling number reduces the number of rounds for convergence but results in a longer expected time per-round due to limited wireless bandwidth. Experimental results from both hardware prototype and simulation demonstrate that our proposed sampling scheme significantly reduces the convergence time compared to several baseline sampling schemes.","sentences":["Federated learning (FL) algorithms usually sample a fraction of clients in each round (partial participation) when the number of participants is large and the server's communication bandwidth is limited.","Recent works on the convergence analysis of FL have focused on unbiased client sampling, e.g., sampling uniformly at random, which suffers from slow wall-clock time for convergence due to high degrees of system heterogeneity and statistical heterogeneity.","This paper aims to design an adaptive client sampling algorithm for FL over wireless networks that tackles both system and statistical heterogeneity to minimize the wall-clock convergence time.","We obtain a new tractable convergence bound for FL algorithms with arbitrary client sampling probability.","Based on the bound, we analytically establish the relationship between the total learning time and sampling probability with an adaptive bandwidth allocation scheme, which results in a non-convex optimization problem.","We design an efficient algorithm for learning the unknown parameters in the convergence bound and develop a low-complexity algorithm to approximately solve the non-convex problem.","Our solution reveals the impact of system and statistical heterogeneity parameters on the optimal client sampling design.","Moreover, our solution shows that as the number of sampled clients increases, the total convergence time first decreases and then increases because a larger sampling number reduces the number of rounds for convergence but results in a longer expected time per-round due to limited wireless bandwidth.","Experimental results from both hardware prototype and simulation demonstrate that our proposed sampling scheme significantly reduces the convergence time compared to several baseline sampling schemes."],"url":"http://arxiv.org/abs/2404.13804v1","category":"cs.DC"}
{"created":"2024-04-21 23:48:28","title":"Dynamics of Polar-Core Spin Vortices in Inhomogeneous Spin-1 Bose-Einstein Condensates","abstract":"In the easy-plane phase, a ferromagnetic spin-1 Bose-Einstein condensate is magnetized in a plane transverse to the applied Zeeman field. This phase supports polar-core spin vortices (PCVs), which consist of phase windings of transverse magnetization. Here we show that spin-changing collisions cause a PCV to accelerate down density gradients in an inhomogeneous condensate. The dynamics is well-described by a simplified model adapted from scalar systems, which predicts the dependence of the dynamics on trap tightness and quadratic Zeeman energy. In a harmonic trap, a PCV accelerates radially to the condensate boundary, in stark contrast to the azimuthal motion of vortices in a scalar condensate. In a trap that has a local potential maximum at the centre, the PCV exhibits oscillations around the trap centre, which persist for a remarkably long time. The oscillations coincide with the emission and reabsorption of axial spin waves, which reflect off the condensate boundary.","sentences":["In the easy-plane phase, a ferromagnetic spin-1 Bose-Einstein condensate is magnetized in a plane transverse to the applied Zeeman field.","This phase supports polar-core spin vortices (PCVs), which consist of phase windings of transverse magnetization.","Here we show that spin-changing collisions cause a PCV to accelerate down density gradients in an inhomogeneous condensate.","The dynamics is well-described by a simplified model adapted from scalar systems, which predicts the dependence of the dynamics on trap tightness and quadratic Zeeman energy.","In a harmonic trap, a PCV accelerates radially to the condensate boundary, in stark contrast to the azimuthal motion of vortices in a scalar condensate.","In a trap that has a local potential maximum at the centre, the PCV exhibits oscillations around the trap centre, which persist for a remarkably long time.","The oscillations coincide with the emission and reabsorption of axial spin waves, which reflect off the condensate boundary."],"url":"http://arxiv.org/abs/2404.13800v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-21 21:13:46","title":"Explainable Interfaces for Rapid Gaze-Based Interactions in Mixed Reality","abstract":"Gaze-based interactions offer a potential way for users to naturally engage with mixed reality (XR) interfaces. Black-box machine learning models enabled higher accuracy for gaze-based interactions. However, due to the black-box nature of the model, users might not be able to understand and effectively adapt their gaze behaviour to achieve high quality interaction. We posit that explainable AI (XAI) techniques can facilitate understanding of and interaction with gaze-based model-driven system in XR. To study this, we built a real-time, multi-level XAI interface for gaze-based interaction using a deep learning model, and evaluated it during a visual search task in XR. A between-subjects study revealed that participants who interacted with XAI made more accurate selections compared to those who did not use the XAI system (i.e., F1 score increase of 10.8%). Additionally, participants who used the XAI system adapted their gaze behavior over time to make more effective selections. These findings suggest that XAI can potentially be used to assist users in more effective collaboration with model-driven interactions in XR.","sentences":["Gaze-based interactions offer a potential way for users to naturally engage with mixed reality (XR) interfaces.","Black-box machine learning models enabled higher accuracy for gaze-based interactions.","However, due to the black-box nature of the model, users might not be able to understand and effectively adapt their gaze behaviour to achieve high quality interaction.","We posit that explainable AI (XAI) techniques can facilitate understanding of and interaction with gaze-based model-driven system in XR.","To study this, we built a real-time, multi-level XAI interface for gaze-based interaction using a deep learning model, and evaluated it during a visual search task in XR.","A between-subjects study revealed that participants who interacted with XAI made more accurate selections compared to those who did not use the XAI system (i.e., F1 score increase of 10.8%).","Additionally, participants who used the XAI system adapted their gaze behavior over time to make more effective selections.","These findings suggest that XAI can potentially be used to assist users in more effective collaboration with model-driven interactions in XR."],"url":"http://arxiv.org/abs/2404.13777v1","category":"cs.HC"}
{"created":"2024-04-21 20:21:24","title":"Using Adaptive Empathetic Responses for Teaching English","abstract":"Existing English-teaching chatbots rarely incorporate empathy explicitly in their feedback, but empathetic feedback could help keep students engaged and reduce learner anxiety. Toward this end, we propose the task of negative emotion detection via audio, for recognizing empathetic feedback opportunities in language learning. We then build the first spoken English-teaching chatbot with adaptive, empathetic feedback. This feedback is synthesized through automatic prompt optimization of ChatGPT and is evaluated with English learners. We demonstrate the effectiveness of our system through a preliminary user study.","sentences":["Existing English-teaching chatbots rarely incorporate empathy explicitly in their feedback, but empathetic feedback could help keep students engaged and reduce learner anxiety.","Toward this end, we propose the task of negative emotion detection via audio, for recognizing empathetic feedback opportunities in language learning.","We then build the first spoken English-teaching chatbot with adaptive, empathetic feedback.","This feedback is synthesized through automatic prompt optimization of ChatGPT and is evaluated with English learners.","We demonstrate the effectiveness of our system through a preliminary user study."],"url":"http://arxiv.org/abs/2404.13764v1","category":"cs.CL"}
{"created":"2024-04-21 18:24:43","title":"Stochastic Multi-round Submodular Optimization with Budget","abstract":"In this work we study the problem of Stochastic Budgeted Multi-round Submodular Maximization (SBMSm), in which we would like to maximize the sum over multiple rounds of the value of a monotone and submodular objective function, subject to the fact that the values of this function depend on the realization of stochastic events and the number of observations that we can make over all rounds is limited by a given budget. This problem extends, and generalizes to multiple round settings, well-studied problems such as (adaptive) influence maximization and stochastic probing.   We first show that whenever a certain single-round optimization problem can be optimally solved in polynomial time, then there is a polynomial time dynamic programming algorithm that returns the same solution as the optimal algorithm, that can adaptively choose both which observations to make and in which round to have them. Unfortunately, this dynamic programming approach cannot be extended to work when the single-round optimization problem cannot be efficiently solved (even if we allow it would be approximated within an arbitrary small constant). Anyway, in this case we are able to provide a simple greedy algorithm for the problem. It guarantees a $(1/2-\\epsilon)$-approximation to the optimal value, even if it non-adaptively allocates the budget to rounds.","sentences":["In this work we study the problem of Stochastic Budgeted Multi-round Submodular Maximization (SBMSm), in which we would like to maximize the sum over multiple rounds of the value of a monotone and submodular objective function, subject to the fact that the values of this function depend on the realization of stochastic events and the number of observations that we can make over all rounds is limited by a given budget.","This problem extends, and generalizes to multiple round settings, well-studied problems such as (adaptive) influence maximization and stochastic probing.   ","We first show that whenever a certain single-round optimization problem can be optimally solved in polynomial time, then there is a polynomial time dynamic programming algorithm that returns the same solution as the optimal algorithm, that can adaptively choose both which observations to make and in which round to have them.","Unfortunately, this dynamic programming approach cannot be extended to work when the single-round optimization problem cannot be efficiently solved (even if we allow it would be approximated within an arbitrary small constant).","Anyway, in this case we are able to provide a simple greedy algorithm for the problem.","It guarantees a $(1/2-\\epsilon)$-approximation to the optimal value, even if it non-adaptively allocates the budget to rounds."],"url":"http://arxiv.org/abs/2404.13737v1","category":"cs.DS"}
{"created":"2024-04-21 18:19:27","title":"Elucidating the Design Space of Dataset Condensation","abstract":"Dataset condensation, a concept within data-centric learning, efficiently transfers critical attributes from an original dataset to a synthetic version, maintaining both diversity and realism. This approach significantly improves model training efficiency and is adaptable across multiple application areas. Previous methods in dataset condensation have faced challenges: some incur high computational costs which limit scalability to larger datasets (e.g., MTT, DREAM, and TESLA), while others are restricted to less optimal design spaces, which could hinder potential improvements, especially in smaller datasets (e.g., SRe2L, G-VBSM, and RDED). To address these limitations, we propose a comprehensive design framework that includes specific, effective strategies like implementing soft category-aware matching and adjusting the learning rate schedule. These strategies are grounded in empirical evidence and theoretical backing. Our resulting approach, Elucidate Dataset Condensation (EDC), establishes a benchmark for both small and large-scale dataset condensation. In our testing, EDC achieves state-of-the-art accuracy, reaching 48.6% on ImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a compression ratio of 0.78%. This performance exceeds those of SRe2L, G-VBSM, and RDED by margins of 27.3%, 17.2%, and 6.6%, respectively.","sentences":["Dataset condensation, a concept within data-centric learning, efficiently transfers critical attributes from an original dataset to a synthetic version, maintaining both diversity and realism.","This approach significantly improves model training efficiency and is adaptable across multiple application areas.","Previous methods in dataset condensation have faced challenges: some incur high computational costs which limit scalability to larger datasets (e.g., MTT, DREAM, and TESLA), while others are restricted to less optimal design spaces, which could hinder potential improvements, especially in smaller datasets (e.g., SRe2L, G-VBSM, and RDED).","To address these limitations, we propose a comprehensive design framework that includes specific, effective strategies like implementing soft category-aware matching and adjusting the learning rate schedule.","These strategies are grounded in empirical evidence and theoretical backing.","Our resulting approach, Elucidate Dataset Condensation (EDC), establishes a benchmark for both small and large-scale dataset condensation.","In our testing, EDC achieves state-of-the-art accuracy, reaching 48.6% on ImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a compression ratio of 0.78%.","This performance exceeds those of SRe2L, G-VBSM, and RDED by margins of 27.3%, 17.2%, and 6.6%, respectively."],"url":"http://arxiv.org/abs/2404.13733v1","category":"cs.LG"}
{"created":"2024-04-21 18:14:59","title":"Strong Existence and Uniqueness for Singular SDEs Driven by Stable Processes","abstract":"We consider the one-dimensional stochastic differential equation   \\begin{equation*}   X_t = x_0 + L_t + \\int_0^t \\mu(X_s)ds, \\quad t \\geq 0,   \\end{equation*} where $\\mu$ is a finite measure of Kato class $K_{\\eta}$ with $\\eta \\in (0,\\alpha-1]$ and $(L_t)_{t \\geq 0}$ is a symmetric $\\alpha$-stable process with $\\alpha \\in (1,2)$. We derive weak and strong well posedness for this equation when $\\eta \\leq\\alpha-1$ and $\\eta < \\alpha-1$, respectively, and show that the condition $\\eta \\leq \\alpha-1$ is sharp for weak existence. We furthermore reformulate the equation in terms of the local time of the solution $(X_{t})_{t \\geq 0}$ and prove its well posedness. To this end, we also derive a Tanaka-type formula for a symmetric, $\\alpha$-stable processes with $\\alpha \\in (1,2)$ that is perturbed by an adapted, right-continuous process of finite variation.","sentences":["We consider the one-dimensional stochastic differential equation   \\begin{equation*}   X_t = x_0 + L_t + \\int_0^t \\mu(X_s)ds, \\quad t \\geq 0,   \\end{equation*} where $\\mu$ is a finite measure of Kato class $K_{\\eta}$ with $\\eta \\in (0,\\alpha-1]$ and $(L_t)_{t \\geq 0}$ is a symmetric $\\alpha$-stable process with $\\alpha \\in (1,2)$. We derive weak and strong well posedness for this equation","when $\\eta \\leq\\alpha-1$ and $\\eta < \\alpha-1$, respectively, and show that the condition $\\eta \\leq \\alpha-1$ is sharp for weak existence.","We furthermore reformulate the equation in terms of the local time of the solution $(X_{t})_{t \\geq 0}$ and prove its well posedness.","To this end, we also derive a Tanaka-type formula for a symmetric, $\\alpha$-stable processes with $\\alpha \\in (1,2)$ that is perturbed by an adapted, right-continuous process of finite variation."],"url":"http://arxiv.org/abs/2404.13729v1","category":"math.PR"}
{"created":"2024-04-21 17:35:04","title":"MatInf -- an Extensible Open-Source Solution for Research Digitalisation in Materials Science","abstract":"Information technology and data science development stimulate transformation in many fields of scientific knowledge. In recent years, a large number of specialized systems for information and knowledge management have been created in materials science. However, the development and deployment of open adaptive systems for research support in materials science based on the acquisition, storage, and processing of different types of information remains unsolved. We propose MatInf - an extensible, open-source solution for research digitalisation in materials science based on an adaptive, flexible information management system for heterogeneous data sources. MatInf can be easily adapted to any materials science laboratory and is especially useful for collaborative projects between several labs. As an example, we demonstrate its application in high-throughput experimentation.","sentences":["Information technology and data science development stimulate transformation in many fields of scientific knowledge.","In recent years, a large number of specialized systems for information and knowledge management have been created in materials science.","However, the development and deployment of open adaptive systems for research support in materials science based on the acquisition, storage, and processing of different types of information remains unsolved.","We propose MatInf - an extensible, open-source solution for research digitalisation in materials science based on an adaptive, flexible information management system for heterogeneous data sources.","MatInf can be easily adapted to any materials science laboratory and is especially useful for collaborative projects between several labs.","As an example, we demonstrate its application in high-throughput experimentation."],"url":"http://arxiv.org/abs/2404.13722v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-21 16:45:35","title":"ArtNeRF: A Stylized Neural Field for 3D-Aware Cartoonized Face Synthesis","abstract":"Recent advances in generative visual models and neural radiance fields have greatly boosted 3D-aware image synthesis and stylization tasks. However, previous NeRF-based work is limited to single scene stylization, training a model to generate 3D-aware cartoon faces with arbitrary styles remains unsolved. We propose ArtNeRF, a novel face stylization framework derived from 3D-aware GAN to tackle this problem. In this framework, we utilize an expressive generator to synthesize stylized faces and a triple-branch discriminator module to improve the visual quality and style consistency of the generated faces. Specifically, a style encoder based on contrastive learning is leveraged to extract robust low-dimensional embeddings of style images, empowering the generator with the knowledge of various styles. To smooth the training process of cross-domain transfer learning, we propose an adaptive style blending module which helps inject style information and allows users to freely tune the level of stylization. We further introduce a neural rendering module to achieve efficient real-time rendering of images with higher resolutions. Extensive experiments demonstrate that ArtNeRF is versatile in generating high-quality 3D-aware cartoon faces with arbitrary styles.","sentences":["Recent advances in generative visual models and neural radiance fields have greatly boosted 3D-aware image synthesis and stylization tasks.","However, previous NeRF-based work is limited to single scene stylization, training a model to generate 3D-aware cartoon faces with arbitrary styles remains unsolved.","We propose ArtNeRF, a novel face stylization framework derived from 3D-aware GAN to tackle this problem.","In this framework, we utilize an expressive generator to synthesize stylized faces and a triple-branch discriminator module to improve the visual quality and style consistency of the generated faces.","Specifically, a style encoder based on contrastive learning is leveraged to extract robust low-dimensional embeddings of style images, empowering the generator with the knowledge of various styles.","To smooth the training process of cross-domain transfer learning, we propose an adaptive style blending module which helps inject style information and allows users to freely tune the level of stylization.","We further introduce a neural rendering module to achieve efficient real-time rendering of images with higher resolutions.","Extensive experiments demonstrate that ArtNeRF is versatile in generating high-quality 3D-aware cartoon faces with arbitrary styles."],"url":"http://arxiv.org/abs/2404.13711v1","category":"cs.CV"}
{"created":"2024-04-21 16:29:49","title":"PEMMA: Parameter-Efficient Multi-Modal Adaptation for Medical Image Segmentation","abstract":"Imaging modalities such as Computed Tomography (CT) and Positron Emission Tomography (PET) are key in cancer detection, inspiring Deep Neural Networks (DNN) models that merge these scans for tumor segmentation. When both CT and PET scans are available, it is common to combine them as two channels of the input to the segmentation model. However, this method requires both scan types during training and inference, posing a challenge due to the limited availability of PET scans, thereby sometimes limiting the process to CT scans only. Hence, there is a need to develop a flexible DNN architecture that can be trained/updated using only CT scans but can effectively utilize PET scans when they become available. In this work, we propose a parameter-efficient multi-modal adaptation (PEMMA) framework for lightweight upgrading of a transformer-based segmentation model trained only on CT scans to also incorporate PET scans. The benefits of the proposed approach are two-fold. Firstly, we leverage the inherent modularity of the transformer architecture and perform low-rank adaptation (LoRA) of the attention weights to achieve parameter-efficient adaptation. Secondly, since the PEMMA framework attempts to minimize cross modal entanglement, it is possible to subsequently update the combined model using only one modality, without causing catastrophic forgetting of the other modality. Our proposed method achieves comparable results with the performance of early fusion techniques with just 8% of the trainable parameters, especially with a remarkable +28% improvement on the average dice score on PET scans when trained on a single modality.","sentences":["Imaging modalities such as Computed Tomography (CT) and Positron Emission Tomography (PET) are key in cancer detection, inspiring Deep Neural Networks (DNN) models that merge these scans for tumor segmentation.","When both CT and PET scans are available, it is common to combine them as two channels of the input to the segmentation model.","However, this method requires both scan types during training and inference, posing a challenge due to the limited availability of PET scans, thereby sometimes limiting the process to CT scans only.","Hence, there is a need to develop a flexible DNN architecture that can be trained/updated using only CT scans but can effectively utilize PET scans when they become available.","In this work, we propose a parameter-efficient multi-modal adaptation (PEMMA) framework for lightweight upgrading of a transformer-based segmentation model trained only on CT scans to also incorporate PET scans.","The benefits of the proposed approach are two-fold.","Firstly, we leverage the inherent modularity of the transformer architecture and perform low-rank adaptation (LoRA) of the attention weights to achieve parameter-efficient adaptation.","Secondly, since the PEMMA framework attempts to minimize cross modal entanglement, it is possible to subsequently update the combined model using only one modality, without causing catastrophic forgetting of the other modality.","Our proposed method achieves comparable results with the performance of early fusion techniques with just 8% of the trainable parameters, especially with a remarkable +28% improvement on the average dice score on PET scans when trained on a single modality."],"url":"http://arxiv.org/abs/2404.13704v1","category":"eess.IV"}
{"created":"2024-04-22 17:59:24","title":"A covariant formulation for cosmological radiative transfer of the 21-cm line","abstract":"The 21-cm hyperfine line of neutral hydrogen is a useful tool to probe the conditions of the Universe during the Dark Ages, Cosmic Dawn, and the Epoch of Reionisation. In most of the current calculations, the 21-cm line signals at given frequencies are computed, using an integrated line-of-sight line opacity, with the correction for cosmological expansion. These calculations have not fully captured the line and continuum interactions in the radiative transfer, in response to evolution of the radiation field and the variations of thermal and dynamic properties of the line-of-sight medium. We construct a covariant formulation for the radiative transfer of the 21-cm line and derive the cosmological 21-cm line radiative transfer (C21LRT) equation. The formulation properly accounts for local emission and absorption processes and the interaction between the line and continuum when the radiation propagates across the expanding Universe to the present observer. Our C21LRT calculations show that methods simply summing the line optical depth could lead to error of $5\\%$ in the 21-cm signals for redshift $z \\sim 12-35$ and of $>10\\%$ for redshift $z \\lesssim 8$. Proper covariant radiative transfer is therefore necessary for producing correct theoretical templates for extracting information of the structural evolution of the Universe through the Epoch of Reionisation from the 21-cm tomographic data.","sentences":["The 21-cm hyperfine line of neutral hydrogen is a useful tool to probe the conditions of the Universe during the Dark Ages, Cosmic Dawn, and the Epoch of Reionisation.","In most of the current calculations, the 21-cm line signals at given frequencies are computed, using an integrated line-of-sight line opacity, with the correction for cosmological expansion.","These calculations have not fully captured the line and continuum interactions in the radiative transfer, in response to evolution of the radiation field and the variations of thermal and dynamic properties of the line-of-sight medium.","We construct a covariant formulation for the radiative transfer of the 21-cm line and derive the cosmological 21-cm line radiative transfer (C21LRT) equation.","The formulation properly accounts for local emission and absorption processes and the interaction between the line and continuum when the radiation propagates across the expanding Universe to the present observer.","Our C21LRT calculations show that methods simply summing the line optical depth could lead to error of $5\\%$ in the 21-cm signals for redshift $z \\sim 12-35$ and of $>10\\%$ for redshift $z \\lesssim 8$. Proper covariant radiative transfer is therefore necessary for producing correct theoretical templates for extracting information of the structural evolution of the Universe through the Epoch of Reionisation from the 21-cm tomographic data."],"url":"http://arxiv.org/abs/2404.14407v1","category":"astro-ph.CO"}
{"created":"2024-04-22 17:58:36","title":"A mean curvature flow arising in adversarial training","abstract":"We connect adversarial training for binary classification to a geometric evolution equation for the decision boundary. Relying on a perspective that recasts adversarial training as a regularization problem, we introduce a modified training scheme that constitutes a minimizing movements scheme for a nonlocal perimeter functional. We prove that the scheme is monotone and consistent as the adversarial budget vanishes and the perimeter localizes, and as a consequence we rigorously show that the scheme approximates a weighted mean curvature flow. This highlights that the efficacy of adversarial training may be due to locally minimizing the length of the decision boundary. In our analysis, we introduce a variety of tools for working with the subdifferential of a supremal-type nonlocal total variation and its regularity properties.","sentences":["We connect adversarial training for binary classification to a geometric evolution equation for the decision boundary.","Relying on a perspective that recasts adversarial training as a regularization problem, we introduce a modified training scheme that constitutes a minimizing movements scheme for a nonlocal perimeter functional.","We prove that the scheme is monotone and consistent as the adversarial budget vanishes and the perimeter localizes, and as a consequence we rigorously show that the scheme approximates a weighted mean curvature flow.","This highlights that the efficacy of adversarial training may be due to locally minimizing the length of the decision boundary.","In our analysis, we introduce a variety of tools for working with the subdifferential of a supremal-type nonlocal total variation and its regularity properties."],"url":"http://arxiv.org/abs/2404.14402v1","category":"math.AP"}
{"created":"2024-04-22 17:58:34","title":"A Python GPU-accelerated solver for the Gross-Pitaevskii equation and applications to many-body cavity QED","abstract":"TorchGPE is a general-purpose Python package developed for solving the Gross-Pitaevskii equation (GPE). This solver is designed to integrate wave functions across a spectrum of linear and non-linear potentials. A distinctive aspect of TorchGPE is its modular approach, which allows the incorporation of arbitrary self-consistent and time-dependent potentials, e.g., those relevant in many-body cavity QED models. The package employs a symmetric split-step Fourier propagation method, effective in both real and imaginary time. In our work, we demonstrate a significant improvement in computational efficiency by leveraging GPU computing capabilities. With the integration of the latter technology, TorchGPE achieves a substantial speed-up with respect to conventional CPU-based methods, greatly expanding the scope and potential of research in this field.","sentences":["TorchGPE is a general-purpose Python package developed for solving the Gross-Pitaevskii equation (GPE).","This solver is designed to integrate wave functions across a spectrum of linear and non-linear potentials.","A distinctive aspect of TorchGPE is its modular approach, which allows the incorporation of arbitrary self-consistent and time-dependent potentials, e.g., those relevant in many-body cavity QED models.","The package employs a symmetric split-step Fourier propagation method, effective in both real and imaginary time.","In our work, we demonstrate a significant improvement in computational efficiency by leveraging GPU computing capabilities.","With the integration of the latter technology, TorchGPE achieves a substantial speed-up with respect to conventional CPU-based methods, greatly expanding the scope and potential of research in this field."],"url":"http://arxiv.org/abs/2404.14401v2","category":"physics.comp-ph"}
{"created":"2024-04-22 17:58:29","title":"On the convergence rates of discrete solutions to the Wave Kinetic Equation","abstract":"In this paper, we consider the long-term behavior of some special solutions to the Wave Kinetic Equation (WKE). This equation provides a mesoscopic description of wave systems interacting nonlinearly via the cubic NLS equation. Escobedo and Vel\\'azquez showed that, starting with initial data given by countably many Dirac masses, solutions remain a linear combination of countably many Dirac masses at all times. Moreover, there is convergence to a single Dirac mass at long times. The first goal of this paper is to give quantitative rates for the speed of said convergence. In order to study the optimality of the bounds we obtain, we introduce and analyze a toy model accounting only for the leading order quadratic interactions.","sentences":["In this paper, we consider the long-term behavior of some special solutions to the Wave Kinetic Equation (WKE).","This equation provides a mesoscopic description of wave systems interacting nonlinearly via the cubic NLS equation.","Escobedo and Vel\\'azquez showed that, starting with initial data given by countably many Dirac masses, solutions remain a linear combination of countably many Dirac masses at all times.","Moreover, there is convergence to a single Dirac mass at long times.","The first goal of this paper is to give quantitative rates for the speed of said convergence.","In order to study the optimality of the bounds we obtain, we introduce and analyze a toy model accounting only for the leading order quadratic interactions."],"url":"http://arxiv.org/abs/2404.14400v1","category":"math.AP"}
{"created":"2024-04-22 17:36:34","title":"A unified theory of tunneling times promoted by Ramsey clocks","abstract":"What time does a clock tell after quantum tunneling? Predictions and indirect measurements range from superluminal or instantaneous tunneling to finite durations, depending on the specific experiment and the precise definition of the elapsed time. Proposals and implementations utilize the atomic motion to define this delay, even though the inherent quantum nature of atoms implies a delocalization and is in sharp contrast to classical trajectories. Here, we rely on an operational approach: we prepare atoms in a coherent superposition of internal states and study the time read off via a Ramsey sequence after the tunneling process without the notion of classical trajectories or velocities. Our operational framework (a) unifies definitions of tunneling delay within one approach; (b) connects the time to a frequency standard given by a conventional atomic clock which can be boosted by differential light shifts; and (c) highlights that there exists no superluminal or instantaneous tunneling.","sentences":["What time does a clock tell after quantum tunneling?","Predictions and indirect measurements range from superluminal or instantaneous tunneling to finite durations, depending on the specific experiment and the precise definition of the elapsed time.","Proposals and implementations utilize the atomic motion to define this delay, even though the inherent quantum nature of atoms implies a delocalization and is in sharp contrast to classical trajectories.","Here, we rely on an operational approach: we prepare atoms in a coherent superposition of internal states and study the time read off via a Ramsey sequence after the tunneling process without the notion of classical trajectories or velocities.","Our operational framework (a) unifies definitions of tunneling delay within one approach; (b) connects the time to a frequency standard given by a conventional atomic clock which can be boosted by differential light shifts; and (c) highlights that there exists no superluminal or instantaneous tunneling."],"url":"http://arxiv.org/abs/2404.14382v1","category":"quant-ph"}
{"created":"2024-04-22 17:18:36","title":"An inverse problem in Polya-Schur theory. I. Non-genegerate and degenerate operators","abstract":"Given a linear ordinary differential operator T with polynomial coefficients, we study the class of closed subsets of the complex plane such that T sends any polynomial (resp. any polynomial of degree exceeding a given positive integer) with all roots in a given subset to a polynomial with all roots in the same subset or to 0. Below we discuss some general properties of such invariant subsets as well as the problem of existence of the minimal under inclusion invariant subset.","sentences":["Given a linear ordinary differential operator T with polynomial coefficients, we study the class of closed subsets of the complex plane such that T sends any polynomial (resp.","any polynomial of degree exceeding a given positive integer) with all roots in a given subset to a polynomial with all roots in the same subset or to 0.","Below we discuss some general properties of such invariant subsets as well as the problem of existence of the minimal under inclusion invariant subset."],"url":"http://arxiv.org/abs/2404.14365v1","category":"math.CA"}
{"created":"2024-04-22 17:12:58","title":"A General Continuous-Time Formulation of Stochastic ADMM and Its Variants","abstract":"Stochastic versions of the alternating direction method of multiplier (ADMM) and its variants play a key role in many modern large-scale machine learning problems. In this work, we introduce a unified algorithmic framework called generalized stochastic ADMM and investigate their continuous-time analysis. The generalized framework widely includes many stochastic ADMM variants such as standard, linearized and gradient-based ADMM. Our continuous-time analysis provides us with new insights into stochastic ADMM and variants, and we rigorously prove that under some proper scaling, the trajectory of stochastic ADMM weakly converges to the solution of a stochastic differential equation with small noise. Our analysis also provides a theoretical explanation of why the relaxation parameter should be chosen between 0 and 2.","sentences":["Stochastic versions of the alternating direction method of multiplier (ADMM) and its variants play a key role in many modern large-scale machine learning problems.","In this work, we introduce a unified algorithmic framework called generalized stochastic ADMM and investigate their continuous-time analysis.","The generalized framework widely includes many stochastic ADMM variants such as standard, linearized and gradient-based ADMM.","Our continuous-time analysis provides us with new insights into stochastic ADMM and variants, and we rigorously prove that under some proper scaling, the trajectory of stochastic ADMM weakly converges to the solution of a stochastic differential equation with small noise.","Our analysis also provides a theoretical explanation of why the relaxation parameter should be chosen between 0 and 2."],"url":"http://arxiv.org/abs/2404.14358v1","category":"math.OC"}
{"created":"2024-04-22 17:03:16","title":"Integration of first-order ODEs by Jacobi fields","abstract":"A new class of vector fields enabling the integration of first-order ordinary differential equations (ODEs) is introduced. These vector fields are not, in general, Lie point symmetries. The results are based on a relation between 2-dimensional Riemannian manifolds and the integrability of first-order ODEs, which was established in a previous work of the authors. An integration procedure is provided, together with several examples to illustrate it. A connection between integrating factors of first-order ODEs and Schr\\\"odinger-type equations is highlighted.","sentences":["A new class of vector fields enabling the integration of first-order ordinary differential equations (ODEs) is introduced.","These vector fields are not, in general, Lie point symmetries.","The results are based on a relation between 2-dimensional Riemannian manifolds and the integrability of first-order ODEs, which was established in a previous work of the authors.","An integration procedure is provided, together with several examples to illustrate it.","A connection between integrating factors of first-order ODEs and Schr\\\"odinger-type equations is highlighted."],"url":"http://arxiv.org/abs/2404.14352v1","category":"math.CA"}
{"created":"2024-04-22 16:58:05","title":"Quantifying Scalar Field Dynamics with DESI 2024 Y1 BAO measurements","abstract":"Quintessence scalar fields are a natural candidate for evolving dark energy. Unlike the phenomenological $w_0w_a$ parameterization of the dark energy equation of state, they cannot accommodate the phantom regime of dark energy $w(z) < -1$, or crossings into the phantom regime. Recent baryon acoustic oscillation (BAO) measurements by the Dark Energy Spectroscopic Instrument (DESI) indicate a preference for evolving dark energy over a cosmological constant, ranging from $2.6\\sigma-3.9\\sigma$ when fitting to $w_0w_a$, and combining the DESI BAO measurements with other cosmological probes. In this work, we directly fit three simple scalar field models to the DESI BAO data, combined with cosmic microwave background anisotropy measurements and supernova data sets. Quantifying the preference for scalar field dynamics exhibited by the data, we find that $2-4\\%$ of kinetic scalar field energy $\\Omega_{\\rm scf,k}$, is preferred over $\\Lambda$CDM at the $95\\%$ confidence level, for a canonical scalar field with a quadratic or linear potential. Fitting to the supernova data sets Pantheon, Pantheon+, DES-Y5, and Union3, we show that the mild tension ($n_{\\sigma}< 3.4 $) under $\\Lambda$CDM emerges from a BAO preference for smaller values of fractional mass-energy density $\\Omega_m < 0.29$, while all supernova data sets, except for Pantheon, prefer larger values, $\\Omega_m > 0.3$. The tension under $\\Lambda$CDM remains noticeable ($n_{\\sigma} <2.8$), when replacing two of the DESI BAO measurements redshift bins with effective redshifts $z_{\\text{eff}} =0.51$, and $z_{\\text{eff}}= 0.706$ with comparable BOSS DR 12 BAO measurements at $z_{\\text{eff}} =0.51$, and $z_{\\text{eff}}= 0.61$. Canonical scalar fields as dark energy are successful in mitigating that tension.","sentences":["Quintessence scalar fields are a natural candidate for evolving dark energy.","Unlike the phenomenological $w_0w_a$ parameterization of the dark energy equation of state, they cannot accommodate the phantom regime of dark energy $w(z) <","-1$, or crossings into the phantom regime.","Recent baryon acoustic oscillation (BAO) measurements by the Dark Energy Spectroscopic Instrument (DESI) indicate a preference for evolving dark energy over a cosmological constant, ranging from $2.6\\sigma-3.9\\sigma$ when fitting to $w_0w_a$, and combining the DESI BAO measurements with other cosmological probes.","In this work, we directly fit three simple scalar field models to the DESI BAO data, combined with cosmic microwave background anisotropy measurements and supernova data sets.","Quantifying the preference for scalar field dynamics exhibited by the data, we find that $2-4\\%$ of kinetic scalar field energy $\\Omega_{\\rm scf,k}$, is preferred over $\\Lambda$CDM at the $95\\%$ confidence level, for a canonical scalar field with a quadratic or linear potential.","Fitting to the supernova data sets Pantheon, Pantheon+, DES-Y5, and Union3, we show that the mild tension ($n_{\\sigma}< 3.4 $) under $\\Lambda$CDM emerges from a BAO preference for smaller values of fractional mass-energy density $\\Omega_m < 0.29$, while all supernova data sets, except for Pantheon, prefer larger values, $\\Omega_m > 0.3$.","The tension under $\\Lambda$CDM remains noticeable ($n_{\\sigma} <2.8$), when replacing two of the DESI BAO measurements redshift bins with effective redshifts $z_{\\text{eff}} =0.51$, and $z_{\\text{eff}}= 0.706$ with comparable BOSS DR 12 BAO measurements at $z_{\\text{eff}} =0.51$, and $z_{\\text{eff}}= 0.61$. Canonical scalar fields as dark energy are successful in mitigating that tension."],"url":"http://arxiv.org/abs/2404.14341v1","category":"astro-ph.CO"}
{"created":"2024-04-22 16:46:14","title":"Divergence-free framings of three-manifolds via eigenspinors","abstract":"Gromov used convex integration to prove that any closed orientable three-manifold equipped with a volume form admits three divergence-free vector fields which are linearly independent at every point. We provide an alternative proof of this (inspired by Seiberg-Witten theory) using geometric properties of eigenspinors in three dimensions. In fact, our proof shows that for any Riemannian metric, one can find three divergence-free vector fields such that at every point they are orthogonal and have the same non-zero length.","sentences":["Gromov used convex integration to prove that any closed orientable three-manifold equipped with a volume form admits three divergence-free vector fields which are linearly independent at every point.","We provide an alternative proof of this (inspired by Seiberg-Witten theory) using geometric properties of eigenspinors in three dimensions.","In fact, our proof shows that for any Riemannian metric, one can find three divergence-free vector fields such that at every point they are orthogonal and have the same non-zero length."],"url":"http://arxiv.org/abs/2404.14331v1","category":"math.DG"}
{"created":"2024-04-22 16:31:32","title":"Bisecting masses with families of parallel hyperplanes","abstract":"We prove a common generalization to several mass partition results using hyperplane arrangements to split $\\mathbb{R}^d$ into two sets. Our main result implies the ham-sandwich theorem, the necklace splitting theorem for two thieves, a theorem about chessboard splittings with hyperplanes with fixed directions, and all known cases of Langerman's conjecture about equipartitions with $n$ hyperplanes.   Our main result also confirms an infinite number of previously unknown cases of the following conjecture of Takahashi and Sober\\'on:   For any $d+k-1$ measures in $\\mathbb{R}^d$, there exist an arrangement of $k$ parallel hyperplanes that bisects each of the measures.   The general result follows from the case of measures that are supported on a finite set with an odd number of points. The proof for this case is inspired by ideas of differential and algebraic topology, but it is a completely elementary parity argument.","sentences":["We prove a common generalization to several mass partition results using hyperplane arrangements to split $\\mathbb{R}^d$ into two sets.","Our main result implies the ham-sandwich theorem, the necklace splitting theorem for two thieves, a theorem about chessboard splittings with hyperplanes with fixed directions, and all known cases of Langerman's conjecture about equipartitions with $n$ hyperplanes.   ","Our main result also confirms an infinite number of previously unknown cases of the following conjecture of Takahashi and Sober\\'on:   For any $d+k-1$ measures in $\\mathbb{R}^d$, there exist an arrangement of $k$ parallel hyperplanes that bisects each of the measures.   ","The general result follows from the case of measures that are supported on a finite set with an odd number of points.","The proof for this case is inspired by ideas of differential and algebraic topology, but it is a completely elementary parity argument."],"url":"http://arxiv.org/abs/2404.14320v1","category":"math.CO"}
{"created":"2024-04-22 16:16:06","title":"Structure-preserving neural networks for the regularzied entropy-based closure of the Boltzmann moment system","abstract":"The main challenge of large-scale numerical simulation of radiation transport is the high memory and computation time requirements of discretization methods for kinetic equations. In this work, we derive and investigate a neural network-based approximation to the entropy closure method to accurately compute the solution of the multi-dimensional moment system with a low memory footprint and competitive computational time. We extend methods developed for the standard entropy-based closure to the context of regularized entropy-based closures. The main idea is to interpret structure-preserving neural network approximations of the regularized entropy closure as a two-stage approximation to the original entropy closure. We conduct a numerical analysis of this approximation and investigate optimal parameter choices. Our numerical experiments demonstrate that the method has a much lower memory footprint than traditional methods with competitive computation times and simulation accuracy. The code and all trained networks are provided on GitHub\\footnote{\\url{https://github.com/ScSteffen/neuralEntropyClosures}}$^,$\\footnote{\\url{https://github.com/CSMMLab/KiT-RT}}.","sentences":["The main challenge of large-scale numerical simulation of radiation transport is the high memory and computation time requirements of discretization methods for kinetic equations.","In this work, we derive and investigate a neural network-based approximation to the entropy closure method to accurately compute the solution of the multi-dimensional moment system with a low memory footprint and competitive computational time.","We extend methods developed for the standard entropy-based closure to the context of regularized entropy-based closures.","The main idea is to interpret structure-preserving neural network approximations of the regularized entropy closure as a two-stage approximation to the original entropy closure.","We conduct a numerical analysis of this approximation and investigate optimal parameter choices.","Our numerical experiments demonstrate that the method has a much lower memory footprint than traditional methods with competitive computation times and simulation accuracy.","The code and all trained networks are provided on GitHub\\footnote{\\url{https://github.com/ScSteffen/neuralEntropyClosures}}$^,$\\footnote{\\url{https://github.com/CSMMLab/KiT-RT}}."],"url":"http://arxiv.org/abs/2404.14312v1","category":"math.NA"}
{"created":"2024-04-22 15:47:00","title":"Navier-Stokes equations for nearly integrable quantum gases","abstract":"The Navier-Stokes equations are paradigmatic equations describing hydrodynamics of an interacting system with microscopic interactions encoded in transport coefficients. In this work we show how the Navier-Stokes equations arise from the microscopic dynamics of nearly integrable $1d$ quantum many-body systems. We build upon the recently developed hydrodynamics of integrable models to study the effective Boltzmann equation with collision integral taking into account the non-integrable interactions. We compute the transport coefficients and find that the resulting Navier-Stokes equations have two regimes, which differ in the viscous properties of the resulting fluid. We illustrate the method by computing the transport coefficients for an experimentally relevant case of coupled 1d cold-atomic gases.","sentences":["The Navier-Stokes equations are paradigmatic equations describing hydrodynamics of an interacting system with microscopic interactions encoded in transport coefficients.","In this work we show how the Navier-Stokes equations arise from the microscopic dynamics of nearly integrable $1d$ quantum many-body systems.","We build upon the recently developed hydrodynamics of integrable models to study the effective Boltzmann equation with collision integral taking into account the non-integrable interactions.","We compute the transport coefficients and find that the resulting Navier-Stokes equations have two regimes, which differ in the viscous properties of the resulting fluid.","We illustrate the method by computing the transport coefficients for an experimentally relevant case of coupled 1d cold-atomic gases."],"url":"http://arxiv.org/abs/2404.14292v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-22 15:39:25","title":"The asymptotic stability on the line of ground states of the pure power NLS with $0<|p-3|\\ll 1$","abstract":"For exponents $p$ satisfying $0<|p-3|\\ll 1$ and only in the context of spatially even solutions we prove that the ground states of the nonlinear Schr\\\"odinger equation (NLS) with pure power nonlinearity of exponent $p$ in the line are asymptotically stable. The proof is similar to a related result of Martel, preprint arXiv:2312.11016, for a cubic quintic NLS. Here we modify the second part of Martel's argument, replacing the second virial inequality for a transformed problem with a smoothing estimate on the initial problem, appropriately tamed by multiplying the initial variables and equations by a cutoff.","sentences":["For exponents $p$ satisfying $0<|p-3|\\ll 1$ and only in the context of spatially even solutions we prove that the ground states of the nonlinear Schr\\\"odinger equation (NLS) with pure power nonlinearity of exponent $p$ in the line are asymptotically stable.","The proof is similar to a related result of Martel, preprint arXiv:2312.11016, for a cubic quintic NLS.","Here we modify the second part of Martel's argument, replacing the second virial inequality for a transformed problem with a smoothing estimate on the initial problem, appropriately tamed by multiplying the initial variables and equations by a cutoff."],"url":"http://arxiv.org/abs/2404.14287v1","category":"math.AP"}
{"created":"2024-04-22 15:28:42","title":"Co-designing a Sub-millisecond Latency Event-based Eye Tracking System with Submanifold Sparse CNN","abstract":"Eye-tracking technology is integral to numerous consumer electronics applications, particularly in the realm of virtual and augmented reality (VR/AR). These applications demand solutions that excel in three crucial aspects: low-latency, low-power consumption, and precision. Yet, achieving optimal performance across all these fronts presents a formidable challenge, necessitating a balance between sophisticated algorithms and efficient backend hardware implementations. In this study, we tackle this challenge through a synergistic software/hardware co-design of the system with an event camera. Leveraging the inherent sparsity of event-based input data, we integrate a novel sparse FPGA dataflow accelerator customized for submanifold sparse convolution neural networks (SCNN). The SCNN implemented on the accelerator can efficiently extract the embedding feature vector from each representation of event slices by only processing the non-zero activations. Subsequently, these vectors undergo further processing by a gated recurrent unit (GRU) and a fully connected layer on the host CPU to generate the eye centers. Deployment and evaluation of our system reveal outstanding performance metrics. On the Event-based Eye-Tracking-AIS2024 dataset, our system achieves 81% p5 accuracy, 99.5% p10 accuracy, and 3.71 Mean Euclidean Distance with 0.7 ms latency while only consuming 2.29 mJ per inference. Notably, our solution opens up opportunities for future eye-tracking systems. Code is available at https://github.com/CASR-HKU/ESDA/tree/eye_tracking.","sentences":["Eye-tracking technology is integral to numerous consumer electronics applications, particularly in the realm of virtual and augmented reality (VR/AR).","These applications demand solutions that excel in three crucial aspects: low-latency, low-power consumption, and precision.","Yet, achieving optimal performance across all these fronts presents a formidable challenge, necessitating a balance between sophisticated algorithms and efficient backend hardware implementations.","In this study, we tackle this challenge through a synergistic software/hardware co-design of the system with an event camera.","Leveraging the inherent sparsity of event-based input data, we integrate a novel sparse FPGA dataflow accelerator customized for submanifold sparse convolution neural networks (SCNN).","The SCNN implemented on the accelerator can efficiently extract the embedding feature vector from each representation of event slices by only processing the non-zero activations.","Subsequently, these vectors undergo further processing by a gated recurrent unit (GRU) and a fully connected layer on the host CPU to generate the eye centers.","Deployment and evaluation of our system reveal outstanding performance metrics.","On the Event-based Eye-Tracking-AIS2024 dataset, our system achieves 81% p5 accuracy, 99.5% p10 accuracy, and 3.71 Mean Euclidean Distance with 0.7 ms latency while only consuming 2.29 mJ per inference.","Notably, our solution opens up opportunities for future eye-tracking systems.","Code is available at https://github.com/CASR-HKU/ESDA/tree/eye_tracking."],"url":"http://arxiv.org/abs/2404.14279v1","category":"cs.CV"}
{"created":"2024-04-22 15:23:28","title":"A Locally Divergence-Free Oscillation-Eliminating Discontinuous Galerkin Method for Ideal Magnetohydrodynamic Equations","abstract":"Numerical simulations of ideal compressible magnetohydrodynamic (MHD) equations are challenging, as the solutions are required to be magnetic divergence-free for general cases as well as oscillation-free for cases involving discontinuities. To overcome these difficulties, we develop a locally divergence-free oscillation-eliminating discontinuous Galerkin (LDF-OEDG) method for ideal compressible MHD equations. In the LDF-OEDG method, the numerical solution is advanced in time by using a strong stability preserving Runge-Kutta scheme. Following the solution update in each Runge-Kutta stage, an oscillation-eliminating (OE) procedure is performed to suppress spurious oscillations near discontinuities by damping the modal coefficients of the numerical solution. Subsequently, on each element, the magnetic filed of the oscillation-free DG solution is projected onto a local divergence-free space, to satisfy the divergence-free condition. The OE procedure and the LDF projection are fully decoupled from the Runge-Kutta stage update, and can be non-intrusively integrated into existing DG codes as independent modules. The damping equation of the OE procedure can be solved exactly, making the LDF-OEDG method remain stable under normal CFL conditions. These features enable a straightforward implementation of a high-order LDF-OEDG solver, which can be used to efficiently simulate the ideal compressible MHD equations. Numerical results for benchmark cases demonstrate the high-order accuracy, strong shock capturing capability and robustness of the LDF-OEDG method.","sentences":["Numerical simulations of ideal compressible magnetohydrodynamic (MHD) equations are challenging, as the solutions are required to be magnetic divergence-free for general cases as well as oscillation-free for cases involving discontinuities.","To overcome these difficulties, we develop a locally divergence-free oscillation-eliminating discontinuous Galerkin (LDF-OEDG) method for ideal compressible MHD equations.","In the LDF-OEDG method, the numerical solution is advanced in time by using a strong stability preserving Runge-Kutta scheme.","Following the solution update in each Runge-Kutta stage, an oscillation-eliminating (OE) procedure is performed to suppress spurious oscillations near discontinuities by damping the modal coefficients of the numerical solution.","Subsequently, on each element, the magnetic filed of the oscillation-free DG solution is projected onto a local divergence-free space, to satisfy the divergence-free condition.","The OE procedure and the LDF projection are fully decoupled from the Runge-Kutta stage update, and can be non-intrusively integrated into existing DG codes as independent modules.","The damping equation of the OE procedure can be solved exactly, making the LDF-OEDG method remain stable under normal CFL conditions.","These features enable a straightforward implementation of a high-order LDF-OEDG solver, which can be used to efficiently simulate the ideal compressible MHD equations.","Numerical results for benchmark cases demonstrate the high-order accuracy, strong shock capturing capability and robustness of the LDF-OEDG method."],"url":"http://arxiv.org/abs/2404.14274v1","category":"math.NA"}
{"created":"2024-04-22 15:20:23","title":"Electrical spin manipulation in double SrTiO$_3$/LaAlO$_3$ quantum dots","abstract":"The spin dynamics in two electron double quantum dots embedded in two dimensional electron gas at the interface between SrTiO$_3$ and LaAlO$_3$ is studied by an exact numerical solution of the time-dependent Schr\\\"odinger equation, in the context of the electric dipole spin resonance experiment. Based on the three band model of $3d$-electrons localized at Ti ions on the square lattice we analyze in details the singlet-triplet transition induced by the AC electric field, in the magnetic field range close to the avoided crossing which appears as a result of the spin-orbit coupling. Our calculations show that for symmetric double quantum dots the single photon spin-flip transitions is prohibited due to the parity symmetry and the transition can occur only by the higher order two-photon processes. For a weakly asymmetric system, when the first order singlet-triplet transitions are released due to the parity symmetry breaking, the spin-flip transition has a character of the Rabi oscillations for a low electric field amplitude. As the amplitude is increased the frequency of the transition is blueshifted (redshifted) for the magnetic field below (above) the single-triplet avoided crossing. Interestingly, for a sufficiently high magnetic field and high AC field amplitude the electric field drives the system across the avoided crossing inducing the spin-flip by the Landau-Zener-Stueckelberg-Majorana transitions with 100\\% spin flip probability for a slow sweep. Finally, the optimization of the geometrical parameters of the system with respect to the time of spin-flip of its fidelity is also presented.","sentences":["The spin dynamics in two electron double quantum dots embedded in two dimensional electron gas at the interface between SrTiO$_3$ and LaAlO$_3$ is studied by an exact numerical solution of the time-dependent Schr\\\"odinger equation, in the context of the electric dipole spin resonance experiment.","Based on the three band model of $3d$-electrons localized at Ti ions on the square lattice we analyze in details the singlet-triplet transition induced by the AC electric field, in the magnetic field range close to the avoided crossing which appears as a result of the spin-orbit coupling.","Our calculations show that for symmetric double quantum dots the single photon spin-flip transitions is prohibited due to the parity symmetry and the transition can occur only by the higher order two-photon processes.","For a weakly asymmetric system, when the first order singlet-triplet transitions are released due to the parity symmetry breaking, the spin-flip transition has a character of the Rabi oscillations for a low electric field amplitude.","As the amplitude is increased the frequency of the transition is blueshifted (redshifted) for the magnetic field below (above) the single-triplet avoided crossing.","Interestingly, for a sufficiently high magnetic field and high AC field amplitude the electric field drives the system across the avoided crossing inducing the spin-flip by the Landau-Zener-Stueckelberg-Majorana transitions with 100\\% spin flip probability for a slow sweep.","Finally, the optimization of the geometrical parameters of the system with respect to the time of spin-flip of its fidelity is also presented."],"url":"http://arxiv.org/abs/2404.14272v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-22 15:16:59","title":"Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance Propagation","abstract":"Explainability is a key component in many applications involving deep neural networks (DNNs). However, current explanation methods for DNNs commonly leave it to the human observer to distinguish relevant explanations from spurious noise. This is not feasible anymore when going from easily human-accessible data such as images to more complex data such as genome sequences. To facilitate the accessibility of DNN outputs from such complex data and to increase explainability, we present a modification of the widely used explanation method layer-wise relevance propagation. Our approach enforces sparsity directly by pruning the relevance propagation for the different layers. Thereby, we achieve sparser relevance attributions for the input features as well as for the intermediate layers. As the relevance propagation is input-specific, we aim to prune the relevance propagation rather than the underlying model architecture. This allows to prune different neurons for different inputs and hence, might be more appropriate to the local nature of explanation methods. To demonstrate the efficacy of our method, we evaluate it on two types of data, images and genomic sequences. We show that our modification indeed leads to noise reduction and concentrates relevance on the most important features compared to the baseline.","sentences":["Explainability is a key component in many applications involving deep neural networks (DNNs).","However, current explanation methods for DNNs commonly leave it to the human observer to distinguish relevant explanations from spurious noise.","This is not feasible anymore when going from easily human-accessible data such as images to more complex data such as genome sequences.","To facilitate the accessibility of DNN outputs from such complex data and to increase explainability, we present a modification of the widely used explanation method layer-wise relevance propagation.","Our approach enforces sparsity directly by pruning the relevance propagation for the different layers.","Thereby, we achieve sparser relevance attributions for the input features as well as for the intermediate layers.","As the relevance propagation is input-specific, we aim to prune the relevance propagation rather than the underlying model architecture.","This allows to prune different neurons for different inputs and hence, might be more appropriate to the local nature of explanation methods.","To demonstrate the efficacy of our method, we evaluate it on two types of data, images and genomic sequences.","We show that our modification indeed leads to noise reduction and concentrates relevance on the most important features compared to the baseline."],"url":"http://arxiv.org/abs/2404.14271v1","category":"cs.LG"}
{"created":"2024-04-22 15:12:55","title":"Superoscillations in High Energy Physics and Gravity","abstract":"We explore superoscillations within the context of classical and quantum field theories, presenting novel solutions to Klein-Gordon's, Dirac's, Maxwell's and Einstein's equations. In particular, we illustrate a procedure of second quantization of fields and how to construct a Fock space which encompasses Superoscillating states. Furthermore, we extend the application of superoscillations to quantum tunnelings, scatterings and mixings of particles, squeezed states and potential advancements in laser interferometry, which could open new avenues for experimental tests of Quantum Gravity effects. By delving into the relationship among superoscillations and phenomena such as Hawking radiation, the Black Hole (BH) information and the Firewall paradox, we propose an alternative mechanism for information transfer across the BH event horizon.","sentences":["We explore superoscillations within the context of classical and quantum field theories, presenting novel solutions to Klein-Gordon's, Dirac's, Maxwell's and Einstein's equations.","In particular, we illustrate a procedure of second quantization of fields and how to construct a Fock space which encompasses Superoscillating states.","Furthermore, we extend the application of superoscillations to quantum tunnelings, scatterings and mixings of particles, squeezed states and potential advancements in laser interferometry, which could open new avenues for experimental tests of Quantum Gravity effects.","By delving into the relationship among superoscillations and phenomena such as Hawking radiation, the Black Hole (BH) information and the Firewall paradox, we propose an alternative mechanism for information transfer across the BH event horizon."],"url":"http://arxiv.org/abs/2404.14266v1","category":"hep-th"}
{"created":"2024-04-22 15:12:47","title":"Deep Learning as Ricci Flow","abstract":"Deep neural networks (DNNs) are powerful tools for approximating the distribution of complex data. It is known that data passing through a trained DNN classifier undergoes a series of geometric and topological simplifications. While some progress has been made toward understanding these transformations in neural networks with smooth activation functions, an understanding in the more general setting of non-smooth activation functions, such as the rectified linear unit (ReLU), which tend to perform better, is required. Here we propose that the geometric transformations performed by DNNs during classification tasks have parallels to those expected under Hamilton's Ricci flow - a tool from differential geometry that evolves a manifold by smoothing its curvature, in order to identify its topology. To illustrate this idea, we present a computational framework to quantify the geometric changes that occur as data passes through successive layers of a DNN, and use this framework to motivate a notion of `global Ricci network flow' that can be used to assess a DNN's ability to disentangle complex data geometries to solve classification problems. By training more than $1,500$ DNN classifiers of different widths and depths on synthetic and real-world data, we show that the strength of global Ricci network flow-like behaviour correlates with accuracy for well-trained DNNs, independently of depth, width and data set. Our findings motivate the use of tools from differential and discrete geometry to the problem of explainability in deep learning.","sentences":["Deep neural networks (DNNs) are powerful tools for approximating the distribution of complex data.","It is known that data passing through a trained DNN classifier undergoes a series of geometric and topological simplifications.","While some progress has been made toward understanding these transformations in neural networks with smooth activation functions, an understanding in the more general setting of non-smooth activation functions, such as the rectified linear unit (ReLU), which tend to perform better, is required.","Here we propose that the geometric transformations performed by DNNs during classification tasks have parallels to those expected under Hamilton's Ricci flow - a tool from differential geometry that evolves a manifold by smoothing its curvature, in order to identify its topology.","To illustrate this idea, we present a computational framework to quantify the geometric changes that occur as data passes through successive layers of a DNN, and use this framework to motivate a notion of `global Ricci network flow' that can be used to assess a DNN's ability to disentangle complex data geometries to solve classification problems.","By training more than $1,500$ DNN classifiers of different widths and depths on synthetic and real-world data, we show that the strength of global Ricci network flow-like behaviour correlates with accuracy for well-trained DNNs, independently of depth, width and data set.","Our findings motivate the use of tools from differential and discrete geometry to the problem of explainability in deep learning."],"url":"http://arxiv.org/abs/2404.14265v1","category":"cs.LG"}
{"created":"2024-04-22 15:12:04","title":"Out-of-equilibrium Chiral Magnetic Effect from simulations on Euclidean lattices","abstract":"We introduce the Euclidean-time correlator of axial charge and electric current as an observable that can be used to study the out-of-equilibrium Chiral Magnetic Effect (CME) in first-principle lattice QCD simulations with background magnetic field. This observable directly reflects the fact that in the background magnetic field, a state with nonzero axial charge features nonzero electric current. For free fermions, the axial-vector correlator only receives contributions from the Lowest Landau Level, and features a linear dependence on both magnetic field and temperature with a universal coefficient. With an appropriate regularization, non-vanishing axial-vector correlator is compatible with the vanishing of the CME current in thermal equilibrium state with nonzero chiral chemical potential $\\mu_5$. We demonstrate that the real-time counterpart of the Euclidean-time axial-vector correlator is intimately related to the real-time form of the axial anomaly equation, which strongly limits possible corrections in full QCD. We present numerical results for the Euclidean-time axial-vector correlator in $SU(2)$ lattice gauge theory with $N_f = 2$ light quark flavours, demonstrating perfect agreement with free fermion result on both sides of the chiral crossover. The proposed methodology should help to answer the question whether the QCD corrections might be responsible for non-observation of CME in RHIC isobar run.","sentences":["We introduce the Euclidean-time correlator of axial charge and electric current as an observable that can be used to study the out-of-equilibrium Chiral Magnetic Effect (CME) in first-principle lattice QCD simulations with background magnetic field.","This observable directly reflects the fact that in the background magnetic field, a state with nonzero axial charge features nonzero electric current.","For free fermions, the axial-vector correlator only receives contributions from the Lowest Landau Level, and features a linear dependence on both magnetic field and temperature with a universal coefficient.","With an appropriate regularization, non-vanishing axial-vector correlator is compatible with the vanishing of the CME current in thermal equilibrium state with nonzero chiral chemical potential $\\mu_5$. We demonstrate that the real-time counterpart of the Euclidean-time axial-vector correlator is intimately related to the real-time form of the axial anomaly equation, which strongly limits possible corrections in full QCD.","We present numerical results for the Euclidean-time axial-vector correlator in $SU(2)$ lattice gauge theory with $N_f = 2$ light quark flavours, demonstrating perfect agreement with free fermion result on both sides of the chiral crossover.","The proposed methodology should help to answer the question whether the QCD corrections might be responsible for non-observation of CME in RHIC isobar run."],"url":"http://arxiv.org/abs/2404.14263v1","category":"hep-lat"}
{"created":"2024-04-22 15:07:57","title":"Quantum-Enhanced Neural Exchange-Correlation Functionals","abstract":"Kohn-Sham Density Functional Theory (KS-DFT) provides the exact ground state energy and electron density of a molecule, contingent on the as-yet-unknown universal exchange-correlation (XC) functional. Recent research has demonstrated that neural networks can efficiently learn to represent approximations to that functional, offering accurate generalizations to molecules not present during the training process. With the latest advancements in quantum-enhanced machine learning (ML), evidence is growing that Quantum Neural Network (QNN) models may offer advantages in ML applications. In this work, we explore the use of QNNs for representing XC functionals, enhancing and comparing them to classical ML techniques. We present QNNs based on differentiable quantum circuits (DQCs) as quantum (hybrid) models for XC in KS-DFT, implemented across various architectures. We assess their performance on 1D and 3D systems. To that end, we expand existing differentiable KS-DFT frameworks and propose strategies for efficient training of such functionals, highlighting the importance of fractional orbital occupation for accurate results. Our best QNN-based XC functional yields energy profiles of the H$_2$ and planar H$_4$ molecules that deviate by no more than 1 mHa from the reference DMRG and FCI/6-31G results, respectively. Moreover, they reach chemical precision on a system, H$_2$H$_2$, not present in the training dataset, using only a few variational parameters. This work lays the foundation for the integration of quantum models in KS-DFT, thereby opening new avenues for expressing XC functionals in a differentiable way and facilitating computations of various properties.","sentences":["Kohn-Sham Density Functional Theory (KS-DFT) provides the exact ground state energy and electron density of a molecule, contingent on the as-yet-unknown universal exchange-correlation (XC) functional.","Recent research has demonstrated that neural networks can efficiently learn to represent approximations to that functional, offering accurate generalizations to molecules not present during the training process.","With the latest advancements in quantum-enhanced machine learning (ML), evidence is growing that Quantum Neural Network (QNN) models may offer advantages in ML applications.","In this work, we explore the use of QNNs for representing XC functionals, enhancing and comparing them to classical ML techniques.","We present QNNs based on differentiable quantum circuits (DQCs) as quantum (hybrid) models for XC in KS-DFT, implemented across various architectures.","We assess their performance on 1D and 3D systems.","To that end, we expand existing differentiable KS-DFT frameworks and propose strategies for efficient training of such functionals, highlighting the importance of fractional orbital occupation for accurate results.","Our best QNN-based XC functional yields energy profiles of the H$_2$ and planar H$_4$ molecules that deviate by no more than 1 mHa from the reference DMRG and FCI/6-31G results, respectively.","Moreover, they reach chemical precision on a system, H$_2$H$_2$, not present in the training dataset, using only a few variational parameters.","This work lays the foundation for the integration of quantum models in KS-DFT, thereby opening new avenues for expressing XC functionals in a differentiable way and facilitating computations of various properties."],"url":"http://arxiv.org/abs/2404.14258v1","category":"quant-ph"}
{"created":"2024-04-22 15:07:19","title":"Multielectron effect in strong-field ionization of CO","abstract":"We investigate the effects of the multielectron polarization of the ion described by the induced dipole potential in photoelectron momentum distributions produced in ionization of the CO molecule by a strong laser field. We present results of the numerical solution of the time-dependent Schr\\\"{o}dinger equation in three spatial dimensions and semiclassical simulations accounting for quantum interference. We predict the change of the asymmetry and interference patterns in two-dimensional photoelectron momentum distributions as well as longitudinal momentum distributions. By using a semiclassical model we identify the mechanism responsible for the observed effects. It is shown that the modifications of electron momentum distributions are caused by a combined effect of the force acting on photoelectrons due to the induced dipole potential and the linear Stark-shift of the ionization potential.","sentences":["We investigate the effects of the multielectron polarization of the ion described by the induced dipole potential in photoelectron momentum distributions produced in ionization of the CO molecule by a strong laser field.","We present results of the numerical solution of the time-dependent Schr\\\"{o}dinger equation in three spatial dimensions and semiclassical simulations accounting for quantum interference.","We predict the change of the asymmetry and interference patterns in two-dimensional photoelectron momentum distributions as well as longitudinal momentum distributions.","By using a semiclassical model we identify the mechanism responsible for the observed effects.","It is shown that the modifications of electron momentum distributions are caused by a combined effect of the force acting on photoelectrons due to the induced dipole potential and the linear Stark-shift of the ionization potential."],"url":"http://arxiv.org/abs/2404.14254v1","category":"physics.atom-ph"}
{"created":"2024-04-22 14:46:20","title":"Good, but not very good orbifolds","abstract":"We construct examples of (effective) closed orbifolds which are covered by manifolds, but not finitely so.","sentences":["We construct examples of (effective) closed orbifolds which are covered by manifolds, but not finitely so."],"url":"http://arxiv.org/abs/2404.14234v1","category":"math.GT"}
{"created":"2024-04-22 14:40:30","title":"Colored Stochastic Multiplicative Processes with Additive Noise Unveil a Third-Order PDE, Defying Conventional FPE and Fick-Law Paradigms","abstract":"Research on stochastic differential equations (SDE) involving both additive and multiplicative noise has been extensive. In situations where the primary process is driven by a multiplicative stochastic process, additive white noise typically represents an intrinsic and unavoidable fast factor, including phenomena like thermal fluctuations, inherent uncertainties in measurement processes, or rapid wind forcing in ocean dynamics. This work focuses on a significant class of such systems, particularly those characterized by linear drift and multiplicative noise, extensively explored in the literature. Conventionally, multiplicative stochastic processes are also treated as white noise in existing studies. However, when considering colored multiplicative noise, the emphasis has been on characterizing the far tails of the probability density function (PDF), regardless of the spectral properties of the noise. In the absence of additive noise and with a general colored multiplicative SDE, standard perturbation approaches lead to a second-order PDE known as the Fokker-Planck Equation (FPE), consistent with Fick's law. This investigation unveils a notable departure from this standard behavior when introducing additive white noise. At the leading order of the stochastic process strength, perturbation approaches yield a \\textit{third-order PDE}, irrespective of the white noise intensity. The breakdown of the FPE further signifies the breakdown of Fick's law. Additionally, we derive the explicit solution for the equilibrium PDF corresponding to this third-order PDE Master Equation. Through numerical simulations, we demonstrate significant deviations from outcomes derived using the FPE obtained through the application of Fick's law.","sentences":["Research on stochastic differential equations (SDE) involving both additive and multiplicative noise has been extensive.","In situations where the primary process is driven by a multiplicative stochastic process, additive white noise typically represents an intrinsic and unavoidable fast factor, including phenomena like thermal fluctuations, inherent uncertainties in measurement processes, or rapid wind forcing in ocean dynamics.","This work focuses on a significant class of such systems, particularly those characterized by linear drift and multiplicative noise, extensively explored in the literature.","Conventionally, multiplicative stochastic processes are also treated as white noise in existing studies.","However, when considering colored multiplicative noise, the emphasis has been on characterizing the far tails of the probability density function (PDF), regardless of the spectral properties of the noise.","In the absence of additive noise and with a general colored multiplicative SDE, standard perturbation approaches lead to a second-order PDE known as the Fokker-Planck Equation (FPE), consistent with Fick's law.","This investigation unveils a notable departure from this standard behavior when introducing additive white noise.","At the leading order of the stochastic process strength, perturbation approaches yield a \\textit{third-order PDE}, irrespective of the white noise intensity.","The breakdown of the FPE further signifies the breakdown of Fick's law.","Additionally, we derive the explicit solution for the equilibrium PDF corresponding to this third-order PDE Master Equation.","Through numerical simulations, we demonstrate significant deviations from outcomes derived using the FPE obtained through the application of Fick's law."],"url":"http://arxiv.org/abs/2404.14229v1","category":"math.ST"}
{"created":"2024-04-22 14:38:58","title":"A Survey of Decomposition-Based Evolutionary Multi-Objective Optimization: Part II -- A Data Science Perspective","abstract":"This paper presents the second part of the two-part survey series on decomposition-based evolutionary multi-objective optimization where we mainly focus on discussing the literature related to multi-objective evolutionary algorithms based on decomposition (MOEA/D). Complementary to the first part, here we employ a series of advanced data mining approaches to provide a comprehensive anatomy of the enormous landscape of MOEA/D research, which is far beyond the capacity of classic manual literature review protocol. In doing so, we construct a heterogeneous knowledge graph that encapsulates more than 5,400 papers, 10,000 authors, 400 venues, and 1,600 institutions for MOEA/D research. We start our analysis with basic descriptive statistics. Then we delve into prominent research/application topics pertaining to MOEA/D with state-of-the-art topic modeling techniques and interrogate their sptial-temporal and bilateral relationships. We also explored the collaboration and citation networks of MOEA/D, uncovering hidden patterns in the growth of literature as well as collaboration between researchers. Our data mining results here, combined with the expert review in Part I, together offer a holistic view of the MOEA/D research, and demonstrate the potential of an exciting new paradigm for conducting scientific surveys from a data science perspective.","sentences":["This paper presents the second part of the two-part survey series on decomposition-based evolutionary multi-objective optimization where we mainly focus on discussing the literature related to multi-objective evolutionary algorithms based on decomposition (MOEA/D).","Complementary to the first part, here we employ a series of advanced data mining approaches to provide a comprehensive anatomy of the enormous landscape of MOEA/D research, which is far beyond the capacity of classic manual literature review protocol.","In doing so, we construct a heterogeneous knowledge graph that encapsulates more than 5,400 papers, 10,000 authors, 400 venues, and 1,600 institutions for MOEA/D research.","We start our analysis with basic descriptive statistics.","Then we delve into prominent research/application topics pertaining to MOEA/D with state-of-the-art topic modeling techniques and interrogate their sptial-temporal and bilateral relationships.","We also explored the collaboration and citation networks of MOEA/D, uncovering hidden patterns in the growth of literature as well as collaboration between researchers.","Our data mining results here, combined with the expert review in Part I, together offer a holistic view of the MOEA/D research, and demonstrate the potential of an exciting new paradigm for conducting scientific surveys from a data science perspective."],"url":"http://arxiv.org/abs/2404.14228v1","category":"cs.NE"}
{"created":"2024-04-22 14:35:05","title":"Modeling principles for a physiology-based whole-body model of human metabolism","abstract":"Physiological whole-body models are valuable tools for the development of novel drugs where understanding the system aspects is important. This paper presents a generalized model that encapsulates the structure and flow of whole-body human physiology. The model contains vascular, interstitial, and cellular subcompartments for each organ. Scaling of volumes and blood flows is described to allow for investigation across populations or specific patient groups. The model equations and the corresponding parameters are presented along with a catalog of functions that can be used to define the organ transport model and the biochemical reaction model. A simple example illustrates the procedure.","sentences":["Physiological whole-body models are valuable tools for the development of novel drugs where understanding the system aspects is important.","This paper presents a generalized model that encapsulates the structure and flow of whole-body human physiology.","The model contains vascular, interstitial, and cellular subcompartments for each organ.","Scaling of volumes and blood flows is described to allow for investigation across populations or specific patient groups.","The model equations and the corresponding parameters are presented along with a catalog of functions that can be used to define the organ transport model and the biochemical reaction model.","A simple example illustrates the procedure."],"url":"http://arxiv.org/abs/2404.14224v1","category":"q-bio.QM"}
{"created":"2024-04-22 14:21:37","title":"Toward Routing River Water in Land Surface Models with Recurrent Neural Networks","abstract":"Machine learning is playing an increasing role in hydrology, supplementing or replacing physics-based models. One notable example is the use of recurrent neural networks (RNNs) for forecasting streamflow given observed precipitation and geographic characteristics. Training of such a model over the continental United States has demonstrated that a single set of model parameters can be used across independent catchments, and that RNNs can outperform physics-based models. In this work, we take a next step and study the performance of RNNs for river routing in land surface models (LSMs). Instead of observed precipitation, the LSM-RNN uses instantaneous runoff calculated from physics-based models as an input. We train the model with data from river basins spanning the globe and test it in streamflow hindcasts. The model demonstrates skill at generalization across basins (predicting streamflow in unseen catchments) and across time (predicting streamflow during years not used in training). We compare the predictions from the LSM-RNN to an existing physics-based model calibrated with a similar dataset and find that the LSM-RNN outperforms the physics-based model. Our results give further evidence that RNNs are effective for global streamflow prediction from runoff inputs and motivate the development of complete routing models that can capture nested sub-basis connections.","sentences":["Machine learning is playing an increasing role in hydrology, supplementing or replacing physics-based models.","One notable example is the use of recurrent neural networks (RNNs) for forecasting streamflow given observed precipitation and geographic characteristics.","Training of such a model over the continental United States has demonstrated that a single set of model parameters can be used across independent catchments, and that RNNs can outperform physics-based models.","In this work, we take a next step and study the performance of RNNs for river routing in land surface models (LSMs).","Instead of observed precipitation, the LSM-RNN uses instantaneous runoff calculated from physics-based models as an input.","We train the model with data from river basins spanning the globe and test it in streamflow hindcasts.","The model demonstrates skill at generalization across basins (predicting streamflow in unseen catchments) and across time (predicting streamflow during years not used in training).","We compare the predictions from the LSM-RNN to an existing physics-based model calibrated with a similar dataset and find that the LSM-RNN outperforms the physics-based model.","Our results give further evidence that RNNs are effective for global streamflow prediction from runoff inputs and motivate the development of complete routing models that can capture nested sub-basis connections."],"url":"http://arxiv.org/abs/2404.14212v1","category":"physics.comp-ph"}
{"created":"2024-04-22 14:14:46","title":"Properties of the `friend of a friend' model for network generation","abstract":"The way in which a social network is generated, in terms of how individuals attach to each other, determines the properties of the resulting network. Here we study an intuitively appealing `friend of a friend' model, where a network is formed by each newly added individual attaching first to a randomly chosen target and then to $n_q\\geq 1$ randomly chosen friends of the target, each with probability $0<q\\leq1$. We revisit the master equation of the expected degree distribution for this model, providing an exact solution for the case when $n_q$ allows for attachment to all of the chosen target's friends (a case previously studied by \\cite{lambiotte2016}), and demonstrating why such a solution is hard to obtain when $n_q$ is fixed (a case previously studied by \\cite{Levens2022}.) In the case where attachment to all friends is allowed, we also show that when $q<q^*\\approx0.5671$, the expected degree distribution of the model is stationary as the network size tends to infinity. We go on to look at the clustering behaviour and the triangle count, focusing on the cases where $n_q$ is fixed.","sentences":["The way in which a social network is generated, in terms of how individuals attach to each other, determines the properties of the resulting network.","Here we study an intuitively appealing `friend of a friend' model, where a network is formed by each newly added individual attaching first to a randomly chosen target and then to $n_q\\geq 1$ randomly chosen friends of the target, each with probability $0<q\\leq1$.","We revisit the master equation of the expected degree distribution for this model, providing an exact solution for the case when $n_q$ allows for attachment to all of the chosen target's friends (a case previously studied by \\cite{lambiotte2016}), and demonstrating why such a solution is hard to obtain when $n_q$ is fixed (a case previously studied by \\cite{Levens2022}.)","In the case where attachment to all friends is allowed, we also show that when $q<q^*\\approx0.5671$, the expected degree distribution of the model is stationary as the network size tends to infinity.","We go on to look at the clustering behaviour and the triangle count, focusing on the cases where $n_q$ is fixed."],"url":"http://arxiv.org/abs/2404.14205v1","category":"physics.soc-ph"}
{"created":"2024-04-22 14:13:36","title":"TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading","abstract":"Next-generation mobile networks are expected to facilitate fast AI model downloading to end users. By caching models on edge servers, mobile networks can deliver models to end users with low latency, resulting in a paradigm called edge model caching. In this paper, we develop a novel model placement scheme, called parameter-sharing model caching (TrimCaching). TrimCaching exploits the key observation that a wide range of AI models, such as convolutional neural networks or large language models, can share a significant proportion of parameter blocks containing reusable knowledge, thereby improving storage efficiency. To this end, we formulate a parameter-sharing model placement problem to maximize the cache hit ratio in multi-edge wireless networks by balancing the fundamental tradeoff between storage efficiency and service latency. We show that the formulated problem is a submodular maximization problem with submodular constraints, for which no polynomial-time approximation algorithm exists. To overcome this challenge, we study an important special case, where a small fixed number of parameter blocks are shared across models, which often holds in practice. In such a case, a polynomial-time algorithm with $\\left(1-\\epsilon\\right)/2$-approximation guarantee is developed. Subsequently, we address the original problem for the general case by developing a greedy algorithm. Simulation results demonstrate that the proposed TrimCaching framework significantly improves the cache hit ratio compared with state-of-the-art content caching without exploiting shared parameters in AI models.","sentences":["Next-generation mobile networks are expected to facilitate fast AI model downloading to end users.","By caching models on edge servers, mobile networks can deliver models to end users with low latency, resulting in a paradigm called edge model caching.","In this paper, we develop a novel model placement scheme, called parameter-sharing model caching (TrimCaching).","TrimCaching exploits the key observation that a wide range of AI models, such as convolutional neural networks or large language models, can share a significant proportion of parameter blocks containing reusable knowledge, thereby improving storage efficiency.","To this end, we formulate a parameter-sharing model placement problem to maximize the cache hit ratio in multi-edge wireless networks by balancing the fundamental tradeoff between storage efficiency and service latency.","We show that the formulated problem is a submodular maximization problem with submodular constraints, for which no polynomial-time approximation algorithm exists.","To overcome this challenge, we study an important special case, where a small fixed number of parameter blocks are shared across models, which often holds in practice.","In such a case, a polynomial-time algorithm with $\\left(1-\\epsilon\\right)/2$-approximation guarantee is developed.","Subsequently, we address the original problem for the general case by developing a greedy algorithm.","Simulation results demonstrate that the proposed TrimCaching framework significantly improves the cache hit ratio compared with state-of-the-art content caching without exploiting shared parameters in AI models."],"url":"http://arxiv.org/abs/2404.14204v1","category":"cs.NI"}
{"created":"2024-04-22 14:09:53","title":"Generalizable Neural Human Renderer","abstract":"While recent advancements in animatable human rendering have achieved remarkable results, they require test-time optimization for each subject which can be a significant limitation for real-world applications. To address this, we tackle the challenging task of learning a Generalizable Neural Human Renderer (GNH), a novel method for rendering animatable humans from monocular video without any test-time optimization. Our core method focuses on transferring appearance information from the input video to the output image plane by utilizing explicit body priors and multi-view geometry. To render the subject in the intended pose, we utilize a straightforward CNN-based image renderer, foregoing the more common ray-sampling or rasterizing-based rendering modules. Our GNH achieves remarkable generalizable, photorealistic rendering with unseen subjects with a three-stage process. We quantitatively and qualitatively demonstrate that GNH significantly surpasses current state-of-the-art methods, notably achieving a 31.3% improvement in LPIPS.","sentences":["While recent advancements in animatable human rendering have achieved remarkable results, they require test-time optimization for each subject which can be a significant limitation for real-world applications.","To address this, we tackle the challenging task of learning a Generalizable Neural Human Renderer (GNH), a novel method for rendering animatable humans from monocular video without any test-time optimization.","Our core method focuses on transferring appearance information from the input video to the output image plane by utilizing explicit body priors and multi-view geometry.","To render the subject in the intended pose, we utilize a straightforward CNN-based image renderer, foregoing the more common ray-sampling or rasterizing-based rendering modules.","Our GNH achieves remarkable generalizable, photorealistic rendering with unseen subjects with a three-stage process.","We quantitatively and qualitatively demonstrate that GNH significantly surpasses current state-of-the-art methods, notably achieving a 31.3% improvement in LPIPS."],"url":"http://arxiv.org/abs/2404.14199v1","category":"cs.CV"}
{"created":"2024-04-22 14:03:55","title":"On h-refined meshless solution to Navier-Stokes problem in porous media: comparing meshless Lattice Boltzman Method with ACM RBF-FD approach","abstract":"In this paper, two mesh-free CFD solvers for pore-scale fluid flow through porous media are considered, namely the Lattice Boltzmann Method with the two relaxation time collision term and the direct Navier-Stokes solver under the artificial compressibility limit. The porous media is built with a regular arrangement of spherical grains with variable radii, which allows control of the porosity. Both solvers use the same h-refined meshless spatial discretization to adequately capture the underlying geometry and the same Radial Basis Function (RBF) method to approximate the involved fields and partial differential operators. First, the results are compared with the data from the literature in terms of drag coefficient and permeability at different porosities achieving excellent agreement with the reported results. Next, the simulations are extended beyond the porosity range reported in the literature using proposed h-refined CFD solvers. The results are supported by convergence and timing analyses and discussions on meshless parameters such as stencil size and refinement settings.","sentences":["In this paper, two mesh-free CFD solvers for pore-scale fluid flow through porous media are considered, namely the Lattice Boltzmann Method with the two relaxation time collision term and the direct Navier-Stokes solver under the artificial compressibility limit.","The porous media is built with a regular arrangement of spherical grains with variable radii, which allows control of the porosity.","Both solvers use the same h-refined meshless spatial discretization to adequately capture the underlying geometry and the same Radial Basis Function (RBF) method to approximate the involved fields and partial differential operators.","First, the results are compared with the data from the literature in terms of drag coefficient and permeability at different porosities achieving excellent agreement with the reported results.","Next, the simulations are extended beyond the porosity range reported in the literature using proposed h-refined CFD solvers.","The results are supported by convergence and timing analyses and discussions on meshless parameters such as stencil size and refinement settings."],"url":"http://arxiv.org/abs/2404.14195v1","category":"physics.flu-dyn"}
{"created":"2024-04-22 13:32:29","title":"Sliding window-aided ordered statistics decoding for short LDPC codes","abstract":"This paper introduces an innovative approach to the design of efficient decoders that meet the rigorous requirements of modern communication systems, particularly in terms of ultra-reliability and low latency. We enhance an established hybrid decoding framework by proposing an ordered statistical decoding scheme augmented with a sliding window technique. This novel component replaces a key element of the current architecture, significantly reducing average complexity. A critical aspect of our scheme is the integration of a pre-trained neural network model that dynamically determines the progression or halt of the sliding window process. Furthermore, we present a user-defined soft margin mechanism that adeptly balances the trade-off between decoding accuracy and complexity. Empirical results, supported by a thorough complexity analysis, demonstrate that the proposed scheme holds a competitive advantage over existing state-of-the-art decoders, notably in addressing the decoding failures prevalent in neural min-sum decoders. Additionally, our research uncovers that short LDPC codes can deliver performance comparable to that of short classical linear codes within the critical waterfall region of the SNR, highlighting their potential for practical applications.","sentences":["This paper introduces an innovative approach to the design of efficient decoders that meet the rigorous requirements of modern communication systems, particularly in terms of ultra-reliability and low latency.","We enhance an established hybrid decoding framework by proposing an ordered statistical decoding scheme augmented with a sliding window technique.","This novel component replaces a key element of the current architecture, significantly reducing average complexity.","A critical aspect of our scheme is the integration of a pre-trained neural network model that dynamically determines the progression or halt of the sliding window process.","Furthermore, we present a user-defined soft margin mechanism that adeptly balances the trade-off between decoding accuracy and complexity.","Empirical results, supported by a thorough complexity analysis, demonstrate that the proposed scheme holds a competitive advantage over existing state-of-the-art decoders, notably in addressing the decoding failures prevalent in neural min-sum decoders.","Additionally, our research uncovers that short LDPC codes can deliver performance comparable to that of short classical linear codes within the critical waterfall region of the SNR, highlighting their potential for practical applications."],"url":"http://arxiv.org/abs/2404.14165v1","category":"cs.IT"}
{"created":"2024-04-22 13:01:52","title":"Complete $CP$ Eigen-bases of Mesonic Chiral Lagrangian up to $p^8$-order","abstract":"Chiral perturbation theory systematically describes the low energy dynamics of meson and baryons using nonlinear Nambu-Goldstone fields. Using the Young tensor technique, we construct the pure mesonic effective operators up to $p^8$-order, one-to-one corresponding to contact amplitudes with the on-shell Adler zero condition. The off-shell external sources, non-vanishing under equation-of-motion conditions, are also added to the operator bases. We also show the invariant tensor bases using the Young tableau is equivalent to the trace bases with Cayley-Hamilton relations. Separated into different $CP$ eigenstates, at $\\mathcal{O}(p^8)$ we obtain the operator lists of the 567 $C$+$P$+ operators, 483 $C$+$P$- operators, 376 $C$-$P$+ operators, and 408 $C$-$P$- operators for $SU(2)$ case, while there are 1959 $C$+$P$+ operators, 1809 $C$+$P$- operators, 1520 $C$-$P$+ operators, and 1594 $C$-$P$- operators for $SU(3)$ case, consistent with results using the Hilbert series.","sentences":["Chiral perturbation theory systematically describes the low energy dynamics of meson and baryons using nonlinear Nambu-Goldstone fields.","Using the Young tensor technique, we construct the pure mesonic effective operators up to $p^8$-order, one-to-one corresponding to contact amplitudes with the on-shell Adler zero condition.","The off-shell external sources, non-vanishing under equation-of-motion conditions, are also added to the operator bases.","We also show the invariant tensor bases using the Young tableau is equivalent to the trace bases with Cayley-Hamilton relations.","Separated into different $CP$ eigenstates, at $\\mathcal{O}(p^8)$ we obtain the operator lists of the 567 $C$+$P$+ operators, 483 $C$+$P$- operators, 376 $C$-$P$+ operators, and 408 $C$-$P$- operators for $SU(2)$ case, while there are 1959 $C$+$P$+ operators, 1809 $C$+$P$- operators, 1520 $C$-$P$+ operators, and 1594 $C$-$P$- operators for $SU(3)$ case, consistent with results using the Hilbert series."],"url":"http://arxiv.org/abs/2404.14152v1","category":"hep-ph"}
{"created":"2024-04-22 12:56:22","title":"Solutions of local and nonlocal discrete complex modified Korteweg-de Vries equations and continuum limits","abstract":"Cauchy matrix approach for the discrete Ablowitz-Kaup-Newell-Segur equations is reconsidered, where two `proper' discrete Ablowitz-Kaup-Newell-Segur equations and two `unproper' discrete Ablowitz-Kaup-Newell-Segur equations are derived. The `proper' equations admit local reduction, while the `unproper' equations admit nonlocal reduction. By imposing the local and nonlocal complex reductions on the obtained discrete Ablowitz-Kaup-Newell-Segur equations, two local and nonlocal discrete complex modified Korteweg-de Vries equations are constructed. For the obtained local and nonlocal discrete complex modified Korteweg-de Vries equations, soliton solutions and Jordan-block solutions are presented by solving the determining equation set. The dynamical behaviors of 1-soliton solution are analyzed and illustrated. Continuum limits of the resulting local and nonlocal discrete complex modified Korteweg-de Vries equations are discussed.","sentences":["Cauchy matrix approach for the discrete Ablowitz-Kaup-Newell-Segur equations is reconsidered, where two `proper' discrete Ablowitz-Kaup-Newell-Segur equations and two `unproper' discrete Ablowitz-Kaup-Newell-Segur equations are derived.","The `proper' equations admit local reduction, while the `unproper' equations admit nonlocal reduction.","By imposing the local and nonlocal complex reductions on the obtained discrete Ablowitz-Kaup-Newell-Segur equations, two local and nonlocal discrete complex modified Korteweg-de Vries equations are constructed.","For the obtained local and nonlocal discrete complex modified Korteweg-de Vries equations, soliton solutions and Jordan-block solutions are presented by solving the determining equation set.","The dynamical behaviors of 1-soliton solution are analyzed and illustrated.","Continuum limits of the resulting local and nonlocal discrete complex modified Korteweg-de Vries equations are discussed."],"url":"http://arxiv.org/abs/2404.14150v1","category":"nlin.SI"}
{"created":"2024-04-22 12:55:55","title":"A Linear Relationship between Correlation and Cohen's Kappa for Binary Data and Simulating Multivariate Nominal and Ordinal Data with Specified Kappa Matrix","abstract":"Cohen's kappa is a useful measure for agreement between the judges, inter-rater reliability, and also goodness of fit in classification problems. For binary nominal and ordinal data, kappa and correlation are equally applicable. We have found a linear relationship between correlation and kappa for binary data. Exact bounds of kappa are much more important as kappa can be only .5 even if there is very strong agreement. The exact upper bound was developed by Cohen (1960) but the exact lower bound is also important if the range of kappa is small for some marginals. We have developed an algorithm to find the exact lower bound given marginal proportions. Our final contribution is a method to generate multivariate nominal and ordinal data with a specified kappa matrix based on the rearrangement of independently generated marginal data to a multidimensional contingency table, where cell counts are found by solving system of linear equations for positive roots.","sentences":["Cohen's kappa is a useful measure for agreement between the judges, inter-rater reliability, and also goodness of fit in classification problems.","For binary nominal and ordinal data, kappa and correlation are equally applicable.","We have found a linear relationship between correlation and kappa for binary data.","Exact bounds of kappa are much more important as kappa can be only .5","even if there is very strong agreement.","The exact upper bound was developed by Cohen (1960) but the exact lower bound is also important if the range of kappa is small for some marginals.","We have developed an algorithm to find the exact lower bound given marginal proportions.","Our final contribution is a method to generate multivariate nominal and ordinal data with a specified kappa matrix based on the rearrangement of independently generated marginal data to a multidimensional contingency table, where cell counts are found by solving system of linear equations for positive roots."],"url":"http://arxiv.org/abs/2404.14149v1","category":"stat.ME"}
{"created":"2024-04-22 12:39:12","title":"Text in the Dark: Extremely Low-Light Text Image Enhancement","abstract":"Extremely low-light text images are common in natural scenes, making scene text detection and recognition challenging. One solution is to enhance these images using low-light image enhancement methods before text extraction. However, previous methods often do not try to particularly address the significance of low-level features, which are crucial for optimal performance on downstream scene text tasks. Further research is also hindered by the lack of extremely low-light text datasets. To address these limitations, we propose a novel encoder-decoder framework with an edge-aware attention module to focus on scene text regions during enhancement. Our proposed method uses novel text detection and edge reconstruction losses to emphasize low-level scene text features, leading to successful text extraction. Additionally, we present a Supervised Deep Curve Estimation (Supervised-DCE) model to synthesize extremely low-light images based on publicly available scene text datasets such as ICDAR15 (IC15). We also labeled texts in the extremely low-light See In the Dark (SID) and ordinary LOw-Light (LOL) datasets to allow for objective assessment of extremely low-light image enhancement through scene text tasks. Extensive experiments show that our model outperforms state-of-the-art methods in terms of both image quality and scene text metrics on the widely-used LOL, SID, and synthetic IC15 datasets. Code and dataset will be released publicly at https://github.com/chunchet-ng/Text-in-the-Dark.","sentences":["Extremely low-light text images are common in natural scenes, making scene text detection and recognition challenging.","One solution is to enhance these images using low-light image enhancement methods before text extraction.","However, previous methods often do not try to particularly address the significance of low-level features, which are crucial for optimal performance on downstream scene text tasks.","Further research is also hindered by the lack of extremely low-light text datasets.","To address these limitations, we propose a novel encoder-decoder framework with an edge-aware attention module to focus on scene text regions during enhancement.","Our proposed method uses novel text detection and edge reconstruction losses to emphasize low-level scene text features, leading to successful text extraction.","Additionally, we present a Supervised Deep Curve Estimation (Supervised-DCE) model to synthesize extremely low-light images based on publicly available scene text datasets such as ICDAR15 (IC15).","We also labeled texts in the extremely low-light See In the Dark (SID) and ordinary LOw-Light (LOL) datasets to allow for objective assessment of extremely low-light image enhancement through scene text tasks.","Extensive experiments show that our model outperforms state-of-the-art methods in terms of both image quality and scene text metrics on the widely-used LOL, SID, and synthetic IC15 datasets.","Code and dataset will be released publicly at https://github.com/chunchet-ng/Text-in-the-Dark."],"url":"http://arxiv.org/abs/2404.14135v1","category":"cs.CV"}
{"created":"2024-04-22 12:25:00","title":"Brane mechanics and gapped Lie n-algebroids","abstract":"We draw a parallel between the BV/BRST formalism for higher-dimensional ($\\ge 2$) Hamiltonian mechanics and higher notions of torsion and basic curvature tensors for generalized connections in specific Lie $n$-algebroids based on homotopy Poisson structures. The gauge systems we consider include Poisson sigma models in any dimension and ``generalised R-flux'' deformations thereof, such as models with an $(n+2)$-form-twisted R-Poisson target space. Their BV/BRST action includes interaction terms among the fields, ghosts and antifields whose coefficients acquire a geometric meaning by considering twisted Koszul multibrackets that endow the target space with a structure that we call a gapped almost Lie $n$-algebroid. Studying covariant derivatives along $n$-forms, we define suitable polytorsion and basic polycurvature tensors and identify them with the interaction coefficients in the gauge theory, thus relating models for topological $n$-branes to differential geometry on Lie $n$-algebroids.","sentences":["We draw a parallel between the BV/BRST formalism for higher-dimensional ($\\ge 2$) Hamiltonian mechanics and higher notions of torsion and basic curvature tensors for generalized connections in specific Lie $n$-algebroids based on homotopy Poisson structures.","The gauge systems we consider include Poisson sigma models in any dimension and ``generalised R-flux'' deformations thereof, such as models with an $(n+2)$-form-twisted R-Poisson target space.","Their BV/BRST action includes interaction terms among the fields, ghosts and antifields whose coefficients acquire a geometric meaning by considering twisted Koszul multibrackets that endow the target space with a structure that we call a gapped almost Lie $n$-algebroid.","Studying covariant derivatives along $n$-forms, we define suitable polytorsion and basic polycurvature tensors and identify them with the interaction coefficients in the gauge theory, thus relating models for topological $n$-branes to differential geometry on Lie $n$-algebroids."],"url":"http://arxiv.org/abs/2404.14126v1","category":"hep-th"}
{"created":"2024-04-22 12:23:09","title":"Cooperativity, information gain, and energy cost during early LTP in dendritic spines","abstract":"We investigate a mutual relationship between information and energy during early phase of LTP induction and maintenance in a large-scale system of mutually coupled dendritic spines, with discrete internal states and probabilistic dynamics, within the framework of nonequilibrium stochastic thermodynamics. In order to analyze this computationally intractable stochastic multidimensional system, we introduce a pair approximation, which allows us to reduce the spine dynamics into a lower dimensional manageable system of closed equations. It is found that the rates of information gain and energy attain their maximal values during an initial period of LTP (i.e. during stimulation), and after that they recover to their baseline low values, as opposed to a memory trace that lasts much longer. This suggests that learning phase is much more energy demanding than the memory phase. We show that positive correlations between neighboring spines increase both a duration of memory trace and energy cost during LTP, but the memory time per invested energy increases dramatically for very strong positive synaptic cooperativity, suggesting a beneficial role of synaptic clustering on memory duration. In contrast, information gain after LTP is the largest for negative correlations, and energy efficiency of that information generally declines with increasing synaptic cooperativity. We also find that dendritic spines can use sparse representations for encoding of long-term information, as both energetic and structural efficiencies of retained information and its lifetime exhibit maxima for low fractions of stimulated synapses during LTP. In general, our stochastic thermodynamics approach provides a unifying framework for studying, from first principles, information encoding and its energy cost during learning and memory in stochastic systems of interacting synapses.","sentences":["We investigate a mutual relationship between information and energy during early phase of LTP induction and maintenance in a large-scale system of mutually coupled dendritic spines, with discrete internal states and probabilistic dynamics, within the framework of nonequilibrium stochastic thermodynamics.","In order to analyze this computationally intractable stochastic multidimensional system, we introduce a pair approximation, which allows us to reduce the spine dynamics into a lower dimensional manageable system of closed equations.","It is found that the rates of information gain and energy attain their maximal values during an initial period of LTP (i.e. during stimulation), and after that they recover to their baseline low values, as opposed to a memory trace that lasts much longer.","This suggests that learning phase is much more energy demanding than the memory phase.","We show that positive correlations between neighboring spines increase both a duration of memory trace and energy cost during LTP, but the memory time per invested energy increases dramatically for very strong positive synaptic cooperativity, suggesting a beneficial role of synaptic clustering on memory duration.","In contrast, information gain after LTP is the largest for negative correlations, and energy efficiency of that information generally declines with increasing synaptic cooperativity.","We also find that dendritic spines can use sparse representations for encoding of long-term information, as both energetic and structural efficiencies of retained information and its lifetime exhibit maxima for low fractions of stimulated synapses during LTP.","In general, our stochastic thermodynamics approach provides a unifying framework for studying, from first principles, information encoding and its energy cost during learning and memory in stochastic systems of interacting synapses."],"url":"http://arxiv.org/abs/2404.14123v1","category":"q-bio.NC"}
{"created":"2024-04-22 12:19:00","title":"Prediction of flow and elastic stresses in a viscoelastic turbulent channel flow using convolutional neural networks","abstract":"Neural-network models have been employed to predict the instantaneous flow close to the wall in a viscoelastic turbulent channel flow. The numerical simulation data at the wall is utilized to predict the instantaneous velocity fluctuations and polymeric-stress fluctuations at three different wall-normal positions. Apart from predicting the velocity fluctuations well in a hibernating flow, the neural-network models are also shown to predict the polymeric shear stress and the trace of the polymeric stresses at a given wall-normal location with reasonably good accuracy. These non-intrusive sensing models can be integrated in an experimental setting to construct the polymeric-stress field in turbulent flows, which otherwise may not be directly quantifiable in experimental measurements.","sentences":["Neural-network models have been employed to predict the instantaneous flow close to the wall in a viscoelastic turbulent channel flow.","The numerical simulation data at the wall is utilized to predict the instantaneous velocity fluctuations and polymeric-stress fluctuations at three different wall-normal positions.","Apart from predicting the velocity fluctuations well in a hibernating flow, the neural-network models are also shown to predict the polymeric shear stress and the trace of the polymeric stresses at a given wall-normal location with reasonably good accuracy.","These non-intrusive sensing models can be integrated in an experimental setting to construct the polymeric-stress field in turbulent flows, which otherwise may not be directly quantifiable in experimental measurements."],"url":"http://arxiv.org/abs/2404.14121v1","category":"physics.flu-dyn"}
{"created":"2024-04-22 12:13:40","title":"Non-degeneracy of the bubble in a fractional and singular 1D Liouville equation","abstract":"We prove the non-degeneracy of solutions to a fractional and singular Liouville equation defined on the whole real line in presence of a singular term. We use conformal transformations to rewrite the linearized equation as a Steklov eigenvalue problem posed in a bounded domain, which is defined either by an intersection or a union of two disks. We conclude by proving the simplicity of the corresponding eigenvalue.","sentences":["We prove the non-degeneracy of solutions to a fractional and singular Liouville equation defined on the whole real line in presence of a singular term.","We use conformal transformations to rewrite the linearized equation as a Steklov eigenvalue problem posed in a bounded domain, which is defined either by an intersection or a union of two disks.","We conclude by proving the simplicity of the corresponding eigenvalue."],"url":"http://arxiv.org/abs/2404.14119v1","category":"math.AP"}
{"created":"2024-04-22 12:07:51","title":"Unconstrained Lagrangian Formulation for Bosonic Continuous Spin Theory in Flat Spacetime of Arbitrary Dimension","abstract":"We have discovered two unconstrained forms of free Lagrangian for continuous spin(CS) theory in arbitrary flat spacetime dimension for bosonic case. These Lagrangians, unlike that by Schuster and Toro, do not include delta functions and are conventional. The first form consists of five kinds of totally symmetric helicity fields and one kind of gauge parameter. By introducing auxiliary creation and annihilation operators, each is combined into a state vector in Fock space, including all ranks one by one. The Lagrangian imposes no constraints, such as trace conditions, on these fields or the gauge parameter field. Additionally, the Lagrangian does not contain higher-order derivative terms. In the limit as CS parameter $\\mu$ approaches zero, it naturally reproduces a Lagrangian for helicity fields in higher spin(HS) theory, known as unconstrained quartet formulation. Permitting third-order derivatives, we also obtain the second unconstrained form of Lagrangian that can be written in terms of three kinds of fields, including $\\mu$, similar to the formulation by Francia and Sagnotti. Partial gauge fixing and partial use of equation of motion(EOM) on this Lagrangian yield a Fronsdal-like Lagrangian with a single double-traceless field, including $\\mu$. By imposing further gauge fixing on the field in the EOM with respect to divergence and trace, we confirm the reproduction of the modified Wigner equations already known in literature.","sentences":["We have discovered two unconstrained forms of free Lagrangian for continuous spin(CS) theory in arbitrary flat spacetime dimension for bosonic case.","These Lagrangians, unlike that by Schuster and Toro, do not include delta functions and are conventional.","The first form consists of five kinds of totally symmetric helicity fields and one kind of gauge parameter.","By introducing auxiliary creation and annihilation operators, each is combined into a state vector in Fock space, including all ranks one by one.","The Lagrangian imposes no constraints, such as trace conditions, on these fields or the gauge parameter field.","Additionally, the Lagrangian does not contain higher-order derivative terms.","In the limit as CS parameter $\\mu$ approaches zero, it naturally reproduces a Lagrangian for helicity fields in higher spin(HS) theory, known as unconstrained quartet formulation.","Permitting third-order derivatives, we also obtain the second unconstrained form of Lagrangian that can be written in terms of three kinds of fields, including $\\mu$, similar to the formulation by Francia and Sagnotti.","Partial gauge fixing and partial use of equation of motion(EOM) on this Lagrangian yield a Fronsdal-like Lagrangian with a single double-traceless field, including $\\mu$. By imposing further gauge fixing on the field in the EOM with respect to divergence and trace, we confirm the reproduction of the modified Wigner equations already known in literature."],"url":"http://arxiv.org/abs/2404.14118v2","category":"hep-th"}
{"created":"2024-04-22 11:58:49","title":"Desynchronization of temporal solitons in Kerr cavities with pulsed injection","abstract":"A numerical and analytical study was conducted to investigate the bifurcation mechanisms that cause desynchronization between the soliton repetition frequency and the frequency of external pulsed injection in a Kerr cavity described by the Lugiato-Lefever equation. The results suggest that desynchronization typically occurs through an Andronov-Hopf bifurcation. Additionally, a simple and intuitive criterion for this bifurcation to occur is proposed.","sentences":["A numerical and analytical study was conducted to investigate the bifurcation mechanisms that cause desynchronization between the soliton repetition frequency and the frequency of external pulsed injection in a Kerr cavity described by the Lugiato-Lefever equation.","The results suggest that desynchronization typically occurs through an Andronov-Hopf bifurcation.","Additionally, a simple and intuitive criterion for this bifurcation to occur is proposed."],"url":"http://arxiv.org/abs/2404.14113v1","category":"physics.optics"}
{"created":"2024-04-22 11:52:23","title":"PGNAA Spectral Classification of Aluminium and Copper Alloys with Machine Learning","abstract":"In this paper, we explore the optimization of metal recycling with a focus on real-time differentiation between alloys of copper and aluminium. Spectral data, obtained through Prompt Gamma Neutron Activation Analysis (PGNAA), is utilized for classification. The study compares data from two detectors, cerium bromide (CeBr$_{3}$) and high purity germanium (HPGe), considering their energy resolution and sensitivity. We test various data generation, preprocessing, and classification methods, with Maximum Likelihood Classifier (MLC) and Conditional Variational Autoencoder (CVAE) yielding the best results. The study also highlights the impact of different detector types on classification accuracy, with CeBr$_{3}$ excelling in short measurement times and HPGe performing better in longer durations. The findings suggest the importance of selecting the appropriate detector and methodology based on specific application requirements.","sentences":["In this paper, we explore the optimization of metal recycling with a focus on real-time differentiation between alloys of copper and aluminium.","Spectral data, obtained through Prompt Gamma Neutron Activation Analysis (PGNAA), is utilized for classification.","The study compares data from two detectors, cerium bromide (CeBr$_{3}$) and high purity germanium (HPGe), considering their energy resolution and sensitivity.","We test various data generation, preprocessing, and classification methods, with Maximum Likelihood Classifier (MLC) and Conditional Variational Autoencoder (CVAE) yielding the best results.","The study also highlights the impact of different detector types on classification accuracy, with CeBr$_{3}$ excelling in short measurement times and HPGe performing better in longer durations.","The findings suggest the importance of selecting the appropriate detector and methodology based on specific application requirements."],"url":"http://arxiv.org/abs/2404.14107v1","category":"cs.LG"}
{"created":"2024-04-22 11:52:11","title":"DPTraj-PM: Differentially Private Trajectory Synthesis Using Prefix Tree and Markov Process","abstract":"The increasing use of GPS-enabled devices has generated a large amount of trajectory data. These data offer us vital insights to understand the movements of individuals and populations, benefiting a broad range of applications from transportation planning to epidemic modeling. However, improper release of trajectory data is increasing concerns on individual privacy. Previous attempts either lack strong privacy guarantees, or fail to preserve sufficient basic characteristics of the original data. In this paper, we propose DPTraj-PM, a method to synthesize trajectory dataset under the differential privacy (DP) framework while ensures high data utility. Based on the assumption that an individual's trajectory could be mainly determined by the initial trajectory segment (which depicts the starting point and the initial direction) and the next location point, DPTraj-PM discretizes the raw trajectories into neighboring cells, and models them by combining a prefix tree structure and an m-order Markov process. After adding noise to the model under differential privacy, DPTraj-PM generates a synthetic dataset from the noisy model to enable a wider spectrum of data mining and modeling tasks. The output traces crafted by DPTraj-PM not only preserves the patterns and variability in individuals' mobility behaviors, but also protects individual privacy. Experiments on two real-world datasets demonstrate that DPTraj-PM substantially outperforms the state-of-the-art techniques in terms of data utility. Our code is available at https://github.com/wnn5/DP-PrefixTreeMarkov.","sentences":["The increasing use of GPS-enabled devices has generated a large amount of trajectory data.","These data offer us vital insights to understand the movements of individuals and populations, benefiting a broad range of applications from transportation planning to epidemic modeling.","However, improper release of trajectory data is increasing concerns on individual privacy.","Previous attempts either lack strong privacy guarantees, or fail to preserve sufficient basic characteristics of the original data.","In this paper, we propose DPTraj-PM, a method to synthesize trajectory dataset under the differential privacy (DP) framework while ensures high data utility.","Based on the assumption that an individual's trajectory could be mainly determined by the initial trajectory segment (which depicts the starting point and the initial direction) and the next location point, DPTraj-PM discretizes the raw trajectories into neighboring cells, and models them by combining a prefix tree structure and an m-order Markov process.","After adding noise to the model under differential privacy, DPTraj-PM generates a synthetic dataset from the noisy model to enable a wider spectrum of data mining and modeling tasks.","The output traces crafted by DPTraj-PM not only preserves the patterns and variability in individuals' mobility behaviors, but also protects individual privacy.","Experiments on two real-world datasets demonstrate that DPTraj-PM substantially outperforms the state-of-the-art techniques in terms of data utility.","Our code is available at https://github.com/wnn5/DP-PrefixTreeMarkov."],"url":"http://arxiv.org/abs/2404.14106v1","category":"cs.CR"}
{"created":"2024-04-22 11:44:48","title":"Dynamical quantum Ansatz tree approach for the heat equation","abstract":"Quantum computers can be used for the solution of various problems of mathematical physics. In the present paper, we consider a discretized version of the heat equation and address its solution on quantum computer using variational Anzats tree approach (ATA). We extend this method originally proposed for the system of linear equations to tackle full time dependent heat equation. The key ingredients of our method are (i) special probabilistic quantum circuit in order to add heat sources to temperature distribution, (ii) limiting auxiliary register in the preparation of quantum state, (iii) utilizing a robust cluster of repetitive nodes in the anzats tree structure. We suggest that our procedure provides an exponential speedup compared to the classical algorithms in the case of time dependent heat equation.","sentences":["Quantum computers can be used for the solution of various problems of mathematical physics.","In the present paper, we consider a discretized version of the heat equation and address its solution on quantum computer using variational Anzats tree approach (ATA).","We extend this method originally proposed for the system of linear equations to tackle full time dependent heat equation.","The key ingredients of our method are (i) special probabilistic quantum circuit in order to add heat sources to temperature distribution, (ii) limiting auxiliary register in the preparation of quantum state, (iii) utilizing a robust cluster of repetitive nodes in the anzats tree structure.","We suggest that our procedure provides an exponential speedup compared to the classical algorithms in the case of time dependent heat equation."],"url":"http://arxiv.org/abs/2404.14102v1","category":"quant-ph"}
{"created":"2024-04-22 17:59:07","title":"Learning H-Infinity Locomotion Control","abstract":"Stable locomotion in precipitous environments is an essential capability of quadruped robots, demanding the ability to resist various external disturbances. However, recent learning-based policies only use basic domain randomization to improve the robustness of learned policies, which cannot guarantee that the robot has adequate disturbance resistance capabilities. In this paper, we propose to model the learning process as an adversarial interaction between the actor and a newly introduced disturber and ensure their optimization with $H_{\\infty}$ constraint. In contrast to the actor that maximizes the discounted overall reward, the disturber is responsible for generating effective external forces and is optimized by maximizing the error between the task reward and its oracle, i.e., \"cost\" in each iteration. To keep joint optimization between the actor and the disturber stable, our $H_{\\infty}$ constraint mandates the bound of ratio between the cost to the intensity of the external forces. Through reciprocal interaction throughout the training phase, the actor can acquire the capability to navigate increasingly complex physical disturbances. We verify the robustness of our approach on quadrupedal locomotion tasks with Unitree Aliengo robot, and also a more challenging task with Unitree A1 robot, where the quadruped is expected to perform locomotion merely on its hind legs as if it is a bipedal robot. The simulated quantitative results show improvement against baselines, demonstrating the effectiveness of the method and each design choice. On the other hand, real-robot experiments qualitatively exhibit how robust the policy is when interfering with various disturbances on various terrains, including stairs, high platforms, slopes, and slippery terrains. All code, checkpoints, and real-world deployment guidance will be made public.","sentences":["Stable locomotion in precipitous environments is an essential capability of quadruped robots, demanding the ability to resist various external disturbances.","However, recent learning-based policies only use basic domain randomization to improve the robustness of learned policies, which cannot guarantee that the robot has adequate disturbance resistance capabilities.","In this paper, we propose to model the learning process as an adversarial interaction between the actor and a newly introduced disturber and ensure their optimization with $H_{\\infty}$ constraint.","In contrast to the actor that maximizes the discounted overall reward, the disturber is responsible for generating effective external forces and is optimized by maximizing the error between the task reward and its oracle, i.e., \"cost\" in each iteration.","To keep joint optimization between the actor and the disturber stable, our $H_{\\infty}$ constraint mandates the bound of ratio between the cost to the intensity of the external forces.","Through reciprocal interaction throughout the training phase, the actor can acquire the capability to navigate increasingly complex physical disturbances.","We verify the robustness of our approach on quadrupedal locomotion tasks with Unitree Aliengo robot, and also a more challenging task with Unitree A1 robot, where the quadruped is expected to perform locomotion merely on its hind legs as if it is a bipedal robot.","The simulated quantitative results show improvement against baselines, demonstrating the effectiveness of the method and each design choice.","On the other hand, real-robot experiments qualitatively exhibit how robust the policy is when interfering with various disturbances on various terrains, including stairs, high platforms, slopes, and slippery terrains.","All code, checkpoints, and real-world deployment guidance will be made public."],"url":"http://arxiv.org/abs/2404.14405v1","category":"cs.RO"}
{"created":"2024-04-22 17:59:50","title":"Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses","abstract":"In this paper, we present a method to reconstruct the world and multiple dynamic humans in 3D from a monocular video input. As a key idea, we represent both the world and multiple humans via the recently emerging 3D Gaussian Splatting (3D-GS) representation, enabling to conveniently and efficiently compose and render them together. In particular, we address the scenarios with severely limited and sparse observations in 3D human reconstruction, a common challenge encountered in the real world. To tackle this challenge, we introduce a novel approach to optimize the 3D-GS representation in a canonical space by fusing the sparse cues in the common space, where we leverage a pre-trained 2D diffusion model to synthesize unseen views while keeping the consistency with the observed 2D appearances. We demonstrate our method can reconstruct high-quality animatable 3D humans in various challenging examples, in the presence of occlusion, image crops, few-shot, and extremely sparse observations. After reconstruction, our method is capable of not only rendering the scene in any novel views at arbitrary time instances, but also editing the 3D scene by removing individual humans or applying different motions for each human. Through various experiments, we demonstrate the quality and efficiency of our methods over alternative existing approaches.","sentences":["In this paper, we present a method to reconstruct the world and multiple dynamic humans in 3D from a monocular video input.","As a key idea, we represent both the world and multiple humans via the recently emerging 3D Gaussian Splatting (3D-GS) representation, enabling to conveniently and efficiently compose and render them together.","In particular, we address the scenarios with severely limited and sparse observations in 3D human reconstruction, a common challenge encountered in the real world.","To tackle this challenge, we introduce a novel approach to optimize the 3D-GS representation in a canonical space by fusing the sparse cues in the common space, where we leverage a pre-trained 2D diffusion model to synthesize unseen views while keeping the consistency with the observed 2D appearances.","We demonstrate our method can reconstruct high-quality animatable 3D humans in various challenging examples, in the presence of occlusion, image crops, few-shot, and extremely sparse observations.","After reconstruction, our method is capable of not only rendering the scene in any novel views at arbitrary time instances, but also editing the 3D scene by removing individual humans or applying different motions for each human.","Through various experiments, we demonstrate the quality and efficiency of our methods over alternative existing approaches."],"url":"http://arxiv.org/abs/2404.14410v1","category":"cs.CV"}
{"created":"2024-04-22 17:58:36","title":"GeoDiffuser: Geometry-Based Image Editing with Diffusion Models","abstract":"The success of image generative models has enabled us to build methods that can edit images based on text or other user input. However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits. We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method. Our key insight is to view image editing operations as geometric transformations. We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations. Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows. It also inpaints disoccluded parts of the image where the object was originally located. Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing. GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal. We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods. Visit https://ivl.cs.brown.edu/research/geodiffuser.html for more information.","sentences":["The success of image generative models has enabled us to build methods that can edit images based on text or other user input.","However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits.","We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method.","Our key insight is to view image editing operations as geometric transformations.","We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations.","Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows.","It also inpaints disoccluded parts of the image where the object was originally located.","Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing.","GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal.","We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods.","Visit https://ivl.cs.brown.edu/research/geodiffuser.html for more information."],"url":"http://arxiv.org/abs/2404.14403v1","category":"cs.CV"}
{"created":"2024-04-22 17:58:13","title":"MLQAOA: Graph Learning Accelerated Hybrid Quantum-Classical Multilevel QAOA","abstract":"Learning the problem structure at multiple levels of coarseness to inform the decomposition-based hybrid quantum-classical combinatorial optimization solvers is a promising approach to scaling up variational approaches. We introduce a multilevel algorithm reinforced with the spectral graph representation learning-based accelerator to tackle large-scale graph maximum cut instances and fused with several versions of the quantum approximate optimization algorithm (QAOA) and QAOA-inspired algorithms. The graph representation learning model utilizes the idea of QAOA variational parameters concentration and substantially improves the performance of QAOA. We demonstrate the potential of using multilevel QAOA and representation learning-based approaches on very large graphs by achieving high-quality solutions in a much faster time.\\\\ Reproducibility: Our source code and results are available at \\url{https://github.com/bachbao/MLQAOA}","sentences":["Learning the problem structure at multiple levels of coarseness to inform the decomposition-based hybrid quantum-classical combinatorial optimization solvers is a promising approach to scaling up variational approaches.","We introduce a multilevel algorithm reinforced with the spectral graph representation learning-based accelerator to tackle large-scale graph maximum cut instances and fused with several versions of the quantum approximate optimization algorithm (QAOA) and QAOA-inspired algorithms.","The graph representation learning model utilizes the idea of QAOA variational parameters concentration and substantially improves the performance of QAOA.","We demonstrate the potential of using multilevel QAOA and representation learning-based approaches on very large graphs by achieving high-quality solutions in a much faster time.\\\\ Reproducibility:","Our source code and results are available at \\url{https://github.com/bachbao/MLQAOA}"],"url":"http://arxiv.org/abs/2404.14399v1","category":"quant-ph"}
{"created":"2024-04-22 17:50:27","title":"Poisoning Attacks on Federated Learning-based Wireless Traffic Prediction","abstract":"Federated Learning (FL) offers a distributed framework to train a global control model across multiple base stations without compromising the privacy of their local network data. This makes it ideal for applications like wireless traffic prediction (WTP), which plays a crucial role in optimizing network resources, enabling proactive traffic flow management, and enhancing the reliability of downstream communication-aided applications, such as IoT devices, autonomous vehicles, and industrial automation systems. Despite its promise, the security aspects of FL-based distributed wireless systems, particularly in regression-based WTP problems, remain inadequately investigated. In this paper, we introduce a novel fake traffic injection (FTI) attack, designed to undermine the FL-based WTP system by injecting fabricated traffic distributions with minimal knowledge. We further propose a defense mechanism, termed global-local inconsistency detection (GLID), which strategically removes abnormal model parameters that deviate beyond a specific percentile range estimated through statistical methods in each dimension. Extensive experimental evaluations, performed on real-world wireless traffic datasets, demonstrate that both our attack and defense strategies significantly outperform existing baselines.","sentences":["Federated Learning (FL) offers a distributed framework to train a global control model across multiple base stations without compromising the privacy of their local network data.","This makes it ideal for applications like wireless traffic prediction (WTP), which plays a crucial role in optimizing network resources, enabling proactive traffic flow management, and enhancing the reliability of downstream communication-aided applications, such as IoT devices, autonomous vehicles, and industrial automation systems.","Despite its promise, the security aspects of FL-based distributed wireless systems, particularly in regression-based WTP problems, remain inadequately investigated.","In this paper, we introduce a novel fake traffic injection (FTI) attack, designed to undermine the FL-based WTP system by injecting fabricated traffic distributions with minimal knowledge.","We further propose a defense mechanism, termed global-local inconsistency detection (GLID), which strategically removes abnormal model parameters that deviate beyond a specific percentile range estimated through statistical methods in each dimension.","Extensive experimental evaluations, performed on real-world wireless traffic datasets, demonstrate that both our attack and defense strategies significantly outperform existing baselines."],"url":"http://arxiv.org/abs/2404.14389v1","category":"cs.NI"}
{"created":"2024-04-22 17:46:29","title":"STROOBnet Optimization via GPU-Accelerated Proximal Recurrence Strategies","abstract":"Spatiotemporal networks' observational capabilities are crucial for accurate data gathering and informed decisions across multiple sectors. This study focuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network (STROOBnet), linking observational nodes (e.g., surveillance cameras) to events within defined geographical regions, enabling efficient monitoring. Using data from Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New Orleans, where RTCC combats rising crime amidst reduced police presence, we address the network's initial observational imbalances. Aiming for uniform observational efficacy, we propose the Proximal Recurrence approach. It outperformed traditional clustering methods like k-means and DBSCAN by offering holistic event frequency and spatial consideration, enhancing observational coverage.","sentences":["Spatiotemporal networks' observational capabilities are crucial for accurate data gathering and informed decisions across multiple sectors.","This study focuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network (STROOBnet), linking observational nodes (e.g., surveillance cameras) to events within defined geographical regions, enabling efficient monitoring.","Using data from Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New Orleans, where RTCC combats rising crime amidst reduced police presence, we address the network's initial observational imbalances.","Aiming for uniform observational efficacy, we propose the Proximal Recurrence approach.","It outperformed traditional clustering methods like k-means and DBSCAN by offering holistic event frequency and spatial consideration, enhancing observational coverage."],"url":"http://arxiv.org/abs/2404.14388v1","category":"cs.LG"}
{"created":"2024-04-22 17:37:17","title":"A New Optimization Model for Multiple-Control Toffoli Quantum Circuit Design","abstract":"As quantum technology is advancing, the efficient design of quantum circuits has become an important area of research. This paper provides an introduction to the MCT quantum circuit design problem for reversible Boolean functions without assuming a prior background in quantum computing. While this is a well-studied problem, optimization models that minimize the true objective have only been explored recently. This paper introduces a new optimization model and symmetry-breaking constraints that improve solving time by up to two orders of magnitude compared to earlier work when a Constraint Programming solver is used. Experiments with up to seven qubits and using up to 15 quantum gates result in several new best-known circuits for well-known benchmarks. Finally, an extensive comparison with other approaches shows that optimization models may require more time but can provide superior circuits with optimality guarantees.","sentences":["As quantum technology is advancing, the efficient design of quantum circuits has become an important area of research.","This paper provides an introduction to the MCT quantum circuit design problem for reversible Boolean functions without assuming a prior background in quantum computing.","While this is a well-studied problem, optimization models that minimize the true objective have only been explored recently.","This paper introduces a new optimization model and symmetry-breaking constraints that improve solving time by up to two orders of magnitude compared to earlier work when a Constraint Programming solver is used.","Experiments with up to seven qubits and using up to 15 quantum gates result in several new best-known circuits for well-known benchmarks.","Finally, an extensive comparison with other approaches shows that optimization models may require more time but can provide superior circuits with optimality guarantees."],"url":"http://arxiv.org/abs/2404.14384v1","category":"math.OC"}
{"created":"2024-04-22 17:37:04","title":"Observational characterisation of large-scale transport and horizontal turbulent diffusivity in the quiet Sun","abstract":"The Sun is a magnetic star, and the only spatio-temporally resolved astrophysical system displaying turbulent MHD thermal convection. This makes it a privileged object of study to understand fluid turbulence in extreme regimes and its interactions with magnetic fields. Global analyses of high-resolution solar observations provided by the NASA Solar Dynamics Observatory can shed light on the physical processes underlying large-scale emergent phenomena such as the solar dynamo cycle. Combining a Coherent Structure Tracking reconstruction of photospheric flows, based on photometric data, and a statistical analysis of virtual passive tracers trajectories advected by these flows, we characterise one of the most important such processes, turbulent diffusion, over an unprecedentedly long monitoring period of 6 consecutive days of a significant fraction of the solar disc. We first confirm, and provide a new global view of the emergence of a remarkable dynamical pattern of Lagrangian Coherent Structures tiling the entire surface. These structures act as transport barriers on the time and spatial scale of supergranulation and, by transiently accumulating particles and magnetic fields, regulate large-scale turbulent surface diffusion. We then further statistically characterise the turbulent transport regime using two different methods, and obtain an effective horizontal turbulent diffusivity $D=2-3\\times10^8~\\mathrm{m}^2~\\mathrm{s}^{-1}$ on the longest timescales probed. This estimate is consistent with the transport coefficients required in large-scale mean-field solar dynamo models, and is in broad agreement with the results of global simulations. Our analysis may also have implications for understanding the connections between solar-surface, coronal and solar-wind dynamics, and provides valuable lessons to characterise turbulent transport in other, unresolved turbulent astrophysical systems.","sentences":["The Sun is a magnetic star, and the only spatio-temporally resolved astrophysical system displaying turbulent MHD thermal convection.","This makes it a privileged object of study to understand fluid turbulence in extreme regimes and its interactions with magnetic fields.","Global analyses of high-resolution solar observations provided by the NASA Solar Dynamics Observatory can shed light on the physical processes underlying large-scale emergent phenomena such as the solar dynamo cycle.","Combining a Coherent Structure Tracking reconstruction of photospheric flows, based on photometric data, and a statistical analysis of virtual passive tracers trajectories advected by these flows, we characterise one of the most important such processes, turbulent diffusion, over an unprecedentedly long monitoring period of 6 consecutive days of a significant fraction of the solar disc.","We first confirm, and provide a new global view of the emergence of a remarkable dynamical pattern of Lagrangian Coherent Structures tiling the entire surface.","These structures act as transport barriers on the time and spatial scale of supergranulation and, by transiently accumulating particles and magnetic fields, regulate large-scale turbulent surface diffusion.","We then further statistically characterise the turbulent transport regime using two different methods, and obtain an effective horizontal turbulent diffusivity $D=2-3\\times10^8~\\mathrm{m}^2~\\mathrm{s}^{-1}$ on the longest timescales probed.","This estimate is consistent with the transport coefficients required in large-scale mean-field solar dynamo models, and is in broad agreement with the results of global simulations.","Our analysis may also have implications for understanding the connections between solar-surface, coronal and solar-wind dynamics, and provides valuable lessons to characterise turbulent transport in other, unresolved turbulent astrophysical systems."],"url":"http://arxiv.org/abs/2404.14383v1","category":"astro-ph.SR"}
{"created":"2024-04-22 17:21:39","title":"Analysing the interaction of expansion decisions by end customers and grid development in the context of a municipal energy system","abstract":"In order to achieve greenhouse gas neutrality by 2045, the Climate Protection Act sets emission reduction targets for the years 2030 and 2040, as well as decreasing annual emission volumes for some sectors, including the building sector. Measures to decarbonize the building sector include energy retrofits and the expansion of renewable, decentralized power generators and low-CO2 heat generators. These measures thus change both the load and the generation of the future energy supply concept. Considering the interactions of the changed installed technologies on the building level and their influence on the electrical grid infrastructure is necessary. The grid operator will remedy the future congested grid states by grid expansion measures and pass on the costs to the connected grid users, which in turn could influence their behaviour and decisions. The aim of this work is a holistic analysis of the staggered interactions of generation expansion and grid expansion for a future decentralized energy supply concept conditioned by the expansion in the field of self-generation. To enable the analysis of the interactions, a multi-criteria optimization procedure for expansion and operation decisions at the building level is combined with an approach to determine grid expansion. As part of this work, the effect of an expansion of hosting capacity on the grid charges and thus the decision-making behaviour was investigated.","sentences":["In order to achieve greenhouse gas neutrality by 2045, the Climate Protection Act sets emission reduction targets for the years 2030 and 2040, as well as decreasing annual emission volumes for some sectors, including the building sector.","Measures to decarbonize the building sector include energy retrofits and the expansion of renewable, decentralized power generators and low-CO2 heat generators.","These measures thus change both the load and the generation of the future energy supply concept.","Considering the interactions of the changed installed technologies on the building level and their influence on the electrical grid infrastructure is necessary.","The grid operator will remedy the future congested grid states by grid expansion measures and pass on the costs to the connected grid users, which in turn could influence their behaviour and decisions.","The aim of this work is a holistic analysis of the staggered interactions of generation expansion and grid expansion for a future decentralized energy supply concept conditioned by the expansion in the field of self-generation.","To enable the analysis of the interactions, a multi-criteria optimization procedure for expansion and operation decisions at the building level is combined with an approach to determine grid expansion.","As part of this work, the effect of an expansion of hosting capacity on the grid charges and thus the decision-making behaviour was investigated."],"url":"http://arxiv.org/abs/2404.14371v1","category":"eess.SY"}
{"created":"2024-04-22 17:12:06","title":"A Stochastic Geo-spatiotemporal Bipartite Network to Optimize GCOOS Sensor Placement Strategies","abstract":"This paper proposes two new measures applicable in a spatial bipartite network model: coverage and coverage robustness. The bipartite network must consist of observer nodes, observable nodes, and edges that connect observer nodes to observable nodes. The coverage and coverage robustness scores evaluate the effectiveness of the observer node placements. This measure is beneficial for stochastic data as it may be coupled with Monte Carlo simulations to identify optimal placements for new observer nodes. In this paper, we construct a Geo-SpatioTemporal Bipartite Network (GSTBN) within the stochastic and dynamical environment of the Gulf of Mexico. This GSTBN consists of GCOOS sensor nodes and HYCOM Region of Interest (RoI) event nodes. The goal is to identify optimal placements to expand GCOOS to improve the forecasting outcomes by the HYCOM ocean prediction model.","sentences":["This paper proposes two new measures applicable in a spatial bipartite network model: coverage and coverage robustness.","The bipartite network must consist of observer nodes, observable nodes, and edges that connect observer nodes to observable nodes.","The coverage and coverage robustness scores evaluate the effectiveness of the observer node placements.","This measure is beneficial for stochastic data as it may be coupled with Monte Carlo simulations to identify optimal placements for new observer nodes.","In this paper, we construct a Geo-SpatioTemporal Bipartite Network (GSTBN) within the stochastic and dynamical environment of the Gulf of Mexico.","This GSTBN consists of GCOOS sensor nodes and HYCOM Region of Interest (RoI) event nodes.","The goal is to identify optimal placements to expand GCOOS to improve the forecasting outcomes by the HYCOM ocean prediction model."],"url":"http://arxiv.org/abs/2404.14357v1","category":"cs.MA"}
{"created":"2024-04-22 17:00:48","title":"Operando Analysis of Adsorption-Limited Hydrogen Oxidation Reaction at Palladium Surfaces","abstract":"Palladium (Pd) catalysts have been extensively studied for the direct synthesis of H2O through the hydrogen oxidation reaction at ambient conditions. This heterogeneous catalytic reaction not only holds considerable practical significance but also serves as a classical model for investigating fundamental mechanisms, including adsorption and reactions between adsorbates. Nonetheless, the governing mechanisms and kinetics of its intermediate reaction stages under varying gas conditions remains elusive. This is attributed to the intricate interplay between adsorption, atomic diffusion, and concurrent phase transformation of catalyst. Herein, the Pd-catalyzed, water-forming hydrogen oxidation is studied, in situ, to investigate intermediate reaction stages via fluid cell transmission electron microscopy. The dynamic behaviors of water generation, associated with reversible palladium hydride formation, are captured in real time with a nanoscale spatial resolution. Our findings suggest that the hydrogen oxidation rate catalyzed by Pd is significantly affected by the sequence in which gases are introduced. Through direct evidence of electron diffraction and density functional theory calculation, we demonstrate that the hydrogen oxidation rate is limited by adsorption processes of gas precursors. These nanoscale insights help identify the optimal reaction conditions for Pd-catalyzed hydrogen oxidation, which has substantial implications for water production technologies. The developed understanding also advocates a broader exploration of analogous mechanisms in other metal-catalyzed reactions.","sentences":["Palladium (Pd) catalysts have been extensively studied for the direct synthesis of H2O through the hydrogen oxidation reaction at ambient conditions.","This heterogeneous catalytic reaction not only holds considerable practical significance but also serves as a classical model for investigating fundamental mechanisms, including adsorption and reactions between adsorbates.","Nonetheless, the governing mechanisms and kinetics of its intermediate reaction stages under varying gas conditions remains elusive.","This is attributed to the intricate interplay between adsorption, atomic diffusion, and concurrent phase transformation of catalyst.","Herein, the Pd-catalyzed, water-forming hydrogen oxidation is studied, in situ, to investigate intermediate reaction stages via fluid cell transmission electron microscopy.","The dynamic behaviors of water generation, associated with reversible palladium hydride formation, are captured in real time with a nanoscale spatial resolution.","Our findings suggest that the hydrogen oxidation rate catalyzed by Pd is significantly affected by the sequence in which gases are introduced.","Through direct evidence of electron diffraction and density functional theory calculation, we demonstrate that the hydrogen oxidation rate is limited by adsorption processes of gas precursors.","These nanoscale insights help identify the optimal reaction conditions for Pd-catalyzed hydrogen oxidation, which has substantial implications for water production technologies.","The developed understanding also advocates a broader exploration of analogous mechanisms in other metal-catalyzed reactions."],"url":"http://arxiv.org/abs/2404.14348v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 16:56:39","title":"Managing Expectations and Imbalanced Training Data in Reactive Force Field Development: an Application to Water Adsorption on Alumina","abstract":"ReaxFF is a computationally efficient model for reactive molecular dynamics simulations, which has been applied to a wide variety of chemical systems. When ReaxFF parameters are not yet available for a chemistry of interest, they must be (re)optimized, for which one defines a set of training data that the new ReaxFF parameters should reproduce. ReaxFF training sets typically contain diverse properties with different units, some of which are more abundant (by orders of magnitude) than others. To find the best parameters, one conventionally minimizes a weighted sum of squared errors over all data in the training set. One of the challenges in such numerical optimizations is to assign weights so that the optimized parameters represent a good compromise between all the requirements defined in the training set. This work introduces a new loss function, called Balanced Loss, and a workflow that replaces weight assignment with a more manageable procedure. The training data is divided into categories with corresponding \"tolerances\", i.e. acceptable root-mean-square errors for the categories, which define the expectations for the optimized ReaxFF parameters. Through the Log-Sum-Exp form of Balanced Loss, the parameter optimization is also a validation of one's expectations, providing meaningful feedback that can be used to reconfigure the tolerances if needed. The new methodology is demonstrated with a non-trivial parameterization of ReaxFF for water adsorption on alumina. This results in a new force field that reproduces both rare and frequent properties of a validation set not used for training. We also demonstrate the robustness of the new force field with a molecular dynamics simulation of water desorption from a $\\gamma$-Al$_2$O$_3$ slab model.","sentences":["ReaxFF is a computationally efficient model for reactive molecular dynamics simulations, which has been applied to a wide variety of chemical systems.","When ReaxFF parameters are not yet available for a chemistry of interest, they must be (re)optimized, for which one defines a set of training data that the new ReaxFF parameters should reproduce.","ReaxFF training sets typically contain diverse properties with different units, some of which are more abundant (by orders of magnitude) than others.","To find the best parameters, one conventionally minimizes a weighted sum of squared errors over all data in the training set.","One of the challenges in such numerical optimizations is to assign weights so that the optimized parameters represent a good compromise between all the requirements defined in the training set.","This work introduces a new loss function, called Balanced Loss, and a workflow that replaces weight assignment with a more manageable procedure.","The training data is divided into categories with corresponding \"tolerances\", i.e. acceptable root-mean-square errors for the categories, which define the expectations for the optimized ReaxFF parameters.","Through the Log-Sum-Exp form of Balanced Loss, the parameter optimization is also a validation of one's expectations, providing meaningful feedback that can be used to reconfigure the tolerances if needed.","The new methodology is demonstrated with a non-trivial parameterization of ReaxFF for water adsorption on alumina.","This results in a new force field that reproduces both rare and frequent properties of a validation set not used for training.","We also demonstrate the robustness of the new force field with a molecular dynamics simulation of water desorption from a $\\gamma$-Al$_2$O$_3$ slab model."],"url":"http://arxiv.org/abs/2404.14338v1","category":"physics.chem-ph"}
{"created":"2024-04-22 16:30:03","title":"Multi-Agent Hybrid SAC for Joint SS-DSA in CRNs","abstract":"Opportunistic spectrum access has the potential to increase the efficiency of spectrum utilization in cognitive radio networks (CRNs). In CRNs, both spectrum sensing and resource allocation (SSRA) are critical to maximizing system throughput while minimizing collisions of secondary users with the primary network. However, many works in dynamic spectrum access do not consider the impact of imperfect sensing information such as mis-detected channels, which the additional information available in joint SSRA can help remediate. In this work, we examine joint SSRA as an optimization which seeks to maximize a CRN's net communication rate subject to constraints on channel sensing, channel access, and transmit power. Given the non-trivial nature of the problem, we leverage multi-agent reinforcement learning to enable a network of secondary users to dynamically access unoccupied spectrum via only local test statistics, formulated under the energy detection paradigm of spectrum sensing. In doing so, we develop a novel multi-agent implementation of hybrid soft actor critic, MHSAC, based on the QMIX mixing scheme. Through experiments, we find that our SSRA algorithm, HySSRA, is successful in maximizing the CRN's utilization of spectrum resources while also limiting its interference with the primary network, and outperforms the current state-of-the-art by a wide margin. We also explore the impact of wireless variations such as coherence time on the efficacy of the system.","sentences":["Opportunistic spectrum access has the potential to increase the efficiency of spectrum utilization in cognitive radio networks (CRNs).","In CRNs, both spectrum sensing and resource allocation (SSRA) are critical to maximizing system throughput while minimizing collisions of secondary users with the primary network.","However, many works in dynamic spectrum access do not consider the impact of imperfect sensing information such as mis-detected channels, which the additional information available in joint SSRA can help remediate.","In this work, we examine joint SSRA as an optimization which seeks to maximize a CRN's net communication rate subject to constraints on channel sensing, channel access, and transmit power.","Given the non-trivial nature of the problem, we leverage multi-agent reinforcement learning to enable a network of secondary users to dynamically access unoccupied spectrum via only local test statistics, formulated under the energy detection paradigm of spectrum sensing.","In doing so, we develop a novel multi-agent implementation of hybrid soft actor critic, MHSAC, based on the QMIX mixing scheme.","Through experiments, we find that our SSRA algorithm, HySSRA, is successful in maximizing the CRN's utilization of spectrum resources while also limiting its interference with the primary network, and outperforms the current state-of-the-art by a wide margin.","We also explore the impact of wireless variations such as coherence time on the efficacy of the system."],"url":"http://arxiv.org/abs/2404.14319v1","category":"eess.SY"}
{"created":"2024-04-22 16:29:26","title":"Meta-GGAs vs. Hybrid Functionals for Point Defects: The Best of Both Worlds Applied to Layered MnO$_2$, NiO$_2$ and KCoO$_2$","abstract":"Defects in a material can significantly tune its properties and enhance its utility. Hybrid functionals like HSE06 are often used to describe solids with defects. However, geometry optimization using hybrid functionals (e.g., HSE06), often used to describe solids with defects, is challenging for a large supercell, as needed for defect study. The proposed r$^2$SCAN+rVV10+U+U$_d$ method, which is computationally much cheaper and faster than hybrid functionals, can successfully describe defects in materials with the proper choice of U (for the d orbitals of the host atom) and U$_d$ (for those of the defect atom), as shown here for small polarons in layered transition-metal oxides. We use a literature value of U or U$_d$ appropriate to a given transition-metal ion and its oxidation state. The materials MnO$_2$ and NiO$_2$, with one K atom intercalated between layers in a supercell, are found to have one localized occupied e$_g$ state on the transition metal ion that takes an electron from the K atom, when the geometry is calculated as above, for standard U values but not for U=U$_d$=0. K-intercalated KCoO$_2$ is surprisingly different, due to a dramatic change of electronic configuration of the defected Co$^{+2}$ ion.","sentences":["Defects in a material can significantly tune its properties and enhance its utility.","Hybrid functionals like HSE06 are often used to describe solids with defects.","However, geometry optimization using hybrid functionals (e.g., HSE06), often used to describe solids with defects, is challenging for a large supercell, as needed for defect study.","The proposed r$^2$SCAN+rVV10+U+U$_d$ method, which is computationally much cheaper and faster than hybrid functionals, can successfully describe defects in materials with the proper choice of U (for the d orbitals of the host atom) and U$_d$ (for those of the defect atom), as shown here for small polarons in layered transition-metal oxides.","We use a literature value of U or U$_d$ appropriate to a given transition-metal ion and its oxidation state.","The materials MnO$_2$ and NiO$_2$, with one K atom intercalated between layers in a supercell, are found to have one localized occupied e$_g$ state on the transition metal ion that takes an electron from the K atom, when the geometry is calculated as above, for standard U values but not for U=U$_d$=0.","K-intercalated KCoO$_2$ is surprisingly different, due to a dramatic change of electronic configuration of the defected Co$^{+2}$ ion."],"url":"http://arxiv.org/abs/2404.14317v1","category":"physics.comp-ph"}
{"created":"2024-04-22 16:10:38","title":"Towards Better Adversarial Purification via Adversarial Denoising Diffusion Training","abstract":"Recently, diffusion-based purification (DBP) has emerged as a promising approach for defending against adversarial attacks. However, previous studies have used questionable methods to evaluate the robustness of DBP models, their explanations of DBP robustness also lack experimental support. We re-examine DBP robustness using precise gradient, and discuss the impact of stochasticity on DBP robustness. To better explain DBP robustness, we assess DBP robustness under a novel attack setting, Deterministic White-box, and pinpoint stochasticity as the main factor in DBP robustness. Our results suggest that DBP models rely on stochasticity to evade the most effective attack direction, rather than directly countering adversarial perturbations. To improve the robustness of DBP models, we propose Adversarial Denoising Diffusion Training (ADDT). This technique uses Classifier-Guided Perturbation Optimization (CGPO) to generate adversarial perturbation through guidance from a pre-trained classifier, and uses Rank-Based Gaussian Mapping (RBGM) to convert adversarial pertubation into a normal Gaussian distribution. Empirical results show that ADDT improves the robustness of DBP models. Further experiments confirm that ADDT equips DBP models with the ability to directly counter adversarial perturbations.","sentences":["Recently, diffusion-based purification (DBP) has emerged as a promising approach for defending against adversarial attacks.","However, previous studies have used questionable methods to evaluate the robustness of DBP models, their explanations of DBP robustness also lack experimental support.","We re-examine DBP robustness using precise gradient, and discuss the impact of stochasticity on DBP robustness.","To better explain DBP robustness, we assess DBP robustness under a novel attack setting, Deterministic White-box, and pinpoint stochasticity as the main factor in DBP robustness.","Our results suggest that DBP models rely on stochasticity to evade the most effective attack direction, rather than directly countering adversarial perturbations.","To improve the robustness of DBP models, we propose Adversarial Denoising Diffusion Training (ADDT).","This technique uses Classifier-Guided Perturbation Optimization (CGPO) to generate adversarial perturbation through guidance from a pre-trained classifier, and uses Rank-Based Gaussian Mapping (RBGM) to convert adversarial pertubation into a normal Gaussian distribution.","Empirical results show that ADDT improves the robustness of DBP models.","Further experiments confirm that ADDT equips DBP models with the ability to directly counter adversarial perturbations."],"url":"http://arxiv.org/abs/2404.14309v1","category":"cs.CV"}
{"created":"2024-04-22 16:08:52","title":"One Trillion True Random Bits Generated with a Field Programmable Gate Array Actuated Magnetic Tunnel Junction","abstract":"Large quantities of random numbers are crucial in a wide range of applications. We have recently demonstrated that perpendicular nanopillar magnetic tunnel junctions (pMTJs) can produce true random bits when actuated with short pulses. However, our implementation used high-end and expensive electronics, such as a high bandwidth arbitrary waveform generator and analog-to-digital converter, and was limited to relatively low data rates. Here, we significantly increase the speed of true random number generation (TRNG) of our stochastic actuated pMTJs (SMART-pMTJs) using Field Programmable Gate Arrays (FPGAs), demonstrating the generation of over $10^{12}$ bits at rates exceeding 10Mb/s. The resulting bitstreams pass the NIST Statistical Test Suite for randomness with only one XOR operation. In addition to a hundred-fold reduction in the setup cost and a thousand-fold increase in bitrate, the advancement includes simplifying and optimizing random bit generation with a custom-designed analog daughter board to interface an FPGA and SMART-pMTJ. The resulting setup further enables FPGA at-speed processing of MTJ data for stochastic modeling and cryptography.","sentences":["Large quantities of random numbers are crucial in a wide range of applications.","We have recently demonstrated that perpendicular nanopillar magnetic tunnel junctions (pMTJs) can produce true random bits when actuated with short pulses.","However, our implementation used high-end and expensive electronics, such as a high bandwidth arbitrary waveform generator and analog-to-digital converter, and was limited to relatively low data rates.","Here, we significantly increase the speed of true random number generation (TRNG) of our stochastic actuated pMTJs","(SMART-pMTJs) using Field Programmable Gate Arrays (FPGAs), demonstrating the generation of over $10^{12}$ bits at rates exceeding 10Mb/s. The resulting bitstreams pass the NIST Statistical Test Suite for randomness with only one XOR operation.","In addition to a hundred-fold reduction in the setup cost and a thousand-fold increase in bitrate, the advancement includes simplifying and optimizing random bit generation with a custom-designed analog daughter board to interface an FPGA and SMART-pMTJ.","The resulting setup further enables FPGA at-speed processing of MTJ data for stochastic modeling and cryptography."],"url":"http://arxiv.org/abs/2404.14307v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-22 16:00:24","title":"Linear Search for an Escaping Target with Unknown Speed","abstract":"We consider linear search for an escaping target whose speed and initial position are unknown to the searcher. A searcher (an autonomous mobile agent) is initially placed at the origin of the real line and can move with maximum speed $1$ in either direction along the line. An oblivious mobile target that is moving away from the origin with an unknown constant speed $v<1$ is initially placed by an adversary on the infinite line at distance $d$ from the origin in an unknown direction. We consider two cases, depending on whether $d$ is known or unknown. The main contribution of this paper is to prove a new lower bound and give algorithms leading to new upper bounds for search in these settings. This results in an optimal (up to lower order terms in the exponent) competitive ratio in the case where $d$ is known and improved upper and lower bounds for the case where $d$ is unknown. Our results solve an open problem proposed in [Coleman et al., Proc. OPODIS 2022].","sentences":["We consider linear search for an escaping target whose speed and initial position are unknown to the searcher.","A searcher (an autonomous mobile agent) is initially placed at the origin of the real line and can move with maximum speed $1$ in either direction along the line.","An oblivious mobile target that is moving away from the origin with an unknown constant speed $v<1$ is initially placed by an adversary on the infinite line at distance $d$ from the origin in an unknown direction.","We consider two cases, depending on whether $d$ is known or unknown.","The main contribution of this paper is to prove a new lower bound and give algorithms leading to new upper bounds for search in these settings.","This results in an optimal (up to lower order terms in the exponent) competitive ratio in the case where $d$ is known and improved upper and lower bounds for the case where $d$ is unknown.","Our results solve an open problem proposed in [Coleman et al., Proc.","OPODIS 2022]."],"url":"http://arxiv.org/abs/2404.14300v2","category":"cs.DM"}
{"created":"2024-04-22 15:44:33","title":"Comparison of h-BN and graphene layers as grain boundary materials for granular FePt-$\\text{L}1_0$ thin films","abstract":"Granular $\\text{L}1_0$-FePt thin films with small columnar grains are essential for heat-assisted magnetic recording media. While hexagonal boron nitride(h-BN) has proven effective for promoting columnar FePt grains, we explored multilayer graphene as an alternative grain boundary material leveraging its structural similarity to h-BN. The FePt granular thin films with carbon-based grain boundary materials(GBMs) were deposited by cosputtering on Si/SiO2 substrates with substrate bias at 650{\\deg}C. The RF bias and high temperature facilitated formation of interlinked graphene nanoribbons wrapping around FePt grains, yielding 7.5 nm diameter, 8 nm height grains with an order parameter of 0.78 and a perpendicular coercivity of 40 kOe. However, the formation of graphene nanoribbons could not effectively promote columnar structures, likely due to co-existing amorphous carbon in grain boundaries. Optimizing deposition to improve graphene grain boundary quality is necessary to realize this 2D material's potential for achieving desirable microstructures for HAMR media.","sentences":["Granular $\\text{L}1_0$-FePt thin films with small columnar grains are essential for heat-assisted magnetic recording media.","While hexagonal boron nitride(h-BN) has proven effective for promoting columnar FePt grains, we explored multilayer graphene as an alternative grain boundary material leveraging its structural similarity to h-BN.","The FePt granular thin films with carbon-based grain boundary materials(GBMs) were deposited by cosputtering on Si/SiO2 substrates with substrate bias at 650{\\deg}C.","The RF bias and high temperature facilitated formation of interlinked graphene nanoribbons wrapping around FePt grains, yielding 7.5 nm diameter, 8 nm height grains with an order parameter of 0.78 and a perpendicular coercivity of 40 kOe.","However, the formation of graphene nanoribbons could not effectively promote columnar structures, likely due to co-existing amorphous carbon in grain boundaries.","Optimizing deposition to improve graphene grain boundary quality is necessary to realize this 2D material's potential for achieving desirable microstructures for HAMR media."],"url":"http://arxiv.org/abs/2404.14290v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 15:44:08","title":"Laser-synthesized TiN nanoparticles as novel efficient sorbent for environmental water cleaning","abstract":"Dyes used in industries such as textile, paper, and leather are known to be harmful to both human health and aquatic ecosystems. Therefore, finding effective and sustainable methods to remove dyes from wastewater is crucial for mitigating the detrimental effects of pollution.TiN nanoparticles have good absorption and conversion of light energy into thermal energy in the visible range of the spectrum, which makes them promising in various applications, from biomedical to environmental protection. In this work, it is shown that titanium nitride nanoparticles also possess promising adsorption capabilitieseffect. TiN nanoparticles were synthesized by laser ablation method in liquid. Water, acetone and acetonitrile are used as solvent. Nanoparticles were characterized by scanning and transmission microscopy, Raman spectroscopy, which showed the formation of the under-stoichiometric titanium nitride (TiN1-x). TiN nanoparticles are investigated as a promising object for high adsorption It is shown that adsorption of TiN nanoparticles is associated with the electrostatic effect and the presence of pores in the synthesized nanoparticles. Optimal dye absorption capabilities were found to be associated with a low amount of Ti vacancies and high amount of N vacancies acting as donor states. The particles synthesized in water have the highest sorption capacity of dye achieving the value of 136.5 mg/g.","sentences":["Dyes used in industries such as textile, paper, and leather are known to be harmful to both human health and aquatic ecosystems.","Therefore, finding effective and sustainable methods to remove dyes from wastewater is crucial for mitigating the detrimental effects of pollution.","TiN nanoparticles have good absorption and conversion of light energy into thermal energy in the visible range of the spectrum, which makes them promising in various applications, from biomedical to environmental protection.","In this work, it is shown that titanium nitride nanoparticles also possess promising adsorption capabilitieseffect.","TiN nanoparticles were synthesized by laser ablation method in liquid.","Water, acetone and acetonitrile are used as solvent.","Nanoparticles were characterized by scanning and transmission microscopy, Raman spectroscopy, which showed the formation of the under-stoichiometric titanium nitride (TiN1-x).","TiN nanoparticles are investigated as a promising object for high adsorption It is shown that adsorption of TiN nanoparticles is associated with the electrostatic effect and the presence of pores in the synthesized nanoparticles.","Optimal dye absorption capabilities were found to be associated with a low amount of Ti vacancies and high amount of N vacancies acting as donor states.","The particles synthesized in water have the highest sorption capacity of dye achieving the value of 136.5 mg/g."],"url":"http://arxiv.org/abs/2404.14289v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 15:15:06","title":"Hybrid Fusion for 802.11ax Wi-Fi-based Passive Radars Exploiting Beamforming Feedbacks","abstract":"Passive Wi-Fi-based radars (PWRs) are devices that enable the localization of targets using Wi-Fi signals of opportunity transmitted by an access point. Unlike active radars that optimize their transmitted waveform for localization, PWRs align with the 802.11 amendments. Specifically, during the channel sounding session preceding a multi-user multiple-input multiple-output downlink transmission, an access point isotropically transmits a null data packet (NDP) with a known preamble. From these known symbols, client user equipments derive their channel state information and transmit an unencrypted beamforming feedback (BFF) back to the access point. The BFF comprises the right singular matrix of the channel and the corresponding stream gain for each subcarrier, which allows the computation of a beamforming matrix at the access point. In a classical PWR processing, only the preamble symbols from the NDP are exploited during the channel sounding session. In this study, we investigate multiple target localization by a PWR exploiting hybrid information sources. On one hand, the joint angle-of-departure and angle-of-arrival evaluated from the NDP. On another hand, the line-of-sight angle-of-departures inferred from the BFFs. The processing steps at the PWR are defined and an optimal hybrid fusion rule is derived in the maximum likelihood framework. Monte-Carlo simulations assess the enhanced accuracy of the proposed combination method compared to classical PWR processing based solely on the NDP, and compare the localisation performance between client and non-client targets.","sentences":["Passive Wi-Fi-based radars (PWRs) are devices that enable the localization of targets using Wi-Fi signals of opportunity transmitted by an access point.","Unlike active radars that optimize their transmitted waveform for localization, PWRs align with the 802.11 amendments.","Specifically, during the channel sounding session preceding a multi-user multiple-input multiple-output downlink transmission, an access point isotropically transmits a null data packet (NDP) with a known preamble.","From these known symbols, client user equipments derive their channel state information and transmit an unencrypted beamforming feedback (BFF) back to the access point.","The BFF comprises the right singular matrix of the channel and the corresponding stream gain for each subcarrier, which allows the computation of a beamforming matrix at the access point.","In a classical PWR processing, only the preamble symbols from the NDP are exploited during the channel sounding session.","In this study, we investigate multiple target localization by a PWR exploiting hybrid information sources.","On one hand, the joint angle-of-departure and angle-of-arrival evaluated from the NDP.","On another hand, the line-of-sight angle-of-departures inferred from the BFFs.","The processing steps at the PWR are defined and an optimal hybrid fusion rule is derived in the maximum likelihood framework.","Monte-Carlo simulations assess the enhanced accuracy of the proposed combination method compared to classical PWR processing based solely on the NDP, and compare the localisation performance between client and non-client targets."],"url":"http://arxiv.org/abs/2404.14269v1","category":"eess.SP"}
{"created":"2024-04-22 15:07:52","title":"Hierarchical NMPC for Obstacle Avoidance on Skid-steer Loaders","abstract":"This paper introduces a novel NMPC formulation for real-time obstacle avoidance on heavy equipment by modeling both vehicle and obstacles as convex superellipsoids. The combination of this approach with the separating hyperplane theorem and Optimization Engine (OpEn) allows to achieve efficient obstacle avoidance in autonomous heavy equipment and robotics. We demonstrate the efficacy of the approach through simulated and experimental results, showcasing a skid-steer loader's capability to navigate in obstructed environments.","sentences":["This paper introduces a novel NMPC formulation for real-time obstacle avoidance on heavy equipment by modeling both vehicle and obstacles as convex superellipsoids.","The combination of this approach with the separating hyperplane theorem and Optimization Engine (OpEn) allows to achieve efficient obstacle avoidance in autonomous heavy equipment and robotics.","We demonstrate the efficacy of the approach through simulated and experimental results, showcasing a skid-steer loader's capability to navigate in obstructed environments."],"url":"http://arxiv.org/abs/2404.14257v1","category":"math.OC"}
{"created":"2024-04-22 14:36:53","title":"General degree divergence-free finite element methods for the Stokes problem on smooth domains","abstract":"In this paper, we construct and analyze divergence-free finite element methods for the Stokes problem on smooth domains. The discrete spaces are based on the Scott-Vogelius finite element pair of arbitrary polynomial degree greater than two. By combining the Piola transform with the classical isoparametric framework, and with a judicious choice of degrees of freedom, we prove that the method converges with optimal order in the energy norm. We also show that the discrete velocity error converges with optimal order in the $L^2$-norm. Numerical experiments are presented, which support the theoretical results.","sentences":["In this paper, we construct and analyze divergence-free finite element methods for the Stokes problem on smooth domains.","The discrete spaces are based on the Scott-Vogelius finite element pair of arbitrary polynomial degree greater than two.","By combining the Piola transform with the classical isoparametric framework, and with a judicious choice of degrees of freedom, we prove that the method converges with optimal order in the energy norm.","We also show that the discrete velocity error converges with optimal order in the $L^2$-norm.","Numerical experiments are presented, which support the theoretical results."],"url":"http://arxiv.org/abs/2404.14226v1","category":"math.NA"}
{"created":"2024-04-22 14:18:28","title":"Minimizing the Number of Tardy Jobs with Uniform Processing Times on Parallel Machines","abstract":"In this work, we study the computational (parameterized) complexity of $P \\mid r_j, p_j=p \\mid \\sum_j w_j U_j$. Here, we are given $m$ identical parallel machines and $n$ jobs with equal processing time, each characterized by a release date, a due date, and a weight. The task is to find a feasible schedule, that is, an assignment of the jobs to starting times on machines, such that no job starts before its release date and no machine processes several jobs at the same time, that minimizes the weighted number of tardy jobs. A job is considered tardy if it finishes after its due date.   Our main contribution is showing that $P \\mid r_j, p_j=p \\mid \\sum_j U_j$ (the unweighted version of the problem) is NP-hard and W[2]-hard when parameterized by the number of machines. The former resolves an open problem in Note 2.1.19 by Kravchenko and Werner [Journal of Scheduling, 2011] and Open Problem 2 by Sgall [ESA, 2012], and the latter resolves Open Problem 7 by Mnich and van Bevern [Computers & Operations Research, 2018]. Furthermore, our result shows that the known XP-algorithm for $P \\mid r_j, p_j=p \\mid \\sum_j w_j U_j$ parameterized by the number of machines is optimal from a classification standpoint.   On the algorithmic side, we provide alternative running time bounds for the above-mentioned known XP-algorithm. Our analysis shows that $P \\mid r_j, p_j=p \\mid \\sum_j w_j U_j$ is contained in XP when parameterized by the processing time, and that it is contained in FPT when parameterized by the combination of the number of machines and the processing time. Finally, we give an FPT-algorithm for $P \\mid r_j, p_j=p \\mid \\sum_j w_j U_j$ parameterized by the number of release dates or the number of due dates. With this work, we lay out the foundation for a systematic study of the parameterized complexity of $P \\mid r_j, p_j=p \\mid \\sum_j w_j U_j$.","sentences":["In this work, we study the computational (parameterized) complexity of $P \\mid r_j, p_j=p \\mid \\sum_j w_j","U_j$. Here, we are given $m$ identical parallel machines and $n$ jobs with equal processing time, each characterized by a release date, a due date, and a weight.","The task is to find a feasible schedule, that is, an assignment of the jobs to starting times on machines, such that no job starts before its release date and no machine processes several jobs at the same time, that minimizes the weighted number of tardy jobs.","A job is considered tardy if it finishes after its due date.   ","Our main contribution is showing that $P \\mid r_j, p_j=p \\mid \\sum_j U_j$ (the unweighted version of the problem) is NP-hard and W[2]-hard when parameterized by the number of machines.","The former resolves an open problem in Note 2.1.19 by Kravchenko and Werner [Journal of Scheduling, 2011] and Open Problem 2 by Sgall [ESA, 2012], and the latter resolves Open Problem 7 by Mnich and van Bevern [Computers & Operations Research, 2018].","Furthermore, our result shows that the known XP-algorithm for $P \\mid r_j, p_j=p \\mid \\sum_j w_j","U_j$ parameterized by the number of machines is optimal from a classification standpoint.   ","On the algorithmic side, we provide alternative running time bounds for the above-mentioned known XP-algorithm.","Our analysis shows that $P \\mid r_j, p_j=p \\mid \\sum_j w_j","U_j$ is contained in XP when parameterized by the processing time, and that it is contained in FPT when parameterized by the combination of the number of machines and the processing time.","Finally, we give an FPT-algorithm for $P \\mid r_j, p_j=p \\mid \\sum_j w_j","U_j$ parameterized by the number of release dates or the number of due dates.","With this work, we lay out the foundation for a systematic study of the parameterized complexity of $P \\mid r_j, p_j=p \\mid \\sum_j w_j","U_j$."],"url":"http://arxiv.org/abs/2404.14208v1","category":"cs.DS"}
{"created":"2024-04-22 14:03:46","title":"Optimal Multiparameter Metrology: The Quantum Compass Solution","abstract":"We study optimal quantum sensing of multiple physical parameters using repeated measurements. In this scenario, the Fisher information framework sets the fundamental limits on sensing performance, yet the optimal states and corresponding measurements that attain these limits remain to be discovered. To address this, we extend the Fisher information approach with a second optimality requirement for a sensor to provide unambiguous estimation of unknown parameters. We propose a systematic method integrating Fisher information and Bayesian approaches to quantum metrology to identify the combination of input states and measurements that satisfies both optimality criteria. Specifically, we frame the optimal sensing problem as an optimization of an asymptotic Bayesian cost function that can be efficiently solved numerically and, in many cases, analytically. We refer to the resulting optimal sensor as a `quantum compass' solution, which serves as a direct multiparameter counterpart to the Greenberger-Horne-Zeilinger state-based interferometer, renowned for achieving the Heisenberg limit in single-parameter metrology. We provide exact quantum compass solutions for paradigmatic multiparameter problem of sensing two and three parameters using an SU(2) sensor. Our metrological cost function opens avenues for quantum variational techniques to design low-depth quantum circuits approaching the optimal sensing performance in the many-repetition scenario. We demonstrate this by constructing simple quantum circuits that achieve the Heisenberg limit for vector field and 3D rotations estimation using a limited set of gates available on a trapped-ion platform. Our work introduces and optimizes sensors for a practical notion of optimality, keeping in mind the ultimate goal of quantum sensors to precisely estimate unknown parameters.","sentences":["We study optimal quantum sensing of multiple physical parameters using repeated measurements.","In this scenario, the Fisher information framework sets the fundamental limits on sensing performance, yet the optimal states and corresponding measurements that attain these limits remain to be discovered.","To address this, we extend the Fisher information approach with a second optimality requirement for a sensor to provide unambiguous estimation of unknown parameters.","We propose a systematic method integrating Fisher information and Bayesian approaches to quantum metrology to identify the combination of input states and measurements that satisfies both optimality criteria.","Specifically, we frame the optimal sensing problem as an optimization of an asymptotic Bayesian cost function that can be efficiently solved numerically and, in many cases, analytically.","We refer to the resulting optimal sensor as a `quantum compass' solution, which serves as a direct multiparameter counterpart to the Greenberger-Horne-Zeilinger state-based interferometer, renowned for achieving the Heisenberg limit in single-parameter metrology.","We provide exact quantum compass solutions for paradigmatic multiparameter problem of sensing two and three parameters using an SU(2) sensor.","Our metrological cost function opens avenues for quantum variational techniques to design low-depth quantum circuits approaching the optimal sensing performance in the many-repetition scenario.","We demonstrate this by constructing simple quantum circuits that achieve the Heisenberg limit for vector field and 3D rotations estimation using a limited set of gates available on a trapped-ion platform.","Our work introduces and optimizes sensors for a practical notion of optimality, keeping in mind the ultimate goal of quantum sensors to precisely estimate unknown parameters."],"url":"http://arxiv.org/abs/2404.14194v1","category":"quant-ph"}
{"created":"2024-04-22 13:57:30","title":"Bayesian Windkessel calibration using optimized 0D surrogate models","abstract":"Boundary condition (BC) calibration to assimilate clinical measurements is an essential step in any subject-specific simulation of cardiovascular fluid dynamics. Bayesian calibration approaches have successfully quantified the uncertainties inherent in identified parameters. Yet, routinely estimating the posterior distribution for all BC parameters in 3D simulations has been unattainable due to the infeasible computational demand. We propose an efficient method to identify Windkessel parameter posteriors using results from a single high-fidelity three-dimensional (3D) model evaluation. We only evaluate the 3D model once for an initial choice of BCs and use the result to create a highly accurate zero-dimensional (0D) surrogate. We then perform Sequential Monte Carlo (SMC) using the optimized 0D model to derive the high-dimensional Windkessel BC posterior distribution. We validate this approach in a publicly available dataset of N=72 subject-specific vascular models. We found that optimizing 0D models to match 3D data a priori lowered their median approximation error by nearly one order of magnitude. In a subset of models, we confirm that the optimized 0D models still generalize to a wide range of BCs. Finally, we present the high-dimensional Windkessel parameter posterior for different measured signal-to-noise ratios in a vascular model using SMC. We further validate that the 0D-derived posterior is a good approximation of the 3D posterior. The minimal computational demand of our method using a single 3D simulation, combined with the open-source nature of all software and data used in this work, will increase access and efficiency of Bayesian Windkessel calibration in cardiovascular fluid dynamics simulations.","sentences":["Boundary condition (BC) calibration to assimilate clinical measurements is an essential step in any subject-specific simulation of cardiovascular fluid dynamics.","Bayesian calibration approaches have successfully quantified the uncertainties inherent in identified parameters.","Yet, routinely estimating the posterior distribution for all BC parameters in 3D simulations has been unattainable due to the infeasible computational demand.","We propose an efficient method to identify Windkessel parameter posteriors using results from a single high-fidelity three-dimensional (3D) model evaluation.","We only evaluate the 3D model once for an initial choice of BCs and use the result to create a highly accurate zero-dimensional (0D) surrogate.","We then perform Sequential Monte Carlo (SMC) using the optimized 0D model to derive the high-dimensional Windkessel BC posterior distribution.","We validate this approach in a publicly available dataset of N=72 subject-specific vascular models.","We found that optimizing 0D models to match 3D data a priori lowered their median approximation error by nearly one order of magnitude.","In a subset of models, we confirm that the optimized 0D models still generalize to a wide range of BCs.","Finally, we present the high-dimensional Windkessel parameter posterior for different measured signal-to-noise ratios in a vascular model using SMC.","We further validate that the 0D-derived posterior is a good approximation of the 3D posterior.","The minimal computational demand of our method using a single 3D simulation, combined with the open-source nature of all software and data used in this work, will increase access and efficiency of Bayesian Windkessel calibration in cardiovascular fluid dynamics simulations."],"url":"http://arxiv.org/abs/2404.14187v1","category":"cs.CE"}
{"created":"2024-04-22 13:54:26","title":"Mathematical Crystal Chemistry","abstract":"Efficient heuristics have predicted many functional materials such as high-temperature superconducting hydrides, while inorganic structural chemistry explains why and how the crystal structures are stabilized. Here we develop the paired mathematical programming formalism for searching and systematizing the structural prototypes of crystals. The first is the minimization of the volume of the unit cell under the constraints of only the minimum and maximum distances between pairs of atoms. We show the capabilities of linear relaxations of inequality constraints to optimize structures by the steepest-descent method, which is computationally very efficient. The second is the discrete optimization to assign five kinds of geometrical constraints including chemical bonds for pairs of atoms. Under the constraints, the two object functions, formulated as mathematical programming, are alternately optimized to realize the given coordination numbers of atoms. This approach successfully generates a wide variety of crystal structures of oxides such as spinel, pyrochlore-$\\alpha$, and $\\mathrm{K}_2 \\mathrm{NiF}_4$ structures.","sentences":["Efficient heuristics have predicted many functional materials such as high-temperature superconducting hydrides, while inorganic structural chemistry explains why and how the crystal structures are stabilized.","Here we develop the paired mathematical programming formalism for searching and systematizing the structural prototypes of crystals.","The first is the minimization of the volume of the unit cell under the constraints of only the minimum and maximum distances between pairs of atoms.","We show the capabilities of linear relaxations of inequality constraints to optimize structures by the steepest-descent method, which is computationally very efficient.","The second is the discrete optimization to assign five kinds of geometrical constraints including chemical bonds for pairs of atoms.","Under the constraints, the two object functions, formulated as mathematical programming, are alternately optimized to realize the given coordination numbers of atoms.","This approach successfully generates a wide variety of crystal structures of oxides such as spinel, pyrochlore-$\\alpha$, and $\\mathrm{K}_2 \\mathrm{NiF}_4$ structures."],"url":"http://arxiv.org/abs/2404.14181v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 13:49:43","title":"Non-trivial $r$-wise agreeing families","abstract":"A family of sets is $r$-wise agreeing if for any $r$ sets from the family there is an element $x$ that is either contained in all or contained in none of the $r$ sets. The study of such families is motivated by questions in discrete optimization. In this paper, we determine the size of the largest non-trivial $r$-wise agreeing family. This can be seen as a generalization of the classical Brace-Daykin theorem.","sentences":["A family of sets is $r$-wise agreeing if for any $r$ sets from the family there is an element $x$ that is either contained in all or contained in none of the $r$ sets.","The study of such families is motivated by questions in discrete optimization.","In this paper, we determine the size of the largest non-trivial $r$-wise agreeing family.","This can be seen as a generalization of the classical Brace-Daykin theorem."],"url":"http://arxiv.org/abs/2404.14178v1","category":"math.CO"}
{"created":"2024-04-22 13:26:42","title":"New Solutions Based on the Generalized Eigenvalue Problem for the Data Collaboration Analysis","abstract":"In recent years, the accumulation of data across various institutions has garnered attention for the technology of confidential data analysis, which improves analytical accuracy by sharing data between multiple institutions while protecting sensitive information. Among these methods, Data Collaboration Analysis (DCA) is noted for its efficiency in terms of computational cost and communication load, facilitating data sharing and analysis across different institutions while safeguarding confidential information. However, existing optimization problems for determining the necessary collaborative functions have faced challenges, such as the optimal solution for the collaborative representation often being a zero matrix and the difficulty in understanding the process of deriving solutions. This research addresses these issues by formulating the optimization problem through the segmentation of matrices into column vectors and proposing a solution method based on the generalized eigenvalue problem. Additionally, we demonstrate methods for constructing collaborative functions more effectively through weighting and the selection of efficient algorithms suited to specific situations. Experiments using real-world datasets have shown that our proposed formulation and solution for the collaborative function optimization problem achieve superior predictive accuracy compared to existing methods.","sentences":["In recent years, the accumulation of data across various institutions has garnered attention for the technology of confidential data analysis, which improves analytical accuracy by sharing data between multiple institutions while protecting sensitive information.","Among these methods, Data Collaboration Analysis (DCA) is noted for its efficiency in terms of computational cost and communication load, facilitating data sharing and analysis across different institutions while safeguarding confidential information.","However, existing optimization problems for determining the necessary collaborative functions have faced challenges, such as the optimal solution for the collaborative representation often being a zero matrix and the difficulty in understanding the process of deriving solutions.","This research addresses these issues by formulating the optimization problem through the segmentation of matrices into column vectors and proposing a solution method based on the generalized eigenvalue problem.","Additionally, we demonstrate methods for constructing collaborative functions more effectively through weighting and the selection of efficient algorithms suited to specific situations.","Experiments using real-world datasets have shown that our proposed formulation and solution for the collaborative function optimization problem achieve superior predictive accuracy compared to existing methods."],"url":"http://arxiv.org/abs/2404.14164v1","category":"cs.LG"}
{"created":"2024-04-22 13:18:59","title":"Semirandom Planted Clique and the Restricted Isometry Property","abstract":"We give a simple, greedy $O(n^{\\omega+0.5})=O(n^{2.872})$-time algorithm to list-decode planted cliques in a semirandom model introduced in [CSV17] (following [FK01]) that succeeds whenever the size of the planted clique is $k\\geq O(\\sqrt{n} \\log^2 n)$. In the model, the edges touching the vertices in the planted $k$-clique are drawn independently with probability $p=1/2$ while the edges not touching the planted clique are chosen by an adversary in response to the random choices. Our result shows that the computational threshold in the semirandom setting is within a $O(\\log^2 n)$ factor of the information-theoretic one [Ste17] thus resolving an open question of Steinhardt. This threshold also essentially matches the conjectured computational threshold for the well-studied special case of fully random planted clique.   All previous algorithms [CSV17, MMT20, BKS23] in this model are based on rather sophisticated rounding algorithms for entropy-constrained semidefinite programming relaxations and their sum-of-squares strengthenings and the best known guarantee is a $n^{O(1/\\epsilon)}$-time algorithm to list-decode planted cliques of size $k \\geq \\tilde{O}(n^{1/2+\\epsilon})$. In particular, the guarantee trivializes to quasi-polynomial time if the planted clique is of size $O(\\sqrt{n} \\operatorname{polylog} n)$. Our algorithm achieves an almost optimal guarantee with a surprisingly simple greedy algorithm.   The prior state-of-the-art algorithmic result above is based on a reduction to certifying bounds on the size of unbalanced bicliques in random graphs -- closely related to certifying the restricted isometry property (RIP) of certain random matrices and known to be hard in the low-degree polynomial model. Our key idea is a new approach that relies on the truth of -- but not efficient certificates for -- RIP of a new class of matrices built from the input graphs.","sentences":["We give a simple, greedy $O(n^{\\omega+0.5})=O(n^{2.872})$-time algorithm to list-decode planted cliques in a semirandom model introduced in [CSV17] (following [FK01]) that succeeds whenever the size of the planted clique is $k\\geq O(\\sqrt{n} \\log^2","n)$. In the model, the edges touching the vertices in the planted $k$-clique are drawn independently with probability $p=1/2$ while the edges not touching the planted clique are chosen by an adversary in response to the random choices.","Our result shows that the computational threshold in the semirandom setting is within a $O(\\log^2","n)$ factor of the information-theoretic one [Ste17] thus resolving an open question of Steinhardt.","This threshold also essentially matches the conjectured computational threshold for the well-studied special case of fully random planted clique.   ","All previous algorithms [CSV17, MMT20, BKS23] in this model are based on rather sophisticated rounding algorithms for entropy-constrained semidefinite programming relaxations and their sum-of-squares strengthenings and the best known guarantee is a $n^{O(1/\\epsilon)}$-time algorithm to list-decode planted cliques of size $k \\geq \\tilde{O}(n^{1/2+\\epsilon})$.","In particular, the guarantee trivializes to quasi-polynomial time if the planted clique is of size $O(\\sqrt{n} \\operatorname{polylog} n)$. Our algorithm achieves an almost optimal guarantee with a surprisingly simple greedy algorithm.   ","The prior state-of-the-art algorithmic result above is based on a reduction to certifying bounds on the size of unbalanced bicliques in random graphs -- closely related to certifying the restricted isometry property (RIP) of certain random matrices and known to be hard in the low-degree polynomial model.","Our key idea is a new approach that relies on the truth of -- but not efficient certificates for -- RIP of a new class of matrices built from the input graphs."],"url":"http://arxiv.org/abs/2404.14159v1","category":"cs.DS"}
{"created":"2024-04-22 13:02:38","title":"185 mW, 1 MHz, 15 fs carrier-envelope phase-stable pulse generation via polarization-optimized down-conversion from gas-filled hollow-core fiber","abstract":"Gas-filled hollow core fibers allow the generation of single-cycle pulses at megahertz repetition rates. When coupled with difference frequency generation, they can be an ideal driver for the generation of carrier-envelope phase stable, octave-spanning pulses in the short-wavelength infrared. In this work, we investigate the dependence of the polarization state in gas-filled hollow-core fibers on the subsequent difference frequency generation stage. We show that by adjusting the input polarization state of light in geometrically symmetric systems, such as hollow-core fibers, one can achieve precise control over the polarization state of the output pulses. Importantly, this manipulation preserves the temporal characteristics of the ultrashort pulses generated, especially when operating near the single-cycle regime. We leverage this property to boost the down-conversion efficiency of these pulses in a type I difference frequency generation stage. Our technique overcomes the bandwidth and dispersion constraints of the previous methods that rely on broadband waveplates or adjustment of crystal axes relative to the laboratory frame. This advancement is crucial for experiments demanding pure polarization states in the eigenmodes of the laboratory frame.","sentences":["Gas-filled hollow core fibers allow the generation of single-cycle pulses at megahertz repetition rates.","When coupled with difference frequency generation, they can be an ideal driver for the generation of carrier-envelope phase stable, octave-spanning pulses in the short-wavelength infrared.","In this work, we investigate the dependence of the polarization state in gas-filled hollow-core fibers on the subsequent difference frequency generation stage.","We show that by adjusting the input polarization state of light in geometrically symmetric systems, such as hollow-core fibers, one can achieve precise control over the polarization state of the output pulses.","Importantly, this manipulation preserves the temporal characteristics of the ultrashort pulses generated, especially when operating near the single-cycle regime.","We leverage this property to boost the down-conversion efficiency of these pulses in a type I difference frequency generation stage.","Our technique overcomes the bandwidth and dispersion constraints of the previous methods that rely on broadband waveplates or adjustment of crystal axes relative to the laboratory frame.","This advancement is crucial for experiments demanding pure polarization states in the eigenmodes of the laboratory frame."],"url":"http://arxiv.org/abs/2404.14153v1","category":"physics.optics"}
{"created":"2024-04-22 12:55:04","title":"Physics-based reward driven image analysis in microscopy","abstract":"The rise of electron microscopy has expanded our ability to acquire nanometer and atomically resolved images of complex materials. The resulting vast datasets are typically analyzed by human operators, an intrinsically challenging process due to the multiple possible analysis steps and the corresponding need to build and optimize complex analysis workflows. We present a methodology based on the concept of a Reward Function coupled with Bayesian Optimization, to optimize image analysis workflows dynamically. The Reward Function is engineered to closely align with the experimental objectives and broader context and is quantifiable upon completion of the analysis. Here, cross-section, high-angle annular dark field (HAADF) images of ion-irradiated $(Y, Dy)Ba_2Cu_3O_{7-\\delta}$ thin-films were used as a model system. The reward functions were formed based on the expected materials density and atomic spacings and used to drive multi-objective optimization of the classical Laplacian-of-Gaussian (LoG) method. These results can be benchmarked against the DCNN segmentation. This optimized LoG* compares favorably against DCNN in the presence of the additional noise. We further extend the reward function approach towards the identification of partially-disordered regions, creating a physics-driven reward function and action space of high-dimensional clustering. We pose that with correct definition, the reward function approach allows real-time optimization of complex analysis workflows at much higher speeds and lower computational costs than classical DCNN-based inference, ensuring the attainment of results that are both precise and aligned with the human-defined objectives.","sentences":["The rise of electron microscopy has expanded our ability to acquire nanometer and atomically resolved images of complex materials.","The resulting vast datasets are typically analyzed by human operators, an intrinsically challenging process due to the multiple possible analysis steps and the corresponding need to build and optimize complex analysis workflows.","We present a methodology based on the concept of a Reward Function coupled with Bayesian Optimization, to optimize image analysis workflows dynamically.","The Reward Function is engineered to closely align with the experimental objectives and broader context and is quantifiable upon completion of the analysis.","Here, cross-section, high-angle annular dark field (HAADF) images of ion-irradiated $(Y, Dy)Ba_2Cu_3O_{7-\\delta}$ thin-films were used as a model system.","The reward functions were formed based on the expected materials density and atomic spacings and used to drive multi-objective optimization of the classical Laplacian-of-Gaussian (LoG) method.","These results can be benchmarked against the DCNN segmentation.","This optimized LoG* compares favorably against DCNN in the presence of the additional noise.","We further extend the reward function approach towards the identification of partially-disordered regions, creating a physics-driven reward function and action space of high-dimensional clustering.","We pose that with correct definition, the reward function approach allows real-time optimization of complex analysis workflows at much higher speeds and lower computational costs than classical DCNN-based inference, ensuring the attainment of results that are both precise and aligned with the human-defined objectives."],"url":"http://arxiv.org/abs/2404.14146v2","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 12:03:43","title":"Dionysos.jl: a Modular Platform for Smart Symbolic Control","abstract":"We introduce Dionysos.jl, a modular package for solving optimal control problems for complex dynamical systems using state-of-the-art and experimental techniques from symbolic control, optimization, and learning. More often than not with Cyber-Physical systems, the only sensible way of developing a controller is by discretizing the different variables, thus transforming the control task into a purely combinatorial problem on a finite-state mathematical object, called an abstraction of this system. Although this approach offers a safety-critical framework, the available techniques suffer important scalability issues. In order to render these techniques practical, it is necessary to construct smarter abstractions that differ from classical techniques by partitioning the state-space in a non trivial way.","sentences":["We introduce Dionysos.jl, a modular package for solving optimal control problems for complex dynamical systems using state-of-the-art and experimental techniques from symbolic control, optimization, and learning.","More often than not with Cyber-Physical systems, the only sensible way of developing a controller is by discretizing the different variables, thus transforming the control task into a purely combinatorial problem on a finite-state mathematical object, called an abstraction of this system.","Although this approach offers a safety-critical framework, the available techniques suffer important scalability issues.","In order to render these techniques practical, it is necessary to construct smarter abstractions that differ from classical techniques by partitioning the state-space in a non trivial way."],"url":"http://arxiv.org/abs/2404.14114v1","category":"eess.SY"}
{"created":"2024-04-22 11:54:54","title":"Achieving binary topology optimization solutions via automatic projection parameter increase","abstract":"A method is created to automatically increase the threshold projection parameter in three-field density-based topology optimization to achieve a near binary design. The parameter increase each iteration is based on an exponential growth function, where the growth rate is dynamically changed during optimization by linking it to the change in objective function. This results in a method that does not need to be tuned for specific problems, or optimizers, and the same set of hyper-parameters can be used for a wide range of problems. The effectiveness of the method is demonstrated on several 2D benchmark problems, including linear buckling and geometrically nonlinear problems.","sentences":["A method is created to automatically increase the threshold projection parameter in three-field density-based topology optimization to achieve a near binary design.","The parameter increase each iteration is based on an exponential growth function, where the growth rate is dynamically changed during optimization by linking it to the change in objective function.","This results in a method that does not need to be tuned for specific problems, or optimizers, and the same set of hyper-parameters can be used for a wide range of problems.","The effectiveness of the method is demonstrated on several 2D benchmark problems, including linear buckling and geometrically nonlinear problems."],"url":"http://arxiv.org/abs/2404.14111v1","category":"math.OC"}
{"created":"2024-04-22 11:52:40","title":"CKD: Contrastive Knowledge Distillation from A Sample-wise Perspective","abstract":"In this paper, we present a simple yet effective contrastive knowledge distillation approach, which can be formulated as a sample-wise alignment problem with intra- and inter-sample constraints. Unlike traditional knowledge distillation methods that concentrate on maximizing feature similarities or preserving class-wise semantic correlations between teacher and student features, our method attempts to recover the \"dark knowledge\" by aligning sample-wise teacher and student logits. Specifically, our method first minimizes logit differences within the same sample by considering their numerical values, thus preserving intra-sample similarities. Next, we bridge semantic disparities by leveraging dissimilarities across different samples. Note that constraints on intra-sample similarities and inter-sample dissimilarities can be efficiently and effectively reformulated into a contrastive learning framework with newly designed positive and negative pairs. The positive pair consists of the teacher's and student's logits derived from an identical sample, while the negative pairs are formed by using logits from different samples. With this formulation, our method benefits from the simplicity and efficiency of contrastive learning through the optimization of InfoNCE, yielding a run-time complexity that is far less than $O(n^2)$, where $n$ represents the total number of training samples. Furthermore, our method can eliminate the need for hyperparameter tuning, particularly related to temperature parameters and large batch sizes. We conduct comprehensive experiments on three datasets including CIFAR-100, ImageNet-1K, and MS COCO. Experimental results clearly confirm the effectiveness of the proposed method on both image classification and object detection tasks. Our source codes will be publicly available at https://github.com/wencheng-zhu/CKD.","sentences":["In this paper, we present a simple yet effective contrastive knowledge distillation approach, which can be formulated as a sample-wise alignment problem with intra- and inter-sample constraints.","Unlike traditional knowledge distillation methods that concentrate on maximizing feature similarities or preserving class-wise semantic correlations between teacher and student features, our method attempts to recover the \"dark knowledge\" by aligning sample-wise teacher and student logits.","Specifically, our method first minimizes logit differences within the same sample by considering their numerical values, thus preserving intra-sample similarities.","Next, we bridge semantic disparities by leveraging dissimilarities across different samples.","Note that constraints on intra-sample similarities and inter-sample dissimilarities can be efficiently and effectively reformulated into a contrastive learning framework with newly designed positive and negative pairs.","The positive pair consists of the teacher's and student's logits derived from an identical sample, while the negative pairs are formed by using logits from different samples.","With this formulation, our method benefits from the simplicity and efficiency of contrastive learning through the optimization of InfoNCE, yielding a run-time complexity that is far less than $O(n^2)$, where $n$ represents the total number of training samples.","Furthermore, our method can eliminate the need for hyperparameter tuning, particularly related to temperature parameters and large batch sizes.","We conduct comprehensive experiments on three datasets including CIFAR-100, ImageNet-1K, and MS COCO.","Experimental results clearly confirm the effectiveness of the proposed method on both image classification and object detection tasks.","Our source codes will be publicly available at https://github.com/wencheng-zhu/CKD."],"url":"http://arxiv.org/abs/2404.14109v1","category":"cs.CV"}
{"created":"2024-04-22 11:40:08","title":"Efficient molecular conformation generation with quantum-inspired algorithm","abstract":"Conformation generation, also known as molecular unfolding (MU), is a crucial step in structure-based drug design, remaining a challenging combinatorial optimization problem. Quantum annealing (QA) has shown great potential for solving certain combinatorial optimization problems over traditional classical methods such as simulated annealing (SA). However, a recent study showed that a 2000-qubit QA hardware was still unable to outperform SA for the MU problem. Here, we propose the use of quantum-inspired algorithm to solve the MU problem, in order to go beyond traditional SA. We introduce a highly-compact phase encoding method which can exponentially reduce the representation space, compared with the previous one-hot encoding method. For benchmarking, we tested this new approach on the public QM9 dataset generated by density functional theory (DFT). The root-mean-square deviation between the conformation determined by our approach and DFT is negligible (less than about 0.5 Angstrom), which underpins the validity of our approach. Furthermore, the median time-to-target metric can be reduced by a factor of five compared to SA. Additionally, we demonstrate a simulation experiment by MindQuantum using quantum approximate optimization algorithm (QAOA) to reach optimal results. These results indicate that quantum-inspired algorithms can be applied to solve practical problems even before quantum hardware become mature.","sentences":["Conformation generation, also known as molecular unfolding (MU), is a crucial step in structure-based drug design, remaining a challenging combinatorial optimization problem.","Quantum annealing (QA) has shown great potential for solving certain combinatorial optimization problems over traditional classical methods such as simulated annealing (SA).","However, a recent study showed that a 2000-qubit QA hardware was still unable to outperform SA for the MU problem.","Here, we propose the use of quantum-inspired algorithm to solve the MU problem, in order to go beyond traditional SA.","We introduce a highly-compact phase encoding method which can exponentially reduce the representation space, compared with the previous one-hot encoding method.","For benchmarking, we tested this new approach on the public QM9 dataset generated by density functional theory (DFT).","The root-mean-square deviation between the conformation determined by our approach and DFT is negligible (less than about 0.5 Angstrom), which underpins the validity of our approach.","Furthermore, the median time-to-target metric can be reduced by a factor of five compared to SA.","Additionally, we demonstrate a simulation experiment by MindQuantum using quantum approximate optimization algorithm (QAOA) to reach optimal results.","These results indicate that quantum-inspired algorithms can be applied to solve practical problems even before quantum hardware become mature."],"url":"http://arxiv.org/abs/2404.14101v1","category":"quant-ph"}
{"created":"2024-04-22 10:49:46","title":"Research on Robot Path Planning Based on Reinforcement Learning","abstract":"This project has conducted research on robot path planning based on Visual SLAM. The main work of this project is as follows: (1) Construction of Visual SLAM system. Research has been conducted on the basic architecture of Visual SLAM. A Visual SLAM system is developed based on ORB-SLAM3 system, which can conduct dense point cloud mapping. (2) The map suitable for two-dimensional path planning is obtained through map conversion. This part converts the dense point cloud map obtained by Visual SLAM system into an octomap and then performs projection transformation to the grid map. The map conversion converts the dense point cloud map containing a large amount of redundant map information into an extremely lightweight grid map suitable for path planning. (3) Research on path planning algorithm based on reinforcement learning. This project has conducted experimental comparisons between the Q-learning algorithm, the DQN algorithm, and the SARSA algorithm, and found that DQN is the algorithm with the fastest convergence and best performance in high-dimensional complex environments. This project has conducted experimental verification of the Visual SLAM system in a simulation environment. The experimental results obtained based on open-source dataset and self-made dataset prove the feasibility and effectiveness of the designed Visual SLAM system. At the same time, this project has also conducted comparative experiments on the three reinforcement learning algorithms under the same experimental condition to obtain the optimal algorithm under the experimental condition.","sentences":["This project has conducted research on robot path planning based on Visual SLAM.","The main work of this project is as follows: (1) Construction of Visual SLAM system.","Research has been conducted on the basic architecture of Visual SLAM.","A Visual SLAM system is developed based on ORB-SLAM3 system, which can conduct dense point cloud mapping.","(2) The map suitable for two-dimensional path planning is obtained through map conversion.","This part converts the dense point cloud map obtained by Visual SLAM system into an octomap and then performs projection transformation to the grid map.","The map conversion converts the dense point cloud map containing a large amount of redundant map information into an extremely lightweight grid map suitable for path planning.","(3) Research on path planning algorithm based on reinforcement learning.","This project has conducted experimental comparisons between the Q-learning algorithm, the DQN algorithm, and the SARSA algorithm, and found that DQN is the algorithm with the fastest convergence and best performance in high-dimensional complex environments.","This project has conducted experimental verification of the Visual SLAM system in a simulation environment.","The experimental results obtained based on open-source dataset and self-made dataset prove the feasibility and effectiveness of the designed Visual SLAM system.","At the same time, this project has also conducted comparative experiments on the three reinforcement learning algorithms under the same experimental condition to obtain the optimal algorithm under the experimental condition."],"url":"http://arxiv.org/abs/2404.14077v1","category":"cs.RO"}
{"created":"2024-04-22 10:35:37","title":"Likelihood analysis of the newly observed $f_0(2020)$, $f_0(2330)$ and $f_0(2470)$ in $J/\u03c8\\to \u03b3\u03b7^\\prime\u03b7^\\prime$ as high-lying unflavored scalar mesons","abstract":"Inspired by the newly observed three scalar states $f_0(2020)$, $f_0(2330)$, and $f_0(2470)$ by the BESII Collaboration in $J/\\psi\\to \\gamma\\eta^\\prime\\eta^\\prime$, we carried out the study of spectroscopic behavior of these high-lying unflavored scalar mesonic states. In this work, using the Regge trajectory analysis and the quark pair creation model, we discussed the assignments of the $f_0(2020)$, $f_0(2330)$, and $f_0(2470)$ associated with the $f_0(2200)$ as the isoscalar high-lying scalar mesonic states and predicted the spectroscopic properties of their high-lying partners. The present study may provide valuable information for the construction of the scalar meson family.","sentences":["Inspired by the newly observed three scalar states $f_0(2020)$, $f_0(2330)$, and $f_0(2470)$ by the BESII Collaboration in $J/\\psi\\to \\gamma\\eta^\\prime\\eta^\\prime$, we carried out the study of spectroscopic behavior of these high-lying unflavored scalar mesonic states.","In this work, using the Regge trajectory analysis and the quark pair creation model, we discussed the assignments of the $f_0(2020)$, $f_0(2330)$, and $f_0(2470)$ associated with the $f_0(2200)$ as the isoscalar high-lying scalar mesonic states and predicted the spectroscopic properties of their high-lying partners.","The present study may provide valuable information for the construction of the scalar meson family."],"url":"http://arxiv.org/abs/2404.14075v1","category":"hep-ph"}
{"created":"2024-04-22 10:21:41","title":"Multi-view Disentanglement for Reinforcement Learning with Multiple Cameras","abstract":"The performance of image-based Reinforcement Learning (RL) agents can vary depending on the position of the camera used to capture the images. Training on multiple cameras simultaneously, including a first-person egocentric camera, can leverage information from different camera perspectives to improve the performance of RL. However, hardware constraints may limit the availability of multiple cameras in real-world deployment. Additionally, cameras may become damaged in the real-world preventing access to all cameras that were used during training. To overcome these hardware constraints, we propose Multi-View Disentanglement (MVD), which uses multiple cameras to learn a policy that achieves zero-shot generalisation to any single camera from the training set. Our approach is a self-supervised auxiliary task for RL that learns a disentangled representation from multiple cameras, with a shared representation that is aligned across all cameras to allow generalisation to a single camera, and a private representation that is camera-specific. We show experimentally that an RL agent trained on a single third-person camera is unable to learn an optimal policy in many control tasks; but, our approach, benefiting from multiple cameras during training, is able to solve the task using only the same single third-person camera.","sentences":["The performance of image-based Reinforcement Learning (RL) agents can vary depending on the position of the camera used to capture the images.","Training on multiple cameras simultaneously, including a first-person egocentric camera, can leverage information from different camera perspectives to improve the performance of RL.","However, hardware constraints may limit the availability of multiple cameras in real-world deployment.","Additionally, cameras may become damaged in the real-world preventing access to all cameras that were used during training.","To overcome these hardware constraints, we propose Multi-View Disentanglement (MVD), which uses multiple cameras to learn a policy that achieves zero-shot generalisation to any single camera from the training set.","Our approach is a self-supervised auxiliary task for RL that learns a disentangled representation from multiple cameras, with a shared representation that is aligned across all cameras to allow generalisation to a single camera, and a private representation that is camera-specific.","We show experimentally that an RL agent trained on a single third-person camera is unable to learn an optimal policy in many control tasks; but, our approach, benefiting from multiple cameras during training, is able to solve the task using only the same single third-person camera."],"url":"http://arxiv.org/abs/2404.14064v1","category":"cs.LG"}
{"created":"2024-04-22 10:16:18","title":"Volumes of components of Lelong upper level sets II","abstract":"Let $X$ be a compact K\\\"ahler manifold of dimension $n$, and let $T$ be a closed positive $(1,1)$-current in a nef cohomology class on $X$. We establish an optimal upper bound for the volume of components of Lelong upper level sets of $T$ in terms of cohomology classes of non-pluripolar self-products of $T$.","sentences":["Let $X$ be a compact K\\\"ahler manifold of dimension $n$, and let $T$ be a closed positive $(1,1)$-current in a nef cohomology class on $X$. We establish an optimal upper bound for the volume of components of Lelong upper level sets of $T$ in terms of cohomology classes of non-pluripolar self-products of $T$."],"url":"http://arxiv.org/abs/2404.14058v1","category":"math.CV"}
{"created":"2024-04-22 10:10:29","title":"Solving Combinatorial Optimization Problems with a Block Encoding Quantum Optimizer","abstract":"In the pursuit of achieving near-term quantum advantage for combinatorial optimization problems, the Quantum Approximate Optimization Algorithm (QAOA) and the Variational Quantum Eigensolver (VQE) are the primary methods of interest, but their practical effectiveness remains uncertain. Therefore, there is a persistent need to develop and evaluate alternative variational quantum algorithms. This study presents an investigation of the Block ENcoding Quantum Optimizer (BENQO), a hybrid quantum solver that uses block encoding to represent the cost function. BENQO is designed to be universally applicable across discrete optimization problems. Beyond Maximum Cut, we evaluate BENQO's performance in the context of the Traveling Salesperson Problem, which is of greater practical relevance. Our findings confirm that BENQO performs significantly better than QAOA and competes with VQE across a variety of performance metrics. We conclude that BENQO is a promising novel hybrid quantum-classical algorithm that should be further investigated and optimized to realize its full potential.","sentences":["In the pursuit of achieving near-term quantum advantage for combinatorial optimization problems, the Quantum Approximate Optimization Algorithm (QAOA) and the Variational Quantum Eigensolver (VQE) are the primary methods of interest, but their practical effectiveness remains uncertain.","Therefore, there is a persistent need to develop and evaluate alternative variational quantum algorithms.","This study presents an investigation of the Block ENcoding Quantum Optimizer (BENQO), a hybrid quantum solver that uses block encoding to represent the cost function.","BENQO is designed to be universally applicable across discrete optimization problems.","Beyond Maximum Cut, we evaluate BENQO's performance in the context of the Traveling Salesperson Problem, which is of greater practical relevance.","Our findings confirm that BENQO performs significantly better than QAOA and competes with VQE across a variety of performance metrics.","We conclude that BENQO is a promising novel hybrid quantum-classical algorithm that should be further investigated and optimized to realize its full potential."],"url":"http://arxiv.org/abs/2404.14054v1","category":"quant-ph"}
{"created":"2024-04-22 10:07:04","title":"Pressure gain combustion for gas turbines: Analysis of a fully coupled engine model","abstract":"The ``Shockless Explosion Combustion\" (SEC) concept for gas turbine combustors, introduced in 2014, approximates constant volume combustion (CVC) by harnessing acoustic confinement of autoigniting gas packets. The resulting pressure waves simultaneously transmit combustion energy to a turbine plenum and facilitate the combustor's recharging against an average pressure gain. Challenges in actualizing an SEC-driven gas turbine include i) the creation of charge stratifications for nearly homogeneous autoignition, ii) protecting the turbo components from combustion-induced pressure fluctuations, iii) providing evidence that efficiency gains comparable to those of CVC over deflagrative combustion can be realized, and iv) designing an effective one-way intake valve. This work addresses challenges i)-iii) utilizing computational engine models incorporating a quasi-one-dimensional combustor, zero- and two-dimensional compressor and turbine plena, and quasi-stationary turbo components. Two SEC operational modes are identified which fire at roughly one and two times the combustors' acoustic frequencies. Results for SEC-driven gas turbines with compressor pressure ratios of 6:1 and 20:1 reveal 1.5-fold mean pressure gains across the combustors. Assuming ideally efficient compressors and turbines, efficiency gains over engines with deflagration-based combustors of 30% and 18% are realized, respectively. With absolute values of 52% and 66%, the obtained efficiencies are close to the theoretical Humphrey cycle efficiencies of 54% and 65% for the mentioned pre-compression ratios. Detailed thermodynamic cycle analyses for individual gas parcels suggest that there is room for further efficiency gains through optimized plenum and combustor designs.","sentences":["The ``Shockless Explosion Combustion\" (SEC) concept for gas turbine combustors, introduced in 2014, approximates constant volume combustion (CVC) by harnessing acoustic confinement of autoigniting gas packets.","The resulting pressure waves simultaneously transmit combustion energy to a turbine plenum and facilitate the combustor's recharging against an average pressure gain.","Challenges in actualizing an SEC-driven gas turbine include i) the creation of charge stratifications for nearly homogeneous autoignition, ii) protecting the turbo components from combustion-induced pressure fluctuations, iii) providing evidence that efficiency gains comparable to those of CVC over deflagrative combustion can be realized, and iv) designing an effective one-way intake valve.","This work addresses challenges i)-iii) utilizing computational engine models incorporating a quasi-one-dimensional combustor, zero- and two-dimensional compressor and turbine plena, and quasi-stationary turbo components.","Two SEC operational modes are identified which fire at roughly one and two times the combustors' acoustic frequencies.","Results for SEC-driven gas turbines with compressor pressure ratios of 6:1 and 20:1 reveal 1.5-fold mean pressure gains across the combustors.","Assuming ideally efficient compressors and turbines, efficiency gains over engines with deflagration-based combustors of 30% and 18% are realized, respectively.","With absolute values of 52% and 66%, the obtained efficiencies are close to the theoretical Humphrey cycle efficiencies of 54% and 65% for the mentioned pre-compression ratios.","Detailed thermodynamic cycle analyses for individual gas parcels suggest that there is room for further efficiency gains through optimized plenum and combustor designs."],"url":"http://arxiv.org/abs/2404.14053v1","category":"physics.flu-dyn"}
{"created":"2024-04-22 10:03:24","title":"Optimization-based Heuristic for Vehicle Dynamic Coordination in Mixed Traffic Intersections","abstract":"In this paper, we address a coordination problem for connected and autonomous vehicles (CAVs) in mixed traffic settings with human-driven vehicles (HDVs). The main objective is to have a safe and optimal crossing order for vehicles approaching unsignalized intersections. This problem results in a mixed-integer quadratic programming (MIQP) formulation which is unsuitable for real-time applications. Therefore, we propose a computationally tractable optimization-based heuristic that monitors platoons of CAVs and HDVs to evaluate whether alternative crossing orders can perform better. It first checks the future constraint violation that consistently occurs between pairs of platoons to determine a potential swap. Next, the costs of quadratic programming (QP) formulations associated with the current and alternative orders are compared in a depth-first branching fashion. In simulations, we show that the heuristic can be a hundred times faster than the original and simplified MIQPs and yields solutions that are close to optimal and have better order consistency.","sentences":["In this paper, we address a coordination problem for connected and autonomous vehicles (CAVs) in mixed traffic settings with human-driven vehicles (HDVs).","The main objective is to have a safe and optimal crossing order for vehicles approaching unsignalized intersections.","This problem results in a mixed-integer quadratic programming (MIQP) formulation which is unsuitable for real-time applications.","Therefore, we propose a computationally tractable optimization-based heuristic that monitors platoons of CAVs and HDVs to evaluate whether alternative crossing orders can perform better.","It first checks the future constraint violation that consistently occurs between pairs of platoons to determine a potential swap.","Next, the costs of quadratic programming (QP) formulations associated with the current and alternative orders are compared in a depth-first branching fashion.","In simulations, we show that the heuristic can be a hundred times faster than the original and simplified MIQPs and yields solutions that are close to optimal and have better order consistency."],"url":"http://arxiv.org/abs/2404.14048v1","category":"eess.SY"}
{"created":"2024-04-22 09:51:43","title":"Optimal Structure of Receive Beamforming for Over-the-Air Computation","abstract":"We investigate fast data aggregation via over-the-air computation (AirComp) over wireless networks. In this scenario, an access point (AP) with multiple antennas aims to recover the arithmetic mean of sensory data from multiple wireless devices. To minimize estimation distortion, we formulate a mean-squared-error (MSE) minimization problem that considers joint optimization of transmit scalars at wireless devices, denoising factor, and receive beamforming vector at the AP. We derive closed-form expressions for the transmit scalars and denoising factor, resulting in a non-convex quadratic constrained quadratic programming (QCQP) problem concerning the receive beamforming vector. To tackle the computational complexity of the beamforming design, particularly relevant in massive multiple-input multiple-output (MIMO) AirComp systems, we explore the optimal structure of receive beamforming using successive convex approximation (SCA) and Lagrange duality. By leveraging the proposed optimal beamforming structure, we develop two efficient algorithms based on SCA and semi-definite relaxation (SDR). These algorithms enable fast wireless aggregation with low computational complexity and yield almost identical mean square error (MSE) performance compared to baseline algorithms. Simulation results validate the effectiveness of our proposed methods.","sentences":["We investigate fast data aggregation via over-the-air computation (AirComp) over wireless networks.","In this scenario, an access point (AP) with multiple antennas aims to recover the arithmetic mean of sensory data from multiple wireless devices.","To minimize estimation distortion, we formulate a mean-squared-error (MSE) minimization problem that considers joint optimization of transmit scalars at wireless devices, denoising factor, and receive beamforming vector at the AP.","We derive closed-form expressions for the transmit scalars and denoising factor, resulting in a non-convex quadratic constrained quadratic programming (QCQP) problem concerning the receive beamforming vector.","To tackle the computational complexity of the beamforming design, particularly relevant in massive multiple-input multiple-output (MIMO) AirComp systems, we explore the optimal structure of receive beamforming using successive convex approximation (SCA) and Lagrange duality.","By leveraging the proposed optimal beamforming structure, we develop two efficient algorithms based on SCA and semi-definite relaxation (SDR).","These algorithms enable fast wireless aggregation with low computational complexity and yield almost identical mean square error (MSE) performance compared to baseline algorithms.","Simulation results validate the effectiveness of our proposed methods."],"url":"http://arxiv.org/abs/2404.14036v1","category":"cs.IT"}
{"created":"2024-04-22 09:29:14","title":"Ungeneralizable Examples","abstract":"The training of contemporary deep learning models heavily relies on publicly available data, posing a risk of unauthorized access to online data and raising concerns about data privacy. Current approaches to creating unlearnable data involve incorporating small, specially designed noises, but these methods strictly limit data usability, overlooking its potential usage in authorized scenarios. In this paper, we extend the concept of unlearnable data to conditional data learnability and introduce \\textbf{U}n\\textbf{G}eneralizable \\textbf{E}xamples (UGEs). UGEs exhibit learnability for authorized users while maintaining unlearnability for potential hackers. The protector defines the authorized network and optimizes UGEs to match the gradients of the original data and its ungeneralizable version, ensuring learnability. To prevent unauthorized learning, UGEs are trained by maximizing a designated distance loss in a common feature space. Additionally, to further safeguard the authorized side from potential attacks, we introduce additional undistillation optimization. Experimental results on multiple datasets and various networks demonstrate that the proposed UGEs framework preserves data usability while reducing training performance on hacker networks, even under different types of attacks.","sentences":["The training of contemporary deep learning models heavily relies on publicly available data, posing a risk of unauthorized access to online data and raising concerns about data privacy.","Current approaches to creating unlearnable data involve incorporating small, specially designed noises, but these methods strictly limit data usability, overlooking its potential usage in authorized scenarios.","In this paper, we extend the concept of unlearnable data to conditional data learnability and introduce \\textbf{U}n\\textbf{G}eneralizable \\textbf{E}xamples (UGEs).","UGEs exhibit learnability for authorized users while maintaining unlearnability for potential hackers.","The protector defines the authorized network and optimizes UGEs to match the gradients of the original data and its ungeneralizable version, ensuring learnability.","To prevent unauthorized learning, UGEs are trained by maximizing a designated distance loss in a common feature space.","Additionally, to further safeguard the authorized side from potential attacks, we introduce additional undistillation optimization.","Experimental results on multiple datasets and various networks demonstrate that the proposed UGEs framework preserves data usability while reducing training performance on hacker networks, even under different types of attacks."],"url":"http://arxiv.org/abs/2404.14016v1","category":"cs.LG"}
{"created":"2024-04-22 09:18:33","title":"A Stochastic Rounding-Enabled Low-Precision Floating-Point MAC for DNN Training","abstract":"Training Deep Neural Networks (DNNs) can be computationally demanding, particularly when dealing with large models. Recent work has aimed to mitigate this computational challenge by introducing 8-bit floating-point (FP8) formats for multiplication. However, accumulations are still done in either half (16-bit) or single (32-bit) precision arithmetic. In this paper, we investigate lowering accumulator word length while maintaining the same model accuracy. We present a multiply-accumulate (MAC) unit with FP8 multiplier inputs and FP12 accumulations, which leverages an optimized stochastic rounding (SR) implementation to mitigate swamping errors that commonly arise during low precision accumulations. We investigate the hardware implications and accuracy impact associated with varying the number of random bits used for rounding operations. We additionally attempt to reduce MAC area and power by proposing a new scheme to support SR in floating-point MAC and by removing support for subnormal values. Our optimized eager SR unit significantly reduces delay and area when compared to a classic lazy SR design. Moreover, when compared to MACs utilizing single-or half-precision adders, our design showcases notable savings in all metrics. Furthermore, our approach consistently maintains near baseline accuracy across a diverse range of computer vision tasks, making it a promising alternative for low-precision DNN training.","sentences":["Training Deep Neural Networks (DNNs) can be computationally demanding, particularly when dealing with large models.","Recent work has aimed to mitigate this computational challenge by introducing 8-bit floating-point (FP8) formats for multiplication.","However, accumulations are still done in either half (16-bit) or single (32-bit) precision arithmetic.","In this paper, we investigate lowering accumulator word length while maintaining the same model accuracy.","We present a multiply-accumulate (MAC) unit with FP8 multiplier inputs and FP12 accumulations, which leverages an optimized stochastic rounding (SR) implementation to mitigate swamping errors that commonly arise during low precision accumulations.","We investigate the hardware implications and accuracy impact associated with varying the number of random bits used for rounding operations.","We additionally attempt to reduce MAC area and power by proposing a new scheme to support SR in floating-point MAC and by removing support for subnormal values.","Our optimized eager SR unit significantly reduces delay and area when compared to a classic lazy SR design.","Moreover, when compared to MACs utilizing single-or half-precision adders, our design showcases notable savings in all metrics.","Furthermore, our approach consistently maintains near baseline accuracy across a diverse range of computer vision tasks, making it a promising alternative for low-precision DNN training."],"url":"http://arxiv.org/abs/2404.14010v1","category":"cs.AR"}
{"created":"2024-04-22 09:17:18","title":"Carleman estimates for higher order partial differential operators and its applications","abstract":"In this paper, we obtain a Carleman estimate for the higher order partial differential operator. In the process of establishing this estimate, we developed a new method, which is called the back-propagation method (the BPM, for short). This method can also be used to build up Carleman estimates for some other partial differential operators, and might provide assistance with corresponding numerical analyses. As an application of the above-mentioned Carleman estimate, we proved the conditional stability of a Cauchy problem for a time fractional diffusion equation.","sentences":["In this paper, we obtain a Carleman estimate for the higher order partial differential operator.","In the process of establishing this estimate, we developed a new method, which is called the back-propagation method (the BPM, for short).","This method can also be used to build up Carleman estimates for some other partial differential operators, and might provide assistance with corresponding numerical analyses.","As an application of the above-mentioned Carleman estimate, we proved the conditional stability of a Cauchy problem for a time fractional diffusion equation."],"url":"http://arxiv.org/abs/2404.14008v1","category":"math.AP"}
{"created":"2024-04-22 08:58:30","title":"5GC$^2$ache: Improving 5G UPF Performance via Cache Optimization","abstract":"Last Level Cache (LLC) is a precious and critical resource that impacts the performance of applications running on top of CPUs. In this paper, we reveal the significant impact of LLC on the performance of the 5G user plane function (UPF) when running a cloudified 5G core on general-purposed servers. With extensive measurements showing that the throughput can degrade by over 50\\% when the precious LLC resource of UPF is not properly allocated, we identify three categories of performance degradation caused by incorrect LLC usage: DMA leakage problem, hot/cold mbuf problem and cache contention. To address these problems, we introduce the design and implementation of 5GC$^2$ache that monitors the LLC status as well as the throughput performance and dynamically adjusts key parameters of the LLC resource allocation. Our experiments show that 5GC$^2$ache enables a commercial 5G core to increase its throughput to 76.41Gbps, 39.41\\% higher than the original performance and 29.55\\% higher than the state-of-the-art.","sentences":["Last Level Cache (LLC) is a precious and critical resource that impacts the performance of applications running on top of CPUs.","In this paper, we reveal the significant impact of LLC on the performance of the 5G user plane function (UPF) when running a cloudified 5G core on general-purposed servers.","With extensive measurements showing that the throughput can degrade by over 50\\% when the precious LLC resource of UPF is not properly allocated, we identify three categories of performance degradation caused by incorrect LLC usage: DMA leakage problem, hot/cold mbuf problem and cache contention.","To address these problems, we introduce the design and implementation of 5GC$^2$ache that monitors the LLC status as well as the throughput performance and dynamically adjusts key parameters of the LLC resource allocation.","Our experiments show that 5GC$^2$ache enables a commercial 5G core to increase its throughput to 76.41Gbps, 39.41\\% higher than the original performance and 29.55\\% higher than the state-of-the-art."],"url":"http://arxiv.org/abs/2404.13991v1","category":"cs.NI"}
{"created":"2024-04-22 08:27:14","title":"HamilToniQ: An Open-Source Benchmark Toolkit for Quantum Computers","abstract":"In this paper, we introduce HamilToniQ, an open-source, and application-oriented benchmarking toolkit for the comprehensive evaluation of Quantum Processing Units (QPUs). Designed to navigate the complexities of quantum computations, HamilToniQ incorporates a methodological framework assessing QPU types, topologies, and multi-QPU systems. The toolkit facilitates the evaluation of QPUs' performance through multiple steps including quantum circuit compilation and quantum error mitigation (QEM), integrating strategies that are unique to each stage. HamilToniQ's standardized score, H-Score, quantifies the fidelity and reliability of QPUs, providing a multidimensional perspective of QPU performance. With a focus on the Quantum Approximate Optimization Algorithm (QAOA), the toolkit enables direct, comparable analysis of QPUs, enhancing transparency and equity in benchmarking. Demonstrated in this paper, HamilToniQ has been validated on various IBM QPUs, affirming its effectiveness and robustness. Overall, HamilToniQ significantly contributes to the advancement of the quantum computing field by offering precise and equitable benchmarking metrics.","sentences":["In this paper, we introduce HamilToniQ, an open-source, and application-oriented benchmarking toolkit for the comprehensive evaluation of Quantum Processing Units (QPUs).","Designed to navigate the complexities of quantum computations, HamilToniQ incorporates a methodological framework assessing QPU types, topologies, and multi-QPU systems.","The toolkit facilitates the evaluation of QPUs' performance through multiple steps including quantum circuit compilation and quantum error mitigation (QEM), integrating strategies that are unique to each stage.","HamilToniQ's standardized score, H-Score, quantifies the fidelity and reliability of QPUs, providing a multidimensional perspective of QPU performance.","With a focus on the Quantum Approximate Optimization Algorithm (QAOA), the toolkit enables direct, comparable analysis of QPUs, enhancing transparency and equity in benchmarking.","Demonstrated in this paper, HamilToniQ has been validated on various IBM QPUs, affirming its effectiveness and robustness.","Overall, HamilToniQ significantly contributes to the advancement of the quantum computing field by offering precise and equitable benchmarking metrics."],"url":"http://arxiv.org/abs/2404.13971v1","category":"quant-ph"}
