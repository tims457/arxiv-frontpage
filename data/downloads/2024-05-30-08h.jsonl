{"created":"2024-05-29 17:59:58","title":"X-VILA: Cross-Modality Alignment for Large Language Model","abstract":"We introduce X-VILA, an omni-modality model designed to extend the capabilities of large language models (LLMs) by incorporating image, video, and audio modalities. By aligning modality-specific encoders with LLM inputs and diffusion decoders with LLM outputs, X-VILA achieves cross-modality understanding, reasoning, and generation. To facilitate this cross-modality alignment, we curate an effective interleaved any-to-any modality instruction-following dataset. Furthermore, we identify a significant problem with the current cross-modality alignment method, which results in visual information loss. To address the issue, we propose a visual alignment mechanism with a visual embedding highway module. We then introduce a resource-efficient recipe for training X-VILA, that exhibits proficiency in any-to-any modality conversation, surpassing previous approaches by large margins. X-VILA also showcases emergent properties across modalities even in the absence of similar training data. The project will be made open-source.","sentences":["We introduce X-VILA, an omni-modality model designed to extend the capabilities of large language models (LLMs) by incorporating image, video, and audio modalities.","By aligning modality-specific encoders with LLM inputs and diffusion decoders with LLM outputs, X-VILA achieves cross-modality understanding, reasoning, and generation.","To facilitate this cross-modality alignment, we curate an effective interleaved any-to-any modality instruction-following dataset.","Furthermore, we identify a significant problem with the current cross-modality alignment method, which results in visual information loss.","To address the issue, we propose a visual alignment mechanism with a visual embedding highway module.","We then introduce a resource-efficient recipe for training X-VILA, that exhibits proficiency in any-to-any modality conversation, surpassing previous approaches by large margins.","X-VILA also showcases emergent properties across modalities even in the absence of similar training data.","The project will be made open-source."],"url":"http://arxiv.org/abs/2405.19335v1","category":"cs.CV"}
{"created":"2024-05-29 17:59:20","title":"LLMs Meet Multimodal Generation and Editing: A Survey","abstract":"With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on understanding. This survey elaborates on multimodal generation across different domains, including image, video, 3D, and audio, where we highlight the notable advancements with milestone works in these fields. Specifically, we exhaustively investigate the key technical components behind methods and multimodal datasets utilized in these studies. Moreover, we dig into tool-augmented multimodal agents that can use existing generative models for human-computer interaction. Lastly, we also comprehensively discuss the advancement in AI safety and investigate emerging applications as well as future prospects. Our work provides a systematic and insightful overview of multimodal generation, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models. A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation","sentences":["With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning.","Previous surveys of multimodal large language models (MLLMs) mainly focus on understanding.","This survey elaborates on multimodal generation across different domains, including image, video, 3D, and audio, where we highlight the notable advancements with milestone works in these fields.","Specifically, we exhaustively investigate the key technical components behind methods and multimodal datasets utilized in these studies.","Moreover, we dig into tool-augmented multimodal agents that can use existing generative models for human-computer interaction.","Lastly, we also comprehensively discuss the advancement in AI safety and investigate emerging applications as well as future prospects.","Our work provides a systematic and insightful overview of multimodal generation, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models.","A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation"],"url":"http://arxiv.org/abs/2405.19334v1","category":"cs.AI"}
{"created":"2024-05-29 17:59:10","title":"Multi-Modal Generative Embedding Model","abstract":"Most multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding. To explore the minimalism of multi-modal paradigms, we attempt to achieve only one model per modality in this work. We propose a Multi-Modal Generative Embedding Model (MM-GEM), whereby the generative and embedding objectives are encapsulated in one Large Language Model. We also propose a PoolAggregator to boost efficiency and enable the ability of fine-grained embedding and generation. A surprising finding is that these two objectives do not significantly conflict with each other. For example, MM-GEM instantiated from ViT-Large and TinyLlama shows competitive performance on benchmarks for multimodal embedding models such as cross-modal retrieval and zero-shot classification, while has good ability of image captioning. Additionally, MM-GEM can seamlessly execute region-level image caption generation and retrieval tasks. Besides, the advanced text model in MM-GEM brings over 5% improvement in Recall@1 for long text and image retrieval.","sentences":["Most multi-modal tasks can be formulated into problems of either generation or embedding.","Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding.","To explore the minimalism of multi-modal paradigms, we attempt to achieve only one model per modality in this work.","We propose a Multi-Modal Generative Embedding Model (MM-GEM), whereby the generative and embedding objectives are encapsulated in one Large Language Model.","We also propose a PoolAggregator to boost efficiency and enable the ability of fine-grained embedding and generation.","A surprising finding is that these two objectives do not significantly conflict with each other.","For example, MM-GEM instantiated from ViT-Large and TinyLlama shows competitive performance on benchmarks for multimodal embedding models such as cross-modal retrieval and zero-shot classification, while has good ability of image captioning.","Additionally, MM-GEM can seamlessly execute region-level image caption generation and retrieval tasks.","Besides, the advanced text model in MM-GEM brings over 5% improvement in Recall@1 for long text and image retrieval."],"url":"http://arxiv.org/abs/2405.19333v1","category":"cs.CV"}
{"created":"2024-05-29 17:59:07","title":"Self-Exploring Language Models: Active Preference Elicitation for Online Alignment","abstract":"Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed dataset, online feedback collection from humans or AI on model generations typically leads to more capable reward models and better-aligned LLMs through an iterative process. However, achieving a globally accurate reward model requires systematic exploration to generate diverse responses that span the vast space of natural language. Random sampling from standard reward-maximizing LLMs alone is insufficient to fulfill this requirement. To address this issue, we propose a bilevel objective optimistically biased towards potentially high-reward responses to actively explore out-of-distribution regions. By solving the inner-level problem with the reparameterized reward function, the resulting algorithm, named Self-Exploring Language Models (SELM), eliminates the need for a separate RM and iteratively updates the LLM with a straightforward objective. Compared to Direct Preference Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen extrapolations and enhances exploration efficiency. Our experimental results demonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instruct models, SELM significantly boosts the performance on instruction-following benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard academic benchmarks in different settings. Our code and models are available at https://github.com/shenao-zhang/SELM.","sentences":["Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions.","Unlike offline alignment with a fixed dataset, online feedback collection from humans or AI on model generations typically leads to more capable reward models and better-aligned LLMs through an iterative process.","However, achieving a globally accurate reward model requires systematic exploration to generate diverse responses that span the vast space of natural language.","Random sampling from standard reward-maximizing LLMs alone is insufficient to fulfill this requirement.","To address this issue, we propose a bilevel objective optimistically biased towards potentially high-reward responses to actively explore out-of-distribution regions.","By solving the inner-level problem with the reparameterized reward function, the resulting algorithm, named Self-Exploring Language Models (SELM), eliminates the need for a separate RM and iteratively updates the LLM with a straightforward objective.","Compared to Direct Preference Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen extrapolations and enhances exploration efficiency.","Our experimental results demonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instruct models, SELM significantly boosts the performance on instruction-following benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard academic benchmarks in different settings.","Our code and models are available at https://github.com/shenao-zhang/SELM."],"url":"http://arxiv.org/abs/2405.19332v1","category":"cs.LG"}
{"created":"2024-05-29 17:58:09","title":"NPGA: Neural Parametric Gaussian Avatars","abstract":"The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our method around 3D Gaussian Splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds. In contrast to previous work, we condition our avatars' dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering. All remaining fine-scale, expression-dependent details are learned from the multi-view videos. To increase the representational capacity of our avatars, we augment the canonical Gaussian point cloud using per-primitive latent features which govern its dynamic behavior. To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos.","sentences":["The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives.","Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance.","In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings.","We build our method around 3D Gaussian Splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds.","In contrast to previous work, we condition our avatars' dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs.","To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering.","All remaining fine-scale, expression-dependent details are learned from the multi-view videos.","To increase the representational capacity of our avatars, we augment the canonical Gaussian point cloud using per-primitive latent features which govern its dynamic behavior.","To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics.","We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR.","Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos."],"url":"http://arxiv.org/abs/2405.19331v1","category":"cs.CV"}
{"created":"2024-05-29 17:57:30","title":"Normative Modules: A Generative Agent Architecture for Learning Norms that Supports Multi-Agent Cooperation","abstract":"Generative agents, which implement behaviors using a large language model (LLM) to interpret and evaluate an environment, has demonstrated the capacity to solve complex tasks across many social and technological domains. However, when these agents interact with other agents and humans in presence of social structures such as existing norms, fostering cooperation between them is a fundamental challenge. In this paper, we develop the framework of a 'Normative Module': an architecture designed to enhance cooperation by enabling agents to recognize and adapt to the normative infrastructure of a given environment. We focus on the equilibrium selection aspect of the cooperation problem and inform our agent design based on the existence of classification institutions that implement correlated equilibrium to provide effective resolution of the equilibrium selection problem. Specifically, the normative module enables agents to learn through peer interactions which of multiple candidate institutions in the environment, does a group treat as authoritative. By enabling normative competence in this sense, agents gain ability to coordinate their sanctioning behaviour; coordinated sanctioning behaviour in turn shapes primary behaviour within a social environment, leading to higher average welfare. We design a new environment that supports institutions and evaluate the proposed framework based on two key criteria derived from agent interactions with peers and institutions: (i) the agent's ability to disregard non-authoritative institutions and (ii) the agent's ability to identify authoritative institutions among several options. We show that these capabilities allow the agent to achieve more stable cooperative outcomes compared to baseline agents without the normative module, paving the way for research in a new avenue of designing environments and agents that account for normative infrastructure.","sentences":["Generative agents, which implement behaviors using a large language model (LLM) to interpret and evaluate an environment, has demonstrated the capacity to solve complex tasks across many social and technological domains.","However, when these agents interact with other agents and humans in presence of social structures such as existing norms, fostering cooperation between them is a fundamental challenge.","In this paper, we develop the framework of a 'Normative Module': an architecture designed to enhance cooperation by enabling agents to recognize and adapt to the normative infrastructure of a given environment.","We focus on the equilibrium selection aspect of the cooperation problem and inform our agent design based on the existence of classification institutions that implement correlated equilibrium to provide effective resolution of the equilibrium selection problem.","Specifically, the normative module enables agents to learn through peer interactions which of multiple candidate institutions in the environment, does a group treat as authoritative.","By enabling normative competence in this sense, agents gain ability to coordinate their sanctioning behaviour; coordinated sanctioning behaviour in turn shapes primary behaviour within a social environment, leading to higher average welfare.","We design a new environment that supports institutions and evaluate the proposed framework based on two key criteria derived from agent interactions with peers and institutions: (i) the agent's ability to disregard non-authoritative institutions and (ii) the agent's ability to identify authoritative institutions among several options.","We show that these capabilities allow the agent to achieve more stable cooperative outcomes compared to baseline agents without the normative module, paving the way for research in a new avenue of designing environments and agents that account for normative infrastructure."],"url":"http://arxiv.org/abs/2405.19328v1","category":"cs.MA"}
{"created":"2024-05-29 17:57:16","title":"MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series","abstract":"Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.","sentences":["Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks.","However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details.","Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs.","However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.)","being undisclosed.","To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided.","These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks.","However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes.","To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens.","Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs.","Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided.","Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs."],"url":"http://arxiv.org/abs/2405.19327v1","category":"cs.CL"}
{"created":"2024-05-29 17:56:07","title":"Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models","abstract":"In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentation for parts searching and localization for objects, which is a new paradigm to 3D segmentation that transcends limitations for previous category-specific 3D semantic segmentation, 3D instance segmentation, and open-vocabulary 3D segmentation. We design a simple baseline method, Reasoning3D, with the capability to understand and execute complex commands for (fine-grained) segmenting specific parts for 3D meshes with contextual awareness and reasoned answers for interactive segmentation. Specifically, Reasoning3D leverages an off-the-shelf pre-trained 2D segmentation network, powered by Large Language Models (LLMs), to interpret user input queries in a zero-shot manner. Previous research have shown that extensive pre-training endows foundation models with prior world knowledge, enabling them to comprehend complex commands, a capability we can harness to \"segment anything\" in 3D with limited 3D datasets (source efficient). Experimentation reveals that our approach is generalizable and can effectively localize and highlight parts of 3D objects (in 3D mesh) based on implicit textual queries, including these articulated 3d objects and real-world scanned data. Our method can also generate natural language explanations corresponding to these 3D models and the decomposition. Moreover, our training-free approach allows rapid deployment and serves as a viable universal baseline for future research of part-level 3d (semantic) object understanding in various fields including robotics, object manipulation, part assembly, autonomous driving applications, augment reality and virtual reality (AR/VR), and medical applications. The code, the model weight, the deployment guide, and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/","sentences":["In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentation for parts searching and localization for objects, which is a new paradigm to 3D segmentation that transcends limitations for previous category-specific 3D semantic segmentation, 3D instance segmentation, and open-vocabulary 3D segmentation.","We design a simple baseline method, Reasoning3D, with the capability to understand and execute complex commands for (fine-grained) segmenting specific parts for 3D meshes with contextual awareness and reasoned answers for interactive segmentation.","Specifically, Reasoning3D leverages an off-the-shelf pre-trained 2D segmentation network, powered by Large Language Models (LLMs), to interpret user input queries in a zero-shot manner.","Previous research have shown that extensive pre-training endows foundation models with prior world knowledge, enabling them to comprehend complex commands, a capability we can harness to \"segment anything\" in 3D with limited 3D datasets (source efficient).","Experimentation reveals that our approach is generalizable and can effectively localize and highlight parts of 3D objects (in 3D mesh) based on implicit textual queries, including these articulated 3d objects and real-world scanned data.","Our method can also generate natural language explanations corresponding to these 3D models and the decomposition.","Moreover, our training-free approach allows rapid deployment and serves as a viable universal baseline for future research of part-level 3d (semantic) object understanding in various fields including robotics, object manipulation, part assembly, autonomous driving applications, augment reality and virtual reality (AR/VR), and medical applications.","The code, the model weight, the deployment guide, and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/"],"url":"http://arxiv.org/abs/2405.19326v1","category":"cs.CV"}
{"created":"2024-05-29 17:55:03","title":"Nearest Neighbor Speculative Decoding for LLM Generation and Attribution","abstract":"Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts. In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources. NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus. It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token. NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B.","sentences":["Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations.","Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store.","However, these models often exhibit slow inference speeds and produce non-fluent texts.","In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources.","NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus.","It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token.","NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation.","In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B."],"url":"http://arxiv.org/abs/2405.19325v1","category":"cs.CL"}
{"created":"2024-05-29 17:54:22","title":"Are Large Language Models Chameleons?","abstract":"Do large language models (LLMs) have their own worldviews and personality tendencies? Simulations in which an LLM was asked to answer subjective questions were conducted more than 1 million times. Comparison of the responses from different LLMs with real data from the European Social Survey (ESS) suggests that the effect of prompts on bias and variability is fundamental, highlighting major cultural, age, and gender biases. Methods for measuring the difference between LLMs and survey data are discussed, such as calculating weighted means and a new proposed measure inspired by Jaccard similarity. We conclude that it is important to analyze the robustness and variability of prompts before using LLMs to model individual decisions or collective behavior, as their imitation abilities are approximate at best.","sentences":["Do large language models (LLMs) have their own worldviews and personality tendencies?","Simulations in which an LLM was asked to answer subjective questions were conducted more than 1 million times.","Comparison of the responses from different LLMs with real data from the European Social Survey (ESS) suggests that the effect of prompts on bias and variability is fundamental, highlighting major cultural, age, and gender biases.","Methods for measuring the difference between LLMs and survey data are discussed, such as calculating weighted means and a new proposed measure inspired by Jaccard similarity.","We conclude that it is important to analyze the robustness and variability of prompts before using LLMs to model individual decisions or collective behavior, as their imitation abilities are approximate at best."],"url":"http://arxiv.org/abs/2405.19323v1","category":"cs.CL"}
{"created":"2024-05-29 17:52:22","title":"DGD: Dynamic 3D Gaussians Distillation","abstract":"We tackle the task of learning dynamic 3D semantic radiance fields given a single monocular video as input. Our learned semantic radiance field captures per-point semantics as well as color and geometric properties for a dynamic 3D scene, enabling the generation of novel views and their corresponding semantics. This enables the segmentation and tracking of a diverse set of 3D semantic entities, specified using a simple and intuitive interface that includes a user click or a text prompt. To this end, we present DGD, a unified 3D representation for both the appearance and semantics of a dynamic 3D scene, building upon the recently proposed dynamic 3D Gaussians representation. Our representation is optimized over time with both color and semantic information. Key to our method is the joint optimization of the appearance and semantic attributes, which jointly affect the geometric properties of the scene. We evaluate our approach in its ability to enable dense semantic 3D object tracking and demonstrate high-quality results that are fast to render, for a diverse set of scenes. Our project webpage is available on https://isaaclabe.github.io/DGD-Website/","sentences":["We tackle the task of learning dynamic 3D semantic radiance fields given a single monocular video as input.","Our learned semantic radiance field captures per-point semantics as well as color and geometric properties for a dynamic 3D scene, enabling the generation of novel views and their corresponding semantics.","This enables the segmentation and tracking of a diverse set of 3D semantic entities, specified using a simple and intuitive interface that includes a user click or a text prompt.","To this end, we present DGD, a unified 3D representation for both the appearance and semantics of a dynamic 3D scene, building upon the recently proposed dynamic 3D Gaussians representation.","Our representation is optimized over time with both color and semantic information.","Key to our method is the joint optimization of the appearance and semantic attributes, which jointly affect the geometric properties of the scene.","We evaluate our approach in its ability to enable dense semantic 3D object tracking and demonstrate high-quality results that are fast to render, for a diverse set of scenes.","Our project webpage is available on https://isaaclabe.github.io/DGD-Website/"],"url":"http://arxiv.org/abs/2405.19321v1","category":"cs.CV"}
{"created":"2024-05-29 17:51:42","title":"Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF","abstract":"Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.   In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\\textit{sign}$ to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO.","sentences":["Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference.","Depending on the availability of preference data, both online and offline RLHF are active areas of investigation.","A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected.","While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.   ","In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\\textit{sign}$ to indicate whether the optimism or pessimism is chosen.","VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization.","Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts.","Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO."],"url":"http://arxiv.org/abs/2405.19320v1","category":"cs.LG"}
{"created":"2024-05-29 17:50:50","title":"ACE: A general-purpose non-Markovian open quantum systems simulation toolkit based on process tensors","abstract":"We describe a general-purpose computational toolkit for simulating open quantum systems, which provides numerically exact solutions for composites of zero-dimensional quantum systems that may be strongly coupled to multiple, quite general non-Markovian environments. It is based on process tensor matrix product operators (PT-MPOs), which efficiently encapsulate environment influences. The code features implementations of several PT-MPO algorithms, in particular, Automated Compression of Environments (ACE) for general environments comprised of independent modes as well as schemes for generalized spin boson models. The latter includes a divide-and-conquer scheme for periodic PT-MPOs, which enable million time step simulations for realistic models. PT-MPOs can be precalculated and reused for efficiently probing different time-dependent system Hamiltonians. They can also be stacked together and combined to provide numerically complete solutions of small networks of open quantum systems. The code is written in C++ and is fully controllable by configuration files, for which we have developed a versatile and compact human-readable format.","sentences":["We describe a general-purpose computational toolkit for simulating open quantum systems, which provides numerically exact solutions for composites of zero-dimensional quantum systems that may be strongly coupled to multiple, quite general non-Markovian environments.","It is based on process tensor matrix product operators (PT-MPOs), which efficiently encapsulate environment influences.","The code features implementations of several PT-MPO algorithms, in particular, Automated Compression of Environments (ACE) for general environments comprised of independent modes as well as schemes for generalized spin boson models.","The latter includes a divide-and-conquer scheme for periodic PT-MPOs, which enable million time step simulations for realistic models.","PT-MPOs can be precalculated and reused for efficiently probing different time-dependent system Hamiltonians.","They can also be stacked together and combined to provide numerically complete solutions of small networks of open quantum systems.","The code is written in C++ and is fully controllable by configuration files, for which we have developed a versatile and compact human-readable format."],"url":"http://arxiv.org/abs/2405.19319v1","category":"quant-ph"}
{"created":"2024-05-29 17:44:22","title":"Cosmology Based on Finsler and Finsler-like Metric Structure of Gravitational Field","abstract":"In this article, we review some aspects of gravitational field and cosmology based on Finsler and Finsler-like generalized metric structures. The geometrical framework of these spaces allows further investigation of locally-anisotropic phenomena related to the gravitational field and cosmological considerations, e.g the extracted geodesics, deflection of light, Finsler-Einstein gravitational field equations , the Friedmann equations and the Raychaudhuri equations include extra anisotropic terms that in the Riemannian framework of General Relativity (GR) are not interpreted. This approach gives us the opportunity to extend the research with more degrees of freedom on the tangent bundle of a spacetime manifold. In the above mentioned generalizations omitting the extra anisotropic terms we recover the framework of GR. In addition, we study the gravitational Magnus effect in a generalized metric framework.   Based on this approach, we describe further properties of Finsler-Randers (FR) and Schwarzschild Finsler Randers (SFR) cosmological models which are useful for the description and evolution of the universe.","sentences":["In this article, we review some aspects of gravitational field and cosmology based on Finsler and Finsler-like generalized metric structures.","The geometrical framework of these spaces allows further investigation of locally-anisotropic phenomena related to the gravitational field and cosmological considerations, e.g the extracted geodesics, deflection of light, Finsler-Einstein gravitational field equations , the Friedmann equations and the Raychaudhuri equations include extra anisotropic terms that in the Riemannian framework of General Relativity (GR) are not interpreted.","This approach gives us the opportunity to extend the research with more degrees of freedom on the tangent bundle of a spacetime manifold.","In the above mentioned generalizations omitting the extra anisotropic terms we recover the framework of GR.","In addition, we study the gravitational Magnus effect in a generalized metric framework.   ","Based on this approach, we describe further properties of Finsler-Randers (FR) and Schwarzschild Finsler Randers (SFR) cosmological models which are useful for the description and evolution of the universe."],"url":"http://arxiv.org/abs/2405.19318v1","category":"gr-qc"}
{"created":"2024-05-29 17:43:13","title":"Adaptive Generalized Neyman Allocation: Local Asymptotic Minimax Optimal Best Arm Identification","abstract":"This study investigates a local asymptotic minimax optimal strategy for fixed-budget best arm identification (BAI). We propose the Adaptive Generalized Neyman Allocation (AGNA) strategy and show that its worst-case upper bound of the probability of misidentifying the best arm aligns with the worst-case lower bound under the small-gap regime, where the gap between the expected outcomes of the best and suboptimal arms is small. Our strategy corresponds to a generalization of the Neyman allocation for two-armed bandits (Neyman, 1934; Kaufmann et al., 2016) and a refinement of existing strategies such as the ones proposed by Glynn & Juneja (2004) and Shin et al. (2018). Compared to Komiyama et al. (2022), which proposes a minimax rate-optimal strategy, our proposed strategy has a tighter upper bound that exactly matches the lower bound, including the constant terms, by restricting the class of distributions to the class of small-gap distributions. Our result contributes to the longstanding open issue about the existence of asymptotically optimal strategies in fixed-budget BAI, by presenting the local asymptotic minimax optimal strategy.","sentences":["This study investigates a local asymptotic minimax optimal strategy for fixed-budget best arm identification (BAI).","We propose the Adaptive Generalized Neyman Allocation (AGNA) strategy and show that its worst-case upper bound of the probability of misidentifying the best arm aligns with the worst-case lower bound under the small-gap regime, where the gap between the expected outcomes of the best and suboptimal arms is small.","Our strategy corresponds to a generalization of the Neyman allocation for two-armed bandits (Neyman, 1934; Kaufmann et al., 2016) and a refinement of existing strategies such as the ones proposed by Glynn & Juneja (2004) and Shin et al.","(2018).","Compared to Komiyama et al. (2022), which proposes a minimax rate-optimal strategy, our proposed strategy has a tighter upper bound that exactly matches the lower bound, including the constant terms, by restricting the class of distributions to the class of small-gap distributions.","Our result contributes to the longstanding open issue about the existence of asymptotically optimal strategies in fixed-budget BAI, by presenting the local asymptotic minimax optimal strategy."],"url":"http://arxiv.org/abs/2405.19317v1","category":"cs.LG"}
{"created":"2024-05-29 17:39:48","title":"Robust Preference Optimization through Reward Model Distillation","abstract":"Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations. Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning. However, typical preference datasets have only a single, or at most a few, annotation per preference pair, which causes DPO to overconfidently assign rewards that trend towards infinite magnitude. This frequently leads to degenerate policies, sometimes causing even the probabilities of the preferred generations to go to zero. In this work, we analyze this phenomenon and propose distillation to get a better proxy for the true preference distribution over generation pairs: we train the LM to produce probabilities that match the distribution induced by a reward model trained on the preference data. Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models that, as a whole, is likely to include at least one reasonable proxy for the preference distribution. Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO.","sentences":["Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations.","Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning.","However, typical preference datasets have only a single, or at most a few, annotation per preference pair, which causes DPO to overconfidently assign rewards that trend towards infinite magnitude.","This frequently leads to degenerate policies, sometimes causing even the probabilities of the preferred generations to go to zero.","In this work, we analyze this phenomenon and propose distillation to get a better proxy for the true preference distribution over generation pairs: we train the LM to produce probabilities that match the distribution induced by a reward model trained on the preference data.","Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models that, as a whole, is likely to include at least one reasonable proxy for the preference distribution.","Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO."],"url":"http://arxiv.org/abs/2405.19316v1","category":"cs.LG"}
{"created":"2024-05-29 17:37:14","title":"Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice","abstract":"The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition. However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models. For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences. Consequently, the origins of these behavioral similarities are not well understood. In this paper, we propose a novel way to enhance the utility of LLMs as cognitive models. This approach involves (i) leveraging computationally equivalent tasks that both an LLM and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for an LLM to exhibit human-like behaviors. We apply this approach to decision-making -- specifically risky and intertemporal choice -- where the key computationally equivalent task is the arithmetic of expected value calculations. We show that an LLM pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models. Pretraining LLMs on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making. Our results also suggest that LLMs used as cognitive models should be carefully investigated via ablation studies of the pretraining data.","sentences":["The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition.","However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models.","For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences.","Consequently, the origins of these behavioral similarities are not well understood.","In this paper, we propose a novel way to enhance the utility of LLMs as cognitive models.","This approach involves (i) leveraging computationally equivalent tasks that both an LLM and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for an LLM to exhibit human-like behaviors.","We apply this approach to decision-making -- specifically risky and intertemporal choice -- where the key computationally equivalent task is the arithmetic of expected value calculations.","We show that an LLM pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models.","Pretraining LLMs on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making.","Our results also suggest that LLMs used as cognitive models should be carefully investigated via ablation studies of the pretraining data."],"url":"http://arxiv.org/abs/2405.19313v1","category":"cs.AI"}
{"created":"2024-05-29 17:35:46","title":"Network Connectivity--Information Freshness Tradeoff in Information Dissemination Over Networks","abstract":"We consider a gossip network consisting of a source generating updates and $n$ nodes connected according to a given graph structure. The source keeps updates of a process, that might be generated or observed, and shares them with the gossiping network. The nodes in the network communicate with their neighbors and disseminate these version updates using a push-style gossip strategy. We use the version age metric to quantify the timeliness of information at the nodes. We first find an upper bound for the average version age for a set of nodes in a general network. Using this, we find the average version age scaling of a node in several network graph structures, such as two-dimensional grids, generalized rings and hyper-cubes. Prior to our work, it was known that when $n$ nodes are connected on a ring the version age scales as $O(n^{\\frac{1}{2}})$, and when they are connected on a fully-connected graph the version age scales as $O(\\log n)$. Ours is the first work to show an age scaling result for a connectivity structure other than the ring and the fully-connected network, which constitute the two extremes of network connectivity. Our work helps fill the gap between these two extremes by analyzing a large variety of graphs with intermediate connectivity, thus providing insight into the relationship between the connectivity structure of the network and the version age, and uncovering a network connectivity--information freshness tradeoff.","sentences":["We consider a gossip network consisting of a source generating updates and $n$ nodes connected according to a given graph structure.","The source keeps updates of a process, that might be generated or observed, and shares them with the gossiping network.","The nodes in the network communicate with their neighbors and disseminate these version updates using a push-style gossip strategy.","We use the version age metric to quantify the timeliness of information at the nodes.","We first find an upper bound for the average version age for a set of nodes in a general network.","Using this, we find the average version age scaling of a node in several network graph structures, such as two-dimensional grids, generalized rings and hyper-cubes.","Prior to our work, it was known that when $n$ nodes are connected on a ring the version age scales as $O(n^{\\frac{1}{2}})$, and when they are connected on a fully-connected graph the version age scales as $O(\\log n)$. Ours is the first work to show an age scaling result for a connectivity structure other than the ring and the fully-connected network, which constitute the two extremes of network connectivity.","Our work helps fill the gap between these two extremes by analyzing a large variety of graphs with intermediate connectivity, thus providing insight into the relationship between the connectivity structure of the network and the version age, and uncovering a network connectivity--information freshness tradeoff."],"url":"http://arxiv.org/abs/2405.19310v1","category":"cs.IT"}
{"created":"2024-05-29 17:31:25","title":"Data Efficient Behavior Cloning for Fine Manipulation via Continuity-based Corrective Labels","abstract":"We consider imitation learning with access only to expert demonstrations, whose real-world application is often limited by covariate shift due to compounding errors during execution. We investigate the effectiveness of the Continuity-based Corrective Labels for Imitation Learning (CCIL) framework in mitigating this issue for real-world fine manipulation tasks. CCIL generates corrective labels by learning a locally continuous dynamics model from demonstrations to guide the agent back toward expert states. Through extensive experiments on peg insertion and fine grasping, we provide the first empirical validation that CCIL can significantly improve imitation learning performance despite discontinuities present in contact-rich manipulation. We find that: (1) real-world manipulation exhibits sufficient local smoothness to apply CCIL, (2) generated corrective labels are most beneficial in low-data regimes, and (3) label filtering based on estimated dynamics model error enables performance gains. To effectively apply CCIL to robotic domains, we offer a practical instantiation of the framework and insights into design choices and hyperparameter selection. Our work demonstrates CCIL's practicality for alleviating compounding errors in imitation learning on physical robots.","sentences":["We consider imitation learning with access only to expert demonstrations, whose real-world application is often limited by covariate shift due to compounding errors during execution.","We investigate the effectiveness of the Continuity-based Corrective Labels for Imitation Learning (CCIL) framework in mitigating this issue for real-world fine manipulation tasks.","CCIL generates corrective labels by learning a locally continuous dynamics model from demonstrations to guide the agent back toward expert states.","Through extensive experiments on peg insertion and fine grasping, we provide the first empirical validation that CCIL can significantly improve imitation learning performance despite discontinuities present in contact-rich manipulation.","We find that: (1) real-world manipulation exhibits sufficient local smoothness to apply CCIL, (2) generated corrective labels are most beneficial in low-data regimes, and (3) label filtering based on estimated dynamics model error enables performance gains.","To effectively apply CCIL to robotic domains, we offer a practical instantiation of the framework and insights into design choices and hyperparameter selection.","Our work demonstrates CCIL's practicality for alleviating compounding errors in imitation learning on physical robots."],"url":"http://arxiv.org/abs/2405.19307v1","category":"cs.RO"}
{"created":"2024-05-29 17:28:28","title":"Morse Theory for Chromatic Delaunay Triangulations","abstract":"The chromatic alpha filtration is a generalization of the alpha filtration that can encode spatial relationships among classes of labelled point cloud data, and has applications in topological data analysis of multi-species data. In this paper we introduce the chromatic Delaunay--\\v{C}ech and chromatic Delaunay--Rips filtrations, which are computationally favourable alternatives to the chromatic alpha filtration. We use generalized discrete Morse theory to show that the \\v{C}ech, chromatic Delaunay--\\v{C}ech, and chromatic alpha filtrations are related by simplicial collapses. Our result generalizes a result of Bauer and Edelsbrunner from the non-chromatic to the chromatic setting. We also show that the chromatic Delaunay--Rips filtration is locally stable to perturbations of the underlying point cloud. Our results provide theoretical justification for the use of chromatic Delaunay--\\v{C}ech and chromatic Delaunay--Rips filtrations in applications, and we demonstrate their computational advantage with numerical experiments.","sentences":["The chromatic alpha filtration is a generalization of the alpha filtration that can encode spatial relationships among classes of labelled point cloud data, and has applications in topological data analysis of multi-species data.","In this paper we introduce the chromatic Delaunay--\\v{C}ech and chromatic Delaunay--Rips filtrations, which are computationally favourable alternatives to the chromatic alpha filtration.","We use generalized discrete Morse theory to show that the \\v{C}ech, chromatic Delaunay--\\v{C}ech, and chromatic alpha filtrations are related by simplicial collapses.","Our result generalizes a result of Bauer and Edelsbrunner from the non-chromatic to the chromatic setting.","We also show that the chromatic Delaunay--Rips filtration is locally stable to perturbations of the underlying point cloud.","Our results provide theoretical justification for the use of chromatic Delaunay--\\v{C}ech and chromatic Delaunay--Rips filtrations in applications, and we demonstrate their computational advantage with numerical experiments."],"url":"http://arxiv.org/abs/2405.19303v1","category":"math.AT"}
{"created":"2024-05-29 17:27:08","title":"Measuring and Mitigating Bias for Tabular Datasets with Multiple Protected Attributes","abstract":"Motivated by the recital (67) of the current corrigendum of the AI Act in the European Union, we propose and present measures and mitigation strategies for discrimination in tabular datasets. We specifically focus on datasets that contain multiple protected attributes, such as nationality, age, and sex. This makes measuring and mitigating bias more challenging, as many existing methods are designed for a single protected attribute. This paper comes with a twofold contribution: Firstly, new discrimination measures are introduced. These measures are categorized in our framework along with existing ones, guiding researchers and practitioners in choosing the right measure to assess the fairness of the underlying dataset. Secondly, a novel application of an existing bias mitigation method, FairDo, is presented. We show that this strategy can mitigate any type of discrimination, including intersectional discrimination, by transforming the dataset. By conducting experiments on real-world datasets (Adult, Bank, Compas), we demonstrate that de-biasing datasets with multiple protected attributes is achievable. Further, the transformed fair datasets do not compromise any of the tested machine learning models' performances significantly when trained on these datasets compared to the original datasets. Discrimination was reduced by up to 83% in our experimentation. For most experiments, the disparity between protected groups was reduced by at least 7% and 27% on average. Generally, the findings show that the mitigation strategy used is effective, and this study contributes to the ongoing discussion on the implementation of the European Union's AI Act.","sentences":["Motivated by the recital (67) of the current corrigendum of the AI Act in the European Union, we propose and present measures and mitigation strategies for discrimination in tabular datasets.","We specifically focus on datasets that contain multiple protected attributes, such as nationality, age, and sex.","This makes measuring and mitigating bias more challenging, as many existing methods are designed for a single protected attribute.","This paper comes with a twofold contribution: Firstly, new discrimination measures are introduced.","These measures are categorized in our framework along with existing ones, guiding researchers and practitioners in choosing the right measure to assess the fairness of the underlying dataset.","Secondly, a novel application of an existing bias mitigation method, FairDo, is presented.","We show that this strategy can mitigate any type of discrimination, including intersectional discrimination, by transforming the dataset.","By conducting experiments on real-world datasets (Adult, Bank, Compas), we demonstrate that de-biasing datasets with multiple protected attributes is achievable.","Further, the transformed fair datasets do not compromise any of the tested machine learning models' performances significantly when trained on these datasets compared to the original datasets.","Discrimination was reduced by up to 83% in our experimentation.","For most experiments, the disparity between protected groups was reduced by at least 7% and 27% on average.","Generally, the findings show that the mitigation strategy used is effective, and this study contributes to the ongoing discussion on the implementation of the European Union's AI Act."],"url":"http://arxiv.org/abs/2405.19300v1","category":"cs.LG"}
{"created":"2024-05-29 17:26:52","title":"Expert-Guided Extinction of Toxic Tokens for Debiased Generation","abstract":"Large language models (LLMs) can elicit social bias during generations, especially when inference with toxic prompts. Controlling the sensitive attributes in generation encounters challenges in data distribution, generalizability, and efficiency. Specifically, fine-tuning and retrieval demand extensive unbiased corpus, while direct prompting requires meticulously curated instructions for correcting the output in multiple rounds of thoughts but poses challenges on memory and inference latency. In this work, we propose the Expert-Guided Extinction of Toxic Tokens for Debiased Generation (EXPOSED) to eliminate the undesired harmful outputs for LLMs without the aforementioned requirements. EXPOSED constructs a debiasing expert based on the abundant toxic corpus to expose and elicit the potentially dangerous tokens. It then processes the output to the LLMs and constructs a fair distribution by suppressing and attenuating the toxic tokens. EXPOSED is evaluated on fairness benchmarks over three LLM families. Extensive experiments demonstrate that compared with other baselines, the proposed EXPOSED significantly reduces the potential social bias while balancing fairness and generation performance.","sentences":["Large language models (LLMs) can elicit social bias during generations, especially when inference with toxic prompts.","Controlling the sensitive attributes in generation encounters challenges in data distribution, generalizability, and efficiency.","Specifically, fine-tuning and retrieval demand extensive unbiased corpus, while direct prompting requires meticulously curated instructions for correcting the output in multiple rounds of thoughts but poses challenges on memory and inference latency.","In this work, we propose the Expert-Guided Extinction of Toxic Tokens for Debiased Generation (EXPOSED) to eliminate the undesired harmful outputs for LLMs without the aforementioned requirements.","EXPOSED constructs a debiasing expert based on the abundant toxic corpus to expose and elicit the potentially dangerous tokens.","It then processes the output to the LLMs and constructs a fair distribution by suppressing and attenuating the toxic tokens.","EXPOSED is evaluated on fairness benchmarks over three LLM families.","Extensive experiments demonstrate that compared with other baselines, the proposed EXPOSED significantly reduces the potential social bias while balancing fairness and generation performance."],"url":"http://arxiv.org/abs/2405.19299v1","category":"cs.CL"}
{"created":"2024-05-29 17:26:09","title":"Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare","abstract":"While recent advancements in large multimodal models (LMMs) have significantly improved their abilities in image quality assessment (IQA) relying on absolute quality rating, how to transfer reliable relative quality comparison outputs to continuous perceptual quality scores remains largely unexplored. To address this gap, we introduce Compare2Score-an all-around LMM-based no-reference IQA (NR-IQA) model, which is capable of producing qualitatively comparative responses and effectively translating these discrete comparative levels into a continuous quality score. Specifically, during training, we present to generate scaled-up comparative instructions by comparing images from the same IQA dataset, allowing for more flexible integration of diverse IQA datasets. Utilizing the established large-scale training corpus, we develop a human-like visual quality comparator. During inference, moving beyond binary choices, we propose a soft comparison method that calculates the likelihood of the test image being preferred over multiple predefined anchor images. The quality score is further optimized by maximum a posteriori estimation with the resulting probability matrix. Extensive experiments on nine IQA datasets validate that the Compare2Score effectively bridges text-defined comparative levels during training with converted single image quality score for inference, surpassing state-of-the-art IQA models across diverse scenarios. Moreover, we verify that the probability-matrix-based inference conversion not only improves the rating accuracy of Compare2Score but also zero-shot general-purpose LMMs, suggesting its intrinsic effectiveness.","sentences":["While recent advancements in large multimodal models (LMMs) have significantly improved their abilities in image quality assessment (IQA) relying on absolute quality rating, how to transfer reliable relative quality comparison outputs to continuous perceptual quality scores remains largely unexplored.","To address this gap, we introduce Compare2Score-an all-around LMM-based no-reference IQA (NR-IQA) model, which is capable of producing qualitatively comparative responses and effectively translating these discrete comparative levels into a continuous quality score.","Specifically, during training, we present to generate scaled-up comparative instructions by comparing images from the same IQA dataset, allowing for more flexible integration of diverse IQA datasets.","Utilizing the established large-scale training corpus, we develop a human-like visual quality comparator.","During inference, moving beyond binary choices, we propose a soft comparison method that calculates the likelihood of the test image being preferred over multiple predefined anchor images.","The quality score is further optimized by maximum a posteriori estimation with the resulting probability matrix.","Extensive experiments on nine IQA datasets validate that the Compare2Score effectively bridges text-defined comparative levels during training with converted single image quality score for inference, surpassing state-of-the-art IQA models across diverse scenarios.","Moreover, we verify that the probability-matrix-based inference conversion not only improves the rating accuracy of Compare2Score but also zero-shot general-purpose LMMs, suggesting its intrinsic effectiveness."],"url":"http://arxiv.org/abs/2405.19298v1","category":"cs.CV"}
{"created":"2024-05-29 17:24:25","title":"Neural Isometries: Taming Transformations for Equivariant ML","abstract":"Real-world geometry and 3D vision tasks are replete with challenging symmetries that defy tractable analytical expression. In this paper, we introduce Neural Isometries, an autoencoder framework which learns to map the observation space to a general-purpose latent space wherein encodings are related by isometries whenever their corresponding observations are geometrically related in world space. Specifically, we regularize the latent space such that maps between encodings preserve a learned inner product and commute with a learned functional operator, in the same manner as rigid-body transformations commute with the Laplacian. This approach forms an effective backbone for self-supervised representation learning, and we demonstrate that a simple off-the-shelf equivariant network operating in the pre-trained latent space can achieve results on par with meticulously-engineered, handcrafted networks designed to handle complex, nonlinear symmetries. Furthermore, isometric maps capture information about the respective transformations in world space, and we show that this allows us to regress camera poses directly from the coefficients of the maps between encodings of adjacent views of a scene.","sentences":["Real-world geometry and 3D vision tasks are replete with challenging symmetries that defy tractable analytical expression.","In this paper, we introduce Neural Isometries, an autoencoder framework which learns to map the observation space to a general-purpose latent space wherein encodings are related by isometries whenever their corresponding observations are geometrically related in world space.","Specifically, we regularize the latent space such that maps between encodings preserve a learned inner product and commute with a learned functional operator, in the same manner as rigid-body transformations commute with the Laplacian.","This approach forms an effective backbone for self-supervised representation learning, and we demonstrate that a simple off-the-shelf equivariant network operating in the pre-trained latent space can achieve results on par with meticulously-engineered, handcrafted networks designed to handle complex, nonlinear symmetries.","Furthermore, isometric maps capture information about the respective transformations in world space, and we show that this allows us to regress camera poses directly from the coefficients of the maps between encodings of adjacent views of a scene."],"url":"http://arxiv.org/abs/2405.19296v1","category":"cs.CV"}
{"created":"2024-05-29 17:19:15","title":"Grasp as You Say: Language-guided Dexterous Grasp Generation","abstract":"This paper explores a novel task \"\"Dexterous Grasp as You Say\"\" (DexGYS), enabling robots to perform dexterous grasping based on human commands expressed in natural language. However, the development of this field is hindered by the lack of datasets with natural human guidance; thus, we propose a language-guided dexterous grasp dataset, named DexGYSNet, offering high-quality dexterous grasp annotations along with flexible and fine-grained human language guidance. Our dataset construction is cost-efficient, with the carefully-design hand-object interaction retargeting strategy, and the LLM-assisted language guidance annotation system. Equipped with this dataset, we introduce the DexGYSGrasp framework for generating dexterous grasps based on human language instructions, with the capability of producing grasps that are intent-aligned, high quality and diversity. To achieve this capability, our framework decomposes the complex learning process into two manageable progressive objectives and introduce two components to realize them. The first component learns the grasp distribution focusing on intention alignment and generation diversity. And the second component refines the grasp quality while maintaining intention consistency. Extensive experiments are conducted on DexGYSNet and real world environment for validation.","sentences":["This paper explores a novel task \"\"Dexterous Grasp as You Say\"\" (DexGYS), enabling robots to perform dexterous grasping based on human commands expressed in natural language.","However, the development of this field is hindered by the lack of datasets with natural human guidance; thus, we propose a language-guided dexterous grasp dataset, named DexGYSNet, offering high-quality dexterous grasp annotations along with flexible and fine-grained human language guidance.","Our dataset construction is cost-efficient, with the carefully-design hand-object interaction retargeting strategy, and the LLM-assisted language guidance annotation system.","Equipped with this dataset, we introduce the DexGYSGrasp framework for generating dexterous grasps based on human language instructions, with the capability of producing grasps that are intent-aligned, high quality and diversity.","To achieve this capability, our framework decomposes the complex learning process into two manageable progressive objectives and introduce two components to realize them.","The first component learns the grasp distribution focusing on intention alignment and generation diversity.","And the second component refines the grasp quality while maintaining intention consistency.","Extensive experiments are conducted on DexGYSNet and real world environment for validation."],"url":"http://arxiv.org/abs/2405.19291v1","category":"cs.RO"}
{"created":"2024-05-29 17:18:38","title":"Archetype-Based Redshift Estimation for the Dark Energy Spectroscopic Instrument Survey","abstract":"We present a computationally efficient galaxy archetype-based redshift estimation and spectral classification method for the Dark Energy Survey Instrument (DESI) survey. The DESI survey currently relies on a redshift fitter and spectral classifier using a linear combination of PCA-derived templates, which is very efficient in processing large volumes of DESI spectra within a short time frame. However, this method occasionally yields unphysical model fits for galaxies and fails to adequately absorb calibration errors that may still be occasionally visible in the reduced spectra. Our proposed approach improves upon this existing method by refitting the spectra with carefully generated physical galaxy archetypes combined with additional terms designed to absorb data reduction defects and provide more physical models to the DESI spectra. We test our method on an extensive dataset derived from the survey validation (SV) and Year 1 (Y1) data of DESI. Our findings indicate that the new method delivers marginally better redshift success for SV tiles while reducing catastrophic redshift failure by $10-30\\%$. At the same time, results from millions of targets from the main survey show that our model has relatively higher redshift success and purity rates ($0.5-0.8\\%$ higher) for galaxy targets while having similar success for QSOs. These improvements also demonstrate that the main DESI redshift pipeline is generally robust. Additionally, it reduces the false positive redshift estimation by $5-40\\%$ for sky fibers. We also discuss the generic nature of our method and how it can be extended to other large spectroscopic surveys, along with possible future improvements.","sentences":["We present a computationally efficient galaxy archetype-based redshift estimation and spectral classification method for the Dark Energy Survey Instrument (DESI) survey.","The DESI survey currently relies on a redshift fitter and spectral classifier using a linear combination of PCA-derived templates, which is very efficient in processing large volumes of DESI spectra within a short time frame.","However, this method occasionally yields unphysical model fits for galaxies and fails to adequately absorb calibration errors that may still be occasionally visible in the reduced spectra.","Our proposed approach improves upon this existing method by refitting the spectra with carefully generated physical galaxy archetypes combined with additional terms designed to absorb data reduction defects and provide more physical models to the DESI spectra.","We test our method on an extensive dataset derived from the survey validation (SV) and Year 1 (Y1) data of DESI.","Our findings indicate that the new method delivers marginally better redshift success for SV tiles while reducing catastrophic redshift failure by $10-30\\%$. At the same time, results from millions of targets from the main survey show that our model has relatively higher redshift success and purity rates ($0.5-0.8\\%$ higher) for galaxy targets while having similar success for QSOs.","These improvements also demonstrate that the main DESI redshift pipeline is generally robust.","Additionally, it reduces the false positive redshift estimation by $5-40\\%$ for sky fibers.","We also discuss the generic nature of our method and how it can be extended to other large spectroscopic surveys, along with possible future improvements."],"url":"http://arxiv.org/abs/2405.19288v1","category":"astro-ph.CO"}
{"created":"2024-05-29 17:16:59","title":"Optimizing Foundation Model Inference on a Many-tiny-core Open-source RISC-V Platform","abstract":"Transformer-based foundation models have become crucial for various domains, most notably natural language processing (NLP) or computer vision (CV). These models are predominantly deployed on high-performance GPUs or hardwired accelerators with highly customized, proprietary instruction sets. Until now, limited attention has been given to RISC-V-based general-purpose platforms. In our work, we present the first end-to-end inference results of transformer models on an open-source many-tiny-core RISC-V platform implementing distributed Softmax primitives and leveraging ISA extensions for SIMD floating-point operand streaming and instruction repetition, as well as specialized DMA engines to minimize costly main memory accesses and to tolerate their latency. We focus on two foundational transformer topologies, encoder-only and decoder-only models. For encoder-only models, we demonstrate a speedup of up to 12.8x between the most optimized implementation and the baseline version. We reach over 79% FPU utilization and 294 GFLOPS/W, outperforming State-of-the-Art (SoA) accelerators by more than 2x utilizing the HW platform while achieving comparable throughput per computational unit. For decoder-only topologies, we achieve 16.1x speedup in the Non-Autoregressive (NAR) mode and up to 35.6x speedup in the Autoregressive (AR) mode compared to the baseline implementation. Compared to the best SoA dedicated accelerator, we achieve 2.04x higher FPU utilization.","sentences":["Transformer-based foundation models have become crucial for various domains, most notably natural language processing (NLP) or computer vision (CV).","These models are predominantly deployed on high-performance GPUs or hardwired accelerators with highly customized, proprietary instruction sets.","Until now, limited attention has been given to RISC-V-based general-purpose platforms.","In our work, we present the first end-to-end inference results of transformer models on an open-source many-tiny-core RISC-V platform implementing distributed Softmax primitives and leveraging ISA extensions for SIMD floating-point operand streaming and instruction repetition, as well as specialized DMA engines to minimize costly main memory accesses and to tolerate their latency.","We focus on two foundational transformer topologies, encoder-only and decoder-only models.","For encoder-only models, we demonstrate a speedup of up to 12.8x between the most optimized implementation and the baseline version.","We reach over 79% FPU utilization and 294 GFLOPS/W, outperforming State-of-the-Art (SoA) accelerators by more than 2x utilizing the HW platform while achieving comparable throughput per computational unit.","For decoder-only topologies, we achieve 16.1x speedup in the Non-Autoregressive (NAR) mode and up to 35.6x speedup in the Autoregressive (AR) mode compared to the baseline implementation.","Compared to the best SoA dedicated accelerator, we achieve 2.04x higher FPU utilization."],"url":"http://arxiv.org/abs/2405.19284v1","category":"cs.DC"}
{"created":"2024-05-29 17:14:55","title":"Programmable Motion Generation for Open-Set Motion Control Tasks","abstract":"Character animation in real-world scenarios necessitates a variety of constraints, such as trajectories, key-frames, interactions, etc. Existing methodologies typically treat single or a finite set of these constraint(s) as separate control tasks. They are often specialized, and the tasks they address are rarely extendable or customizable. We categorize these as solutions to the close-set motion control problem. In response to the complexity of practical motion control, we propose and attempt to solve the open-set motion control problem. This problem is characterized by an open and fully customizable set of motion control tasks. To address this, we introduce a new paradigm, programmable motion generation. In this paradigm, any given motion control task is broken down into a combination of atomic constraints. These constraints are then programmed into an error function that quantifies the degree to which a motion sequence adheres to them. We utilize a pre-trained motion generation model and optimize its latent code to minimize the error function of the generated motion. Consequently, the generated motion not only inherits the prior of the generative model but also satisfies the required constraints. Experiments show that we can generate high-quality motions when addressing a wide range of unseen tasks. These tasks encompass motion control by motion dynamics, geometric constraints, physical laws, interactions with scenes, objects or the character own body parts, etc. All of these are achieved in a unified approach, without the need for ad-hoc paired training data collection or specialized network designs. During the programming of novel tasks, we observed the emergence of new skills beyond those of the prior model. With the assistance of large language models, we also achieved automatic programming. We hope that this work will pave the way for the motion control of general AI agents.","sentences":["Character animation in real-world scenarios necessitates a variety of constraints, such as trajectories, key-frames, interactions, etc.","Existing methodologies typically treat single or a finite set of these constraint(s) as separate control tasks.","They are often specialized, and the tasks they address are rarely extendable or customizable.","We categorize these as solutions to the close-set motion control problem.","In response to the complexity of practical motion control, we propose and attempt to solve the open-set motion control problem.","This problem is characterized by an open and fully customizable set of motion control tasks.","To address this, we introduce a new paradigm, programmable motion generation.","In this paradigm, any given motion control task is broken down into a combination of atomic constraints.","These constraints are then programmed into an error function that quantifies the degree to which a motion sequence adheres to them.","We utilize a pre-trained motion generation model and optimize its latent code to minimize the error function of the generated motion.","Consequently, the generated motion not only inherits the prior of the generative model but also satisfies the required constraints.","Experiments show that we can generate high-quality motions when addressing a wide range of unseen tasks.","These tasks encompass motion control by motion dynamics, geometric constraints, physical laws, interactions with scenes, objects or the character own body parts, etc.","All of these are achieved in a unified approach, without the need for ad-hoc paired training data collection or specialized network designs.","During the programming of novel tasks, we observed the emergence of new skills beyond those of the prior model.","With the assistance of large language models, we also achieved automatic programming.","We hope that this work will pave the way for the motion control of general AI agents."],"url":"http://arxiv.org/abs/2405.19283v1","category":"cs.CV"}
{"created":"2024-05-29 17:14:34","title":"On the theory of relativistic Brownian motion","abstract":"The approach to the theory of a relativistic random process is considered by the path integral method as Brownian motion taking into account the boundedness of speed. An attempt was made to build a relativistic analogue of the Wiener measure as a weak limit of finite-difference approximations. A formula has been proposed for calculating the probability particle transition during relativistic Brownian motion. Calculations were carried out by three different methods with identical results. Along the way, exact and asymptotic formulas for the volume of some parts and sections of an N-1-dimensional unit cube were obtained. They can have independent value.","sentences":["The approach to the theory of a relativistic random process is considered by the path integral method as Brownian motion taking into account the boundedness of speed.","An attempt was made to build a relativistic analogue of the Wiener measure as a weak limit of finite-difference approximations.","A formula has been proposed for calculating the probability particle transition during relativistic Brownian motion.","Calculations were carried out by three different methods with identical results.","Along the way, exact and asymptotic formulas for the volume of some parts and sections of an N-1-dimensional unit cube were obtained.","They can have independent value."],"url":"http://arxiv.org/abs/2405.19282v1","category":"gr-qc"}
{"created":"2024-05-29 17:10:30","title":"Causal Data Fusion with Quantum Confounders","abstract":"From the modern perspective of causal inference, Bell's theorem -- a fundamental signature of quantum theory -- is a particular case where quantum correlations are incompatible with the classical theory of causality, and the generalization of Bell's theorem to quantum networks has led to several breakthrough results and novel applications. Here, we consider the problem of causal data fusion, where we piece together multiple datasets collected under heterogeneous conditions. In particular, we show quantum experiments can generate observational and interventional data with a non-classical signature when pieced together that cannot be reproduced classically. We prove this quantum non-classicality emerges from the fusion of the datasets and is present in a plethora of scenarios, even where standard Bell non-classicality is impossible. Furthermore, we show that non-classicality genuine to the fusion of multiple data tables is achievable with quantum resources. Our work shows incorporating interventions -- a central tool in causal inference -- can be a powerful tool to detect non-classicality beyond the violation of a standard Bell inequality. In a companion article \"Quantum Non-classicality from Causal Data Fusion\", we extend our investigation considering all latent exogenous causal structures with 3 observable variables.","sentences":["From the modern perspective of causal inference, Bell's theorem -- a fundamental signature of quantum theory -- is a particular case where quantum correlations are incompatible with the classical theory of causality, and the generalization of Bell's theorem to quantum networks has led to several breakthrough results and novel applications.","Here, we consider the problem of causal data fusion, where we piece together multiple datasets collected under heterogeneous conditions.","In particular, we show quantum experiments can generate observational and interventional data with a non-classical signature when pieced together that cannot be reproduced classically.","We prove this quantum non-classicality emerges from the fusion of the datasets and is present in a plethora of scenarios, even where standard Bell non-classicality is impossible.","Furthermore, we show that non-classicality genuine to the fusion of multiple data tables is achievable with quantum resources.","Our work shows incorporating interventions -- a central tool in causal inference -- can be a powerful tool to detect non-classicality beyond the violation of a standard Bell inequality.","In a companion article \"Quantum Non-classicality from Causal Data Fusion\", we extend our investigation considering all latent exogenous causal structures with 3 observable variables."],"url":"http://arxiv.org/abs/2405.19278v1","category":"quant-ph"}
{"created":"2024-05-29 17:07:33","title":"Deep Latent Variable Modeling of Physiological Signals","abstract":"A deep latent variable model is a powerful method for capturing complex distributions. These models assume that underlying structures, but unobserved, are present within the data. In this dissertation, we explore high-dimensional problems related to physiological monitoring using latent variable models. First, we present a novel deep state-space model to generate electrical waveforms of the heart using optically obtained signals as inputs. This can bring about clinical diagnoses of heart disease via simple assessment through wearable devices. Second, we present a brain signal modeling scheme that combines the strengths of probabilistic graphical models and deep adversarial learning. The structured representations can provide interpretability and encode inductive biases to reduce the data complexity of neural oscillations. The efficacy of the learned representations is further studied in epilepsy seizure detection formulated as an unsupervised learning problem. Third, we propose a framework for the joint modeling of physiological measures and behavior. Existing methods to combine multiple sources of brain data provided are limited. Direct analysis of the relationship between different types of physiological measures usually does not involve behavioral data. Our method can identify the unique and shared contributions of brain regions to behavior and can be used to discover new functions of brain regions. The success of these innovative computational methods would allow the translation of biomarker findings across species and provide insight into neurocognitive analysis in numerous biological studies and clinical diagnoses, as well as emerging consumer applications.","sentences":["A deep latent variable model is a powerful method for capturing complex distributions.","These models assume that underlying structures, but unobserved, are present within the data.","In this dissertation, we explore high-dimensional problems related to physiological monitoring using latent variable models.","First, we present a novel deep state-space model to generate electrical waveforms of the heart using optically obtained signals as inputs.","This can bring about clinical diagnoses of heart disease via simple assessment through wearable devices.","Second, we present a brain signal modeling scheme that combines the strengths of probabilistic graphical models and deep adversarial learning.","The structured representations can provide interpretability and encode inductive biases to reduce the data complexity of neural oscillations.","The efficacy of the learned representations is further studied in epilepsy seizure detection formulated as an unsupervised learning problem.","Third, we propose a framework for the joint modeling of physiological measures and behavior.","Existing methods to combine multiple sources of brain data provided are limited.","Direct analysis of the relationship between different types of physiological measures usually does not involve behavioral data.","Our method can identify the unique and shared contributions of brain regions to behavior and can be used to discover new functions of brain regions.","The success of these innovative computational methods would allow the translation of biomarker findings across species and provide insight into neurocognitive analysis in numerous biological studies and clinical diagnoses, as well as emerging consumer applications."],"url":"http://arxiv.org/abs/2405.19277v1","category":"cs.LG"}
{"created":"2024-05-29 17:07:02","title":"The Future of Child Development in the AI Era. Cross-Disciplinary Perspectives Between AI and Child Development Experts","abstract":"This report explores the potential implications of rapidly integrating Artificial Intelligence (AI) applications into children's environments. The introduction of AI in our daily lives necessitates scrutiny considering the significant role of the environment in shaping cognition, socio-emotional skills, and behaviors, especially during the first 25 years of cerebral development. As AI becomes prevalent in educational and leisure activities, it will significantly modify the experiences of children and adolescents, presenting both challenges and opportunities for their developmental trajectories. This analysis was informed by consulting with 15 experts from pertinent disciplines (AI, product development, child development, and neurosciences), along with a comprehensive review of scientific literature on children development and child-technology interactions. Overall, AI experts anticipate that AI will transform leisure activities, revolutionize education, and redefine human-machine interactions. While AI offers substantial benefits in fostering interactive engagement, it also poses risks that require careful considerations, especially during sensitive developmental periods. The report advocates for proactive international collaboration across multiple disciplines and increased research into how technological innovations affect child development. Such efforts are crucial for designing a sustainable and ethical future for the next generation through specific child-centered regulations, and helping to educate all potential stakeholders (regulators, developers, parents and educators, children) about responsible AI use and its potential impacts on child development.","sentences":["This report explores the potential implications of rapidly integrating Artificial Intelligence (AI) applications into children's environments.","The introduction of AI in our daily lives necessitates scrutiny considering the significant role of the environment in shaping cognition, socio-emotional skills, and behaviors, especially during the first 25 years of cerebral development.","As AI becomes prevalent in educational and leisure activities, it will significantly modify the experiences of children and adolescents, presenting both challenges and opportunities for their developmental trajectories.","This analysis was informed by consulting with 15 experts from pertinent disciplines (AI, product development, child development, and neurosciences), along with a comprehensive review of scientific literature on children development and child-technology interactions.","Overall, AI experts anticipate that AI will transform leisure activities, revolutionize education, and redefine human-machine interactions.","While AI offers substantial benefits in fostering interactive engagement, it also poses risks that require careful considerations, especially during sensitive developmental periods.","The report advocates for proactive international collaboration across multiple disciplines and increased research into how technological innovations affect child development.","Such efforts are crucial for designing a sustainable and ethical future for the next generation through specific child-centered regulations, and helping to educate all potential stakeholders (regulators, developers, parents and educators, children) about responsible AI use and its potential impacts on child development."],"url":"http://arxiv.org/abs/2405.19275v1","category":"cs.HC"}
{"created":"2024-05-29 17:05:04","title":"Stable degeneration of families of klt singularities with constant local volume","abstract":"We prove that for a locally stable family of klt singularities with constant local volume, the ideal sequences of the minimizing valuations for the normalized volume function form a family of ideals with flat cosupport, which induces a degeneration to a locally stable family of K-semistable log Fano cone singularities. Our proof is a family version of the method of C. Xu and Z. Zhuang proving finite generation by Koll\\'ar models and multiple degenerations.","sentences":["We prove that for a locally stable family of klt singularities with constant local volume, the ideal sequences of the minimizing valuations for the normalized volume function form a family of ideals with flat cosupport, which induces a degeneration to a locally stable family of K-semistable log Fano cone singularities.","Our proof is a family version of the method of C. Xu and Z. Zhuang proving finite generation by Koll\\'ar models and multiple degenerations."],"url":"http://arxiv.org/abs/2405.19273v1","category":"math.AG"}
{"created":"2024-05-29 16:59:38","title":"PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications","abstract":"Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline. In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. Our model and dataset will be open-source for community development.","sentences":["Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce.","Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures.","To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands.","Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline.","In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation.","Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models.","After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses.","In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery.","Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs.","Our model and dataset will be open-source for community development."],"url":"http://arxiv.org/abs/2405.19266v1","category":"cs.CL"}
{"created":"2024-05-29 16:57:33","title":"AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data","abstract":"Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance. However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained Code LLMs. In this paper, we present AlchemistCoder, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed AlchemistPrompts, to harmonize different data sources and instruction-response pairs. Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review. Extensive experiments demonstrate that AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence.","sentences":["Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance.","However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained Code LLMs.","In this paper, we present AlchemistCoder, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data.","To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed AlchemistPrompts, to harmonize different data sources and instruction-response pairs.","Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review.","Extensive experiments demonstrate that AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence."],"url":"http://arxiv.org/abs/2405.19265v1","category":"cs.CL"}
{"created":"2024-05-29 16:55:32","title":"Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models","abstract":"Large language models are usually fine-tuned to align with human preferences. However, fine-tuning a large language model can be challenging. In this work, we introduce $\\textit{weak-to-strong search}$, framing the alignment of a large language model as a test-time greedy search to maximize the log-likelihood difference between small tuned and untuned models while sampling from the frozen large model. This method serves both as (i) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (ii) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance. Empirically, we demonstrate the flexibility of weak-to-strong search across different tasks. In controlled-sentiment generation and summarization, we use tuned and untuned $\\texttt{gpt2}$s to effectively improve the alignment of large models without additional training. Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0, we show that reusing off-the-shelf small model pairs (e.g., $\\texttt{zephyr-7b-beta}$ and its untuned version) can significantly improve the length-controlled win rates of both white-box and black-box large models against $\\texttt{gpt-4-turbo}$ (e.g., $34.4 \\rightarrow 37.9$ for $\\texttt{Llama-3-70B-Instruct}$ and $16.0 \\rightarrow 20.1$ for $\\texttt{gpt-3.5-turbo-instruct}$), despite the small models' low win rates $\\approx 10.0$.","sentences":["Large language models are usually fine-tuned to align with human preferences.","However, fine-tuning a large language model can be challenging.","In this work, we introduce $\\textit{weak-to-strong search}$, framing the alignment of a large language model as a test-time greedy search to maximize the log-likelihood difference between small tuned and untuned models while sampling from the frozen large model.","This method serves both as (i) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (ii) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance.","Empirically, we demonstrate the flexibility of weak-to-strong search across different tasks.","In controlled-sentiment generation and summarization, we use tuned and untuned $\\texttt{gpt2}$s to effectively improve the alignment of large models without additional training.","Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0, we show that reusing off-the-shelf small model pairs (e.g., $\\texttt{zephyr-7b-beta}$ and its untuned version) can significantly improve the length-controlled win rates of both white-box and black-box large models against $\\texttt{gpt-4-turbo}$ (e.g., $34.4 \\rightarrow 37.9$ for $\\texttt{Llama-3-70B-Instruct}$ and $16.0 \\rightarrow 20.1$ for $\\texttt{gpt-3.5-turbo-instruct}$), despite the small models' low win rates $\\approx 10.0$."],"url":"http://arxiv.org/abs/2405.19262v1","category":"cs.CL"}
{"created":"2024-05-29 16:55:08","title":"Faster Cascades via Speculative Decoding","abstract":"Cascades and speculative decoding are two common approaches to improving language models' inference efficiency. Both approaches involve interleaving models of different sizes, but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for \"hard\" inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode. These mechanisms offer different benefits: empirically, cascades are often capable of yielding better quality than even the larger model, while theoretically, speculative decoding offers a guarantee of quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule. Through experiments with T5 models on benchmark language tasks, we show that the proposed approach yields better cost-quality trade-offs than cascading and speculative decoding baselines.","sentences":["Cascades and speculative decoding are two common approaches to improving language models' inference efficiency.","Both approaches involve interleaving models of different sizes, but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for \"hard\" inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode.","These mechanisms offer different benefits: empirically, cascades are often capable of yielding better quality than even the larger model, while theoretically, speculative decoding offers a guarantee of quality-neutrality.","In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution.","We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule.","Through experiments with T5 models on benchmark language tasks, we show that the proposed approach yields better cost-quality trade-offs than cascading and speculative decoding baselines."],"url":"http://arxiv.org/abs/2405.19261v1","category":"cs.CL"}
{"created":"2024-05-29 16:46:30","title":"Polyhedral coproducts","abstract":"Dualising the construction of a polyhedral product, we introduce the notion of a polyhedral coproduct as a certain homotopy limit over the face poset of a simplicial complex. We begin a study of the basic properties of polyhedral coproducts, surveying the Eckmann-Hilton duals of various familiar examples and properties of polyhedral products. In particular, we show that polyhedral coproducts give a functorial interpolation between the wedge and cartesian product of spaces which differs from the one given by polyhedral products, and we establish a general loop space decomposition for these spaces which is dual to the suspension splitting of a polyhedral product due to Bahri, Bendersky, Cohen and Gitler.","sentences":["Dualising the construction of a polyhedral product, we introduce the notion of a polyhedral coproduct as a certain homotopy limit over the face poset of a simplicial complex.","We begin a study of the basic properties of polyhedral coproducts, surveying the Eckmann-Hilton duals of various familiar examples and properties of polyhedral products.","In particular, we show that polyhedral coproducts give a functorial interpolation between the wedge and cartesian product of spaces which differs from the one given by polyhedral products, and we establish a general loop space decomposition for these spaces which is dual to the suspension splitting of a polyhedral product due to Bahri, Bendersky, Cohen and Gitler."],"url":"http://arxiv.org/abs/2405.19258v1","category":"math.AT"}
{"created":"2024-05-29 16:41:42","title":"Weak Generative Sampler to Efficiently Sample Invariant Distribution of Stochastic Differential Equation","abstract":"Sampling invariant distributions from an Ito diffusion process presents a significant challenge in stochastic simulation. Traditional numerical solvers for stochastic differential equations require both a fine step size and a lengthy simulation period, resulting in both biased and correlated samples. Current deep learning-based method solves the stationary Fokker--Planck equation to determine the invariant probability density function in form of deep neural networks, but they generally do not directly address the problem of sampling from the computed density function. In this work, we introduce a framework that employs a weak generative sampler (WGS) to directly generate independent and identically distributed (iid) samples induced by a transformation map derived from the stationary Fokker--Planck equation. Our proposed loss function is based on the weak form of the Fokker--Planck equation, integrating normalizing flows to characterize the invariant distribution and facilitate sample generation from the base distribution. Our randomized test function circumvents the need for mini-max optimization in the traditional weak formulation. Distinct from conventional generative models, our method neither necessitates the computationally intensive calculation of the Jacobian determinant nor the invertibility of the transformation map. A crucial component of our framework is the adaptively chosen family of test functions in the form of Gaussian kernel functions with centres selected from the generated data samples. Experimental results on several benchmark examples demonstrate the effectiveness of our method, which offers both low computational costs and excellent capability in exploring multiple metastable states.","sentences":["Sampling invariant distributions from an Ito diffusion process presents a significant challenge in stochastic simulation.","Traditional numerical solvers for stochastic differential equations require both a fine step size and a lengthy simulation period, resulting in both biased and correlated samples.","Current deep learning-based method solves the stationary Fokker--Planck equation to determine the invariant probability density function in form of deep neural networks, but they generally do not directly address the problem of sampling from the computed density function.","In this work, we introduce a framework that employs a weak generative sampler (WGS) to directly generate independent and identically distributed (iid) samples induced by a transformation map derived from the stationary Fokker--Planck equation.","Our proposed loss function is based on the weak form of the Fokker--Planck equation, integrating normalizing flows to characterize the invariant distribution and facilitate sample generation from the base distribution.","Our randomized test function circumvents the need for mini-max optimization in the traditional weak formulation.","Distinct from conventional generative models, our method neither necessitates the computationally intensive calculation of the Jacobian determinant nor the invertibility of the transformation map.","A crucial component of our framework is the adaptively chosen family of test functions in the form of Gaussian kernel functions with centres selected from the generated data samples.","Experimental results on several benchmark examples demonstrate the effectiveness of our method, which offers both low computational costs and excellent capability in exploring multiple metastable states."],"url":"http://arxiv.org/abs/2405.19256v1","category":"cs.LG"}
{"created":"2024-05-29 16:40:31","title":"Towards Next-Generation Urban Decision Support Systems through AI-Powered Generation of Scientific Ontology using Large Language Models -- A Case in Optimizing Intermodal Freight Transportation","abstract":"The incorporation of Artificial Intelligence (AI) models into various optimization systems is on the rise. Yet, addressing complex urban and environmental management problems normally requires in-depth domain science and informatics expertise. This expertise is essential for deriving data and simulation-driven for informed decision support. In this context, we investigate the potential of leveraging the pre-trained Large Language Models (LLMs). By adopting ChatGPT API as the reasoning core, we outline an integrated workflow that encompasses natural language processing, methontology-based prompt tuning, and transformers. This workflow automates the creation of scenario-based ontology using existing research articles and technical manuals of urban datasets and simulations. The outcomes of our methodology are knowledge graphs in widely adopted ontology languages (e.g., OWL, RDF, SPARQL). These facilitate the development of urban decision support systems by enhancing the data and metadata modeling, the integration of complex datasets, the coupling of multi-domain simulation models, and the formulation of decision-making metrics and workflow. The feasibility of our methodology is evaluated through a comparative analysis that juxtaposes our AI-generated ontology with the well-known Pizza Ontology employed in tutorials for popular ontology software (e.g., prot\\'eg\\'e). We close with a real-world case study of optimizing the complex urban system of multi-modal freight transportation by generating anthologies of various domain data and simulations to support informed decision-making.","sentences":["The incorporation of Artificial Intelligence (AI) models into various optimization systems is on the rise.","Yet, addressing complex urban and environmental management problems normally requires in-depth domain science and informatics expertise.","This expertise is essential for deriving data and simulation-driven for informed decision support.","In this context, we investigate the potential of leveraging the pre-trained Large Language Models (LLMs).","By adopting ChatGPT API as the reasoning core, we outline an integrated workflow that encompasses natural language processing, methontology-based prompt tuning, and transformers.","This workflow automates the creation of scenario-based ontology using existing research articles and technical manuals of urban datasets and simulations.","The outcomes of our methodology are knowledge graphs in widely adopted ontology languages (e.g., OWL, RDF, SPARQL).","These facilitate the development of urban decision support systems by enhancing the data and metadata modeling, the integration of complex datasets, the coupling of multi-domain simulation models, and the formulation of decision-making metrics and workflow.","The feasibility of our methodology is evaluated through a comparative analysis that juxtaposes our AI-generated ontology with the well-known Pizza Ontology employed in tutorials for popular ontology software (e.g., prot\\'eg\\'e).","We close with a real-world case study of optimizing the complex urban system of multi-modal freight transportation by generating anthologies of various domain data and simulations to support informed decision-making."],"url":"http://arxiv.org/abs/2405.19255v1","category":"cs.AI"}
{"created":"2024-05-29 16:34:54","title":"Dark matter admixed neutron stars with a realistic nuclear equation of state from chiral nuclear interactions","abstract":"We study the effects of dark matter on the structural properties of neutron stars. In particular we investigate how the presence of a dark matter component influences the mass-radius relation, the value of the maximum mass of a neutron star and others stellar properties. To model ordinary matter we use a state-of-the-art equation of state of $\\beta$-stable nuclear matter obtained using the Brueckner-Hartree-Fock quantum many-body approach starting from two-body and three-body nuclear interactions derived from chiral effective field theory. The dark matter component of the star is modeled as a non-self-annihilating system of spin $1/2$ fermions and its equation of state as an ideal relativistic Fermi gas. The equilibrium configurations of these dark matter admixed neutron stars (DANS) are calculated by solving a generalization of the Tolman-Oppenheimer-Volkoff equations to the case where the system consists of two perfect fluids interacting solely through gravity. We find that, depending on the dark matter particle mass $m_\\chi$, one can have somehow opposite effects on the stellar properties. In the case $m_\\chi = 1\\, \\mathrm{GeV}$, the stellar gravitational maximum mass $M_{max}$ decreases, whereas in the case $m_\\chi = 0.1\\, \\mathrm{GeV}$, $M_{max}$ increases with respect to the maximum mass of ordinary neutron stars. We also show that the presence of dark matter has indirect sizeable effect on the proton fraction in the ordinary matter fluid and, in the case $m_\\chi = 1\\, \\mathrm{GeV}$, results in a decrease of the threshold gravitational mass $M_{tot}^{durca}$ for having direct URCA processes and fast stellar cooling. Finally we study the stability of dark matter admixed neutron stars with respect to radial perturbations.","sentences":["We study the effects of dark matter on the structural properties of neutron stars.","In particular we investigate how the presence of a dark matter component influences the mass-radius relation, the value of the maximum mass of a neutron star and others stellar properties.","To model ordinary matter we use a state-of-the-art equation of state of $\\beta$-stable nuclear matter obtained using the Brueckner-Hartree-Fock quantum many-body approach starting from two-body and three-body nuclear interactions derived from chiral effective field theory.","The dark matter component of the star is modeled as a non-self-annihilating system of spin $1/2$ fermions and its equation of state as an ideal relativistic Fermi gas.","The equilibrium configurations of these dark matter admixed neutron stars (DANS) are calculated by solving a generalization of the Tolman-Oppenheimer-Volkoff equations to the case where the system consists of two perfect fluids interacting solely through gravity.","We find that, depending on the dark matter particle mass $m_\\chi$, one can have somehow opposite effects on the stellar properties.","In the case $m_\\chi = 1\\, \\mathrm{GeV}$, the stellar gravitational maximum mass $M_{max}$ decreases, whereas in the case $m_\\chi = 0.1\\, \\mathrm{GeV}$, $M_{max}$ increases with respect to the maximum mass of ordinary neutron stars.","We also show that the presence of dark matter has indirect sizeable effect on the proton fraction in the ordinary matter fluid and, in the case $m_\\chi = 1\\, \\mathrm{GeV}$, results in a decrease of the threshold gravitational mass $M_{tot}^{durca}$ for having direct URCA processes and fast stellar cooling.","Finally we study the stability of dark matter admixed neutron stars with respect to radial perturbations."],"url":"http://arxiv.org/abs/2405.19251v1","category":"astro-ph.HE"}
{"created":"2024-05-29 16:33:50","title":"Kotlin ML Pack: Technical Report","abstract":"In this technical report, we present three novel datasets of Kotlin code: KStack, KStack-clean, and KExercises. We also describe the results of fine-tuning CodeLlama and DeepSeek models on this data. Additionally, we present a version of the HumanEval benchmark rewritten by human experts into Kotlin - both the solutions and the tests. Our results demonstrate that small, high-quality datasets (KStack-clean and KExercises) can significantly improve model performance on code generation tasks, achieving up to a 16-point increase in pass rate on the HumanEval benchmark. Lastly, we discuss potential future work in the field of improving language modeling for Kotlin, including the use of static analysis tools in the learning process and the introduction of more intricate and realistic benchmarks.","sentences":["In this technical report, we present three novel datasets of Kotlin code: KStack, KStack-clean, and KExercises.","We also describe the results of fine-tuning CodeLlama and DeepSeek models on this data.","Additionally, we present a version of the HumanEval benchmark rewritten by human experts into Kotlin - both the solutions and the tests.","Our results demonstrate that small, high-quality datasets (KStack-clean and KExercises) can significantly improve model performance on code generation tasks, achieving up to a 16-point increase in pass rate on the HumanEval benchmark.","Lastly, we discuss potential future work in the field of improving language modeling for Kotlin, including the use of static analysis tools in the learning process and the introduction of more intricate and realistic benchmarks."],"url":"http://arxiv.org/abs/2405.19250v1","category":"cs.SE"}
{"created":"2024-05-29 16:24:38","title":"Challenge-Device-Synthesis: A multi-disciplinary approach for the development of social innovation competences for students of Artificial Intelligence","abstract":"The advent of Artificial Intelligence is expected to imply profound changes in the short-term. It is therefore imperative for Academia, and particularly for the Computer Science scope, to develop cross-disciplinary tools that bond AI developments to their social dimension. To this aim, we introduce the Challenge-Device-Synthesis methodology (CDS), in which a specific challenge is presented to the students of AI, who are required to develop a device as a solution for the challenge. The device becomes the object of study for the different dimensions of social transformation, and the conclusions addressed by the students during the discussion around the device are presented in a synthesis piece in the shape of a 10-page scientific paper. The latter is evaluated taking into account both the depth of analysis and the level to which it genuinely reflects the social transformations associated with the proposed AI-based device. We provide data obtained during the pilot for the implementation phase of CDS within the subject of Social Innovation, a 6-ECTS subject from the 6th semester of the Degree of Artificial Intelligence, UAB-Barcelona. We provide details on temporalisation, task distribution, methodological tools used and assessment delivery procedure, as well as qualitative analysis of the results obtained.","sentences":["The advent of Artificial Intelligence is expected to imply profound changes in the short-term.","It is therefore imperative for Academia, and particularly for the Computer Science scope, to develop cross-disciplinary tools that bond AI developments to their social dimension.","To this aim, we introduce the Challenge-Device-Synthesis methodology (CDS), in which a specific challenge is presented to the students of AI, who are required to develop a device as a solution for the challenge.","The device becomes the object of study for the different dimensions of social transformation, and the conclusions addressed by the students during the discussion around the device are presented in a synthesis piece in the shape of a 10-page scientific paper.","The latter is evaluated taking into account both the depth of analysis and the level to which it genuinely reflects the social transformations associated with the proposed AI-based device.","We provide data obtained during the pilot for the implementation phase of CDS within the subject of Social Innovation, a 6-ECTS subject from the 6th semester of the Degree of Artificial Intelligence, UAB-Barcelona.","We provide details on temporalisation, task distribution, methodological tools used and assessment delivery procedure, as well as qualitative analysis of the results obtained."],"url":"http://arxiv.org/abs/2405.19243v1","category":"cs.AI"}
{"created":"2024-05-29 16:24:16","title":"Modeling public opinion control by a charismatic leader","abstract":"We study the average long-time behavior of the binary opinions of a social group with peer-to-peer interactions under the influence of an external bias and a persuadable leader, a strongly-biased agent with a dynamic opinion with the intention of spreading it across the system. We use a generalized, fully-connected Ising model, with each spin representing the binary opinion of an agent at a given time and a single, super spin representing the opinion of the leader. External fields and interaction constants model the opinion bias and peer-to-peer interactions, respectively, while the temperature $T$ models an idealized social climate, representing an authoritarian regime if $T$ is low or a liberal one if $T$ is high. We derive a mean-field solution for the average magnetization $m$, the \"social mood\", and investigate how $m$ and the super spin magnetization vary as a function of $T$. We find that, depending on the initial conditions, due to the presence of metastable states, the sign of the average magnetization depends on the temperature. Finally, we verify that this effect is also present even if we consider only nearest-neighbor interactions within the social group.","sentences":["We study the average long-time behavior of the binary opinions of a social group with peer-to-peer interactions under the influence of an external bias and a persuadable leader, a strongly-biased agent with a dynamic opinion with the intention of spreading it across the system.","We use a generalized, fully-connected Ising model, with each spin representing the binary opinion of an agent at a given time and a single, super spin representing the opinion of the leader.","External fields and interaction constants model the opinion bias and peer-to-peer interactions, respectively, while the temperature $T$ models an idealized social climate, representing an authoritarian regime if $T$ is low or a liberal one if $T$ is high.","We derive a mean-field solution for the average magnetization $m$, the \"social mood\", and investigate how $m$ and the super spin magnetization vary as a function of $T$. We find that, depending on the initial conditions, due to the presence of metastable states, the sign of the average magnetization depends on the temperature.","Finally, we verify that this effect is also present even if we consider only nearest-neighbor interactions within the social group."],"url":"http://arxiv.org/abs/2405.19242v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-29 16:22:50","title":"Intermediate-mass-ratio inspirals with general dynamical friction in dark matter minispikes","abstract":"The intermediate-mass-ratio inspirals (IMRIs) may be surrounded by dark matter (DM) minispikes. The dynamical friction from these DM minispike structures can affect the dynamics and the gravitational wave (GW) emission of the IMRIs. We analyze the effects of general dynamical friction, with a particular contribution from DM particles moving faster than the stellar-mass black hole in an eccentric IMRI. The results show that the dynamical friction caused by these DM particles tends to eccentricify the orbit, and the general dynamical friction is able to increase the eccentricity. We also analyze the effects of general dynamical friction on the GW characteristic strain. The results indicate that the peak value of the characteristic strain occurs at higher frequencies as the power law index of DM minispike $\\gamma_\\mathrm{sp}$ increases. For the first time, a general analytical relation between the frequency peak value of characteristic strain of GWs and $\\gamma_\\mathrm{sp}$ is established. Using the analytical relation, the presence of DM and its halo density may be determined potentially from future GW data.","sentences":["The intermediate-mass-ratio inspirals (IMRIs) may be surrounded by dark matter (DM) minispikes.","The dynamical friction from these DM minispike structures can affect the dynamics and the gravitational wave (GW) emission of the IMRIs.","We analyze the effects of general dynamical friction, with a particular contribution from DM particles moving faster than the stellar-mass black hole in an eccentric IMRI.","The results show that the dynamical friction caused by these DM particles tends to eccentricify the orbit, and the general dynamical friction is able to increase the eccentricity.","We also analyze the effects of general dynamical friction on the GW characteristic strain.","The results indicate that the peak value of the characteristic strain occurs at higher frequencies as the power law index of DM minispike $\\gamma_\\mathrm{sp}$ increases.","For the first time, a general analytical relation between the frequency peak value of characteristic strain of GWs and $\\gamma_\\mathrm{sp}$ is established.","Using the analytical relation, the presence of DM and its halo density may be determined potentially from future GW data."],"url":"http://arxiv.org/abs/2405.19240v1","category":"astro-ph.HE"}
{"created":"2024-05-29 16:20:51","title":"Explanation-based Belief Revision: Moving Beyond Minimalism to Explanatory Understanding","abstract":"In belief revision, agents typically modify their beliefs when they receive some new piece of information that is in conflict with them. The guiding principle behind most belief revision frameworks is that of minimalism, which advocates minimal changes to existing beliefs. However, minimalism may not necessarily capture the nuanced ways in which human agents reevaluate and modify their beliefs. In contrast, the explanatory hypothesis indicates that people are inherently driven to seek explanations for inconsistencies, thereby striving for explanatory coherence rather than minimal changes when revising beliefs. Our contribution in this paper is two-fold. Motivated by the explanatory hypothesis, we first present a novel, yet simple belief revision operator that, given a belief base and an explanation for an explanandum, it revises the belief bases in a manner that preserves the explanandum and is not necessarily minimal. We call this operator explanation-based belief revision. Second, we conduct two human-subject studies to empirically validate our approach and investigate belief revision behavior in real-world scenarios. Our findings support the explanatory hypothesis and provide insights into the strategies people employ when resolving inconsistencies.","sentences":["In belief revision, agents typically modify their beliefs when they receive some new piece of information that is in conflict with them.","The guiding principle behind most belief revision frameworks is that of minimalism, which advocates minimal changes to existing beliefs.","However, minimalism may not necessarily capture the nuanced ways in which human agents reevaluate and modify their beliefs.","In contrast, the explanatory hypothesis indicates that people are inherently driven to seek explanations for inconsistencies, thereby striving for explanatory coherence rather than minimal changes when revising beliefs.","Our contribution in this paper is two-fold.","Motivated by the explanatory hypothesis, we first present a novel, yet simple belief revision operator that, given a belief base and an explanation for an explanandum, it revises the belief bases in a manner that preserves the explanandum and is not necessarily minimal.","We call this operator explanation-based belief revision.","Second, we conduct two human-subject studies to empirically validate our approach and investigate belief revision behavior in real-world scenarios.","Our findings support the explanatory hypothesis and provide insights into the strategies people employ when resolving inconsistencies."],"url":"http://arxiv.org/abs/2405.19238v1","category":"cs.AI"}
{"created":"2024-05-29 16:19:37","title":"ConceptPrune: Concept Editing in Diffusion Models via Skilled Neuron Pruning","abstract":"While large-scale text-to-image diffusion models have demonstrated impressive image-generation capabilities, there are significant concerns about their potential misuse for generating unsafe content, violating copyright, and perpetuating societal biases. Recently, the text-to-image generation community has begun addressing these concerns by editing or unlearning undesired concepts from pre-trained models. However, these methods often involve data-intensive and inefficient fine-tuning or utilize various forms of token remapping, rendering them susceptible to adversarial jailbreaks. In this paper, we present a simple and effective training-free approach, ConceptPrune, wherein we first identify critical regions within pre-trained models responsible for generating undesirable concepts, thereby facilitating straightforward concept unlearning via weight pruning. Experiments across a range of concepts including artistic styles, nudity, object erasure, and gender debiasing demonstrate that target concepts can be efficiently erased by pruning a tiny fraction, approximately 0.12% of total weights, enabling multi-concept erasure and robustness against various white-box and black-box adversarial attacks.","sentences":["While large-scale text-to-image diffusion models have demonstrated impressive image-generation capabilities, there are significant concerns about their potential misuse for generating unsafe content, violating copyright, and perpetuating societal biases.","Recently, the text-to-image generation community has begun addressing these concerns by editing or unlearning undesired concepts from pre-trained models.","However, these methods often involve data-intensive and inefficient fine-tuning or utilize various forms of token remapping, rendering them susceptible to adversarial jailbreaks.","In this paper, we present a simple and effective training-free approach, ConceptPrune, wherein we first identify critical regions within pre-trained models responsible for generating undesirable concepts, thereby facilitating straightforward concept unlearning via weight pruning.","Experiments across a range of concepts including artistic styles, nudity, object erasure, and gender debiasing demonstrate that target concepts can be efficiently erased by pruning a tiny fraction, approximately 0.12% of total weights, enabling multi-concept erasure and robustness against various white-box and black-box adversarial attacks."],"url":"http://arxiv.org/abs/2405.19237v1","category":"cs.CV"}
{"created":"2024-05-29 16:17:19","title":"Exploring the impact of traffic signal control and connected and automated vehicles on intersections safety: A deep reinforcement learning approach","abstract":"In transportation networks, intersections pose significant risks of collisions due to conflicting movements of vehicles approaching from different directions. To address this issue, various tools can exert influence on traffic safety both directly and indirectly. This study focuses on investigating the impact of adaptive signal control and connected and automated vehicles (CAVs) on intersection safety using a deep reinforcement learning approach. The objective is to assess the individual and combined effects of CAVs and adaptive traffic signal control on traffic safety, considering rear-end and crossing conflicts. The study employs a Deep Q Network (DQN) to regulate traffic signals and driving behaviors of both CAVs and Human Drive Vehicles (HDVs), and uses Time To Collision (TTC) metric to evaluate safety. The findings demonstrate a significant reduction in rear-end and crossing conflicts through the combined implementation of CAVs and DQNs-based traffic signal control. Additionally, the long-term positive effects of CAVs on safety are similar to the short-term effects of combined CAVs and DQNs-based traffic signal control. Overall, the study emphasizes the potential benefits of integrating CAVs and adaptive traffic signal control approaches in order to enhance traffic safety. The findings of this study could provide valuable insights for city officials and transportation authorities in developing effective strategies to improve safety at signalized intersections.","sentences":["In transportation networks, intersections pose significant risks of collisions due to conflicting movements of vehicles approaching from different directions.","To address this issue, various tools can exert influence on traffic safety both directly and indirectly.","This study focuses on investigating the impact of adaptive signal control and connected and automated vehicles (CAVs) on intersection safety using a deep reinforcement learning approach.","The objective is to assess the individual and combined effects of CAVs and adaptive traffic signal control on traffic safety, considering rear-end and crossing conflicts.","The study employs a Deep Q Network (DQN) to regulate traffic signals and driving behaviors of both CAVs and Human Drive Vehicles (HDVs), and uses Time To Collision (TTC) metric to evaluate safety.","The findings demonstrate a significant reduction in rear-end and crossing conflicts through the combined implementation of CAVs and DQNs-based traffic signal control.","Additionally, the long-term positive effects of CAVs on safety are similar to the short-term effects of combined CAVs and DQNs-based traffic signal control.","Overall, the study emphasizes the potential benefits of integrating CAVs and adaptive traffic signal control approaches in order to enhance traffic safety.","The findings of this study could provide valuable insights for city officials and transportation authorities in developing effective strategies to improve safety at signalized intersections."],"url":"http://arxiv.org/abs/2405.19236v1","category":"cs.AI"}
{"created":"2024-05-29 16:12:14","title":"DiPPeST: Diffusion-based Path Planner for Synthesizing Trajectories Applied on Quadruped Robots","abstract":"We present DiPPeST, a novel image and goal conditioned diffusion-based trajectory generator for quadrupedal robot path planning. DiPPeST is a zero-shot adaptation of our previously introduced diffusion-based 2D global trajectory generator (DiPPeR). The introduced system incorporates a novel strategy for local real-time path refinements, that is reactive to camera input, without requiring any further training, image processing, or environment interpretation techniques. DiPPeST achieves 92% success rate in obstacle avoidance for nominal environments and an average of 88% success rate when tested in environments that are up to 3.5 times more complex in pixel variation than DiPPeR. A visual-servoing framework is developed to allow for real-world execution, tested on the quadruped robot, achieving 80% success rate in different environments and showcasing improved behavior than complex state-of-the-art local planners, in narrow environments.","sentences":["We present DiPPeST, a novel image and goal conditioned diffusion-based trajectory generator for quadrupedal robot path planning.","DiPPeST is a zero-shot adaptation of our previously introduced diffusion-based 2D global trajectory generator (DiPPeR).","The introduced system incorporates a novel strategy for local real-time path refinements, that is reactive to camera input, without requiring any further training, image processing, or environment interpretation techniques.","DiPPeST achieves 92% success rate in obstacle avoidance for nominal environments and an average of 88% success rate when tested in environments that are up to 3.5 times more complex in pixel variation than DiPPeR.","A visual-servoing framework is developed to allow for real-world execution, tested on the quadruped robot, achieving 80% success rate in different environments and showcasing improved behavior than complex state-of-the-art local planners, in narrow environments."],"url":"http://arxiv.org/abs/2405.19232v1","category":"cs.RO"}
{"created":"2024-05-29 16:07:31","title":"On Generating Monolithic and Model Reconciling Explanations in Probabilistic Scenarios","abstract":"Explanation generation frameworks aim to make AI systems' decisions transparent and understandable to human users. However, generating explanations in uncertain environments characterized by incomplete information and probabilistic models remains a significant challenge. In this paper, we propose a novel framework for generating probabilistic monolithic explanations and model reconciling explanations. Monolithic explanations provide self-contained reasons for an explanandum without considering the agent receiving the explanation, while model reconciling explanations account for the knowledge of the agent receiving the explanation. For monolithic explanations, our approach integrates uncertainty by utilizing probabilistic logic to increase the probability of the explanandum. For model reconciling explanations, we propose a framework that extends the logic-based variant of the model reconciliation problem to account for probabilistic human models, where the goal is to find explanations that increase the probability of the explanandum while minimizing conflicts between the explanation and the probabilistic human model. We introduce explanatory gain and explanatory power as quantitative metrics to assess the quality of these explanations. Further, we present algorithms that exploit the duality between minimal correction sets and minimal unsatisfiable sets to efficiently compute both types of explanations in probabilistic contexts. Extensive experimental evaluations on various benchmarks demonstrate the effectiveness and scalability of our approach in generating explanations under uncertainty.","sentences":["Explanation generation frameworks aim to make AI systems' decisions transparent and understandable to human users.","However, generating explanations in uncertain environments characterized by incomplete information and probabilistic models remains a significant challenge.","In this paper, we propose a novel framework for generating probabilistic monolithic explanations and model reconciling explanations.","Monolithic explanations provide self-contained reasons for an explanandum without considering the agent receiving the explanation, while model reconciling explanations account for the knowledge of the agent receiving the explanation.","For monolithic explanations, our approach integrates uncertainty by utilizing probabilistic logic to increase the probability of the explanandum.","For model reconciling explanations, we propose a framework that extends the logic-based variant of the model reconciliation problem to account for probabilistic human models, where the goal is to find explanations that increase the probability of the explanandum while minimizing conflicts between the explanation and the probabilistic human model.","We introduce explanatory gain and explanatory power as quantitative metrics to assess the quality of these explanations.","Further, we present algorithms that exploit the duality between minimal correction sets and minimal unsatisfiable sets to efficiently compute both types of explanations in probabilistic contexts.","Extensive experimental evaluations on various benchmarks demonstrate the effectiveness and scalability of our approach in generating explanations under uncertainty."],"url":"http://arxiv.org/abs/2405.19229v1","category":"cs.AI"}
{"created":"2024-05-29 16:04:03","title":"A study on the adequacy of common IQA measures for medical images","abstract":"Image quality assessment (IQA) is standard practice in the development stage of novel machine learning algorithms that operate on images. The most commonly used IQA measures have been developed and tested for natural images, but not in the medical setting. Reported inconsistencies arising in medical images are not surprising, as they have different properties than natural images. In this study, we test the applicability of common IQA measures for medical image data by comparing their assessment to manually rated chest X-ray (5 experts) and photoacoustic image data (1 expert). Moreover, we include supplementary studies on grayscale natural images and accelerated brain MRI data. The results of all experiments show a similar outcome in line with previous findings for medical imaging: PSNR and SSIM in the default setting are in the lower range of the result list and HaarPSI outperforms the other tested measures in the overall performance. Also among the top performers in our medical experiments are the full reference measures DISTS, FSIM, LPIPS and MS-SSIM. Generally, the results on natural images yield considerably higher correlations, suggesting that the additional employment of tailored IQA measures for medical imaging algorithms is needed.","sentences":["Image quality assessment (IQA) is standard practice in the development stage of novel machine learning algorithms that operate on images.","The most commonly used IQA measures have been developed and tested for natural images, but not in the medical setting.","Reported inconsistencies arising in medical images are not surprising, as they have different properties than natural images.","In this study, we test the applicability of common IQA measures for medical image data by comparing their assessment to manually rated chest X-ray (5 experts) and photoacoustic image data (1 expert).","Moreover, we include supplementary studies on grayscale natural images and accelerated brain MRI data.","The results of all experiments show a similar outcome in line with previous findings for medical imaging: PSNR and SSIM in the default setting are in the lower range of the result list and HaarPSI outperforms the other tested measures in the overall performance.","Also among the top performers in our medical experiments are the full reference measures DISTS, FSIM, LPIPS and MS-SSIM.","Generally, the results on natural images yield considerably higher correlations, suggesting that the additional employment of tailored IQA measures for medical imaging algorithms is needed."],"url":"http://arxiv.org/abs/2405.19224v1","category":"eess.IV"}
{"created":"2024-05-29 16:00:46","title":"WRDScore: New Metric for Evaluation of Natural Language Generation Models","abstract":"The problem of natural language generation, and, more specifically, method name prediction, faces significant difficulties when proposed models need to be evaluated on test data. Such a metric would need to consider the versatility with which a single method can be named, with respect to both semantics and syntax. Measuring the direct overlap between the predicted and reference (true) sequences will not be able to capture these subtleties. Other existing embedding based metrics either do not measure precision and recall or impose strict unrealistic assumptions on both sequences. To address these issues, we propose a new metric that, on the one hand, is very simple and lightweight, and, on the other hand, is able to calculate precision and recall without resorting to any assumptions while obtaining good performance with respect to the human judgement.","sentences":["The problem of natural language generation, and, more specifically, method name prediction, faces significant difficulties when proposed models need to be evaluated on test data.","Such a metric would need to consider the versatility with which a single method can be named, with respect to both semantics and syntax.","Measuring the direct overlap between the predicted and reference (true) sequences will not be able to capture these subtleties.","Other existing embedding based metrics either do not measure precision and recall or impose strict unrealistic assumptions on both sequences.","To address these issues, we propose a new metric that, on the one hand, is very simple and lightweight, and, on the other hand, is able to calculate precision and recall without resorting to any assumptions while obtaining good performance with respect to the human judgement."],"url":"http://arxiv.org/abs/2405.19220v1","category":"cs.CL"}
{"created":"2024-05-29 16:00:44","title":"Least multivariate Chebyshev polynomials on diagonally determined domains","abstract":"We consider a new multivariate generalization of the classical monic (univariate) Chebyshev polynomial that minimizes the uniform norm on the interval $[-1,1]$. Let $\\Pi^*_n$ be the subset of polynomials of degree at most $n$ in $d$ variables, whose homogeneous part of degree $n$ has coefficients summing up to $1$. The problem is determining a polynomial in $\\Pi^*_n$ with the smallest uniform norm on a domain $\\Omega$, which we call a least Chebyshev polynomial (associated with $\\Omega$). Our main result solves the problem for $\\Omega$ belonging to a non-trivial class of domains, defined by a property of its diagonal, and establishes the remarkable result that a least Chebyshev polynomial can be given via the classical, univariate, Chebyshev polynomial. In particular, the solution can be independent of the dimension. The result is valid for fairly general domains that can be non-convex and highly irregular.","sentences":["We consider a new multivariate generalization of the classical monic (univariate) Chebyshev polynomial that minimizes the uniform norm on the interval $","[-1,1]$. Let $\\Pi^*_n$ be the subset of polynomials of degree at most $n$ in $d$ variables, whose homogeneous part of degree $n$ has coefficients summing up to $1$.","The problem is determining a polynomial in $\\Pi^*_n$ with the smallest uniform norm on a domain $\\Omega$, which we call a least Chebyshev polynomial (associated with $\\Omega$).","Our main result solves the problem for $\\Omega$ belonging to a non-trivial class of domains, defined by a property of its diagonal, and establishes the remarkable result that a least Chebyshev polynomial can be given via the classical, univariate, Chebyshev polynomial.","In particular, the solution can be independent of the dimension.","The result is valid for fairly general domains that can be non-convex and highly irregular."],"url":"http://arxiv.org/abs/2405.19219v1","category":"math.OC"}
{"created":"2024-05-29 15:56:33","title":"HawkVision: Low-Latency Modeless Edge AI Serving","abstract":"The trend of modeless ML inference is increasingly growing in popularity as it hides the complexity of model inference from users and caters to diverse user and application accuracy requirements. Previous work mostly focuses on modeless inference in data centers. To provide low-latency inference, in this paper, we promote modeless inference at the edge. The edge environment introduces additional challenges related to low power consumption, limited device memory, and volatile network environments.   To address these challenges, we propose HawkVision, which provides low-latency modeless serving of vision DNNs. HawkVision leverages a two-layer edge-DC architecture that employs confidence scaling to reduce the number of model options while meeting diverse accuracy requirements. It also supports lossy inference under volatile network environments. Our experimental results show that HawkVision outperforms current serving systems by up to 1.6X in P99 latency for providing modeless service. Our FPGA prototype demonstrates similar performance at certain accuracy levels with up to a 3.34X reduction in power consumption.","sentences":["The trend of modeless ML inference is increasingly growing in popularity as it hides the complexity of model inference from users and caters to diverse user and application accuracy requirements.","Previous work mostly focuses on modeless inference in data centers.","To provide low-latency inference, in this paper, we promote modeless inference at the edge.","The edge environment introduces additional challenges related to low power consumption, limited device memory, and volatile network environments.   ","To address these challenges, we propose HawkVision, which provides low-latency modeless serving of vision DNNs.","HawkVision leverages a two-layer edge-DC architecture that employs confidence scaling to reduce the number of model options while meeting diverse accuracy requirements.","It also supports lossy inference under volatile network environments.","Our experimental results show that HawkVision outperforms current serving systems by up to 1.6X in P99 latency for providing modeless service.","Our FPGA prototype demonstrates similar performance at certain accuracy levels with up to a 3.34X reduction in power consumption."],"url":"http://arxiv.org/abs/2405.19213v1","category":"eess.SY"}
{"created":"2024-05-29 15:54:03","title":"Partial Information Decomposition for Data Interpretability and Feature Selection","abstract":"In this paper, we introduce Partial Information Decomposition of Features (PIDF), a new paradigm for simultaneous data interpretability and feature selection. Contrary to traditional methods that assign a single importance value, our approach is based on three metrics per feature: the mutual information shared with the target variable, the feature's contribution to synergistic information, and the amount of this information that is redundant. In particular, we develop a novel procedure based on these three metrics, which reveals not only how features are correlated with the target but also the additional and overlapping information provided by considering them in combination with other features. We extensively evaluate PIDF using both synthetic and real-world data, demonstrating its potential applications and effectiveness, by considering case studies from genetics and neuroscience.","sentences":["In this paper, we introduce Partial Information Decomposition of Features (PIDF), a new paradigm for simultaneous data interpretability and feature selection.","Contrary to traditional methods that assign a single importance value, our approach is based on three metrics per feature: the mutual information shared with the target variable, the feature's contribution to synergistic information, and the amount of this information that is redundant.","In particular, we develop a novel procedure based on these three metrics, which reveals not only how features are correlated with the target but also the additional and overlapping information provided by considering them in combination with other features.","We extensively evaluate PIDF using both synthetic and real-world data, demonstrating its potential applications and effectiveness, by considering case studies from genetics and neuroscience."],"url":"http://arxiv.org/abs/2405.19212v1","category":"cs.LG"}
{"created":"2024-05-29 15:51:40","title":"Gradient Guided Hypotheses: A unified solution to enable machine learning models on scarce and noisy data regimes","abstract":"Ensuring high-quality data is paramount for maximizing the performance of machine learning models and business intelligence systems. However, challenges in data quality, including noise in data capture, missing records, limited data production, and confounding variables, significantly constrain the potential performance of these systems. In this study, we propose an architecture-agnostic algorithm, Gradient Guided Hypotheses (GGH), designed to address these challenges. GGH analyses gradients from hypotheses as a proxy of distinct and possibly contradictory patterns in the data. This framework entails an additional step in machine learning training, where gradients can be included or excluded from backpropagation. In this manner, missing and noisy data are addressed through a unified solution that perceives both challenges as facets of the same overarching issue: the propagation of erroneous information. Experimental validation of GGH is conducted using real-world open-source datasets, where records with missing rates of up to 98.5% are simulated. Comparative analysis with state-of-the-art imputation methods demonstrates a substantial improvement in model performance achieved by GGH. Specifically in very high scarcity regimes, GGH was found to be the only viable solution. Additionally, GGH's noise detection capabilities are showcased by introducing simulated noise into the datasets and observing enhanced model performance after filtering out the noisy data. This study presents GGH as a promising solution for improving data quality and model performance in various applications.","sentences":["Ensuring high-quality data is paramount for maximizing the performance of machine learning models and business intelligence systems.","However, challenges in data quality, including noise in data capture, missing records, limited data production, and confounding variables, significantly constrain the potential performance of these systems.","In this study, we propose an architecture-agnostic algorithm, Gradient Guided Hypotheses (GGH), designed to address these challenges.","GGH analyses gradients from hypotheses as a proxy of distinct and possibly contradictory patterns in the data.","This framework entails an additional step in machine learning training, where gradients can be included or excluded from backpropagation.","In this manner, missing and noisy data are addressed through a unified solution that perceives both challenges as facets of the same overarching issue: the propagation of erroneous information.","Experimental validation of GGH is conducted using real-world open-source datasets, where records with missing rates of up to 98.5% are simulated.","Comparative analysis with state-of-the-art imputation methods demonstrates a substantial improvement in model performance achieved by GGH.","Specifically in very high scarcity regimes, GGH was found to be the only viable solution.","Additionally, GGH's noise detection capabilities are showcased by introducing simulated noise into the datasets and observing enhanced model performance after filtering out the noisy data.","This study presents GGH as a promising solution for improving data quality and model performance in various applications."],"url":"http://arxiv.org/abs/2405.19210v1","category":"cs.LG"}
{"created":"2024-05-29 15:49:09","title":"VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos","abstract":"Video-language understanding tasks have focused on short video clips, often struggling with long-form video understanding tasks. Recently, many long video-language understanding approaches have leveraged the reasoning capabilities of Large Language Models (LLMs) to perform long video QA, transforming videos into densely sampled frame captions, and asking LLMs to respond to text queries over captions. However, the frames used for captioning are often redundant and contain irrelevant information, making dense sampling inefficient, and ignoring the fact that video QA requires varying levels of granularity, with some video segments being highly relevant to the question (needing more fine-grained detail) while others being less relevant. Thus, these LLM-based approaches are prone to missing information and operate on large numbers of irrelevant captions, lowering both performance and efficiency. To address these issues, we introduce VideoTree, a query-adaptive and hierarchical framework for long-video understanding with LLMs. VideoTree dynamically extracts query-related information from a video and builds a tree-based representation for LLM reasoning. First, VideoTree adaptively selects frames for captioning by iteratively clustering frames based on their visual features and scoring clusters using their relevance to the query. Second, it organizes visual clusters into a query-adaptive and hierarchical tree structure; the tree encodes varying levels of granularity, with higher resolution on relevant segments. Finally, VideoTree produces an answer by traversing the tree's keyframes and passing their captions to an LLM answerer. Our method improves both reasoning accuracy and efficiency compared to existing methods: VideoTree achieves a 7.0%, 2.2%, and 2.7% accuracy gain over baselines on the EgoSchema, NExT-QA, and IntentQA benchmarks, respectively, while reducing inference time by 40%.","sentences":["Video-language understanding tasks have focused on short video clips, often struggling with long-form video understanding tasks.","Recently, many long video-language understanding approaches have leveraged the reasoning capabilities of Large Language Models (LLMs) to perform long video QA, transforming videos into densely sampled frame captions, and asking LLMs to respond to text queries over captions.","However, the frames used for captioning are often redundant and contain irrelevant information, making dense sampling inefficient, and ignoring the fact that video QA requires varying levels of granularity, with some video segments being highly relevant to the question (needing more fine-grained detail) while others being less relevant.","Thus, these LLM-based approaches are prone to missing information and operate on large numbers of irrelevant captions, lowering both performance and efficiency.","To address these issues, we introduce VideoTree, a query-adaptive and hierarchical framework for long-video understanding with LLMs.","VideoTree dynamically extracts query-related information from a video and builds a tree-based representation for LLM reasoning.","First, VideoTree adaptively selects frames for captioning by iteratively clustering frames based on their visual features and scoring clusters using their relevance to the query.","Second, it organizes visual clusters into a query-adaptive and hierarchical tree structure; the tree encodes varying levels of granularity, with higher resolution on relevant segments.","Finally, VideoTree produces an answer by traversing the tree's keyframes and passing their captions to an LLM answerer.","Our method improves both reasoning accuracy and efficiency compared to existing methods: VideoTree achieves a 7.0%, 2.2%, and 2.7% accuracy gain over baselines on the EgoSchema, NExT-QA, and IntentQA benchmarks, respectively, while reducing inference time by 40%."],"url":"http://arxiv.org/abs/2405.19209v1","category":"cs.CV"}
{"created":"2024-05-29 15:49:03","title":"Quasimetric spaces with few lines","abstract":"Chen and Chv\\'atal conjectured in 2008 that in any finite metric space either there is a line containing all the points - a universal line -, or the number of lines is at least the number of points. This is a generalization of a classical result due to Erd\\H{o}s that says that a set of $n$ non-collinear points in the Euclidean plane defines at least $n$ different lines.   A line of a metric space with metric $\\rho$ is defined in terms of a notion called the betweenness of the space which is the set of all triples $(x,z,y)$ such that $\\rho(x,y)=\\rho(x,z)+\\rho(z,y)$.   In this work we prove that for each $n\\geq 4$ there are $p_3(n)$ non isomorphic betweennesses arising from \\emph{quasimetric} spaces with $n$ points, without universal lines and with exactly 3 lines, where $p_3(n)$ is the number of partitions of an integer $n$ into three parts. We also prove that for $n\\geq 5$, there are $2p_3(n-1)$ non isomorphic betweennesses arising from quasimetric spaces on $n$ points, without universal lines and with exactly 4 lines. Here two betweennesses are isomorphic if they are isomorphic as relational structures.   None of the betweennesses mentioned above is metric which implies that Chen and Chv\\'atal's conjecture is valid for metric spaces with at most five points.","sentences":["Chen and Chv\\'atal conjectured in 2008 that in any finite metric space either there is a line containing all the points - a universal line -, or the number of lines is at least the number of points.","This is a generalization of a classical result due to Erd\\H{o}s that says that a set of $n$ non-collinear points in the Euclidean plane defines at least $n$ different lines.   ","A line of a metric space with metric $\\rho$ is defined in terms of a notion called the betweenness of the space which is the set of all triples $(x,z,y)$ such that $\\rho(x,y)=\\rho(x,z)+\\rho(z,y)$.   In this work we prove that for each $n\\geq 4$ there are $p_3(n)$ non isomorphic betweennesses arising from \\emph{quasimetric} spaces with $n$ points, without universal lines and with exactly 3 lines, where $p_3(n)$ is the number of partitions of an integer $n$ into three parts.","We also prove that for $n\\geq 5$, there are $2p_3(n-1)$ non isomorphic betweennesses arising from quasimetric spaces on $n$ points, without universal lines and with exactly 4 lines.","Here two betweennesses are isomorphic if they are isomorphic as relational structures.   ","None of the betweennesses mentioned above is metric which implies that Chen and Chv\\'atal's conjecture is valid for metric spaces with at most five points."],"url":"http://arxiv.org/abs/2405.19208v1","category":"math.CO"}
{"created":"2024-05-29 15:47:57","title":"A Multi-Source Retrieval Question Answering Framework Based on RAG","abstract":"With the rapid development of large-scale language models, Retrieval-Augmented Generation (RAG) has been widely adopted. However, existing RAG paradigms are inevitably influenced by erroneous retrieval information, thereby reducing the reliability and correctness of generated results. Therefore, to improve the relevance of retrieval information, this study proposes a method that replaces traditional retrievers with GPT-3.5, leveraging its vast corpus knowledge to generate retrieval information. We also propose a web retrieval based method to implement fine-grained knowledge retrieval, Utilizing the powerful reasoning capability of GPT-3.5 to realize semantic partitioning of problem.In order to mitigate the illusion of GPT retrieval and reduce noise in Web retrieval,we proposes a multi-source retrieval framework, named MSRAG, which combines GPT retrieval with web retrieval. Experiments on multiple knowledge-intensive QA datasets demonstrate that the proposed framework in this study performs better than existing RAG framework in enhancing the overall efficiency and accuracy of QA systems.","sentences":["With the rapid development of large-scale language models, Retrieval-Augmented Generation (RAG) has been widely adopted.","However, existing RAG paradigms are inevitably influenced by erroneous retrieval information, thereby reducing the reliability and correctness of generated results.","Therefore, to improve the relevance of retrieval information, this study proposes a method that replaces traditional retrievers with GPT-3.5, leveraging its vast corpus knowledge to generate retrieval information.","We also propose a web retrieval based method to implement fine-grained knowledge retrieval, Utilizing the powerful reasoning capability of GPT-3.5 to realize semantic partitioning of problem.","In order to mitigate the illusion of GPT retrieval and reduce noise in Web retrieval,we proposes a multi-source retrieval framework, named MSRAG, which combines GPT retrieval with web retrieval.","Experiments on multiple knowledge-intensive QA datasets demonstrate that the proposed framework in this study performs better than existing RAG framework in enhancing the overall efficiency and accuracy of QA systems."],"url":"http://arxiv.org/abs/2405.19207v1","category":"cs.IR"}
{"created":"2024-05-29 15:47:35","title":"Matrix Manifold Neural Networks++","abstract":"Deep neural networks (DNNs) on Riemannian manifolds have garnered increasing interest in various applied areas. For instance, DNNs on spherical and hyperbolic manifolds have been designed to solve a wide range of computer vision and nature language processing tasks. One of the key factors that contribute to the success of these networks is that spherical and hyperbolic manifolds have the rich algebraic structures of gyrogroups and gyrovector spaces. This enables principled and effective generalizations of the most successful DNNs to these manifolds. Recently, some works have shown that many concepts in the theory of gyrogroups and gyrovector spaces can also be generalized to matrix manifolds such as Symmetric Positive Definite (SPD) and Grassmann manifolds. As a result, some building blocks for SPD and Grassmann neural networks, e.g., isometric models and multinomial logistic regression (MLR) can be derived in a way that is fully analogous to their spherical and hyperbolic counterparts. Building upon these works, we design fully-connected (FC) and convolutional layers for SPD neural networks. We also develop MLR on Symmetric Positive Semi-definite (SPSD) manifolds, and propose a method for performing backpropagation with the Grassmann logarithmic map in the projector perspective. We demonstrate the effectiveness of the proposed approach in the human action recognition and node classification tasks.","sentences":["Deep neural networks (DNNs) on Riemannian manifolds have garnered increasing interest in various applied areas.","For instance, DNNs on spherical and hyperbolic manifolds have been designed to solve a wide range of computer vision and nature language processing tasks.","One of the key factors that contribute to the success of these networks is that spherical and hyperbolic manifolds have the rich algebraic structures of gyrogroups and gyrovector spaces.","This enables principled and effective generalizations of the most successful DNNs to these manifolds.","Recently, some works have shown that many concepts in the theory of gyrogroups and gyrovector spaces can also be generalized to matrix manifolds such as Symmetric Positive Definite (SPD) and Grassmann manifolds.","As a result, some building blocks for SPD and Grassmann neural networks, e.g., isometric models and multinomial logistic regression (MLR) can be derived in a way that is fully analogous to their spherical and hyperbolic counterparts.","Building upon these works, we design fully-connected (FC) and convolutional layers for SPD neural networks.","We also develop MLR on Symmetric Positive Semi-definite (SPSD) manifolds, and propose a method for performing backpropagation with the Grassmann logarithmic map in the projector perspective.","We demonstrate the effectiveness of the proposed approach in the human action recognition and node classification tasks."],"url":"http://arxiv.org/abs/2405.19206v1","category":"stat.ML"}
{"created":"2024-05-29 15:43:49","title":"$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation","abstract":"This paper aims to introduce 3D Gaussian for efficient, expressive, and editable digital avatar generation. This task faces two major challenges: (1) The unstructured nature of 3D Gaussian makes it incompatible with current generation pipelines; (2) the expressive animation of 3D Gaussian in a generative setting that involves training with multiple subjects remains unexplored. In this paper, we propose a novel avatar generation method named $E^3$Gen, to effectively address these challenges. First, we propose a novel generative UV features plane representation that encodes unstructured 3D Gaussian onto a structured 2D UV space defined by the SMPL-X parametric model. This novel representation not only preserves the representation ability of the original 3D Gaussian but also introduces a shared structure among subjects to enable generative learning of the diffusion model. To tackle the second challenge, we propose a part-aware deformation module to achieve robust and accurate full-body expressive pose control. Extensive experiments demonstrate that our method achieves superior performance in avatar generation and enables expressive full-body pose control and editing.","sentences":["This paper aims to introduce 3D Gaussian for efficient, expressive, and editable digital avatar generation.","This task faces two major challenges: (1) The unstructured nature of 3D Gaussian makes it incompatible with current generation pipelines; (2) the expressive animation of 3D Gaussian in a generative setting that involves training with multiple subjects remains unexplored.","In this paper, we propose a novel avatar generation method named $E^3$Gen, to effectively address these challenges.","First, we propose a novel generative UV features plane representation that encodes unstructured 3D Gaussian onto a structured 2D UV space defined by the SMPL-X parametric model.","This novel representation not only preserves the representation ability of the original 3D Gaussian but also introduces a shared structure among subjects to enable generative learning of the diffusion model.","To tackle the second challenge, we propose a part-aware deformation module to achieve robust and accurate full-body expressive pose control.","Extensive experiments demonstrate that our method achieves superior performance in avatar generation and enables expressive full-body pose control and editing."],"url":"http://arxiv.org/abs/2405.19203v1","category":"cs.CV"}
{"created":"2024-05-29 15:42:10","title":"Vulnerable Road User Detection and Safety Enhancement: A Comprehensive Survey","abstract":"Traffic incidents involving vulnerable road users (VRUs) constitute a significant proportion of global road accidents. Advances in traffic communication ecosystems, coupled with sophisticated signal processing and machine learning techniques, have facilitated the utilization of data from diverse sensors. Despite these advancements and the availability of extensive datasets, substantial progress is required to mitigate traffic casualties. This paper provides a comprehensive survey of state-of-the-art technologies and methodologies to enhance the safety of VRUs. The study delves into the communication networks between vehicles and VRUs, emphasizing the integration of advanced sensors and the availability of relevant datasets. It explores preprocessing techniques and data fusion methods to enhance sensor data quality. Furthermore, our study assesses critical simulation environments essential for developing and testing VRU safety systems. Our research also highlights recent advances in VRU detection and classification algorithms, addressing challenges such as variable environmental conditions. Additionally, we cover cutting-edge research in predicting VRU intentions and behaviors, which is crucial for proactive collision avoidance strategies. Through this survey, we aim to provide a comprehensive understanding of the current landscape of VRU safety technologies, identifying areas of progress and areas needing further research and development.","sentences":["Traffic incidents involving vulnerable road users (VRUs) constitute a significant proportion of global road accidents.","Advances in traffic communication ecosystems, coupled with sophisticated signal processing and machine learning techniques, have facilitated the utilization of data from diverse sensors.","Despite these advancements and the availability of extensive datasets, substantial progress is required to mitigate traffic casualties.","This paper provides a comprehensive survey of state-of-the-art technologies and methodologies to enhance the safety of VRUs.","The study delves into the communication networks between vehicles and VRUs, emphasizing the integration of advanced sensors and the availability of relevant datasets.","It explores preprocessing techniques and data fusion methods to enhance sensor data quality.","Furthermore, our study assesses critical simulation environments essential for developing and testing VRU safety systems.","Our research also highlights recent advances in VRU detection and classification algorithms, addressing challenges such as variable environmental conditions.","Additionally, we cover cutting-edge research in predicting VRU intentions and behaviors, which is crucial for proactive collision avoidance strategies.","Through this survey, we aim to provide a comprehensive understanding of the current landscape of VRU safety technologies, identifying areas of progress and areas needing further research and development."],"url":"http://arxiv.org/abs/2405.19202v1","category":"cs.LG"}
{"created":"2024-05-29 15:41:53","title":"Going beyond compositional generalization, DDPMs can produce zero-shot interpolation","abstract":"Denoising Diffusion Probabilistic Models (DDPMs) exhibit remarkable capabilities in image generation, with studies suggesting that they can generalize by composing latent factors learned from the training data. In this work, we go further and study DDPMs trained on strictly separate subsets of the data distribution with large gaps on the support of the latent factors. We show that such a model can effectively generate images in the unexplored, intermediate regions of the distribution. For instance, when trained on clearly smiling and non-smiling faces, we demonstrate a sampling procedure which can generate slightly smiling faces without reference images (zero-shot interpolation). We replicate these findings for other attributes as well as other datasets. $\\href{https://github.com/jdeschena/ddpm-zero-shot-interpolation}{\\text{Our code is available on GitHub.}}$","sentences":["Denoising Diffusion Probabilistic Models (DDPMs) exhibit remarkable capabilities in image generation, with studies suggesting that they can generalize by composing latent factors learned from the training data.","In this work, we go further and study DDPMs trained on strictly separate subsets of the data distribution with large gaps on the support of the latent factors.","We show that such a model can effectively generate images in the unexplored, intermediate regions of the distribution.","For instance, when trained on clearly smiling and non-smiling faces, we demonstrate a sampling procedure which can generate slightly smiling faces without reference images (zero-shot interpolation).","We replicate these findings for other attributes as well as other datasets.","$\\href{https://github.com/jdeschena/ddpm-zero-shot-interpolation}{\\text{Our code is available on GitHub.}}$"],"url":"http://arxiv.org/abs/2405.19201v1","category":"cs.CV"}
{"created":"2024-05-29 15:37:23","title":"Mass formulas for supergravity black holes with string singularities","abstract":"We extend the derivation of mass formulas for stationary axisymmetric asymptotically locally flat solutions with string singularities on the polar axis to general supergravity actions containing vector and scalar fields. It is based on the rod structure of the solutions in Weyl coordinates and is applicable to black holes with Dirac and Misner strings. The obtained formulas differ from the corresponding ones in Einstein-Maxwell theory only by summation over all independent electric charges.","sentences":["We extend the derivation of mass formulas for stationary axisymmetric asymptotically locally flat solutions with string singularities on the polar axis to general supergravity actions containing vector and scalar fields.","It is based on the rod structure of the solutions in Weyl coordinates and is applicable to black holes with Dirac and Misner strings.","The obtained formulas differ from the corresponding ones in Einstein-Maxwell theory only by summation over all independent electric charges."],"url":"http://arxiv.org/abs/2405.19196v1","category":"gr-qc"}
{"created":"2024-05-29 15:33:15","title":"Sparse High Dimensional Expanders via Local Lifts","abstract":"High dimensional expanders (HDXs) are a hypergraph generalization of expander graphs. They are extensively studied in the math and TCS communities due to their many applications. Like expander graphs, HDXs are especially interesting for applications when they are bounded degree, namely, if the number of edges adjacent to every vertex is bounded. However, only a handful of constructions are known to have this property, all of which rely on non-trivial algebraic techniques. In particular, no random or combinatorial construction of bounded degree HDXs is known. As a result, our understanding of these objects is limited.   The degree of an $i$-face in an HDX is the number of $(i+1)$-faces containing it. In this work we construct HDXs whose higher dimensional faces have bounded degree. This is done by giving an elementary and deterministic algorithm that takes as input a regular $k$-dimensional HDX $X$ and outputs another $k$-dimensional HDX $\\widehat{X}$ with twice as many vertices. While the degree of vertices in $\\widehat{X}$ grows, the degree of the $(k-1)$-faces in $\\widehat{X}$ stays the same. As a result, we obtain a new `algebra-free' construction of HDXs whose $(k-1)$-face degree is bounded.   Our algorithm is based on a simple and natural generalization of the construction by Bilu and Linial (Combinatorica, 2006), which build expanders using lifts coming from edge signings. Our construction is based on local lifts of HDXs, where a local lift is a complex whose top-level links are lifts of links in the original complex. We demonstrate that a local lift of an HDX is an HDX in many cases.   In addition, combining local lifts with existing bounded degree constructions creates new families of bounded degree HDXs with significantly different links than before. We use this technique to construct bounded degree high dimensional expanders with links that have arbitrarily large diameters.","sentences":["High dimensional expanders (HDXs) are a hypergraph generalization of expander graphs.","They are extensively studied in the math and TCS communities due to their many applications.","Like expander graphs, HDXs are especially interesting for applications when they are bounded degree, namely, if the number of edges adjacent to every vertex is bounded.","However, only a handful of constructions are known to have this property, all of which rely on non-trivial algebraic techniques.","In particular, no random or combinatorial construction of bounded degree HDXs is known.","As a result, our understanding of these objects is limited.   ","The degree of an $i$-face in an HDX is the number of $(i+1)$-faces containing it.","In this work we construct HDXs whose higher dimensional faces have bounded degree.","This is done by giving an elementary and deterministic algorithm that takes as input a regular $k$-dimensional HDX $X$ and outputs another $k$-dimensional HDX $\\widehat{X}$ with twice as many vertices.","While the degree of vertices in $\\widehat{X}$ grows, the degree of the $(k-1)$-faces in $\\widehat{X}$ stays the same.","As a result, we obtain a new `algebra-free' construction of HDXs whose $(k-1)$-face degree is bounded.   ","Our algorithm is based on a simple and natural generalization of the construction by Bilu and Linial (Combinatorica, 2006), which build expanders using lifts coming from edge signings.","Our construction is based on local lifts of HDXs, where a local lift is a complex whose top-level links are lifts of links in the original complex.","We demonstrate that a local lift of an HDX is an HDX in many cases.   ","In addition, combining local lifts with existing bounded degree constructions creates new families of bounded degree HDXs with significantly different links than before.","We use this technique to construct bounded degree high dimensional expanders with links that have arbitrarily large diameters."],"url":"http://arxiv.org/abs/2405.19191v1","category":"cs.DM"}
{"created":"2024-05-29 15:30:14","title":"Reconstructing the G2HDM Charged Higgs Boson at the LHC","abstract":"We study the discovery prospects for a charged Higgs boson via the $b g\\to c H^- \\to c \\bar t b$ process at the Large Hadron Collier (LHC). Focusing on the general Two Higgs Doublet Model (G2HDM) that possesses extra Yukawa couplings, the process is controlled by extra top couplings $\\rho_{tc}$ and $\\rho_{tt}$, which can drive electroweak baryogenesis (EWBG) to account for the baryon asymmetry of the Universe (BAU). We propose benchmark points (BPs) and demonstrate that evidence could emerge at the LHC with 14 TeV collision energy and luminosity of 300 fb$^{-1}$, with discovery potential at 600 fb$^{-1}$.","sentences":["We study the discovery prospects for a charged Higgs boson via the $b g\\to c H^- \\to c \\bar t b$ process at the Large Hadron Collier (LHC).","Focusing on the general Two Higgs Doublet Model (G2HDM) that possesses extra Yukawa couplings, the process is controlled by extra top couplings $\\rho_{tc}$ and $\\rho_{tt}$, which can drive electroweak baryogenesis (EWBG) to account for the baryon asymmetry of the Universe (BAU).","We propose benchmark points (BPs) and demonstrate that evidence could emerge at the LHC with 14 TeV collision energy and luminosity of 300 fb$^{-1}$, with discovery potential at 600 fb$^{-1}$."],"url":"http://arxiv.org/abs/2405.19190v1","category":"hep-ph"}
{"created":"2024-05-29 15:29:46","title":"Diffusion-based Dynamics Models for Long-Horizon Rollout in Offline Reinforcement Learning","abstract":"With the great success of diffusion models (DMs) in generating realistic synthetic vision data, many researchers have investigated their potential in decision-making and control. Most of these works utilized DMs to sample directly from the trajectory space, where DMs can be viewed as a combination of dynamics models and policies. In this work, we explore how to decouple DMs' ability as dynamics models in fully offline settings, allowing the learning policy to roll out trajectories. As DMs learn the data distribution from the dataset, their intrinsic policy is actually the behavior policy induced from the dataset, which results in a mismatch between the behavior policy and the learning policy. We propose Dynamics Diffusion, short as DyDiff, which can inject information from the learning policy to DMs iteratively. DyDiff ensures long-horizon rollout accuracy while maintaining policy consistency and can be easily deployed on model-free algorithms. We provide theoretical analysis to show the advantage of DMs on long-horizon rollout over models and demonstrate the effectiveness of DyDiff in the context of offline reinforcement learning, where the rollout dataset is provided but no online environment for interaction. Our code is at https://github.com/FineArtz/DyDiff.","sentences":["With the great success of diffusion models (DMs) in generating realistic synthetic vision data, many researchers have investigated their potential in decision-making and control.","Most of these works utilized DMs to sample directly from the trajectory space, where DMs can be viewed as a combination of dynamics models and policies.","In this work, we explore how to decouple DMs' ability as dynamics models in fully offline settings, allowing the learning policy to roll out trajectories.","As DMs learn the data distribution from the dataset, their intrinsic policy is actually the behavior policy induced from the dataset, which results in a mismatch between the behavior policy and the learning policy.","We propose Dynamics Diffusion, short as DyDiff, which can inject information from the learning policy to DMs iteratively.","DyDiff ensures long-horizon rollout accuracy while maintaining policy consistency and can be easily deployed on model-free algorithms.","We provide theoretical analysis to show the advantage of DMs on long-horizon rollout over models and demonstrate the effectiveness of DyDiff in the context of offline reinforcement learning, where the rollout dataset is provided but no online environment for interaction.","Our code is at https://github.com/FineArtz/DyDiff."],"url":"http://arxiv.org/abs/2405.19189v1","category":"cs.LG"}
{"created":"2024-05-29 15:29:21","title":"Personalized Interiors at Scale: Leveraging AI for Efficient and Customizable Design Solutions","abstract":"In this paper, we introduce an innovative application of artificial intelligence in the realm of interior design through the integration of Stable Diffusion and Dreambooth models. This paper explores the potential of these advanced generative models to streamline and democratize the process of room interior generation, offering a significant departure from conventional, labor-intensive techniques. Our approach leverages the capabilities of Stable Diffusion for generating high-quality images and Dreambooth for rapid customization with minimal training data, addressing the need for efficiency and personalization in the design industry. We detail a comprehensive methodology that combines these models, providing a robust framework for the creation of tailored room interiors that reflect individual tastes and functional requirements. We presents an extensive evaluation of our method, supported by experimental results that demonstrate its effectiveness and a series of case studies that illustrate its practical application in interior design projects. Our study contributes to the ongoing discourse on the role of AI in creative fields, highlighting the benefits of leveraging generative models to enhance creativity and reshape the future of interior design.","sentences":["In this paper, we introduce an innovative application of artificial intelligence in the realm of interior design through the integration of Stable Diffusion and Dreambooth models.","This paper explores the potential of these advanced generative models to streamline and democratize the process of room interior generation, offering a significant departure from conventional, labor-intensive techniques.","Our approach leverages the capabilities of Stable Diffusion for generating high-quality images and Dreambooth for rapid customization with minimal training data, addressing the need for efficiency and personalization in the design industry.","We detail a comprehensive methodology that combines these models, providing a robust framework for the creation of tailored room interiors that reflect individual tastes and functional requirements.","We presents an extensive evaluation of our method, supported by experimental results that demonstrate its effectiveness and a series of case studies that illustrate its practical application in interior design projects.","Our study contributes to the ongoing discourse on the role of AI in creative fields, highlighting the benefits of leveraging generative models to enhance creativity and reshape the future of interior design."],"url":"http://arxiv.org/abs/2405.19188v1","category":"cs.HC"}
{"created":"2024-05-29 15:28:42","title":"MetaToken: Detecting Hallucination in Image Descriptions by Meta Classification","abstract":"Large Vision Language Models (LVLMs) have shown remarkable capabilities in multimodal tasks like visual question answering or image captioning. However, inconsistencies between the visual information and the generated text, a phenomenon referred to as hallucinations, remain an unsolved problem with regard to the trustworthiness of LVLMs. To address this problem, recent works proposed to incorporate computationally costly Large (Vision) Language Models in order to detect hallucinations on a sentence- or subsentence-level. In this work, we introduce MetaToken, a lightweight binary classifier to detect hallucinations on the token-level at negligible cost. Based on a statistical analysis, we reveal key factors of hallucinations in LVLMs which have been overseen in previous works. MetaToken can be applied to any open-source LVLM without any knowledge about ground truth data providing a reliable detection of hallucinations. We evaluate our method on four state-of-the-art LVLMs demonstrating the effectiveness of our approach.","sentences":["Large Vision Language Models (LVLMs) have shown remarkable capabilities in multimodal tasks like visual question answering or image captioning.","However, inconsistencies between the visual information and the generated text, a phenomenon referred to as hallucinations, remain an unsolved problem with regard to the trustworthiness of LVLMs.","To address this problem, recent works proposed to incorporate computationally costly Large (Vision) Language Models in order to detect hallucinations on a sentence- or subsentence-level.","In this work, we introduce MetaToken, a lightweight binary classifier to detect hallucinations on the token-level at negligible cost.","Based on a statistical analysis, we reveal key factors of hallucinations in LVLMs which have been overseen in previous works.","MetaToken can be applied to any open-source LVLM without any knowledge about ground truth data providing a reliable detection of hallucinations.","We evaluate our method on four state-of-the-art LVLMs demonstrating the effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.19186v1","category":"cs.CV"}
{"created":"2024-05-29 15:24:28","title":"Promoting Two-sided Fairness in Dynamic Vehicle Routing Problem","abstract":"Dynamic Vehicle Routing Problem (DVRP), is an extension of the classic Vehicle Routing Problem (VRP), which is a fundamental problem in logistics and transportation. Typically, DVRPs involve two stakeholders: service providers that deliver services to customers and customers who raise requests from different locations. Many real-world applications can be formulated as DVRP such as ridesharing and non-compliance capture. Apart from original objectives like optimising total utility or efficiency, DVRP should also consider fairness for all parties. Unfairness can induce service providers and customers to give up on the systems, leading to negative financial and social impacts. However, most existing DVRP-related applications focus on improving fairness from a single side, and there have been few works considering two-sided fairness and utility optimisation concurrently. To this end, we propose a novel framework, a Two-sided Fairness-aware Genetic Algorithm (named 2FairGA), which expands the genetic algorithm from the original objective solely focusing on utility to multi-objectives that incorporate two-sided fairness. Subsequently, the impact of injecting two fairness definitions into the utility-focused model and the correlation between any pair of the three objectives are explored. Extensive experiments demonstrate the superiority of our proposed framework compared to the state-of-the-art.","sentences":["Dynamic Vehicle Routing Problem (DVRP), is an extension of the classic Vehicle Routing Problem (VRP), which is a fundamental problem in logistics and transportation.","Typically, DVRPs involve two stakeholders: service providers that deliver services to customers and customers who raise requests from different locations.","Many real-world applications can be formulated as DVRP such as ridesharing and non-compliance capture.","Apart from original objectives like optimising total utility or efficiency, DVRP should also consider fairness for all parties.","Unfairness can induce service providers and customers to give up on the systems, leading to negative financial and social impacts.","However, most existing DVRP-related applications focus on improving fairness from a single side, and there have been few works considering two-sided fairness and utility optimisation concurrently.","To this end, we propose a novel framework, a Two-sided Fairness-aware Genetic Algorithm (named 2FairGA), which expands the genetic algorithm from the original objective solely focusing on utility to multi-objectives that incorporate two-sided fairness.","Subsequently, the impact of injecting two fairness definitions into the utility-focused model and the correlation between any pair of the three objectives are explored.","Extensive experiments demonstrate the superiority of our proposed framework compared to the state-of-the-art."],"url":"http://arxiv.org/abs/2405.19184v1","category":"cs.AI"}
{"created":"2024-05-29 15:24:22","title":"Conditional Latent ODEs for Motion Prediction in Autonomous Driving","abstract":"This paper addresses imitation learning for motion prediction problem in autonomous driving, especially in multi-agent setting. Different from previous methods based on GAN, we present the conditional latent ordinary differential equation (cLODE) to leverage both the generative strength of conditional VAE and the continuous representation of neural ODE. Our network architecture is inspired from the Latent-ODE model. The experiment shows that our method outperform the baseline methods in the simulation of multi-agent driving and is very efficient in term of GPU memory consumption. Our code and docker image are publicly available: https://github.com/TruongKhang/cLODE; https://hub.docker.com/r/kim4375731/clode.","sentences":["This paper addresses imitation learning for motion prediction problem in autonomous driving, especially in multi-agent setting.","Different from previous methods based on GAN, we present the conditional latent ordinary differential equation (cLODE) to leverage both the generative strength of conditional VAE and the continuous representation of neural ODE.","Our network architecture is inspired from the Latent-ODE model.","The experiment shows that our method outperform the baseline methods in the simulation of multi-agent driving and is very efficient in term of GPU memory consumption.","Our code and docker image are publicly available: https://github.com/TruongKhang/cLODE; https://hub.docker.com/r/kim4375731/clode."],"url":"http://arxiv.org/abs/2405.19183v1","category":"cs.RO"}
{"created":"2024-05-29 15:21:51","title":"Post-Minkowskian Theory Meets the Spinning Effective-One-Body Approach for Bound-Orbit Waveforms","abstract":"Driven by advances in scattering amplitudes and worldline-based methods, recent years have seen significant progress in our ability to calculate gravitational two-body scattering observables. These observables effectively encapsulate the gravitational two-body problem in the weak-field and high-velocity regime (post-Minkowskian, PM), with applications to the bound two-body problem and gravitational-wave modeling. We leverage PM data to construct a complete inspiral-merger-ringdown waveform model for non-precessing spinning black holes within the effective-one-body (EOB) formalism: SEOBNR-PM. This model is closely based on the highly successful SEOBNRv5 model, used by the LIGO-Virgo-KAGRA Collaboration, with its key new feature being an EOB Hamiltonian derived by matching the two-body scattering angle in a perturbative PM expansion. The model performs remarkably well, showing a median mismatch against 441 numerical-relativity (NR) simulations that is somewhat lower than a similarly calibrated version of SEOBNRv5. Comparisons of the binding energy with NR also demonstrate better agreement than SEOBNRv5, despite the latter containing additional calibration to NR simulations.","sentences":["Driven by advances in scattering amplitudes and worldline-based methods, recent years have seen significant progress in our ability to calculate gravitational two-body scattering observables.","These observables effectively encapsulate the gravitational two-body problem in the weak-field and high-velocity regime (post-Minkowskian, PM), with applications to the bound two-body problem and gravitational-wave modeling.","We leverage PM data to construct a complete inspiral-merger-ringdown waveform model for non-precessing spinning black holes within the effective-one-body (EOB) formalism: SEOBNR-PM.","This model is closely based on the highly successful SEOBNRv5 model, used by the LIGO-Virgo-KAGRA Collaboration, with its key new feature being an EOB Hamiltonian derived by matching the two-body scattering angle in a perturbative PM expansion.","The model performs remarkably well, showing a median mismatch against 441 numerical-relativity (NR) simulations that is somewhat lower than a similarly calibrated version of SEOBNRv5.","Comparisons of the binding energy with NR also demonstrate better agreement than SEOBNRv5, despite the latter containing additional calibration to NR simulations."],"url":"http://arxiv.org/abs/2405.19181v1","category":"gr-qc"}
{"created":"2024-05-29 15:18:39","title":"Model-independent cosmological inference post DESI DR1 BAO measurements","abstract":"In this work, we implement Gaussian process regression to reconstruct the expansion history of the universe in a model-agnostic manner, using the Pantheon-Plus SN-Ia compilation in combination with two different BAO measurements (SDSS-IV and DESI DR1). In both the reconstructions, the $\\Lambda$CDM model is always included in the 95\\% confidence intervals. We find evidence that the DESI LRG data at $z_{\\text{eff}} = 0.51$ is not an outlier within our model-independent framework. We study the $\\mathcal{O}m$-diagnostics and the evolution of the total equation of state (EoS) of our universe, which hint towards the possibility of a quintessence-like dark energy scenario with a very slowly varying EoS, and a phantom-crossing in higher $z$. The entire exercise is later complemented by considering two more SN-Ia compilations - DES-5YR and Union3 - in combination with DESI BAO. Reconstruction with the DESI BAO + DES-5YR SN data sets predicts that the $\\Lambda$CDM model lies outside the 3$\\sigma$ confidence levels, whereas with DESI BAO + Union3 data, the $\\Lambda$CDM model is always included within 1$\\sigma$. We also report constraints on $H_0 r_d$ from our model-agnostic analysis, independent of the pre-recombination physics. Our results point towards an $\\approx$ 2$\\sigma$ discrepancy between the DESI + Pantheon-Plus and DESI + DES-5YR data sets, which calls for further investigation.","sentences":["In this work, we implement Gaussian process regression to reconstruct the expansion history of the universe in a model-agnostic manner, using the Pantheon-Plus SN-Ia compilation in combination with two different BAO measurements (SDSS-IV and DESI DR1).","In both the reconstructions, the $\\Lambda$CDM model is always included in the 95\\% confidence intervals.","We find evidence that the DESI LRG data at $z_{\\text{eff}} = 0.51$ is not an outlier within our model-independent framework.","We study the $\\mathcal{O}m$-diagnostics and the evolution of the total equation of state (EoS) of our universe, which hint towards the possibility of a quintessence-like dark energy scenario with a very slowly varying EoS, and a phantom-crossing in higher $z$. The entire exercise is later complemented by considering two more SN-Ia compilations - DES-5YR and Union3 - in combination with DESI BAO.","Reconstruction with the DESI BAO + DES-5YR SN data sets predicts that the $\\Lambda$CDM model lies outside the 3$\\sigma$ confidence levels, whereas with DESI BAO + Union3 data, the $\\Lambda$CDM model is always included within 1$\\sigma$.","We also report constraints on $H_0 r_d$ from our model-agnostic analysis, independent of the pre-recombination physics.","Our results point towards an $\\approx$ 2$\\sigma$ discrepancy between the DESI + Pantheon-Plus and DESI + DES-5YR data sets, which calls for further investigation."],"url":"http://arxiv.org/abs/2405.19178v1","category":"astro-ph.CO"}
{"created":"2024-05-29 15:18:13","title":"The ethical situation of DALL-E 2","abstract":"A hot topic of Artificial Intelligence right now is image generation from prompts. DALL-E 2 is one of the biggest names in this domain, as it allows people to create images from simple text inputs, to even more complicated ones. The company that made this possible, OpenAI, has assured everyone that visited their website that their mission is to ensure that artificial general intelligence benefits all humanity. A noble idea in our opinion, that also stood as the motive behind us choosing this subject. This paper analyzes the ethical implications of an AI image generative system, with an emphasis on how society is responding to it, how it probably will and how it should if all the right measures are taken.","sentences":["A hot topic of Artificial Intelligence right now is image generation from prompts.","DALL-E 2 is one of the biggest names in this domain, as it allows people to create images from simple text inputs, to even more complicated ones.","The company that made this possible, OpenAI, has assured everyone that visited their website that their mission is to ensure that artificial general intelligence benefits all humanity.","A noble idea in our opinion, that also stood as the motive behind us choosing this subject.","This paper analyzes the ethical implications of an AI image generative system, with an emphasis on how society is responding to it, how it probably will and how it should if all the right measures are taken."],"url":"http://arxiv.org/abs/2405.19176v1","category":"cs.CY"}
{"created":"2024-05-29 15:15:52","title":"Exploring AI-based Anonymization of Industrial Image and Video Data in the Context of Feature Preservation","abstract":"With rising technologies, the protection of privacy-sensitive information is becoming increasingly important. In industry and production facilities, image or video recordings are beneficial for documentation, tracing production errors or coordinating workflows. Individuals in images or videos need to be anonymized. However, the anonymized data should be reusable for further applications. In this work, we apply the Deep Learning-based full-body anonymization framework DeepPrivacy2, which generates artificial identities, to industrial image and video data. We compare its performance with conventional anonymization techniques. Therefore, we consider the quality of identity generation, temporal consistency, and the applicability of pose estimation and action recognition.","sentences":["With rising technologies, the protection of privacy-sensitive information is becoming increasingly important.","In industry and production facilities, image or video recordings are beneficial for documentation, tracing production errors or coordinating workflows.","Individuals in images or videos need to be anonymized.","However, the anonymized data should be reusable for further applications.","In this work, we apply the Deep Learning-based full-body anonymization framework DeepPrivacy2, which generates artificial identities, to industrial image and video data.","We compare its performance with conventional anonymization techniques.","Therefore, we consider the quality of identity generation, temporal consistency, and the applicability of pose estimation and action recognition."],"url":"http://arxiv.org/abs/2405.19173v1","category":"cs.CV"}
{"created":"2024-05-29 15:15:33","title":"Dedekind-MacNeille and related completions: subfitness, regularity, and Booleanness","abstract":"Completions play an important r\\^ole for studying structure by supplying elements that in some sense ``ought to be.\" Among these, the Dedekind-MacNeille completion is of particular importance. In 1968 Janowitz provided necessary and sufficient conditions for it to be subfit or Boolean. Another natural separation axiom situated between the two is regularity. We explore similar characterizations of when closely related completions are subfit, regular, or Boolean. We are mainly interested in the Bruns-Lakser, ideal, and canonical completions, which are useful in pointfree topology since (unlike the Dedekind-MacNeille completion) they satisfy stronger forms of distributivity.","sentences":["Completions play an important r\\^ole for studying structure by supplying elements that in some sense ``ought to be.\"","Among these, the Dedekind-MacNeille completion is of particular importance.","In 1968 Janowitz provided necessary and sufficient conditions for it to be subfit or Boolean.","Another natural separation axiom situated between the two is regularity.","We explore similar characterizations of when closely related completions are subfit, regular, or Boolean.","We are mainly interested in the Bruns-Lakser, ideal, and canonical completions, which are useful in pointfree topology since (unlike the Dedekind-MacNeille completion) they satisfy stronger forms of distributivity."],"url":"http://arxiv.org/abs/2405.19171v1","category":"math.GN"}
{"created":"2024-05-29 15:15:33","title":"Chromatic and Clique number of Generalized Sierpi\u0144ski Gasket Graph $S[G,t]$","abstract":"WE study the clique number and the chromatic number of generalized Sierpinski graphs in which the base graph is an arbitrary simple graph.","sentences":["WE study the clique number and the chromatic number of generalized Sierpinski graphs in which the base graph is an arbitrary simple graph."],"url":"http://arxiv.org/abs/2405.19172v1","category":"math.CO"}
{"created":"2024-05-29 15:11:10","title":"Measuring the Higgs spin and parity in $H\\to \\ell^-\\ell^+ Z$","abstract":"We develop an effective and methodical algorithm for the construction of general covariant four-point $H\\ell\\ell Z$ vertices, accommodating leptons $\\ell=e, \\mu$, and designed to handle a Higgs boson $H$ of any integer spin, not merely confined to spins up to 2. While our numerical analysis assumes the Higgs boson mass to be $m_H=125\\,{\\rm GeV}$, the analytical framework we propose is versatile, enabling the examination of various mass scenarios. These meticulously devised general covariant four-point $H\\ell\\ell Z$ vertices are pivotal in the systematic determination of the spin and parity of the Standard Model Higgs boson, especially via one of its primary decay channels, the three-body decay process $H\\to \\ell^-\\ell^+ Z$, observable at the Large Hadron Collider. Our innovative strategy extends beyond the limitations of previous investigations on the Higgs spin and parity determinations. It not only encompasses the analysis of the two-step cascade decay $H\\to Z^{\\star} Z \\to \\ell^-\\ell^+ Z$, featuring an intermediate virtual $Z^{\\star}$ exchange but also integrates consideration for the direct 4-point contact $H\\ell\\ell Z$ interaction. Our significantly expanded scheme provides a definitive and unequivocal platform for determining and confirming the spinless nature and even parity of the SM Higgs boson by leveraging threshold effects and angular correlations, even though achieving such conclusive results in practical and exhaustive analyses necessitates high event rates.","sentences":["We develop an effective and methodical algorithm for the construction of general covariant four-point $H\\ell\\ell Z$ vertices, accommodating leptons $\\ell=e, \\mu$, and designed to handle a Higgs boson $H$ of any integer spin, not merely confined to spins up to 2.","While our numerical analysis assumes the Higgs boson mass to be $m_H=125\\,{\\rm GeV}$, the analytical framework we propose is versatile, enabling the examination of various mass scenarios.","These meticulously devised general covariant four-point $H\\ell\\ell Z$ vertices are pivotal in the systematic determination of the spin and parity of the Standard Model Higgs boson, especially via one of its primary decay channels, the three-body decay process $H\\to \\ell^-\\ell^+ Z$, observable at the Large Hadron Collider.","Our innovative strategy extends beyond the limitations of previous investigations on the Higgs spin and parity determinations.","It not only encompasses the analysis of the two-step cascade decay $H\\to Z^{\\star} Z \\to \\ell^-\\ell^+ Z$, featuring an intermediate virtual $Z^{\\star}$ exchange but also integrates consideration for the direct 4-point contact $H\\ell\\ell Z$ interaction.","Our significantly expanded scheme provides a definitive and unequivocal platform for determining and confirming the spinless nature and even parity of the SM Higgs boson by leveraging threshold effects and angular correlations, even though achieving such conclusive results in practical and exhaustive analyses necessitates high event rates."],"url":"http://arxiv.org/abs/2405.19167v1","category":"hep-ph"}
{"created":"2024-05-29 15:10:24","title":"Transformers as Neural Operators for Solutions of Differential Equations with Finite Regularity","abstract":"Neural operator learning models have emerged as very effective surrogates in data-driven methods for partial differential equations (PDEs) across different applications from computational science and engineering. Such operator learning models not only predict particular instances of a physical or biological system in real-time but also forecast classes of solutions corresponding to a distribution of initial and boundary conditions or forcing terms. % DeepONet is the first neural operator model and has been tested extensively for a broad class of solutions, including Riemann problems. Transformers have not been used in that capacity, and specifically, they have not been tested for solutions of PDEs with low regularity. %   In this work, we first establish the theoretical groundwork that transformers possess the universal approximation property as operator learning models.   We then apply transformers to forecast solutions of diverse dynamical systems with solutions of finite regularity for a plurality of initial conditions and forcing terms. In particular, we consider three examples: the Izhikevich neuron model, the tempered fractional-order Leaky Integrate-and-Fire (LIF) model, and the one-dimensional Euler equation Riemann problem. For the latter problem, we also compare with variants of DeepONet, and we find that transformers outperform DeepONet in accuracy but they are computationally more expensive.","sentences":["Neural operator learning models have emerged as very effective surrogates in data-driven methods for partial differential equations (PDEs) across different applications from computational science and engineering.","Such operator learning models not only predict particular instances of a physical or biological system in real-time but also forecast classes of solutions corresponding to a distribution of initial and boundary conditions or forcing terms.","% DeepONet is the first neural operator model and has been tested extensively for a broad class of solutions, including Riemann problems.","Transformers have not been used in that capacity, and specifically, they have not been tested for solutions of PDEs with low regularity.","%   In this work, we first establish the theoretical groundwork that transformers possess the universal approximation property as operator learning models.   ","We then apply transformers to forecast solutions of diverse dynamical systems with solutions of finite regularity for a plurality of initial conditions and forcing terms.","In particular, we consider three examples: the Izhikevich neuron model, the tempered fractional-order Leaky Integrate-and-Fire (LIF) model, and the one-dimensional Euler equation Riemann problem.","For the latter problem, we also compare with variants of DeepONet, and we find that transformers outperform DeepONet in accuracy but they are computationally more expensive."],"url":"http://arxiv.org/abs/2405.19166v1","category":"cs.LG"}
{"created":"2024-05-29 15:09:34","title":"Hilbert Series of Bipartite Field Theories","abstract":"We study the algebraic structure of the mesonic moduli spaces of bipartite field theories by computing the Hilbert series. Bipartite field theories form a large family of 4d N=1 supersymmetric gauge theories that are defined by bipartite graphs on Riemann surfaces with boundaries. By calculating the Hilbert series, we are able to identify the generators and defining generator relations of the mesonic moduli spaces of these theories. Moreover, we show that certain bipartite field theories exhibit enhanced global symmetries which can be identified through the computation of the corresponding refined Hilbert series. As part of our study, we introduce two one-parameter families of bipartite field theories defined on cylinders whose mesonic moduli spaces are all complete intersection toric Calabi-Yau 3-folds.","sentences":["We study the algebraic structure of the mesonic moduli spaces of bipartite field theories by computing the Hilbert series.","Bipartite field theories form a large family of 4d N=1","supersymmetric gauge theories that are defined by bipartite graphs on Riemann surfaces with boundaries.","By calculating the Hilbert series, we are able to identify the generators and defining generator relations of the mesonic moduli spaces of these theories.","Moreover, we show that certain bipartite field theories exhibit enhanced global symmetries which can be identified through the computation of the corresponding refined Hilbert series.","As part of our study, we introduce two one-parameter families of bipartite field theories defined on cylinders whose mesonic moduli spaces are all complete intersection toric Calabi-Yau 3-folds."],"url":"http://arxiv.org/abs/2405.19165v1","category":"hep-th"}
{"created":"2024-05-29 15:08:55","title":"Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in eDiscovery","abstract":"Electronic Discovery (eDiscovery) involves identifying relevant documents from a vast collection based on legal production requests. The integration of artificial intelligence (AI) and natural language processing (NLP) has transformed this process, helping document review and enhance efficiency and cost-effectiveness. Although traditional approaches like BM25 or fine-tuned pre-trained models are common in eDiscovery, they face performance, computational, and interpretability challenges. In contrast, Large Language Model (LLM)-based methods prioritize interpretability but sacrifice performance and throughput. This paper introduces DISCOvery Graph (DISCOG), a hybrid approach that combines the strengths of two worlds: a heterogeneous graph-based method for accurate document relevance prediction and subsequent LLM-driven approach for reasoning. Graph representational learning generates embeddings and predicts links, ranking the corpus for a given request, and the LLMs provide reasoning for document relevance. Our approach handles datasets with balanced and imbalanced distributions, outperforming baselines in F1-score, precision, and recall by an average of 12%, 3%, and 16%, respectively. In an enterprise context, our approach drastically reduces document review costs by 99.9% compared to manual processes and by 95% compared to LLM-based classification methods","sentences":["Electronic Discovery (eDiscovery) involves identifying relevant documents from a vast collection based on legal production requests.","The integration of artificial intelligence (AI) and natural language processing (NLP) has transformed this process, helping document review and enhance efficiency and cost-effectiveness.","Although traditional approaches like BM25 or fine-tuned pre-trained models are common in eDiscovery, they face performance, computational, and interpretability challenges.","In contrast, Large Language Model (LLM)-based methods prioritize interpretability but sacrifice performance and throughput.","This paper introduces DISCOvery Graph (DISCOG), a hybrid approach that combines the strengths of two worlds: a heterogeneous graph-based method for accurate document relevance prediction and subsequent LLM-driven approach for reasoning.","Graph representational learning generates embeddings and predicts links, ranking the corpus for a given request, and the LLMs provide reasoning for document relevance.","Our approach handles datasets with balanced and imbalanced distributions, outperforming baselines in F1-score, precision, and recall by an average of 12%, 3%, and 16%, respectively.","In an enterprise context, our approach drastically reduces document review costs by 99.9% compared to manual processes and by 95% compared to LLM-based classification methods"],"url":"http://arxiv.org/abs/2405.19164v1","category":"cs.AI"}
{"created":"2024-05-29 15:06:10","title":"Does learning the right latent variables necessarily improve in-context learning?","abstract":"Large autoregressive models like Transformers can solve tasks through in-context learning (ICL) without learning new weights, suggesting avenues for efficiently solving new tasks. For many tasks, e.g., linear regression, the data factorizes: examples are independent given a task latent that generates the data, e.g., linear coefficients. While an optimal predictor leverages this factorization by inferring task latents, it is unclear if Transformers implicitly do so or if they instead exploit heuristics and statistical shortcuts enabled by attention layers. Both scenarios have inspired active ongoing work. In this paper, we systematically investigate the effect of explicitly inferring task latents. We minimally modify the Transformer architecture with a bottleneck designed to prevent shortcuts in favor of more structured solutions, and then compare performance against standard Transformers across various ICL tasks. Contrary to intuition and some recent works, we find little discernible difference between the two; biasing towards task-relevant latent variables does not lead to better out-of-distribution performance, in general. Curiously, we find that while the bottleneck effectively learns to extract latent task variables from context, downstream processing struggles to utilize them for robust prediction. Our study highlights the intrinsic limitations of Transformers in achieving structured ICL solutions that generalize, and shows that while inferring the right latents aids interpretability, it is not sufficient to alleviate this problem.","sentences":["Large autoregressive models like Transformers can solve tasks through in-context learning (ICL) without learning new weights, suggesting avenues for efficiently solving new tasks.","For many tasks, e.g., linear regression, the data factorizes: examples are independent given a task latent that generates the data, e.g., linear coefficients.","While an optimal predictor leverages this factorization by inferring task latents, it is unclear if Transformers implicitly do so or if they instead exploit heuristics and statistical shortcuts enabled by attention layers.","Both scenarios have inspired active ongoing work.","In this paper, we systematically investigate the effect of explicitly inferring task latents.","We minimally modify the Transformer architecture with a bottleneck designed to prevent shortcuts in favor of more structured solutions, and then compare performance against standard Transformers across various ICL tasks.","Contrary to intuition and some recent works, we find little discernible difference between the two; biasing towards task-relevant latent variables does not lead to better out-of-distribution performance, in general.","Curiously, we find that while the bottleneck effectively learns to extract latent task variables from context, downstream processing struggles to utilize them for robust prediction.","Our study highlights the intrinsic limitations of Transformers in achieving structured ICL solutions that generalize, and shows that while inferring the right latents aids interpretability, it is not sufficient to alleviate this problem."],"url":"http://arxiv.org/abs/2405.19162v1","category":"cs.LG"}
{"created":"2024-05-29 15:02:42","title":"Probing ultrafast magnetization dynamics via synthetic axion fields","abstract":"Spatial structuring of materials at subwavelength scales underlies the concept of metamaterials possessing exotic properties beyond those of the constituent media. Temporal modulation of material parameters enables further functionalities. Here, we show that high-frequency oscillations of spatially uniform magnetization generate an effective dynamic axion field embedding the amplitude and phase of magnetization oscillations. This allows one to map ultrafast magnetization dynamics using a probe signal with much lower frequency.","sentences":["Spatial structuring of materials at subwavelength scales underlies the concept of metamaterials possessing exotic properties beyond those of the constituent media.","Temporal modulation of material parameters enables further functionalities.","Here, we show that high-frequency oscillations of spatially uniform magnetization generate an effective dynamic axion field embedding the amplitude and phase of magnetization oscillations.","This allows one to map ultrafast magnetization dynamics using a probe signal with much lower frequency."],"url":"http://arxiv.org/abs/2405.19160v1","category":"physics.optics"}
{"created":"2024-05-29 15:00:42","title":"Which are the True Defeasible Logics?","abstract":"The class of defeasible logics is only vaguely defined -- it is defined by a few exemplars and the general idea of efficient reasoning with defeasible rules. The recent definition of the defeasible logic $DL(\\partial_{||})$ introduced new features to such logics, which have repercussions that we explore. In particular, we define a class of logics that accommodates the new logic while retaining the traditional properties of defeasible logics.","sentences":["The class of defeasible logics is only vaguely defined -- it is defined by a few exemplars and the general idea of efficient reasoning with defeasible rules.","The recent definition of the defeasible logic $DL(\\partial_{||})$ introduced new features to such logics, which have repercussions that we explore.","In particular, we define a class of logics that accommodates the new logic while retaining the traditional properties of defeasible logics."],"url":"http://arxiv.org/abs/2405.19157v1","category":"cs.LO"}
{"created":"2024-05-29 14:59:49","title":"A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning","abstract":"Continual learning with deep neural networks presents challenges distinct from both the fixed-dataset and convex continual learning regimes. One such challenge is plasticity loss, wherein a neural network trained in an online fashion displays a degraded ability to fit new tasks. This problem has been extensively studied in both supervised learning and off-policy reinforcement learning (RL), where a number of remedies have been proposed. Still, plasticity loss has received less attention in the on-policy deep RL setting. Here we perform an extensive set of experiments examining plasticity loss and a variety of mitigation methods in on-policy deep RL. We demonstrate that plasticity loss is pervasive under domain shift in this regime, and that a number of methods developed to resolve it in other settings fail, sometimes even resulting in performance that is worse than performing no intervention at all. In contrast, we find that a class of ``regenerative'' methods are able to consistently mitigate plasticity loss in a variety of contexts, including in gridworld tasks and more challenging environments like Montezuma's Revenge and ProcGen.","sentences":["Continual learning with deep neural networks presents challenges distinct from both the fixed-dataset and convex continual learning regimes.","One such challenge is plasticity loss, wherein a neural network trained in an online fashion displays a degraded ability to fit new tasks.","This problem has been extensively studied in both supervised learning and off-policy reinforcement learning (RL), where a number of remedies have been proposed.","Still, plasticity loss has received less attention in the on-policy deep RL setting.","Here we perform an extensive set of experiments examining plasticity loss and a variety of mitigation methods in on-policy deep RL.","We demonstrate that plasticity loss is pervasive under domain shift in this regime, and that a number of methods developed to resolve it in other settings fail, sometimes even resulting in performance that is worse than performing no intervention at all.","In contrast, we find that a class of ``regenerative'' methods are able to consistently mitigate plasticity loss in a variety of contexts, including in gridworld tasks and more challenging environments like Montezuma's Revenge and ProcGen."],"url":"http://arxiv.org/abs/2405.19153v1","category":"cs.LG"}
{"created":"2024-05-29 14:59:15","title":"First high peak and average power single-pass THz FEL based on high brightness photoinjector","abstract":"Advanced experiments using THz pump and X-ray probe pulses at modern free-electron lasers (FELs) like the European X-ray FEL require a frequency-tunable, high-power, narrow-band THz source maintaining the repetition rate and pulse structure of the X-ray pulses. This paper reports the first results from a THz source, that is based on a single-pass high-gain THz FEL operating with a central wavelength of 100 micrometers. The THz FEL prototype is currently in operation at the Photo Injector Test facility at DESY in Zeuthen (PITZ) and uses the same type of electron source as the European XFEL photo injector. A self-amplified spontaneous emission (SASE) FEL was envisioned as the main mechanism for generating the THz pulses. Although the THz FEL at PITZ is supposed to use the same mechanism as at X-ray facilities, it cannot be considered as a simple scaling of the radiation wavelength because there is a large difference in the number of electrons per radiation wavelength, which is five orders of magnitude higher for the THz case. The bunching factor arising from the electron beam current profile contributes strongly to the initial spontaneous emission starting the FEL process. Proof-of-principle experiments were done at PITZ using an LCLS-I undulator to generate the first high-power, high-repetition-rate single-pass THz FEL radiation. Electron bunches with a beam energy of ~17 MeV and a bunch charge of up to several nC are used to generate THz pulses with a pulse energy of several tens of microjoules. For example, for an electron beam with a charge of ~2.4 nC, more than 100 microjoules were generated at a central wavelength of 100 micrometers. The narrowband spectrum was also demonstrated by spectral measurements. These proof-of-principle experiments pave the way for a tunable, high-repetition-rate THz source providing pulses with energies in the millijoule range.","sentences":["Advanced experiments using THz pump and X-ray probe pulses at modern free-electron lasers (FELs) like the European X-ray FEL require a frequency-tunable, high-power, narrow-band THz source maintaining the repetition rate and pulse structure of the X-ray pulses.","This paper reports the first results from a THz source, that is based on a single-pass high-gain THz FEL operating with a central wavelength of 100 micrometers.","The THz FEL prototype is currently in operation at the Photo Injector Test facility at DESY in Zeuthen (PITZ) and uses the same type of electron source as the European XFEL photo injector.","A self-amplified spontaneous emission (SASE) FEL was envisioned as the main mechanism for generating the THz pulses.","Although the THz FEL at PITZ is supposed to use the same mechanism as at X-ray facilities, it cannot be considered as a simple scaling of the radiation wavelength because there is a large difference in the number of electrons per radiation wavelength, which is five orders of magnitude higher for the THz case.","The bunching factor arising from the electron beam current profile contributes strongly to the initial spontaneous emission starting the FEL process.","Proof-of-principle experiments were done at PITZ using an LCLS-I undulator to generate the first high-power, high-repetition-rate single-pass THz FEL radiation.","Electron bunches with a beam energy of ~17 MeV and a bunch charge of up to several nC are used to generate THz pulses with a pulse energy of several tens of microjoules.","For example, for an electron beam with a charge of ~2.4 nC, more than 100 microjoules were generated at a central wavelength of 100 micrometers.","The narrowband spectrum was also demonstrated by spectral measurements.","These proof-of-principle experiments pave the way for a tunable, high-repetition-rate THz source providing pulses with energies in the millijoule range."],"url":"http://arxiv.org/abs/2405.19152v1","category":"physics.acc-ph"}
{"created":"2024-05-29 14:52:10","title":"CaLa: Complementary Association Learning for Augmenting Composed Image Retrieval","abstract":"Composed Image Retrieval (CIR) involves searching for target images based on an image-text pair query. While current methods treat this as a query-target matching problem, we argue that CIR triplets contain additional associations beyond this primary relation. In our paper, we identify two new relations within triplets, treating each triplet as a graph node. Firstly, we introduce the concept of text-bridged image alignment, where the query text serves as a bridge between the query image and the target image. We propose a hinge-based cross-attention mechanism to incorporate this relation into network learning. Secondly, we explore complementary text reasoning, considering CIR as a form of cross-modal retrieval where two images compose to reason about complementary text. To integrate these perspectives effectively, we design a twin attention-based compositor. By combining these complementary associations with the explicit query pair-target image relation, we establish a comprehensive set of constraints for CIR. Our framework, CaLa (Complementary Association Learning for Augmenting Composed Image Retrieval), leverages these insights. We evaluate CaLa on CIRR and FashionIQ benchmarks with multiple backbones, demonstrating its superiority in composed image retrieval.","sentences":["Composed Image Retrieval (CIR) involves searching for target images based on an image-text pair query.","While current methods treat this as a query-target matching problem, we argue that CIR triplets contain additional associations beyond this primary relation.","In our paper, we identify two new relations within triplets, treating each triplet as a graph node.","Firstly, we introduce the concept of text-bridged image alignment, where the query text serves as a bridge between the query image and the target image.","We propose a hinge-based cross-attention mechanism to incorporate this relation into network learning.","Secondly, we explore complementary text reasoning, considering CIR as a form of cross-modal retrieval where two images compose to reason about complementary text.","To integrate these perspectives effectively, we design a twin attention-based compositor.","By combining these complementary associations with the explicit query pair-target image relation, we establish a comprehensive set of constraints for CIR.","Our framework, CaLa (Complementary Association Learning for Augmenting Composed Image Retrieval), leverages these insights.","We evaluate CaLa on CIRR and FashionIQ benchmarks with multiple backbones, demonstrating its superiority in composed image retrieval."],"url":"http://arxiv.org/abs/2405.19149v1","category":"cs.CV"}
{"created":"2024-05-29 14:47:24","title":"Identification of defects and the origins of surface noise on hydrogen-terminated (100) diamond","abstract":"Near-surface nitrogen-vacancy centres are critical to many diamond-based quantum technologies such as information processors and nanosensors. Surface defects play an important role in the design and performance of these devices. The targeted creation of defects is central to proposed bottom-up approaches to nanofabrication of quantum diamond processors, and uncontrolled surface defects may generate noise and charge trapping which degrade shallow NV device performance. Surface preparation protocols may be able to control the production of desired defects and eliminate unwanted defects, but only if their atomic structure can first be conclusively identified. This work uses a combination of scanning tunnelling microscopy (STM) imaging and first-principles simulations to identify several surface defects on H:C(100)-2x1 surfaces prepared using chemical vapour deposition (CVD). The atomic structure of these defects is elucidated, from which the microscopic origins of magnetic noise and charge trapping is determined based on modelling of their paramagnetic properties and acceptor states. Rudimentary control of these deleterious properties is demonstrated through STM tip-induced manipulation of the defect structure. Furthermore, the results validate accepted models for CVD diamond growth by identifying key adsorbates responsible for nucleation of new layers.","sentences":["Near-surface nitrogen-vacancy centres are critical to many diamond-based quantum technologies such as information processors and nanosensors.","Surface defects play an important role in the design and performance of these devices.","The targeted creation of defects is central to proposed bottom-up approaches to nanofabrication of quantum diamond processors, and uncontrolled surface defects may generate noise and charge trapping which degrade shallow NV device performance.","Surface preparation protocols may be able to control the production of desired defects and eliminate unwanted defects, but only if their atomic structure can first be conclusively identified.","This work uses a combination of scanning tunnelling microscopy (STM) imaging and first-principles simulations to identify several surface defects on H:C(100)-2x1 surfaces prepared using chemical vapour deposition (CVD).","The atomic structure of these defects is elucidated, from which the microscopic origins of magnetic noise and charge trapping is determined based on modelling of their paramagnetic properties and acceptor states.","Rudimentary control of these deleterious properties is demonstrated through STM tip-induced manipulation of the defect structure.","Furthermore, the results validate accepted models for CVD diamond growth by identifying key adsorbates responsible for nucleation of new layers."],"url":"http://arxiv.org/abs/2405.19140v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-29 14:47:01","title":"DGRC: An Effective Fine-tuning Framework for Distractor Generation in Chinese Multi-choice Reading Comprehension","abstract":"When evaluating a learner's knowledge proficiency, the multiple-choice question is an efficient and widely used format in standardized tests. Nevertheless, generating these questions, particularly plausible distractors (incorrect options), poses a considerable challenge. Generally, the distractor generation can be classified into cloze-style distractor generation (CDG) and natural questions distractor generation (NQDG). In contrast to the CDG, utilizing pre-trained language models (PLMs) for NQDG presents three primary challenges: (1) PLMs are typically trained to generate ``correct'' content, like answers, while rarely trained to generate ``plausible\" content, like distractors; (2) PLMs often struggle to produce content that aligns well with specific knowledge and the style of exams; (3) NQDG necessitates the model to produce longer, context-sensitive, and question-relevant distractors. In this study, we introduce a fine-tuning framework named DGRC for NQDG in Chinese multi-choice reading comprehension from authentic examinations. DGRC comprises three major components: hard chain-of-thought, multi-task learning, and generation mask patterns. The experiment results demonstrate that DGRC significantly enhances generation performance, achieving a more than 2.5-fold improvement in BLEU scores.","sentences":["When evaluating a learner's knowledge proficiency, the multiple-choice question is an efficient and widely used format in standardized tests.","Nevertheless, generating these questions, particularly plausible distractors (incorrect options), poses a considerable challenge.","Generally, the distractor generation can be classified into cloze-style distractor generation (CDG) and natural questions distractor generation (NQDG).","In contrast to the CDG, utilizing pre-trained language models (PLMs) for NQDG presents three primary challenges: (1) PLMs are typically trained to generate ``correct'' content, like answers, while rarely trained to generate ``plausible\" content, like distractors; (2) PLMs often struggle to produce content that aligns well with specific knowledge and the style of exams; (3) NQDG necessitates the model to produce longer, context-sensitive, and question-relevant distractors.","In this study, we introduce a fine-tuning framework named DGRC for NQDG in Chinese multi-choice reading comprehension from authentic examinations.","DGRC comprises three major components: hard chain-of-thought, multi-task learning, and generation mask patterns.","The experiment results demonstrate that DGRC significantly enhances generation performance, achieving a more than 2.5-fold improvement in BLEU scores."],"url":"http://arxiv.org/abs/2405.19139v1","category":"cs.CL"}
{"created":"2024-05-29 14:42:24","title":"Multi-Channel Multi-Step Spectrum Prediction Using Transformer and Stacked Bi-LSTM","abstract":"Spectrum prediction is considered as a key technology to assist spectrum decision. Despite the great efforts that have been put on the construction of spectrum prediction, achieving accurate spectrum prediction emphasizes the need for more advanced solutions. In this paper, we propose a new multichannel multi-step spectrum prediction method using Transformer and stacked bidirectional LSTM (Bi- LSTM), named TSB. Specifically, we use multi-head attention and stacked Bi-LSTM to build a new Transformer based on encoder-decoder architecture. The self-attention mechanism composed of multiple layers of multi-head attention can continuously attend to all positions of the multichannel spectrum sequences. The stacked Bi-LSTM can learn these focused coding features by multi-head attention layer by layer. The advantage of this fusion mode is that it can deeply capture the long-term dependence of multichannel spectrum data. We have conducted extensive experiments on a dataset generated by a real simulation platform. The results show that the proposed algorithm performs better than the baselines.","sentences":["Spectrum prediction is considered as a key technology to assist spectrum decision.","Despite the great efforts that have been put on the construction of spectrum prediction, achieving accurate spectrum prediction emphasizes the need for more advanced solutions.","In this paper, we propose a new multichannel multi-step spectrum prediction method using Transformer and stacked bidirectional LSTM (Bi- LSTM), named TSB.","Specifically, we use multi-head attention and stacked Bi-LSTM to build a new Transformer based on encoder-decoder architecture.","The self-attention mechanism composed of multiple layers of multi-head attention can continuously attend to all positions of the multichannel spectrum sequences.","The stacked Bi-LSTM can learn these focused coding features by multi-head attention layer by layer.","The advantage of this fusion mode is that it can deeply capture the long-term dependence of multichannel spectrum data.","We have conducted extensive experiments on a dataset generated by a real simulation platform.","The results show that the proposed algorithm performs better than the baselines."],"url":"http://arxiv.org/abs/2405.19138v1","category":"eess.SP"}
{"created":"2024-05-29 14:41:43","title":"Photometric Completeness Modelled With Neural Networks","abstract":"In almost any study involving optical/NIR photometry, understanding the completeness of detection and recovery is an essential part of the work. The recovery fraction is, in general, a function of several variables including magnitude, color, background sky noise, and crowding. We explore how completeness can be modelled, {with the use of artificial-star tests,} in a way that includes all of these parameters \\emph{simultaneously} within a neural network (NN) framework. The method is able to manage common issues including asymmetric completeness functions and the bilinear dependence of the detection limit on color index. We test the method with two sample HST (Hubble Space Telescope) datasets: the first involves photometry of the star cluster population around the giant Perseus galaxy NGC 1275, and the second involves the halo-star population in the nearby elliptical galaxy NGC 3377. The NN-based method achieves a classification accuracy of $>$\\,94\\%, and produces results entirely consistent with more traditional techniques for determining completeness. Additional advantages of the method are that none of the issues arising from binning of the data are present, and that a recovery probability can be assigned to every individual star in the real photometry. Our data, models, and code (called COINTOSS) can be found online on Zenodo at the following link: https://doi.org/10.5281/zenodo.8306488.","sentences":["In almost any study involving optical/NIR photometry, understanding the completeness of detection and recovery is an essential part of the work.","The recovery fraction is, in general, a function of several variables including magnitude, color, background sky noise, and crowding.","We explore how completeness can be modelled, {with the use of artificial-star tests,} in a way that includes all of these parameters \\emph{simultaneously} within a neural network (NN) framework.","The method is able to manage common issues including asymmetric completeness functions and the bilinear dependence of the detection limit on color index.","We test the method with two sample HST (Hubble Space Telescope) datasets: the first involves photometry of the star cluster population around the giant Perseus galaxy NGC 1275, and the second involves the halo-star population in the nearby elliptical galaxy NGC 3377.","The NN-based method achieves a classification accuracy of $>$\\,94\\%, and produces results entirely consistent with more traditional techniques for determining completeness.","Additional advantages of the method are that none of the issues arising from binning of the data are present, and that a recovery probability can be assigned to every individual star in the real photometry.","Our data, models, and code (called COINTOSS) can be found online on Zenodo at the following link: https://doi.org/10.5281/zenodo.8306488."],"url":"http://arxiv.org/abs/2405.19135v1","category":"astro-ph.IM"}
{"created":"2024-05-29 14:38:32","title":"Analyzing Chat Protocols of Novice Programmers Solving Introductory Programming Tasks with ChatGPT","abstract":"Large Language Models (LLMs) have taken the world by storm, and students are assumed to use related tools at a great scale. In this research paper we aim to gain an understanding of how introductory programming students chat with LLMs and related tools, e.g., ChatGPT-3.5. To address this goal, computing students at a large German university were motivated to solve programming exercises with the assistance of ChatGPT as part of their weekly introductory course exercises. Then students (n=213) submitted their chat protocols (with 2335 prompts in sum) as data basis for this analysis. The data was analyzed w.r.t. the prompts, frequencies, the chats' progress, contents, and other use pattern, which revealed a great variety of interactions, both potentially supportive and concerning. Learning about students' interactions with ChatGPT will help inform and align teaching practices and instructions for future introductory programming courses in higher education.","sentences":["Large Language Models (LLMs) have taken the world by storm, and students are assumed to use related tools at a great scale.","In this research paper we aim to gain an understanding of how introductory programming students chat with LLMs and related tools, e.g., ChatGPT-3.5.","To address this goal, computing students at a large German university were motivated to solve programming exercises with the assistance of ChatGPT as part of their weekly introductory course exercises.","Then students (n=213) submitted their chat protocols (with 2335 prompts in sum) as data basis for this analysis.","The data was analyzed w.r.t.","the prompts, frequencies, the chats' progress, contents, and other use pattern, which revealed a great variety of interactions, both potentially supportive and concerning.","Learning about students' interactions with ChatGPT will help inform and align teaching practices and instructions for future introductory programming courses in higher education."],"url":"http://arxiv.org/abs/2405.19132v1","category":"cs.AI"}
{"created":"2024-05-29 14:34:48","title":"Cognitive biases can move opinion dynamics from consensus to signatures of transient chaos","abstract":"Interest in how democracies form consensus has increased recently, with statistical physics and economics approaches both suggesting that there is convergence to a fixed point in belief networks, but with fluctuations in opinions when there are ``stubborn'' voters. We modify a model of opinion dynamics in which agents are fully Bayesian to account for two cognitive biases: confirmation bias and in-group bias. Confirmation bias occurs when the received information is considered to be more likely when it aligns with the receiver's beliefs. In-group bias occurs when the receiver further considers the information to be more likely when the receiver's beliefs and the sender's beliefs are aligned. We find that when there are no cognitive biases, a network of agents always converges to complete consensus. With confirmation bias alone, polarization can occur. With both biases present, consensus and polarization are possible, but when agents attempt to counteract confirmation bias, there can be signatures of transient chaos and ongoing opinion fluctuations. Based on this simple model, we conjecture that complex opinion fluctuations might be a generic feature of opinion dynamics when agents are Bayesian with biases.","sentences":["Interest in how democracies form consensus has increased recently, with statistical physics and economics approaches both suggesting that there is convergence to a fixed point in belief networks, but with fluctuations in opinions when there are ``stubborn'' voters.","We modify a model of opinion dynamics in which agents are fully Bayesian to account for two cognitive biases: confirmation bias and in-group bias.","Confirmation bias occurs when the received information is considered to be more likely when it aligns with the receiver's beliefs.","In-group bias occurs when the receiver further considers the information to be more likely when the receiver's beliefs and the sender's beliefs are aligned.","We find that when there are no cognitive biases, a network of agents always converges to complete consensus.","With confirmation bias alone, polarization can occur.","With both biases present, consensus and polarization are possible, but when agents attempt to counteract confirmation bias, there can be signatures of transient chaos and ongoing opinion fluctuations.","Based on this simple model, we conjecture that complex opinion fluctuations might be a generic feature of opinion dynamics when agents are Bayesian with biases."],"url":"http://arxiv.org/abs/2405.19128v1","category":"physics.bio-ph"}
{"created":"2024-05-29 14:33:16","title":"Fourier transform and Radon transform for mixed Hodge modules","abstract":"We give a generalization to bi-filtered $\\mathcal D$-modules underlying mixed Hodge modules of the relation between microlocalization along $f_1,...,f_r \\in \\mathcal O_X(X)$ and vanishing cycles along $g = \\sum_{i=1}^r y_i f_i$. This leads to an interesting isomorphism between localization triangles. As an application, we use these results to compare the $k$-plane Radon transform and the Fourier-Laplace transform for mixed Hodge modules. This is then applied to the Hodge module structure of certain GKZ systems.","sentences":["We give a generalization to bi-filtered $\\mathcal D$-modules underlying mixed Hodge modules of the relation between microlocalization along $f_1,...,f_r \\in \\mathcal O_X(X)$ and vanishing cycles along $g = \\sum_{i=1}^r y_i f_i$.","This leads to an interesting isomorphism between localization triangles.","As an application, we use these results to compare the $k$-plane Radon transform and the Fourier-Laplace transform for mixed Hodge modules.","This is then applied to the Hodge module structure of certain GKZ systems."],"url":"http://arxiv.org/abs/2405.19127v1","category":"math.AG"}
{"created":"2024-05-29 14:30:06","title":"Torus diffeomorphisms with parabolic and non-proper actions on the fine curve graph and their generalized rotation sets","abstract":"We prove that a generic element of the Anosov-Katok class of the torus, $\\overline{\\mathcal{O}}^{\\infty}(\\mathbb{T}^2)$, acts parabolically and non-properly on the fine curve graph $C^{\\dagger}(\\mathbb{T}^2)$. Additionally, we show that a generic element of $\\overline{\\mathcal{O}}^{\\infty}(\\mathbb{T}^2)$ admits generalized rotation sets of any point-symmetric compact convex homothety type in the plane.","sentences":["We prove that a generic element of the Anosov-Katok class of the torus, $\\overline{\\mathcal{O}}^{\\infty}(\\mathbb{T}^2)$, acts parabolically and non-properly on the fine curve graph $C^{\\dagger}(\\mathbb{T}^2)$. Additionally, we show that a generic element of $\\overline{\\mathcal{O}}^{\\infty}(\\mathbb{T}^2)$ admits generalized rotation sets of any point-symmetric compact convex homothety type in the plane."],"url":"http://arxiv.org/abs/2405.19123v1","category":"math.DS"}
{"created":"2024-05-29 14:28:17","title":"Apparent horizon tracking in supercritical solutions of the Einstein-scalar field equations in spherical symmetry in affine-null coordinates","abstract":"Choptuik's critical phenomena in general relativity is revisited in the affine-null metric formulation of Einstein's equations for a massless scalar field in spherical symmetry. Numerical solutions are obtained by evolution of initial data using pseudo-spectral methods. The underlying system consists of differential equations along the outgoing null rays which can be solved in sequential form. A new two-parameter family of initial data is presented for which these equations can be integrated analytically. Specific choices of the initial data parameters correspond to either an asymptotically flat null cone, a black hole event horizon or the singular interior of a black hole. Our main focus is on the interior features of a black hole, for which the affine-null system is especially well adapted. We present both analytic and numerical results describing the geometric properties of the apparent horizon and final singularity. Using a re-gridding technique for the affine parameter, numerical evolution of initially asymptotically flat supercritical data can be continued inside the event horizon and track the apparent horizon up to the formation of the final singularity.","sentences":["Choptuik's critical phenomena in general relativity is revisited in the affine-null metric formulation of Einstein's equations for a massless scalar field in spherical symmetry.","Numerical solutions are obtained by evolution of initial data using pseudo-spectral methods.","The underlying system consists of differential equations along the outgoing null rays which can be solved in sequential form.","A new two-parameter family of initial data is presented for which these equations can be integrated analytically.","Specific choices of the initial data parameters correspond to either an asymptotically flat null cone, a black hole event horizon or the singular interior of a black hole.","Our main focus is on the interior features of a black hole, for which the affine-null system is especially well adapted.","We present both analytic and numerical results describing the geometric properties of the apparent horizon and final singularity.","Using a re-gridding technique for the affine parameter, numerical evolution of initially asymptotically flat supercritical data can be continued inside the event horizon and track the apparent horizon up to the formation of the final singularity."],"url":"http://arxiv.org/abs/2405.19122v1","category":"gr-qc"}
{"created":"2024-05-29 14:28:08","title":"Spatio-Spectral Graph Neural Networks","abstract":"Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of l-step MPGNNs are that their \"receptive field\" is typically limited to the l-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S$^2$GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S$^2$GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain general-purpose S$^2$GNNs, we propose spectrally parametrized filters for directed graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs scale to millions of nodes.","sentences":["Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data.","However, key limitations of l-step MPGNNs are that their \"receptive field\" is typically limited to the l-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing.","Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters.","Parameterizing filters partially in the frequency domain enables global yet efficient information propagation.","We show that S$^2$GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs.","Further, rethinking graph convolutions at a fundamental level unlocks new design spaces.","For example, S$^2$GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Lehman (WL) test.","Moreover, to obtain general-purpose S$^2$GNNs, we propose spectrally parametrized filters for directed graphs.","S$^2$GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling.","On a 40 GB GPU, S$^2$GNNs scale to millions of nodes."],"url":"http://arxiv.org/abs/2405.19121v1","category":"cs.LG"}
{"created":"2024-05-29 14:24:42","title":"ChartFormer: A Large Vision Language Model for Converting Chart Images into Tactile Accessible SVGs","abstract":"Visualizations, such as charts, are crucial for interpreting complex data. However, they are often provided as raster images, which are not compatible with assistive technologies for people with blindness and visual impairments, such as embossed papers or tactile displays. At the same time, creating accessible vector graphics requires a skilled sighted person and is time-intensive. In this work, we leverage advancements in the field of chart analysis to generate tactile charts in an end-to-end manner. Our three key contributions are as follows: (1) introducing the ChartFormer model trained to convert raster chart images into tactile-accessible SVGs, (2) training this model on the Chart2Tactile dataset, a synthetic chart dataset we created following accessibility standards, and (3) evaluating the effectiveness of our SVGs through a pilot user study with an refreshable two-dimensional tactile display. Our work is publicly available at https://github.com/nsothman/ChartFormer .","sentences":["Visualizations, such as charts, are crucial for interpreting complex data.","However, they are often provided as raster images, which are not compatible with assistive technologies for people with blindness and visual impairments, such as embossed papers or tactile displays.","At the same time, creating accessible vector graphics requires a skilled sighted person and is time-intensive.","In this work, we leverage advancements in the field of chart analysis to generate tactile charts in an end-to-end manner.","Our three key contributions are as follows: (1) introducing the ChartFormer model trained to convert raster chart images into tactile-accessible SVGs, (2) training this model on the Chart2Tactile dataset, a synthetic chart dataset we created following accessibility standards, and (3) evaluating the effectiveness of our SVGs through a pilot user study with an refreshable two-dimensional tactile display.","Our work is publicly available at https://github.com/nsothman/ChartFormer ."],"url":"http://arxiv.org/abs/2405.19117v1","category":"cs.CV"}
{"created":"2024-05-29 14:23:20","title":"Typical Ramsey properties of the primes, abelian groups and other discrete structures","abstract":"Given a matrix $A$ with integer entries, a subset $S$ of an abelian group and $r \\in \\mathbb N$, we say that $S$ is $(A,r)$-Rado if any $r$-colouring of $S$ yields a monochromatic solution to the system of equations $Ax=0$. A classical result of Rado characterises all those matrices $A$ such that $\\mathbb N$ is $(A,r)$-Rado for all $r \\in \\mathbb N$. R\\\"odl and Ruci\\'nski and Friedgut, R\\\"odl and Schacht proved a random version of Rado's theorem where one considers a random subset of $[n]:=\\{1,\\dots,n\\}$ instead of $\\mathbb N$.   In this paper, we investigate the analogous random Ramsey problem in the more general setting of abelian groups. Given a sequence $(S_n)_{n\\in\\mathbb N}$ of finite subsets of abelian groups, let $S_{n,p}$ be a random subset of $S_n$ obtained by including each element of $S_n$ independently with probability $p$. We are interested in determining the probability threshold $\\hat p:=\\hat p(n)$ such that   $$\\lim _{n \\rightarrow \\infty} \\mathbb P [ S_{n,p} \\text{ is } (A,r)\\text{-Rado}]= \\begin{cases} 0 &\\text{ if } p=o(\\hat p); \\\\ 1 &\\text{ if } p=\\omega(\\hat p). \\end{cases}$$   Our main result, which we coin the random Rado lemma, is a general black box to tackle problems of this type. Using this tool in conjunction with a series of supersaturation results, we determine the probability threshold for a number of different cases. A consequence of the Green-Tao theorem is the van der Waerden theorem for the primes: every finite colouring of the primes contains arbitrarily long monochromatic arithmetic progressions. Using our machinery, we obtain a random version of this result. We also prove a novel supersaturation result for $S_n:=[n]^d$ and use it to prove an integer lattice generalisation of the random version of Rado's theorem. Various threshold results for abelian groups are also given.","sentences":["Given a matrix $A$ with integer entries, a subset $S$ of an abelian group and $r \\in \\mathbb N$, we say that $S$ is $(A,r)$-Rado if any $r$-colouring of $S$ yields a monochromatic solution to the system of equations $Ax=0$. A classical result of Rado characterises all those matrices $A$ such that $\\mathbb N$ is $(A,r)$-Rado for all $r \\in \\mathbb N$. R\\\"odl and Ruci\\'nski and Friedgut, R\\\"odl and Schacht proved a random version of Rado's theorem where one considers a random subset of $[n]:=\\{1,\\dots,n\\}$ instead of $\\mathbb N$.   In this paper, we investigate the analogous random Ramsey problem in the more general setting of abelian groups.","Given a sequence $(S_n)_{n\\in\\mathbb N}$ of finite subsets of abelian groups, let $S_{n,p}$ be a random subset of $S_n$ obtained by including each element of $S_n$ independently with probability $p$. We are interested in determining the probability threshold $\\hat p:=\\hat p(n)$ such that   $$\\lim _{n \\rightarrow \\infty} \\mathbb P [ S_{n,p} \\text{ is } (A,r)\\text{-Rado}]= \\begin{cases} 0 &\\text{ if } p=o(\\hat p); \\\\ 1 &\\text{ if } p=\\omega(\\hat p).","\\end{cases}$$   ","Our main result, which we coin the random Rado lemma, is a general black box to tackle problems of this type.","Using this tool in conjunction with a series of supersaturation results, we determine the probability threshold for a number of different cases.","A consequence of the Green-Tao theorem is the van der Waerden theorem for the primes: every finite colouring of the primes contains arbitrarily long monochromatic arithmetic progressions.","Using our machinery, we obtain a random version of this result.","We also prove a novel supersaturation result for $S_n:=[n]^d$ and use it to prove an integer lattice generalisation of the random version of Rado's theorem.","Various threshold results for abelian groups are also given."],"url":"http://arxiv.org/abs/2405.19113v1","category":"math.CO"}
{"created":"2024-05-29 14:20:46","title":"Reconstructing Interpretable Features in Computational Super-Resolution microscopy via Regularized Latent Search","abstract":"Supervised deep learning approaches can artificially increase the resolution of microscopy images by learning a mapping between two image resolutions or modalities. However, such methods often require a large set of hard-to-get low-res/high-res image pairs and produce synthetic images with a moderate increase in resolution. Conversely, recent methods based on GAN latent search offered a drastic increase in resolution without the need of paired images. However, they offer limited reconstruction of the high-resolution image interpretable features. Here, we propose a robust super-resolution method based on regularized latent search~(RLS) that offers an actionable balance between fidelity to the ground-truth and realism of the recovered image given a distribution prior. The latter allows to split the analysis of a low-resolution image into a computational super-resolution task performed by deep learning followed by a quantification task performed by a handcrafted algorithm and based on interpretable biological features. This two-step process holds potential for various applications such as diagnostics on mobile devices, where the main aim is not to recover the high-resolution details of a specific sample but rather to obtain high-resolution images that preserve explainable and quantifiable differences between conditions.","sentences":["Supervised deep learning approaches can artificially increase the resolution of microscopy images by learning a mapping between two image resolutions or modalities.","However, such methods often require a large set of hard-to-get low-res/high-res image pairs and produce synthetic images with a moderate increase in resolution.","Conversely, recent methods based on GAN latent search offered a drastic increase in resolution without the need of paired images.","However, they offer limited reconstruction of the high-resolution image interpretable features.","Here, we propose a robust super-resolution method based on regularized latent search~(RLS) that offers an actionable balance between fidelity to the ground-truth and realism of the recovered image given a distribution prior.","The latter allows to split the analysis of a low-resolution image into a computational super-resolution task performed by deep learning followed by a quantification task performed by a handcrafted algorithm and based on interpretable biological features.","This two-step process holds potential for various applications such as diagnostics on mobile devices, where the main aim is not to recover the high-resolution details of a specific sample but rather to obtain high-resolution images that preserve explainable and quantifiable differences between conditions."],"url":"http://arxiv.org/abs/2405.19112v1","category":"eess.IV"}
{"created":"2024-05-29 14:19:57","title":"Alt4Blind: A User Interface to Simplify Charts Alt-Text Creation","abstract":"Alternative Texts (Alt-Text) for chart images are essential for making graphics accessible to people with blindness and visual impairments. Traditionally, Alt-Text is manually written by authors but often encounters issues such as oversimplification or complication. Recent trends have seen the use of AI for Alt-Text generation. However, existing models are susceptible to producing inaccurate or misleading information. We address this challenge by retrieving high-quality alt-texts from similar chart images, serving as a reference for the user when creating alt-texts. Our three contributions are as follows: (1) we introduce a new benchmark comprising 5,000 real images with semantically labeled high-quality Alt-Texts, collected from Human Computer Interaction venues. (2) We developed a deep learning-based model to rank and retrieve similar chart images that share the same visual and textual semantics. (3) We designed a user interface (UI) to facilitate the alt-text creation process. Our preliminary interviews and investigations highlight the usability of our UI. For the dataset and further details, please refer to our project page: https://moured.github.io/alt4blind/.","sentences":["Alternative Texts (Alt-Text) for chart images are essential for making graphics accessible to people with blindness and visual impairments.","Traditionally, Alt-Text is manually written by authors but often encounters issues such as oversimplification or complication.","Recent trends have seen the use of AI for Alt-Text generation.","However, existing models are susceptible to producing inaccurate or misleading information.","We address this challenge by retrieving high-quality alt-texts from similar chart images, serving as a reference for the user when creating alt-texts.","Our three contributions are as follows: (1) we introduce a new benchmark comprising 5,000 real images with semantically labeled high-quality Alt-Texts, collected from Human Computer Interaction venues.","(2) We developed a deep learning-based model to rank and retrieve similar chart images that share the same visual and textual semantics.","(3) We designed a user interface (UI) to facilitate the alt-text creation process.","Our preliminary interviews and investigations highlight the usability of our UI.","For the dataset and further details, please refer to our project page: https://moured.github.io/alt4blind/."],"url":"http://arxiv.org/abs/2405.19111v1","category":"cs.CV"}
{"created":"2024-05-29 14:17:17","title":"Benchmarking topological insulator-based photodetector: Photo-induced excess noise and role of barrier height inhomogeneity","abstract":"Topological insulators (TIs) with symmetry-protected surface states, offer exciting opportunities for next-generation photonic and optoelectronic device applications. The heterojunctions of TIs and semiconductors (e.g. Si, Ge) have been observed to excellent photo-responsive characteristics. However, the realization of high-frequency operations in these heterojunctions can be hindered by unwanted 1/f (or Flicker) noise and phase noise. Therefore, an in-depth understanding of 1/f noise figures becomes paramount for the effective utilization of such materials.Here we report optoelectronic response and 1/f noise characteristics of a p-n diode fabricated using topological insulator, Bi2Se3 and silicon for potential photo-detector. Through meticulous temperature-dependent current-voltage (I-V) and capacitance-voltage (C-V) measurements, we ascertain crucial parameters like barrier height, ideality factor, and reverse saturation current of the photodetector. The low-frequency 1/f conductance noise spectra suggest a significant presence of trap states influencing the optoelectronic transport properties. The forward noise characteristics exhibit typical 1/f features, having a uni-slope across four decades of frequency, suggesting a homogeneous distribution of barrier height. The spectral and photocurrent-dependent responses show the power law behavior of noise level on photon flux. The hybrid heterojunction demonstrates excellent photo-response and reasonably low noise level, promising signatures for the room-temperature visible photodetector applications.","sentences":["Topological insulators (TIs) with symmetry-protected surface states, offer exciting opportunities for next-generation photonic and optoelectronic device applications.","The heterojunctions of TIs and semiconductors (e.g. Si, Ge) have been observed to excellent photo-responsive characteristics.","However, the realization of high-frequency operations in these heterojunctions can be hindered by unwanted 1/f (or Flicker) noise and phase noise.","Therefore, an in-depth understanding of 1/f noise figures becomes paramount for the effective utilization of such materials.","Here we report optoelectronic response and 1/f noise characteristics of a p-n diode fabricated using topological insulator, Bi2Se3 and silicon for potential photo-detector.","Through meticulous temperature-dependent current-voltage (I-V) and capacitance-voltage (C-V) measurements, we ascertain crucial parameters like barrier height, ideality factor, and reverse saturation current of the photodetector.","The low-frequency 1/f conductance noise spectra suggest a significant presence of trap states influencing the optoelectronic transport properties.","The forward noise characteristics exhibit typical 1/f features, having a uni-slope across four decades of frequency, suggesting a homogeneous distribution of barrier height.","The spectral and photocurrent-dependent responses show the power law behavior of noise level on photon flux.","The hybrid heterojunction demonstrates excellent photo-response and reasonably low noise level, promising signatures for the room-temperature visible photodetector applications."],"url":"http://arxiv.org/abs/2405.19110v1","category":"cond-mat.str-el"}
{"created":"2024-05-29 14:14:05","title":"PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering","abstract":"Logical reasoning task has attracted great interest since it was proposed. Faced with such a task, current competitive models, even large language models (e.g., ChatGPT and PaLM 2), still perform badly. Previous promising LMs struggle in logical consistency modeling and logical structure perception. To this end, we model the logical reasoning task by transforming each logical sample into reasoning paths and propose an architecture \\textbf{PathReasoner}. It addresses the task from the views of both data and model. To expand the diversity of the logical samples, we propose an atom extension strategy supported by equivalent logical formulas, to form new reasoning paths. From the model perspective, we design a stack of transformer-style blocks. In particular, we propose a path-attention module to joint model in-atom and cross-atom relations with the high-order diffusion strategy. Experiments show that PathReasoner achieves competitive performances on two logical reasoning benchmarks and great generalization abilities.","sentences":["Logical reasoning task has attracted great interest since it was proposed.","Faced with such a task, current competitive models, even large language models (e.g., ChatGPT and PaLM 2), still perform badly.","Previous promising LMs struggle in logical consistency modeling and logical structure perception.","To this end, we model the logical reasoning task by transforming each logical sample into reasoning paths and propose an architecture \\textbf{PathReasoner}.","It addresses the task from the views of both data and model.","To expand the diversity of the logical samples, we propose an atom extension strategy supported by equivalent logical formulas, to form new reasoning paths.","From the model perspective, we design a stack of transformer-style blocks.","In particular, we propose a path-attention module to joint model in-atom and cross-atom relations with the high-order diffusion strategy.","Experiments show that PathReasoner achieves competitive performances on two logical reasoning benchmarks and great generalization abilities."],"url":"http://arxiv.org/abs/2405.19109v1","category":"cs.CL"}
{"created":"2024-05-29 14:13:59","title":"Efficient and operational quantifier of divisibility in terms of channel discrimination","abstract":"The understanding of open quantum systems is crucial for the development of quantum technologies. Of particular relevance is the characterisation of divisible quantum dynamics, seen as a generalisation of Markovian processes to the quantum setting. Here, we propose a way to detect divisibility and quantify how non-divisible a quantum channel is through the concept of channel discrimination. We ask how well we can distinguish generic dynamics from divisible dynamics. We show that this question can be answered efficiently through semidefinite programming, which provides us with an operational and efficient way to quantify divisibility.","sentences":["The understanding of open quantum systems is crucial for the development of quantum technologies.","Of particular relevance is the characterisation of divisible quantum dynamics, seen as a generalisation of Markovian processes to the quantum setting.","Here, we propose a way to detect divisibility and quantify how non-divisible a quantum channel is through the concept of channel discrimination.","We ask how well we can distinguish generic dynamics from divisible dynamics.","We show that this question can be answered efficiently through semidefinite programming, which provides us with an operational and efficient way to quantify divisibility."],"url":"http://arxiv.org/abs/2405.19108v1","category":"quant-ph"}
{"created":"2024-05-29 14:11:29","title":"Offline Regularised Reinforcement Learning for Large Language Models Alignment","abstract":"The dominant framework for alignment of large language models (LLM), whether through reinforcement learning from human feedback or direct preference optimisation, is to learn from preference data. This involves building datasets where each element is a quadruplet composed of a prompt, two independent responses (completions of the prompt) and a human preference between the two independent responses, yielding a preferred and a dis-preferred response. Such data is typically scarce and expensive to collect. On the other hand, \\emph{single-trajectory} datasets where each element is a triplet composed of a prompt, a response and a human feedback is naturally more abundant. The canonical element of such datasets is for instance an LLM's response to a user's prompt followed by a user's feedback such as a thumbs-up/down. Consequently, in this work, we propose DRO, or \\emph{Direct Reward Optimisation}, as a framework and associated algorithms that do not require pairwise preferences. DRO uses a simple mean-squared objective that can be implemented in various ways. We validate our findings empirically, using T5 encoder-decoder language models, and show DRO's performance over selected baselines such as Kahneman-Tversky Optimization (KTO). Thus, we confirm that DRO is a simple and empirically compelling method for single-trajectory policy optimisation.","sentences":["The dominant framework for alignment of large language models (LLM), whether through reinforcement learning from human feedback or direct preference optimisation, is to learn from preference data.","This involves building datasets where each element is a quadruplet composed of a prompt, two independent responses (completions of the prompt) and a human preference between the two independent responses, yielding a preferred and a dis-preferred response.","Such data is typically scarce and expensive to collect.","On the other hand, \\emph{single-trajectory} datasets where each element is a triplet composed of a prompt, a response and a human feedback is naturally more abundant.","The canonical element of such datasets is for instance an LLM's response to a user's prompt followed by a user's feedback such as a thumbs-up/down.","Consequently, in this work, we propose DRO, or \\emph{Direct Reward Optimisation}, as a framework and associated algorithms that do not require pairwise preferences.","DRO uses a simple mean-squared objective that can be implemented in various ways.","We validate our findings empirically, using T5 encoder-decoder language models, and show DRO's performance over selected baselines such as Kahneman-Tversky Optimization (KTO).","Thus, we confirm that DRO is a simple and empirically compelling method for single-trajectory policy optimisation."],"url":"http://arxiv.org/abs/2405.19107v1","category":"cs.LG"}
{"created":"2024-05-29 14:07:52","title":"Phase transitions in debt recycling","abstract":"Debt recycling is an aggressive equity extraction strategy that potentially permits faster repayment of a mortgage. While equity progressively builds up as the mortgage is repaid monthly, mortgage holders may obtain another loan they could use to invest on a risky asset. The wealth produced by a successful investment is then used to repay the mortgage faster. The strategy is riskier than a standard repayment plan since fluctuations in the house market and investment's volatility may also lead to a fast default, as both the mortgage and the liquidity loan are secured against the same good. The general conditions of the mortgage holder and the outside market under which debt recycling may be recommended or discouraged have not been fully investigated. In this paper, to evaluate the effectiveness of traditional monthly mortgage repayment versus debt recycling strategies, we build a dynamical model of debt recycling and study the time evolution of equity and mortgage balance as a function of loan-to-value ratio, house market performance, and return of the risky investment. We find that the model has a rich behavior as a function of its main parameters, showing strongly and weakly successful phases - where the mortgage is eventually repaid faster and slower than the standard monthly repayment strategy, respectively - a default phase where the equity locked in the house vanishes before the mortgage is repaid, signalling a failure of the debt recycling strategy, and a permanent re-mortgaging phase - where further investment funds from the lender are continuously secured, but the mortgage is never fully repaid. The strategy's effectiveness is found to be highly sensitive to the initial mortgage-to-equity ratio, the monthly amount of scheduled repayments, and the economic parameters at the outset. The analytical results are corroborated with numerical simulations with excellent agreement.","sentences":["Debt recycling is an aggressive equity extraction strategy that potentially permits faster repayment of a mortgage.","While equity progressively builds up as the mortgage is repaid monthly, mortgage holders may obtain another loan they could use to invest on a risky asset.","The wealth produced by a successful investment is then used to repay the mortgage faster.","The strategy is riskier than a standard repayment plan since fluctuations in the house market and investment's volatility may also lead to a fast default, as both the mortgage and the liquidity loan are secured against the same good.","The general conditions of the mortgage holder and the outside market under which debt recycling may be recommended or discouraged have not been fully investigated.","In this paper, to evaluate the effectiveness of traditional monthly mortgage repayment versus debt recycling strategies, we build a dynamical model of debt recycling and study the time evolution of equity and mortgage balance as a function of loan-to-value ratio, house market performance, and return of the risky investment.","We find that the model has a rich behavior as a function of its main parameters, showing strongly and weakly successful phases - where the mortgage is eventually repaid faster and slower than the standard monthly repayment strategy, respectively - a default phase where the equity locked in the house vanishes before the mortgage is repaid, signalling a failure of the debt recycling strategy, and a permanent re-mortgaging phase - where further investment funds from the lender are continuously secured, but the mortgage is never fully repaid.","The strategy's effectiveness is found to be highly sensitive to the initial mortgage-to-equity ratio, the monthly amount of scheduled repayments, and the economic parameters at the outset.","The analytical results are corroborated with numerical simulations with excellent agreement."],"url":"http://arxiv.org/abs/2405.19104v1","category":"q-fin.RM"}
{"created":"2024-05-29 14:07:44","title":"Voice Jailbreak Attacks Against GPT-4o","abstract":"Recently, the concept of artificial assistants has evolved from science fiction into real-world applications. GPT-4o, the newest multimodal large language model (MLLM) across audio, vision, and text, has further blurred the line between fiction and reality by enabling more natural human-computer interactions. However, the advent of GPT-4o's voice mode may also introduce a new attack surface. In this paper, we present the first systematic measurement of jailbreak attacks against the voice mode of GPT-4o. We show that GPT-4o demonstrates good resistance to forbidden questions and text jailbreak prompts when directly transferring them to voice mode. This resistance is primarily due to GPT-4o's internal safeguards and the difficulty of adapting text jailbreak prompts to voice mode. Inspired by GPT-4o's human-like behaviors, we propose VoiceJailbreak, a novel voice jailbreak attack that humanizes GPT-4o and attempts to persuade it through fictional storytelling (setting, character, and plot). VoiceJailbreak is capable of generating simple, audible, yet effective jailbreak prompts, which significantly increases the average attack success rate (ASR) from 0.033 to 0.778 in six forbidden scenarios. We also conduct extensive experiments to explore the impacts of interaction steps, key elements of fictional writing, and different languages on VoiceJailbreak's effectiveness and further enhance the attack performance with advanced fictional writing techniques. We hope our study can assist the research community in building more secure and well-regulated MLLMs.","sentences":["Recently, the concept of artificial assistants has evolved from science fiction into real-world applications.","GPT-4o, the newest multimodal large language model (MLLM) across audio, vision, and text, has further blurred the line between fiction and reality by enabling more natural human-computer interactions.","However, the advent of GPT-4o's voice mode may also introduce a new attack surface.","In this paper, we present the first systematic measurement of jailbreak attacks against the voice mode of GPT-4o.","We show that GPT-4o demonstrates good resistance to forbidden questions and text jailbreak prompts when directly transferring them to voice mode.","This resistance is primarily due to GPT-4o's internal safeguards and the difficulty of adapting text jailbreak prompts to voice mode.","Inspired by GPT-4o's human-like behaviors, we propose VoiceJailbreak, a novel voice jailbreak attack that humanizes GPT-4o and attempts to persuade it through fictional storytelling (setting, character, and plot).","VoiceJailbreak is capable of generating simple, audible, yet effective jailbreak prompts, which significantly increases the average attack success rate (ASR) from 0.033 to 0.778 in six forbidden scenarios.","We also conduct extensive experiments to explore the impacts of interaction steps, key elements of fictional writing, and different languages on VoiceJailbreak's effectiveness and further enhance the attack performance with advanced fictional writing techniques.","We hope our study can assist the research community in building more secure and well-regulated MLLMs."],"url":"http://arxiv.org/abs/2405.19103v1","category":"cs.CR"}
{"created":"2024-05-29 14:06:51","title":"Poseidon: Efficient Foundation Models for PDEs","abstract":"We introduce Poseidon, a foundation model for learning the solution operators of PDEs. It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations. A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed. Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics. It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators. We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy. Poseidon also generalizes very well to new physics that is not seen during pretraining. Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks. Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model. Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz.","sentences":["We introduce Poseidon, a foundation model for learning the solution operators of PDEs.","It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations.","A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed.","Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics.","It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators.","We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy.","Poseidon also generalizes very well to new physics that is not seen during pretraining.","Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks.","Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model.","Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz."],"url":"http://arxiv.org/abs/2405.19101v1","category":"cs.LG"}
{"created":"2024-05-29 14:06:09","title":"Enhancing Zero-Shot Facial Expression Recognition by LLM Knowledge Transfer","abstract":"Current facial expression recognition (FER) models are often designed in a supervised learning manner thus are constrained by the lack of large-scale facial expression images with high-quality annotations. Consequently, these models often fail to generalize well, performing poorly on unseen images in training. Vision-language-based zero-shot models demonstrate a promising potential for addressing such challenges. However, these models lack task-specific knowledge therefore are not optimized for the nuances of recognizing facial expressions. To bridge this gap, this work proposes a novel method, Exp-CLIP, to enhance zero-shot FER by transferring the task knowledge from large language models (LLMs). Specifically, based on the pre-trained vision-language encoders, we incorporate a projection head designed to map the initial joint vision-language space into a space that captures representations of facial actions. To train this projection head for subsequent zero-shot predictions, we propose to align the projected visual representations with task-specific semantic meanings derived from the LLM encoder, and the text instruction-based strategy is employed to customize the LLM knowledge. Given unlabelled facial data and efficient training of the projection head, Exp-CLIP achieves superior zero-shot results to the CLIP models and several other large vision-language models (LVLMs) on seven in-the-wild FER datasets. The code and pre-trained models are available at \\url{https://github.com/zengqunzhao/Exp-CLIP}.","sentences":["Current facial expression recognition (FER) models are often designed in a supervised learning manner thus are constrained by the lack of large-scale facial expression images with high-quality annotations.","Consequently, these models often fail to generalize well, performing poorly on unseen images in training.","Vision-language-based zero-shot models demonstrate a promising potential for addressing such challenges.","However, these models lack task-specific knowledge therefore are not optimized for the nuances of recognizing facial expressions.","To bridge this gap, this work proposes a novel method, Exp-CLIP, to enhance zero-shot FER by transferring the task knowledge from large language models (LLMs).","Specifically, based on the pre-trained vision-language encoders, we incorporate a projection head designed to map the initial joint vision-language space into a space that captures representations of facial actions.","To train this projection head for subsequent zero-shot predictions, we propose to align the projected visual representations with task-specific semantic meanings derived from the LLM encoder, and the text instruction-based strategy is employed to customize the LLM knowledge.","Given unlabelled facial data and efficient training of the projection head, Exp-CLIP achieves superior zero-shot results to the CLIP models and several other large vision-language models (LVLMs) on seven in-the-wild FER datasets.","The code and pre-trained models are available at \\url{https://github.com/zengqunzhao/Exp-CLIP}."],"url":"http://arxiv.org/abs/2405.19100v1","category":"cs.CV"}
{"created":"2024-05-29 14:05:16","title":"Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior","abstract":"This paper studies the challenging black-box adversarial attack that aims to generate adversarial examples against a black-box model by only using output feedback of the model to input queries. Some previous methods improve the query efficiency by incorporating the gradient of a surrogate white-box model into query-based attacks due to the adversarial transferability. However, the localized gradient is not informative enough, making these methods still query-intensive. In this paper, we propose a Prior-guided Bayesian Optimization (P-BO) algorithm that leverages the surrogate model as a global function prior in black-box adversarial attacks. As the surrogate model contains rich prior information of the black-box one, P-BO models the attack objective with a Gaussian process whose mean function is initialized as the surrogate model's loss. Our theoretical analysis on the regret bound indicates that the performance of P-BO may be affected by a bad prior. Therefore, we further propose an adaptive integration strategy to automatically adjust a coefficient on the function prior by minimizing the regret bound. Extensive experiments on image classifiers and large vision-language models demonstrate the superiority of the proposed algorithm in reducing queries and improving attack success rates compared with the state-of-the-art black-box attacks. Code is available at https://github.com/yibo-miao/PBO-Attack.","sentences":["This paper studies the challenging black-box adversarial attack that aims to generate adversarial examples against a black-box model by only using output feedback of the model to input queries.","Some previous methods improve the query efficiency by incorporating the gradient of a surrogate white-box model into query-based attacks due to the adversarial transferability.","However, the localized gradient is not informative enough, making these methods still query-intensive.","In this paper, we propose a Prior-guided Bayesian Optimization (P-BO) algorithm that leverages the surrogate model as a global function prior in black-box adversarial attacks.","As the surrogate model contains rich prior information of the black-box one, P-BO models the attack objective with a Gaussian process whose mean function is initialized as the surrogate model's loss.","Our theoretical analysis on the regret bound indicates that the performance of P-BO may be affected by a bad prior.","Therefore, we further propose an adaptive integration strategy to automatically adjust a coefficient on the function prior by minimizing the regret bound.","Extensive experiments on image classifiers and large vision-language models demonstrate the superiority of the proposed algorithm in reducing queries and improving attack success rates compared with the state-of-the-art black-box attacks.","Code is available at https://github.com/yibo-miao/PBO-Attack."],"url":"http://arxiv.org/abs/2405.19098v1","category":"cs.LG"}
{"created":"2024-05-29 13:57:16","title":"The Precise Complexity of Reasoning in $\\mathcal{ALC}$ with $\u03c9$-Admissible Concrete Domains (Extended Version)","abstract":"Concrete domains have been introduced in the context of Description Logics to allow references to qualitative and quantitative values. In particular, the class of $\\omega$-admissible concrete domains, which includes Allen's interval algebra, the region connection calculus (RCC8), and the rational numbers with ordering and equality, has been shown to yield extensions of $\\mathcal{ALC}$ for which concept satisfiability w.r.t. a general TBox is decidable. In this paper, we present an algorithm based on type elimination and use it to show that deciding the consistency of an $\\mathcal{ALC}(\\mathfrak{D})$ ontology is ExpTime-complete if the concrete domain $\\mathfrak{D}$ is $\\omega$-admissible and its constraint satisfaction problem is decidable in exponential time.   While this allows us to reason with concept and role assertions, we also investigate feature assertions $f(a,c)$ that can specify a constant $c$ as the value of a feature $f$ for an individual $a$. We show that, under conditions satisfied by all known $\\omega$-admissible domains, we can add feature assertions without affecting the complexity.","sentences":["Concrete domains have been introduced in the context of Description Logics to allow references to qualitative and quantitative values.","In particular, the class of $\\omega$-admissible concrete domains, which includes Allen's interval algebra, the region connection calculus (RCC8), and the rational numbers with ordering and equality, has been shown to yield extensions of $\\mathcal{ALC}$ for which concept satisfiability w.r.t.","a general TBox is decidable.","In this paper, we present an algorithm based on type elimination and use it to show that deciding the consistency of an $\\mathcal{ALC}(\\mathfrak{D})$ ontology is ExpTime-complete if the concrete domain $\\mathfrak{D}$ is $\\omega$-admissible and its constraint satisfaction problem is decidable in exponential time.   ","While this allows us to reason with concept and role assertions, we also investigate feature assertions $f(a,c)$ that can specify a constant $c$ as the value of a feature $f$ for an individual $a$. We show that, under conditions satisfied by all known $\\omega$-admissible domains, we can add feature assertions without affecting the complexity."],"url":"http://arxiv.org/abs/2405.19096v1","category":"cs.LO"}
{"created":"2024-05-29 13:55:06","title":"Faithful Chart Summarization with ChaTS-Pi","abstract":"Chart-to-summary generation can help explore data, communicate insights, and help the visually impaired people. Multi-modal generative models have been used to produce fluent summaries, but they can suffer from factual and perceptual errors. In this work we present CHATS-CRITIC, a reference-free chart summarization metric for scoring faithfulness. CHATS-CRITIC is composed of an image-to-text model to recover the table from a chart, and a tabular entailment model applied to score the summary sentence by sentence. We find that CHATS-CRITIC evaluates the summary quality according to human ratings better than reference-based metrics, either learned or n-gram based, and can be further used to fix candidate summaries by removing not supported sentences. We then introduce CHATS-PI, a chart-to-summary pipeline that leverages CHATS-CRITIC during inference to fix and rank sampled candidates from any chart-summarization model. We evaluate CHATS-PI and CHATS-CRITIC using human raters, establishing state-of-the-art results on two popular chart-to-summary datasets.","sentences":["Chart-to-summary generation can help explore data, communicate insights, and help the visually impaired people.","Multi-modal generative models have been used to produce fluent summaries, but they can suffer from factual and perceptual errors.","In this work we present CHATS-CRITIC, a reference-free chart summarization metric for scoring faithfulness.","CHATS-CRITIC is composed of an image-to-text model to recover the table from a chart, and a tabular entailment model applied to score the summary sentence by sentence.","We find that CHATS-CRITIC evaluates the summary quality according to human ratings better than reference-based metrics, either learned or n-gram based, and can be further used to fix candidate summaries by removing not supported sentences.","We then introduce CHATS-PI, a chart-to-summary pipeline that leverages CHATS-CRITIC during inference to fix and rank sampled candidates from any chart-summarization model.","We evaluate CHATS-PI and CHATS-CRITIC using human raters, establishing state-of-the-art results on two popular chart-to-summary datasets."],"url":"http://arxiv.org/abs/2405.19094v1","category":"cs.CL"}
{"created":"2024-05-29 13:54:12","title":"Benchmarking and Improving Detail Image Caption","abstract":"Image captioning has long been regarded as a fundamental task in visual understanding. Recently, however, few large vision-language model (LVLM) research discusses model's image captioning performance because of the outdated short-caption benchmarks and unreliable evaluation metrics. In this work, we propose to benchmark detail image caption task by curating high-quality evaluation datasets annotated by human experts, GPT-4V and Gemini-1.5-Pro. We also design a more reliable caption evaluation metric called CAPTURE (CAPtion evaluation by exTracting and coUpling coRE information). CAPTURE extracts visual elements, e.g., objects, attributes and relations from captions, and then matches these elements through three stages, achieving the highest consistency with expert judgements over other rule-based or model-based caption metrics. The proposed benchmark and metric provide reliable evaluation for LVLM's detailed image captioning ability. Guided by this evaluation, we further explore to unleash LVLM's detail caption capabilities by synthesizing high-quality data through a five-stage data construction pipeline. Our pipeline only uses a given LVLM itself and other open-source tools, without any human or GPT-4V annotation in the loop. Experiments show that the proposed data construction strategy significantly improves model-generated detail caption data quality for LVLMs with leading performance, and the data quality can be further improved in a self-looping paradigm. All code and dataset will be publicly available at https://github.com/foundation-multimodal-models/CAPTURE.","sentences":["Image captioning has long been regarded as a fundamental task in visual understanding.","Recently, however, few large vision-language model (LVLM) research discusses model's image captioning performance because of the outdated short-caption benchmarks and unreliable evaluation metrics.","In this work, we propose to benchmark detail image caption task by curating high-quality evaluation datasets annotated by human experts, GPT-4V and Gemini-1.5-Pro.","We also design a more reliable caption evaluation metric called CAPTURE (CAPtion evaluation by exTracting and coUpling coRE information).","CAPTURE extracts visual elements, e.g., objects, attributes and relations from captions, and then matches these elements through three stages, achieving the highest consistency with expert judgements over other rule-based or model-based caption metrics.","The proposed benchmark and metric provide reliable evaluation for LVLM's detailed image captioning ability.","Guided by this evaluation, we further explore to unleash LVLM's detail caption capabilities by synthesizing high-quality data through a five-stage data construction pipeline.","Our pipeline only uses a given LVLM itself and other open-source tools, without any human or GPT-4V annotation in the loop.","Experiments show that the proposed data construction strategy significantly improves model-generated detail caption data quality for LVLMs with leading performance, and the data quality can be further improved in a self-looping paradigm.","All code and dataset will be publicly available at https://github.com/foundation-multimodal-models/CAPTURE."],"url":"http://arxiv.org/abs/2405.19092v1","category":"cs.CV"}
{"created":"2024-05-29 13:52:36","title":"Probing the strength of radial migration via churning by using metal-rich red giant stars from APOGEE","abstract":"Making use of the APOGEE DR17 catalogue with high quality data for 143,509 red giant branch stars we explore the strength of different mechanisms that causes a star to radially migrate in the Milky Way stellar disk. At any position in the disk we find stars that are more metal-rich than the local interstellar medium. This is surprising and normally attributed to the migration of these stars after their formation inside their current Galactocentric-radius. Such stars are prime candidates for studying the strength of different migratory processes. We specifically select two types of metal-rich stars: i) super metal-rich stars ([Fe/H] > 0.2) and ii) stars that are more metal-rich than their local environment. For both, we explore the distribution of orbital parameters and ages as evidence of their migration history. We find that most super metal-rich stars have experienced some amount of churning as they have orbits with Rg >= 5 kpc. Furthermore, about half of the super metal-rich stars are on non-circular orbits (ecc > 0.15) and therefore also have experienced blurring. The metallicity of young stars in our sample is generally the same as the metallicity of the interstellar medium, suggesting they have not radially migrated yet. Stars with lower metallicity than the local environment have intermediate to old ages. We further find that super metal-rich stars have approximately the same age distribution at all Galactocentric-radii, which suggests that radial migration is a key mechanism responsible for the chemical compositions of stellar populations in the Milky Way.","sentences":["Making use of the APOGEE DR17 catalogue with high quality data for 143,509 red giant branch stars we explore the strength of different mechanisms that causes a star to radially migrate in the Milky Way stellar disk.","At any position in the disk we find stars that are more metal-rich than the local interstellar medium.","This is surprising and normally attributed to the migration of these stars after their formation inside their current Galactocentric-radius.","Such stars are prime candidates for studying the strength of different migratory processes.","We specifically select two types of metal-rich stars: i) super metal-rich stars ([Fe/H] > 0.2) and ii) stars that are more metal-rich than their local environment.","For both, we explore the distribution of orbital parameters and ages as evidence of their migration history.","We find that most super metal-rich stars have experienced some amount of churning as they have orbits with Rg >= 5 kpc.","Furthermore, about half of the super metal-rich stars are on non-circular orbits (ecc > 0.15) and therefore also have experienced blurring.","The metallicity of young stars in our sample is generally the same as the metallicity of the interstellar medium, suggesting they have not radially migrated yet.","Stars with lower metallicity than the local environment have intermediate to old ages.","We further find that super metal-rich stars have approximately the same age distribution at all Galactocentric-radii, which suggests that radial migration is a key mechanism responsible for the chemical compositions of stellar populations in the Milky Way."],"url":"http://arxiv.org/abs/2405.19089v1","category":"astro-ph.GA"}
{"created":"2024-05-29 13:49:44","title":"MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors","abstract":"Model editing aims to efficiently alter the behavior of Large Language Models (LLMs) within a desired scope, while ensuring no adverse impact on other inputs. Recent years have witnessed various model editing methods been proposed. However, these methods either exhibit poor overall performance or struggle to strike a balance between generalization and locality. We propose MOMoE, a model editing adapter utilizing a Mixture of Experts (MoE) architecture with a knowledge anchor routing strategy. MOMoE updates knowledge using a bypass MoE structure, keeping the original parameters unchanged to preserve the general ability of LLMs. And, the knowledge anchor routing ensures that inputs requiring similar knowledge are routed to the same expert, thereby enhancing the generalization of the updated knowledge. Experimental results show the superiority of our approach over both batch editing and sequential batch editing tasks, exhibiting exceptional overall performance alongside outstanding balance between generalization and locality. Our code will be available.","sentences":["Model editing aims to efficiently alter the behavior of Large Language Models (LLMs) within a desired scope, while ensuring no adverse impact on other inputs.","Recent years have witnessed various model editing methods been proposed.","However, these methods either exhibit poor overall performance or struggle to strike a balance between generalization and locality.","We propose MOMoE, a model editing adapter utilizing a Mixture of Experts (MoE) architecture with a knowledge anchor routing strategy.","MOMoE updates knowledge using a bypass MoE structure, keeping the original parameters unchanged to preserve the general ability of LLMs.","And, the knowledge anchor routing ensures that inputs requiring similar knowledge are routed to the same expert, thereby enhancing the generalization of the updated knowledge.","Experimental results show the superiority of our approach over both batch editing and sequential batch editing tasks, exhibiting exceptional overall performance alongside outstanding balance between generalization and locality.","Our code will be available."],"url":"http://arxiv.org/abs/2405.19086v1","category":"cs.CL"}
{"created":"2024-05-29 13:47:32","title":"Patch-enhanced Mask Encoder Prompt Image Generation","abstract":"Artificial Intelligence Generated Content(AIGC), known for its superior visual results, represents a promising mitigation method for high-cost advertising applications. Numerous approaches have been developed to manipulate generated content under different conditions. However, a crucial limitation lies in the accurate description of products in advertising applications. Applying previous methods directly may lead to considerable distortion and deformation of advertised products, primarily due to oversimplified content control conditions. Hence, in this work, we propose a patch-enhanced mask encoder approach to ensure accurate product descriptions while preserving diverse backgrounds. Our approach consists of three components Patch Flexible Visibility, Mask Encoder Prompt Adapter and an image Foundation Model. Patch Flexible Visibility is used for generating a more reasonable background image. Mask Encoder Prompt Adapter enables region-controlled fusion. We also conduct an analysis of the structure and operational mechanisms of the Generation Module. Experimental results show our method can achieve the highest visual results and FID scores compared with other methods.","sentences":["Artificial Intelligence Generated Content(AIGC), known for its superior visual results, represents a promising mitigation method for high-cost advertising applications.","Numerous approaches have been developed to manipulate generated content under different conditions.","However, a crucial limitation lies in the accurate description of products in advertising applications.","Applying previous methods directly may lead to considerable distortion and deformation of advertised products, primarily due to oversimplified content control conditions.","Hence, in this work, we propose a patch-enhanced mask encoder approach to ensure accurate product descriptions while preserving diverse backgrounds.","Our approach consists of three components Patch Flexible Visibility, Mask Encoder Prompt Adapter and an image Foundation Model.","Patch Flexible Visibility is used for generating a more reasonable background image.","Mask Encoder Prompt Adapter enables region-controlled fusion.","We also conduct an analysis of the structure and operational mechanisms of the Generation Module.","Experimental results show our method can achieve the highest visual results and FID scores compared with other methods."],"url":"http://arxiv.org/abs/2405.19085v1","category":"cs.AI"}
{"created":"2024-05-29 13:36:36","title":"OMPO: A Unified Framework for RL under Policy and Dynamics Shifts","abstract":"Training reinforcement learning policies using environment interaction data collected from varying policies or dynamics presents a fundamental challenge. Existing works often overlook the distribution discrepancies induced by policy or dynamics shifts, or rely on specialized algorithms with task priors, thus often resulting in suboptimal policy performances and high learning variances. In this paper, we identify a unified strategy for online RL policy learning under diverse settings of policy and dynamics shifts: transition occupancy matching. In light of this, we introduce a surrogate policy learning objective by considering the transition occupancy discrepancies and then cast it into a tractable min-max optimization problem through dual reformulation. Our method, dubbed Occupancy-Matching Policy Optimization (OMPO), features a specialized actor-critic structure equipped with a distribution discriminator and a small-size local buffer. We conduct extensive experiments based on the OpenAI Gym, Meta-World, and Panda Robots environments, encompassing policy shifts under stationary and nonstationary dynamics, as well as domain adaption. The results demonstrate that OMPO outperforms the specialized baselines from different categories in all settings. We also find that OMPO exhibits particularly strong performance when combined with domain randomization, highlighting its potential in RL-based robotics applications","sentences":["Training reinforcement learning policies using environment interaction data collected from varying policies or dynamics presents a fundamental challenge.","Existing works often overlook the distribution discrepancies induced by policy or dynamics shifts, or rely on specialized algorithms with task priors, thus often resulting in suboptimal policy performances and high learning variances.","In this paper, we identify a unified strategy for online RL policy learning under diverse settings of policy and dynamics shifts: transition occupancy matching.","In light of this, we introduce a surrogate policy learning objective by considering the transition occupancy discrepancies and then cast it into a tractable min-max optimization problem through dual reformulation.","Our method, dubbed Occupancy-Matching Policy Optimization (OMPO), features a specialized actor-critic structure equipped with a distribution discriminator and a small-size local buffer.","We conduct extensive experiments based on the OpenAI Gym, Meta-World, and Panda Robots environments, encompassing policy shifts under stationary and nonstationary dynamics, as well as domain adaption.","The results demonstrate that OMPO outperforms the specialized baselines from different categories in all settings.","We also find that OMPO exhibits particularly strong performance when combined with domain randomization, highlighting its potential in RL-based robotics applications"],"url":"http://arxiv.org/abs/2405.19080v1","category":"cs.LG"}
{"created":"2024-05-29 13:34:56","title":"The largest Laplacian eigenvalue and the balancedness of simplicial complexes","abstract":"Let $K$ be a simplical complex, and let $\\mathcal{L}_i^{up}(K), \\mathcal{Q}_i^{up}(K)$ be the $i$-th up Laplacian and signless Laplacian of $K$, respectively. In this paper we proved that the largest eigenvalue of $\\mathcal{L}_i^{up}(K)$ is not greater than the largest eigenvalue of $\\mathcal{Q}_i^{up}(K)$; furthermore, if $K$ is $(i+1)$-path connected, then the equality holds if and only if the $i$-th incidence signed graph $B_i(K)$ of $K$ is balanced. As an application we provided an upper bound for the largest eigenvalue of the $i$-th up Laplacian of $K$, which improves the bound given by Horak and Jost and generalizes the result of Anderson and Morley on graphs.We characterized the balancedness of simplicial complexes under operations such as wedge sum, join, Cartesian product and duplication of motifs. For each $i \\ge 0$, by using wedge sum or duplication of motifs, we can construct an infinitely many $(i+1)$-path connected simplicial complexes $K$ with $B_i(K)$ being balanced.","sentences":["Let $K$ be a simplical complex, and let $\\mathcal{L}_i^{up}(K), \\mathcal{Q}_i^{up}(K)$ be the $i$-th up Laplacian and signless Laplacian of $K$, respectively.","In this paper we proved that the largest eigenvalue of $\\mathcal{L}_i^{up}(K)$ is not greater than the largest eigenvalue of $\\mathcal{Q}_i^{up}(K)$; furthermore, if $K$ is $(i+1)$-path connected, then the equality holds if and only if the $i$-th incidence signed graph $B_i(K)$ of $K$ is balanced.","As an application we provided an upper bound for the largest eigenvalue of the $i$-th up Laplacian of $K$, which improves the bound given by Horak and Jost and generalizes the result of Anderson and Morley on graphs.","We characterized the balancedness of simplicial complexes under operations such as wedge sum, join, Cartesian product and duplication of motifs.","For each $i \\ge 0$, by using wedge sum or duplication of motifs, we can construct an infinitely many $(i+1)$-path connected simplicial complexes $K$ with $B_i(K)$ being balanced."],"url":"http://arxiv.org/abs/2405.19078v1","category":"math.CO"}
{"created":"2024-05-29 13:34:32","title":"Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials Analysis and Design","abstract":"We present Cephalo, a series of multimodal vision large language models (V-LLMs) designed for materials science applications, integrating visual and linguistic data for enhanced understanding and interaction within human-AI and multi-agent AI frameworks. A key innovation of Cephalo is its advanced dataset generation method, which employs a sophisticated algorithm to accurately detect and separate images and their corresponding textual descriptions from PDF documents, such as scientific papers. The method includes a careful refinement of image-text pairs through integrated vision and language processing, ensuring high-quality, contextually relevant, and well reasoned training data. Cephalo is trained on integrated image and text data extracted from thousands of scientific papers and science-focused Wikipedia pages demonstrates can interpret complex visual scenes, generate precise language descriptions, and answer queries about images effectively. The combination of a vision encoder with an autoregressive transformer supports complex natural language understanding in an integrated model, which can be coupled with other generative methods to create an image-to-text-to-image or image-to-text-to-3D pipeline. To explore the development of larger models from smaller ones, we merge sets of layers that originate from different pre-trained source models. This hybrid approach allows us to leverage the domain-specific expertise and general conversational capabilities to harness the strengths of multiple models. We examine the models in diverse use cases that incorporate biological materials, fracture and engineering analysis, protein biophysics, and bio-inspired design based on insect behavior. Generative applications include bio-inspired designs, including pollen-inspired architected materials, as well as the synthesis of bio-inspired material microstructures from a photograph of a solar eclipse.","sentences":["We present Cephalo, a series of multimodal vision large language models (V-LLMs) designed for materials science applications, integrating visual and linguistic data for enhanced understanding and interaction within human-AI and multi-agent AI frameworks.","A key innovation of Cephalo is its advanced dataset generation method, which employs a sophisticated algorithm to accurately detect and separate images and their corresponding textual descriptions from PDF documents, such as scientific papers.","The method includes a careful refinement of image-text pairs through integrated vision and language processing, ensuring high-quality, contextually relevant, and well reasoned training data.","Cephalo is trained on integrated image and text data extracted from thousands of scientific papers and science-focused Wikipedia pages demonstrates can interpret complex visual scenes, generate precise language descriptions, and answer queries about images effectively.","The combination of a vision encoder with an autoregressive transformer supports complex natural language understanding in an integrated model, which can be coupled with other generative methods to create an image-to-text-to-image or image-to-text-to-3D pipeline.","To explore the development of larger models from smaller ones, we merge sets of layers that originate from different pre-trained source models.","This hybrid approach allows us to leverage the domain-specific expertise and general conversational capabilities to harness the strengths of multiple models.","We examine the models in diverse use cases that incorporate biological materials, fracture and engineering analysis, protein biophysics, and bio-inspired design based on insect behavior.","Generative applications include bio-inspired designs, including pollen-inspired architected materials, as well as the synthesis of bio-inspired material microstructures from a photograph of a solar eclipse."],"url":"http://arxiv.org/abs/2405.19076v1","category":"cs.CV"}
{"created":"2024-05-29 13:32:30","title":"Worst-cases of distortion riskmetrics and weighted entropy with partial information","abstract":"In this paper, we discuss the worst-case of distortion riskmetrics for general distributions when only partial information (mean and variance) is known. This result is applicable to general class of distortion risk measures and variability measures. Furthermore, we also consider worst-case of weighted entropy for general distributions when only partial information is available. Specifically, we provide some applications for entropies, weighted entropies and risk measures. The commonly used entropies include Gini functional, cumulative residual entropy, tail-Gini functional, cumulative Tsallis past entropy, extended Gini coefficient and so on. The risk measures contain some premium principles and shortfalls based on entropy. The shortfalls include the Gini shortfall, extended Gini shortfall, shortfall of cumulative residual entropy and shortfall of cumulative residual Tsallis entropy with order $\\alpha$.","sentences":["In this paper, we discuss the worst-case of distortion riskmetrics for general distributions when only partial information (mean and variance) is known.","This result is applicable to general class of distortion risk measures and variability measures.","Furthermore, we also consider worst-case of weighted entropy for general distributions when only partial information is available.","Specifically, we provide some applications for entropies, weighted entropies and risk measures.","The commonly used entropies include Gini functional, cumulative residual entropy, tail-Gini functional, cumulative Tsallis past entropy, extended Gini coefficient and so on.","The risk measures contain some premium principles and shortfalls based on entropy.","The shortfalls include the Gini shortfall, extended Gini shortfall, shortfall of cumulative residual entropy and shortfall of cumulative residual Tsallis entropy with order $\\alpha$."],"url":"http://arxiv.org/abs/2405.19075v1","category":"q-fin.RM"}
{"created":"2024-05-29 13:31:42","title":"Resurrecting Old Classes with New Data for Exemplar-Free Continual Learning","abstract":"Continual learning methods are known to suffer from catastrophic forgetting, a phenomenon that is particularly hard to counter for methods that do not store exemplars of previous tasks. Therefore, to reduce potential drift in the feature extractor, existing exemplar-free methods are typically evaluated in settings where the first task is significantly larger than subsequent tasks. Their performance drops drastically in more challenging settings starting with a smaller first task. To address this problem of feature drift estimation for exemplar-free methods, we propose to adversarially perturb the current samples such that their embeddings are close to the old class prototypes in the old model embedding space. We then estimate the drift in the embedding space from the old to the new model using the perturbed images and compensate the prototypes accordingly. We exploit the fact that adversarial samples are transferable from the old to the new feature space in a continual learning setting. The generation of these images is simple and computationally cheap. We demonstrate in our experiments that the proposed approach better tracks the movement of prototypes in embedding space and outperforms existing methods on several standard continual learning benchmarks as well as on fine-grained datasets. Code is available at https://github.com/dipamgoswami/ADC.","sentences":["Continual learning methods are known to suffer from catastrophic forgetting, a phenomenon that is particularly hard to counter for methods that do not store exemplars of previous tasks.","Therefore, to reduce potential drift in the feature extractor, existing exemplar-free methods are typically evaluated in settings where the first task is significantly larger than subsequent tasks.","Their performance drops drastically in more challenging settings starting with a smaller first task.","To address this problem of feature drift estimation for exemplar-free methods, we propose to adversarially perturb the current samples such that their embeddings are close to the old class prototypes in the old model embedding space.","We then estimate the drift in the embedding space from the old to the new model using the perturbed images and compensate the prototypes accordingly.","We exploit the fact that adversarial samples are transferable from the old to the new feature space in a continual learning setting.","The generation of these images is simple and computationally cheap.","We demonstrate in our experiments that the proposed approach better tracks the movement of prototypes in embedding space and outperforms existing methods on several standard continual learning benchmarks as well as on fine-grained datasets.","Code is available at https://github.com/dipamgoswami/ADC."],"url":"http://arxiv.org/abs/2405.19074v1","category":"cs.CV"}
{"created":"2024-05-29 13:20:50","title":"Implementing arbitrary multi-mode continuous-variable quantum gates with fixed non-Gaussian states and adaptive linear optics","abstract":"Non-Gaussian quantum gates are essential components for optical quantum information processing. However, the efficient implementation of practically important multi-mode higher-order non-Gaussian gates has not been comprehensively studied. We propose a measurement-based method to directly implement general, multi-mode, and higher-order non-Gaussian gates using only fixed non-Gaussian ancillary states and adaptive linear optics. Compared to existing methods, our method allows for a more resource-efficient and experimentally feasible implementation of multi-mode gates that are important for various applications in optical quantum technology, such as the two-mode cubic quantum non-demolition gate or the three-mode continuous-variable Toffoli gate, and their higher-order extensions. Our results will expedite the progress toward fault-tolerant universal quantum computing with light.","sentences":["Non-Gaussian quantum gates are essential components for optical quantum information processing.","However, the efficient implementation of practically important multi-mode higher-order non-Gaussian gates has not been comprehensively studied.","We propose a measurement-based method to directly implement general, multi-mode, and higher-order non-Gaussian gates using only fixed non-Gaussian ancillary states and adaptive linear optics.","Compared to existing methods, our method allows for a more resource-efficient and experimentally feasible implementation of multi-mode gates that are important for various applications in optical quantum technology, such as the two-mode cubic quantum non-demolition gate or the three-mode continuous-variable Toffoli gate, and their higher-order extensions.","Our results will expedite the progress toward fault-tolerant universal quantum computing with light."],"url":"http://arxiv.org/abs/2405.19067v1","category":"quant-ph"}
{"created":"2024-05-29 13:17:47","title":"A Note on the Subcubes of the $n$-Cube","abstract":"In the year 1976, Sergiu Hart considered and solved the following combinatorial problem: given two parameters $k$ and $n$, find a set of k vertices in the $n$-cube which has a maximal number of interconnecting edges. Here we consider the more general problem with $q$-dimensional induced subcubes (for some fixed but arbitrary choice of $q$) instead of the interconnecting edges. In a different, but provably equivalent formulation, this problem has been solved quite recently but the proof that had been given is complicated. In this paper, we present a direct and comparably simple proof for the extension of Hart's result.","sentences":["In the year 1976, Sergiu Hart considered and solved the following combinatorial problem: given two parameters $k$ and $n$, find a set of k vertices in the $n$-cube which has a maximal number of interconnecting edges.","Here we consider the more general problem with $q$-dimensional induced subcubes (for some fixed but arbitrary choice of $q$) instead of the interconnecting edges.","In a different, but provably equivalent formulation, this problem has been solved quite recently but the proof that had been given is complicated.","In this paper, we present a direct and comparably simple proof for the extension of Hart's result."],"url":"http://arxiv.org/abs/2405.19066v1","category":"math.CO"}
{"created":"2024-05-29 13:16:46","title":"xTern: Energy-Efficient Ternary Neural Network Inference on RISC-V-Based Edge Systems","abstract":"Ternary neural networks (TNNs) offer a superior accuracy-energy trade-off compared to binary neural networks. However, until now, they have required specialized accelerators to realize their efficiency potential, which has hindered widespread adoption. To address this, we present xTern, a lightweight extension of the RISC-V instruction set architecture (ISA) targeted at accelerating TNN inference on general-purpose cores. To complement the ISA extension, we developed a set of optimized kernels leveraging xTern, achieving 67% higher throughput than their 2-bit equivalents. Power consumption is only marginally increased by 5.2%, resulting in an energy efficiency improvement by 57.1%. We demonstrate that the proposed xTern extension, integrated into an octa-core compute cluster, incurs a minimal silicon area overhead of 0.9% with no impact on timing. In end-to-end benchmarks, we demonstrate that xTern enables the deployment of TNNs achieving up to 1.6 percentage points higher CIFAR-10 classification accuracy than 2-bit networks at equal inference latency. Our results show that xTern enables RISC-V-based ultra-low-power edge AI platforms to benefit from the efficiency potential of TNNs.","sentences":["Ternary neural networks (TNNs) offer a superior accuracy-energy trade-off compared to binary neural networks.","However, until now, they have required specialized accelerators to realize their efficiency potential, which has hindered widespread adoption.","To address this, we present xTern, a lightweight extension of the RISC-V instruction set architecture (ISA) targeted at accelerating TNN inference on general-purpose cores.","To complement the ISA extension, we developed a set of optimized kernels leveraging xTern, achieving 67% higher throughput than their 2-bit equivalents.","Power consumption is only marginally increased by 5.2%, resulting in an energy efficiency improvement by 57.1%.","We demonstrate that the proposed xTern extension, integrated into an octa-core compute cluster, incurs a minimal silicon area overhead of 0.9% with no impact on timing.","In end-to-end benchmarks, we demonstrate that xTern enables the deployment of TNNs achieving up to 1.6 percentage points higher CIFAR-10 classification accuracy than 2-bit networks at equal inference latency.","Our results show that xTern enables RISC-V-based ultra-low-power edge AI platforms to benefit from the efficiency potential of TNNs."],"url":"http://arxiv.org/abs/2405.19065v1","category":"cs.AR"}
{"created":"2024-05-29 13:15:24","title":"Arnold-Thom conjecture for the arrival time of surfaces","abstract":"Following \\L ojasiewicz's uniqueness theorem and Thom's gradient conjecture, Arnold proposed a stronger version about the existence of limit tangents of gradient flow lines for analytic functions. We prove \\L ojasiewicz's theorem and Arnold's conjecture in the context of arrival time functions for mean curvature flows in $\\mathbb R^{n+1}$ with neck or non-degenerate cylindrical singularities. In particular, we prove the conjectures for all mean convex mean curvature flows of surfaces, including the cases when the arrival time functions are not $C^2.$ The results also apply to mean curvature flows starting from two-spheres or generic closed surfaces.","sentences":["Following \\L ojasiewicz's uniqueness theorem and Thom's gradient conjecture, Arnold proposed a stronger version about the existence of limit tangents of gradient flow lines for analytic functions.","We prove \\L ojasiewicz's theorem and Arnold's conjecture in the context of arrival time functions for mean curvature flows in $\\mathbb R^{n+1}$ with neck or non-degenerate cylindrical singularities.","In particular, we prove the conjectures for all mean convex mean curvature flows of surfaces, including the cases when the arrival time functions are not $C^2.$ The results also apply to mean curvature flows starting from two-spheres or generic closed surfaces."],"url":"http://arxiv.org/abs/2405.19064v1","category":"math.DG"}
{"created":"2024-05-29 13:14:18","title":"Weighted sieves with switching","abstract":"Weighted sieves are used to detect numbers with at most $S$ prime factors with $S \\in \\mathbb{N}$ as small as possible. When one studies problems with two variables in somewhat symmetric roles (such as Chen primes, that is primes $p$ such that $p+2$ has at most two prime factors), one can utilize the switching principle. Here we discuss how different sieve weights work in such a situation, concentrating in particular in detecting a prime along with a product of at most three primes.   As applications, we improve on the works of Yang and Harman concerning Diophantine approximation with a prime and an almost prime, and prove that, in general, one can find a pair $(p, P_3)$ when both the original and the switched problem have level of distribution at least $0.267$.","sentences":["Weighted sieves are used to detect numbers with at most $S$ prime factors with $S \\in \\mathbb{N}$ as small as possible.","When one studies problems with two variables in somewhat symmetric roles (such as Chen primes, that is primes $p$ such that $p+2$ has at most two prime factors), one can utilize the switching principle.","Here we discuss how different sieve weights work in such a situation, concentrating in particular in detecting a prime along with a product of at most three primes.   ","As applications, we improve on the works of Yang and Harman concerning Diophantine approximation with a prime and an almost prime, and prove that, in general, one can find a pair $(p, P_3)$ when both the original and the switched problem have level of distribution at least $0.267$."],"url":"http://arxiv.org/abs/2405.19063v1","category":"math.NT"}
{"created":"2024-05-29 13:09:33","title":"SIG: Efficient Self-Interpretable Graph Neural Network for Continuous-time Dynamic Graphs","abstract":"While dynamic graph neural networks have shown promise in various applications, explaining their predictions on continuous-time dynamic graphs (CTDGs) is difficult. This paper investigates a new research task: self-interpretable GNNs for CTDGs. We aim to predict future links within the dynamic graph while simultaneously providing causal explanations for these predictions. There are two key challenges: (1) capturing the underlying structural and temporal information that remains consistent across both independent and identically distributed (IID) and out-of-distribution (OOD) data, and (2) efficiently generating high-quality link prediction results and explanations. To tackle these challenges, we propose a novel causal inference model, namely the Independent and Confounded Causal Model (ICCM). ICCM is then integrated into a deep learning architecture that considers both effectiveness and efficiency. Extensive experiments demonstrate that our proposed model significantly outperforms existing methods across link prediction accuracy, explanation quality, and robustness to shortcut features. Our code and datasets are anonymously released at https://github.com/2024SIG/SIG.","sentences":["While dynamic graph neural networks have shown promise in various applications, explaining their predictions on continuous-time dynamic graphs (CTDGs) is difficult.","This paper investigates a new research task: self-interpretable GNNs for CTDGs.","We aim to predict future links within the dynamic graph while simultaneously providing causal explanations for these predictions.","There are two key challenges: (1) capturing the underlying structural and temporal information that remains consistent across both independent and identically distributed (IID) and out-of-distribution (OOD) data, and (2) efficiently generating high-quality link prediction results and explanations.","To tackle these challenges, we propose a novel causal inference model, namely the Independent and Confounded Causal Model (ICCM).","ICCM is then integrated into a deep learning architecture that considers both effectiveness and efficiency.","Extensive experiments demonstrate that our proposed model significantly outperforms existing methods across link prediction accuracy, explanation quality, and robustness to shortcut features.","Our code and datasets are anonymously released at https://github.com/2024SIG/SIG."],"url":"http://arxiv.org/abs/2405.19062v1","category":"cs.LG"}
{"created":"2024-05-29 12:56:07","title":"Dependency equilibria: Boundary cases and their real algebraic geometry","abstract":"This paper is a significant step forward in understanding dependency equilibria within the framework of real algebraic geometry encompassing both pure and mixed equilibria. We start by breaking down the concept for a general audience, using concrete examples to illustrate the main results. In alignment with Spohn's original definition of dependency equilibria, we propose three alternative definitions, allowing for an algebro-geometric comprehensive study of all dependency equilibria. We give a sufficient condition for the existence of a pure dependency equilibrium and show that every Nash equilibrium lies on the Spohn variety, the algebraic model for dependency equilibria. For generic games, the set of real points of the Spohn variety is Zariski dense. Furthermore, every Nash equilibrium in this case is a dependency equilibrium. Finally, we present a detailed analysis of the geometric structure of dependency equilibria for $(2\\times2)$-games.","sentences":["This paper is a significant step forward in understanding dependency equilibria within the framework of real algebraic geometry encompassing both pure and mixed equilibria.","We start by breaking down the concept for a general audience, using concrete examples to illustrate the main results.","In alignment with Spohn's original definition of dependency equilibria, we propose three alternative definitions, allowing for an algebro-geometric comprehensive study of all dependency equilibria.","We give a sufficient condition for the existence of a pure dependency equilibrium and show that every Nash equilibrium lies on the Spohn variety, the algebraic model for dependency equilibria.","For generic games, the set of real points of the Spohn variety is Zariski dense.","Furthermore, every Nash equilibrium in this case is a dependency equilibrium.","Finally, we present a detailed analysis of the geometric structure of dependency equilibria for $(2\\times2)$-games."],"url":"http://arxiv.org/abs/2405.19054v1","category":"math.AG"}
{"created":"2024-05-29 12:54:22","title":"Multiscale Spatio-Temporal Enhanced Short-term Load Forecasting of Electric Vehicle Charging Stations","abstract":"The rapid expansion of electric vehicles (EVs) has rendered the load forecasting of electric vehicle charging stations (EVCS) increasingly critical. The primary challenge in achieving precise load forecasting for EVCS lies in accounting for the nonlinear of charging behaviors, the spatial interactions among different stations, and the intricate temporal variations in usage patterns. To address these challenges, we propose a Multiscale Spatio-Temporal Enhanced Model (MSTEM) for effective load forecasting at EVCS. MSTEM incorporates a multiscale graph neural network to discern hierarchical nonlinear temporal dependencies across various time scales. Besides, it also integrates a recurrent learning component and a residual fusion mechanism, enhancing its capability to accurately capture spatial and temporal variations in charging patterns. The effectiveness of the proposed MSTEM has been validated through comparative analysis with six baseline models using three evaluation metrics. The case studies utilize real-world datasets for both fast and slow charging loads at EVCS in Perth, UK. The experimental results demonstrate the superiority of MSTEM in short-term continuous load forecasting for EVCS.","sentences":["The rapid expansion of electric vehicles (EVs) has rendered the load forecasting of electric vehicle charging stations (EVCS) increasingly critical.","The primary challenge in achieving precise load forecasting for EVCS lies in accounting for the nonlinear of charging behaviors, the spatial interactions among different stations, and the intricate temporal variations in usage patterns.","To address these challenges, we propose a Multiscale Spatio-Temporal Enhanced Model (MSTEM) for effective load forecasting at EVCS.","MSTEM incorporates a multiscale graph neural network to discern hierarchical nonlinear temporal dependencies across various time scales.","Besides, it also integrates a recurrent learning component and a residual fusion mechanism, enhancing its capability to accurately capture spatial and temporal variations in charging patterns.","The effectiveness of the proposed MSTEM has been validated through comparative analysis with six baseline models using three evaluation metrics.","The case studies utilize real-world datasets for both fast and slow charging loads at EVCS in Perth, UK.","The experimental results demonstrate the superiority of MSTEM in short-term continuous load forecasting for EVCS."],"url":"http://arxiv.org/abs/2405.19053v1","category":"eess.SY"}
{"created":"2024-05-29 12:51:05","title":"Constructing new geometries: a generalized approach to halving for hypertopes","abstract":"Given a residually connected incidence geometry $\\Gamma$ that satisfies two conditions, denoted $(B_1)$ and $(B_2)$, we construct a new geometry $H(\\Gamma)$ with properties similar to those of $\\Gamma$. This new geometry $H(\\Gamma)$ is inspired by a construction of Percsy, Percsy and Leemans [1]. We show how $H(\\Gamma)$ relates to the classical halving operation on polytopes, allowing us to generalize the halving operation to a broader class of geometries, that we call non-degenerate leaf hypertopes. Finally, we apply this generalization to cubic toroids in order to generate new examples of regular hypertopes.","sentences":["Given a residually connected incidence geometry $\\Gamma$ that satisfies two conditions, denoted $(B_1)$ and $(B_2)$, we construct a new geometry $H(\\Gamma)$ with properties similar to those of $\\Gamma$. This new geometry $H(\\Gamma)$ is inspired by a construction of Percsy, Percsy and Leemans [1].","We show how $H(\\Gamma)$ relates to the classical halving operation on polytopes, allowing us to generalize the halving operation to a broader class of geometries, that we call non-degenerate leaf hypertopes.","Finally, we apply this generalization to cubic toroids in order to generate new examples of regular hypertopes."],"url":"http://arxiv.org/abs/2405.19050v1","category":"math.CO"}
{"created":"2024-05-29 12:46:57","title":"Quantum Circuit Switching with One-Way Repeaters in Star Networks","abstract":"Distributing quantum states reliably among distant locations is a key challenge in the field of quantum networks. One-way quantum networks address this by using one-way communication and quantum error correction. Here, we analyze quantum circuit switching as a protocol to distribute quantum states in one-way quantum networks. In quantum circuit switching, pairs of users can request the delivery of multiple quantum states from one user to the other. After waiting for approval from the network, the states can be distributed either sequentially, forwarding one at a time along a path of quantum repeaters, or in parallel, sending batches of quantum states from repeater to repeater. Since repeaters can only forward a finite number of quantum states at a time, a pivotal question arises: is it advantageous to send them sequentially (allowing for multiple requests simultaneously) or in parallel (reducing processing time but handling only one request at a time)? We compare both approaches in a quantum network with a star topology. Using tools from queuing theory, we show that requests are met at a higher rate when packets are distributed in parallel, although sequential distribution can generally provide service to a larger number of users simultaneously. We also show that using a large number of quantum repeaters to combat channel losses limits the maximum distance between users, as each repeater introduces additional processing delays. These findings provide insight into the design of protocols for distributing quantum states in one-way quantum networks.","sentences":["Distributing quantum states reliably among distant locations is a key challenge in the field of quantum networks.","One-way quantum networks address this by using one-way communication and quantum error correction.","Here, we analyze quantum circuit switching as a protocol to distribute quantum states in one-way quantum networks.","In quantum circuit switching, pairs of users can request the delivery of multiple quantum states from one user to the other.","After waiting for approval from the network, the states can be distributed either sequentially, forwarding one at a time along a path of quantum repeaters, or in parallel, sending batches of quantum states from repeater to repeater.","Since repeaters can only forward a finite number of quantum states at a time, a pivotal question arises: is it advantageous to send them sequentially (allowing for multiple requests simultaneously) or in parallel (reducing processing time but handling only one request at a time)?","We compare both approaches in a quantum network with a star topology.","Using tools from queuing theory, we show that requests are met at a higher rate when packets are distributed in parallel, although sequential distribution can generally provide service to a larger number of users simultaneously.","We also show that using a large number of quantum repeaters to combat channel losses limits the maximum distance between users, as each repeater introduces additional processing delays.","These findings provide insight into the design of protocols for distributing quantum states in one-way quantum networks."],"url":"http://arxiv.org/abs/2405.19049v1","category":"quant-ph"}
{"created":"2024-05-29 12:44:41","title":"Statistical Context Detection for Deep Lifelong Reinforcement Learning","abstract":"Context detection involves labeling segments of an online stream of data as belonging to different tasks. Task labels are used in lifelong learning algorithms to perform consolidation or other procedures that prevent catastrophic forgetting. Inferring task labels from online experiences remains a challenging problem. Most approaches assume finite and low-dimension observation spaces or a preliminary training phase during which task labels are learned. Moreover, changes in the transition or reward functions can be detected only in combination with a policy, and therefore are more difficult to detect than changes in the input distribution. This paper presents an approach to learning both policies and labels in an online deep reinforcement learning setting. The key idea is to use distance metrics, obtained via optimal transport methods, i.e., Wasserstein distance, on suitable latent action-reward spaces to measure distances between sets of data points from past and current streams. Such distances can then be used for statistical tests based on an adapted Kolmogorov-Smirnov calculation to assign labels to sequences of experiences. A rollback procedure is introduced to learn multiple policies by ensuring that only the appropriate data is used to train the corresponding policy. The combination of task detection and policy deployment allows for the optimization of lifelong reinforcement learning agents without an oracle that provides task labels. The approach is tested using two benchmarks and the results show promising performance when compared with related context detection algorithms. The results suggest that optimal transport statistical methods provide an explainable and justifiable procedure for online context detection and reward optimization in lifelong reinforcement learning.","sentences":["Context detection involves labeling segments of an online stream of data as belonging to different tasks.","Task labels are used in lifelong learning algorithms to perform consolidation or other procedures that prevent catastrophic forgetting.","Inferring task labels from online experiences remains a challenging problem.","Most approaches assume finite and low-dimension observation spaces or a preliminary training phase during which task labels are learned.","Moreover, changes in the transition or reward functions can be detected only in combination with a policy, and therefore are more difficult to detect than changes in the input distribution.","This paper presents an approach to learning both policies and labels in an online deep reinforcement learning setting.","The key idea is to use distance metrics, obtained via optimal transport methods, i.e., Wasserstein distance, on suitable latent action-reward spaces to measure distances between sets of data points from past and current streams.","Such distances can then be used for statistical tests based on an adapted Kolmogorov-Smirnov calculation to assign labels to sequences of experiences.","A rollback procedure is introduced to learn multiple policies by ensuring that only the appropriate data is used to train the corresponding policy.","The combination of task detection and policy deployment allows for the optimization of lifelong reinforcement learning agents without an oracle that provides task labels.","The approach is tested using two benchmarks and the results show promising performance when compared with related context detection algorithms.","The results suggest that optimal transport statistical methods provide an explainable and justifiable procedure for online context detection and reward optimization in lifelong reinforcement learning."],"url":"http://arxiv.org/abs/2405.19047v1","category":"cs.LG"}
{"created":"2024-05-29 12:41:43","title":"To RL or not to RL? An Algorithmic Cheat-Sheet for AI-Based Radio Resource Management","abstract":"Several Radio Resource Management (RRM) use cases can be framed as sequential decision planning problems, where an agent (the base station, typically) makes decisions that influence the network utility and state. While Reinforcement Learning (RL) in its general form can address this scenario, it is known to be sample inefficient. Following the principle of Occam's razor, we argue that the choice of the solution technique for RRM should be guided by questions such as, \"Is it a short or long-term planning problem?\", \"Is the underlying model known or does it need to be learned?\", \"Can we solve the problem analytically?\" or \"Is an expert-designed policy available?\". A wide range of techniques exists to address these questions, including static and stochastic optimization, bandits, model predictive control (MPC) and, indeed, RL. We review some of these techniques that have already been successfully applied to RRM, and we believe that others, such as MPC, may present exciting research opportunities for the future.","sentences":["Several Radio Resource Management (RRM) use cases can be framed as sequential decision planning problems, where an agent (the base station, typically) makes decisions that influence the network utility and state.","While Reinforcement Learning (RL) in its general form can address this scenario, it is known to be sample inefficient.","Following the principle of Occam's razor, we argue that the choice of the solution technique for RRM should be guided by questions such as, \"Is it a short or long-term planning problem?\", \"Is the underlying model known or does it need to be learned?\", \"Can we solve the problem analytically?\" or \"Is an expert-designed policy available?\".","A wide range of techniques exists to address these questions, including static and stochastic optimization, bandits, model predictive control (MPC) and, indeed, RL.","We review some of these techniques that have already been successfully applied to RRM, and we believe that others, such as MPC, may present exciting research opportunities for the future."],"url":"http://arxiv.org/abs/2405.19045v1","category":"cs.NI"}
{"created":"2024-05-29 12:32:08","title":"BLSP-KD: Bootstrapping Language-Speech Pre-training via Knowledge Distillation","abstract":"Recent end-to-end approaches have shown promise in extending large language models (LLMs) to speech inputs, but face limitations in directly assessing and optimizing alignment quality and fail to achieve fine-grained alignment due to speech-text length mismatch. We introduce BLSP-KD, a novel approach for Bootstrapping Language-Speech Pretraining via Knowledge Distillation, which addresses these limitations through two key techniques. First, it optimizes speech-text alignment by minimizing the divergence between the LLM's next-token prediction distributions for speech and text inputs using knowledge distillation. Second, it employs a continuous-integrate-andfire strategy to segment speech into tokens that correspond one-to-one with text tokens, enabling fine-grained alignment. We also introduce Partial LoRA (PLoRA), a new adaptation method supporting LLM finetuning for speech inputs under knowledge distillation. Quantitative evaluation shows that BLSP-KD outperforms previous end-to-end baselines and cascaded systems with comparable scale of parameters, facilitating general instruction-following capabilities for LLMs with speech inputs. This approach provides new possibilities for extending LLMs to spoken language interactions.","sentences":["Recent end-to-end approaches have shown promise in extending large language models (LLMs) to speech inputs, but face limitations in directly assessing and optimizing alignment quality and fail to achieve fine-grained alignment due to speech-text length mismatch.","We introduce BLSP-KD, a novel approach for Bootstrapping Language-Speech Pretraining via Knowledge Distillation, which addresses these limitations through two key techniques.","First, it optimizes speech-text alignment by minimizing the divergence between the LLM's next-token prediction distributions for speech and text inputs using knowledge distillation.","Second, it employs a continuous-integrate-andfire strategy to segment speech into tokens that correspond one-to-one with text tokens, enabling fine-grained alignment.","We also introduce Partial LoRA (PLoRA), a new adaptation method supporting LLM finetuning for speech inputs under knowledge distillation.","Quantitative evaluation shows that BLSP-KD outperforms previous end-to-end baselines and cascaded systems with comparable scale of parameters, facilitating general instruction-following capabilities for LLMs with speech inputs.","This approach provides new possibilities for extending LLMs to spoken language interactions."],"url":"http://arxiv.org/abs/2405.19041v1","category":"cs.CL"}
{"created":"2024-05-29 12:24:08","title":"On the formalization of the notion of an interactive algorithm","abstract":"An earlier paper gives an account of a quest for a satisfactory formalization of the classical informal notion of an algorithm. In this paper, an attempt is made to generalize the results of that quest to the informal notion of an interactive algorithm. The notion of an interactive proto-algorithm is introduced. Interactive algorithms are expected to be equivalence classes of interactive proto-algorithms under an appropriate equivalence relation. As in the non-interactive case, three equivalence relations are defined. Two of them are deemed to be bounds for an appropriate equivalence relation and the third is likely an appropriate one.","sentences":["An earlier paper gives an account of a quest for a satisfactory formalization of the classical informal notion of an algorithm.","In this paper, an attempt is made to generalize the results of that quest to the informal notion of an interactive algorithm.","The notion of an interactive proto-algorithm is introduced.","Interactive algorithms are expected to be equivalence classes of interactive proto-algorithms under an appropriate equivalence relation.","As in the non-interactive case, three equivalence relations are defined.","Two of them are deemed to be bounds for an appropriate equivalence relation and the third is likely an appropriate one."],"url":"http://arxiv.org/abs/2405.19037v1","category":"cs.CC"}
{"created":"2024-05-29 12:22:59","title":"CiliaGraph: Enabling Expression-enhanced Hyper-Dimensional Computation in Ultra-Lightweight and One-Shot Graph Classification on Edge","abstract":"Graph Neural Networks (GNNs) are computationally demanding and inefficient when applied to graph classification tasks in resource-constrained edge scenarios due to their inherent process, involving multiple rounds of forward and backward propagation. As a lightweight alternative, Hyper-Dimensional Computing (HDC), which leverages high-dimensional vectors for data encoding and processing, offers a more efficient solution by addressing computational bottleneck. However, current HDC methods primarily focus on static graphs and neglect to effectively capture node attributes and structural information, which leads to poor accuracy. In this work, we propose CiliaGraph, an enhanced expressive yet ultra-lightweight HDC model for graph classification. This model introduces a novel node encoding strategy that preserves relative distance isomorphism for accurate node connection representation. In addition, node distances are utilized as edge weights for information aggregation, and the encoded node attributes and structural information are concatenated to obtain a comprehensive graph representation. Furthermore, we explore the relationship between orthogonality and dimensionality to reduce the dimensions, thereby further enhancing computational efficiency. Compared to the SOTA GNNs, extensive experiments show that CiliaGraph reduces memory usage and accelerates training speed by an average of 292 times(up to 2341 times) and 103 times(up to 313 times) respectively while maintaining comparable accuracy.","sentences":["Graph Neural Networks (GNNs) are computationally demanding and inefficient when applied to graph classification tasks in resource-constrained edge scenarios due to their inherent process, involving multiple rounds of forward and backward propagation.","As a lightweight alternative, Hyper-Dimensional Computing (HDC), which leverages high-dimensional vectors for data encoding and processing, offers a more efficient solution by addressing computational bottleneck.","However, current HDC methods primarily focus on static graphs and neglect to effectively capture node attributes and structural information, which leads to poor accuracy.","In this work, we propose CiliaGraph, an enhanced expressive yet ultra-lightweight HDC model for graph classification.","This model introduces a novel node encoding strategy that preserves relative distance isomorphism for accurate node connection representation.","In addition, node distances are utilized as edge weights for information aggregation, and the encoded node attributes and structural information are concatenated to obtain a comprehensive graph representation.","Furthermore, we explore the relationship between orthogonality and dimensionality to reduce the dimensions, thereby further enhancing computational efficiency.","Compared to the SOTA GNNs, extensive experiments show that CiliaGraph reduces memory usage and accelerates training speed by an average of 292 times(up to 2341 times) and 103 times(up to 313 times) respectively while maintaining comparable accuracy."],"url":"http://arxiv.org/abs/2405.19033v1","category":"cs.LG"}
{"created":"2024-05-29 12:18:51","title":"Large Language Models for Code Summarization","abstract":"Recently, there has been increasing activity in using deep learning for software engineering, including tasks like code generation and summarization. In particular, the most recent coding Large Language Models seem to perform well on these problems. In this technical report, we aim to review how these models perform in code explanation/summarization, while also investigating their code generation capabilities (based on natural language descriptions).","sentences":["Recently, there has been increasing activity in using deep learning for software engineering, including tasks like code generation and summarization.","In particular, the most recent coding Large Language Models seem to perform well on these problems.","In this technical report, we aim to review how these models perform in code explanation/summarization, while also investigating their code generation capabilities (based on natural language descriptions)."],"url":"http://arxiv.org/abs/2405.19032v1","category":"cs.AI"}
{"created":"2024-05-29 12:17:09","title":"Convex neural network synthesis for robustness in the 1-norm","abstract":"With neural networks being used to control safety-critical systems, they increasingly have to be both accurate (in the sense of matching inputs to outputs) and robust. However, these two properties are often at odds with each other and a trade-off has to be navigated. To address this issue, this paper proposes a method to generate an approximation of a neural network which is certifiably more robust. Crucially, the method is fully convex and posed as a semi-definite programme. An application to robustifying model predictive control is used to demonstrate the results. The aim of this work is to introduce a method to navigate the neural network robustness/accuracy trade-off.","sentences":["With neural networks being used to control safety-critical systems, they increasingly have to be both accurate (in the sense of matching inputs to outputs) and robust.","However, these two properties are often at odds with each other and a trade-off has to be navigated.","To address this issue, this paper proposes a method to generate an approximation of a neural network which is certifiably more robust.","Crucially, the method is fully convex and posed as a semi-definite programme.","An application to robustifying model predictive control is used to demonstrate the results.","The aim of this work is to introduce a method to navigate the neural network robustness/accuracy trade-off."],"url":"http://arxiv.org/abs/2405.19029v1","category":"eess.SY"}
{"created":"2024-05-29 12:12:09","title":"DiveR-CT: Diversity-enhanced Red Teaming with Relaxing Constraints","abstract":"Recent advances in large language models (LLMs) have made them indispensable, raising significant concerns over managing their safety. Automated red teaming offers a promising alternative to the labor-intensive and error-prone manual probing for vulnerabilities, providing more consistent and scalable safety evaluations. However, existing approaches often compromise diversity by focusing on maximizing attack success rate. Additionally, methods that decrease the cosine similarity from historical embeddings with semantic diversity rewards lead to novelty stagnation as history grows. To address these issues, we introduce DiveR-CT, which relaxes conventional constraints on the objective and semantic reward, granting greater freedom for the policy to enhance diversity. Our experiments demonstrate DiveR-CT's marked superiority over baselines by 1) generating data that perform better in various diversity metrics across different attack success rate levels, 2) better-enhancing resiliency in blue team models through safety tuning based on collected data, 3) allowing dynamic control of objective weights for reliable and controllable attack success rates, and 4) reducing susceptibility to reward overoptimization. Project details and code can be found at https://andrewzh112.github.io/#diverct.","sentences":["Recent advances in large language models (LLMs) have made them indispensable, raising significant concerns over managing their safety.","Automated red teaming offers a promising alternative to the labor-intensive and error-prone manual probing for vulnerabilities, providing more consistent and scalable safety evaluations.","However, existing approaches often compromise diversity by focusing on maximizing attack success rate.","Additionally, methods that decrease the cosine similarity from historical embeddings with semantic diversity rewards lead to novelty stagnation as history grows.","To address these issues, we introduce DiveR-CT, which relaxes conventional constraints on the objective and semantic reward, granting greater freedom for the policy to enhance diversity.","Our experiments demonstrate DiveR-CT's marked superiority over baselines by 1) generating data that perform better in various diversity metrics across different attack success rate levels, 2) better-enhancing resiliency in blue team models through safety tuning based on collected data, 3) allowing dynamic control of objective weights for reliable and controllable attack success rates, and 4) reducing susceptibility to reward overoptimization.","Project details and code can be found at https://andrewzh112.github.io/#diverct."],"url":"http://arxiv.org/abs/2405.19026v1","category":"cs.LG"}
{"created":"2024-05-29 12:07:17","title":"Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory","abstract":"We consider inverse reinforcement learning problems with concave utilities. Concave Utility Reinforcement Learning (CURL) is a generalisation of the standard RL objective, which employs a concave function of the state occupancy measure, rather than a linear function. CURL has garnered recent attention for its ability to represent instances of many important applications including the standard RL such as imitation learning, pure exploration, constrained MDPs, offline RL, human-regularized RL, and others. Inverse reinforcement learning is a powerful paradigm that focuses on recovering an unknown reward function that can rationalize the observed behaviour of an agent. There has been recent theoretical advances in inverse RL where the problem is formulated as identifying the set of feasible reward functions. However, inverse RL for CURL problems has not been considered previously. In this paper we show that most of the standard IRL results do not apply to CURL in general, since CURL invalidates the classical Bellman equations. This calls for a new theoretical framework for the inverse CURL problem. Using a recent equivalence result between CURL and Mean-field Games, we propose a new definition for the feasible rewards for I-CURL by proving that this problem is equivalent to an inverse game theory problem in a subclass of mean-field games. We present initial query and sample complexity results for the I-CURL problem under assumptions such as Lipschitz-continuity. Finally, we outline future directions and applications in human--AI collaboration enabled by our results.","sentences":["We consider inverse reinforcement learning problems with concave utilities.","Concave Utility Reinforcement Learning (CURL) is a generalisation of the standard RL objective, which employs a concave function of the state occupancy measure, rather than a linear function.","CURL has garnered recent attention for its ability to represent instances of many important applications including the standard RL such as imitation learning, pure exploration, constrained MDPs, offline RL, human-regularized RL, and others.","Inverse reinforcement learning is a powerful paradigm that focuses on recovering an unknown reward function that can rationalize the observed behaviour of an agent.","There has been recent theoretical advances in inverse RL where the problem is formulated as identifying the set of feasible reward functions.","However, inverse RL for CURL problems has not been considered previously.","In this paper we show that most of the standard IRL results do not apply to CURL in general, since CURL invalidates the classical Bellman equations.","This calls for a new theoretical framework for the inverse CURL problem.","Using a recent equivalence result between CURL and Mean-field Games, we propose a new definition for the feasible rewards for I-CURL by proving that this problem is equivalent to an inverse game theory problem in a subclass of mean-field games.","We present initial query and sample complexity results for the I-CURL problem under assumptions such as Lipschitz-continuity.","Finally, we outline future directions and applications in human--AI collaboration enabled by our results."],"url":"http://arxiv.org/abs/2405.19024v1","category":"cs.LG"}
{"created":"2024-05-29 12:06:38","title":"Ideal Torsion Pairs for Artin Algebras","abstract":"For the module category of an Artin algebra, we generalize the notion of torsion pairs to ideal torsion pairs. Instead of full subcategories of modules, ideals of morphisms of the ambient category are considered. We characterize the functorially finite ideal torsion pairs, which are those fulfilling some nice approximation conditions, first through corresponding functors and then through the notion of ideals determined by objects introduced in this work. As an application of this theory, we generalize preprojective modules, introduce a new homological dimension, the torsion dimension, and establish its connection with the Krull-Gabriel dimension. In particular, it is shown that both dimensions coincide for hereditary Artin algebras.","sentences":["For the module category of an Artin algebra, we generalize the notion of torsion pairs to ideal torsion pairs.","Instead of full subcategories of modules, ideals of morphisms of the ambient category are considered.","We characterize the functorially finite ideal torsion pairs, which are those fulfilling some nice approximation conditions, first through corresponding functors and then through the notion of ideals determined by objects introduced in this work.","As an application of this theory, we generalize preprojective modules, introduce a new homological dimension, the torsion dimension, and establish its connection with the Krull-Gabriel dimension.","In particular, it is shown that both dimensions coincide for hereditary Artin algebras."],"url":"http://arxiv.org/abs/2405.19023v1","category":"math.RT"}
{"created":"2024-05-29 12:03:45","title":"Towards Standardizing AI Bias Exploration","abstract":"Creating fair AI systems is a complex problem that involves the assessment of context-dependent bias concerns. Existing research and programming libraries express specific concerns as measures of bias that they aim to constrain or mitigate. In practice, one should explore a wide variety of (sometimes incompatible) measures before deciding which ones warrant corrective action, but their narrow scope means that most new situations can only be examined after devising new measures. In this work, we present a mathematical framework that distils literature measures of bias into building blocks, hereby facilitating new combinations to cover a wide range of fairness concerns, such as classification or recommendation differences across multiple multi-value sensitive attributes (e.g., many genders and races, and their intersections). We show how this framework generalizes existing concepts and present frequently used blocks. We provide an open-source implementation of our framework as a Python library, called FairBench, that facilitates systematic and extensible exploration of potential bias concerns.","sentences":["Creating fair AI systems is a complex problem that involves the assessment of context-dependent bias concerns.","Existing research and programming libraries express specific concerns as measures of bias that they aim to constrain or mitigate.","In practice, one should explore a wide variety of (sometimes incompatible) measures before deciding which ones warrant corrective action, but their narrow scope means that most new situations can only be examined after devising new measures.","In this work, we present a mathematical framework that distils literature measures of bias into building blocks, hereby facilitating new combinations to cover a wide range of fairness concerns, such as classification or recommendation differences across multiple multi-value sensitive attributes (e.g., many genders and races, and their intersections).","We show how this framework generalizes existing concepts and present frequently used blocks.","We provide an open-source implementation of our framework as a Python library, called FairBench, that facilitates systematic and extensible exploration of potential bias concerns."],"url":"http://arxiv.org/abs/2405.19022v1","category":"cs.LG"}
{"created":"2024-05-29 12:01:49","title":"Physics-Aware Neural Implicit Solvers for multiscale, parametric PDEs with applications in heterogeneous media","abstract":"We propose Physics-Aware Neural Implicit Solvers (PANIS), a novel, data-driven framework for learning surrogates for parametrized Partial Differential Equations (PDEs). It consists of a probabilistic, learning objective in which weighted residuals are used to probe the PDE and provide a source of {\\em virtual} data i.e. the actual PDE never needs to be solved. This is combined with a physics-aware implicit solver that consists of a much coarser, discretized version of the original PDE, which provides the requisite information bottleneck for high-dimensional problems and enables generalization in out-of-distribution settings (e.g. different boundary conditions). We demonstrate its capability in the context of random heterogeneous materials where the input parameters represent the material microstructure. We extend the framework to multiscale problems and show that a surrogate can be learned for the effective (homogenized) solution without ever solving the reference problem. We further demonstrate how the proposed framework can accommodate and generalize several existing learning objectives and architectures while yielding probabilistic surrogates that can quantify predictive uncertainty.","sentences":["We propose Physics-Aware Neural Implicit Solvers (PANIS), a novel, data-driven framework for learning surrogates for parametrized Partial Differential Equations (PDEs).","It consists of a probabilistic, learning objective in which weighted residuals are used to probe the PDE and provide a source of {\\em virtual} data i.e. the actual PDE never needs to be solved.","This is combined with a physics-aware implicit solver that consists of a much coarser, discretized version of the original PDE, which provides the requisite information bottleneck for high-dimensional problems and enables generalization in out-of-distribution settings (e.g. different boundary conditions).","We demonstrate its capability in the context of random heterogeneous materials where the input parameters represent the material microstructure.","We extend the framework to multiscale problems and show that a surrogate can be learned for the effective (homogenized) solution without ever solving the reference problem.","We further demonstrate how the proposed framework can accommodate and generalize several existing learning objectives and architectures while yielding probabilistic surrogates that can quantify predictive uncertainty."],"url":"http://arxiv.org/abs/2405.19019v1","category":"stat.ML"}
{"created":"2024-05-29 11:51:33","title":"Implicit Neural Image Field for Biological Microscopy Image Compression","abstract":"The rapid pace of innovation in biological microscopy imaging has led to large images, putting pressure on data storage and impeding efficient sharing, management, and visualization. This necessitates the development of efficient compression solutions. Traditional CODEC methods struggle to adapt to the diverse bioimaging data and often suffer from sub-optimal compression. In this study, we propose an adaptive compression workflow based on Implicit Neural Representation (INR). This approach permits application-specific compression objectives, capable of compressing images of any shape and arbitrary pixel-wise decompression. We demonstrated on a wide range of microscopy images from real applications that our workflow not only achieved high, controllable compression ratios (e.g., 512x) but also preserved detailed information critical for downstream analysis.","sentences":["The rapid pace of innovation in biological microscopy imaging has led to large images, putting pressure on data storage and impeding efficient sharing, management, and visualization.","This necessitates the development of efficient compression solutions.","Traditional CODEC methods struggle to adapt to the diverse bioimaging data and often suffer from sub-optimal compression.","In this study, we propose an adaptive compression workflow based on Implicit Neural Representation (INR).","This approach permits application-specific compression objectives, capable of compressing images of any shape and arbitrary pixel-wise decompression.","We demonstrated on a wide range of microscopy images from real applications that our workflow not only achieved high, controllable compression ratios (e.g., 512x) but also preserved detailed information critical for downstream analysis."],"url":"http://arxiv.org/abs/2405.19012v1","category":"cs.AI"}
{"created":"2024-05-29 11:49:57","title":"Examining the development of attitude scales using Large Language Models (LLMs)","abstract":"For nearly a century, social researchers and psychologists have debated the efficacy of psychometric scales for attitude measurement, focusing on Thurstone's equal appearing interval scales and Likert's summated rating scales. Thurstone scales fell out of favour due to the labour intensive process of gathering judges' opinions on the initial items. However, advancements in technology have mitigated these challenges, nullifying the simplicity advantage of Likert scales, which have their own methodological issues. This study explores a methodological experiment to develop a Thurstone scale for assessing attitudes towards individuals living with AIDS. An electronic questionnaire was distributed to a group of judges, including undergraduate, postgraduate, and PhD students from disciplines such as social policy, law, medicine, and computer engineering, alongside established social researchers, and their responses were statistically analysed. The primary innovation of this study is the incorporation of an Artificial Intelligence (AI) Large Language Model (LLM) to evaluate the initial 63 items, comparing its assessments with those of the human judges. Interestingly, the AI provided also detailed explanations for its categorisation. Results showed no significant difference between AI and human judges for 35 items, minor differences for 23 items, and major differences for 5 items. This experiment demonstrates the potential of integrating AI with traditional psychometric methods to enhance the development of attitude measurement scales.","sentences":["For nearly a century, social researchers and psychologists have debated the efficacy of psychometric scales for attitude measurement, focusing on Thurstone's equal appearing interval scales and Likert's summated rating scales.","Thurstone scales fell out of favour due to the labour intensive process of gathering judges' opinions on the initial items.","However, advancements in technology have mitigated these challenges, nullifying the simplicity advantage of Likert scales, which have their own methodological issues.","This study explores a methodological experiment to develop a Thurstone scale for assessing attitudes towards individuals living with AIDS.","An electronic questionnaire was distributed to a group of judges, including undergraduate, postgraduate, and PhD students from disciplines such as social policy, law, medicine, and computer engineering, alongside established social researchers, and their responses were statistically analysed.","The primary innovation of this study is the incorporation of an Artificial Intelligence (AI) Large Language Model (LLM) to evaluate the initial 63 items, comparing its assessments with those of the human judges.","Interestingly, the AI provided also detailed explanations for its categorisation.","Results showed no significant difference between AI and human judges for 35 items, minor differences for 23 items, and major differences for 5 items.","This experiment demonstrates the potential of integrating AI with traditional psychometric methods to enhance the development of attitude measurement scales."],"url":"http://arxiv.org/abs/2405.19011v1","category":"stat.AP"}
{"created":"2024-05-29 11:48:27","title":"Evaluating the External and Parametric Knowledge Fusion of Large Language Models","abstract":"Integrating external knowledge into large language models (LLMs) presents a promising solution to overcome the limitations imposed by their antiquated and static parametric memory. Prior studies, however, have tended to over-reliance on external knowledge, underestimating the valuable contributions of an LLMs' intrinsic parametric knowledge. The efficacy of LLMs in blending external and parametric knowledge remains largely unexplored, especially in cases where external knowledge is incomplete and necessitates supplementation by their parametric knowledge. We propose to deconstruct knowledge fusion into four distinct scenarios, offering the first thorough investigation of LLM behavior across each. We develop a systematic pipeline for data construction and knowledge infusion to simulate these fusion scenarios, facilitating a series of controlled experiments. Our investigation reveals that enhancing parametric knowledge within LLMs can significantly bolster their capability for knowledge integration. Nonetheless, we identify persistent challenges in memorizing and eliciting parametric knowledge, and determining parametric knowledge boundaries. Our findings aim to steer future explorations on harmonizing external and parametric knowledge within LLMs.","sentences":["Integrating external knowledge into large language models (LLMs) presents a promising solution to overcome the limitations imposed by their antiquated and static parametric memory.","Prior studies, however, have tended to over-reliance on external knowledge, underestimating the valuable contributions of an LLMs' intrinsic parametric knowledge.","The efficacy of LLMs in blending external and parametric knowledge remains largely unexplored, especially in cases where external knowledge is incomplete and necessitates supplementation by their parametric knowledge.","We propose to deconstruct knowledge fusion into four distinct scenarios, offering the first thorough investigation of LLM behavior across each.","We develop a systematic pipeline for data construction and knowledge infusion to simulate these fusion scenarios, facilitating a series of controlled experiments.","Our investigation reveals that enhancing parametric knowledge within LLMs can significantly bolster their capability for knowledge integration.","Nonetheless, we identify persistent challenges in memorizing and eliciting parametric knowledge, and determining parametric knowledge boundaries.","Our findings aim to steer future explorations on harmonizing external and parametric knowledge within LLMs."],"url":"http://arxiv.org/abs/2405.19010v1","category":"cs.CL"}
{"created":"2024-05-29 11:45:09","title":"Testing eccentric corrections to the radiation-reaction force in the test-mass limit of effective-one-body models","abstract":"In this work, we test an effective-one-body radiation-reaction force for eccentric planar orbits of a test mass in a Kerr background, which contains third-order post-Newtonian (PN) non-spinning and second-order PN spin contributions. We compare the analytical fluxes connected to two different resummations of this force, truncated at different PN orders in the eccentric sector, with the numerical fluxes computed through the use of frequency- and time-domain Teukolsky-equation codes. We find that the different PN truncations of the radiation-reaction force show the expected scaling in the weak gravitational-field regime, and we observe a fractional difference with the numerical fluxes that is $<5 \\%$, for orbits characterized by eccentricity $0 \\le e \\le 0.7$, central black-hole spin $-0.99 M \\le a \\le 0.99 M$ and fixed orbital-averaged quantity $x=\\langle M\\Omega \\rangle^{2/3} = 0.06$, corresponding to the mildly strong-field regime with semilatera recta $9 M<p<17 M$. Our analysis provides useful information for the development of spin-aligned eccentric models in the comparable-mass case.","sentences":["In this work, we test an effective-one-body radiation-reaction force for eccentric planar orbits of a test mass in a Kerr background, which contains third-order post-Newtonian (PN) non-spinning and second-order PN spin contributions.","We compare the analytical fluxes connected to two different resummations of this force, truncated at different PN orders in the eccentric sector, with the numerical fluxes computed through the use of frequency- and time-domain Teukolsky-equation codes.","We find that the different PN truncations of the radiation-reaction force show the expected scaling in the weak gravitational-field regime, and we observe a fractional difference with the numerical fluxes that is $<5 \\%$, for orbits characterized by eccentricity $0 \\le e \\le 0.7$, central black-hole spin $-0.99 M \\le a \\le 0.99 M$ and fixed orbital-averaged quantity $x=\\langle M\\Omega \\rangle^{2/3} = 0.06$, corresponding to the mildly strong-field regime with semilatera recta $9 M<p<17 M$.","Our analysis provides useful information for the development of spin-aligned eccentric models in the comparable-mass case."],"url":"http://arxiv.org/abs/2405.19006v1","category":"gr-qc"}
{"created":"2024-05-29 11:42:02","title":"Auto-selected Knowledge Adapters for Lifelong Person Re-identification","abstract":"Lifelong Person Re-Identification (LReID) extends traditional ReID by requiring systems to continually learn from non-overlapping datasets across different times and locations, adapting to new identities while preserving knowledge of previous ones. Existing approaches, either rehearsal-free or rehearsal-based, still suffer from the problem of catastrophic forgetting since they try to cram diverse knowledge into one fixed model. To overcome this limitation, we introduce a novel framework AdalReID, that adopts knowledge adapters and a parameter-free auto-selection mechanism for lifelong learning. Concretely, we incrementally build distinct adapters to learn domain-specific knowledge at each step, which can effectively learn and preserve knowledge across different datasets. Meanwhile, the proposed auto-selection strategy adaptively calculates the knowledge similarity between the input set and the adapters. On the one hand, the appropriate adapters are selected for the inputs to process ReID, and on the other hand, the knowledge interaction and fusion between adapters are enhanced to improve the generalization ability of the model. Extensive experiments are conducted to demonstrate the superiority of our AdalReID, which significantly outperforms SOTAs by about 10$\\sim$20\\% mAP on both seen and unseen domains.","sentences":["Lifelong Person Re-Identification (LReID) extends traditional ReID by requiring systems to continually learn from non-overlapping datasets across different times and locations, adapting to new identities while preserving knowledge of previous ones.","Existing approaches, either rehearsal-free or rehearsal-based, still suffer from the problem of catastrophic forgetting since they try to cram diverse knowledge into one fixed model.","To overcome this limitation, we introduce a novel framework AdalReID, that adopts knowledge adapters and a parameter-free auto-selection mechanism for lifelong learning.","Concretely, we incrementally build distinct adapters to learn domain-specific knowledge at each step, which can effectively learn and preserve knowledge across different datasets.","Meanwhile, the proposed auto-selection strategy adaptively calculates the knowledge similarity between the input set and the adapters.","On the one hand, the appropriate adapters are selected for the inputs to process ReID, and on the other hand, the knowledge interaction and fusion between adapters are enhanced to improve the generalization ability of the model.","Extensive experiments are conducted to demonstrate the superiority of our AdalReID, which significantly outperforms SOTAs by about 10$\\sim$20\\% mAP on both seen and unseen domains."],"url":"http://arxiv.org/abs/2405.19005v1","category":"cs.CV"}
{"created":"2024-05-29 11:36:12","title":"Exact results, transient generalized Gibbs ensembles, and analytic approximations for spacetime propagators of massive, real scalar fields in one spatial dimension","abstract":"The massive, real scalar field described by the Klein-Gordon equation in one spatial dimension is the most elementary example of a bosonic quantum field theory, and has been investigated for many decades either as a simple academic theory or as a realistic emergent many-body theory in low-dimensional systems. Despite this, the space and time behavior of its propagators have rarely been in the foreground, and although exact results are known, there remain gaps in the description and a lack of an in-depth physical analysis. The aim of this paper is to address the deficits by providing a comprehensive discussion of the results, and to show that this old theory still allows for several new results and insights. To start, known results are rederived in full detail, with an added discussion on how exactly space and time variables need to be extended to complex values to ensure analyticity throughout spacetime. This procedure shows also how singularities on the lightcone need to be regularized to remain compatible with the analyticity and the physical limit of a vanishing mass. An extension to nonzero temperatures is provided by considering the contact of the field to a nonrelativistic thermal reservoir, such as is necessary for emerging field theories in condensed matter systems. Subsequently, it is shown that the transient, short spacetime propagation can be understood in the context of the modern development of a generalized Gibbs ensemble, which describes a massless theory with an effective temperature that is set by the Klein-Gordon mass and the physical temperature. Finally, an approximation scheme is presented that captures the non-trivial mass dependence of the propagators throughout all spacetime but involves only elementary functions.","sentences":["The massive, real scalar field described by the Klein-Gordon equation in one spatial dimension is the most elementary example of a bosonic quantum field theory, and has been investigated for many decades either as a simple academic theory or as a realistic emergent many-body theory in low-dimensional systems.","Despite this, the space and time behavior of its propagators have rarely been in the foreground, and although exact results are known, there remain gaps in the description and a lack of an in-depth physical analysis.","The aim of this paper is to address the deficits by providing a comprehensive discussion of the results, and to show that this old theory still allows for several new results and insights.","To start, known results are rederived in full detail, with an added discussion on how exactly space and time variables need to be extended to complex values to ensure analyticity throughout spacetime.","This procedure shows also how singularities on the lightcone need to be regularized to remain compatible with the analyticity and the physical limit of a vanishing mass.","An extension to nonzero temperatures is provided by considering the contact of the field to a nonrelativistic thermal reservoir, such as is necessary for emerging field theories in condensed matter systems.","Subsequently, it is shown that the transient, short spacetime propagation can be understood in the context of the modern development of a generalized Gibbs ensemble, which describes a massless theory with an effective temperature that is set by the Klein-Gordon mass and the physical temperature.","Finally, an approximation scheme is presented that captures the non-trivial mass dependence of the propagators throughout all spacetime but involves only elementary functions."],"url":"http://arxiv.org/abs/2405.19002v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-29 11:35:01","title":"\"Stumbling-to-Fetters\" mechanism and Virginia Creeper model in hydrogel for designing bionic cardiovascular system","abstract":"Manufacturing hydrogels with identical electrochemical properties are typically riddled with unresolved inquiries and challenges. Here, we utilized ultra-light graphene flakes to trace the influence of convection phenomena during reactions on hydrogels' formation and structural non-uniformity, elucidating its mechanisms. Furthermore, we confirmed that an external electric field induced the orientation of functional groups of hydrogels along the direction of this field, revealing the mechanism of its influence on the structural non-uniformity and electrochemical properties of hydrogels. Additionally, we discovered that ion diffusion was \"Stumbling-to-Fetters\" by the functional groups on the polymer chains within the hydrogel, unveiling this mechanism and developing the Virginia Creeper (VC) model for hydrogels. We demonstrated the scalability and application of the VC model. Furthermore, we proposed a molecular-ion diffusion and current decay equation to describe the electrochemical properties of hydrogels. As an application of the VC model, we developed a bionic cardiovascular system and proved its potential to seamlessly interface with living organisms and generate bio-like bioelectricity. Our findings provide novel insights into triboelectricity and guidance for producing hydrogels with identical electrochemical properties, and offer a new pathway for bioelectric generation and the design of new hydrogel devices.","sentences":["Manufacturing hydrogels with identical electrochemical properties are typically riddled with unresolved inquiries and challenges.","Here, we utilized ultra-light graphene flakes to trace the influence of convection phenomena during reactions on hydrogels' formation and structural non-uniformity, elucidating its mechanisms.","Furthermore, we confirmed that an external electric field induced the orientation of functional groups of hydrogels along the direction of this field, revealing the mechanism of its influence on the structural non-uniformity and electrochemical properties of hydrogels.","Additionally, we discovered that ion diffusion was \"Stumbling-to-Fetters\" by the functional groups on the polymer chains within the hydrogel, unveiling this mechanism and developing the Virginia Creeper (VC) model for hydrogels.","We demonstrated the scalability and application of the VC model.","Furthermore, we proposed a molecular-ion diffusion and current decay equation to describe the electrochemical properties of hydrogels.","As an application of the VC model, we developed a bionic cardiovascular system and proved its potential to seamlessly interface with living organisms and generate bio-like bioelectricity.","Our findings provide novel insights into triboelectricity and guidance for producing hydrogels with identical electrochemical properties, and offer a new pathway for bioelectric generation and the design of new hydrogel devices."],"url":"http://arxiv.org/abs/2405.18445v1","category":"cond-mat.soft"}
{"created":"2024-05-29 11:28:06","title":"FedMAP: Unlocking Potential in Personalized Federated Learning through Bi-Level MAP Optimization","abstract":"Federated Learning (FL) enables collaborative training of machine learning models on decentralized data while preserving data privacy. However, data across clients often differs significantly due to class imbalance, feature distribution skew, sample size imbalance, and other phenomena. Leveraging information from these not identically distributed (non-IID) datasets poses substantial challenges. FL methods based on a single global model cannot effectively capture the variations in client data and underperform in non-IID settings. Consequently, Personalized FL (PFL) approaches that adapt to each client's data distribution but leverage other clients' data are essential but currently underexplored. We propose a novel Bayesian PFL framework using bi-level optimization to tackle the data heterogeneity challenges. Our proposed framework utilizes the global model as a prior distribution within a Maximum A Posteriori (MAP) estimation of personalized client models. This approach facilitates PFL by integrating shared knowledge from the prior, thereby enhancing local model performance, generalization ability, and communication efficiency. We extensively evaluated our bi-level optimization approach on real-world and synthetic datasets, demonstrating significant improvements in model accuracy compared to existing methods while reducing communication overhead. This study contributes to PFL by establishing a solid theoretical foundation for the proposed method and offering a robust, ready-to-use framework that effectively addresses the challenges posed by non-IID data in FL.","sentences":["Federated Learning (FL) enables collaborative training of machine learning models on decentralized data while preserving data privacy.","However, data across clients often differs significantly due to class imbalance, feature distribution skew, sample size imbalance, and other phenomena.","Leveraging information from these not identically distributed (non-IID) datasets poses substantial challenges.","FL methods based on a single global model cannot effectively capture the variations in client data and underperform in non-IID settings.","Consequently, Personalized FL (PFL) approaches that adapt to each client's data distribution but leverage other clients' data are essential but currently underexplored.","We propose a novel Bayesian PFL framework using bi-level optimization to tackle the data heterogeneity challenges.","Our proposed framework utilizes the global model as a prior distribution within a Maximum A Posteriori (MAP) estimation of personalized client models.","This approach facilitates PFL by integrating shared knowledge from the prior, thereby enhancing local model performance, generalization ability, and communication efficiency.","We extensively evaluated our bi-level optimization approach on real-world and synthetic datasets, demonstrating significant improvements in model accuracy compared to existing methods while reducing communication overhead.","This study contributes to PFL by establishing a solid theoretical foundation for the proposed method and offering a robust, ready-to-use framework that effectively addresses the challenges posed by non-IID data in FL."],"url":"http://arxiv.org/abs/2405.19000v1","category":"cs.LG"}
{"created":"2024-05-29 11:25:53","title":"Continuously Optimizing Radar Placement with Model Predictive Path Integrals","abstract":"Continuously optimizing sensor placement is essential for precise target localization in various military and civilian applications. While information theory has shown promise in optimizing sensor placement, many studies oversimplify sensor measurement models or neglect dynamic constraints of mobile sensors. To address these challenges, we employ a range measurement model that incorporates radar parameters and radar-target distance, coupled with Model Predictive Path Integral (MPPI) control to manage complex environmental obstacles and dynamic constraints. We compare the proposed approach against stationary radars or simplified range measurement models based on the root mean squared error (RMSE) of the Cubature Kalman Filter (CKF) estimator for the targets' state. Additionally, we visualize the evolving geometry of radars and targets over time, highlighting areas of highest measurement information gain, demonstrating the strengths of the approach. The proposed strategy outperforms stationary radars and simplified range measurement models in target localization, achieving a 38-74% reduction in mean RMSE and a 33-79% reduction in the upper tail of the 90% Highest Density Interval (HDI) over 500 Monte Carl (MC) trials across all time steps.   Code will be made publicly available upon acceptance.","sentences":["Continuously optimizing sensor placement is essential for precise target localization in various military and civilian applications.","While information theory has shown promise in optimizing sensor placement, many studies oversimplify sensor measurement models or neglect dynamic constraints of mobile sensors.","To address these challenges, we employ a range measurement model that incorporates radar parameters and radar-target distance, coupled with Model Predictive Path Integral (MPPI) control to manage complex environmental obstacles and dynamic constraints.","We compare the proposed approach against stationary radars or simplified range measurement models based on the root mean squared error (RMSE) of the Cubature Kalman Filter (CKF) estimator for the targets' state.","Additionally, we visualize the evolving geometry of radars and targets over time, highlighting areas of highest measurement information gain, demonstrating the strengths of the approach.","The proposed strategy outperforms stationary radars and simplified range measurement models in target localization, achieving a 38-74% reduction in mean RMSE and a 33-79% reduction in the upper tail of the 90% Highest Density Interval (HDI) over 500 Monte Carl (MC) trials across all time steps.   ","Code will be made publicly available upon acceptance."],"url":"http://arxiv.org/abs/2405.18999v1","category":"stat.AP"}
{"created":"2024-05-29 11:15:12","title":"ParsEval: Evaluation of Parsing Behavior using Real-world Out-in-the-wild X.509 Certificates","abstract":"X.509 certificates play a crucial role in establishing secure communication over the internet by enabling authentication and data integrity. Equipped with a rich feature set, the X.509 standard is defined by multiple, comprehensive ISO/IEC documents. Due to its internet-wide usage, there are different implementations in multiple programming languages leading to a large and fragmented ecosystem. This work addresses the research question \"Are there user-visible and security-related differences between X.509 certificate parsers?\". Relevant libraries offering APIs for parsing X.509 certificates were investigated and an appropriate test suite was developed. From 34 libraries 6 were chosen for further analysis. The X.509 parsing modules of the chosen libraries were called with 186,576,846 different certificates from a real-world dataset and the observed error codes were investigated. This study reveals an anomaly in wolfSSL's X.509 parsing module and that there are fundamental differences in the ecosystem. While related studies nowadays mostly focus on fuzzing techniques resulting in artificial certificates, this study confirms that available X.509 parsing modules differ largely and yield different results, even for real-world out-in-the-wild certificates.","sentences":["X.509 certificates play a crucial role in establishing secure communication over the internet by enabling authentication and data integrity.","Equipped with a rich feature set, the X.509 standard is defined by multiple, comprehensive ISO/IEC documents.","Due to its internet-wide usage, there are different implementations in multiple programming languages leading to a large and fragmented ecosystem.","This work addresses the research question \"Are there user-visible and security-related differences between X.509 certificate parsers?\".","Relevant libraries offering APIs for parsing X.509 certificates were investigated and an appropriate test suite was developed.","From 34 libraries 6 were chosen for further analysis.","The X.509 parsing modules of the chosen libraries were called with 186,576,846 different certificates from a real-world dataset and the observed error codes were investigated.","This study reveals an anomaly in wolfSSL's X.509 parsing module and that there are fundamental differences in the ecosystem.","While related studies nowadays mostly focus on fuzzing techniques resulting in artificial certificates, this study confirms that available X.509 parsing modules differ largely and yield different results, even for real-world out-in-the-wild certificates."],"url":"http://arxiv.org/abs/2405.18993v1","category":"cs.CR"}
{"created":"2024-05-29 11:11:07","title":"EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture","abstract":"This paper presents EasyAnimate, an advanced method for video generation that leverages the power of transformer architecture for high-performance outcomes. We have expanded the DiT framework originally designed for 2D image synthesis to accommodate the complexities of 3D video generation by incorporating a motion module block. It is used to capture temporal dynamics, thereby ensuring the production of consistent frames and seamless motion transitions. The motion module can be adapted to various DiT baseline methods to generate video with different styles. It can also generate videos with different frame rates and resolutions during both training and inference phases, suitable for both images and videos. Moreover, we introduce slice VAE, a novel approach to condense the temporal axis, facilitating the generation of long duration videos. Currently, EasyAnimate exhibits the proficiency to generate videos with 144 frames. We provide a holistic ecosystem for video production based on DiT, encompassing aspects such as data pre-processing, VAE training, DiT models training (both the baseline model and LoRA model), and end-to-end video inference. Code is available at: https://github.com/aigc-apps/EasyAnimate. We are continuously working to enhance the performance of our method.","sentences":["This paper presents EasyAnimate, an advanced method for video generation that leverages the power of transformer architecture for high-performance outcomes.","We have expanded the DiT framework originally designed for 2D image synthesis to accommodate the complexities of 3D video generation by incorporating a motion module block.","It is used to capture temporal dynamics, thereby ensuring the production of consistent frames and seamless motion transitions.","The motion module can be adapted to various DiT baseline methods to generate video with different styles.","It can also generate videos with different frame rates and resolutions during both training and inference phases, suitable for both images and videos.","Moreover, we introduce slice VAE, a novel approach to condense the temporal axis, facilitating the generation of long duration videos.","Currently, EasyAnimate exhibits the proficiency to generate videos with 144 frames.","We provide a holistic ecosystem for video production based on DiT, encompassing aspects such as data pre-processing, VAE training, DiT models training (both the baseline model and LoRA model), and end-to-end video inference.","Code is available at: https://github.com/aigc-apps/EasyAnimate.","We are continuously working to enhance the performance of our method."],"url":"http://arxiv.org/abs/2405.18991v1","category":"cs.CV"}
{"created":"2024-05-29 11:03:42","title":"Robust Optimization in Protein Fitness Landscapes Using Reinforcement Learning in Latent Space","abstract":"Proteins are complex molecules responsible for different functions in nature. Enhancing the functionality of proteins and cellular fitness can significantly impact various industries. However, protein optimization using computational methods remains challenging, especially when starting from low-fitness sequences. We propose LatProtRL, an optimization method to efficiently traverse a latent space learned by an encoder-decoder leveraging a large protein language model. To escape local optima, our optimization is modeled as a Markov decision process using reinforcement learning acting directly in latent space. We evaluate our approach on two important fitness optimization tasks, demonstrating its ability to achieve comparable or superior fitness over baseline methods. Our findings and in vitro evaluation show that the generated sequences can reach high-fitness regions, suggesting a substantial potential of LatProtRL in lab-in-the-loop scenarios.","sentences":["Proteins are complex molecules responsible for different functions in nature.","Enhancing the functionality of proteins and cellular fitness can significantly impact various industries.","However, protein optimization using computational methods remains challenging, especially when starting from low-fitness sequences.","We propose LatProtRL, an optimization method to efficiently traverse a latent space learned by an encoder-decoder leveraging a large protein language model.","To escape local optima, our optimization is modeled as a Markov decision process using reinforcement learning acting directly in latent space.","We evaluate our approach on two important fitness optimization tasks, demonstrating its ability to achieve comparable or superior fitness over baseline methods.","Our findings and in vitro evaluation show that the generated sequences can reach high-fitness regions, suggesting a substantial potential of LatProtRL in lab-in-the-loop scenarios."],"url":"http://arxiv.org/abs/2405.18986v1","category":"cs.LG"}
{"created":"2024-05-29 10:58:01","title":"On Magnetic Compression in Gyrokinetic Field Theory","abstract":"The issue of finite magnetic compressibility in low-beta magnetised plasmas is considered within the gyrokinetic description. The gauge transformation method of Littlejohn is used to obtain a Lagrangian which contains this effect additionally. The field theory version obtains a system model which guarantees exact energetic consistency. Gyrocenter drifts under this model are considered within a Chew-Goldberger-Low MHD equilibrium allowing for pressure anisotropy. The contributions to the current divergence balance, hence the dynamics, due to the difference between the curvature and grad-B drifts and to the compressibility are shown to cancel up to corrections of order beta. This recovers an earlier result with the same conclusion within linear theory of kinetic ballooning modes.","sentences":["The issue of finite magnetic compressibility in low-beta magnetised plasmas is considered within the gyrokinetic description.","The gauge transformation method of Littlejohn is used to obtain a Lagrangian which contains this effect additionally.","The field theory version obtains a system model which guarantees exact energetic consistency.","Gyrocenter drifts under this model are considered within a Chew-Goldberger-Low MHD equilibrium allowing for pressure anisotropy.","The contributions to the current divergence balance, hence the dynamics, due to the difference between the curvature and grad-B drifts and to the compressibility are shown to cancel up to corrections of order beta.","This recovers an earlier result with the same conclusion within linear theory of kinetic ballooning modes."],"url":"http://arxiv.org/abs/2405.18985v1","category":"physics.plasm-ph"}
{"created":"2024-05-29 10:57:25","title":"Optimizing Vehicular Networks with Variational Quantum Circuits-based Reinforcement Learning","abstract":"In vehicular networks (VNets), ensuring both road safety and dependable network connectivity is of utmost importance. Achieving this necessitates the creation of resilient and efficient decision-making policies that prioritize multiple objectives. In this paper, we develop a Variational Quantum Circuit (VQC)-based multi-objective reinforcement learning (MORL) framework to characterize efficient network selection and autonomous driving policies in a vehicular network (VNet). Numerical results showcase notable enhancements in both convergence rates and rewards when compared to conventional deep-Q networks (DQNs), validating the efficacy of the VQC-MORL solution.","sentences":["In vehicular networks (VNets), ensuring both road safety and dependable network connectivity is of utmost importance.","Achieving this necessitates the creation of resilient and efficient decision-making policies that prioritize multiple objectives.","In this paper, we develop a Variational Quantum Circuit (VQC)-based multi-objective reinforcement learning (MORL) framework to characterize efficient network selection and autonomous driving policies in a vehicular network (VNet).","Numerical results showcase notable enhancements in both convergence rates and rewards when compared to conventional deep-Q networks (DQNs), validating the efficacy of the VQC-MORL solution."],"url":"http://arxiv.org/abs/2405.18984v1","category":"cs.LG"}
{"created":"2024-05-29 10:56:13","title":"Federated Learning under Partially Class-Disjoint Data via Manifold Reshaping","abstract":"Statistical heterogeneity severely limits the performance of federated learning (FL), motivating several explorations e.g., FedProx, MOON and FedDyn, to alleviate this problem. Despite effectiveness, their considered scenario generally requires samples from almost all classes during the local training of each client, although some covariate shifts may exist among clients. In fact, the natural case of partially class-disjoint data (PCDD), where each client contributes a few classes (instead of all classes) of samples, is practical yet underexplored. Specifically, the unique collapse and invasion characteristics of PCDD can induce the biased optimization direction in local training, which prevents the efficiency of federated learning. To address this dilemma, we propose a manifold reshaping approach called FedMR to calibrate the feature space of local training. Our FedMR adds two interplaying losses to the vanilla federated learning: one is intra-class loss to decorrelate feature dimensions for anti-collapse; and the other one is inter-class loss to guarantee the proper margin among categories in the feature expansion. We conduct extensive experiments on a range of datasets to demonstrate that our FedMR achieves much higher accuracy and better communication efficiency. Source code is available at: https://github.com/MediaBrain-SJTU/FedMR.git.","sentences":["Statistical heterogeneity severely limits the performance of federated learning (FL), motivating several explorations e.g., FedProx, MOON and FedDyn, to alleviate this problem.","Despite effectiveness, their considered scenario generally requires samples from almost all classes during the local training of each client, although some covariate shifts may exist among clients.","In fact, the natural case of partially class-disjoint data (PCDD), where each client contributes a few classes (instead of all classes) of samples, is practical yet underexplored.","Specifically, the unique collapse and invasion characteristics of PCDD can induce the biased optimization direction in local training, which prevents the efficiency of federated learning.","To address this dilemma, we propose a manifold reshaping approach called FedMR to calibrate the feature space of local training.","Our FedMR adds two interplaying losses to the vanilla federated learning: one is intra-class loss to decorrelate feature dimensions for anti-collapse; and the other one is inter-class loss to guarantee the proper margin among categories in the feature expansion.","We conduct extensive experiments on a range of datasets to demonstrate that our FedMR achieves much higher accuracy and better communication efficiency.","Source code is available at: https://github.com/MediaBrain-SJTU/FedMR.git."],"url":"http://arxiv.org/abs/2405.18983v1","category":"cs.LG"}
{"created":"2024-05-29 10:45:06","title":"MANO: Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under Distribution Shifts","abstract":"Leveraging the models' outputs, specifically the logits, is a common approach to estimating the test accuracy of a pre-trained neural network on out-of-distribution (OOD) samples without requiring access to the corresponding ground truth labels. Despite their ease of implementation and computational efficiency, current logit-based methods are vulnerable to overconfidence issues, leading to prediction bias, especially under the natural shift. In this work, we first study the relationship between logits and generalization performance from the view of low-density separation assumption. Our findings motivate our proposed method MaNo which (1) applies a data-dependent normalization on the logits to reduce prediction bias, and (2) takes the $L_p$ norm of the matrix of normalized logits as the estimation score. Our theoretical analysis highlights the connection between the provided score and the model's uncertainty. We conduct an extensive empirical study on common unsupervised accuracy estimation benchmarks and demonstrate that MaNo achieves state-of-the-art performance across various architectures in the presence of synthetic, natural, or subpopulation shifts.","sentences":["Leveraging the models' outputs, specifically the logits, is a common approach to estimating the test accuracy of a pre-trained neural network on out-of-distribution (OOD) samples without requiring access to the corresponding ground truth labels.","Despite their ease of implementation and computational efficiency, current logit-based methods are vulnerable to overconfidence issues, leading to prediction bias, especially under the natural shift.","In this work, we first study the relationship between logits and generalization performance from the view of low-density separation assumption.","Our findings motivate our proposed method MaNo which (1) applies a data-dependent normalization on the logits to reduce prediction bias, and (2) takes the $L_p$ norm of the matrix of normalized logits as the estimation score.","Our theoretical analysis highlights the connection between the provided score and the model's uncertainty.","We conduct an extensive empirical study on common unsupervised accuracy estimation benchmarks and demonstrate that MaNo achieves state-of-the-art performance across various architectures in the presence of synthetic, natural, or subpopulation shifts."],"url":"http://arxiv.org/abs/2405.18979v1","category":"cs.LG"}
{"created":"2024-05-29 10:40:49","title":"Accelerated Mirror Descent for Non-Euclidean Star-convex Functions","abstract":"Acceleration for non-convex functions has been an important problem in optimisation. We revisit star-convex functions, which are strictly unimodal on all lines through a minimizer. In [1], the authors accelerate gradient descent for star-convex functions with gradients that are Lipschitz with respect to the Euclidean norm in an unconstrained domain. In this paper, we introduce a new assumption about the regularity of the derivative of a general norm and we accelerate mirror descent for this class of normed spaces. We show that, under it, our algorithms show sharp convergence rates for star-convex functions with -H\"older continuous gradients. We also prove that our convergence rate is near optimal for -norms.   [1] Near-Optimal Methods for Minimizing Star-Convex Functions and Beyond, Hinder Oliver and Sidford Aaron and Sohoni Nimit","sentences":["Acceleration for non-convex functions has been an important problem in optimisation.","We revisit star-convex functions, which are strictly unimodal on all lines through a minimizer.","In [1], the authors accelerate gradient descent for star-convex functions with gradients that are Lipschitz with respect to the Euclidean norm in an unconstrained domain.","In this paper, we introduce a new assumption about the regularity of the derivative of a general norm and we accelerate mirror descent for this class of normed spaces.","We show that, under it, our algorithms show sharp convergence rates for star-convex functions with -H\"older continuous gradients.","We also prove that our convergence rate is near optimal for -norms.   ","[1] Near-Optimal Methods for Minimizing Star-Convex Functions and Beyond, Hinder Oliver and Sidford Aaron and Sohoni Nimit"],"url":"http://arxiv.org/abs/2405.18976v1","category":"math.OC"}
{"created":"2024-05-29 10:38:25","title":"Hierarchical Classification Auxiliary Network for Time Series Forecasting","abstract":"Deep learning has significantly advanced time series forecasting through its powerful capacity to capture sequence relationships. However, training these models with the Mean Square Error (MSE) loss often results in over-smooth predictions, making it challenging to handle the complexity and learn high-entropy features from time series data with high variability and unpredictability. In this work, we introduce a novel approach by tokenizing time series values to train forecasting models via cross-entropy loss, while considering the continuous nature of time series data. Specifically, we propose Hierarchical Classification Auxiliary Network, HCAN, a general model-agnostic component that can be integrated with any forecasting model. HCAN is based on a Hierarchy-Aware Attention module that integrates multi-granularity high-entropy features at different hierarchy levels. At each level, we assign a class label for timesteps to train an Uncertainty-Aware Classifier. This classifier mitigates the over-confidence in softmax loss via evidence theory. We also implement a Hierarchical Consistency Loss to maintain prediction consistency across hierarchy levels. Extensive experiments integrating HCAN with state-of-the-art forecasting models demonstrate substantial improvements over baselines on several real-world datasets. Code is available at:https://github.com/syrGitHub/HCAN.","sentences":["Deep learning has significantly advanced time series forecasting through its powerful capacity to capture sequence relationships.","However, training these models with the Mean Square Error (MSE) loss often results in over-smooth predictions, making it challenging to handle the complexity and learn high-entropy features from time series data with high variability and unpredictability.","In this work, we introduce a novel approach by tokenizing time series values to train forecasting models via cross-entropy loss, while considering the continuous nature of time series data.","Specifically, we propose Hierarchical Classification Auxiliary Network, HCAN, a general model-agnostic component that can be integrated with any forecasting model.","HCAN is based on a Hierarchy-Aware Attention module that integrates multi-granularity high-entropy features at different hierarchy levels.","At each level, we assign a class label for timesteps to train an Uncertainty-Aware Classifier.","This classifier mitigates the over-confidence in softmax loss via evidence theory.","We also implement a Hierarchical Consistency Loss to maintain prediction consistency across hierarchy levels.","Extensive experiments integrating HCAN with state-of-the-art forecasting models demonstrate substantial improvements over baselines on several real-world datasets.","Code is available at:https://github.com/syrGitHub/HCAN."],"url":"http://arxiv.org/abs/2405.18975v1","category":"cs.LG"}
{"created":"2024-05-29 10:37:28","title":"Encoding Hierarchical Schema via Concept Flow for Multifaceted Ideology Detection","abstract":"Multifaceted ideology detection (MID) aims to detect the ideological leanings of texts towards multiple facets. Previous studies on ideology detection mainly focus on one generic facet and ignore label semantics and explanatory descriptions of ideologies, which are a kind of instructive information and reveal the specific concepts of ideologies. In this paper, we develop a novel concept semantics-enhanced framework for the MID task. Specifically, we propose a bidirectional iterative concept flow (BICo) method to encode multifaceted ideologies. BICo enables the concepts to flow across levels of the schema tree and enriches concept representations with multi-granularity semantics. Furthermore, we explore concept attentive matching and concept-guided contrastive learning strategies to guide the model to capture ideology features with the learned concept semantics. Extensive experiments on the benchmark dataset show that our approach achieves state-of-the-art performance in MID, including in the cross-topic scenario.","sentences":["Multifaceted ideology detection (MID) aims to detect the ideological leanings of texts towards multiple facets.","Previous studies on ideology detection mainly focus on one generic facet and ignore label semantics and explanatory descriptions of ideologies, which are a kind of instructive information and reveal the specific concepts of ideologies.","In this paper, we develop a novel concept semantics-enhanced framework for the MID task.","Specifically, we propose a bidirectional iterative concept flow (BICo) method to encode multifaceted ideologies.","BICo enables the concepts to flow across levels of the schema tree and enriches concept representations with multi-granularity semantics.","Furthermore, we explore concept attentive matching and concept-guided contrastive learning strategies to guide the model to capture ideology features with the learned concept semantics.","Extensive experiments on the benchmark dataset show that our approach achieves state-of-the-art performance in MID, including in the cross-topic scenario."],"url":"http://arxiv.org/abs/2405.18974v1","category":"cs.CL"}
{"created":"2024-05-29 10:34:44","title":"Federated Learning with Bilateral Curation for Partially Class-Disjoint Data","abstract":"Partially class-disjoint data (PCDD), a common yet under-explored data formation where each client contributes a part of classes (instead of all classes) of samples, severely challenges the performance of federated algorithms. Without full classes, the local objective will contradict the global objective, yielding the angle collapse problem for locally missing classes and the space waste problem for locally existing classes. As far as we know, none of the existing methods can intrinsically mitigate PCDD challenges to achieve holistic improvement in the bilateral views (both global view and local view) of federated learning. To address this dilemma, we are inspired by the strong generalization of simplex Equiangular Tight Frame~(ETF) on the imbalanced data, and propose a novel approach called FedGELA where the classifier is globally fixed as a simplex ETF while locally adapted to the personal distributions. Globally, FedGELA provides fair and equal discrimination for all classes and avoids inaccurate updates of the classifier, while locally it utilizes the space of locally missing classes for locally existing classes. We conduct extensive experiments on a range of datasets to demonstrate that our FedGELA achieves promising performance~(averaged improvement of 3.9% to FedAvg and 1.5% to best baselines) and provide both local and global convergence guarantees. Source code is available at:https://github.com/MediaBrain-SJTU/FedGELA.git.","sentences":["Partially class-disjoint data (PCDD), a common yet under-explored data formation where each client contributes a part of classes (instead of all classes) of samples, severely challenges the performance of federated algorithms.","Without full classes, the local objective will contradict the global objective, yielding the angle collapse problem for locally missing classes and the space waste problem for locally existing classes.","As far as we know, none of the existing methods can intrinsically mitigate PCDD challenges to achieve holistic improvement in the bilateral views (both global view and local view) of federated learning.","To address this dilemma, we are inspired by the strong generalization of simplex Equiangular Tight Frame~(ETF) on the imbalanced data, and propose a novel approach called FedGELA where the classifier is globally fixed as a simplex ETF while locally adapted to the personal distributions.","Globally, FedGELA provides fair and equal discrimination for all classes and avoids inaccurate updates of the classifier, while locally it utilizes the space of locally missing classes for locally existing classes.","We conduct extensive experiments on a range of datasets to demonstrate that our FedGELA achieves promising performance~(averaged improvement of 3.9% to FedAvg and 1.5% to best baselines) and provide both local and global convergence guarantees.","Source code is available at:https://github.com/MediaBrain-SJTU/FedGELA.git."],"url":"http://arxiv.org/abs/2405.18972v1","category":"cs.LG"}
{"created":"2024-05-29 10:26:16","title":"UniIF: Unified Molecule Inverse Folding","abstract":"Molecule inverse folding has been a long-standing challenge in chemistry and biology, with the potential to revolutionize drug discovery and material science. Despite specified models have been proposed for different small- or macro-molecules, few have attempted to unify the learning process, resulting in redundant efforts. Complementary to recent advancements in molecular structure prediction, such as RoseTTAFold All-Atom and AlphaFold3, we propose the unified model UniIF for the inverse folding of all molecules. We do such unification in two levels: 1) Data-Level: We propose a unified block graph data form for all molecules, including the local frame building and geometric feature initialization. 2) Model-Level: We introduce a geometric block attention network, comprising a geometric interaction, interactive attention and virtual long-term dependency modules, to capture the 3D interactions of all molecules. Through comprehensive evaluations across various tasks such as protein design, RNA design, and material design, we demonstrate that our proposed method surpasses state-of-the-art methods on all tasks. UniIF offers a versatile and effective solution for general molecule inverse folding.","sentences":["Molecule inverse folding has been a long-standing challenge in chemistry and biology, with the potential to revolutionize drug discovery and material science.","Despite specified models have been proposed for different small- or macro-molecules, few have attempted to unify the learning process, resulting in redundant efforts.","Complementary to recent advancements in molecular structure prediction, such as RoseTTAFold All-Atom and AlphaFold3, we propose the unified model UniIF for the inverse folding of all molecules.","We do such unification in two levels: 1) Data-Level: We propose a unified block graph data form for all molecules, including the local frame building and geometric feature initialization.","2) Model-Level: We introduce a geometric block attention network, comprising a geometric interaction, interactive attention and virtual long-term dependency modules, to capture the 3D interactions of all molecules.","Through comprehensive evaluations across various tasks such as protein design, RNA design, and material design, we demonstrate that our proposed method surpasses state-of-the-art methods on all tasks.","UniIF offers a versatile and effective solution for general molecule inverse folding."],"url":"http://arxiv.org/abs/2405.18968v1","category":"cs.AI"}
{"created":"2024-05-29 10:24:56","title":"svds-C: A Multi-Thread C Code for Computing Truncated Singular Value Decomposition","abstract":"This article presents svds-C, an open-source and high-performance C program for accurately and robustly computing truncated SVD, e.g. computing several largest singular values and corresponding singular vectors. We have re-implemented the algorithm of svds in Matlab in C based on MKL or OpenBLAS and multi-thread computing to obtain the parallel program named svds-C. svds-C running on shared-memory computer consumes less time and memory than svds thanks to careful implementation of multi-thread parallelization and memory management. Numerical experiments on different test cases which are synthetically generated or directly from real world datasets show that, svds-C runs remarkably faster than svds with averagely 4.7X and at most 12X speedup for 16-thread parallel computing on a computer with Intel CPU, while preserving same accuracy and consuming about half memory space. Experimental results also demonstrate that svds-C has similar advantages over svds on the computer with AMD CPU, and outperforms other state-of-the-art algorithms for truncated SVD on computing time and robustness.","sentences":["This article presents svds-C, an open-source and high-performance C program for accurately and robustly computing truncated SVD, e.g. computing several largest singular values and corresponding singular vectors.","We have re-implemented the algorithm of svds in Matlab in C based on MKL or OpenBLAS and multi-thread computing to obtain the parallel program named svds-C. svds-C running on shared-memory computer consumes less time and memory than svds thanks to careful implementation of multi-thread parallelization and memory management.","Numerical experiments on different test cases which are synthetically generated or directly from real world datasets show that, svds-C runs remarkably faster than svds with averagely 4.7X and at most 12X speedup for 16-thread parallel computing on a computer with Intel CPU, while preserving same accuracy and consuming about half memory space.","Experimental results also demonstrate that svds-C has similar advantages over svds on the computer with AMD CPU, and outperforms other state-of-the-art algorithms for truncated SVD on computing time and robustness."],"url":"http://arxiv.org/abs/2405.18966v1","category":"cs.MS"}
{"created":"2024-05-29 10:24:40","title":"Exploring Probabilistic Distance Fields in Robotics","abstract":"The success of intelligent robotic missions relies on integrating various research tasks, each demanding distinct representations. Designing task-specific representations for each task is costly and impractical. Unified representations suitable for multiple tasks remain unexplored. My outline introduces a series of research outcomes of GP-based probabilistic distance field (GPDF) representation that mathematically models the fundamental property of Euclidean distance field (EDF) along with gradients, surface normals and dense reconstruction. The progress to date and ongoing future works show that GPDF has the potential to offer a unified solution of representation for multiple tasks such as localisation, mapping, motion planning, obstacle avoidance, grasping, human-robot collaboration, and dense visualisation. I believe that GPDF serves as the cornerstone for robots to accomplish more complex and challenging tasks. By leveraging GPDF, robots can navigate through intricate environments, understand spatial relationships, and interact with objects and humans seamlessly.","sentences":["The success of intelligent robotic missions relies on integrating various research tasks, each demanding distinct representations.","Designing task-specific representations for each task is costly and impractical.","Unified representations suitable for multiple tasks remain unexplored.","My outline introduces a series of research outcomes of GP-based probabilistic distance field (GPDF) representation that mathematically models the fundamental property of Euclidean distance field (EDF) along with gradients, surface normals and dense reconstruction.","The progress to date and ongoing future works show that GPDF has the potential to offer a unified solution of representation for multiple tasks such as localisation, mapping, motion planning, obstacle avoidance, grasping, human-robot collaboration, and dense visualisation.","I believe that GPDF serves as the cornerstone for robots to accomplish more complex and challenging tasks.","By leveraging GPDF, robots can navigate through intricate environments, understand spatial relationships, and interact with objects and humans seamlessly."],"url":"http://arxiv.org/abs/2405.18965v1","category":"cs.RO"}
{"created":"2024-05-29 10:13:53","title":"Aharonov-Bohm Scattering From Knots","abstract":"The celebrated Aharonov-Bohm effect is perhaps the first example in which the the interplay between classical topology and quantum theory was explored. This connection has continued to shed light on diverse areas of physics like quantum statistics, anomalies, condensed matter physics, and gauge theories. Several attempts were made to generalize the Aharonov-Bohm effect by modifying the simple solenoidal current distribution used by them to the case of multiple solenoids, and a toroidal solenoid, for example. A particularly ambitious task is to confine the magnetic flux to the interior of a knotted solenoid. While it is to be expected that a non-trivial phase factor will be picked up by the wave function of a charged particle travelling in the complement of the knot in three-dimensional space, the lack of symmetry defied attempts to explicitly solve the associated scattering problem. In this paper we report on a way to make progress towards this problem based on multipole expansions. The vector potential produced by a knot is obtained by making a multipole expansion, which is then used to calculate the S-matrix for the scattering of the charged particle, in the Born approximation. It is found that the S-matrix carries an imprint of the knottedness at the octopole order. For the case of a torus knot, a curious factorization property is seen to hold.","sentences":["The celebrated Aharonov-Bohm effect is perhaps the first example in which the the interplay between classical topology and quantum theory was explored.","This connection has continued to shed light on diverse areas of physics like quantum statistics, anomalies, condensed matter physics, and gauge theories.","Several attempts were made to generalize the Aharonov-Bohm effect by modifying the simple solenoidal current distribution used by them to the case of multiple solenoids, and a toroidal solenoid, for example.","A particularly ambitious task is to confine the magnetic flux to the interior of a knotted solenoid.","While it is to be expected that a non-trivial phase factor will be picked up by the wave function of a charged particle travelling in the complement of the knot in three-dimensional space, the lack of symmetry defied attempts to explicitly solve the associated scattering problem.","In this paper we report on a way to make progress towards this problem based on multipole expansions.","The vector potential produced by a knot is obtained by making a multipole expansion, which is then used to calculate the S-matrix for the scattering of the charged particle, in the Born approximation.","It is found that the S-matrix carries an imprint of the knottedness at the octopole order.","For the case of a torus knot, a curious factorization property is seen to hold."],"url":"http://arxiv.org/abs/2405.18956v1","category":"quant-ph"}
{"created":"2024-05-29 10:11:31","title":"Determining state space anomalies in mean field games","abstract":"In this paper, we are concerned with the inverse problem of determining anomalies in the state space associated with the stationary mean field game (MFG) system. We establish novel unique identifiability results for the intrinsic structure of these anomalies in mean field games systems, including their topological structure and parameter configurations, in several general scenarios of practical interest, including traffic flow, market economics and epidemics. To the best of our knowledge, this is the first work that considers anomalies in the state space for the nonlinear coupled MFG system.","sentences":["In this paper, we are concerned with the inverse problem of determining anomalies in the state space associated with the stationary mean field game (MFG) system.","We establish novel unique identifiability results for the intrinsic structure of these anomalies in mean field games systems, including their topological structure and parameter configurations, in several general scenarios of practical interest, including traffic flow, market economics and epidemics.","To the best of our knowledge, this is the first work that considers anomalies in the state space for the nonlinear coupled MFG system."],"url":"http://arxiv.org/abs/2405.18954v1","category":"math.AP"}
{"created":"2024-05-29 10:08:31","title":"Are You Sure? Rank Them Again: Repeated Ranking For Better Preference Datasets","abstract":"Training Large Language Models (LLMs) with Reinforcement Learning from AI Feedback (RLAIF) aligns model outputs more closely with human preferences. This involves an evaluator model ranking multiple candidate responses to user prompts. However, the rankings from popular evaluator models such as GPT-4 can be inconsistent. We propose the Repeat Ranking method - where we evaluate the same responses multiple times and train only on those responses which are consistently ranked. Using 2,714 prompts in 62 languages, we generated responses from 7 top multilingual LLMs and had GPT-4 rank them five times each. Evaluating on MT-Bench chat benchmarks in six languages, our method outperformed the standard practice of training on all available prompts. Our work highlights the quality versus quantity trade-off in RLAIF dataset generation and offers a stackable strategy for enhancing dataset and thus model quality.","sentences":["Training Large Language Models (LLMs) with Reinforcement Learning from AI Feedback (RLAIF) aligns model outputs more closely with human preferences.","This involves an evaluator model ranking multiple candidate responses to user prompts.","However, the rankings from popular evaluator models such as GPT-4 can be inconsistent.","We propose the Repeat Ranking method - where we evaluate the same responses multiple times and train only on those responses which are consistently ranked.","Using 2,714 prompts in 62 languages, we generated responses from 7 top multilingual LLMs and had GPT-4 rank them five times each.","Evaluating on MT-Bench chat benchmarks in six languages, our method outperformed the standard practice of training on all available prompts.","Our work highlights the quality versus quantity trade-off in RLAIF dataset generation and offers a stackable strategy for enhancing dataset and thus model quality."],"url":"http://arxiv.org/abs/2405.18952v1","category":"cs.CL"}
{"created":"2024-05-29 09:58:58","title":"On Structured Perturbations of Positive Semigroups","abstract":"In this note we generalize perturbation results for positive $C_0$-semigroups on AM- and AL-spaces and give a Weiss--Staffans type perturbation result for generators of positive semigroups on Banach lattices. The abstract results are applied to domain perturbations of generators, a heat equation with boundary feedback and perturbations of the first derivative.","sentences":["In this note we generalize perturbation results for positive $C_0$-semigroups on AM- and AL-spaces and give a Weiss--Staffans type perturbation result for generators of positive semigroups on Banach lattices.","The abstract results are applied to domain perturbations of generators, a heat equation with boundary feedback and perturbations of the first derivative."],"url":"http://arxiv.org/abs/2405.18947v1","category":"math.FA"}
{"created":"2024-05-29 09:58:33","title":"Duality and degeneracy lifting in two-dimensional electron liquids on SrTiO$_3$(001)","abstract":"Two-dimensional electron liquids (2DELs) have increasing technological relevance for ultrafast electronics and spintronics, yet significant gaps in their fundamental understanding are exemplified on the prototypical SrTiO$_3$. We correlate the exact SrTiO$_3$(001) surface structure with distinct 2DELs through combined microscopic angle-resolved photoemission spectroscopy and non-contact atomic force microscopy on truly bulk-terminated surfaces that alleviate structural uncertainties inherent to this long-studied system. The SrO termination is shown to develop a 2DEL following the creation of oxygen vacancies, unlike the intrinsically metallic TiO$_2$ termination. Differences in degeneracy of the 2DELs, that share the same band filling and identical band bending, are assigned to polar distortions of the Ti atoms in combination with spin order, supported with the extraction of fundamental electron-phonon coupling strength. These results not only resolve the ambiguities regarding 2DELs on SrTiO$_3$ thus far, but also pave the way to manipulating band filling and spin order in oxide 2DELs in general.","sentences":["Two-dimensional electron liquids (2DELs) have increasing technological relevance for ultrafast electronics and spintronics, yet significant gaps in their fundamental understanding are exemplified on the prototypical SrTiO$_3$. We correlate the exact SrTiO$_3$(001) surface structure with distinct 2DELs through combined microscopic angle-resolved photoemission spectroscopy and non-contact atomic force microscopy on truly bulk-terminated surfaces that alleviate structural uncertainties inherent to this long-studied system.","The SrO termination is shown to develop a 2DEL following the creation of oxygen vacancies, unlike the intrinsically metallic TiO$_2$ termination.","Differences in degeneracy of the 2DELs, that share the same band filling and identical band bending, are assigned to polar distortions of the Ti atoms in combination with spin order, supported with the extraction of fundamental electron-phonon coupling strength.","These results not only resolve the ambiguities regarding 2DELs on SrTiO$_3$ thus far, but also pave the way to manipulating band filling and spin order in oxide 2DELs in general."],"url":"http://arxiv.org/abs/2405.18946v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 09:52:02","title":"Decoding a mean field game by the Cauchy data around its unknown stationary states","abstract":"In recent years, mean field games (MFGs) have garnered considerable attention and emerged as a dynamic and actively researched field across various domains, including economics, social sciences, finance, and transportation. The inverse design and decoding of MFGs offer valuable means to extract information from observed data and gain insights into the intricate underlying dynamics and strategies of these complex physical systems. This paper presents a novel approach to the study of inverse problems in MFGs by analyzing the Cauchy data around their unknown stationary states. This study distinguishes itself from existing inverse problem investigations in three key significant aspects: Firstly, we consider MFG problems in a highly general form. Secondly, we address the technical challenge of the probability measure constraint by utilizing Cauchy data in our inverse problem study. Thirdly, we enhance existing high order linearization methods by introducing a novel approach that involves conducting linearization around non-trivial stationary states of the MFG system, which are not a-priori known. These contributions provide new insights and offer promising avenues for studying inverse problems for MFGs. By unraveling the hidden structure of MFGs, researchers and practitioners can make informed decisions, optimize system performance, and address real-world challenges more effectively.","sentences":["In recent years, mean field games (MFGs) have garnered considerable attention and emerged as a dynamic and actively researched field across various domains, including economics, social sciences, finance, and transportation.","The inverse design and decoding of MFGs offer valuable means to extract information from observed data and gain insights into the intricate underlying dynamics and strategies of these complex physical systems.","This paper presents a novel approach to the study of inverse problems in MFGs by analyzing the Cauchy data around their unknown stationary states.","This study distinguishes itself from existing inverse problem investigations in three key significant aspects: Firstly, we consider MFG problems in a highly general form.","Secondly, we address the technical challenge of the probability measure constraint by utilizing Cauchy data in our inverse problem study.","Thirdly, we enhance existing high order linearization methods by introducing a novel approach that involves conducting linearization around non-trivial stationary states of the MFG system, which are not a-priori known.","These contributions provide new insights and offer promising avenues for studying inverse problems for MFGs.","By unraveling the hidden structure of MFGs, researchers and practitioners can make informed decisions, optimize system performance, and address real-world challenges more effectively."],"url":"http://arxiv.org/abs/2405.18943v1","category":"math.AP"}
{"created":"2024-05-29 09:50:43","title":"Verifiably Robust Conformal Prediction","abstract":"Conformal Prediction (CP) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable. In such a case, CP's prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability. Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage. Recently, several approaches have been put forward to recover CP guarantees in this setting. These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations. They are, however, limited in that they only support $\\ell^2$-bounded perturbations and classification tasks. This paper introduces \\emph{VRCP (Verifiably Robust Conformal Prediction)}, a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks. Our VRCP method is the first to support perturbations bounded by arbitrary norms including $\\ell^1$, $\\ell^2$, and $\\ell^\\infty$, as well as regression tasks. We evaluate and compare our approach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet) and regression tasks for deep reinforcement learning environments. In every case, VRCP achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the SotA.","sentences":["Conformal Prediction (CP) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable.","In such a case, CP's prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability.","Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage.","Recently, several approaches have been put forward to recover CP guarantees in this setting.","These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations.","They are, however, limited in that they only support $\\ell^2$-bounded perturbations and classification tasks.","This paper introduces \\emph{VRCP (Verifiably Robust Conformal Prediction)}, a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks.","Our VRCP method is the first to support perturbations bounded by arbitrary norms including $\\ell^1$, $\\ell^2$, and $\\ell^\\infty$, as well as regression tasks.","We evaluate and compare our approach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet) and regression tasks for deep reinforcement learning environments.","In every case, VRCP achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the SotA."],"url":"http://arxiv.org/abs/2405.18942v1","category":"cs.LO"}
{"created":"2024-05-29 09:50:39","title":"Content-Agnostic Moderation for Stance-Neutral Recommendation","abstract":"Personalized recommendation systems often drive users towards more extreme content, exacerbating opinion polarization. While (content-aware) moderation has been proposed to mitigate these effects, such approaches risk curtailing the freedom of speech and of information. To address this concern, we propose and explore the feasibility of \\emph{content-agnostic} moderation as an alternative approach for reducing polarization. Content-agnostic moderation does not rely on the actual content being moderated, arguably making it less prone to forms of censorship. We establish theoretically that content-agnostic moderation cannot be guaranteed to work in a fully generic setting. However, we show that it can often be effectively achieved in practice with plausible assumptions. We introduce two novel content-agnostic moderation methods that modify the recommendations from the content recommender to disperse user-item co-clusters without relying on content features.   To evaluate the potential of content-agnostic moderation in controlled experiments, we built a simulation environment to analyze the closed-loop behavior of a system with a given set of users, recommendation system, and moderation approach. Through comprehensive experiments in this environment, we show that our proposed moderation methods significantly enhance stance neutrality and maintain high recommendation quality across various data scenarios. Our results indicate that achieving stance neutrality without direct content information is not only feasible but can also help in developing more balanced and informative recommendation systems without substantially degrading user engagement.","sentences":["Personalized recommendation systems often drive users towards more extreme content, exacerbating opinion polarization.","While (content-aware) moderation has been proposed to mitigate these effects, such approaches risk curtailing the freedom of speech and of information.","To address this concern, we propose and explore the feasibility of \\emph{content-agnostic} moderation as an alternative approach for reducing polarization.","Content-agnostic moderation does not rely on the actual content being moderated, arguably making it less prone to forms of censorship.","We establish theoretically that content-agnostic moderation cannot be guaranteed to work in a fully generic setting.","However, we show that it can often be effectively achieved in practice with plausible assumptions.","We introduce two novel content-agnostic moderation methods that modify the recommendations from the content recommender to disperse user-item co-clusters without relying on content features.   ","To evaluate the potential of content-agnostic moderation in controlled experiments, we built a simulation environment to analyze the closed-loop behavior of a system with a given set of users, recommendation system, and moderation approach.","Through comprehensive experiments in this environment, we show that our proposed moderation methods significantly enhance stance neutrality and maintain high recommendation quality across various data scenarios.","Our results indicate that achieving stance neutrality without direct content information is not only feasible but can also help in developing more balanced and informative recommendation systems without substantially degrading user engagement."],"url":"http://arxiv.org/abs/2405.18941v1","category":"cs.IR"}
{"created":"2024-05-29 09:49:25","title":"Brenke polynomials with real zeros and the Riemann Hypothesis","abstract":"If $A(z)=\\sum_{n=0}^\\infty a_nz^n$ and $B(z)=\\sum_{n=0}^\\infty b_nz^n$ are two formal power series, with $a_n,b_n\\in \\mathbb{R}$, the polynomials $(p_n)_n$ defined by the generating function $$ A(z)B(xz)=\\sum_{n=0}^\\infty p_n(x)z^n $$ are called the Brenke polynomials generated by $A$ and associated to $B$. We say that $A\\in \\mathcal{R}_B$ if the Brenke polynomials $(p_n)_n$ have only real zeros. Among other results, in this paper we find necessary and sufficient conditions on $B$ such that $\\mathcal{R}_B=\\mathcal{L}\\text{-}\\mathcal{P}$, where $\\mathcal{L}\\text{-}\\mathcal{P}$ denotes the Laguerre-P\\'olya class (of entire functions). These results can be considered an extension to Brenke polynomials of the Jensen, and P\\'olya and Schur characterization $\\mathcal{R}_{e^z}=\\mathcal{L}\\text{-}\\mathcal{P}$, for Appell polynomials. When applying our results to a relative of the Riemann zeta function, we find new equivalencies for the Riemann Hypothesis in terms of real-rootedness of some sequences of Brenke polynomials.","sentences":["If $A(z)=\\sum_{n=0}^\\infty a_nz^n$ and $B(z)=\\sum_{n=0}^\\infty b_nz^n$ are two formal power series, with $a_n,b_n\\in \\mathbb{R}$, the polynomials $(p_n)_n$ defined by the generating function $$ A(z)B(xz)=\\sum_{n=0}^\\infty p_n(x)z^n $$ are called the Brenke polynomials generated by $A$ and associated to $B$. We say that $A\\in \\mathcal{R}_B$ if the Brenke polynomials $(p_n)_n$ have only real zeros.","Among other results, in this paper we find necessary and sufficient conditions on $B$ such that $\\mathcal{R}_B=\\mathcal{L}\\text{-}\\mathcal{P}$, where $\\mathcal{L}\\text{-}\\mathcal{P}$ denotes the Laguerre-P\\'olya class (of entire functions).","These results can be considered an extension to Brenke polynomials of the Jensen, and P\\'olya and Schur characterization $\\mathcal{R}_{e^z}=\\mathcal{L}\\text{-}\\mathcal{P}$, for Appell polynomials.","When applying our results to a relative of the Riemann zeta function, we find new equivalencies for the Riemann Hypothesis in terms of real-rootedness of some sequences of Brenke polynomials."],"url":"http://arxiv.org/abs/2405.18940v1","category":"math.CA"}
{"created":"2024-05-29 09:43:48","title":"Kestrel: Point Grounding Multimodal LLM for Part-Aware 3D Vision-Language Understanding","abstract":"While 3D MLLMs have achieved significant progress, they are restricted to object and scene understanding and struggle to understand 3D spatial structures at the part level. In this paper, we introduce Kestrel, representing a novel approach that empowers 3D MLLMs with part-aware understanding, enabling better interpretation and segmentation grounding of 3D objects at the part level. Despite its significance, the current landscape lacks tasks and datasets that endow and assess this capability. Therefore, we propose two novel tasks: (1) Part-Aware Point Grounding, the model is tasked with directly predicting a part-level segmentation mask based on user instructions, and (2) Part-Aware Point Grounded Captioning, the model provides a detailed caption that includes part-level descriptions and their corresponding masks. To support learning and evaluating for these tasks, we introduce 3DCoMPaT Grounded Instructions Dataset (3DCoMPaT-GRIN). 3DCoMPaT-GRIN Vanilla, comprising 789k part-aware point cloud-instruction-segmentation mask triplets, is used to evaluate MLLMs' ability of part-aware segmentation grounding. 3DCoMPaT-GRIN Grounded Caption, containing 107k part-aware point cloud-instruction-grounded caption triplets, assesses both MLLMs' part-aware language comprehension and segmentation grounding capabilities. Our introduced tasks, dataset, and Kestrel represent a preliminary effort to bridge the gap between human cognition and 3D MLLMs, i.e., the ability to perceive and engage with the environment at both global and part levels. Extensive experiments on the 3DCoMPaT-GRIN show that Kestrel can generate user-specified segmentation masks, a capability not present in any existing 3D MLLM. Kestrel thus established a benchmark for evaluating the part-aware language comprehension and segmentation grounding of 3D objects. Project page at https://feielysia.github.io/Kestrel.github.io/","sentences":["While 3D MLLMs have achieved significant progress, they are restricted to object and scene understanding and struggle to understand 3D spatial structures at the part level.","In this paper, we introduce Kestrel, representing a novel approach that empowers 3D MLLMs with part-aware understanding, enabling better interpretation and segmentation grounding of 3D objects at the part level.","Despite its significance, the current landscape lacks tasks and datasets that endow and assess this capability.","Therefore, we propose two novel tasks: (1) Part-Aware Point Grounding, the model is tasked with directly predicting a part-level segmentation mask based on user instructions, and (2) Part-Aware Point Grounded Captioning, the model provides a detailed caption that includes part-level descriptions and their corresponding masks.","To support learning and evaluating for these tasks, we introduce 3DCoMPaT Grounded Instructions Dataset (3DCoMPaT-GRIN).","3DCoMPaT-GRIN Vanilla, comprising 789k part-aware point cloud-instruction-segmentation mask triplets, is used to evaluate MLLMs' ability of part-aware segmentation grounding.","3DCoMPaT-GRIN Grounded Caption, containing 107k part-aware point cloud-instruction-grounded caption triplets, assesses both MLLMs' part-aware language comprehension and segmentation grounding capabilities.","Our introduced tasks, dataset, and Kestrel represent a preliminary effort to bridge the gap between human cognition and 3D MLLMs, i.e., the ability to perceive and engage with the environment at both global and part levels.","Extensive experiments on the 3DCoMPaT-GRIN show that Kestrel can generate user-specified segmentation masks, a capability not present in any existing 3D MLLM.","Kestrel thus established a benchmark for evaluating the part-aware language comprehension and segmentation grounding of 3D objects.","Project page at https://feielysia.github.io/Kestrel.github.io/"],"url":"http://arxiv.org/abs/2405.18937v1","category":"cs.CV"}
{"created":"2024-05-29 09:38:52","title":"$K$-$g$-frames in Hilbert module over locally-$C^*$-algebras","abstract":"This paper explores the concept of $K$-$g$-frames in locally $C^*$-algebras, which are shown to be more general than $g$-frames. The authors first introduce the notion of a $g$-orthonormal basis and utilize it to define the $g$-operator, a crucial element for studying the construction of $K$-$g$-frames in locally $C^*$-algebras. The paper establishes a relationship between $g$-frames and $K$-$g$-frames and introduces the $K$-dual $g$-frame along with its properties. Finally, the authors characterize $K$-$g$-frames through two other related concepts.","sentences":["This paper explores the concept of $K$-$g$-frames in locally $C^*$-algebras, which are shown to be more general than $g$-frames.","The authors first introduce the notion of a $g$-orthonormal basis and utilize it to define the $g$-operator, a crucial element for studying the construction of $K$-$g$-frames in locally $C^*$-algebras.","The paper establishes a relationship between $g$-frames and $K$-$g$-frames and introduces the $K$-dual $g$-frame along with its properties.","Finally, the authors characterize $K$-$g$-frames through two other related concepts."],"url":"http://arxiv.org/abs/2405.18935v1","category":"math.OA"}
{"created":"2024-05-29 09:38:04","title":"Towards a computationally-efficient follow-up pipeline for blind continuous gravitational-wave searches","abstract":"The sensitivity of continuous gravitational-wave (CW) searches for unknown neutron stars (NSs) is limited by their parameter space breadth. To fit within reasonable computing budgets, hierarchical schemes are used to identify interesting candidates using affordable methods. The resulting sensitivity critically depends on the number of candidates selected to follow-up. In this work, we present a novel framework to evaluate the effectiveness of stochastic CW follow-ups. Our results in realistic datasets allow for a ten-fold reduction of the computing cost of pyfstat, a well-established follow-up method. We also simplify the setup of multi-stage follow-ups by removing the need for parameter-space metrics. The study was conducted on Gaussian and real O3 Advanced LIGO data using realistic durations, and covered both isolated and binary sources. These results will have a positive impact on the sensitivity of all-sky searches in the forthcoming observing runs of the LIGO-Virgo-KAGRA collaboration.","sentences":["The sensitivity of continuous gravitational-wave (CW) searches for unknown neutron stars (NSs) is limited by their parameter space breadth.","To fit within reasonable computing budgets, hierarchical schemes are used to identify interesting candidates using affordable methods.","The resulting sensitivity critically depends on the number of candidates selected to follow-up.","In this work, we present a novel framework to evaluate the effectiveness of stochastic CW follow-ups.","Our results in realistic datasets allow for a ten-fold reduction of the computing cost of pyfstat, a well-established follow-up method.","We also simplify the setup of multi-stage follow-ups by removing the need for parameter-space metrics.","The study was conducted on Gaussian and real O3 Advanced LIGO data using realistic durations, and covered both isolated and binary sources.","These results will have a positive impact on the sensitivity of all-sky searches in the forthcoming observing runs of the LIGO-Virgo-KAGRA collaboration."],"url":"http://arxiv.org/abs/2405.18934v1","category":"gr-qc"}
{"created":"2024-05-29 09:37:23","title":"LSPI: Heterogeneous Graph Neural Network Classification Aggregation Algorithm Based on Size Neighbor Path Identification","abstract":"Existing heterogeneous graph neural network algorithms (HGNNs) mostly rely on meta-paths to capture the rich semantic information contained in heterogeneous graphs (also known as heterogeneous information networks (HINs)), but most of these HGNNs focus on different ways of feature aggre gation and ignore the properties of the meta-paths themselves. This paper studies meta-paths in three commonly used data sets and finds that there are huge differences in the number of neighbors connected by different meta paths. At the same time, the noise information contained in large neigh bor paths will have an adverse impact on model performance. Therefore, this paper proposes a Heterogeneous Graph Neural Network Classification and Aggregation Algorithm Based on Large and Small Neighbor Path Iden tification(LSPI). LSPI firstly divides the meta-paths into large and small neighbor paths through the path discriminator , and in order to reduce the noise interference problem in large neighbor paths, LSPI selects neighbor nodes with higher similarity from both topology and feature perspectives, and passes small neighbor paths and filtered large neighbor paths through different graph convolution components. Aggregation is performed to obtain feature information under different subgraphs, and then LSPI uses subgraph level attention to fuse the feature information under different subgraphs to generate the final node embedding. Finally this paper verifies the superiority of the method through extensive experiments and also gives suggestions on the number of nodes to be retained in large neighbor paths through exper iments. The complete reproducible code adn data has been published at: https://github.com/liuhua811/LSPIA.","sentences":["Existing heterogeneous graph neural network algorithms (HGNNs) mostly rely on meta-paths to capture the rich semantic information contained in heterogeneous graphs (also known as heterogeneous information networks (HINs)), but most of these HGNNs focus on different ways of feature aggre gation and ignore the properties of the meta-paths themselves.","This paper studies meta-paths in three commonly used data sets and finds that there are huge differences in the number of neighbors connected by different meta paths.","At the same time, the noise information contained in large neigh bor paths will have an adverse impact on model performance.","Therefore, this paper proposes a Heterogeneous Graph Neural Network Classification and Aggregation Algorithm Based on Large and Small Neighbor Path Iden tification(LSPI).","LSPI firstly divides the meta-paths into large and small neighbor paths through the path discriminator , and in order to reduce the noise interference problem in large neighbor paths, LSPI selects neighbor nodes with higher similarity from both topology and feature perspectives, and passes small neighbor paths and filtered large neighbor paths through different graph convolution components.","Aggregation is performed to obtain feature information under different subgraphs, and then LSPI uses subgraph level attention to fuse the feature information under different subgraphs to generate the final node embedding.","Finally this paper verifies the superiority of the method through extensive experiments and also gives suggestions on the number of nodes to be retained in large neighbor paths through exper iments.","The complete reproducible code adn data has been published at: https://github.com/liuhua811/LSPIA."],"url":"http://arxiv.org/abs/2405.18933v1","category":"cs.LG"}
{"created":"2024-05-29 09:36:20","title":"EntProp: High Entropy Propagation for Improving Accuracy and Robustness","abstract":"Deep neural networks (DNNs) struggle to generalize to out-of-distribution domains that are different from those in training despite their impressive performance. In practical applications, it is important for DNNs to have both high standard accuracy and robustness against out-of-distribution domains. One technique that achieves both of these improvements is disentangled learning with mixture distribution via auxiliary batch normalization layers (ABNs). This technique treats clean and transformed samples as different domains, allowing a DNN to learn better features from mixed domains. However, if we distinguish the domains of the samples based on entropy, we find that some transformed samples are drawn from the same domain as clean samples, and these samples are not completely different domains. To generate samples drawn from a completely different domain than clean samples, we hypothesize that transforming clean high-entropy samples to further increase the entropy generates out-of-distribution samples that are much further away from the in-distribution domain. On the basis of the hypothesis, we propose high entropy propagation~(EntProp), which feeds high-entropy samples to the network that uses ABNs. We introduce two techniques, data augmentation and free adversarial training, that increase entropy and bring the sample further away from the in-distribution domain. These techniques do not require additional training costs. Our experimental results show that EntProp achieves higher standard accuracy and robustness with a lower training cost than the baseline methods. In particular, EntProp is highly effective at training on small datasets.","sentences":["Deep neural networks (DNNs) struggle to generalize to out-of-distribution domains that are different from those in training despite their impressive performance.","In practical applications, it is important for DNNs to have both high standard accuracy and robustness against out-of-distribution domains.","One technique that achieves both of these improvements is disentangled learning with mixture distribution via auxiliary batch normalization layers (ABNs).","This technique treats clean and transformed samples as different domains, allowing a DNN to learn better features from mixed domains.","However, if we distinguish the domains of the samples based on entropy, we find that some transformed samples are drawn from the same domain as clean samples, and these samples are not completely different domains.","To generate samples drawn from a completely different domain than clean samples, we hypothesize that transforming clean high-entropy samples to further increase the entropy generates out-of-distribution samples that are much further away from the in-distribution domain.","On the basis of the hypothesis, we propose high entropy propagation~(EntProp), which feeds high-entropy samples to the network that uses ABNs.","We introduce two techniques, data augmentation and free adversarial training, that increase entropy and bring the sample further away from the in-distribution domain.","These techniques do not require additional training costs.","Our experimental results show that EntProp achieves higher standard accuracy and robustness with a lower training cost than the baseline methods.","In particular, EntProp is highly effective at training on small datasets."],"url":"http://arxiv.org/abs/2405.18931v1","category":"stat.ML"}
{"created":"2024-05-29 09:34:49","title":"A short review on joint weak and strong cluster lens-mass reconstruction","abstract":"The divide between weak and strong lensing is of course artificial, in that both regimes are manifestations of the same physical phenomenon: gravity bending the path of light. Nevertheless, these two regimes have to a large extent been treated separately, with different methods. This review traces the development of methods combining weak-lensing and strong-lensing data for joint lens-mass reconstruction, with a particular emphasis on cluster lenses, where both effects occur. We conclude that so-called inverse methods have been successful in merging the two regimes insofar data analysis is concerned. However, a number of improvements seem to be needed. First, not many studies include weak lensing data beyond shear. In light of the unprecedented quality of the data of JWST and future surveys, this is a clear point of improvement. Especially so, since flexion terms have proven useful in determining sub-structures. Second, considering the amount of data available, and the complexity of non-parametric lenses, automating the processes of lens-mass reconstruction would be beneficial. Towards this end, invoking machine learning seems like a promising way forward. The silence of the literature on this latter point is in fact somewhat surprising.","sentences":["The divide between weak and strong lensing is of course artificial, in that both regimes are manifestations of the same physical phenomenon: gravity bending the path of light.","Nevertheless, these two regimes have to a large extent been treated separately, with different methods.","This review traces the development of methods combining weak-lensing and strong-lensing data for joint lens-mass reconstruction, with a particular emphasis on cluster lenses, where both effects occur.","We conclude that so-called inverse methods have been successful in merging the two regimes insofar data analysis is concerned.","However, a number of improvements seem to be needed.","First, not many studies include weak lensing data beyond shear.","In light of the unprecedented quality of the data of JWST and future surveys, this is a clear point of improvement.","Especially so, since flexion terms have proven useful in determining sub-structures.","Second, considering the amount of data available, and the complexity of non-parametric lenses, automating the processes of lens-mass reconstruction would be beneficial.","Towards this end, invoking machine learning seems like a promising way forward.","The silence of the literature on this latter point is in fact somewhat surprising."],"url":"http://arxiv.org/abs/2405.18930v1","category":"astro-ph.CO"}
{"created":"2024-05-29 09:34:47","title":"Deep Positive-Unlabeled Anomaly Detection for Contaminated Unlabeled Data","abstract":"Semi-supervised anomaly detection, which aims to improve the performance of the anomaly detector by using a small amount of anomaly data in addition to unlabeled data, has attracted attention. Existing semi-supervised approaches assume that unlabeled data are mostly normal. They train the anomaly detector to minimize the anomaly scores for the unlabeled data, and to maximize those for the anomaly data. However, in practice, the unlabeled data are often contaminated with anomalies. This weakens the effect of maximizing the anomaly scores for anomalies, and prevents us from improving the detection performance. To solve this problem, we propose the positive-unlabeled autoencoder, which is based on positive-unlabeled learning and the anomaly detector such as the autoencoder. With our approach, we can approximate the anomaly scores for normal data using the unlabeled and anomaly data. Therefore, without the labeled normal data, we can train the anomaly detector to minimize the anomaly scores for normal data, and to maximize those for the anomaly data. In addition, our approach is applicable to various anomaly detectors such as the DeepSVDD. Experiments on various datasets show that our approach achieves better detection performance than existing approaches.","sentences":["Semi-supervised anomaly detection, which aims to improve the performance of the anomaly detector by using a small amount of anomaly data in addition to unlabeled data, has attracted attention.","Existing semi-supervised approaches assume that unlabeled data are mostly normal.","They train the anomaly detector to minimize the anomaly scores for the unlabeled data, and to maximize those for the anomaly data.","However, in practice, the unlabeled data are often contaminated with anomalies.","This weakens the effect of maximizing the anomaly scores for anomalies, and prevents us from improving the detection performance.","To solve this problem, we propose the positive-unlabeled autoencoder, which is based on positive-unlabeled learning and the anomaly detector such as the autoencoder.","With our approach, we can approximate the anomaly scores for normal data using the unlabeled and anomaly data.","Therefore, without the labeled normal data, we can train the anomaly detector to minimize the anomaly scores for normal data, and to maximize those for the anomaly data.","In addition, our approach is applicable to various anomaly detectors such as the DeepSVDD.","Experiments on various datasets show that our approach achieves better detection performance than existing approaches."],"url":"http://arxiv.org/abs/2405.18929v1","category":"stat.ML"}
{"created":"2024-05-29 09:29:39","title":"Federated Continual Learning Goes Online: Leveraging Uncertainty for Modality-Agnostic Class-Incremental Learning","abstract":"Given the ability to model more realistic and dynamic problems, Federated Continual Learning (FCL) has been increasingly investigated recently. A well-known problem encountered in this setting is the so-called catastrophic forgetting, for which the learning model is inclined to focus on more recent tasks while forgetting the previously learned knowledge. The majority of the current approaches in FCL propose generative-based solutions to solve said problem. However, this setting requires multiple training epochs over the data, implying an offline setting where datasets are stored locally and remain unchanged over time. Furthermore, the proposed solutions are tailored for vision tasks solely. To overcome these limitations, we propose a new modality-agnostic approach to deal with the online scenario where new data arrive in streams of mini-batches that can only be processed once. To solve catastrophic forgetting, we propose an uncertainty-aware memory-based approach. In particular, we suggest using an estimator based on the Bregman Information (BI) to compute the model's variance at the sample level. Through measures of predictive uncertainty, we retrieve samples with specific characteristics, and - by retraining the model on such samples - we demonstrate the potential of this approach to reduce the forgetting effect in realistic settings.","sentences":["Given the ability to model more realistic and dynamic problems, Federated Continual Learning (FCL) has been increasingly investigated recently.","A well-known problem encountered in this setting is the so-called catastrophic forgetting, for which the learning model is inclined to focus on more recent tasks while forgetting the previously learned knowledge.","The majority of the current approaches in FCL propose generative-based solutions to solve said problem.","However, this setting requires multiple training epochs over the data, implying an offline setting where datasets are stored locally and remain unchanged over time.","Furthermore, the proposed solutions are tailored for vision tasks solely.","To overcome these limitations, we propose a new modality-agnostic approach to deal with the online scenario where new data arrive in streams of mini-batches that can only be processed once.","To solve catastrophic forgetting, we propose an uncertainty-aware memory-based approach.","In particular, we suggest using an estimator based on the Bregman Information (BI) to compute the model's variance at the sample level.","Through measures of predictive uncertainty, we retrieve samples with specific characteristics, and - by retraining the model on such samples - we demonstrate the potential of this approach to reduce the forgetting effect in realistic settings."],"url":"http://arxiv.org/abs/2405.18925v1","category":"cs.LG"}
{"created":"2024-05-29 09:24:25","title":"GLANCE: Global Actions in a Nutshell for Counterfactual Explainability","abstract":"Counterfactual explanations have emerged as an important tool to understand, debug, and audit complex machine learning models. To offer global counterfactual explainability, state-of-the-art methods construct summaries of local explanations, offering a trade-off among conciseness, counterfactual effectiveness, and counterfactual cost or burden imposed on instances. In this work, we provide a concise formulation of the problem of identifying global counterfactuals and establish principled criteria for comparing solutions, drawing inspiration from Pareto dominance. We introduce innovative algorithms designed to address the challenge of finding global counterfactuals for either the entire input space or specific partitions, employing clustering and decision trees as key components. Additionally, we conduct a comprehensive experimental evaluation, considering various instances of the problem and comparing our proposed algorithms with state-of-the-art methods. The results highlight the consistent capability of our algorithms to generate meaningful and interpretable global counterfactual explanations.","sentences":["Counterfactual explanations have emerged as an important tool to understand, debug, and audit complex machine learning models.","To offer global counterfactual explainability, state-of-the-art methods construct summaries of local explanations, offering a trade-off among conciseness, counterfactual effectiveness, and counterfactual cost or burden imposed on instances.","In this work, we provide a concise formulation of the problem of identifying global counterfactuals and establish principled criteria for comparing solutions, drawing inspiration from Pareto dominance.","We introduce innovative algorithms designed to address the challenge of finding global counterfactuals for either the entire input space or specific partitions, employing clustering and decision trees as key components.","Additionally, we conduct a comprehensive experimental evaluation, considering various instances of the problem and comparing our proposed algorithms with state-of-the-art methods.","The results highlight the consistent capability of our algorithms to generate meaningful and interpretable global counterfactual explanations."],"url":"http://arxiv.org/abs/2405.18921v1","category":"cs.LG"}
{"created":"2024-05-29 09:24:19","title":"Achievable Rate Optimization for Large Stacked Intelligent Metasurfaces Based on Statistical CSI","abstract":"Stacked intelligent metasurface (SIM) is an emerging design that consists of multiple layers of metasurfaces. A SIM enables holographic multiple-input multiple-output (HMIMO) precoding in the wave domain, which results in the reduction of energy consumption and hardware cost. On the ground of multiuser beamforming, this letter focuses on the downlink achievable rate and its maximization. Contrary to previous works on multiuser SIM, we consider statistical channel state information (CSI) as opposed to instantaneous CSI to overcome challenges such as large overhead. Also, we examine the performance of large surfaces. We apply an alternating optimization (AO) algorithm regarding the phases of the SIM and the allocated transmit power. Simulations illustrate the performance of the considered large SIM-assisted design as well as the comparison between different CSI considerations.","sentences":["Stacked intelligent metasurface (SIM) is an emerging design that consists of multiple layers of metasurfaces.","A SIM enables holographic multiple-input multiple-output (HMIMO) precoding in the wave domain, which results in the reduction of energy consumption and hardware cost.","On the ground of multiuser beamforming, this letter focuses on the downlink achievable rate and its maximization.","Contrary to previous works on multiuser SIM, we consider statistical channel state information (CSI) as opposed to instantaneous CSI to overcome challenges such as large overhead.","Also, we examine the performance of large surfaces.","We apply an alternating optimization (AO) algorithm regarding the phases of the SIM and the allocated transmit power.","Simulations illustrate the performance of the considered large SIM-assisted design as well as the comparison between different CSI considerations."],"url":"http://arxiv.org/abs/2405.18920v1","category":"cs.IT"}
{"created":"2024-05-29 09:19:50","title":"Causal Action Influence Aware Counterfactual Data Augmentation","abstract":"Offline data are both valuable and practical resources for teaching robots complex behaviors. Ideally, learning agents should not be constrained by the scarcity of available demonstrations, but rather generalize beyond the training distribution. However, the complexity of real-world scenarios typically requires huge amounts of data to prevent neural network policies from picking up on spurious correlations and learning non-causal relationships. We propose CAIAC, a data augmentation method that can create feasible synthetic transitions from a fixed dataset without having access to online environment interactions. By utilizing principled methods for quantifying causal influence, we are able to perform counterfactual reasoning by swapping $\\it{action}$-unaffected parts of the state-space between independent trajectories in the dataset. We empirically show that this leads to a substantial increase in robustness of offline learning algorithms against distributional shift.","sentences":["Offline data are both valuable and practical resources for teaching robots complex behaviors.","Ideally, learning agents should not be constrained by the scarcity of available demonstrations, but rather generalize beyond the training distribution.","However, the complexity of real-world scenarios typically requires huge amounts of data to prevent neural network policies from picking up on spurious correlations and learning non-causal relationships.","We propose CAIAC, a data augmentation method that can create feasible synthetic transitions from a fixed dataset without having access to online environment interactions.","By utilizing principled methods for quantifying causal influence, we are able to perform counterfactual reasoning by swapping $\\it{action}$-unaffected parts of the state-space between independent trajectories in the dataset.","We empirically show that this leads to a substantial increase in robustness of offline learning algorithms against distributional shift."],"url":"http://arxiv.org/abs/2405.18917v1","category":"cs.LG"}
{"created":"2024-05-29 09:17:46","title":"Towards Faithful Chain-of-Thought: Large Language Models are Bridging Reasoners","abstract":"Large language models (LLMs) suffer from serious unfaithful chain-of-thought (CoT) issues. Previous work attempts to measure and explain it but lacks in-depth analysis within CoTs and does not consider the interactions among all reasoning components jointly. In this paper, we first study the CoT faithfulness issue at the granularity of CoT steps, identify two reasoning paradigms: centralized reasoning and distributed reasoning, and find their relationship with faithfulness. Subsequently, we conduct a joint analysis of the causal relevance among the context, CoT, and answer during reasoning. The result proves that, when the LLM predicts answers, it can recall correct information missing in the CoT from the context, leading to unfaithfulness issues. Finally, we propose the inferential bridging method to mitigate this issue, in which we use the attribution method to recall information as hints for CoT generation and filter out noisy CoTs based on their semantic consistency and attribution scores. Extensive experiments demonstrate that our approach effectively alleviates the unfaithful CoT problem.","sentences":["Large language models (LLMs) suffer from serious unfaithful chain-of-thought (CoT) issues.","Previous work attempts to measure and explain it but lacks in-depth analysis within CoTs and does not consider the interactions among all reasoning components jointly.","In this paper, we first study the CoT faithfulness issue at the granularity of CoT steps, identify two reasoning paradigms: centralized reasoning and distributed reasoning, and find their relationship with faithfulness.","Subsequently, we conduct a joint analysis of the causal relevance among the context, CoT, and answer during reasoning.","The result proves that, when the LLM predicts answers, it can recall correct information missing in the CoT from the context, leading to unfaithfulness issues.","Finally, we propose the inferential bridging method to mitigate this issue, in which we use the attribution method to recall information as hints for CoT generation and filter out noisy CoTs based on their semantic consistency and attribution scores.","Extensive experiments demonstrate that our approach effectively alleviates the unfaithful CoT problem."],"url":"http://arxiv.org/abs/2405.18915v1","category":"cs.CL"}
{"created":"2024-05-29 09:11:51","title":"Predicting Parking Availability in Singapore with Cross-Domain Data: A New Dataset and A Data-Driven Approach","abstract":"The increasing number of vehicles highlights the need for efficient parking space management. Predicting real-time Parking Availability (PA) can help mitigate traffic congestion and the corresponding social problems, which is a pressing issue in densely populated cities like Singapore. In this study, we aim to collectively predict future PA across Singapore with complex factors from various domains. The contributions in this paper are listed as follows: (1) A New Dataset: We introduce the \\texttt{SINPA} dataset, containing a year's worth of PA data from 1,687 parking lots in Singapore, enriched with various spatial and temporal factors. (2) A Data-Driven Approach: We present DeepPA, a novel deep-learning framework, to collectively and efficiently predict future PA across thousands of parking lots. (3) Extensive Experiments and Deployment: DeepPA demonstrates a 9.2% reduction in prediction error for up to 3-hour forecasts compared to existing advanced models. Furthermore, we implement DeepPA in a practical web-based platform to provide real-time PA predictions to aid drivers and inform urban planning for the governors in Singapore. We release the dataset and source code at https://github.com/yoshall/SINPA.","sentences":["The increasing number of vehicles highlights the need for efficient parking space management.","Predicting real-time Parking Availability (PA) can help mitigate traffic congestion and the corresponding social problems, which is a pressing issue in densely populated cities like Singapore.","In this study, we aim to collectively predict future PA across Singapore with complex factors from various domains.","The contributions in this paper are listed as follows: (1) A New Dataset: We introduce the \\texttt{SINPA} dataset, containing a year's worth of PA data from 1,687 parking lots in Singapore, enriched with various spatial and temporal factors.","(2) A Data-Driven Approach: We present DeepPA, a novel deep-learning framework, to collectively and efficiently predict future PA across thousands of parking lots.","(3) Extensive Experiments and Deployment: DeepPA demonstrates a 9.2% reduction in prediction error for up to 3-hour forecasts compared to existing advanced models.","Furthermore, we implement DeepPA in a practical web-based platform to provide real-time PA predictions to aid drivers and inform urban planning for the governors in Singapore.","We release the dataset and source code at https://github.com/yoshall/SINPA."],"url":"http://arxiv.org/abs/2405.18910v1","category":"cs.AI"}
{"created":"2024-05-29 09:11:31","title":"Stochastic Continuation of Trajectories in the Circular Restricted Three-Body Problem via Differential Algebra","abstract":"Numerical continuation techniques are powerful tools that have been extensively used to identify particular solutions of nonlinear dynamical systems and enable trajectory design in chaotic astrodynamics problems such as the Circular Restricted Three-Body Problem. However, the applicability of equilibrium points and periodic orbits may be questionable in real-world applications where the uncertainties of the initial conditions of the spacecraft and dynamical parameters of the problem (e.g., mass ratio parameter) are taken into consideration. Due to uncertain parameters and initial conditions, the spacecraft might not follow the reference periodic orbit owing to growing uncertainties that cause the satellite to deviate from its nominal path. Hence, it is crucial to keep track of the probability of finding the spacecraft in a given region. Building on previous work, we extend numerical continuation to moments of the distribution (i.e., stochastic continuation) by directly continuing moments of the probability density function of the spacecraft state. Only assuming normality of the initial conditions, and leveraging moment-generating functions, Isserlis' theorem, and the algebra of truncated polynomials, we propagate the distribution of the spacecraft state at consecutive surface of section crossings while retaining a symbolic map of the final moments of the distribution that depend on the initial mean and covariance matrix only. The goal of the work is to offer a differential algebra-based general framework to continue 3D periodic orbits in the presence of uncertain dynamical systems. The proposed approach is compared against traditional Monte Carlo simulations to validate the uncertainty propagation approach and demonstrate the advantages of the proposed in terms of uncertainty propagation computational burden and access to higher-dimensional problems.","sentences":["Numerical continuation techniques are powerful tools that have been extensively used to identify particular solutions of nonlinear dynamical systems and enable trajectory design in chaotic astrodynamics problems such as the Circular Restricted Three-Body Problem.","However, the applicability of equilibrium points and periodic orbits may be questionable in real-world applications where the uncertainties of the initial conditions of the spacecraft and dynamical parameters of the problem (e.g., mass ratio parameter) are taken into consideration.","Due to uncertain parameters and initial conditions, the spacecraft might not follow the reference periodic orbit owing to growing uncertainties that cause the satellite to deviate from its nominal path.","Hence, it is crucial to keep track of the probability of finding the spacecraft in a given region.","Building on previous work, we extend numerical continuation to moments of the distribution (i.e., stochastic continuation) by directly continuing moments of the probability density function of the spacecraft state.","Only assuming normality of the initial conditions, and leveraging moment-generating functions, Isserlis' theorem, and the algebra of truncated polynomials, we propagate the distribution of the spacecraft state at consecutive surface of section crossings while retaining a symbolic map of the final moments of the distribution that depend on the initial mean and covariance matrix only.","The goal of the work is to offer a differential algebra-based general framework to continue 3D periodic orbits in the presence of uncertain dynamical systems.","The proposed approach is compared against traditional Monte Carlo simulations to validate the uncertainty propagation approach and demonstrate the advantages of the proposed in terms of uncertainty propagation computational burden and access to higher-dimensional problems."],"url":"http://arxiv.org/abs/2405.18909v1","category":"physics.space-ph"}
{"created":"2024-05-29 09:09:00","title":"Language Generation with Strictly Proper Scoring Rules","abstract":"Language generation based on maximum likelihood estimation (MLE) has become the fundamental approach for text generation. Maximum likelihood estimation is typically performed by minimizing the log-likelihood loss, also known as the logarithmic score in statistical decision theory. The logarithmic score is strictly proper in the sense that it encourages honest forecasts, where the expected score is maximized only when the model reports true probabilities. Although many strictly proper scoring rules exist, the logarithmic score is the only local scoring rule among them that depends exclusively on the probability of the observed sample, making it capable of handling the exponentially large sample space of natural text. In this work, we propose a straightforward strategy for adapting scoring rules to language generation, allowing for language modeling with any non-local scoring rules. Leveraging this strategy, we train language generation models using two classic strictly proper scoring rules, the Brier score and the Spherical score, as alternatives to the logarithmic score. Experimental results indicate that simply substituting the loss function, without adjusting other hyperparameters, can yield substantial improvements in model's generation capabilities. Moreover, these improvements can scale up to large language models (LLMs) such as LLaMA-7B and LLaMA-13B. Source code: \\url{https://github.com/shaochenze/ScoringRulesLM}.","sentences":["Language generation based on maximum likelihood estimation (MLE) has become the fundamental approach for text generation.","Maximum likelihood estimation is typically performed by minimizing the log-likelihood loss, also known as the logarithmic score in statistical decision theory.","The logarithmic score is strictly proper in the sense that it encourages honest forecasts, where the expected score is maximized only when the model reports true probabilities.","Although many strictly proper scoring rules exist, the logarithmic score is the only local scoring rule among them that depends exclusively on the probability of the observed sample, making it capable of handling the exponentially large sample space of natural text.","In this work, we propose a straightforward strategy for adapting scoring rules to language generation, allowing for language modeling with any non-local scoring rules.","Leveraging this strategy, we train language generation models using two classic strictly proper scoring rules, the Brier score and the Spherical score, as alternatives to the logarithmic score.","Experimental results indicate that simply substituting the loss function, without adjusting other hyperparameters, can yield substantial improvements in model's generation capabilities.","Moreover, these improvements can scale up to large language models (LLMs) such as LLaMA-7B and LLaMA-13B. Source code: \\url{https://github.com/shaochenze/ScoringRulesLM}."],"url":"http://arxiv.org/abs/2405.18906v1","category":"cs.CL"}
{"created":"2024-05-29 09:08:44","title":"Room Temperature Ferroelectricity and Electrically Tunable Berry Curvature Dipole in III-V Monolayers","abstract":"Two-dimensional ferroelectric monolayers are promising candidates for compact memory devices and flexible electronics. Here, through first-principles calculations, we predict room temperature ferroelectricity in AB-type monolayers comprising group III (A = Al, In, Ga) and group V (B = As, P, Sb) elements. We show that their spontaneous polarization, oriented out-of-plane, ranges from 9.48 to 13.96 pC/m, outperforming most known 2D ferroelectric. We demonstrate electric field tunable Berry curvature dipole and nonlinear Hall current in these monolayers. Additionally, we highlight their applicability in next-generation memory devices by forming efficient ferroelectric tunnel junctions, especially in InP, which supports high tunneling electroresistance. Our findings motivate further exploration of these monolayers for studying the interplay between Berry curvature and ferroelectricity and for integrating these ferroelectric monolayers in next-generation electronic devices.","sentences":["Two-dimensional ferroelectric monolayers are promising candidates for compact memory devices and flexible electronics.","Here, through first-principles calculations, we predict room temperature ferroelectricity in AB-type monolayers comprising group III (A = Al, In, Ga) and group V (B = As, P, Sb) elements.","We show that their spontaneous polarization, oriented out-of-plane, ranges from 9.48 to 13.96 pC/m, outperforming most known 2D ferroelectric.","We demonstrate electric field tunable Berry curvature dipole and nonlinear Hall current in these monolayers.","Additionally, we highlight their applicability in next-generation memory devices by forming efficient ferroelectric tunnel junctions, especially in InP, which supports high tunneling electroresistance.","Our findings motivate further exploration of these monolayers for studying the interplay between Berry curvature and ferroelectricity and for integrating these ferroelectric monolayers in next-generation electronic devices."],"url":"http://arxiv.org/abs/2405.18905v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 09:05:22","title":"Breaking traditions: introducing a surrogate Primer Vector in non Keplerian dynamics","abstract":"In this study, we investigate trajectories involving multiple impulses within the framework of a generic spacecraft dynamics. Revisiting the age-old query of \"How many impulses?\", we present novel manipulations heavily leveraging on the properties ofthe state transition matrix. Surprisingly, we are able to rediscover classical results leading to the introduction of a primer vector, albeit not making use of Pontryagin Maximum Principle as in the original developments by Lawden. Furthermore, our mathematical framework exhibits great flexibility and enables the introduction of what we term a \"surrogate primer vector\" extending a well known concept widely used in mission design. This enhancement allows to derive new simple optimality conditions that provide insights into the possibility to add and/or move multiple impulsive manoeuvres and improve the overall mass budget. This proves especially valuable in scenarios where a baseline trajectory arc is, for example, limited to a single impulse-an instance where traditional primer vector developments become singular and hinder conclusive outcomes. In demonstrating the practical application of the surrogate primer vector, we examine a specific case involving the four-body dynamics of a spacecraft within an Earth-Moon-Sun system. The system is characterized by the high-precision and differentiable VSOP2013 and ELP2000 ephemerides models. The focal point of our investigation is a reference trajectory representing a return from Mars, utilizing the weak stability boundary (WSB) of the Sun-Earth-Moon system. The trajectory incorporates two consecutive lunar flybys to insert the spacecraft into a lunar distant retrograde orbit (DRO). Conventionally, this trajectory necessitates a single maneuver at the DRO injection point.","sentences":["In this study, we investigate trajectories involving multiple impulses within the framework of a generic spacecraft dynamics.","Revisiting the age-old query of \"How many impulses?\", we present novel manipulations heavily leveraging on the properties ofthe state transition matrix.","Surprisingly, we are able to rediscover classical results leading to the introduction of a primer vector, albeit not making use of Pontryagin Maximum Principle as in the original developments by Lawden.","Furthermore, our mathematical framework exhibits great flexibility and enables the introduction of what we term a \"surrogate primer vector\" extending a well known concept widely used in mission design.","This enhancement allows to derive new simple optimality conditions that provide insights into the possibility to add and/or move multiple impulsive manoeuvres and improve the overall mass budget.","This proves especially valuable in scenarios where a baseline trajectory arc is, for example, limited to a single impulse-an instance where traditional primer vector developments become singular and hinder conclusive outcomes.","In demonstrating the practical application of the surrogate primer vector, we examine a specific case involving the four-body dynamics of a spacecraft within an Earth-Moon-Sun system.","The system is characterized by the high-precision and differentiable VSOP2013 and ELP2000 ephemerides models.","The focal point of our investigation is a reference trajectory representing a return from Mars, utilizing the weak stability boundary (WSB) of the Sun-Earth-Moon system.","The trajectory incorporates two consecutive lunar flybys to insert the spacecraft into a lunar distant retrograde orbit (DRO).","Conventionally, this trajectory necessitates a single maneuver at the DRO injection point."],"url":"http://arxiv.org/abs/2405.18903v1","category":"physics.space-ph"}
{"created":"2024-05-29 09:03:44","title":"A Causal Framework for Evaluating Deferring Systems","abstract":"Deferring systems extend supervised Machine Learning (ML) models with the possibility to defer predictions to human experts. However, evaluating the impact of a deferring strategy on system accuracy is still an overlooked area. This paper fills this gap by evaluating deferring systems through a causal lens. We link the potential outcomes framework for causal inference with deferring systems. This allows us to identify the causal impact of the deferring strategy on predictive accuracy. We distinguish two scenarios. In the first one, we can access both the human and the ML model predictions for the deferred instances. In such a case, we can identify the individual causal effects for deferred instances and aggregates of them. In the second scenario, only human predictions are available for the deferred instances. In this case, we can resort to regression discontinuity design to estimate a local causal effect. We empirically evaluate our approach on synthetic and real datasets for seven deferring systems from the literature.","sentences":["Deferring systems extend supervised Machine Learning (ML) models with the possibility to defer predictions to human experts.","However, evaluating the impact of a deferring strategy on system accuracy is still an overlooked area.","This paper fills this gap by evaluating deferring systems through a causal lens.","We link the potential outcomes framework for causal inference with deferring systems.","This allows us to identify the causal impact of the deferring strategy on predictive accuracy.","We distinguish two scenarios.","In the first one, we can access both the human and the ML model predictions for the deferred instances.","In such a case, we can identify the individual causal effects for deferred instances and aggregates of them.","In the second scenario, only human predictions are available for the deferred instances.","In this case, we can resort to regression discontinuity design to estimate a local causal effect.","We empirically evaluate our approach on synthetic and real datasets for seven deferring systems from the literature."],"url":"http://arxiv.org/abs/2405.18902v1","category":"cs.LG"}
{"created":"2024-05-29 08:53:16","title":"Few-Shot Testing: Estimating Uncertainty of Memristive Deep Neural Networks Using One Bayesian Test Vector","abstract":"The performance of deep learning algorithms such as neural networks (NNs) has increased tremendously recently, and they can achieve state-of-the-art performance in many domains. However, due to memory and computation resource constraints, implementing NNs on edge devices is a challenging task. Therefore, hardware accelerators such as computation-in-memory (CIM) with memristive devices have been developed to accelerate the most common operations, i.e., matrix-vector multiplication. However, due to inherent device properties, external environmental factors such as temperature, and an immature fabrication process, memristors suffer from various non-idealities, including defects and variations occurring during manufacturing and runtime. Consequently, there is a lack of complete confidence in the predictions made by the model. To improve confidence in NN predictions made by hardware accelerators in the presence of device non-idealities, in this paper, we propose a Bayesian test vector generation framework that can estimate the model uncertainty of NNs implemented on memristor-based CIM hardware. Compared to the conventional point estimate test vector generation method, our method is more generalizable across different model dimensions and requires storing only one test Bayesian vector in the hardware. Our method is evaluated on different model dimensions, tasks, fault rates, and variation noise to show that it can consistently achieve $100\\%$ coverage with only $0.024$ MB of memory overhead.","sentences":["The performance of deep learning algorithms such as neural networks (NNs) has increased tremendously recently, and they can achieve state-of-the-art performance in many domains.","However, due to memory and computation resource constraints, implementing NNs on edge devices is a challenging task.","Therefore, hardware accelerators such as computation-in-memory (CIM) with memristive devices have been developed to accelerate the most common operations, i.e., matrix-vector multiplication.","However, due to inherent device properties, external environmental factors such as temperature, and an immature fabrication process, memristors suffer from various non-idealities, including defects and variations occurring during manufacturing and runtime.","Consequently, there is a lack of complete confidence in the predictions made by the model.","To improve confidence in NN predictions made by hardware accelerators in the presence of device non-idealities, in this paper, we propose a Bayesian test vector generation framework that can estimate the model uncertainty of NNs implemented on memristor-based CIM hardware.","Compared to the conventional point estimate test vector generation method, our method is more generalizable across different model dimensions and requires storing only one test Bayesian vector in the hardware.","Our method is evaluated on different model dimensions, tasks, fault rates, and variation noise to show that it can consistently achieve $100\\%$ coverage with only $0.024$ MB of memory overhead."],"url":"http://arxiv.org/abs/2405.18894v1","category":"cs.LG"}
{"created":"2024-05-29 08:53:10","title":"Physical States and Transition Amplitudes in Piecewise Flat Quantum Gravity","abstract":"We show how the path integral for gravity and matter on a piecewise flat spacetime can be used to define the physical quantum gravity states and the related transition amplitudes. The physical states are given by the path integrals for open manifolds from a certain topological class, while the corresponding transition amplitudes are obtained by gluing two such open manifolds into a closed one and calculating the corresponding path integral. We also solve the problem of how to associate a quantum gravity state to an effective action. This is done by using the path integral for a closed manifold from the transition-amplitude topological class so that the state which corresponds to the effective action can be choosen to be the Hartle-Hawking or the Vilenkin wavefunction for the vacuum manifold component of the transition-amplitude manifold.","sentences":["We show how the path integral for gravity and matter on a piecewise flat spacetime can be used to define the physical quantum gravity states and the related transition amplitudes.","The physical states are given by the path integrals for open manifolds from a certain topological class, while the corresponding transition amplitudes are obtained by gluing two such open manifolds into a closed one and calculating the corresponding path integral.","We also solve the problem of how to associate a quantum gravity state to an effective action.","This is done by using the path integral for a closed manifold from the transition-amplitude topological class so that the state which corresponds to the effective action can be choosen to be the Hartle-Hawking or the Vilenkin wavefunction for the vacuum manifold component of the transition-amplitude manifold."],"url":"http://arxiv.org/abs/2405.18893v1","category":"gr-qc"}
{"created":"2024-05-29 08:47:53","title":"Inverse Design of Promising Alloys for Electrocatalytic CO$_2$ Reduction via Generative Graph Neural Networks Combined with Bird Swarm Algorithm","abstract":"Directly generating material structures with optimal properties is a long-standing goal in material design. One of the fundamental challenges lies in how to overcome the limitation of traditional generative models to efficiently explore the global chemical space rather than a small localized space. Herein, we develop a framework named MAGECS to address this dilemma, by integrating the bird swarm algorithm and supervised graph neural network to effectively navigate the generative model in the immense chemical space towards materials with target properties. As a demonstration, MAGECS is applied to design compelling alloy electrocatalysts for CO$_2$ reduction reaction (CO$_2$RR) and works extremely well. Specifically, the chemical space of CO$_2$RR is effectively explored, where over 250,000 promising structures with high activity have been generated and notably, the proportion of desired structures is 2.5-fold increased. Moreover, five predicted alloys, i.e., CuAl, AlPd, Sn$_2$Pd$_5$, Sn$_9$Pd$_7$, and CuAlSe$_2$ are successfully synthesized and characterized experimentally, two of which exhibit about 90% Faraday efficiency of CO$_2$RR, and CuAl achieved 76% efficiency for C$_2$ products. This pioneering application of inverse design in CO$_2$RR catalysis showcases the potential of MAGECS to dramatically accelerate the development of functional materials, paving the way for fully automated, artificial intelligence-driven material design.","sentences":["Directly generating material structures with optimal properties is a long-standing goal in material design.","One of the fundamental challenges lies in how to overcome the limitation of traditional generative models to efficiently explore the global chemical space rather than a small localized space.","Herein, we develop a framework named MAGECS to address this dilemma, by integrating the bird swarm algorithm and supervised graph neural network to effectively navigate the generative model in the immense chemical space towards materials with target properties.","As a demonstration, MAGECS is applied to design compelling alloy electrocatalysts for CO$_2$ reduction reaction (CO$_2$RR) and works extremely well.","Specifically, the chemical space of CO$_2$RR is effectively explored, where over 250,000 promising structures with high activity have been generated and notably, the proportion of desired structures is 2.5-fold increased.","Moreover, five predicted alloys, i.e., CuAl, AlPd, Sn$_2$Pd$_5$, Sn$_9$Pd$_7$, and CuAlSe$_2$ are successfully synthesized and characterized experimentally, two of which exhibit about 90% Faraday efficiency of CO$_2$RR, and CuAl achieved 76% efficiency for C$_2$ products.","This pioneering application of inverse design in CO$_2$RR catalysis showcases the potential of MAGECS to dramatically accelerate the development of functional materials, paving the way for fully automated, artificial intelligence-driven material design."],"url":"http://arxiv.org/abs/2405.18891v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 08:46:00","title":"On Perception of Prevalence of Cheating and Usage of Generative AI","abstract":"This report investigates the perceptions of teaching staff on the prevalence of student cheating and the impact of Generative AI on academic integrity. Data was collected via an anonymous survey of teachers at the Department of Information Technology at Uppsala University and analyzed alongside institutional statistics on cheating investigations from 2004 to 2023. The results indicate that while teachers generally do not view cheating as highly prevalent, there is a strong belief that its incidence is increasing, potentially due to the accessibility of Generative AI. Most teachers do not equate AI usage with cheating but acknowledge its widespread use among students. Furthermore, teachers' perceptions align with objective data on cheating trends, highlighting their awareness of the evolving landscape of academic dishonesty.","sentences":["This report investigates the perceptions of teaching staff on the prevalence of student cheating and the impact of Generative AI on academic integrity.","Data was collected via an anonymous survey of teachers at the Department of Information Technology at Uppsala University and analyzed alongside institutional statistics on cheating investigations from 2004 to 2023.","The results indicate that while teachers generally do not view cheating as highly prevalent, there is a strong belief that its incidence is increasing, potentially due to the accessibility of Generative AI.","Most teachers do not equate AI usage with cheating but acknowledge its widespread use among students.","Furthermore, teachers' perceptions align with objective data on cheating trends, highlighting their awareness of the evolving landscape of academic dishonesty."],"url":"http://arxiv.org/abs/2405.18889v1","category":"cs.CY"}
{"created":"2024-05-29 08:45:04","title":"Proactive Load-Shaping Strategies with Privacy-Cost Trade-offs in Residential Households based on Deep Reinforcement Learning","abstract":"Smart meters play a crucial role in enhancing energy management and efficiency, but they raise significant privacy concerns by potentially revealing detailed user behaviors through energy consumption patterns. Recent scholarly efforts have focused on developing battery-aided load-shaping techniques to protect user privacy while balancing costs. This paper proposes a novel deep reinforcement learning-based load-shaping algorithm (PLS-DQN) designed to protect user privacy by proactively creating artificial load signatures that mislead potential attackers. We evaluate our proposed algorithm against a non-intrusive load monitoring (NILM) adversary. The results demonstrate that our approach not only effectively conceals real energy usage patterns but also outperforms state-of-the-art methods in enhancing user privacy while maintaining cost efficiency.","sentences":["Smart meters play a crucial role in enhancing energy management and efficiency, but they raise significant privacy concerns by potentially revealing detailed user behaviors through energy consumption patterns.","Recent scholarly efforts have focused on developing battery-aided load-shaping techniques to protect user privacy while balancing costs.","This paper proposes a novel deep reinforcement learning-based load-shaping algorithm (PLS-DQN) designed to protect user privacy by proactively creating artificial load signatures that mislead potential attackers.","We evaluate our proposed algorithm against a non-intrusive load monitoring (NILM) adversary.","The results demonstrate that our approach not only effectively conceals real energy usage patterns but also outperforms state-of-the-art methods in enhancing user privacy while maintaining cost efficiency."],"url":"http://arxiv.org/abs/2405.18888v1","category":"eess.SY"}
{"created":"2024-05-29 08:42:30","title":"Compressing Large Language Models using Low Rank and Low Precision Decomposition","abstract":"The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices. This work introduces $\\rm CALDERA$ -- a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix $\\mathbf{W}$ by approximating it via a low-rank, low-precision decomposition as $\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$. Here, $\\mathbf{L}$ and $\\mathbf{R}$ are low rank factors, and the entries of $\\mathbf{Q}$, $\\mathbf{L}$ and $\\mathbf{R}$ are quantized. The model is compressed by substituting each layer with its $\\mathbf{Q} + \\mathbf{L}\\mathbf{R}$ decomposition, and the zero-shot performance of the compressed model is evaluated. Additionally, $\\mathbf{L}$ and $\\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance. $\\rm CALDERA$ obtains this decomposition by formulating it as an optimization problem $\\min_{\\mathbf{Q},\\mathbf{L},\\mathbf{R}}\\lVert(\\mathbf{Q} + \\mathbf{L}\\mathbf{R} - \\mathbf{W})\\mathbf{X}^\\top\\rVert_{\\rm F}^2$, where $\\mathbf{X}$ is the calibration data, and $\\mathbf{Q}, \\mathbf{L}, \\mathbf{R}$ are constrained to be representable using low-precision formats. Theoretical upper bounds on the approximation error of $\\rm CALDERA$ are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget. Results illustrate that compressing LlaMa-$2$ $7$B/$70$B and LlaMa-$3$ $8$B models obtained using $\\rm CALDERA$ outperforms existing post-training LLM compression techniques in the regime of less than $2.5$ bits per parameter. The implementation is available at: \\href{https://github.com/pilancilab/caldera}{https://github.com/pilancilab/caldera}.","sentences":["The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices.","This work introduces $\\rm CALDERA$ -- a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix $\\mathbf{W}$ by approximating it via a low-rank, low-precision decomposition as $\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$. Here, $\\mathbf{L}$ and $\\mathbf{R}$ are low rank factors, and the entries of $\\mathbf{Q}$, $\\mathbf{L}$ and $\\mathbf{R}$ are quantized.","The model is compressed by substituting each layer with its $\\mathbf{Q} + \\mathbf{L}\\mathbf{R}$ decomposition, and the zero-shot performance of the compressed model is evaluated.","Additionally, $\\mathbf{L}$ and $\\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance.","$\\rm CALDERA$ obtains this decomposition by formulating it as an optimization problem $\\min_{\\mathbf{Q},\\mathbf{L},\\mathbf{R}}\\lVert(\\mathbf{Q} + \\mathbf{L}\\mathbf{R} - \\mathbf{W})\\mathbf{X}^\\top\\rVert_{\\rm F}^2$, where $\\mathbf{X}$ is the calibration data, and $\\mathbf{Q}, \\mathbf{L}, \\mathbf{R}$ are constrained to be representable using low-precision formats.","Theoretical upper bounds on the approximation error of $\\rm CALDERA$ are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget.","Results illustrate that compressing LlaMa-$2$ $7$B/$70$B and LlaMa-$3$ $8$B models obtained using $\\rm CALDERA$ outperforms existing post-training LLM compression techniques in the regime of less than $2.5$ bits per parameter.","The implementation is available at: \\href{https://github.com/pilancilab/caldera}{https://github.com/pilancilab/caldera}."],"url":"http://arxiv.org/abs/2405.18886v1","category":"cs.LG"}
{"created":"2024-05-29 08:41:49","title":"A general Greenlees-May splitting principle","abstract":"In equivariant topology, Greenlees and May used Mackey functors to show that, rationally, the stable homotopy category of $G$-spectra over a finite group $G$ splits as a product of simpler module categories. We extend the algebraic part (also independently proved by Th\\'evenaz and Webb) of this classical result to Mackey modules over an arbitrary Green functor, and use the case of the complex representation ring Green functor to obtain an algebraic model of the rational equivariant Kasparov category of $G$-cell algebras.","sentences":["In equivariant topology, Greenlees and May used Mackey functors to show that, rationally, the stable homotopy category of $G$-spectra over a finite group $G$ splits as a product of simpler module categories.","We extend the algebraic part (also independently proved by Th\\'evenaz and Webb) of this classical result to Mackey modules over an arbitrary Green functor, and use the case of the complex representation ring Green functor to obtain an algebraic model of the rational equivariant Kasparov category of $G$-cell algebras."],"url":"http://arxiv.org/abs/2405.18885v1","category":"math.KT"}
{"created":"2024-05-29 17:17:57","title":"EDGE: A new model for Nuclear Star Cluster formation in dwarf galaxies","abstract":"Nuclear Star Clusters (NSCs) are amongst the densest stellar systems in the Universe and are found at the centres of many bright spiral and elliptical galaxies, and up to ${\\sim}$40% of dwarf galaxies. However, their formation mechanisms, and possible links to globular clusters (GCs), remain debated. This paper uses the EDGE simulations - a collection of zoom-in, cosmological simulations of isolated dwarf galaxies -- to present a new formation mechanism for NSCs. We find that, at a gas spatial and mass resolution of ${\\sim}3\\,$pc and ${\\sim}161$ M$_\\odot$, respectively, NSCs naturally emerge in a subset of our EDGE dwarfs with redshift-zero halo masses of $\\rm{M}_{\\rm{r}200\\rm{c}} \\sim 5 \\times 10^9$ M$_\\odot$. These dwarfs are quenched by reionisation, but retain a significant reservoir of gas that is unable to cool and form stars. Sometime after reionisation, the dwarfs then undergo a major (${\\sim}$1:1) merger that excites rapid gas cooling, leading to a significant starburst. An NSC forms in this starburst that then quenches star formation thereafter. The result is a nucleated dwarf that has two stellar populations with distinct age: one pre-reionisation and one post-reionisation. Our mechanism is unique for two key reasons. Firstly, the low mass of the host dwarf means that NSCs, formed in this way, can accrete onto galaxies of almost all masses, potentially seeding the formation of NSCs everywhere. Secondly, our model predicts that NSCs should have at least two stellar populations with a large ($\\gtrsim$1 billion year) age separation. This yields a predicted colour magnitude diagram for our nucleated dwarfs that has two distinct main sequence turnoffs. Several GCs orbiting the Milky Way, including Omega Centauri and M54, show exactly this behaviour, suggesting that they may, in fact, be accreted NSCs.","sentences":["Nuclear Star Clusters (NSCs) are amongst the densest stellar systems in the Universe and are found at the centres of many bright spiral and elliptical galaxies, and up to ${\\sim}$40% of dwarf galaxies.","However, their formation mechanisms, and possible links to globular clusters (GCs), remain debated.","This paper uses the EDGE simulations - a collection of zoom-in, cosmological simulations of isolated dwarf galaxies -- to present a new formation mechanism for NSCs.","We find that, at a gas spatial and mass resolution of ${\\sim}3\\,$pc and ${\\sim}161$ M$_\\odot$, respectively, NSCs naturally emerge in a subset of our EDGE dwarfs with redshift-zero halo masses of $\\rm{M}_{\\rm{r}200\\rm{c}} \\sim 5 \\times 10^9$ M$_\\odot$. These dwarfs are quenched by reionisation, but retain a significant reservoir of gas that is unable to cool and form stars.","Sometime after reionisation, the dwarfs then undergo a major (${\\sim}$1:1) merger that excites rapid gas cooling, leading to a significant starburst.","An NSC forms in this starburst that then quenches star formation thereafter.","The result is a nucleated dwarf that has two stellar populations with distinct age: one pre-reionisation and one post-reionisation.","Our mechanism is unique for two key reasons.","Firstly, the low mass of the host dwarf means that NSCs, formed in this way, can accrete onto galaxies of almost all masses, potentially seeding the formation of NSCs everywhere.","Secondly, our model predicts that NSCs should have at least two stellar populations with a large ($\\gtrsim$1 billion year) age separation.","This yields a predicted colour magnitude diagram for our nucleated dwarfs that has two distinct main sequence turnoffs.","Several GCs orbiting the Milky Way, including Omega Centauri and M54, show exactly this behaviour, suggesting that they may, in fact, be accreted NSCs."],"url":"http://arxiv.org/abs/2405.19286v1","category":"astro-ph.GA"}
{"created":"2024-05-29 17:17:22","title":"MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection","abstract":"Abstract Meaning Representation (AMR) is a semantic formalism that captures the core meaning of an utterance. There has been substantial work developing AMR corpora in English and more recently across languages, though the limited size of existing datasets and the cost of collecting more annotations are prohibitive. With both engineering and scientific questions in mind, we introduce MASSIVE-AMR, a dataset with more than 84,000 text-to-graph annotations, currently the largest and most diverse of its kind: AMR graphs for 1,685 information-seeking utterances mapped to 50+ typologically diverse languages. We describe how we built our resource and its unique features before reporting on experiments using large language models for multilingual AMR and SPARQL parsing as well as applying AMRs for hallucination detection in the context of knowledge base question answering, with results shedding light on persistent issues using LLMs for structured parsing.","sentences":["Abstract Meaning Representation (AMR) is a semantic formalism that captures the core meaning of an utterance.","There has been substantial work developing AMR corpora in English and more recently across languages, though the limited size of existing datasets and the cost of collecting more annotations are prohibitive.","With both engineering and scientific questions in mind, we introduce MASSIVE-AMR, a dataset with more than 84,000 text-to-graph annotations, currently the largest and most diverse of its kind: AMR graphs for 1,685 information-seeking utterances mapped to 50+ typologically diverse languages.","We describe how we built our resource and its unique features before reporting on experiments using large language models for multilingual AMR and SPARQL parsing as well as applying AMRs for hallucination detection in the context of knowledge base question answering, with results shedding light on persistent issues using LLMs for structured parsing."],"url":"http://arxiv.org/abs/2405.19285v1","category":"cs.CL"}
{"created":"2024-05-29 16:35:59","title":"Quantum Non-classicality from Causal Data Fusion","abstract":"Bell's theorem, a cornerstone of quantum theory, shows that quantum correlations are incompatible with a classical theory of cause and effect. Through the lens of causal inference, it can be understood as a particular case of causal compatibility, which delves into the alignment of observational data with a given causal structure. Here, we explore the problem of causal data fusion that aims to piece together data tables collected under heterogeneous conditions. We investigate the quantum non-classicality that emerges when integrating both passive observations and interventions within an experimental setup. Referred to as \"non-classicality from data fusion,\" this phenomenon is identified and scrutinized across all latent exogenous causal structures involving three observed variables. Notably, we demonstrate the existence of quantum non-classicality resulting from data fusion, even in scenarios where achieving standard Bell non-classicality is impossible. Furthermore, we showcase the potential for attaining non-classicality across multiple interventions using quantum resources. This work extends a more compact parallel letter on the same subject and provides all the required technical proofs.","sentences":["Bell's theorem, a cornerstone of quantum theory, shows that quantum correlations are incompatible with a classical theory of cause and effect.","Through the lens of causal inference, it can be understood as a particular case of causal compatibility, which delves into the alignment of observational data with a given causal structure.","Here, we explore the problem of causal data fusion that aims to piece together data tables collected under heterogeneous conditions.","We investigate the quantum non-classicality that emerges when integrating both passive observations and interventions within an experimental setup.","Referred to as \"non-classicality from data fusion,\" this phenomenon is identified and scrutinized across all latent exogenous causal structures involving three observed variables.","Notably, we demonstrate the existence of quantum non-classicality resulting from data fusion, even in scenarios where achieving standard Bell non-classicality is impossible.","Furthermore, we showcase the potential for attaining non-classicality across multiple interventions using quantum resources.","This work extends a more compact parallel letter on the same subject and provides all the required technical proofs."],"url":"http://arxiv.org/abs/2405.19252v1","category":"quant-ph"}
{"created":"2024-05-29 15:18:37","title":"Exoplanet Aeronomy: A Case Study of WASP-69b's Variable Thermosphere","abstract":"Aeronomy, the study of Earth's upper atmosphere and its interaction with the local space environment, has long traced changes in the thermospheres of Earth and other solar system planets to solar variability in the X-ray and extreme ultraviolet (collectively, \"XUV\") bands. Extending comparative aeronomy to the short-period extrasolar planets may illuminate whether stellar XUV irradiation powers atmospheric outflows that change planetary radii on astronomical timescales. In recent years, near-infrared transit spectroscopy of metastable HeI has been a prolific tracer of high-altitude planetary gas. We present a case study of exoplanet aeronomy using metastable HeI transit observations from Palomar/WIRC and follow-up high-energy data from the Neil Gehrels Swift Observatory that were taken within one month of the WASP-69 system, a K-type main sequence star with a well-studied hot Jupiter companion. Supplemented by archival data, we find that WASP-69's X-ray flux in 2023 was less than 50% of what was recorded in 2016 and that the metastable HeI absorption from WASP-69b was lower in 2023 versus past epochs from 2017-2019. Via atmospheric modeling, we show that this time-variable metastable HeI signal is in the expected direction given the observed change in stellar XUV, possibly stemming from WASP-69's magnetic activity cycle. Our results underscore the ability of multi-epoch, multi-wavelength observations to paint a cohesive picture of the interaction between an exoplanet's atmosphere and its host star.","sentences":["Aeronomy, the study of Earth's upper atmosphere and its interaction with the local space environment, has long traced changes in the thermospheres of Earth and other solar system planets to solar variability in the X-ray and extreme ultraviolet (collectively, \"XUV\") bands.","Extending comparative aeronomy to the short-period extrasolar planets may illuminate whether stellar XUV irradiation powers atmospheric outflows that change planetary radii on astronomical timescales.","In recent years, near-infrared transit spectroscopy of metastable HeI has been a prolific tracer of high-altitude planetary gas.","We present a case study of exoplanet aeronomy using metastable HeI transit observations from Palomar/WIRC and follow-up high-energy data from the Neil Gehrels Swift Observatory that were taken within one month of the WASP-69 system, a K-type main sequence star with a well-studied hot Jupiter companion.","Supplemented by archival data, we find that WASP-69's X-ray flux in 2023 was less than 50% of what was recorded in 2016 and that the metastable HeI absorption from WASP-69b was lower in 2023 versus past epochs from 2017-2019.","Via atmospheric modeling, we show that this time-variable metastable HeI signal is in the expected direction given the observed change in stellar XUV, possibly stemming from WASP-69's magnetic activity cycle.","Our results underscore the ability of multi-epoch, multi-wavelength observations to paint a cohesive picture of the interaction between an exoplanet's atmosphere and its host star."],"url":"http://arxiv.org/abs/2405.19177v1","category":"astro-ph.EP"}
{"created":"2024-05-29 14:50:08","title":"Lagrangian metric geometry with Riemannian bounds","abstract":"We study collections of exact Lagrangian submanifolds respecting some uniform Riemannian bounds, which we equip with a metric naturally arising in symplectic topology (e.g. the Lagrangian Hofer metric or the spectral metric). We exhibit many metric and symplectic properties of these spaces, such that they have compact completions and that they contain only finitely many Hamiltonian isotopy classes. We then use this to exclude many unusual phenomena from happening in these bounded spaces. Taking limits in the bounds, we also conclude that there are at most countably many Hamiltonian isotopy classes of exact Lagrangian submanifolds in a Liouville manifold. Under some mild topological assumptions, we get analogous results for monotone Lagrangian submanifolds with a fixed monotonicity constant. Finally, in the process of showing these results, we get new results on the Riemannian geometry of cotangent bundles and surfaces which might be of independent interest.","sentences":["We study collections of exact Lagrangian submanifolds respecting some uniform Riemannian bounds, which we equip with a metric naturally arising in symplectic topology (e.g. the Lagrangian Hofer metric or the spectral metric).","We exhibit many metric and symplectic properties of these spaces, such that they have compact completions and that they contain only finitely many Hamiltonian isotopy classes.","We then use this to exclude many unusual phenomena from happening in these bounded spaces.","Taking limits in the bounds, we also conclude that there are at most countably many Hamiltonian isotopy classes of exact Lagrangian submanifolds in a Liouville manifold.","Under some mild topological assumptions, we get analogous results for monotone Lagrangian submanifolds with a fixed monotonicity constant.","Finally, in the process of showing these results, we get new results on the Riemannian geometry of cotangent bundles and surfaces which might be of independent interest."],"url":"http://arxiv.org/abs/2405.19144v1","category":"math.SG"}
{"created":"2024-05-29 14:23:33","title":"Signatures of a Charge Ice State in the Doped Mott Insulator Nb$_3$Cl$_8$","abstract":"The interplay between strong electronic correlations and the inherent frustration of certain lattice geometries is a common mechanism for the formation of nontrivial states of matter. In this work, we theoretically explore the collective electronic effects in the monolayer Nb$_3$Cl$_8$, a recently discovered triangular lattice Mott insulator. Our advanced many-body numerical simulations predict the emergence of a phase separation region upon doping this material. Notably, in close proximity to the phase separation, the static charge susceptibility undergoes a drastic change and reveals a distinctive bow-tie structure in momentum space. The appearance of such a signature in the context of spin degrees of freedom would indicate the formation of a spin ice state. This finding allows us to associate the observed phase separation to a charge ice state, a state with a remarkable power-law dependence of both the effective exchange interaction and correlations between electronic densities in real space.","sentences":["The interplay between strong electronic correlations and the inherent frustration of certain lattice geometries is a common mechanism for the formation of nontrivial states of matter.","In this work, we theoretically explore the collective electronic effects in the monolayer Nb$_3$Cl$_8$, a recently discovered triangular lattice Mott insulator.","Our advanced many-body numerical simulations predict the emergence of a phase separation region upon doping this material.","Notably, in close proximity to the phase separation, the static charge susceptibility undergoes a drastic change and reveals a distinctive bow-tie structure in momentum space.","The appearance of such a signature in the context of spin degrees of freedom would indicate the formation of a spin ice state.","This finding allows us to associate the observed phase separation to a charge ice state, a state with a remarkable power-law dependence of both the effective exchange interaction and correlations between electronic densities in real space."],"url":"http://arxiv.org/abs/2405.19114v1","category":"cond-mat.str-el"}
{"created":"2024-05-29 14:01:40","title":"A study of why we need to reassess full reference image quality assessment with medical images","abstract":"Image quality assessment (IQA) is not just indispensable in clinical practice to ensure high standards, but also in the development stage of novel algorithms that operate on medical images with reference data. This paper provides a structured and comprehensive collection of examples where the two most common full reference (FR) image quality measures prove to be unsuitable for the assessment of novel algorithms using different kinds of medical images, including real-world MRI, CT, OCT, X-Ray, digital pathology and photoacoustic imaging data. In particular, the FR-IQA measures PSNR and SSIM are known and tested for working successfully in many natural imaging tasks, but discrepancies in medical scenarios have been noted in the literature. Inconsistencies arising in medical images are not surprising, as they have very different properties than natural images which have not been targeted nor tested in the development of the mentioned measures, and therefore might imply wrong judgement of novel methods for medical images. Therefore, improvement is urgently needed in particular in this era of AI to increase explainability, reproducibility and generalizability in machine learning for medical imaging and beyond. On top of the pitfalls we will provide ideas for future research as well as suggesting guidelines for the usage of FR-IQA measures applied to medical images.","sentences":["Image quality assessment (IQA) is not just indispensable in clinical practice to ensure high standards, but also in the development stage of novel algorithms that operate on medical images with reference data.","This paper provides a structured and comprehensive collection of examples where the two most common full reference (FR) image quality measures prove to be unsuitable for the assessment of novel algorithms using different kinds of medical images, including real-world MRI, CT, OCT, X-Ray, digital pathology and photoacoustic imaging data.","In particular, the FR-IQA measures PSNR and SSIM are known and tested for working successfully in many natural imaging tasks, but discrepancies in medical scenarios have been noted in the literature.","Inconsistencies arising in medical images are not surprising, as they have very different properties than natural images which have not been targeted nor tested in the development of the mentioned measures, and therefore might imply wrong judgement of novel methods for medical images.","Therefore, improvement is urgently needed in particular in this era of AI to increase explainability, reproducibility and generalizability in machine learning for medical imaging and beyond.","On top of the pitfalls we will provide ideas for future research as well as suggesting guidelines for the usage of FR-IQA measures applied to medical images."],"url":"http://arxiv.org/abs/2405.19097v1","category":"eess.IV"}
{"created":"2024-05-29 13:54:30","title":"Multi-stage Retrieve and Re-rank Model for Automatic Medical Coding Recommendation","abstract":"The International Classification of Diseases (ICD) serves as a definitive medical classification system encompassing a wide range of diseases and conditions. The primary objective of ICD indexing is to allocate a subset of ICD codes to a medical record, which facilitates standardized documentation and management of various health conditions. Most existing approaches have suffered from selecting the proper label subsets from an extremely large ICD collection with a heavy long-tailed label distribution. In this paper, we leverage a multi-stage ``retrieve and re-rank'' framework as a novel solution to ICD indexing, via a hybrid discrete retrieval method, and re-rank retrieved candidates with contrastive learning that allows the model to make more accurate predictions from a simplified label space. The retrieval model is a hybrid of auxiliary knowledge of the electronic health records (EHR) and a discrete retrieval method (BM25), which efficiently collects high-quality candidates. In the last stage, we propose a label co-occurrence guided contrastive re-ranking model, which re-ranks the candidate labels by pulling together the clinical notes with positive ICD codes. Experimental results show the proposed method achieves state-of-the-art performance on a number of measures on the MIMIC-III benchmark.","sentences":["The International Classification of Diseases (ICD) serves as a definitive medical classification system encompassing a wide range of diseases and conditions.","The primary objective of ICD indexing is to allocate a subset of ICD codes to a medical record, which facilitates standardized documentation and management of various health conditions.","Most existing approaches have suffered from selecting the proper label subsets from an extremely large ICD collection with a heavy long-tailed label distribution.","In this paper, we leverage a multi-stage ``retrieve and re-rank'' framework as a novel solution to ICD indexing, via a hybrid discrete retrieval method, and re-rank retrieved candidates with contrastive learning that allows the model to make more accurate predictions from a simplified label space.","The retrieval model is a hybrid of auxiliary knowledge of the electronic health records (EHR) and a discrete retrieval method (BM25), which efficiently collects high-quality candidates.","In the last stage, we propose a label co-occurrence guided contrastive re-ranking model, which re-ranks the candidate labels by pulling together the clinical notes with positive ICD codes.","Experimental results show the proposed method achieves state-of-the-art performance on a number of measures on the MIMIC-III benchmark."],"url":"http://arxiv.org/abs/2405.19093v1","category":"cs.CL"}
{"created":"2024-05-29 13:44:07","title":"Auxiliary Knowledge-Induced Learning for Automatic Multi-Label Medical Document Classification","abstract":"The International Classification of Diseases (ICD) is an authoritative medical classification system of different diseases and conditions for clinical and management purposes. ICD indexing assigns a subset of ICD codes to a medical record. Since human coding is labour-intensive and error-prone, many studies employ machine learning to automate the coding process. ICD coding is a challenging task, as it needs to assign multiple codes to each medical document from an extremely large hierarchically organized collection. In this paper, we propose a novel approach for ICD indexing that adopts three ideas: (1) we use a multi-level deep dilated residual convolution encoder to aggregate the information from the clinical notes and learn document representations across different lengths of the texts; (2) we formalize the task of ICD classification with auxiliary knowledge of the medical records, which incorporates not only the clinical texts but also different clinical code terminologies and drug prescriptions for better inferring the ICD codes; and (3) we introduce a graph convolutional network to leverage the co-occurrence patterns among ICD codes, aiming to enhance the quality of label representations. Experimental results show the proposed method achieves state-of-the-art performance on a number of measures.","sentences":["The International Classification of Diseases (ICD) is an authoritative medical classification system of different diseases and conditions for clinical and management purposes.","ICD indexing assigns a subset of ICD codes to a medical record.","Since human coding is labour-intensive and error-prone, many studies employ machine learning to automate the coding process.","ICD coding is a challenging task, as it needs to assign multiple codes to each medical document from an extremely large hierarchically organized collection.","In this paper, we propose a novel approach for ICD indexing that adopts three ideas: (1) we use a multi-level deep dilated residual convolution encoder to aggregate the information from the clinical notes and learn document representations across different lengths of the texts; (2) we formalize the task of ICD classification with auxiliary knowledge of the medical records, which incorporates not only the clinical texts but also different clinical code terminologies and drug prescriptions for better inferring the ICD codes; and (3) we introduce a graph convolutional network to leverage the co-occurrence patterns among ICD codes, aiming to enhance the quality of label representations.","Experimental results show the proposed method achieves state-of-the-art performance on a number of measures."],"url":"http://arxiv.org/abs/2405.19084v1","category":"cs.CL"}
{"created":"2024-05-29 13:06:10","title":"New perspectives on the optimal placement of detectors for suicide bombers using metaheuristics","abstract":"We consider an operational model of suicide bombing attacks -- an increasingly prevalent form of terrorism -- against specific targets, and the use of protective countermeasures based on the deployment of detectors over the area under threat. These detectors have to be carefully located in order to minimize the expected number of casualties or the economic damage suffered, resulting in a hard optimization problem for which different metaheuristics have been proposed. Rather than assuming random decisions by the attacker, the problem is approached by considering different models of the latter, whereby he takes informed decisions on which objective must be targeted and through which path it has to be reached based on knowledge on the importance or value of the objectives or on the defensive strategy of the defender (a scenario that can be regarded as an adversarial game). We consider four different algorithms, namely a greedy heuristic, a hill climber, tabu search and an evolutionary algorithm, and study their performance on a broad collection of problem instances trying to resemble different realistic settings such as a coastal area, a modern urban area, and the historic core of an old town. It is shown that the adversarial scenario is harder for all techniques, and that the evolutionary algorithm seems to adapt better to the complexity of the resulting search landscape.","sentences":["We consider an operational model of suicide bombing attacks -- an increasingly prevalent form of terrorism -- against specific targets, and the use of protective countermeasures based on the deployment of detectors over the area under threat.","These detectors have to be carefully located in order to minimize the expected number of casualties or the economic damage suffered, resulting in a hard optimization problem for which different metaheuristics have been proposed.","Rather than assuming random decisions by the attacker, the problem is approached by considering different models of the latter, whereby he takes informed decisions on which objective must be targeted and through which path it has to be reached based on knowledge on the importance or value of the objectives or on the defensive strategy of the defender (a scenario that can be regarded as an adversarial game).","We consider four different algorithms, namely a greedy heuristic, a hill climber, tabu search and an evolutionary algorithm, and study their performance on a broad collection of problem instances trying to resemble different realistic settings such as a coastal area, a modern urban area, and the historic core of an old town.","It is shown that the adversarial scenario is harder for all techniques, and that the evolutionary algorithm seems to adapt better to the complexity of the resulting search landscape."],"url":"http://arxiv.org/abs/2405.19060v1","category":"cs.NE"}
{"created":"2024-05-29 12:54:04","title":"Spectropolarimetric characterisation of exoplanet host stars in preparation of the Ariel mission. Magnetic environment of HD 63433","abstract":"The accurate characterisation of the stellar magnetism of planetary host stars has been gaining momentum, especially in the context of transmission spectroscopy investigations of exoplanets. Indeed, the magnetic field regulates the amount of energetic radiation and stellar wind impinging on planets, as well as the presence of inhomogeneities on the stellar surface that hinder the precise extraction of the planetary atmospheric absorption signal. We initiated a spectropolarimetric campaign to unveil the magnetic field properties of known exoplanet hosting stars included in the current list of potential Ariel targets. In this work, we focus on HD 63433, a young solar-like star hosting two sub-Neptunes and an Earth-sized planet. These exoplanets orbit within 0.15 au from the host star and have likely experienced different atmospheric evolutionary paths. We analysed optical spectropolarimetric data collected with ESPaDOnS, HARPSpol, and Neo-Narval to compute the magnetic activity indices (log R'_HK , H$\\alpha$, and Ca ii infrared triplet), measure the longitudinal magnetic field, and reconstruct the large-scale magnetic topology via Zeeman-Doppler imaging (ZDI). The magnetic field map was then employed to simulate the space environment in which the exoplanets orbit. The reconstructed stellar magnetic field has an average strength of 24 G and it features a complex topology with a dominant toroidal component, in agreement with other stars of a similar spectral type and age. Our simulations of the stellar environment locate 10% of the innermost planetary orbit inside the Alfv\\'en surface and, thus, brief magnetic connections between the planet and the star can occur. The outer planets are outside the Alfv\\'en surface and a bow shock between the stellar wind and the planetary magnetosphere could potentially form.","sentences":["The accurate characterisation of the stellar magnetism of planetary host stars has been gaining momentum, especially in the context of transmission spectroscopy investigations of exoplanets.","Indeed, the magnetic field regulates the amount of energetic radiation and stellar wind impinging on planets, as well as the presence of inhomogeneities on the stellar surface that hinder the precise extraction of the planetary atmospheric absorption signal.","We initiated a spectropolarimetric campaign to unveil the magnetic field properties of known exoplanet hosting stars included in the current list of potential Ariel targets.","In this work, we focus on HD 63433, a young solar-like star hosting two sub-Neptunes and an Earth-sized planet.","These exoplanets orbit within 0.15 au from the host star and have likely experienced different atmospheric evolutionary paths.","We analysed optical spectropolarimetric data collected with ESPaDOnS, HARPSpol, and Neo-Narval to compute the magnetic activity indices (log R'_HK , H$\\alpha$, and Ca ii infrared triplet), measure the longitudinal magnetic field, and reconstruct the large-scale magnetic topology via Zeeman-Doppler imaging (ZDI).","The magnetic field map was then employed to simulate the space environment in which the exoplanets orbit.","The reconstructed stellar magnetic field has an average strength of 24 G and it features a complex topology with a dominant toroidal component, in agreement with other stars of a similar spectral type and age.","Our simulations of the stellar environment locate 10% of the innermost planetary orbit inside the Alfv\\'en surface and, thus, brief magnetic connections between the planet and the star can occur.","The outer planets are outside the Alfv\\'en surface and a bow shock between the stellar wind and the planetary magnetosphere could potentially form."],"url":"http://arxiv.org/abs/2405.19052v1","category":"astro-ph.EP"}
{"created":"2024-05-29 12:27:19","title":"Finite-Choice Logic Programming","abstract":"Logic programming, as exemplified by datalog, defines the meaning of a program as the canonical smallest model derived from deductive closure over its inference rules. However, many problems call for an enumeration of models that vary along some set of choices while maintaining structural and logical constraints -- there is no single canonical model. The notion of stable models has successfully captured programmer intuition about the set of valid solutions for such problems, giving rise to a family of programming languages and associated solvers collectively known as answer set programming. Unfortunately, the definition of a stable model is frustratingly indirect, especially in the presence of rules containing free variables.   We propose a new formalism, called finite-choice logic programing, for which the set of stable models can be characterized as the least fixed point of an immediate consequence operator. Our formalism allows straightforward expression of common idioms in both datalog and answer set programming, gives meaning to a new and useful class of programs, enjoys a constructive and direct operational semantics, and admits a predictive cost semantics, which we demonstrate through our implementation.","sentences":["Logic programming, as exemplified by datalog, defines the meaning of a program as the canonical smallest model derived from deductive closure over its inference rules.","However, many problems call for an enumeration of models that vary along some set of choices while maintaining structural and logical constraints -- there is no single canonical model.","The notion of stable models has successfully captured programmer intuition about the set of valid solutions for such problems, giving rise to a family of programming languages and associated solvers collectively known as answer set programming.","Unfortunately, the definition of a stable model is frustratingly indirect, especially in the presence of rules containing free variables.   ","We propose a new formalism, called finite-choice logic programing, for which the set of stable models can be characterized as the least fixed point of an immediate consequence operator.","Our formalism allows straightforward expression of common idioms in both datalog and answer set programming, gives meaning to a new and useful class of programs, enjoys a constructive and direct operational semantics, and admits a predictive cost semantics, which we demonstrate through our implementation."],"url":"http://arxiv.org/abs/2405.19040v1","category":"cs.PL"}
{"created":"2024-05-29 09:33:21","title":"Measurement of the energy dependence of the $e^+e^- \\to B\\bar{B}$, $B\\bar{B}{}^*$, and $B^*\\bar{B}{}^*$ cross sections at Belle~II","abstract":"We report measurements of the $e^+e^- \\to B\\bar{B}$, $B\\bar{B}{}^*$, and $B^*\\bar{B}{}^*$ cross sections at four energies, 10653, 10701, 10746 and 10805 MeV, using data collected by the Belle~II experiment. We reconstruct one $B$ meson in a large number of hadronic final states and use its momentum to identify the production process. In the first $2-5$ MeV above $B^*\\bar{B}{}^*$ threshold, the $e^+e^- \\to B^*\\bar{B}{}^*$ cross section increases rapidly. This may indicate the presence of a pole close to the threshold.","sentences":["We report measurements of the $e^+e^- \\to B\\bar{B}$, $B\\bar{B}{}^*$, and $B^*\\bar{B}{}^*$ cross sections at four energies, 10653, 10701, 10746 and 10805 MeV, using data collected by the Belle~II experiment.","We reconstruct one $B$ meson in a large number of hadronic final states and use its momentum to identify the production process.","In the first $2-5$ MeV above $B^*\\bar{B}{}^*$ threshold, the $e^+e^- \\to B^*\\bar{B}{}^*$ cross section increases rapidly.","This may indicate the presence of a pole close to the threshold."],"url":"http://arxiv.org/abs/2405.18928v1","category":"hep-ex"}
{"created":"2024-05-29 09:29:09","title":"MDIW-13: a New Multi-Lingual and Multi-Script Database and Benchmark for Script Identification","abstract":"Script identification plays a vital role in applications that involve handwriting and document analysis within a multi-script and multi-lingual environment. Moreover, it exhibits a profound connection with human cognition. This paper provides a new database for benchmarking script identification algorithms, which contains both printed and handwritten documents collected from a wide variety of scripts, such as Arabic, Bengali (Bangla), Gujarati, Gurmukhi, Devanagari, Japanese, Kannada, Malayalam, Oriya, Roman, Tamil, Telugu, and Thai. The dataset consists of 1,135 documents scanned from local newspaper and handwritten letters as well as notes from different native writers. Further, these documents are segmented into lines and words, comprising a total of 13,979 and 86,655 lines and words, respectively, in the dataset. Easy-to-go benchmarks are proposed with handcrafted and deep learning methods. The benchmark includes results at the document, line, and word levels with printed and handwritten documents. Results of script identification independent of the document/line/word level and independent of the printed/handwritten letters are also given. The new multi-lingual database is expected to create new script identifiers, present various challenges, including identifying handwritten and printed samples and serve as a foundation for future research in script identification based on the reported results of the three benchmarks.","sentences":["Script identification plays a vital role in applications that involve handwriting and document analysis within a multi-script and multi-lingual environment.","Moreover, it exhibits a profound connection with human cognition.","This paper provides a new database for benchmarking script identification algorithms, which contains both printed and handwritten documents collected from a wide variety of scripts, such as Arabic, Bengali (Bangla), Gujarati, Gurmukhi, Devanagari, Japanese, Kannada, Malayalam, Oriya, Roman, Tamil, Telugu, and Thai.","The dataset consists of 1,135 documents scanned from local newspaper and handwritten letters as well as notes from different native writers.","Further, these documents are segmented into lines and words, comprising a total of 13,979 and 86,655 lines and words, respectively, in the dataset.","Easy-to-go benchmarks are proposed with handcrafted and deep learning methods.","The benchmark includes results at the document, line, and word levels with printed and handwritten documents.","Results of script identification independent of the document/line/word level and independent of the printed/handwritten letters are also given.","The new multi-lingual database is expected to create new script identifiers, present various challenges, including identifying handwritten and printed samples and serve as a foundation for future research in script identification based on the reported results of the three benchmarks."],"url":"http://arxiv.org/abs/2405.18924v1","category":"cs.CV"}
{"created":"2024-05-29 09:16:03","title":"Leveraging Time-Series Foundation Models in Smart Agriculture for Soil Moisture Forecasting","abstract":"The recent surge in foundation models for natural language processing and computer vision has fueled innovation across various domains. Inspired by this progress, we explore the potential of foundation models for time-series forecasting in smart agriculture, a field often plagued by limited data availability. Specifically, this work presents a novel application of $\\texttt{TimeGPT}$, a state-of-the-art (SOTA) time-series foundation model, to predict soil water potential ($\\psi_\\mathrm{soil}$), a key indicator of field water status that is typically used for irrigation advice. Traditionally, this task relies on a wide array of input variables. We explore $\\psi_\\mathrm{soil}$'s ability to forecast $\\psi_\\mathrm{soil}$ in: ($i$) a zero-shot setting, ($ii$) a fine-tuned setting relying solely on historic $\\psi_\\mathrm{soil}$ measurements, and ($iii$) a fine-tuned setting where we also add exogenous variables to the model. We compare $\\texttt{TimeGPT}$'s performance to established SOTA baseline models for forecasting $\\psi_\\mathrm{soil}$. Our results demonstrate that $\\texttt{TimeGPT}$ achieves competitive forecasting accuracy using only historical $\\psi_\\mathrm{soil}$ data, highlighting its remarkable potential for agricultural applications. This research paves the way for foundation time-series models for sustainable development in agriculture by enabling forecasting tasks that were traditionally reliant on extensive data collection and domain expertise.","sentences":["The recent surge in foundation models for natural language processing and computer vision has fueled innovation across various domains.","Inspired by this progress, we explore the potential of foundation models for time-series forecasting in smart agriculture, a field often plagued by limited data availability.","Specifically, this work presents a novel application of $\\texttt{TimeGPT}$, a state-of-the-art (SOTA) time-series foundation model, to predict soil water potential ($\\psi_\\mathrm{soil}$), a key indicator of field water status that is typically used for irrigation advice.","Traditionally, this task relies on a wide array of input variables.","We explore $\\psi_\\mathrm{soil}$'s ability to forecast $\\psi_\\mathrm{soil}$ in: ($i$) a zero-shot setting, ($ii$) a fine-tuned setting relying solely on historic $\\psi_\\mathrm{soil}$ measurements, and ($iii$) a fine-tuned setting where we also add exogenous variables to the model.","We compare $\\texttt{TimeGPT}$'s performance to established SOTA baseline models for forecasting $\\psi_\\mathrm{soil}$. Our results demonstrate that $\\texttt{TimeGPT}$ achieves competitive forecasting accuracy using only historical $\\psi_\\mathrm{soil}$ data, highlighting its remarkable potential for agricultural applications.","This research paves the way for foundation time-series models for sustainable development in agriculture by enabling forecasting tasks that were traditionally reliant on extensive data collection and domain expertise."],"url":"http://arxiv.org/abs/2405.18913v1","category":"cs.LG"}
{"created":"2024-05-29 08:39:39","title":"Tuning-Free Alignment of Diffusion Models with Direct Noise Optimization","abstract":"In this work, we focus on the alignment problem of diffusion models with a continuous reward function, which represents specific objectives for downstream tasks, such as improving human preference. The central goal of the alignment problem is to adjust the distribution learned by diffusion models such that the generated samples maximize the target reward function. We propose a novel alignment approach, named Direct Noise Optimization (DNO), that optimizes the injected noise during the sampling process of diffusion models. By design, DNO is tuning-free and prompt-agnostic, as the alignment occurs in an online fashion during generation. We rigorously study the theoretical properties of DNO and also propose variants to deal with non-differentiable reward functions. Furthermore, we identify that naive implementation of DNO occasionally suffers from the out-of-distribution reward hacking problem, where optimized samples have high rewards but are no longer in the support of the pretrained distribution. To remedy this issue, we leverage classical high-dimensional statistics theory and propose to augment the DNO loss with certain probability regularization. We conduct extensive experiments on several popular reward functions trained on human feedback data and demonstrate that the proposed DNO approach achieves state-of-the-art reward scores as well as high image quality, all within a reasonable time budget for generation.","sentences":["In this work, we focus on the alignment problem of diffusion models with a continuous reward function, which represents specific objectives for downstream tasks, such as improving human preference.","The central goal of the alignment problem is to adjust the distribution learned by diffusion models such that the generated samples maximize the target reward function.","We propose a novel alignment approach, named Direct Noise Optimization (DNO), that optimizes the injected noise during the sampling process of diffusion models.","By design, DNO is tuning-free and prompt-agnostic, as the alignment occurs in an online fashion during generation.","We rigorously study the theoretical properties of DNO and also propose variants to deal with non-differentiable reward functions.","Furthermore, we identify that naive implementation of DNO occasionally suffers from the out-of-distribution reward hacking problem, where optimized samples have high rewards but are no longer in the support of the pretrained distribution.","To remedy this issue, we leverage classical high-dimensional statistics theory and propose to augment the DNO loss with certain probability regularization.","We conduct extensive experiments on several popular reward functions trained on human feedback data and demonstrate that the proposed DNO approach achieves state-of-the-art reward scores as well as high image quality, all within a reasonable time budget for generation."],"url":"http://arxiv.org/abs/2405.18881v1","category":"cs.LG"}
{"created":"2024-05-29 08:39:31","title":"EventZoom: A Progressive Approach to Event-Based Data Augmentation for Enhanced Neuromorphic Vision","abstract":"Event data captured by Dynamic Vision Sensors (DVS) offers a unique approach to visual processing that differs from traditional video capture, showcasing its efficiency in dynamic and real-time scenarios. Despite advantages such as high temporal resolution and low energy consumption, the application of event data faces challenges due to limited dataset size and diversity. To address this, we developed EventZoom -- a data augmentation strategy specifically designed for event data. EventZoom employs a progressive temporal strategy that intelligently blends time and space to enhance the diversity and complexity of the data while maintaining its authenticity. This method aims to improve the quality of data for model training and enhance the adaptability and robustness of algorithms in handling complex dynamic scenes. We have experimentally validated EventZoom across various supervised learning frameworks, including supervised, semi-supervised, and unsupervised learning. Our results demonstrate that EventZoom consistently outperforms other data augmentation methods, confirming its effectiveness and applicability as a powerful event-based data augmentation tool in diverse learning settings.","sentences":["Event data captured by Dynamic Vision Sensors (DVS) offers a unique approach to visual processing that differs from traditional video capture, showcasing its efficiency in dynamic and real-time scenarios.","Despite advantages such as high temporal resolution and low energy consumption, the application of event data faces challenges due to limited dataset size and diversity.","To address this, we developed EventZoom -- a data augmentation strategy specifically designed for event data.","EventZoom employs a progressive temporal strategy that intelligently blends time and space to enhance the diversity and complexity of the data while maintaining its authenticity.","This method aims to improve the quality of data for model training and enhance the adaptability and robustness of algorithms in handling complex dynamic scenes.","We have experimentally validated EventZoom across various supervised learning frameworks, including supervised, semi-supervised, and unsupervised learning.","Our results demonstrate that EventZoom consistently outperforms other data augmentation methods, confirming its effectiveness and applicability as a powerful event-based data augmentation tool in diverse learning settings."],"url":"http://arxiv.org/abs/2405.18880v1","category":"cs.CV"}
{"created":"2024-05-29 08:36:09","title":"Continuous Product Graph Neural Networks","abstract":"Processing multidomain data defined on multiple graphs holds significant potential in various practical applications in computer science. However, current methods are mostly limited to discrete graph filtering operations. Tensorial partial differential equations on graphs (TPDEGs) provide a principled framework for modeling structured data across multiple interacting graphs, addressing the limitations of the existing discrete methodologies. In this paper, we introduce Continuous Product Graph Neural Networks (CITRUS) that emerge as a natural solution to the TPDEG. CITRUS leverages the separability of continuous heat kernels from Cartesian graph products to efficiently implement graph spectral decomposition. We conduct thorough theoretical analyses of the stability and over-smoothing properties of CITRUS in response to domain-specific graph perturbations and graph spectra effects on the performance. We evaluate CITRUS on well-known traffic and weather spatiotemporal forecasting datasets, demonstrating superior performance over existing approaches.","sentences":["Processing multidomain data defined on multiple graphs holds significant potential in various practical applications in computer science.","However, current methods are mostly limited to discrete graph filtering operations.","Tensorial partial differential equations on graphs (TPDEGs) provide a principled framework for modeling structured data across multiple interacting graphs, addressing the limitations of the existing discrete methodologies.","In this paper, we introduce Continuous Product Graph Neural Networks (CITRUS) that emerge as a natural solution to the TPDEG.","CITRUS leverages the separability of continuous heat kernels from Cartesian graph products to efficiently implement graph spectral decomposition.","We conduct thorough theoretical analyses of the stability and over-smoothing properties of CITRUS in response to domain-specific graph perturbations and graph spectra effects on the performance.","We evaluate CITRUS on well-known traffic and weather spatiotemporal forecasting datasets, demonstrating superior performance over existing approaches."],"url":"http://arxiv.org/abs/2405.18877v1","category":"cs.LG"}
{"created":"2024-05-29 08:35:17","title":"Counterfactual Metarules for Local and Global Recourse","abstract":"We introduce T-CREx, a novel model-agnostic method for local and global counterfactual explanation (CE), which summarises recourse options for both individuals and groups in the form of human-readable rules. It leverages tree-based surrogate models to learn the counterfactual rules, alongside 'metarules' denoting their regions of optimality, providing both a global analysis of model behaviour and diverse recourse options for users. Experiments indicate that T-CREx achieves superior aggregate performance over existing rule-based baselines on a range of CE desiderata, while being orders of magnitude faster to run.","sentences":["We introduce T-CREx, a novel model-agnostic method for local and global counterfactual explanation (CE), which summarises recourse options for both individuals and groups in the form of human-readable rules.","It leverages tree-based surrogate models to learn the counterfactual rules, alongside 'metarules' denoting their regions of optimality, providing both a global analysis of model behaviour and diverse recourse options for users.","Experiments indicate that T-CREx achieves superior aggregate performance over existing rule-based baselines on a range of CE desiderata, while being orders of magnitude faster to run."],"url":"http://arxiv.org/abs/2405.18875v1","category":"cs.AI"}
{"created":"2024-05-29 08:31:16","title":"LLMs achieve adult human performance on higher-order theory of mind tasks","abstract":"This paper examines the extent to which large language models (LLMs) have developed higher-order theory of mind (ToM); the human ability to reason about multiple mental and emotional states in a recursive manner (e.g. I think that you believe that she knows). This paper builds on prior work by introducing a handwritten test suite -- Multi-Order Theory of Mind Q&A -- and using it to compare the performance of five LLMs to a newly gathered adult human benchmark. We find that GPT-4 and Flan-PaLM reach adult-level and near adult-level performance on ToM tasks overall, and that GPT-4 exceeds adult performance on 6th order inferences. Our results suggest that there is an interplay between model size and finetuning for the realisation of ToM abilities, and that the best-performing LLMs have developed a generalised capacity for ToM. Given the role that higher-order ToM plays in a wide range of cooperative and competitive human behaviours, these findings have significant implications for user-facing LLM applications.","sentences":["This paper examines the extent to which large language models (LLMs) have developed higher-order theory of mind (ToM); the human ability to reason about multiple mental and emotional states in a recursive manner (e.g. I think that you believe that she knows).","This paper builds on prior work by introducing a handwritten test suite -- Multi-Order Theory of Mind Q&A -- and using it to compare the performance of five LLMs to a newly gathered adult human benchmark.","We find that GPT-4 and Flan-PaLM reach adult-level and near adult-level performance on ToM tasks overall, and that GPT-4 exceeds adult performance on 6th order inferences.","Our results suggest that there is an interplay between model size and finetuning for the realisation of ToM abilities, and that the best-performing LLMs have developed a generalised capacity for ToM. Given the role that higher-order ToM plays in a wide range of cooperative and competitive human behaviours, these findings have significant implications for user-facing LLM applications."],"url":"http://arxiv.org/abs/2405.18870v1","category":"cs.AI"}
{"created":"2024-05-29 08:28:23","title":"Topological Perspectives on Optimal Multimodal Embedding Spaces","abstract":"Recent strides in multimodal model development have ignited a paradigm shift in the realm of text-to-image generation. Among these advancements, CLIP stands out as a remarkable achievement which is a sophisticated autoencoder adept at encoding both textual and visual information within a unified latent space. This paper delves into a comparative analysis between CLIP and its recent counterpart, CLOOB. To unravel the intricate distinctions within the embedding spaces crafted by these models, we employ topological data analysis. Our approach encompasses a comprehensive examination of the modality gap drivers, the clustering structures existing across both high and low dimensions, and the pivotal role that dimension collapse plays in shaping their respective embedding spaces. Empirical experiments substantiate the implications of our analyses on downstream performance across various contextual scenarios. Through this investigation, we aim to shed light on the nuanced intricacies that underlie the comparative efficacy of CLIP and CLOOB, offering insights into their respective strengths and weaknesses, and providing a foundation for further refinement and advancement in multimodal model research.","sentences":["Recent strides in multimodal model development have ignited a paradigm shift in the realm of text-to-image generation.","Among these advancements, CLIP stands out as a remarkable achievement which is a sophisticated autoencoder adept at encoding both textual and visual information within a unified latent space.","This paper delves into a comparative analysis between CLIP and its recent counterpart, CLOOB.","To unravel the intricate distinctions within the embedding spaces crafted by these models, we employ topological data analysis.","Our approach encompasses a comprehensive examination of the modality gap drivers, the clustering structures existing across both high and low dimensions, and the pivotal role that dimension collapse plays in shaping their respective embedding spaces.","Empirical experiments substantiate the implications of our analyses on downstream performance across various contextual scenarios.","Through this investigation, we aim to shed light on the nuanced intricacies that underlie the comparative efficacy of CLIP and CLOOB, offering insights into their respective strengths and weaknesses, and providing a foundation for further refinement and advancement in multimodal model research."],"url":"http://arxiv.org/abs/2405.18867v1","category":"cs.AI"}
{"created":"2024-05-29 08:15:56","title":"Empowering Embodied Manipulation: A Bimanual-Mobile Robot Manipulation Dataset for Household Tasks","abstract":"As Embodied AI advances, it increasingly enables robots to handle the complexity of household manipulation tasks more effectively. However, the application of robots in these settings remains limited due to the scarcity of bimanual-mobile robot manipulation datasets. Existing datasets either focus solely on simple grasping tasks using single-arm robots without mobility, or collect sensor data limited to a narrow scope of sensory inputs. As a result, these datasets often fail to encapsulate the intricate and dynamic nature of real-world tasks that bimanual-mobile robots are expected to perform. To address these limitations, we introduce BRMData, a Bimanual-mobile Robot Manipulation Dataset designed specifically for household applications. The dataset includes 10 diverse household tasks, ranging from simple single-arm manipulation to more complex dual-arm and mobile manipulations. It is collected using multi-view and depth-sensing data acquisition strategies. Human-robot interactions and multi-object manipulations are integrated into the task designs to closely simulate real-world household applications. Moreover, we present a Manipulation Efficiency Score (MES) metric to evaluate both the precision and efficiency of robot manipulation methods. BRMData aims to drive the development of versatile robot manipulation technologies, specifically focusing on advancing imitation learning methods from human demonstrations. The dataset is now open-sourced and available at https://embodiedrobot.github.io/, enhancing research and development efforts in the field of Embodied Manipulation.","sentences":["As Embodied AI advances, it increasingly enables robots to handle the complexity of household manipulation tasks more effectively.","However, the application of robots in these settings remains limited due to the scarcity of bimanual-mobile robot manipulation datasets.","Existing datasets either focus solely on simple grasping tasks using single-arm robots without mobility, or collect sensor data limited to a narrow scope of sensory inputs.","As a result, these datasets often fail to encapsulate the intricate and dynamic nature of real-world tasks that bimanual-mobile robots are expected to perform.","To address these limitations, we introduce BRMData, a Bimanual-mobile Robot Manipulation Dataset designed specifically for household applications.","The dataset includes 10 diverse household tasks, ranging from simple single-arm manipulation to more complex dual-arm and mobile manipulations.","It is collected using multi-view and depth-sensing data acquisition strategies.","Human-robot interactions and multi-object manipulations are integrated into the task designs to closely simulate real-world household applications.","Moreover, we present a Manipulation Efficiency Score (MES) metric to evaluate both the precision and efficiency of robot manipulation methods.","BRMData aims to drive the development of versatile robot manipulation technologies, specifically focusing on advancing imitation learning methods from human demonstrations.","The dataset is now open-sourced and available at https://embodiedrobot.github.io/, enhancing research and development efforts in the field of Embodied Manipulation."],"url":"http://arxiv.org/abs/2405.18860v1","category":"cs.RO"}
{"created":"2024-05-29 08:14:34","title":"High-SNR Comparison of Linear Precoding and DPC in RIS-Aided MIMO Broadcast Channels","abstract":"We compare dirty paper coding (DPC) and linear precoding methods in a reconfigurable intelligent surface (RIS)- aided high-signal-to-noise ratio (SNR) scenario, where the channel between the base station (BS) and the RIS is dominated by a line-of-sight (LOS) component. Furthermore, we consider two groups of users where one group can be efficiently served by the BS, whereas the other one has a negligible direct channel and has to be served via the RIS. These considerations allow us to analytically show fundamental differences between DPC and linear methods. In particular, our analysis addresses two essential aspects, i.e., the orthogonality of the BS-RIS channel with the direct channel and an interference term that is present only for linear precoding techniques. The interference term leads to strong limitations for the linear method, especially for random or statistical phase shifts. Moreover, we discuss under which circumstances this interference term is negligible and in which scenarios DPC and linear precoding lead to the same performance.","sentences":["We compare dirty paper coding (DPC) and linear precoding methods in a reconfigurable intelligent surface (RIS)- aided high-signal-to-noise ratio (SNR) scenario, where the channel between the base station (BS) and the RIS is dominated by a line-of-sight (LOS) component.","Furthermore, we consider two groups of users where one group can be efficiently served by the BS, whereas the other one has a negligible direct channel and has to be served via the RIS.","These considerations allow us to analytically show fundamental differences between DPC and linear methods.","In particular, our analysis addresses two essential aspects, i.e., the orthogonality of the BS-RIS channel with the direct channel and an interference term that is present only for linear precoding techniques.","The interference term leads to strong limitations for the linear method, especially for random or statistical phase shifts.","Moreover, we discuss under which circumstances this interference term is negligible and in which scenarios DPC and linear precoding lead to the same performance."],"url":"http://arxiv.org/abs/2405.18859v1","category":"eess.SP"}
{"created":"2024-05-29 08:03:36","title":"LetsMap: Unsupervised Representation Learning for Semantic BEV Mapping","abstract":"Semantic Bird's Eye View (BEV) maps offer a rich representation with strong occlusion reasoning for various decision making tasks in autonomous driving. However, most BEV mapping approaches employ a fully supervised learning paradigm that relies on large amounts of human-annotated BEV ground truth data. In this work, we address this limitation by proposing the first unsupervised representation learning approach to generate semantic BEV maps from a monocular frontal view (FV) image in a label-efficient manner. Our approach pretrains the network to independently reason about scene geometry and scene semantics using two disjoint neural pathways in an unsupervised manner and then finetunes it for the task of semantic BEV mapping using only a small fraction of labels in the BEV. We achieve label-free pretraining by exploiting spatial and temporal consistency of FV images to learn scene geometry while relying on a novel temporal masked autoencoder formulation to encode the scene representation. Extensive evaluations on the KITTI-360 and nuScenes datasets demonstrate that our approach performs on par with the existing state-of-the-art approaches while using only 1% of BEV labels and no additional labeled data.","sentences":["Semantic Bird's Eye View (BEV) maps offer a rich representation with strong occlusion reasoning for various decision making tasks in autonomous driving.","However, most BEV mapping approaches employ a fully supervised learning paradigm that relies on large amounts of human-annotated BEV ground truth data.","In this work, we address this limitation by proposing the first unsupervised representation learning approach to generate semantic BEV maps from a monocular frontal view (FV) image in a label-efficient manner.","Our approach pretrains the network to independently reason about scene geometry and scene semantics using two disjoint neural pathways in an unsupervised manner and then finetunes it for the task of semantic BEV mapping using only a small fraction of labels in the BEV.","We achieve label-free pretraining by exploiting spatial and temporal consistency of FV images to learn scene geometry while relying on a novel temporal masked autoencoder formulation to encode the scene representation.","Extensive evaluations on the KITTI-360 and nuScenes datasets demonstrate that our approach performs on par with the existing state-of-the-art approaches while using only 1% of BEV labels and no additional labeled data."],"url":"http://arxiv.org/abs/2405.18852v1","category":"cs.CV"}
{"created":"2024-05-29 07:59:06","title":"Anomaly Detection by Context Contrasting","abstract":"Anomaly Detection focuses on identifying samples that deviate from the norm. When working with high-dimensional data such as images, a crucial requirement for detecting anomalous patterns is learning lower-dimensional representations that capture normal concepts seen during training. Recent advances in self-supervised learning have shown great promise in this regard. However, many of the most successful self-supervised anomaly detection methods assume prior knowledge about the structure of anomalies and leverage synthetic anomalies during training. Yet, in many real-world applications, we do not know what to expect from unseen data, and we can solely leverage knowledge about normal data. In this work, we propose Con2, which addresses this problem by setting normal training data into distinct contexts while preserving its normal properties, letting us observe the data from different perspectives. Unseen normal data consequently adheres to learned context representations while anomalies fail to do so, letting us detect them without any knowledge about anomalies during training. Our experiments demonstrate that our approach achieves state-of-the-art performance on various benchmarks while exhibiting superior performance in a more realistic healthcare setting, where knowledge about potential anomalies is often scarce.","sentences":["Anomaly Detection focuses on identifying samples that deviate from the norm.","When working with high-dimensional data such as images, a crucial requirement for detecting anomalous patterns is learning lower-dimensional representations that capture normal concepts seen during training.","Recent advances in self-supervised learning have shown great promise in this regard.","However, many of the most successful self-supervised anomaly detection methods assume prior knowledge about the structure of anomalies and leverage synthetic anomalies during training.","Yet, in many real-world applications, we do not know what to expect from unseen data, and we can solely leverage knowledge about normal data.","In this work, we propose Con2, which addresses this problem by setting normal training data into distinct contexts while preserving its normal properties, letting us observe the data from different perspectives.","Unseen normal data consequently adheres to learned context representations while anomalies fail to do so, letting us detect them without any knowledge about anomalies during training.","Our experiments demonstrate that our approach achieves state-of-the-art performance on various benchmarks while exhibiting superior performance in a more realistic healthcare setting, where knowledge about potential anomalies is often scarce."],"url":"http://arxiv.org/abs/2405.18848v1","category":"cs.LG"}
{"created":"2024-05-29 07:56:08","title":"Simulation, Modelling and Classification of Wiki Contributors: Spotting The Good, The Bad, and The Ugly","abstract":"Data crowdsourcing is a data acquisition process where groups of voluntary contributors feed platforms with highly relevant data ranging from news, comments, and media to knowledge and classifications. It typically processes user-generated data streams to provide and refine popular services such as wikis, collaborative maps, e-commerce sites, and social networks. Nevertheless, this modus operandi raises severe concerns regarding ill-intentioned data manipulation in adversarial environments. This paper presents a simulation, modelling, and classification approach to automatically identify human and non-human (bots) as well as benign and malign contributors by using data fabrication to balance classes within experimental data sets, data stream modelling to build and update contributor profiles and, finally, autonomic data stream classification. By employing WikiVoyage - a free worldwide wiki travel guide open to contribution from the general public - as a testbed, our approach proves to significantly boost the confidence and quality of the classifier by using a class-balanced data stream, comprising both real and synthetic data. Our empirical results show that the proposed method distinguishes between benign and malign bots as well as human contributors with a classification accuracy of up to 92 %.","sentences":["Data crowdsourcing is a data acquisition process where groups of voluntary contributors feed platforms with highly relevant data ranging from news, comments, and media to knowledge and classifications.","It typically processes user-generated data streams to provide and refine popular services such as wikis, collaborative maps, e-commerce sites, and social networks.","Nevertheless, this modus operandi raises severe concerns regarding ill-intentioned data manipulation in adversarial environments.","This paper presents a simulation, modelling, and classification approach to automatically identify human and non-human (bots) as well as benign and malign contributors by using data fabrication to balance classes within experimental data sets, data stream modelling to build and update contributor profiles and, finally, autonomic data stream classification.","By employing WikiVoyage - a free worldwide wiki travel guide open to contribution from the general public - as a testbed, our approach proves to significantly boost the confidence and quality of the classifier by using a class-balanced data stream, comprising both real and synthetic data.","Our empirical results show that the proposed method distinguishes between benign and malign bots as well as human contributors with a classification accuracy of up to 92 %."],"url":"http://arxiv.org/abs/2405.18845v1","category":"cs.CL"}
{"created":"2024-05-29 07:54:17","title":"Optical IRS for Visible Light Communication: Modeling, Design, and Open Issues","abstract":"Optical intelligent reflecting surface (OIRS) offers a new and effective approach to resolving the line-of-sight blockage issue in visible light communication (VLC) by enabling redirection of light to bypass obstacles, thereby dramatically enhancing indoor VLC coverage and reliability. This article provides a comprehensive overview of OIRS for VLC, including channel modeling, design techniques, and open issues. First, we present the characteristics of OIRS-reflected channels and introduce two practical models, namely, optics model and association model, which are then compared in terms of applicable conditions, configuration methods, and channel parameters. Next, under the more practically appealing association model, we discuss the main design techniques for OIRS-aided VLC systems, including beam alignment, channel estimation, and OIRS reflection optimization. Finally, open issues are identified to stimulate future research in this area.","sentences":["Optical intelligent reflecting surface (OIRS) offers a new and effective approach to resolving the line-of-sight blockage issue in visible light communication (VLC) by enabling redirection of light to bypass obstacles, thereby dramatically enhancing indoor VLC coverage and reliability.","This article provides a comprehensive overview of OIRS for VLC, including channel modeling, design techniques, and open issues.","First, we present the characteristics of OIRS-reflected channels and introduce two practical models, namely, optics model and association model, which are then compared in terms of applicable conditions, configuration methods, and channel parameters.","Next, under the more practically appealing association model, we discuss the main design techniques for OIRS-aided VLC systems, including beam alignment, channel estimation, and OIRS reflection optimization.","Finally, open issues are identified to stimulate future research in this area."],"url":"http://arxiv.org/abs/2405.18844v1","category":"cs.IT"}
{"created":"2024-05-29 07:50:47","title":"Data-driven Machinery Fault Detection: A Comprehensive Review","abstract":"In this era of advanced manufacturing, it's now more crucial than ever to diagnose machine faults as early as possible to guarantee their safe and efficient operation. With the massive surge in industrial big data and advancement in sensing and computational technologies, data-driven Machinery Fault Diagnosis (MFD) solutions based on machine/deep learning approaches have been used ubiquitously in manufacturing. Timely and accurately identifying faulty machine signals is vital in industrial applications for which many relevant solutions have been proposed and are reviewed in many articles. Despite the availability of numerous solutions and reviews on MFD, existing works often lack several aspects. Most of the available literature has limited applicability in a wide range of manufacturing settings due to their concentration on a particular type of equipment or method of analysis. Additionally, discussions regarding the challenges associated with implementing data-driven approaches, such as dealing with noisy data, selecting appropriate features, and adapting models to accommodate new or unforeseen faults, are often superficial or completely overlooked. Thus, this survey provides a comprehensive review of the articles using different types of machine learning approaches for the detection and diagnosis of various types of machinery faults, highlights their strengths and limitations, provides a review of the methods used for condition-based analyses, comprehensively discusses the available machinery fault datasets, introduces future researchers to the possible challenges they have to encounter while using these approaches for MFD and recommends the probable solutions to mitigate those problems. The future research prospects are also pointed out for a better understanding of the field. We believe this article will help researchers and contribute to the further development of the field.","sentences":["In this era of advanced manufacturing, it's now more crucial than ever to diagnose machine faults as early as possible to guarantee their safe and efficient operation.","With the massive surge in industrial big data and advancement in sensing and computational technologies, data-driven Machinery Fault Diagnosis (MFD) solutions based on machine/deep learning approaches have been used ubiquitously in manufacturing.","Timely and accurately identifying faulty machine signals is vital in industrial applications for which many relevant solutions have been proposed and are reviewed in many articles.","Despite the availability of numerous solutions and reviews on MFD, existing works often lack several aspects.","Most of the available literature has limited applicability in a wide range of manufacturing settings due to their concentration on a particular type of equipment or method of analysis.","Additionally, discussions regarding the challenges associated with implementing data-driven approaches, such as dealing with noisy data, selecting appropriate features, and adapting models to accommodate new or unforeseen faults, are often superficial or completely overlooked.","Thus, this survey provides a comprehensive review of the articles using different types of machine learning approaches for the detection and diagnosis of various types of machinery faults, highlights their strengths and limitations, provides a review of the methods used for condition-based analyses, comprehensively discusses the available machinery fault datasets, introduces future researchers to the possible challenges they have to encounter while using these approaches for MFD and recommends the probable solutions to mitigate those problems.","The future research prospects are also pointed out for a better understanding of the field.","We believe this article will help researchers and contribute to the further development of the field."],"url":"http://arxiv.org/abs/2405.18843v1","category":"cs.AI"}
{"created":"2024-05-29 07:23:29","title":"MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models","abstract":"Mixture-of-Experts (MoE) large language models (LLM) have memory requirements that often exceed the GPU memory capacity, requiring costly parameter movement from secondary memories to the GPU for expert computation. In this work, we present Mixture of Near-Data Experts (MoNDE), a near-data computing solution that efficiently enables MoE LLM inference. MoNDE reduces the volume of MoE parameter movement by transferring only the $\\textit{hot}$ experts to the GPU, while computing the remaining $\\textit{cold}$ experts inside the host memory device. By replacing the transfers of massive expert parameters with the ones of small activations, MoNDE enables far more communication-efficient MoE inference, thereby resulting in substantial speedups over the existing parameter offloading frameworks for both encoder and decoder operations.","sentences":["Mixture-of-Experts (MoE) large language models (LLM) have memory requirements that often exceed the GPU memory capacity, requiring costly parameter movement from secondary memories to the GPU for expert computation.","In this work, we present Mixture of Near-Data Experts (MoNDE), a near-data computing solution that efficiently enables MoE LLM inference.","MoNDE reduces the volume of MoE parameter movement by transferring only the $\\textit{hot}$ experts to the GPU, while computing the remaining $\\textit{cold}$ experts inside the host memory device.","By replacing the transfers of massive expert parameters with the ones of small activations, MoNDE enables far more communication-efficient MoE inference, thereby resulting in substantial speedups over the existing parameter offloading frameworks for both encoder and decoder operations."],"url":"http://arxiv.org/abs/2405.18832v1","category":"cs.LG"}
{"created":"2024-05-29 07:09:00","title":"Why Reinforcement Learning in Energy Systems Needs Explanations","abstract":"With economic development, the complexity of infrastructure has increased drastically. Similarly, with the shift from fossil fuels to renewable sources of energy, there is a dire need for such systems that not only predict and forecast with accuracy but also help in understanding the process of predictions. Artificial intelligence and machine learning techniques have helped in finding out wellperforming solutions to different problems in the energy sector. However, the usage of state-of-the-art techniques like reinforcement learning is not surprisingly convincing. This paper discusses the application of reinforcement techniques in energy systems and how explanations of these models can be helpful","sentences":["With economic development, the complexity of infrastructure has increased drastically.","Similarly, with the shift from fossil fuels to renewable sources of energy, there is a dire need for such systems that not only predict and forecast with accuracy but also help in understanding the process of predictions.","Artificial intelligence and machine learning techniques have helped in finding out wellperforming solutions to different problems in the energy sector.","However, the usage of state-of-the-art techniques like reinforcement learning is not surprisingly convincing.","This paper discusses the application of reinforcement techniques in energy systems and how explanations of these models can be helpful"],"url":"http://arxiv.org/abs/2405.18823v1","category":"cs.AI"}
{"created":"2024-05-29 07:00:28","title":"Diffeomorphic interpolation for efficient persistence-based topological optimization","abstract":"Topological Data Analysis (TDA) provides a pipeline to extract quantitative topological descriptors from structured objects. This enables the definition of topological loss functions, which assert to what extent a given object exhibits some topological properties. These losses can then be used to perform topological optimizationvia gradient descent routines. While theoretically sounded, topological optimization faces an important challenge: gradients tend to be extremely sparse, in the sense that the loss function typically depends on only very few coordinates of the input object, yielding dramatically slow optimization schemes in practice.Focusing on the central case of topological optimization for point clouds, we propose in this work to overcome this limitation using diffeomorphic interpolation, turning sparse gradients into smooth vector fields defined on the whole space, with quantifiable Lipschitz constants. In particular, we show that our approach combines efficiently with subsampling techniques routinely used in TDA, as the diffeomorphism derived from the gradient computed on a subsample can be used to update the coordinates of the full input object, allowing us to perform topological optimization on point clouds at an unprecedented scale. Finally, we also showcase the relevance of our approach for black-box autoencoder (AE) regularization, where we aim at enforcing topological priors on the latent spaces associated to fixed, pre-trained, black-box AE models, and where we show thatlearning a diffeomorphic flow can be done once and then re-applied to new data in linear time (while vanilla topological optimization has to be re-run from scratch). Moreover, reverting the flow allows us to generate data by sampling the topologically-optimized latent space directly, yielding better interpretability of the model.","sentences":["Topological Data Analysis (TDA) provides a pipeline to extract quantitative topological descriptors from structured objects.","This enables the definition of topological loss functions, which assert to what extent a given object exhibits some topological properties.","These losses can then be used to perform topological optimizationvia gradient descent routines.","While theoretically sounded, topological optimization faces an important challenge: gradients tend to be extremely sparse, in the sense that the loss function typically depends on only very few coordinates of the input object, yielding dramatically slow optimization schemes in practice.","Focusing on the central case of topological optimization for point clouds, we propose in this work to overcome this limitation using diffeomorphic interpolation, turning sparse gradients into smooth vector fields defined on the whole space, with quantifiable Lipschitz constants.","In particular, we show that our approach combines efficiently with subsampling techniques routinely used in TDA, as the diffeomorphism derived from the gradient computed on a subsample can be used to update the coordinates of the full input object, allowing us to perform topological optimization on point clouds at an unprecedented scale.","Finally, we also showcase the relevance of our approach for black-box autoencoder (AE) regularization, where we aim at enforcing topological priors on the latent spaces associated to fixed, pre-trained, black-box AE models, and where we show thatlearning a diffeomorphic flow can be done once and then re-applied to new data in linear time (while vanilla topological optimization has to be re-run from scratch).","Moreover, reverting the flow allows us to generate data by sampling the topologically-optimized latent space directly, yielding better interpretability of the model."],"url":"http://arxiv.org/abs/2405.18820v1","category":"cs.AI"}
{"created":"2024-05-29 06:55:36","title":"Design and Implementation of a New Apparatus for Astrochemistry: Kinetic Measurements of the CH + OCS Reaction and Frequency Comb Spectroscopy in a Cold Uniform Supersonic Flow","abstract":"We present the development of a new astrochemical research tool HILTRAC, the Highly Instrumented Low Temperature ReAction Chamber. The instrument is based on a pulsed form of the CRESU (Cin\\'etique de R\\'eaction en \\'Ecoulement Supersonique Uniforme, meaning reaction kinetics in a uniform supersonic flow) apparatus, with the aim of collecting kinetics and spectroscopic information on gas phase chemical reactions important in interstellar space or planetary atmospheres. We discuss the apparatus design and its flexibility, the implementation of pulsed laser photolysis followed by laser induced fluorescence (PLP-LIF), and the first implementation of direct infrared frequency comb spectroscopy (DFCS) coupled to the uniform supersonic flow. Achievable flow temperatures range from 32(3) - 111(9) K, characterising a total of five Laval nozzles for use with N2 and Ar buffer gases by pressure impact measurements. These results were further validated using LIF and DFCS measurements of the CH radical and OCS, respectively. Spectroscopic constants and linelists for OCS are reported for the 1001 band near $2890 - 2940 cm^{-1}$ for both $OC^{32}S$ and $OC^{34}S$, measured using DFCS. Additional peaks in the spectrum are tentatively assigned to the OCS-Ar complex. The first reaction rate coefficients for the CH + OCS reaction measured between 32(3) K and 58(5) K are reported. The reaction rate coefficient at 32(3) K was measured to be $3.9(4) \\times 10^{10} cm^3 molecule^{-1} s^{-1}$ and the reaction was found to exhibit no observable temperature dependence over this low temperature range.","sentences":["We present the development of a new astrochemical research tool HILTRAC, the Highly Instrumented Low Temperature ReAction Chamber.","The instrument is based on a pulsed form of the CRESU (Cin\\'etique de R\\'eaction en \\'Ecoulement Supersonique Uniforme, meaning reaction kinetics in a uniform supersonic flow) apparatus, with the aim of collecting kinetics and spectroscopic information on gas phase chemical reactions important in interstellar space or planetary atmospheres.","We discuss the apparatus design and its flexibility, the implementation of pulsed laser photolysis followed by laser induced fluorescence (PLP-LIF), and the first implementation of direct infrared frequency comb spectroscopy (DFCS) coupled to the uniform supersonic flow.","Achievable flow temperatures range from 32(3) - 111(9) K, characterising a total of five Laval nozzles for use with N2 and Ar buffer gases by pressure impact measurements.","These results were further validated using LIF and DFCS measurements of the CH radical and OCS, respectively.","Spectroscopic constants and linelists for OCS are reported for the 1001 band near $2890 - 2940 cm^{-1}$ for both $OC^{32}S$ and $OC^{34}S$, measured using DFCS.","Additional peaks in the spectrum are tentatively assigned to the OCS-Ar complex.","The first reaction rate coefficients for the CH + OCS reaction measured between 32(3) K and 58(5) K are reported.","The reaction rate coefficient at 32(3)","K was measured to be $3.9(4)","\\times 10^{10} cm^3 molecule^{-1} s^{-1}$ and the reaction was found to exhibit no observable temperature dependence over this low temperature range."],"url":"http://arxiv.org/abs/2405.18814v1","category":"physics.chem-ph"}
{"created":"2024-05-29 06:53:18","title":"UniPTS: A Unified Framework for Proficient Post-Training Sparsity","abstract":"Post-training Sparsity (PTS) is a recently emerged avenue that chases efficient network sparsity with limited data in need. Existing PTS methods, however, undergo significant performance degradation compared with traditional methods that retrain the sparse networks via the whole dataset, especially at high sparsity ratios. In this paper, we attempt to reconcile this disparity by transposing three cardinal factors that profoundly alter the performance of conventional sparsity into the context of PTS. Our endeavors particularly comprise (1) A base-decayed sparsity objective that promotes efficient knowledge transferring from dense network to the sparse counterpart. (2) A reducing-regrowing search algorithm designed to ascertain the optimal sparsity distribution while circumventing overfitting to the small calibration set in PTS. (3) The employment of dynamic sparse training predicated on the preceding aspects, aimed at comprehensively optimizing the sparsity structure while ensuring training stability. Our proposed framework, termed UniPTS, is validated to be much superior to existing PTS methods across extensive benchmarks. As an illustration, it amplifies the performance of POT, a recently proposed recipe, from 3.9% to 68.6% when pruning ResNet-50 at 90% sparsity ratio on ImageNet. We release the code of our paper at https://github.com/xjjxmu/UniPTS.","sentences":["Post-training Sparsity (PTS) is a recently emerged avenue that chases efficient network sparsity with limited data in need.","Existing PTS methods, however, undergo significant performance degradation compared with traditional methods that retrain the sparse networks via the whole dataset, especially at high sparsity ratios.","In this paper, we attempt to reconcile this disparity by transposing three cardinal factors that profoundly alter the performance of conventional sparsity into the context of PTS.","Our endeavors particularly comprise (1) A base-decayed sparsity objective that promotes efficient knowledge transferring from dense network to the sparse counterpart.","(2) A reducing-regrowing search algorithm designed to ascertain the optimal sparsity distribution while circumventing overfitting to the small calibration set in PTS.","(3) The employment of dynamic sparse training predicated on the preceding aspects, aimed at comprehensively optimizing the sparsity structure while ensuring training stability.","Our proposed framework, termed UniPTS, is validated to be much superior to existing PTS methods across extensive benchmarks.","As an illustration, it amplifies the performance of POT, a recently proposed recipe, from 3.9% to 68.6% when pruning ResNet-50 at 90% sparsity ratio on ImageNet.","We release the code of our paper at https://github.com/xjjxmu/UniPTS."],"url":"http://arxiv.org/abs/2405.18810v1","category":"cs.CV"}
{"created":"2024-05-29 06:50:13","title":"BRACTIVE: A Brain Activation Approach to Human Visual Brain Learning","abstract":"The human brain is a highly efficient processing unit, and understanding how it works can inspire new algorithms and architectures in machine learning. In this work, we introduce a novel framework named Brain Activation Network (BRACTIVE), a transformer-based approach to studying the human visual brain. The main objective of BRACTIVE is to align the visual features of subjects with corresponding brain representations via fMRI signals. It allows us to identify the brain's Regions of Interest (ROI) of the subjects. Unlike previous brain research methods, which can only identify ROIs for one subject at a time and are limited by the number of subjects, BRACTIVE automatically extends this identification to multiple subjects and ROIs. Our experiments demonstrate that BRACTIVE effectively identifies person-specific regions of interest, such as face and body-selective areas, aligning with neuroscience findings and indicating potential applicability to various object categories. More importantly, we found that leveraging human visual brain activity to guide deep neural networks enhances performance across various benchmarks. It encourages the potential of BRACTIVE in both neuroscience and machine intelligence studies.","sentences":["The human brain is a highly efficient processing unit, and understanding how it works can inspire new algorithms and architectures in machine learning.","In this work, we introduce a novel framework named Brain Activation Network (BRACTIVE), a transformer-based approach to studying the human visual brain.","The main objective of BRACTIVE is to align the visual features of subjects with corresponding brain representations via fMRI signals.","It allows us to identify the brain's Regions of Interest (ROI) of the subjects.","Unlike previous brain research methods, which can only identify ROIs for one subject at a time and are limited by the number of subjects, BRACTIVE automatically extends this identification to multiple subjects and ROIs.","Our experiments demonstrate that BRACTIVE effectively identifies person-specific regions of interest, such as face and body-selective areas, aligning with neuroscience findings and indicating potential applicability to various object categories.","More importantly, we found that leveraging human visual brain activity to guide deep neural networks enhances performance across various benchmarks.","It encourages the potential of BRACTIVE in both neuroscience and machine intelligence studies."],"url":"http://arxiv.org/abs/2405.18808v1","category":"cs.CV"}
{"created":"2024-05-29 06:49:09","title":"Emergence of bidirectional cell laning from collective contact guidance","abstract":"Directed collective cell migration is central in morphogenesis, wound healing and cancer progression1,2. Although it is well-accepted that the molecular anisotropy of the micro-environment guides this migration3,4, its impact on the pattern of the cell flows remains largely unexplored. Studying confluent human bronchial epithelial cells (HBECs) in vitro, we show that subcellular microgrooves elicit a polar mode of collective migration in millimeter-long bidirectional lanes that are much wider than a cell size, even though cell flows are highly disordered on featureless surfaces 5. This directed flocking-like transition6,7 can be accounted for by a hydrodynamic theory of active polar fluids and corresponding numerical simulations. This model further predicts that anisotropic friction resulting from the grooves lowers the threshold of the transition, which we confirm experimentally. Therefore, microscopic anisotropy of the environment not only directs the collective motion of the cells in the easy direction, but also shapes the cell migration pattern. Flow patterns induced by collective contact guidance are thus markedly different from those induced by supracellular confinement8, demonstrating that all length-scales of the micro-environment must be considered in a comprehensive description of collective migration. Furthermore, artificial microtopographies designed from theoretical considerations can provide a rational strategy to direct cells to specific geometries and functions, which has broad implications, for instance, for tissue engineering strategies in organoid morphogenesis.","sentences":["Directed collective cell migration is central in morphogenesis, wound healing and cancer progression1,2.","Although it is well-accepted that the molecular anisotropy of the micro-environment guides this migration3,4, its impact on the pattern of the cell flows remains largely unexplored.","Studying confluent human bronchial epithelial cells (HBECs) in vitro, we show that subcellular microgrooves elicit a polar mode of collective migration in millimeter-long bidirectional lanes that are much wider than a cell size, even though cell flows are highly disordered on featureless surfaces 5.","This directed flocking-like transition6,7 can be accounted for by a hydrodynamic theory of active polar fluids and corresponding numerical simulations.","This model further predicts that anisotropic friction resulting from the grooves lowers the threshold of the transition, which we confirm experimentally.","Therefore, microscopic anisotropy of the environment not only directs the collective motion of the cells in the easy direction, but also shapes the cell migration pattern.","Flow patterns induced by collective contact guidance are thus markedly different from those induced by supracellular confinement8, demonstrating that all length-scales of the micro-environment must be considered in a comprehensive description of collective migration.","Furthermore, artificial microtopographies designed from theoretical considerations can provide a rational strategy to direct cells to specific geometries and functions, which has broad implications, for instance, for tissue engineering strategies in organoid morphogenesis."],"url":"http://arxiv.org/abs/2405.18807v1","category":"physics.bio-ph"}
{"created":"2024-05-29 06:47:34","title":"Tilde: Teleoperation for Dexterous In-Hand Manipulation Learning with a DeltaHand","abstract":"Dexterous robotic manipulation remains a challenging domain due to its strict demands for precision and robustness on both hardware and software. While dexterous robotic hands have demonstrated remarkable capabilities in complex tasks, efficiently learning adaptive control policies for hands still presents a significant hurdle given the high dimensionalities of hands and tasks. To bridge this gap, we propose Tilde, an imitation learning-based in-hand manipulation system on a dexterous DeltaHand. It leverages 1) a low-cost, configurable, simple-to-control, soft dexterous robotic hand, DeltaHand, 2) a user-friendly, precise, real-time teleoperation interface, TeleHand, and 3) an efficient and generalizable imitation learning approach with diffusion policies. Our proposed TeleHand has a kinematic twin design to the DeltaHand that enables precise one-to-one joint control of the DeltaHand during teleoperation. This facilitates efficient high-quality data collection of human demonstrations in the real world. To evaluate the effectiveness of our system, we demonstrate the fully autonomous closed-loop deployment of diffusion policies learned from demonstrations across seven dexterous manipulation tasks with an average 90% success rate.","sentences":["Dexterous robotic manipulation remains a challenging domain due to its strict demands for precision and robustness on both hardware and software.","While dexterous robotic hands have demonstrated remarkable capabilities in complex tasks, efficiently learning adaptive control policies for hands still presents a significant hurdle given the high dimensionalities of hands and tasks.","To bridge this gap, we propose Tilde, an imitation learning-based in-hand manipulation system on a dexterous DeltaHand.","It leverages 1) a low-cost, configurable, simple-to-control, soft dexterous robotic hand, DeltaHand, 2) a user-friendly, precise, real-time teleoperation interface, TeleHand, and 3) an efficient and generalizable imitation learning approach with diffusion policies.","Our proposed TeleHand has a kinematic twin design to the DeltaHand that enables precise one-to-one joint control of the DeltaHand during teleoperation.","This facilitates efficient high-quality data collection of human demonstrations in the real world.","To evaluate the effectiveness of our system, we demonstrate the fully autonomous closed-loop deployment of diffusion policies learned from demonstrations across seven dexterous manipulation tasks with an average 90% success rate."],"url":"http://arxiv.org/abs/2405.18804v1","category":"cs.RO"}
{"created":"2024-05-29 06:47:00","title":"Information Dynamics in Evolving Networks Based on the Birth-Death Process: Random Drift and Natural Selection Perspective","abstract":"Dynamic processes in complex networks are crucial for better understanding collective behavior in human societies, biological systems, and the internet. In this paper, we first focus on the continuous Markov-based modeling of evolving networks with the birth-death of individuals. A new individual arrives at the group by the Poisson process, while new links are established in the network through either uniform connection or preferential attachment. Moreover, an existing individual has a limited lifespan before leaving the network. We determine stationary topological properties of these networks, including their size and mean degree. To address the effect of the birth-death evolution, we further study the information dynamics in the proposed network model from the random drift and natural selection perspective, based on assumptions of total-stochastic and fitness-driven evolution, respectively. In simulations, we analyze the fixation probability of individual information and find that means of new connections affect the random drift process but do not affect the natural selection process.","sentences":["Dynamic processes in complex networks are crucial for better understanding collective behavior in human societies, biological systems, and the internet.","In this paper, we first focus on the continuous Markov-based modeling of evolving networks with the birth-death of individuals.","A new individual arrives at the group by the Poisson process, while new links are established in the network through either uniform connection or preferential attachment.","Moreover, an existing individual has a limited lifespan before leaving the network.","We determine stationary topological properties of these networks, including their size and mean degree.","To address the effect of the birth-death evolution, we further study the information dynamics in the proposed network model from the random drift and natural selection perspective, based on assumptions of total-stochastic and fitness-driven evolution, respectively.","In simulations, we analyze the fixation probability of individual information and find that means of new connections affect the random drift process but do not affect the natural selection process."],"url":"http://arxiv.org/abs/2405.18803v1","category":"cs.SI"}
{"created":"2024-05-29 06:46:10","title":"Enhancing Security and Privacy in Federated Learning using Update Digests and Voting-Based Defense","abstract":"Federated Learning (FL) is a promising privacy-preserving machine learning paradigm that allows data owners to collaboratively train models while keeping their data localized. Despite its potential, FL faces challenges related to the trustworthiness of both clients and servers, especially in the presence of curious or malicious adversaries. In this paper, we introduce a novel framework named \\underline{\\textbf{F}}ederated \\underline{\\textbf{L}}earning with \\underline{\\textbf{U}}pdate \\underline{\\textbf{D}}igest (FLUD), which addresses the critical issues of privacy preservation and resistance to Byzantine attacks within distributed learning environments. FLUD utilizes an innovative approach, the $\\mathsf{LinfSample}$ method, allowing clients to compute the $l_{\\infty}$ norm across sliding windows of updates as an update digest. This digest enables the server to calculate a shared distance matrix, significantly reducing the overhead associated with Secure Multi-Party Computation (SMPC) by three orders of magnitude while effectively distinguishing between benign and malicious updates. Additionally, FLUD integrates a privacy-preserving, voting-based defense mechanism that employs optimized SMPC protocols to minimize communication rounds. Our comprehensive experiments demonstrate FLUD's effectiveness in countering Byzantine adversaries while incurring low communication and runtime overhead. FLUD offers a scalable framework for secure and reliable FL in distributed environments, facilitating its application in scenarios requiring robust data management and security.","sentences":["Federated Learning (FL) is a promising privacy-preserving machine learning paradigm that allows data owners to collaboratively train models while keeping their data localized.","Despite its potential, FL faces challenges related to the trustworthiness of both clients and servers, especially in the presence of curious or malicious adversaries.","In this paper, we introduce a novel framework named \\underline{\\textbf{F}}ederated \\underline{\\textbf{L}}earning with \\underline{\\textbf{U}}pdate \\underline{\\textbf{D}}igest (FLUD), which addresses the critical issues of privacy preservation and resistance to Byzantine attacks within distributed learning environments.","FLUD utilizes an innovative approach, the $\\mathsf{LinfSample}$ method, allowing clients to compute the $l_{\\infty}$ norm across sliding windows of updates as an update digest.","This digest enables the server to calculate a shared distance matrix, significantly reducing the overhead associated with Secure Multi-Party Computation (SMPC) by three orders of magnitude while effectively distinguishing between benign and malicious updates.","Additionally, FLUD integrates a privacy-preserving, voting-based defense mechanism that employs optimized SMPC protocols to minimize communication rounds.","Our comprehensive experiments demonstrate FLUD's effectiveness in countering Byzantine adversaries while incurring low communication and runtime overhead.","FLUD offers a scalable framework for secure and reliable FL in distributed environments, facilitating its application in scenarios requiring robust data management and security."],"url":"http://arxiv.org/abs/2405.18802v1","category":"cs.CR"}
{"created":"2024-05-29 06:17:33","title":"Kernel Metric Learning for In-Sample Off-Policy Evaluation of Deterministic RL Policies","abstract":"We consider off-policy evaluation (OPE) of deterministic target policies for reinforcement learning (RL) in environments with continuous action spaces. While it is common to use importance sampling for OPE, it suffers from high variance when the behavior policy deviates significantly from the target policy. In order to address this issue, some recent works on OPE proposed in-sample learning with importance resampling. Yet, these approaches are not applicable to deterministic target policies for continuous action spaces. To address this limitation, we propose to relax the deterministic target policy using a kernel and learn the kernel metrics that minimize the overall mean squared error of the estimated temporal difference update vector of an action value function, where the action value function is used for policy evaluation. We derive the bias and variance of the estimation error due to this relaxation and provide analytic solutions for the optimal kernel metric. In empirical studies using various test domains, we show that the OPE with in-sample learning using the kernel with optimized metric achieves significantly improved accuracy than other baselines.","sentences":["We consider off-policy evaluation (OPE) of deterministic target policies for reinforcement learning (RL) in environments with continuous action spaces.","While it is common to use importance sampling for OPE, it suffers from high variance when the behavior policy deviates significantly from the target policy.","In order to address this issue, some recent works on OPE proposed in-sample learning with importance resampling.","Yet, these approaches are not applicable to deterministic target policies for continuous action spaces.","To address this limitation, we propose to relax the deterministic target policy using a kernel and learn the kernel metrics that minimize the overall mean squared error of the estimated temporal difference update vector of an action value function, where the action value function is used for policy evaluation.","We derive the bias and variance of the estimation error due to this relaxation and provide analytic solutions for the optimal kernel metric.","In empirical studies using various test domains, we show that the OPE with in-sample learning using the kernel with optimized metric achieves significantly improved accuracy than other baselines."],"url":"http://arxiv.org/abs/2405.18792v1","category":"cs.LG"}
{"created":"2024-05-29 06:03:31","title":"Modeling and Control of a Novel Bi-Quadcopter with Auxiliary Thruster Mechanism","abstract":"In this paper, a new under-actuated Bi-Quadcopter Unmanned Aerial Vehicle is introduced. The proposed drone configuration can be controlled similar to a Bicopter. The dynamics of the proposed Bi-Quadcopter is developed using the Newton-Euler approach. Using the force decomposition technique, a mapping between the control wrench and actuator inputs is developed. A nonlinear position control is applied for the Bi-Quadcopter using the quaternion-based cascaded attitude controller. The performance of the proposed UAV with the control algorithm is verified through simulations. Finally, the actuator failure scenarios were analyzed.","sentences":["In this paper, a new under-actuated Bi-Quadcopter Unmanned Aerial Vehicle is introduced.","The proposed drone configuration can be controlled similar to a Bicopter.","The dynamics of the proposed Bi-Quadcopter is developed using the Newton-Euler approach.","Using the force decomposition technique, a mapping between the control wrench and actuator inputs is developed.","A nonlinear position control is applied for the Bi-Quadcopter using the quaternion-based cascaded attitude controller.","The performance of the proposed UAV with the control algorithm is verified through simulations.","Finally, the actuator failure scenarios were analyzed."],"url":"http://arxiv.org/abs/2405.18787v1","category":"eess.SY"}
{"created":"2024-05-29 05:39:37","title":"Quantitative Certification of Bias in Large Language Models","abstract":"Large Language Models (LLMs) can produce responses that exhibit social biases and support stereotypes. However, conventional benchmarking is insufficient to thoroughly evaluate LLM bias, as it can not scale to large sets of prompts and provides no guarantees. Therefore, we propose a novel certification framework QuaCer-B (Quantitative Certification of Bias) that provides formal guarantees on obtaining unbiased responses from target LLMs under large sets of prompts. A certificate consists of high-confidence bounds on the probability of obtaining biased responses from the LLM for any set of prompts containing sensitive attributes, sampled from a distribution. We illustrate the bias certification in LLMs for prompts with various prefixes drawn from given distributions. We consider distributions of random token sequences, mixtures of manual jailbreaks, and jailbreaks in the LLM's embedding space to certify its bias. We certify popular LLMs with QuaCer-B and present novel insights into their biases.","sentences":["Large Language Models (LLMs) can produce responses that exhibit social biases and support stereotypes.","However, conventional benchmarking is insufficient to thoroughly evaluate LLM bias, as it can not scale to large sets of prompts and provides no guarantees.","Therefore, we propose a novel certification framework QuaCer-B (Quantitative Certification of Bias) that provides formal guarantees on obtaining unbiased responses from target LLMs under large sets of prompts.","A certificate consists of high-confidence bounds on the probability of obtaining biased responses from the LLM for any set of prompts containing sensitive attributes, sampled from a distribution.","We illustrate the bias certification in LLMs for prompts with various prefixes drawn from given distributions.","We consider distributions of random token sequences, mixtures of manual jailbreaks, and jailbreaks in the LLM's embedding space to certify its bias.","We certify popular LLMs with QuaCer-B and present novel insights into their biases."],"url":"http://arxiv.org/abs/2405.18780v1","category":"cs.AI"}
{"created":"2024-05-29 05:20:02","title":"Leveraging Many-To-Many Relationships for Defending Against Visual-Language Adversarial Attacks","abstract":"Recent studies have revealed that vision-language (VL) models are vulnerable to adversarial attacks for image-text retrieval (ITR). However, existing defense strategies for VL models primarily focus on zero-shot image classification, which do not consider the simultaneous manipulation of image and text, as well as the inherent many-to-many (N:N) nature of ITR, where a single image can be described in numerous ways, and vice versa. To this end, this paper studies defense strategies against adversarial attacks on VL models for ITR for the first time. Particularly, we focus on how to leverage the N:N relationship in ITR to enhance adversarial robustness. We found that, although adversarial training easily overfits to specific one-to-one (1:1) image-text pairs in the train data, diverse augmentation techniques to create one-to-many (1:N) / many-to-one (N:1) image-text pairs can significantly improve adversarial robustness in VL models. Additionally, we show that the alignment of the augmented image-text pairs is crucial for the effectiveness of the defense strategy, and that inappropriate augmentations can even degrade the model's performance. Based on these findings, we propose a novel defense strategy that leverages the N:N relationship in ITR, which effectively generates diverse yet highly-aligned N:N pairs using basic augmentations and generative model-based augmentations. This work provides a novel perspective on defending against adversarial attacks in VL tasks and opens up new research directions for future work.","sentences":["Recent studies have revealed that vision-language (VL) models are vulnerable to adversarial attacks for image-text retrieval (ITR).","However, existing defense strategies for VL models primarily focus on zero-shot image classification, which do not consider the simultaneous manipulation of image and text, as well as the inherent many-to-many (N:N) nature of ITR, where a single image can be described in numerous ways, and vice versa.","To this end, this paper studies defense strategies against adversarial attacks on VL models for ITR for the first time.","Particularly, we focus on how to leverage the N:N relationship in ITR to enhance adversarial robustness.","We found that, although adversarial training easily overfits to specific one-to-one (1:1) image-text pairs in the train data, diverse augmentation techniques to create one-to-many (1:N) / many-to-one (N:1) image-text pairs can significantly improve adversarial robustness in VL models.","Additionally, we show that the alignment of the augmented image-text pairs is crucial for the effectiveness of the defense strategy, and that inappropriate augmentations can even degrade the model's performance.","Based on these findings, we propose a novel defense strategy that leverages the N:N relationship in ITR, which effectively generates diverse yet highly-aligned N:N pairs using basic augmentations and generative model-based augmentations.","This work provides a novel perspective on defending against adversarial attacks in VL tasks and opens up new research directions for future work."],"url":"http://arxiv.org/abs/2405.18770v1","category":"cs.CV"}
{"created":"2024-05-29 05:07:17","title":"2D hydrodynamic simulation of TeraFETs beyond the gradual-channel approximation for transient, large-signal or ultrahigh-frequency simulations","abstract":"In the past decade, detection of THz radiation by plasma-wave-assisted frequency mixing in antenna-coupled field-effect transistors (TeraFETs) -- implemented in various semiconductor material systems (Si CMOS, GaN/AlGaN, GaAs/AlGaAs, graphene, etc.) -- has matured and led to a practically applied detector technology. This has been supported by the development of powerful device simulation tools which take into account relevant collective carrier dynamics and mixing processes in various approximations. These tools mostly model carrier transport in 1D and they are usually geared towards continuous-wave illumination of the device and small-signal response. Depending on their implementation, it may not be possible readily to simulate large-signal and pulsed operation. Another approximation which may lead to unsatisfactory results is the 1D restriction to calculate only the longitudinal electric field components. Especially at the edges of the gate electrode, solving of the 2D Poisson equation promises better results. This contribution introduces a stable way to solve the 2D Poisson equation self-consistently with the hydrodynamic transport equations including the numerically challenging convection term. We employ a well-balanced approximate Harten-Lax-van-Leer-Contact Riemann solver. The approach is well suited for a future treatment of transient and large-signal cases as well as the Dyakonov-Shur instability. The 2D treatment also generically extends the model beyond the gradual-channel approximation and allows to calculate the FET's response at high THz frequencies where the gate-to-channel potential acquires a non-local character. Model calculations are performed for the exemplary case of a 65-nm Si CMOS TeraFET in the isothermal approximation.","sentences":["In the past decade, detection of THz radiation by plasma-wave-assisted frequency mixing in antenna-coupled field-effect transistors (TeraFETs) -- implemented in various semiconductor material systems (Si CMOS, GaN/AlGaN, GaAs/AlGaAs, graphene, etc.) -- has matured and led to a practically applied detector technology.","This has been supported by the development of powerful device simulation tools which take into account relevant collective carrier dynamics and mixing processes in various approximations.","These tools mostly model carrier transport in 1D and they are usually geared towards continuous-wave illumination of the device and small-signal response.","Depending on their implementation, it may not be possible readily to simulate large-signal and pulsed operation.","Another approximation which may lead to unsatisfactory results is the 1D restriction to calculate only the longitudinal electric field components.","Especially at the edges of the gate electrode, solving of the 2D Poisson equation promises better results.","This contribution introduces a stable way to solve the 2D Poisson equation self-consistently with the hydrodynamic transport equations including the numerically challenging convection term.","We employ a well-balanced approximate Harten-Lax-van-Leer-Contact Riemann solver.","The approach is well suited for a future treatment of transient and large-signal cases as well as the Dyakonov-Shur instability.","The 2D treatment also generically extends the model beyond the gradual-channel approximation and allows to calculate the FET's response at high THz frequencies where the gate-to-channel potential acquires a non-local character.","Model calculations are performed for the exemplary case of a 65-nm Si CMOS TeraFET in the isothermal approximation."],"url":"http://arxiv.org/abs/2405.18764v1","category":"physics.comp-ph"}
{"created":"2024-05-29 05:04:07","title":"Inpaint Biases: A Pathway to Accurate and Unbiased Image Generation","abstract":"This paper examines the limitations of advanced text-to-image models in accurately rendering unconventional concepts which are scarcely represented or absent in their training datasets. We identify how these limitations not only confine the creative potential of these models but also pose risks of reinforcing stereotypes. To address these challenges, we introduce the Inpaint Biases framework, which employs user-defined masks and inpainting techniques to enhance the accuracy of image generation, particularly for novel or inaccurately rendered objects. Through experimental validation, we demonstrate how this framework significantly improves the fidelity of generated images to the user's intent, thereby expanding the models' creative capabilities and mitigating the risk of perpetuating biases. Our study contributes to the advancement of text-to-image models as unbiased, versatile tools for creative expression.","sentences":["This paper examines the limitations of advanced text-to-image models in accurately rendering unconventional concepts which are scarcely represented or absent in their training datasets.","We identify how these limitations not only confine the creative potential of these models but also pose risks of reinforcing stereotypes.","To address these challenges, we introduce the Inpaint Biases framework, which employs user-defined masks and inpainting techniques to enhance the accuracy of image generation, particularly for novel or inaccurately rendered objects.","Through experimental validation, we demonstrate how this framework significantly improves the fidelity of generated images to the user's intent, thereby expanding the models' creative capabilities and mitigating the risk of perpetuating biases.","Our study contributes to the advancement of text-to-image models as unbiased, versatile tools for creative expression."],"url":"http://arxiv.org/abs/2405.18762v1","category":"cs.CV"}
{"created":"2024-05-29 04:53:31","title":"Learning to Continually Learn with the Bayesian Principle","abstract":"In the present era of deep learning, continual learning research is mainly focused on mitigating forgetting when training a neural network with stochastic gradient descent on a non-stationary stream of data. On the other hand, in the more classical literature of statistical machine learning, many models have sequential Bayesian update rules that yield the same learning outcome as the batch training, i.e., they are completely immune to catastrophic forgetting. However, they are often overly simple to model complex real-world data. In this work, we adopt the meta-learning paradigm to combine the strong representational power of neural networks and simple statistical models' robustness to forgetting. In our novel meta-continual learning framework, continual learning takes place only in statistical models via ideal sequential Bayesian update rules, while neural networks are meta-learned to bridge the raw data and the statistical models. Since the neural networks remain fixed during continual learning, they are protected from catastrophic forgetting. This approach not only achieves significantly improved performance but also exhibits excellent scalability. Since our approach is domain-agnostic and model-agnostic, it can be applied to a wide range of problems and easily integrated with existing model architectures.","sentences":["In the present era of deep learning, continual learning research is mainly focused on mitigating forgetting when training a neural network with stochastic gradient descent on a non-stationary stream of data.","On the other hand, in the more classical literature of statistical machine learning, many models have sequential Bayesian update rules that yield the same learning outcome as the batch training, i.e., they are completely immune to catastrophic forgetting.","However, they are often overly simple to model complex real-world data.","In this work, we adopt the meta-learning paradigm to combine the strong representational power of neural networks and simple statistical models' robustness to forgetting.","In our novel meta-continual learning framework, continual learning takes place only in statistical models via ideal sequential Bayesian update rules, while neural networks are meta-learned to bridge the raw data and the statistical models.","Since the neural networks remain fixed during continual learning, they are protected from catastrophic forgetting.","This approach not only achieves significantly improved performance but also exhibits excellent scalability.","Since our approach is domain-agnostic and model-agnostic, it can be applied to a wide range of problems and easily integrated with existing model architectures."],"url":"http://arxiv.org/abs/2405.18758v1","category":"cs.LG"}
{"created":"2024-05-29 04:48:11","title":"Provable Contrastive Continual Learning","abstract":"Continual learning requires learning incremental tasks with dynamic data distributions. So far, it has been observed that employing a combination of contrastive loss and distillation loss for training in continual learning yields strong performance. To the best of our knowledge, however, this contrastive continual learning framework lacks convincing theoretical explanations. In this work, we fill this gap by establishing theoretical performance guarantees, which reveal how the performance of the model is bounded by training losses of previous tasks in the contrastive continual learning framework. Our theoretical explanations further support the idea that pre-training can benefit continual learning. Inspired by our theoretical analysis of these guarantees, we propose a novel contrastive continual learning algorithm called CILA, which uses adaptive distillation coefficients for different tasks. These distillation coefficients are easily computed by the ratio between average distillation losses and average contrastive losses from previous tasks. Our method shows great improvement on standard benchmarks and achieves new state-of-the-art performance.","sentences":["Continual learning requires learning incremental tasks with dynamic data distributions.","So far, it has been observed that employing a combination of contrastive loss and distillation loss for training in continual learning yields strong performance.","To the best of our knowledge, however, this contrastive continual learning framework lacks convincing theoretical explanations.","In this work, we fill this gap by establishing theoretical performance guarantees, which reveal how the performance of the model is bounded by training losses of previous tasks in the contrastive continual learning framework.","Our theoretical explanations further support the idea that pre-training can benefit continual learning.","Inspired by our theoretical analysis of these guarantees, we propose a novel contrastive continual learning algorithm called CILA, which uses adaptive distillation coefficients for different tasks.","These distillation coefficients are easily computed by the ratio between average distillation losses and average contrastive losses from previous tasks.","Our method shows great improvement on standard benchmarks and achieves new state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.18756v1","category":"cs.LG"}
{"created":"2024-05-29 04:29:12","title":"On the Limits of Multi-modal Meta-Learning with Auxiliary Task Modulation Using Conditional Batch Normalization","abstract":"Few-shot learning aims to learn representations that can tackle novel tasks given a small number of examples. Recent studies show that cross-modal learning can improve representations for few-shot classification. More specifically, language is a rich modality that can be used to guide visual learning. In this work, we experiment with a multi-modal architecture for few-shot learning that consists of three components: a classifier, an auxiliary network, and a bridge network. While the classifier performs the main classification task, the auxiliary network learns to predict language representations from the same input, and the bridge network transforms high-level features of the auxiliary network into modulation parameters for layers of the few-shot classifier using conditional batch normalization. The bridge should encourage a form of lightweight semantic alignment between language and vision which could be useful for the classifier. However, after evaluating the proposed approach on two popular few-shot classification benchmarks we find that a) the improvements do not reproduce across benchmarks, and b) when they do, the improvements are due to the additional compute and parameters introduced by the bridge network. We contribute insights and recommendations for future work in multi-modal meta-learning, especially when using language representations.","sentences":["Few-shot learning aims to learn representations that can tackle novel tasks given a small number of examples.","Recent studies show that cross-modal learning can improve representations for few-shot classification.","More specifically, language is a rich modality that can be used to guide visual learning.","In this work, we experiment with a multi-modal architecture for few-shot learning that consists of three components: a classifier, an auxiliary network, and a bridge network.","While the classifier performs the main classification task, the auxiliary network learns to predict language representations from the same input, and the bridge network transforms high-level features of the auxiliary network into modulation parameters for layers of the few-shot classifier using conditional batch normalization.","The bridge should encourage a form of lightweight semantic alignment between language and vision which could be useful for the classifier.","However, after evaluating the proposed approach on two popular few-shot classification benchmarks we find that a) the improvements do not reproduce across benchmarks, and b) when they do, the improvements are due to the additional compute and parameters introduced by the bridge network.","We contribute insights and recommendations for future work in multi-modal meta-learning, especially when using language representations."],"url":"http://arxiv.org/abs/2405.18751v1","category":"cs.CV"}
{"created":"2024-05-29 04:04:36","title":"Musical Phrase Segmentation via Grammatical Induction","abstract":"We outline a solution to the challenge of musical phrase segmentation that uses grammatical induction algorithms, a class of algorithms which infer a context-free grammar from an input sequence. We analyze the performance of five grammatical induction algorithms on three datasets using various musical viewpoint combinations. Our experiments show that the LONGESTFIRST algorithm achieves the best F1 scores across all three datasets and that input encodings that include the duration viewpoint result in the best performance.","sentences":["We outline a solution to the challenge of musical phrase segmentation that uses grammatical induction algorithms, a class of algorithms which infer a context-free grammar from an input sequence.","We analyze the performance of five grammatical induction algorithms on three datasets using various musical viewpoint combinations.","Our experiments show that the LONGESTFIRST algorithm achieves the best F1 scores across all three datasets and that input encodings that include the duration viewpoint result in the best performance."],"url":"http://arxiv.org/abs/2405.18742v1","category":"cs.AI"}
{"created":"2024-05-29 04:04:05","title":"Genshin: General Shield for Natural Language Processing with Large Language Models","abstract":"Large language models (LLMs) like ChatGPT, Gemini, or LLaMA have been trending recently, demonstrating considerable advancement and generalizability power in countless domains. However, LLMs create an even bigger black box exacerbating opacity, with interpretability limited to few approaches. The uncertainty and opacity embedded in LLMs' nature restrict their application in high-stakes domains like financial fraud, phishing, etc. Current approaches mainly rely on traditional textual classification with posterior interpretable algorithms, suffering from attackers who may create versatile adversarial samples to break the system's defense, forcing users to make trade-offs between efficiency and robustness. To address this issue, we propose a novel cascading framework called Genshin (General Shield for Natural Language Processing with Large Language Models), utilizing LLMs as defensive one-time plug-ins. Unlike most applications of LLMs that try to transform text into something new or structural, Genshin uses LLMs to recover text to its original state. Genshin aims to combine the generalizability of the LLM, the discrimination of the median model, and the interpretability of the simple model. Our experiments on the task of sentimental analysis and spam detection have shown fatal flaws of the current median models and exhilarating results on LLMs' recovery ability, demonstrating that Genshin is both effective and efficient. In our ablation study, we unearth several intriguing observations. Utilizing the LLM defender, a tool derived from the 4th paradigm, we have reproduced BERT's 15% optimal mask rate results in the 3rd paradigm of NLP. Additionally, when employing the LLM as a potential adversarial tool, attackers are capable of executing effective attacks that are nearly semantically lossless.","sentences":["Large language models (LLMs) like ChatGPT, Gemini, or LLaMA have been trending recently, demonstrating considerable advancement and generalizability power in countless domains.","However, LLMs create an even bigger black box exacerbating opacity, with interpretability limited to few approaches.","The uncertainty and opacity embedded in LLMs' nature restrict their application in high-stakes domains like financial fraud, phishing, etc.","Current approaches mainly rely on traditional textual classification with posterior interpretable algorithms, suffering from attackers who may create versatile adversarial samples to break the system's defense, forcing users to make trade-offs between efficiency and robustness.","To address this issue, we propose a novel cascading framework called Genshin (General Shield for Natural Language Processing with Large Language Models), utilizing LLMs as defensive one-time plug-ins.","Unlike most applications of LLMs that try to transform text into something new or structural, Genshin uses LLMs to recover text to its original state.","Genshin aims to combine the generalizability of the LLM, the discrimination of the median model, and the interpretability of the simple model.","Our experiments on the task of sentimental analysis and spam detection have shown fatal flaws of the current median models and exhilarating results on LLMs' recovery ability, demonstrating that Genshin is both effective and efficient.","In our ablation study, we unearth several intriguing observations.","Utilizing the LLM defender, a tool derived from the 4th paradigm, we have reproduced BERT's 15% optimal mask rate results in the 3rd paradigm of NLP.","Additionally, when employing the LLM as a potential adversarial tool, attackers are capable of executing effective attacks that are nearly semantically lossless."],"url":"http://arxiv.org/abs/2405.18741v1","category":"cs.CL"}
{"created":"2024-05-29 03:27:30","title":"Efficient Learning in Chinese Checkers: Comparing Parameter Sharing in Multi-Agent Reinforcement Learning","abstract":"We show that multi-agent reinforcement learning (MARL) with full parameter sharing outperforms independent and partially shared architectures in the competitive perfect-information homogenous game of Chinese Checkers. To run our experiments, we develop a new MARL environment: variable-size, six-player Chinese Checkers. This custom environment was developed in PettingZoo and supports all traditional rules of the game including chaining jumps. This is, to the best of our knowledge, the first implementation of Chinese Checkers that remains faithful to the true game.   Chinese Checkers is difficult to learn due to its large branching factor and potentially infinite horizons. We borrow the concept of branching actions (submoves) from complex action spaces in other RL domains, where a submove may not end a player's turn immediately. This drastically reduces the dimensionality of the action space. Our observation space is inspired by AlphaGo with many binary game boards stacked in a 3D array to encode information.   The PettingZoo environment, training and evaluation logic, and analysis scripts can be found on \\href{https://github.com/noahadhikari/pettingzoo-chinese-checkers}{Github}.","sentences":["We show that multi-agent reinforcement learning (MARL) with full parameter sharing outperforms independent and partially shared architectures in the competitive perfect-information homogenous game of Chinese Checkers.","To run our experiments, we develop a new MARL environment: variable-size, six-player Chinese Checkers.","This custom environment was developed in PettingZoo and supports all traditional rules of the game including chaining jumps.","This is, to the best of our knowledge, the first implementation of Chinese Checkers that remains faithful to the true game.   ","Chinese Checkers is difficult to learn due to its large branching factor and potentially infinite horizons.","We borrow the concept of branching actions (submoves) from complex action spaces in other RL domains, where a submove may not end a player's turn immediately.","This drastically reduces the dimensionality of the action space.","Our observation space is inspired by AlphaGo with many binary game boards stacked in a 3D array to encode information.   ","The PettingZoo environment, training and evaluation logic, and analysis scripts can be found on \\href{https://github.com/noahadhikari/pettingzoo-chinese-checkers}{Github}."],"url":"http://arxiv.org/abs/2405.18733v1","category":"cs.AI"}
{"created":"2024-05-29 03:23:34","title":"Gemini & Physical World: Large Language Models Can Estimate the Intensity of Earthquake Shaking from Multi-Modal Social Media Posts","abstract":"This paper presents a novel approach for estimating the ground shaking intensity using social media data and CCTV footage. Employing the Gemini Pro (Reid et al. 2024) model, a multi-modal language model, we demonstrate the ability to extract relevant information from unstructured data utilizing generative AI and natural language processing. The model output, in the form of Modified Mercalli Intensity (MMI) values, align well with independent observational data. Furthermore, our results suggest that beyond its advanced visual and auditory understanding abilities, Gemini appears to utilize additional sources of knowledge, including a simplified understanding of the general relationship between earthquake magnitude, distance, and MMI intensity, which it presumably acquired during its training, in its reasoning and decision-making processes. These findings raise intriguing questions about the extent of Gemini's general understanding of the physical world and its phenomena. The ability of Gemini to generate results consistent with established scientific knowledge highlights the potential of LLMs like Gemini in augmenting our understanding of complex physical phenomena such as earthquakes. More specifically, the results of this study highlight the potential of LLMs like Gemini to revolutionize citizen seismology by enabling rapid, effective, and flexible analysis of crowdsourced data from eyewitness accounts for assessing earthquake impact and providing crisis situational awareness. This approach holds great promise for improving early warning systems, disaster response, and overall resilience in earthquake-prone regions. This study provides a significant step toward harnessing the power of social media and AI for earthquake disaster mitigation.","sentences":["This paper presents a novel approach for estimating the ground shaking intensity using social media data and CCTV footage.","Employing the Gemini Pro (Reid et al. 2024) model, a multi-modal language model, we demonstrate the ability to extract relevant information from unstructured data utilizing generative AI and natural language processing.","The model output, in the form of Modified Mercalli Intensity (MMI) values, align well with independent observational data.","Furthermore, our results suggest that beyond its advanced visual and auditory understanding abilities, Gemini appears to utilize additional sources of knowledge, including a simplified understanding of the general relationship between earthquake magnitude, distance, and MMI intensity, which it presumably acquired during its training, in its reasoning and decision-making processes.","These findings raise intriguing questions about the extent of Gemini's general understanding of the physical world and its phenomena.","The ability of Gemini to generate results consistent with established scientific knowledge highlights the potential of LLMs like Gemini in augmenting our understanding of complex physical phenomena such as earthquakes.","More specifically, the results of this study highlight the potential of LLMs like Gemini to revolutionize citizen seismology by enabling rapid, effective, and flexible analysis of crowdsourced data from eyewitness accounts for assessing earthquake impact and providing crisis situational awareness.","This approach holds great promise for improving early warning systems, disaster response, and overall resilience in earthquake-prone regions.","This study provides a significant step toward harnessing the power of social media and AI for earthquake disaster mitigation."],"url":"http://arxiv.org/abs/2405.18732v1","category":"physics.geo-ph"}
{"created":"2024-05-29 03:21:09","title":"VBIM-Net: Variational Born Iterative Network for Inverse Scattering Problems","abstract":"Recently, studies have shown the potential of integrating field-type iterative methods with deep learning (DL) techniques in solving inverse scattering problems (ISPs). In this article, we propose a novel Variational Born Iterative Network, namely, VBIM-Net, to solve the full-wave ISPs with significantly improved flexibility and inversion quality. The proposed VBIM-Net emulates the alternating updates of the total electric field and the contrast in the variational Born iterative method (VBIM) by multiple layers of subnetworks. We embed the calculation of the contrast variation into each of the subnetworks, converting the scattered field residual into an approximate contrast variation and then enhancing it by a U-Net, thus avoiding the requirement of matched measurement dimension and grid resolution as in existing approaches. The total field and contrast of each layer's output is supervised in the loss function of VBIM-Net, which guarantees the physical interpretability of variables of the subnetworks. In addition, we design a training scheme with extra noise to enhance the model's stability. Extensive numerical results on synthetic and experimental data both verify the inversion quality, generalization ability, and robustness of the proposed VBIM-Net. This work may provide some new inspiration for the design of efficient field-type DL schemes.","sentences":["Recently, studies have shown the potential of integrating field-type iterative methods with deep learning (DL) techniques in solving inverse scattering problems (ISPs).","In this article, we propose a novel Variational Born Iterative Network, namely, VBIM-Net, to solve the full-wave ISPs with significantly improved flexibility and inversion quality.","The proposed VBIM-Net emulates the alternating updates of the total electric field and the contrast in the variational Born iterative method (VBIM) by multiple layers of subnetworks.","We embed the calculation of the contrast variation into each of the subnetworks, converting the scattered field residual into an approximate contrast variation and then enhancing it by a U-Net, thus avoiding the requirement of matched measurement dimension and grid resolution as in existing approaches.","The total field and contrast of each layer's output is supervised in the loss function of VBIM-Net, which guarantees the physical interpretability of variables of the subnetworks.","In addition, we design a training scheme with extra noise to enhance the model's stability.","Extensive numerical results on synthetic and experimental data both verify the inversion quality, generalization ability, and robustness of the proposed VBIM-Net.","This work may provide some new inspiration for the design of efficient field-type DL schemes."],"url":"http://arxiv.org/abs/2405.18731v1","category":"eess.SP"}
{"created":"2024-05-29 03:19:59","title":"Preferred-Action-Optimized Diffusion Policies for Offline Reinforcement Learning","abstract":"Offline reinforcement learning (RL) aims to learn optimal policies from previously collected datasets. Recently, due to their powerful representational capabilities, diffusion models have shown significant potential as policy models for offline RL issues. However, previous offline RL algorithms based on diffusion policies generally adopt weighted regression to improve the policy. This approach optimizes the policy only using the collected actions and is sensitive to Q-values, which limits the potential for further performance enhancement. To this end, we propose a novel preferred-action-optimized diffusion policy for offline RL. In particular, an expressive conditional diffusion model is utilized to represent the diverse distribution of a behavior policy. Meanwhile, based on the diffusion model, preferred actions within the same behavior distribution are automatically generated through the critic function. Moreover, an anti-noise preference optimization is designed to achieve policy improvement by using the preferred actions, which can adapt to noise-preferred actions for stable training. Extensive experiments demonstrate that the proposed method provides competitive or superior performance compared to previous state-of-the-art offline RL methods, particularly in sparse reward tasks such as Kitchen and AntMaze. Additionally, we empirically prove the effectiveness of anti-noise preference optimization.","sentences":["Offline reinforcement learning (RL) aims to learn optimal policies from previously collected datasets.","Recently, due to their powerful representational capabilities, diffusion models have shown significant potential as policy models for offline RL issues.","However, previous offline RL algorithms based on diffusion policies generally adopt weighted regression to improve the policy.","This approach optimizes the policy only using the collected actions and is sensitive to Q-values, which limits the potential for further performance enhancement.","To this end, we propose a novel preferred-action-optimized diffusion policy for offline RL.","In particular, an expressive conditional diffusion model is utilized to represent the diverse distribution of a behavior policy.","Meanwhile, based on the diffusion model, preferred actions within the same behavior distribution are automatically generated through the critic function.","Moreover, an anti-noise preference optimization is designed to achieve policy improvement by using the preferred actions, which can adapt to noise-preferred actions for stable training.","Extensive experiments demonstrate that the proposed method provides competitive or superior performance compared to previous state-of-the-art offline RL methods, particularly in sparse reward tasks such as Kitchen and AntMaze.","Additionally, we empirically prove the effectiveness of anti-noise preference optimization."],"url":"http://arxiv.org/abs/2405.18729v1","category":"cs.LG"}
{"created":"2024-05-29 03:17:16","title":"CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control","abstract":"Retrieval-augmented generation (RAG) has emerged as a promising solution for mitigating hallucinations of large language models (LLMs) with retrieved external knowledge. Adaptive RAG enhances this approach by dynamically assessing the retrieval necessity, aiming to balance external and internal knowledge usage. However, existing adaptive RAG methods primarily realize retrieval on demand by relying on superficially verbalize-based or probability-based feedback of LLMs, or directly fine-tuning LLMs via carefully crafted datasets, resulting in unreliable retrieval necessity decisions, heavy extra costs, and sub-optimal response generation. We present the first attempts to delve into the internal states of LLMs to mitigate such issues by introducing an effective probe-guided adaptive RAG framework, termed CtrlA. Specifically, CtrlA employs an honesty probe to regulate the LLM's behavior by manipulating its representations for increased honesty, and a confidence probe to monitor the internal states of LLM and assess confidence levels, determining the retrieval necessity during generation. Experiments show that CtrlA is superior to existing adaptive RAG methods on a diverse set of tasks, the honesty control can effectively make LLMs more honest and confidence monitoring is proven to be a promising indicator of retrieval trigger. Our codes are available at https://github.com/HSLiu-Initial/CtrlA.git.","sentences":["Retrieval-augmented generation (RAG) has emerged as a promising solution for mitigating hallucinations of large language models (LLMs) with retrieved external knowledge.","Adaptive RAG enhances this approach by dynamically assessing the retrieval necessity, aiming to balance external and internal knowledge usage.","However, existing adaptive RAG methods primarily realize retrieval on demand by relying on superficially verbalize-based or probability-based feedback of LLMs, or directly fine-tuning LLMs via carefully crafted datasets, resulting in unreliable retrieval necessity decisions, heavy extra costs, and sub-optimal response generation.","We present the first attempts to delve into the internal states of LLMs to mitigate such issues by introducing an effective probe-guided adaptive RAG framework, termed CtrlA.","Specifically, CtrlA employs an honesty probe to regulate the LLM's behavior by manipulating its representations for increased honesty, and a confidence probe to monitor the internal states of LLM and assess confidence levels, determining the retrieval necessity during generation.","Experiments show that CtrlA is superior to existing adaptive RAG methods on a diverse set of tasks, the honesty control can effectively make LLMs more honest and confidence monitoring is proven to be a promising indicator of retrieval trigger.","Our codes are available at https://github.com/HSLiu-Initial/CtrlA.git."],"url":"http://arxiv.org/abs/2405.18727v1","category":"cs.CL"}
{"created":"2024-05-29 03:10:21","title":"Adapting Differential Molecular Representation with Hierarchical Prompts for Multi-label Property Prediction","abstract":"Accurate prediction of molecular properties is critical in the field of drug discovery. However, existing methods do not fully consider the fact that molecules in the real world usually possess multiple property labels, and complex high-order relationships may exist among these labels. Therefore, molecular representation learning models should generate differential molecular representations that consider multi-granularity correlation information among tasks. To this end, our research introduces a Hierarchical Prompted Molecular Representation Learning Framework (HiPM), which enhances the differential expression of tasks in molecular representations through task-aware prompts, and utilizes shared information among labels to mitigate negative transfer between different tasks. HiPM primarily consists of two core components: the Molecular Representation Encoder (MRE) and the Task-Aware Prompter (TAP). The MRE employs a hierarchical message-passing network architecture to capture molecular features at both the atomic and motif levels, while the TAP uses agglomerative hierarchical clustering to build a prompt tree that reflects the affinity and distinctiveness of tasks, enabling the model to effectively handle the complexity of multi-label property predictions. Extensive experiments demonstrate that HiPM achieves state-of-the-art performance across various multi-label datasets, offering a new perspective on multi-label molecular representation learning.","sentences":["Accurate prediction of molecular properties is critical in the field of drug discovery.","However, existing methods do not fully consider the fact that molecules in the real world usually possess multiple property labels, and complex high-order relationships may exist among these labels.","Therefore, molecular representation learning models should generate differential molecular representations that consider multi-granularity correlation information among tasks.","To this end, our research introduces a Hierarchical Prompted Molecular Representation Learning Framework (HiPM), which enhances the differential expression of tasks in molecular representations through task-aware prompts, and utilizes shared information among labels to mitigate negative transfer between different tasks.","HiPM primarily consists of two core components: the Molecular Representation Encoder (MRE) and the Task-Aware Prompter (TAP).","The MRE employs a hierarchical message-passing network architecture to capture molecular features at both the atomic and motif levels, while the TAP uses agglomerative hierarchical clustering to build a prompt tree that reflects the affinity and distinctiveness of tasks, enabling the model to effectively handle the complexity of multi-label property predictions.","Extensive experiments demonstrate that HiPM achieves state-of-the-art performance across various multi-label datasets, offering a new perspective on multi-label molecular representation learning."],"url":"http://arxiv.org/abs/2405.18724v1","category":"q-bio.QM"}
{"created":"2024-05-29 03:08:30","title":"Conformal Depression Prediction","abstract":"While existing depression recognition methods based on deep learning show promise, their practical application is hindered by the lack of trustworthiness, as these deep models are often deployed as \\textit{black box} models, leaving us uncertain about the confidence of the model predictions. For high-risk clinical applications like depression recognition, uncertainty quantification is essential in decision-making. In this paper, we introduce conformal depression prediction (CDP), a depression recognition method with uncertainty quantification based on conformal prediction (CP), giving valid confidence intervals with theoretical coverage guarantees for the model predictions. CDP is a plug-and-play module that requires neither model retraining nor an assumption about the depression data distribution. As CDP provides only an average performance guarantee across all inputs rather than per-input performance guarantee, we propose CDP-ACC, an improved conformal prediction with approximate conditional coverage. CDP-ACC firstly estimates the prediction distribution through neighborhood relaxation, and then introduces a conformal score function by constructing nested sequences, so as to provide tighter prediction interval for each specific input. We empirically demonstrate the application of uncertainty quantification in depression recognition, and the effectiveness and superiority of CDP and CDP-ACC on the AVEC 2013 and AVEC 2014 datasets","sentences":["While existing depression recognition methods based on deep learning show promise, their practical application is hindered by the lack of trustworthiness, as these deep models are often deployed as \\textit{black box} models, leaving us uncertain about the confidence of the model predictions.","For high-risk clinical applications like depression recognition, uncertainty quantification is essential in decision-making.","In this paper, we introduce conformal depression prediction (CDP), a depression recognition method with uncertainty quantification based on conformal prediction (CP), giving valid confidence intervals with theoretical coverage guarantees for the model predictions.","CDP is a plug-and-play module that requires neither model retraining nor an assumption about the depression data distribution.","As CDP provides only an average performance guarantee across all inputs rather than per-input performance guarantee, we propose CDP-ACC, an improved conformal prediction with approximate conditional coverage.","CDP-ACC firstly estimates the prediction distribution through neighborhood relaxation, and then introduces a conformal score function by constructing nested sequences, so as to provide tighter prediction interval for each specific input.","We empirically demonstrate the application of uncertainty quantification in depression recognition, and the effectiveness and superiority of CDP and CDP-ACC on the AVEC 2013 and AVEC 2014 datasets"],"url":"http://arxiv.org/abs/2405.18723v1","category":"cs.LG"}
{"created":"2024-05-29 03:05:59","title":"Correctable Landmark Discovery via Large Models for Vision-Language Navigation","abstract":"Vision-Language Navigation (VLN) requires the agent to follow language instructions to reach a target position. A key factor for successful navigation is to align the landmarks implied in the instruction with diverse visual observations. However, previous VLN agents fail to perform accurate modality alignment especially in unexplored scenes, since they learn from limited navigation data and lack sufficient open-world alignment knowledge. In this work, we propose a new VLN paradigm, called COrrectable LaNdmark DiScOvery via Large ModEls (CONSOLE). In CONSOLE, we cast VLN as an open-world sequential landmark discovery problem, by introducing a novel correctable landmark discovery scheme based on two large models ChatGPT and CLIP. Specifically, we use ChatGPT to provide rich open-world landmark cooccurrence commonsense, and conduct CLIP-driven landmark discovery based on these commonsense priors. To mitigate the noise in the priors due to the lack of visual constraints, we introduce a learnable cooccurrence scoring module, which corrects the importance of each cooccurrence according to actual observations for accurate landmark discovery. We further design an observation enhancement strategy for an elegant combination of our framework with different VLN agents, where we utilize the corrected landmark features to obtain enhanced observation features for action decision. Extensive experimental results on multiple popular VLN benchmarks (R2R, REVERIE, R4R, RxR) show the significant superiority of CONSOLE over strong baselines. Especially, our CONSOLE establishes the new state-of-the-art results on R2R and R4R in unseen scenarios. Code is available at https://github.com/expectorlin/CONSOLE.","sentences":["Vision-Language Navigation (VLN) requires the agent to follow language instructions to reach a target position.","A key factor for successful navigation is to align the landmarks implied in the instruction with diverse visual observations.","However, previous VLN agents fail to perform accurate modality alignment especially in unexplored scenes, since they learn from limited navigation data and lack sufficient open-world alignment knowledge.","In this work, we propose a new VLN paradigm, called COrrectable LaNdmark DiScOvery via Large ModEls (CONSOLE).","In CONSOLE, we cast VLN as an open-world sequential landmark discovery problem, by introducing a novel correctable landmark discovery scheme based on two large models ChatGPT and CLIP.","Specifically, we use ChatGPT to provide rich open-world landmark cooccurrence commonsense, and conduct CLIP-driven landmark discovery based on these commonsense priors.","To mitigate the noise in the priors due to the lack of visual constraints, we introduce a learnable cooccurrence scoring module, which corrects the importance of each cooccurrence according to actual observations for accurate landmark discovery.","We further design an observation enhancement strategy for an elegant combination of our framework with different VLN agents, where we utilize the corrected landmark features to obtain enhanced observation features for action decision.","Extensive experimental results on multiple popular VLN benchmarks (R2R, REVERIE, R4R, RxR) show the significant superiority of CONSOLE over strong baselines.","Especially, our CONSOLE establishes the new state-of-the-art results on R2R and R4R in unseen scenarios.","Code is available at https://github.com/expectorlin/CONSOLE."],"url":"http://arxiv.org/abs/2405.18721v1","category":"cs.CV"}
{"created":"2024-05-29 03:05:59","title":"Adaptive and Efficient Learning with Blockwise Missing and Semi-Supervised Data","abstract":"Data fusion is an important way to realize powerful and generalizable analyses across multiple sources. However, different capability of data collection across the sources has become a prominent issue in practice. This could result in the blockwise missingness (BM) of covariates troublesome for integration. Meanwhile, the high cost of obtaining gold-standard labels can cause the missingness of response on a large proportion of samples, known as the semi-supervised (SS) problem. In this paper, we consider a challenging scenario confronting both the BM and SS issues, and propose a novel Data-adaptive projecting Estimation approach for data FUsion in the SEmi-supervised setting (DEFUSE). Starting with a complete-data-only estimator, it involves two successive projection steps to reduce its variance without incurring bias. Compared to existing approaches, DEFUSE achieves a two-fold improvement. First, it leverages the BM labeled sample more efficiently through a novel data-adaptive projection approach robust to model misspecification on the missing covariates, leading to better variance reduction. Second, our method further incorporates the large unlabeled sample to enhance the estimation efficiency through imputation and projection. Compared to the previous SS setting with complete covariates, our work reveals a more essential role of the unlabeled sample in the BM setting. These advantages are justified in asymptotic and simulation studies. We also apply DEFUSE for the risk modeling and inference of heart diseases with the MIMIC-III electronic medical record (EMR) data.","sentences":["Data fusion is an important way to realize powerful and generalizable analyses across multiple sources.","However, different capability of data collection across the sources has become a prominent issue in practice.","This could result in the blockwise missingness (BM) of covariates troublesome for integration.","Meanwhile, the high cost of obtaining gold-standard labels can cause the missingness of response on a large proportion of samples, known as the semi-supervised (SS) problem.","In this paper, we consider a challenging scenario confronting both the BM and SS issues, and propose a novel Data-adaptive projecting Estimation approach for data FUsion in the SEmi-supervised setting (DEFUSE).","Starting with a complete-data-only estimator, it involves two successive projection steps to reduce its variance without incurring bias.","Compared to existing approaches, DEFUSE achieves a two-fold improvement.","First, it leverages the BM labeled sample more efficiently through a novel data-adaptive projection approach robust to model misspecification on the missing covariates, leading to better variance reduction.","Second, our method further incorporates the large unlabeled sample to enhance the estimation efficiency through imputation and projection.","Compared to the previous SS setting with complete covariates, our work reveals a more essential role of the unlabeled sample in the BM setting.","These advantages are justified in asymptotic and simulation studies.","We also apply DEFUSE for the risk modeling and inference of heart diseases with the MIMIC-III electronic medical record (EMR) data."],"url":"http://arxiv.org/abs/2405.18722v1","category":"stat.ME"}
{"created":"2024-05-29 02:57:15","title":"Contextual Position Encoding: Learning to Count What's Important","abstract":"The attention mechanism is a critical component of Large Language Models (LLMs) that allows tokens in a sequence to interact with each other, but is order-invariant. Incorporating position encoding (PE) makes it possible to address by position, such as attending to the i-th token. However, current PE methods use token counts to derive position, and thus cannot generalize to higher levels of abstraction, such as attending to the i-th sentence. In this paper, we propose a new position encoding method, Contextual Position Encoding (CoPE), that allows positions to be conditioned on context by incrementing position only on certain tokens determined by the model. This allows more general position addressing such as attending to the $i$-th particular word, noun, or sentence. We show that CoPE can solve the selective copy, counting and Flip-Flop tasks where popular position embeddings fail, and improves perplexity on language modeling and coding tasks.","sentences":["The attention mechanism is a critical component of Large Language Models (LLMs) that allows tokens in a sequence to interact with each other, but is order-invariant.","Incorporating position encoding (PE) makes it possible to address by position, such as attending to the i-th token.","However, current PE methods use token counts to derive position, and thus cannot generalize to higher levels of abstraction, such as attending to the i-th sentence.","In this paper, we propose a new position encoding method, Contextual Position Encoding (CoPE), that allows positions to be conditioned on context by incrementing position only on certain tokens determined by the model.","This allows more general position addressing such as attending to the $i$-th particular word, noun, or sentence.","We show that CoPE can solve the selective copy, counting and Flip-Flop tasks where popular position embeddings fail, and improves perplexity on language modeling and coding tasks."],"url":"http://arxiv.org/abs/2405.18719v1","category":"cs.CL"}
{"created":"2024-05-29 02:44:12","title":"Calibrating Reasoning in Language Models with Internal Consistency","abstract":"Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious mistakes and contradictions, raising doubts about their ability to robustly process and utilize generated rationales. In this work, we investigate CoT reasoning in LLMs through the lens of internal representations, focusing on how these representations are influenced by generated rationales. Our preliminary analysis reveals that while generated rationales improve answer accuracy, inconsistencies emerge between the model's internal representations in middle layers and those in final layers, potentially undermining the reliability of their reasoning processes. To address this, we propose internal consistency as a measure of the model's confidence by examining the agreement of latent predictions decoded from intermediate layers. Extensive empirical studies across different models and datasets demonstrate that internal consistency effectively distinguishes between correct and incorrect reasoning paths. Motivated by this, we propose a new approach to calibrate CoT reasoning by up-weighting reasoning paths with high internal consistency, resulting in a significant boost in reasoning performance. Further analysis uncovers distinct patterns in attention and feed-forward modules across layers, providing insights into the emergence of internal inconsistency. In summary, our results demonstrate the potential of using internal representations for self-evaluation of LLMs.","sentences":["Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting that elicits verbalized reasoning.","However, LLMs often generate text with obvious mistakes and contradictions, raising doubts about their ability to robustly process and utilize generated rationales.","In this work, we investigate CoT reasoning in LLMs through the lens of internal representations, focusing on how these representations are influenced by generated rationales.","Our preliminary analysis reveals that while generated rationales improve answer accuracy, inconsistencies emerge between the model's internal representations in middle layers and those in final layers, potentially undermining the reliability of their reasoning processes.","To address this, we propose internal consistency as a measure of the model's confidence by examining the agreement of latent predictions decoded from intermediate layers.","Extensive empirical studies across different models and datasets demonstrate that internal consistency effectively distinguishes between correct and incorrect reasoning paths.","Motivated by this, we propose a new approach to calibrate CoT reasoning by up-weighting reasoning paths with high internal consistency, resulting in a significant boost in reasoning performance.","Further analysis uncovers distinct patterns in attention and feed-forward modules across layers, providing insights into the emergence of internal inconsistency.","In summary, our results demonstrate the potential of using internal representations for self-evaluation of LLMs."],"url":"http://arxiv.org/abs/2405.18711v1","category":"cs.AI"}
{"created":"2024-05-29 02:42:23","title":"To FP8 and Back Again: Quantifying the Effects of Reducing Precision on LLM Training Stability","abstract":"The massive computational costs associated with large language model (LLM) pretraining have spurred great interest in reduced-precision floating-point representations to accelerate the process. As a result, the BrainFloat16 (BF16) precision has become the de facto standard for LLM training, with hardware support included in recent accelerators. This trend has gone even further in the latest processors, where FP8 has recently been introduced. However, prior experience with FP16, which was found to be less stable than BF16, raises concerns as to whether FP8, with even fewer bits than FP16, can be a cost-effective option for LLM training. We argue that reduced-precision training schemes must have similar training stability and hyperparameter sensitivities to their higher-precision counterparts in order to be cost-effective. However, we find that currently available methods for FP8 training are not robust enough to allow their use as economical replacements. This prompts us to investigate the stability of reduced-precision LLM training in terms of robustness across random seeds and learning rates. To this end, we propose new evaluation techniques and a new metric for quantifying loss landscape sharpness in autoregressive language models. By simulating incremental bit reductions in floating-point representations, we analyze the relationship between representational power and training stability with the intent of aiding future research into the field.","sentences":["The massive computational costs associated with large language model (LLM) pretraining have spurred great interest in reduced-precision floating-point representations to accelerate the process.","As a result, the BrainFloat16 (BF16) precision has become the de facto standard for LLM training, with hardware support included in recent accelerators.","This trend has gone even further in the latest processors, where FP8 has recently been introduced.","However, prior experience with FP16, which was found to be less stable than BF16, raises concerns as to whether FP8, with even fewer bits than FP16, can be a cost-effective option for LLM training.","We argue that reduced-precision training schemes must have similar training stability and hyperparameter sensitivities to their higher-precision counterparts in order to be cost-effective.","However, we find that currently available methods for FP8 training are not robust enough to allow their use as economical replacements.","This prompts us to investigate the stability of reduced-precision LLM training in terms of robustness across random seeds and learning rates.","To this end, we propose new evaluation techniques and a new metric for quantifying loss landscape sharpness in autoregressive language models.","By simulating incremental bit reductions in floating-point representations, we analyze the relationship between representational power and training stability with the intent of aiding future research into the field."],"url":"http://arxiv.org/abs/2405.18710v1","category":"cs.LG"}
{"created":"2024-05-29 02:35:23","title":"Cognitive Evolutionary Learning to Select Feature Interactions for Recommender Systems","abstract":"Feature interaction selection is a fundamental problem in commercial recommender systems. Most approaches equally enumerate all features and interactions by the same pre-defined operation under expert guidance. Their recommendation is unsatisfactory sometimes due to the following issues: (1)~They cannot ensure the learning abilities of models because their architectures are poorly adaptable to tasks and data; (2)~Useless features and interactions can bring unnecessary noise and complicate the training process. In this paper, we aim to adaptively evolve the model to select appropriate operations, features, and interactions under task guidance. Inspired by the evolution and functioning of natural organisms, we propose a novel \\textsl{Cognitive EvoLutionary Learning (CELL)} framework, where cognitive ability refers to a property of organisms that allows them to react and survive in diverse environments. It consists of three stages, i.e., DNA search, genome search, and model functioning. Specifically, if we regard the relationship between models and tasks as the relationship between organisms and natural environments, interactions of feature pairs can be analogous to double-stranded DNA, of which relevant features and interactions can be analogous to genomes. Along this line, we diagnose the fitness of the model on operations, features, and interactions to simulate the survival rates of organisms for natural selection. We show that CELL can adaptively evolve into different models for different tasks and data, which enables practitioners to access off-the-shelf models. Extensive experiments on four real-world datasets demonstrate that CELL significantly outperforms state-of-the-art baselines. Also, we conduct synthetic experiments to ascertain that CELL can consistently discover the pre-defined interaction patterns for feature pairs.","sentences":["Feature interaction selection is a fundamental problem in commercial recommender systems.","Most approaches equally enumerate all features and interactions by the same pre-defined operation under expert guidance.","Their recommendation is unsatisfactory sometimes due to the following issues: (1)~They cannot ensure the learning abilities of models because their architectures are poorly adaptable to tasks and data; (2)~Useless features and interactions can bring unnecessary noise and complicate the training process.","In this paper, we aim to adaptively evolve the model to select appropriate operations, features, and interactions under task guidance.","Inspired by the evolution and functioning of natural organisms, we propose a novel \\textsl{Cognitive EvoLutionary Learning (CELL)} framework, where cognitive ability refers to a property of organisms that allows them to react and survive in diverse environments.","It consists of three stages, i.e., DNA search, genome search, and model functioning.","Specifically, if we regard the relationship between models and tasks as the relationship between organisms and natural environments, interactions of feature pairs can be analogous to double-stranded DNA, of which relevant features and interactions can be analogous to genomes.","Along this line, we diagnose the fitness of the model on operations, features, and interactions to simulate the survival rates of organisms for natural selection.","We show that CELL can adaptively evolve into different models for different tasks and data, which enables practitioners to access off-the-shelf models.","Extensive experiments on four real-world datasets demonstrate that CELL significantly outperforms state-of-the-art baselines.","Also, we conduct synthetic experiments to ascertain that CELL can consistently discover the pre-defined interaction patterns for feature pairs."],"url":"http://arxiv.org/abs/2405.18708v1","category":"cs.AI"}
{"created":"2024-05-29 02:34:38","title":"Adaptive and Parallel Split Federated Learning in Vehicular Edge Computing","abstract":"Vehicular edge intelligence (VEI) is a promising paradigm for enabling future intelligent transportation systems by accommodating artificial intelligence (AI) at the vehicular edge computing (VEC) system. Federated learning (FL) stands as one of the fundamental technologies facilitating collaborative model training locally and aggregation, while safeguarding the privacy of vehicle data in VEI. However, traditional FL faces challenges in adapting to vehicle heterogeneity, training large models on resource-constrained vehicles, and remaining susceptible to model weight privacy leakage. Meanwhile, split learning (SL) is proposed as a promising collaborative learning framework which can mitigate the risk of model wights leakage, and release the training workload on vehicles. SL sequentially trains a model between a vehicle and an edge cloud (EC) by dividing the entire model into a vehicle-side model and an EC-side model at a given cut layer. In this work, we combine the advantages of SL and FL to develop an Adaptive Split Federated Learning scheme for Vehicular Edge Computing (ASFV). The ASFV scheme adaptively splits the model and parallelizes the training process, taking into account mobile vehicle selection and resource allocation. Our extensive simulations, conducted on non-independent and identically distributed data, demonstrate that the proposed ASFV solution significantly reduces training latency compared to existing benchmarks, while adapting to network dynamics and vehicles' mobility.","sentences":["Vehicular edge intelligence (VEI) is a promising paradigm for enabling future intelligent transportation systems by accommodating artificial intelligence (AI) at the vehicular edge computing (VEC) system.","Federated learning (FL) stands as one of the fundamental technologies facilitating collaborative model training locally and aggregation, while safeguarding the privacy of vehicle data in VEI.","However, traditional FL faces challenges in adapting to vehicle heterogeneity, training large models on resource-constrained vehicles, and remaining susceptible to model weight privacy leakage.","Meanwhile, split learning (SL) is proposed as a promising collaborative learning framework which can mitigate the risk of model wights leakage, and release the training workload on vehicles.","SL sequentially trains a model between a vehicle and an edge cloud (EC) by dividing the entire model into a vehicle-side model and an EC-side model at a given cut layer.","In this work, we combine the advantages of SL and FL to develop an Adaptive Split Federated Learning scheme for Vehicular Edge Computing (ASFV).","The ASFV scheme adaptively splits the model and parallelizes the training process, taking into account mobile vehicle selection and resource allocation.","Our extensive simulations, conducted on non-independent and identically distributed data, demonstrate that the proposed ASFV solution significantly reduces training latency compared to existing benchmarks, while adapting to network dynamics and vehicles' mobility."],"url":"http://arxiv.org/abs/2405.18707v1","category":"cs.LG"}
{"created":"2024-05-29 02:27:50","title":"Shot noise in a metal close to Mott transition","abstract":"SrIrO$_3$ is a metallic complex oxide with unusual electronic and magnetic properties believed to originate from electron correlations due to its proximity to Mott metal-insulator transition. However, the nature of its electronic state and the mechanism of metallic conduction remain poorly understood. We demonstrate that shot noise produced by nanoscale SrIrO$_3$ junctions is strongly suppressed, inconsistent with diffusive quasiparticle transport. Analysis of thermal effects and scaling with the junction length reveals that conduction is mediated by collective hopping of electrons almost localized by correlations. Our results provide insight into the non-Fermi liquid state close to Mott transition, and advance shot noise measurements as a powerful technique for the studies of quantum materials.","sentences":["SrIrO$_3$ is a metallic complex oxide with unusual electronic and magnetic properties believed to originate from electron correlations due to its proximity to Mott metal-insulator transition.","However, the nature of its electronic state and the mechanism of metallic conduction remain poorly understood.","We demonstrate that shot noise produced by nanoscale SrIrO$_3$ junctions is strongly suppressed, inconsistent with diffusive quasiparticle transport.","Analysis of thermal effects and scaling with the junction length reveals that conduction is mediated by collective hopping of electrons almost localized by correlations.","Our results provide insight into the non-Fermi liquid state close to Mott transition, and advance shot noise measurements as a powerful technique for the studies of quantum materials."],"url":"http://arxiv.org/abs/2405.18704v1","category":"cond-mat.str-el"}
{"created":"2024-05-29 02:23:33","title":"Near-Field Localization with RIS via Two-Dimensional Signal Path Classification","abstract":"In this paper, we propose two-dimensional signal path classification (2D-SPC) for reconfigurable intelligent surface (RIS)-assisted near-field (NF) localization. In the NF regime, multiple RIS-driven signal paths (SPs) can contribute to precise localization if these are decomposable and the reflected locations on the RIS are known, referred to as SP decomposition (SPD) and SP labeling (SPL), respectively. To this end, each RIS element modulates the incoming SP's phase by shifting it by one of the values in the phase shift profile (PSP) lists satisfying resolution requirements. By interworking with a conventional orthogonal frequency division multiplexing (OFDM) waveform, the user equipment can construct a 2D spectrum map that couples each SPs time of arrival (ToA) and PSP. Then, we design SPL by mapping SPs with the corresponding reflected RIS elements when they share the same PSP. Given two unlabeled SPs, we derive a geometric discriminant from checking whether the current label is correct. It can be extended to more than three SPs by sorting them using pairwise geometric discriminants between adjacent ones. From simulation results, it has been demonstrated that the proposed 2D SPC achieves consistent localization accuracy even if insufficient PSPs are given.","sentences":["In this paper, we propose two-dimensional signal path classification (2D-SPC) for reconfigurable intelligent surface (RIS)-assisted near-field (NF) localization.","In the NF regime, multiple RIS-driven signal paths (SPs) can contribute to precise localization if these are decomposable and the reflected locations on the RIS are known, referred to as SP decomposition (SPD) and SP labeling (SPL), respectively.","To this end, each RIS element modulates the incoming SP's phase by shifting it by one of the values in the phase shift profile (PSP) lists satisfying resolution requirements.","By interworking with a conventional orthogonal frequency division multiplexing (OFDM) waveform, the user equipment can construct a 2D spectrum map that couples each SPs time of arrival (ToA) and PSP.","Then, we design SPL by mapping SPs with the corresponding reflected RIS elements when they share the same PSP.","Given two unlabeled SPs, we derive a geometric discriminant from checking whether the current label is correct.","It can be extended to more than three SPs by sorting them using pairwise geometric discriminants between adjacent ones.","From simulation results, it has been demonstrated that the proposed 2D SPC achieves consistent localization accuracy even if insufficient PSPs are given."],"url":"http://arxiv.org/abs/2405.18701v1","category":"eess.SP"}
{"created":"2024-05-29 02:17:25","title":"Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees","abstract":"The field of risk-constrained reinforcement learning (RCRL) has been developed to effectively reduce the likelihood of worst-case scenarios by explicitly handling risk-measure-based constraints. However, the nonlinearity of risk measures makes it challenging to achieve convergence and optimality. To overcome the difficulties posed by the nonlinearity, we propose a spectral risk measure-constrained RL algorithm, spectral-risk-constrained policy optimization (SRCPO), a bilevel optimization approach that utilizes the duality of spectral risk measures. In the bilevel optimization structure, the outer problem involves optimizing dual variables derived from the risk measures, while the inner problem involves finding an optimal policy given these dual variables. The proposed method, to the best of our knowledge, is the first to guarantee convergence to an optimum in the tabular setting. Furthermore, the proposed method has been evaluated on continuous control tasks and showed the best performance among other RCRL algorithms satisfying the constraints.","sentences":["The field of risk-constrained reinforcement learning (RCRL) has been developed to effectively reduce the likelihood of worst-case scenarios by explicitly handling risk-measure-based constraints.","However, the nonlinearity of risk measures makes it challenging to achieve convergence and optimality.","To overcome the difficulties posed by the nonlinearity, we propose a spectral risk measure-constrained RL algorithm, spectral-risk-constrained policy optimization (SRCPO), a bilevel optimization approach that utilizes the duality of spectral risk measures.","In the bilevel optimization structure, the outer problem involves optimizing dual variables derived from the risk measures, while the inner problem involves finding an optimal policy given these dual variables.","The proposed method, to the best of our knowledge, is the first to guarantee convergence to an optimum in the tabular setting.","Furthermore, the proposed method has been evaluated on continuous control tasks and showed the best performance among other RCRL algorithms satisfying the constraints."],"url":"http://arxiv.org/abs/2405.18698v1","category":"cs.LG"}
{"created":"2024-05-29 02:06:17","title":"DeepHGNN: Study of Graph Neural Network based Forecasting Methods for Hierarchically Related Multivariate Time Series","abstract":"Graph Neural Networks (GNN) have gained significant traction in the forecasting domain, especially for their capacity to simultaneously account for intra-series temporal correlations and inter-series relationships. This paper introduces a novel Hierarchical GNN (DeepHGNN) framework, explicitly designed for forecasting in complex hierarchical structures. The uniqueness of DeepHGNN lies in its innovative graph-based hierarchical interpolation and an end-to-end reconciliation mechanism. This approach ensures forecast accuracy and coherence across various hierarchical levels while sharing signals across them, addressing a key challenge in hierarchical forecasting. A critical insight in hierarchical time series is the variance in forecastability across levels, with upper levels typically presenting more predictable components. DeepHGNN capitalizes on this insight by pooling and leveraging knowledge from all hierarchy levels, thereby enhancing the overall forecast accuracy. Our comprehensive evaluation set against several state-of-the-art models confirm the superior performance of DeepHGNN. This research not only demonstrates DeepHGNN's effectiveness in achieving significantly improved forecast accuracy but also contributes to the understanding of graph-based methods in hierarchical time series forecasting.","sentences":["Graph Neural Networks (GNN) have gained significant traction in the forecasting domain, especially for their capacity to simultaneously account for intra-series temporal correlations and inter-series relationships.","This paper introduces a novel Hierarchical GNN (DeepHGNN) framework, explicitly designed for forecasting in complex hierarchical structures.","The uniqueness of DeepHGNN lies in its innovative graph-based hierarchical interpolation and an end-to-end reconciliation mechanism.","This approach ensures forecast accuracy and coherence across various hierarchical levels while sharing signals across them, addressing a key challenge in hierarchical forecasting.","A critical insight in hierarchical time series is the variance in forecastability across levels, with upper levels typically presenting more predictable components.","DeepHGNN capitalizes on this insight by pooling and leveraging knowledge from all hierarchy levels, thereby enhancing the overall forecast accuracy.","Our comprehensive evaluation set against several state-of-the-art models confirm the superior performance of DeepHGNN.","This research not only demonstrates DeepHGNN's effectiveness in achieving significantly improved forecast accuracy but also contributes to the understanding of graph-based methods in hierarchical time series forecasting."],"url":"http://arxiv.org/abs/2405.18693v1","category":"cs.LG"}
{"created":"2024-05-29 01:49:20","title":"Efficient Preference-based Reinforcement Learning via Aligned Experience Estimation","abstract":"Preference-based reinforcement learning (PbRL) has shown impressive capabilities in training agents without reward engineering. However, a notable limitation of PbRL is its dependency on substantial human feedback. This dependency stems from the learning loop, which entails accurate reward learning compounded with value/policy learning, necessitating a considerable number of samples. To boost the learning loop, we propose SEER, an efficient PbRL method that integrates label smoothing and policy regularization techniques. Label smoothing reduces overfitting of the reward model by smoothing human preference labels. Additionally, we bootstrap a conservative estimate $\\widehat{Q}$ using well-supported state-action pairs from the current replay memory to mitigate overestimation bias and utilize it for policy learning regularization. Our experimental results across a variety of complex tasks, both in online and offline settings, demonstrate that our approach improves feedback efficiency, outperforming state-of-the-art methods by a large margin. Ablation studies further reveal that SEER achieves a more accurate Q-function compared to prior work.","sentences":["Preference-based reinforcement learning (PbRL) has shown impressive capabilities in training agents without reward engineering.","However, a notable limitation of PbRL is its dependency on substantial human feedback.","This dependency stems from the learning loop, which entails accurate reward learning compounded with value/policy learning, necessitating a considerable number of samples.","To boost the learning loop, we propose SEER, an efficient PbRL method that integrates label smoothing and policy regularization techniques.","Label smoothing reduces overfitting of the reward model by smoothing human preference labels.","Additionally, we bootstrap a conservative estimate $\\widehat{Q}$ using well-supported state-action pairs from the current replay memory to mitigate overestimation bias and utilize it for policy learning regularization.","Our experimental results across a variety of complex tasks, both in online and offline settings, demonstrate that our approach improves feedback efficiency, outperforming state-of-the-art methods by a large margin.","Ablation studies further reveal that SEER achieves a more accurate Q-function compared to prior work."],"url":"http://arxiv.org/abs/2405.18688v1","category":"cs.LG"}
{"created":"2024-05-29 01:12:53","title":"Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical Machine Reading Comprehension","abstract":"Large language models (LLMs) have shown remarkable performance on many tasks in different domains. However, their performance in closed-book biomedical machine reading comprehension (MRC) has not been evaluated in depth. In this work, we evaluate GPT on four closed-book biomedical MRC benchmarks. We experiment with different conventional prompting techniques as well as introduce our own novel prompting method. To solve some of the retrieval problems inherent to LLMs, we propose a prompting strategy named Implicit Retrieval Augmented Generation (RAG) that alleviates the need for using vector databases to retrieve important chunks in traditional RAG setups. Moreover, we report qualitative assessments on the natural language generation outputs from our approach. The results show that our new prompting technique is able to get the best performance in two out of four datasets and ranks second in rest of them. Experiments show that modern-day LLMs like GPT even in a zero-shot setting can outperform supervised models, leading to new state-of-the-art (SoTA) results on two of the benchmarks.","sentences":["Large language models (LLMs) have shown remarkable performance on many tasks in different domains.","However, their performance in closed-book biomedical machine reading comprehension (MRC) has not been evaluated in depth.","In this work, we evaluate GPT on four closed-book biomedical MRC benchmarks.","We experiment with different conventional prompting techniques as well as introduce our own novel prompting method.","To solve some of the retrieval problems inherent to LLMs, we propose a prompting strategy named Implicit Retrieval Augmented Generation (RAG) that alleviates the need for using vector databases to retrieve important chunks in traditional RAG setups.","Moreover, we report qualitative assessments on the natural language generation outputs from our approach.","The results show that our new prompting technique is able to get the best performance in two out of four datasets and ranks second in rest of them.","Experiments show that modern-day LLMs like GPT even in a zero-shot setting can outperform supervised models, leading to new state-of-the-art (SoTA) results on two of the benchmarks."],"url":"http://arxiv.org/abs/2405.18682v1","category":"cs.CL"}
{"created":"2024-05-29 01:07:38","title":"A random-key GRASP for combinatorial optimization","abstract":"This paper proposes a problem-independent GRASP metaheuristic using the random-key optimizer (RKO) paradigm. GRASP (greedy randomized adaptive search procedure) is a metaheuristic for combinatorial optimization that repeatedly applies a semi-greedy construction procedure followed by a local search procedure. The best solution found over all iterations is returned as the solution of the GRASP. Continuous GRASP (C-GRASP) is an extension of GRASP for continuous optimization in the unit hypercube. A random-key optimizer (RKO) uses a vector of random keys to encode a solution to a combinatorial optimization problem. It uses a decoder to evaluate a solution encoded by the vector of random keys. A random-key GRASP is a C-GRASP where points in the unit hypercube are evaluated employing a decoder. We describe random key GRASP consisting of a problem-independent component and a problem-dependent decoder. As a proof of concept, the random-key GRASP is tested on five NP-hard combinatorial optimization problems: traveling salesman problem, tree of hubs location problem, Steiner triple covering problem, node capacitated graph partitioning problem, and job sequencing and tool switching problem.","sentences":["This paper proposes a problem-independent GRASP metaheuristic using the random-key optimizer (RKO) paradigm.","GRASP (greedy randomized adaptive search procedure) is a metaheuristic for combinatorial optimization that repeatedly applies a semi-greedy construction procedure followed by a local search procedure.","The best solution found over all iterations is returned as the solution of the GRASP.","Continuous GRASP (C-GRASP) is an extension of GRASP for continuous optimization in the unit hypercube.","A random-key optimizer (RKO) uses a vector of random keys to encode a solution to a combinatorial optimization problem.","It uses a decoder to evaluate a solution encoded by the vector of random keys.","A random-key GRASP is a C-GRASP where points in the unit hypercube are evaluated employing a decoder.","We describe random key GRASP consisting of a problem-independent component and a problem-dependent decoder.","As a proof of concept, the random-key GRASP is tested on five NP-hard combinatorial optimization problems: traveling salesman problem, tree of hubs location problem, Steiner triple covering problem, node capacitated graph partitioning problem, and job sequencing and tool switching problem."],"url":"http://arxiv.org/abs/2405.18681v1","category":"cs.NE"}
{"created":"2024-05-29 00:33:56","title":"Watermarking Counterfactual Explanations","abstract":"The field of Explainable Artificial Intelligence (XAI) focuses on techniques for providing explanations to end-users about the decision-making processes that underlie modern-day machine learning (ML) models. Within the vast universe of XAI techniques, counterfactual (CF) explanations are often preferred by end-users as they help explain the predictions of ML models by providing an easy-to-understand & actionable recourse (or contrastive) case to individual end-users who are adversely impacted by predicted outcomes. However, recent studies have shown significant security concerns with using CF explanations in real-world applications; in particular, malicious adversaries can exploit CF explanations to perform query-efficient model extraction attacks on proprietary ML models. In this paper, we propose a model-agnostic watermarking framework (for adding watermarks to CF explanations) that can be leveraged to detect unauthorized model extraction attacks (which rely on the watermarked CF explanations). Our novel framework solves a bi-level optimization problem to embed an indistinguishable watermark into the generated CF explanation such that any future model extraction attacks that rely on these watermarked CF explanations can be detected using a null hypothesis significance testing (NHST) scheme, while ensuring that these embedded watermarks do not compromise the quality of the generated CF explanations. We evaluate this framework's performance across a diverse set of real-world datasets, CF explanation methods, and model extraction techniques, and show that our watermarking detection system can be used to accurately identify extracted ML models that are trained using the watermarked CF explanations. Our work paves the way for the secure adoption of CF explanations in real-world applications.","sentences":["The field of Explainable Artificial Intelligence (XAI) focuses on techniques for providing explanations to end-users about the decision-making processes that underlie modern-day machine learning (ML) models.","Within the vast universe of XAI techniques, counterfactual (CF) explanations are often preferred by end-users as they help explain the predictions of ML models by providing an easy-to-understand & actionable recourse (or contrastive) case to individual end-users who are adversely impacted by predicted outcomes.","However, recent studies have shown significant security concerns with using CF explanations in real-world applications; in particular, malicious adversaries can exploit CF explanations to perform query-efficient model extraction attacks on proprietary ML models.","In this paper, we propose a model-agnostic watermarking framework (for adding watermarks to CF explanations) that can be leveraged to detect unauthorized model extraction attacks (which rely on the watermarked CF explanations).","Our novel framework solves a bi-level optimization problem to embed an indistinguishable watermark into the generated CF explanation such that any future model extraction attacks that rely on these watermarked CF explanations can be detected using a null hypothesis significance testing (NHST) scheme, while ensuring that these embedded watermarks do not compromise the quality of the generated CF explanations.","We evaluate this framework's performance across a diverse set of real-world datasets, CF explanation methods, and model extraction techniques, and show that our watermarking detection system can be used to accurately identify extracted ML models that are trained using the watermarked CF explanations.","Our work paves the way for the secure adoption of CF explanations in real-world applications."],"url":"http://arxiv.org/abs/2405.18671v1","category":"cs.LG"}
{"created":"2024-05-29 00:23:55","title":"Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities","abstract":"Integrating multiple generative foundation models, especially those trained on different modalities, into something greater than the sum of its parts poses significant challenges. Two key hurdles are the availability of aligned data (concepts that contain similar meaning but is expressed differently in different modalities), and effectively leveraging unimodal representations in cross-domain generative tasks, without compromising their original unimodal capabilities.   We propose Zipper, a multi-tower decoder architecture that addresses these concerns by using cross-attention to flexibly compose multimodal generative models from independently pre-trained unimodal decoders. In our experiments fusing speech and text modalities, we show the proposed architecture performs very competitively in scenarios with limited aligned text-speech data. We also showcase the flexibility of our model to selectively maintain unimodal (e.g., text-to-text generation) generation performance by freezing the corresponding modal tower (e.g. text). In cross-modal tasks such as automatic speech recognition (ASR) where the output modality is text, we show that freezing the text backbone results in negligible performance degradation. In cross-modal tasks such as text-to-speech generation (TTS) where the output modality is speech, we show that using a pre-trained speech backbone results in superior performance to the baseline.","sentences":["Integrating multiple generative foundation models, especially those trained on different modalities, into something greater than the sum of its parts poses significant challenges.","Two key hurdles are the availability of aligned data (concepts that contain similar meaning but is expressed differently in different modalities), and effectively leveraging unimodal representations in cross-domain generative tasks, without compromising their original unimodal capabilities.   ","We propose Zipper, a multi-tower decoder architecture that addresses these concerns by using cross-attention to flexibly compose multimodal generative models from independently pre-trained unimodal decoders.","In our experiments fusing speech and text modalities, we show the proposed architecture performs very competitively in scenarios with limited aligned text-speech data.","We also showcase the flexibility of our model to selectively maintain unimodal (e.g., text-to-text generation) generation performance by freezing the corresponding modal tower (e.g. text).","In cross-modal tasks such as automatic speech recognition (ASR) where the output modality is text, we show that freezing the text backbone results in negligible performance degradation.","In cross-modal tasks such as text-to-speech generation (TTS) where the output modality is speech, we show that using a pre-trained speech backbone results in superior performance to the baseline."],"url":"http://arxiv.org/abs/2405.18669v1","category":"cs.LG"}
{"created":"2024-05-29 00:01:40","title":"Fast Explainability via Feasible Concept Sets Generator","abstract":"A long-standing dilemma prevents the broader application of explanation methods: general applicability and inference speed. On the one hand, existing model-agnostic explanation methods usually make minimal pre-assumptions about the prediction models to be explained. Still, they require additional queries to the model through propagation or back-propagation to approximate the models' behaviors, resulting in slow inference and hindering their use in time-sensitive tasks. On the other hand, various model-dependent explanations have been proposed that achieve low-cost, fast inference but at the expense of limiting their applicability to specific model structures. In this study, we bridge the gap between the universality of model-agnostic approaches and the efficiency of model-specific approaches by proposing a novel framework without assumptions on the prediction model's structures, achieving high efficiency during inference and allowing for real-time explanations. To achieve this, we first define explanations through a set of human-comprehensible concepts and propose a framework to elucidate model predictions via minimal feasible concept sets. Second, we show that a minimal feasible set generator can be learned as a companion explainer to the prediction model, generating explanations for predictions. Finally, we validate this framework by implementing a novel model-agnostic method that provides robust explanations while facilitating real-time inference. Our claims are substantiated by comprehensive experiments, highlighting the effectiveness and efficiency of our approach.","sentences":["A long-standing dilemma prevents the broader application of explanation methods: general applicability and inference speed.","On the one hand, existing model-agnostic explanation methods usually make minimal pre-assumptions about the prediction models to be explained.","Still, they require additional queries to the model through propagation or back-propagation to approximate the models' behaviors, resulting in slow inference and hindering their use in time-sensitive tasks.","On the other hand, various model-dependent explanations have been proposed that achieve low-cost, fast inference but at the expense of limiting their applicability to specific model structures.","In this study, we bridge the gap between the universality of model-agnostic approaches and the efficiency of model-specific approaches by proposing a novel framework without assumptions on the prediction model's structures, achieving high efficiency during inference and allowing for real-time explanations.","To achieve this, we first define explanations through a set of human-comprehensible concepts and propose a framework to elucidate model predictions via minimal feasible concept sets.","Second, we show that a minimal feasible set generator can be learned as a companion explainer to the prediction model, generating explanations for predictions.","Finally, we validate this framework by implementing a novel model-agnostic method that provides robust explanations while facilitating real-time inference.","Our claims are substantiated by comprehensive experiments, highlighting the effectiveness and efficiency of our approach."],"url":"http://arxiv.org/abs/2405.18664v1","category":"cs.LG"}
{"created":"2024-05-28 23:57:48","title":"Lifelong Learning and Selective Forgetting via Contrastive Strategy","abstract":"Lifelong learning aims to train a model with good performance for new tasks while retaining the capacity of previous tasks. However, some practical scenarios require the system to forget undesirable knowledge due to privacy issues, which is called selective forgetting. The joint task of the two is dubbed Learning with Selective Forgetting (LSF). In this paper, we propose a new framework based on contrastive strategy for LSF. Specifically, for the preserved classes (tasks), we make features extracted from different samples within a same class compacted. And for the deleted classes, we make the features from different samples of a same class dispersed and irregular, i.e., the network does not have any regular response to samples from a specific deleted class as if the network has no training at all. Through maintaining or disturbing the feature distribution, the forgetting and memory of different classes can be or independent of each other. Experiments are conducted on four benchmark datasets, and our method acieves new state-of-the-art.","sentences":["Lifelong learning aims to train a model with good performance for new tasks while retaining the capacity of previous tasks.","However, some practical scenarios require the system to forget undesirable knowledge due to privacy issues, which is called selective forgetting.","The joint task of the two is dubbed Learning with Selective Forgetting (LSF).","In this paper, we propose a new framework based on contrastive strategy for LSF.","Specifically, for the preserved classes (tasks), we make features extracted from different samples within a same class compacted.","And for the deleted classes, we make the features from different samples of a same class dispersed and irregular, i.e., the network does not have any regular response to samples from a specific deleted class as if the network has no training at all.","Through maintaining or disturbing the feature distribution, the forgetting and memory of different classes can be or independent of each other.","Experiments are conducted on four benchmark datasets, and our method acieves new state-of-the-art."],"url":"http://arxiv.org/abs/2405.18663v1","category":"cs.AI"}
{"created":"2024-05-28 23:49:52","title":"D-CoRP: Differentiable Connectivity Refinement for Functional Brain Networks","abstract":"Brain network is an important tool for understanding the brain, offering insights for scientific research and clinical diagnosis. Existing models for brain networks typically primarily focus on brain regions or overlook the complexity of brain connectivities. MRI-derived brain network data is commonly susceptible to connectivity noise, underscoring the necessity of incorporating connectivities into the modeling of brain networks. To address this gap, we introduce a differentiable module for refining brain connectivity. We develop the multivariate optimization based on information bottleneck theory to address the complexity of the brain network and filter noisy or redundant connections. Also, our method functions as a flexible plugin that is adaptable to most graph neural networks. Our extensive experimental results show that the proposed method can significantly improve the performance of various baseline models and outperform other state-of-the-art methods, indicating the effectiveness and generalizability of the proposed method in refining brain network connectivity. The code will be released for public availability.","sentences":["Brain network is an important tool for understanding the brain, offering insights for scientific research and clinical diagnosis.","Existing models for brain networks typically primarily focus on brain regions or overlook the complexity of brain connectivities.","MRI-derived brain network data is commonly susceptible to connectivity noise, underscoring the necessity of incorporating connectivities into the modeling of brain networks.","To address this gap, we introduce a differentiable module for refining brain connectivity.","We develop the multivariate optimization based on information bottleneck theory to address the complexity of the brain network and filter noisy or redundant connections.","Also, our method functions as a flexible plugin that is adaptable to most graph neural networks.","Our extensive experimental results show that the proposed method can significantly improve the performance of various baseline models and outperform other state-of-the-art methods, indicating the effectiveness and generalizability of the proposed method in refining brain network connectivity.","The code will be released for public availability."],"url":"http://arxiv.org/abs/2405.18658v1","category":"q-bio.NC"}
{"created":"2024-05-28 23:44:09","title":"CAVACHON: a hierarchical variational autoencoder to integrate multi-modal single-cell data","abstract":"Paired single-cell sequencing technologies enable the simultaneous measurement of complementary modalities of molecular data at single-cell resolution. Along with the advances in these technologies, many methods based on variational autoencoders have been developed to integrate these data. However, these methods do not explicitly incorporate prior biological relationships between the data modalities, which could significantly enhance modeling and interpretation. We propose a novel probabilistic learning framework that explicitly incorporates conditional independence relationships between multi-modal data as a directed acyclic graph using a generalized hierarchical variational autoencoder. We demonstrate the versatility of our framework across various applications pertinent to single-cell multi-omics data integration. These include the isolation of common and distinct information from different modalities, modality-specific differential analysis, and integrated cell clustering. We anticipate that the proposed framework can facilitate the construction of highly flexible graphical models that can capture the complexities of biological hypotheses and unravel the connections between different biological data types, such as different modalities of paired single-cell multi-omics data. The implementation of the proposed framework can be found in the repository https://github.com/kuijjerlab/CAVACHON.","sentences":["Paired single-cell sequencing technologies enable the simultaneous measurement of complementary modalities of molecular data at single-cell resolution.","Along with the advances in these technologies, many methods based on variational autoencoders have been developed to integrate these data.","However, these methods do not explicitly incorporate prior biological relationships between the data modalities, which could significantly enhance modeling and interpretation.","We propose a novel probabilistic learning framework that explicitly incorporates conditional independence relationships between multi-modal data as a directed acyclic graph using a generalized hierarchical variational autoencoder.","We demonstrate the versatility of our framework across various applications pertinent to single-cell multi-omics data integration.","These include the isolation of common and distinct information from different modalities, modality-specific differential analysis, and integrated cell clustering.","We anticipate that the proposed framework can facilitate the construction of highly flexible graphical models that can capture the complexities of biological hypotheses and unravel the connections between different biological data types, such as different modalities of paired single-cell multi-omics data.","The implementation of the proposed framework can be found in the repository https://github.com/kuijjerlab/CAVACHON."],"url":"http://arxiv.org/abs/2405.18655v1","category":"cs.LG"}
{"created":"2024-05-28 23:22:18","title":"Approximating Human Models During Argumentation-based Dialogues","abstract":"Explainable AI Planning (XAIP) aims to develop AI agents that can effectively explain their decisions and actions to human users, fostering trust and facilitating human-AI collaboration. A key challenge in XAIP is model reconciliation, which seeks to align the mental models of AI agents and humans. While existing approaches often assume a known and deterministic human model, this simplification may not capture the complexities and uncertainties of real-world interactions. In this paper, we propose a novel framework that enables AI agents to learn and update a probabilistic human model through argumentation-based dialogues. Our approach incorporates trust-based and certainty-based update mechanisms, allowing the agent to refine its understanding of the human's mental state based on the human's expressed trust in the agent's arguments and certainty in their own arguments. We employ a probability weighting function inspired by prospect theory to capture the relationship between trust and perceived probability, and use a Bayesian approach to update the agent's probability distribution over possible human models. We conduct a human-subject study to empirically evaluate the effectiveness of our approach in an argumentation scenario, demonstrating its ability to capture the dynamics of human belief formation and adaptation.","sentences":["Explainable AI Planning (XAIP) aims to develop AI agents that can effectively explain their decisions and actions to human users, fostering trust and facilitating human-AI collaboration.","A key challenge in XAIP is model reconciliation, which seeks to align the mental models of AI agents and humans.","While existing approaches often assume a known and deterministic human model, this simplification may not capture the complexities and uncertainties of real-world interactions.","In this paper, we propose a novel framework that enables AI agents to learn and update a probabilistic human model through argumentation-based dialogues.","Our approach incorporates trust-based and certainty-based update mechanisms, allowing the agent to refine its understanding of the human's mental state based on the human's expressed trust in the agent's arguments and certainty in their own arguments.","We employ a probability weighting function inspired by prospect theory to capture the relationship between trust and perceived probability, and use a Bayesian approach to update the agent's probability distribution over possible human models.","We conduct a human-subject study to empirically evaluate the effectiveness of our approach in an argumentation scenario, demonstrating its ability to capture the dynamics of human belief formation and adaptation."],"url":"http://arxiv.org/abs/2405.18650v1","category":"cs.AI"}
{"created":"2024-05-28 23:20:24","title":"Training LLMs to Better Self-Debug and Explain Code","abstract":"In the domain of code generation, self-debugging is crucial. It allows LLMs to refine their generated code based on execution feedback. This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks. Prior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs. In this work, we propose a training framework that significantly improves self-debugging capability of LLMs. Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement. We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories and filtering via execution verification. We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by 9.30% over four benchmarks. RL training brings additional up to 3.54% improvement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show iterative refinement ability, and can keep refining code continuously. Lastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code.","sentences":["In the domain of code generation, self-debugging is crucial.","It allows LLMs to refine their generated code based on execution feedback.","This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks.","Prior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs.","In this work, we propose a training framework that significantly improves self-debugging capability of LLMs.","Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement.","We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories and filtering via execution verification.","We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality.","SFT improves the pass@1 by up to 15.92% and pass@10 by 9.30% over four benchmarks.","RL training brings additional up to 3.54% improvement on pass@1 and 2.55% improvement on pass@10.","The trained LLMs show iterative refinement ability, and can keep refining code continuously.","Lastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code."],"url":"http://arxiv.org/abs/2405.18649v1","category":"cs.CL"}
{"created":"2024-05-28 23:01:57","title":"JADS: A Framework for Self-supervised Joint Aspect Discovery and Summarization","abstract":"To generate summaries that include multiple aspects or topics for text documents, most approaches use clustering or topic modeling to group relevant sentences and then generate a summary for each group. These approaches struggle to optimize the summarization and clustering algorithms jointly. On the other hand, aspect-based summarization requires known aspects. Our solution integrates topic discovery and summarization into a single step. Given text data, our Joint Aspect Discovery and Summarization algorithm (JADS) discovers aspects from the input and generates a summary of the topics, in one step. We propose a self-supervised framework that creates a labeled dataset by first mixing sentences from multiple documents (e.g., CNN/DailyMail articles) as the input and then uses the article summaries from the mixture as the labels. The JADS model outperforms the two-step baselines. With pretraining, the model achieves better performance and stability. Furthermore, embeddings derived from JADS exhibit superior clustering capabilities. Our proposed method achieves higher semantic alignment with ground truth and is factual.","sentences":["To generate summaries that include multiple aspects or topics for text documents, most approaches use clustering or topic modeling to group relevant sentences and then generate a summary for each group.","These approaches struggle to optimize the summarization and clustering algorithms jointly.","On the other hand, aspect-based summarization requires known aspects.","Our solution integrates topic discovery and summarization into a single step.","Given text data, our Joint Aspect Discovery and Summarization algorithm (JADS) discovers aspects from the input and generates a summary of the topics, in one step.","We propose a self-supervised framework that creates a labeled dataset by first mixing sentences from multiple documents (e.g., CNN/DailyMail articles) as the input and then uses the article summaries from the mixture as the labels.","The JADS model outperforms the two-step baselines.","With pretraining, the model achieves better performance and stability.","Furthermore, embeddings derived from JADS exhibit superior clustering capabilities.","Our proposed method achieves higher semantic alignment with ground truth and is factual."],"url":"http://arxiv.org/abs/2405.18642v1","category":"cs.AI"}
{"created":"2024-05-28 22:48:53","title":"Improving Speech Decoding from ECoG with Self-Supervised Pretraining","abstract":"Recent work on intracranial brain-machine interfaces has demonstrated that spoken speech can be decoded with high accuracy, essentially by treating the problem as an instance of supervised learning and training deep neural networks to map from neural activity to text. However, such networks pay for their expressiveness with very large numbers of labeled data, a requirement that is particularly burdensome for invasive neural recordings acquired from human patients. On the other hand, these patients typically produce speech outside of the experimental blocks used for training decoders. Making use of such data, and data from other patients, to improve decoding would ease the burden of data collection -- especially onerous for dys- and anarthric patients. Here we demonstrate that this is possible, by reengineering wav2vec -- a simple, self-supervised, fully convolutional model that learns latent representations of audio using a noise-contrastive loss -- for electrocorticographic (ECoG) data. We train this model on unlabelled ECoG recordings, and subsequently use it to transform ECoG from labeled speech sessions into wav2vec's representation space, before finally training a supervised encoder-decoder to map these representations to text. We experiment with various numbers of labeled blocks; for almost all choices, the new representations yield superior decoding performance to the original ECoG data, and in no cases do they yield worse. Performance can also be improved in some cases by pretraining wav2vec on another patient's data. In the best cases, wav2vec's representations decrease word error rates over the original data by upwards of 50%.","sentences":["Recent work on intracranial brain-machine interfaces has demonstrated that spoken speech can be decoded with high accuracy, essentially by treating the problem as an instance of supervised learning and training deep neural networks to map from neural activity to text.","However, such networks pay for their expressiveness with very large numbers of labeled data, a requirement that is particularly burdensome for invasive neural recordings acquired from human patients.","On the other hand, these patients typically produce speech outside of the experimental blocks used for training decoders.","Making use of such data, and data from other patients, to improve decoding would ease the burden of data collection -- especially onerous for dys- and anarthric patients.","Here we demonstrate that this is possible, by reengineering wav2vec -- a simple, self-supervised, fully convolutional model that learns latent representations of audio using a noise-contrastive loss -- for electrocorticographic (ECoG) data.","We train this model on unlabelled ECoG recordings, and subsequently use it to transform ECoG from labeled speech sessions into wav2vec's representation space, before finally training a supervised encoder-decoder to map these representations to text.","We experiment with various numbers of labeled blocks; for almost all choices, the new representations yield superior decoding performance to the original ECoG data, and in no cases do they yield worse.","Performance can also be improved in some cases by pretraining wav2vec on another patient's data.","In the best cases, wav2vec's representations decrease word error rates over the original data by upwards of 50%."],"url":"http://arxiv.org/abs/2405.18639v1","category":"q-bio.NC"}
{"created":"2024-05-28 22:45:28","title":"ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models","abstract":"In this position paper, we argue that human evaluation of generative large language models (LLMs) should be a multidisciplinary undertaking that draws upon insights from disciplines such as user experience research and human behavioral psychology to ensure that the experimental design and results are reliable. The conclusions from these evaluations, thus, must consider factors such as usability, aesthetics, and cognitive biases. We highlight how cognitive biases can conflate fluent information and truthfulness, and how cognitive uncertainty affects the reliability of rating scores such as Likert. Furthermore, the evaluation should differentiate the capabilities and weaknesses of increasingly powerful large language models -- which requires effective test sets. The scalability of human evaluation is also crucial to wider adoption. Hence, to design an effective human evaluation system in the age of generative NLP, we propose the ConSiDERS-The-Human evaluation framework consisting of 6 pillars --Consistency, Scoring Critera, Differentiating, User Experience, Responsible, and Scalability.","sentences":["In this position paper, we argue that human evaluation of generative large language models (LLMs) should be a multidisciplinary undertaking that draws upon insights from disciplines such as user experience research and human behavioral psychology to ensure that the experimental design and results are reliable.","The conclusions from these evaluations, thus, must consider factors such as usability, aesthetics, and cognitive biases.","We highlight how cognitive biases can conflate fluent information and truthfulness, and how cognitive uncertainty affects the reliability of rating scores such as Likert.","Furthermore, the evaluation should differentiate the capabilities and weaknesses of increasingly powerful large language models -- which requires effective test sets.","The scalability of human evaluation is also crucial to wider adoption.","Hence, to design an effective human evaluation system in the age of generative NLP, we propose the ConSiDERS-The-Human evaluation framework consisting of 6 pillars --Consistency, Scoring Critera, Differentiating, User Experience, Responsible, and Scalability."],"url":"http://arxiv.org/abs/2405.18638v1","category":"cs.CL"}
{"created":"2024-05-28 22:38:24","title":"ChatGPT as the Marketplace of Ideas: Should Truth-Seeking Be the Goal of AI Content Governance?","abstract":"As one of the most enduring metaphors within legal discourse, the marketplace of ideas has wielded considerable influence over the jurisprudential landscape for decades. A century after the inception of this theory, ChatGPT emerged as a revolutionary technological advancement in the twenty-first century. This research finds that ChatGPT effectively manifests the marketplace metaphor. It not only instantiates the promises envisaged by generations of legal scholars but also lays bare the perils discerned through sustained academic critique. Specifically, the workings of ChatGPT and the marketplace of ideas theory exhibit at least four common features: arena, means, objectives, and flaws. These shared attributes are sufficient to render ChatGPT historically the most qualified engine for actualizing the marketplace of ideas theory.   The comparison of the marketplace theory and ChatGPT merely marks a starting point. A more meaningful undertaking entails reevaluating and reframing both internal and external AI policies by referring to the accumulated experience, insights, and suggestions researchers have raised to fix the marketplace theory. Here, a pivotal issue is: should truth-seeking be set as the goal of AI content governance? Given the unattainability of the absolute truth-seeking goal, I argue against adopting zero-risk policies. Instead, a more judicious approach would be to embrace a knowledge-based alternative wherein large language models (LLMs) are trained to generate competing and divergent viewpoints based on sufficient justifications. This research also argues that so-called AI content risks are not created by AI companies but are inherent in the entire information ecosystem. Thus, the burden of managing these risks should be distributed among different social actors, rather than being solely shouldered by chatbot companies.","sentences":["As one of the most enduring metaphors within legal discourse, the marketplace of ideas has wielded considerable influence over the jurisprudential landscape for decades.","A century after the inception of this theory, ChatGPT emerged as a revolutionary technological advancement in the twenty-first century.","This research finds that ChatGPT effectively manifests the marketplace metaphor.","It not only instantiates the promises envisaged by generations of legal scholars but also lays bare the perils discerned through sustained academic critique.","Specifically, the workings of ChatGPT and the marketplace of ideas theory exhibit at least four common features: arena, means, objectives, and flaws.","These shared attributes are sufficient to render ChatGPT historically the most qualified engine for actualizing the marketplace of ideas theory.   ","The comparison of the marketplace theory and ChatGPT merely marks a starting point.","A more meaningful undertaking entails reevaluating and reframing both internal and external AI policies by referring to the accumulated experience, insights, and suggestions researchers have raised to fix the marketplace theory.","Here, a pivotal issue is: should truth-seeking be set as the goal of AI content governance?","Given the unattainability of the absolute truth-seeking goal, I argue against adopting zero-risk policies.","Instead, a more judicious approach would be to embrace a knowledge-based alternative wherein large language models (LLMs) are trained to generate competing and divergent viewpoints based on sufficient justifications.","This research also argues that so-called AI content risks are not created by AI companies but are inherent in the entire information ecosystem.","Thus, the burden of managing these risks should be distributed among different social actors, rather than being solely shouldered by chatbot companies."],"url":"http://arxiv.org/abs/2405.18636v1","category":"cs.AI"}
{"created":"2024-05-28 22:28:50","title":"Large Language Models as Partners in Student Essay Evaluation","abstract":"As the importance of comprehensive evaluation in workshop courses increases, there is a growing demand for efficient and fair assessment methods that reduce the workload for faculty members. This paper presents an evaluation conducted with Large Language Models (LLMs) using actual student essays in three scenarios: 1) without providing guidance such as rubrics, 2) with pre-specified rubrics, and 3) through pairwise comparison of essays. Quantitative analysis of the results revealed a strong correlation between LLM and faculty member assessments in the pairwise comparison scenario with pre-specified rubrics, although concerns about the quality and stability of evaluations remained. Therefore, we conducted a qualitative analysis of LLM assessment comments, showing that: 1) LLMs can match the assessment capabilities of faculty members, 2) variations in LLM assessments should be interpreted as diversity rather than confusion, and 3) assessments by humans and LLMs can differ and complement each other. In conclusion, this paper suggests that LLMs should not be seen merely as assistants to faculty members but as partners in evaluation committees and outlines directions for further research.","sentences":["As the importance of comprehensive evaluation in workshop courses increases, there is a growing demand for efficient and fair assessment methods that reduce the workload for faculty members.","This paper presents an evaluation conducted with Large Language Models (LLMs) using actual student essays in three scenarios: 1) without providing guidance such as rubrics, 2) with pre-specified rubrics, and 3) through pairwise comparison of essays.","Quantitative analysis of the results revealed a strong correlation between LLM and faculty member assessments in the pairwise comparison scenario with pre-specified rubrics, although concerns about the quality and stability of evaluations remained.","Therefore, we conducted a qualitative analysis of LLM assessment comments, showing that: 1) LLMs can match the assessment capabilities of faculty members, 2) variations in LLM assessments should be interpreted as diversity rather than confusion, and 3) assessments by humans and LLMs can differ and complement each other.","In conclusion, this paper suggests that LLMs should not be seen merely as assistants to faculty members but as partners in evaluation committees and outlines directions for further research."],"url":"http://arxiv.org/abs/2405.18632v1","category":"cs.CY"}
{"created":"2024-05-28 22:19:26","title":"PureGen: Universal Data Purification for Train-Time Poison Defense via Generative Model Dynamics","abstract":"Train-time data poisoning attacks threaten machine learning models by introducing adversarial examples during training, leading to misclassification. Current defense methods often reduce generalization performance, are attack-specific, and impose significant training overhead. To address this, we introduce a set of universal data purification methods using a stochastic transform, $\\Psi(x)$, realized via iterative Langevin dynamics of Energy-Based Models (EBMs), Denoising Diffusion Probabilistic Models (DDPMs), or both. These approaches purify poisoned data with minimal impact on classifier generalization. Our specially trained EBMs and DDPMs provide state-of-the-art defense against various attacks (including Narcissus, Bullseye Polytope, Gradient Matching) on CIFAR-10, Tiny-ImageNet, and CINIC-10, without needing attack or classifier-specific information. We discuss performance trade-offs and show that our methods remain highly effective even with poisoned or distributionally shifted generative model training data.","sentences":["Train-time data poisoning attacks threaten machine learning models by introducing adversarial examples during training, leading to misclassification.","Current defense methods often reduce generalization performance, are attack-specific, and impose significant training overhead.","To address this, we introduce a set of universal data purification methods using a stochastic transform, $\\Psi(x)$, realized via iterative Langevin dynamics of Energy-Based Models (EBMs), Denoising Diffusion Probabilistic Models (DDPMs), or both.","These approaches purify poisoned data with minimal impact on classifier generalization.","Our specially trained EBMs and DDPMs provide state-of-the-art defense against various attacks (including Narcissus, Bullseye Polytope, Gradient Matching) on CIFAR-10, Tiny-ImageNet, and CINIC-10, without needing attack or classifier-specific information.","We discuss performance trade-offs and show that our methods remain highly effective even with poisoned or distributionally shifted generative model training data."],"url":"http://arxiv.org/abs/2405.18627v1","category":"cs.LG"}
{"created":"2024-05-28 22:17:57","title":"Causal Contextual Bandits with Adaptive Context","abstract":"We study a variant of causal contextual bandits where the context is chosen based on an initial intervention chosen by the learner. At the beginning of each round, the learner selects an initial action, depending on which a stochastic context is revealed by the environment. Following this, the learner then selects a final action and receives a reward. Given $T$ rounds of interactions with the environment, the objective of the learner is to learn a policy (of selecting the initial and the final action) with maximum expected reward. In this paper we study the specific situation where every action corresponds to intervening on a node in some known causal graph. We extend prior work from the deterministic context setting to obtain simple regret minimization guarantees. This is achieved through an instance-dependent causal parameter, $\\lambda$, which characterizes our upper bound. Furthermore, we prove that our simple regret is essentially tight for a large class of instances. A key feature of our work is that we use convex optimization to address the bandit exploration problem. We also conduct experiments to validate our theoretical results, and release our code at our project GitHub repository: https://github.com/adaptiveContextualCausalBandits/aCCB.","sentences":["We study a variant of causal contextual bandits where the context is chosen based on an initial intervention chosen by the learner.","At the beginning of each round, the learner selects an initial action, depending on which a stochastic context is revealed by the environment.","Following this, the learner then selects a final action and receives a reward.","Given $T$ rounds of interactions with the environment, the objective of the learner is to learn a policy (of selecting the initial and the final action) with maximum expected reward.","In this paper we study the specific situation where every action corresponds to intervening on a node in some known causal graph.","We extend prior work from the deterministic context setting to obtain simple regret minimization guarantees.","This is achieved through an instance-dependent causal parameter, $\\lambda$, which characterizes our upper bound.","Furthermore, we prove that our simple regret is essentially tight for a large class of instances.","A key feature of our work is that we use convex optimization to address the bandit exploration problem.","We also conduct experiments to validate our theoretical results, and release our code at our project GitHub repository: https://github.com/adaptiveContextualCausalBandits/aCCB."],"url":"http://arxiv.org/abs/2405.18626v1","category":"cs.LG"}
{"created":"2024-05-28 22:12:15","title":"Enhancing IoT Security with CNN and LSTM-Based Intrusion Detection Systems","abstract":"Protecting Internet of things (IoT) devices against cyber attacks is imperative owing to inherent security vulnerabilities. These vulnerabilities can include a spectrum of sophisticated attacks that pose significant damage to both individuals and organizations. Employing robust security measures like intrusion detection systems (IDSs) is essential to solve these problems and protect IoT systems from such attacks. In this context, our proposed IDS model consists on a combination of convolutional neural network (CNN) and long short-term memory (LSTM) deep learning (DL) models. This fusion facilitates the detection and classification of IoT traffic into binary categories, benign and malicious activities by leveraging the spatial feature extraction capabilities of CNN for pattern recognition and the sequential memory retention of LSTM for discerning complex temporal dependencies in achieving enhanced accuracy and efficiency. In assessing the performance of our proposed model, the authors employed the new CICIoT2023 dataset for both training and final testing, while further validating the model's performance through a conclusive testing phase utilizing the CICIDS2017 dataset. Our proposed model achieves an accuracy rate of 98.42%, accompanied by a minimal loss of 0.0275. False positive rate(FPR) is equally important, reaching 9.17% with an F1-score of 98.57%. These results demonstrate the effectiveness of our proposed CNN-LSTM IDS model in fortifying IoT environments against potential cyber threats.","sentences":["Protecting Internet of things (IoT) devices against cyber attacks is imperative owing to inherent security vulnerabilities.","These vulnerabilities can include a spectrum of sophisticated attacks that pose significant damage to both individuals and organizations.","Employing robust security measures like intrusion detection systems (IDSs) is essential to solve these problems and protect IoT systems from such attacks.","In this context, our proposed IDS model consists on a combination of convolutional neural network (CNN) and long short-term memory (LSTM) deep learning (DL) models.","This fusion facilitates the detection and classification of IoT traffic into binary categories, benign and malicious activities by leveraging the spatial feature extraction capabilities of CNN for pattern recognition and the sequential memory retention of LSTM for discerning complex temporal dependencies in achieving enhanced accuracy and efficiency.","In assessing the performance of our proposed model, the authors employed the new CICIoT2023 dataset for both training and final testing, while further validating the model's performance through a conclusive testing phase utilizing the CICIDS2017 dataset.","Our proposed model achieves an accuracy rate of 98.42%, accompanied by a minimal loss of 0.0275.","False positive rate(FPR) is equally important, reaching 9.17% with an F1-score of 98.57%.","These results demonstrate the effectiveness of our proposed CNN-LSTM IDS model in fortifying IoT environments against potential cyber threats."],"url":"http://arxiv.org/abs/2405.18624v1","category":"cs.CR"}
{"created":"2024-05-28 22:08:20","title":"I See You: Teacher Analytics with GPT-4 Vision-Powered Observational Assessment","abstract":"This preliminary study explores the integration of GPT-4 Vision (GPT-4V) technology into teacher analytics, focusing on its applicability in observational assessment to enhance reflective teaching practice. This research is grounded in developing a Video-based Automatic Assessment System (VidAAS) empowered by GPT-4V. Our approach aims to revolutionize teachers' assessment of students' practices by leveraging Generative Artificial Intelligence (GenAI) to offer detailed insights into classroom dynamics. Our research methodology encompasses a comprehensive literature review, prototype development of the VidAAS, and usability testing with in-service teachers. The study findings provide future research avenues for VidAAS design, implementation, and integration in teacher analytics, underscoring the potential of GPT-4V to provide real-time, scalable feedback and a deeper understanding of the classroom.","sentences":["This preliminary study explores the integration of GPT-4 Vision (GPT-4V) technology into teacher analytics, focusing on its applicability in observational assessment to enhance reflective teaching practice.","This research is grounded in developing a Video-based Automatic Assessment System (VidAAS) empowered by GPT-4V. Our approach aims to revolutionize teachers' assessment of students' practices by leveraging Generative Artificial Intelligence (GenAI) to offer detailed insights into classroom dynamics.","Our research methodology encompasses a comprehensive literature review, prototype development of the VidAAS, and usability testing with in-service teachers.","The study findings provide future research avenues for VidAAS design, implementation, and integration in teacher analytics, underscoring the potential of GPT-4V to provide real-time, scalable feedback and a deeper understanding of the classroom."],"url":"http://arxiv.org/abs/2405.18623v1","category":"cs.HC"}
{"created":"2024-05-28 21:59:56","title":"RealitySummary: On-Demand Mixed Reality Document Enhancement using Large Language Models","abstract":"We introduce RealitySummary, a mixed reality reading assistant that can enhance any printed or digital document using on-demand text extraction, summarization, and augmentation. While augmented reading tools promise to enhance physical reading experiences with overlaid digital content, prior systems have typically required pre-processed documents, which limits their generalizability and real-world use cases. In this paper, we explore on-demand document augmentation by leveraging large language models. To understand generalizable techniques for diverse documents, we first conducted an exploratory design study which identified five categories of document enhancements (summarization, augmentation, navigation, comparison, and extraction). Based on this, we developed a proof-of-concept system that can automatically extract and summarize text using Google Cloud OCR and GPT-4, then embed information around documents using a Microsoft Hololens 2 and Apple Vision Pro. We demonstrate real-time examples of six specific document augmentations: 1) summaries, 2) comparison tables, 3) timelines, 4) keyword lists, 5) summary highlighting, and 6) information cards. Results from a usability study (N=12) and in-the-wild study (N=11) highlight the potential benefits of on-demand MR document enhancement and opportunities for future research.","sentences":["We introduce RealitySummary, a mixed reality reading assistant that can enhance any printed or digital document using on-demand text extraction, summarization, and augmentation.","While augmented reading tools promise to enhance physical reading experiences with overlaid digital content, prior systems have typically required pre-processed documents, which limits their generalizability and real-world use cases.","In this paper, we explore on-demand document augmentation by leveraging large language models.","To understand generalizable techniques for diverse documents, we first conducted an exploratory design study which identified five categories of document enhancements (summarization, augmentation, navigation, comparison, and extraction).","Based on this, we developed a proof-of-concept system that can automatically extract and summarize text using Google Cloud OCR and GPT-4, then embed information around documents using a Microsoft Hololens 2 and Apple Vision Pro.","We demonstrate real-time examples of six specific document augmentations: 1) summaries, 2) comparison tables, 3) timelines, 4) keyword lists, 5) summary highlighting, and 6) information cards.","Results from a usability study (N=12) and in-the-wild study (N=11) highlight the potential benefits of on-demand MR document enhancement and opportunities for future research."],"url":"http://arxiv.org/abs/2405.18620v1","category":"cs.HC"}
{"created":"2024-05-28 21:40:00","title":"DTR-Bench: An in silico Environment and Benchmark Platform for Reinforcement Learning Based Dynamic Treatment Regime","abstract":"Reinforcement learning (RL) has garnered increasing recognition for its potential to optimise dynamic treatment regimes (DTRs) in personalised medicine, particularly for drug dosage prescriptions and medication recommendations. However, a significant challenge persists: the absence of a unified framework for simulating diverse healthcare scenarios and a comprehensive analysis to benchmark the effectiveness of RL algorithms within these contexts. To address this gap, we introduce \\textit{DTR-Bench}, a benchmarking platform comprising four distinct simulation environments tailored to common DTR applications, including cancer chemotherapy, radiotherapy, glucose management in diabetes, and sepsis treatment. We evaluate various state-of-the-art RL algorithms across these settings, particularly highlighting their performance amidst real-world challenges such as pharmacokinetic/pharmacodynamic (PK/PD) variability, noise, and missing data. Our experiments reveal varying degrees of performance degradation among RL algorithms in the presence of noise and patient variability, with some algorithms failing to converge. Additionally, we observe that using temporal observation representations does not consistently lead to improved performance in DTR settings. Our findings underscore the necessity of developing robust, adaptive RL algorithms capable of effectively managing these complexities to enhance patient-specific healthcare. We have open-sourced our benchmark and code at https://github.com/GilesLuo/DTR-Bench.","sentences":["Reinforcement learning (RL) has garnered increasing recognition for its potential to optimise dynamic treatment regimes (DTRs) in personalised medicine, particularly for drug dosage prescriptions and medication recommendations.","However, a significant challenge persists: the absence of a unified framework for simulating diverse healthcare scenarios and a comprehensive analysis to benchmark the effectiveness of RL algorithms within these contexts.","To address this gap, we introduce \\textit{DTR-Bench}, a benchmarking platform comprising four distinct simulation environments tailored to common DTR applications, including cancer chemotherapy, radiotherapy, glucose management in diabetes, and sepsis treatment.","We evaluate various state-of-the-art RL algorithms across these settings, particularly highlighting their performance amidst real-world challenges such as pharmacokinetic/pharmacodynamic (PK/PD) variability, noise, and missing data.","Our experiments reveal varying degrees of performance degradation among RL algorithms in the presence of noise and patient variability, with some algorithms failing to converge.","Additionally, we observe that using temporal observation representations does not consistently lead to improved performance in DTR settings.","Our findings underscore the necessity of developing robust, adaptive RL algorithms capable of effectively managing these complexities to enhance patient-specific healthcare.","We have open-sourced our benchmark and code at https://github.com/GilesLuo/DTR-Bench."],"url":"http://arxiv.org/abs/2405.18610v1","category":"cs.LG"}
{"created":"2024-05-28 21:33:18","title":"SST-GCN: The Sequential based Spatio-Temporal Graph Convolutional networks for Minute-level and Road-level Traffic Accident Risk Predictio","abstract":"Traffic accidents are recognized as a major social issue worldwide, causing numerous injuries and significant costs annually. Consequently, methods for predicting and preventing traffic accidents have been researched for many years. With advancements in the field of artificial intelligence, various studies have applied Machine Learning and Deep Learning techniques to traffic accident prediction. Modern traffic conditions change rapidly by the minute, and these changes vary significantly across different roads. In other words, the risk of traffic accidents changes minute by minute in various patterns for each road. Therefore, it is desirable to predict traffic accident risk at the Minute-Level and Road-Level. However, because roads have close and complex relationships with adjacent roads, research on predicting traffic accidents at the Minute-Level and Road-Level is challenging. Thus, it is essential to build a model that can reflect the spatial and temporal characteristics of roads for traffic accident prediction. Consequently, recent attempts have been made to use Graph Convolutional Networks to capture the spatial characteristics of roads and Recurrent Neural Networks to capture their temporal characteristics for predicting traffic accident risk. This paper proposes the Sequential based Spatio-Temporal Graph Convolutional Networks (SST-GCN), which combines GCN and LSTM, to predict traffic accidents at the Minute-Level and Road-Level using a road dataset constructed in Seoul, the capital of South Korea. Experiments have demonstrated that SST-GCN outperforms other state-of-the-art models in Minute-Level predictions.","sentences":["Traffic accidents are recognized as a major social issue worldwide, causing numerous injuries and significant costs annually.","Consequently, methods for predicting and preventing traffic accidents have been researched for many years.","With advancements in the field of artificial intelligence, various studies have applied Machine Learning and Deep Learning techniques to traffic accident prediction.","Modern traffic conditions change rapidly by the minute, and these changes vary significantly across different roads.","In other words, the risk of traffic accidents changes minute by minute in various patterns for each road.","Therefore, it is desirable to predict traffic accident risk at the Minute-Level and Road-Level.","However, because roads have close and complex relationships with adjacent roads, research on predicting traffic accidents at the Minute-Level and Road-Level is challenging.","Thus, it is essential to build a model that can reflect the spatial and temporal characteristics of roads for traffic accident prediction.","Consequently, recent attempts have been made to use Graph Convolutional Networks to capture the spatial characteristics of roads and Recurrent Neural Networks to capture their temporal characteristics for predicting traffic accident risk.","This paper proposes the Sequential based Spatio-Temporal Graph Convolutional Networks (SST-GCN), which combines GCN and LSTM, to predict traffic accidents at the Minute-Level and Road-Level using a road dataset constructed in Seoul, the capital of South Korea.","Experiments have demonstrated that SST-GCN outperforms other state-of-the-art models in Minute-Level predictions."],"url":"http://arxiv.org/abs/2405.18602v1","category":"cs.AI"}
{"created":"2024-05-28 21:02:24","title":"The DUNE-DAQ Application Framework","abstract":"The Deep Underground Neutrino Experiment (DUNE) is a next-generation neutrino experiment that will probe the properties of these elusive particles with unparalleled precision. It will also act as an observatory for neutrino bursts caused by nearby supernovae, in the event that one occurs while the experiment is in operation. Given these goals, the DUNE trigger and DAQ system must be able to maintain extremely high uptime and provide a path for full readout of the detectors for very long times (up to 100~s). To achieve these ends, we have designed the DUNE DAQ system around a flexible \"application framework\", which provides a modular interface for specific tasks while handling the interconnections between them. The application framework collects modules into applications which can then be interacted with as units by the control, configuration and monitoring systems. One of the key features of the framework is its communications abstraction layer, which allows for modules to interact with both internal queues and external network connections with a single transport-agnostic interface. We will report on the architecture and features of the framework.","sentences":["The Deep Underground Neutrino Experiment (DUNE) is a next-generation neutrino experiment that will probe the properties of these elusive particles with unparalleled precision.","It will also act as an observatory for neutrino bursts caused by nearby supernovae, in the event that one occurs while the experiment is in operation.","Given these goals, the DUNE trigger and DAQ system must be able to maintain extremely high uptime and provide a path for full readout of the detectors for very long times (up to 100~s).","To achieve these ends, we have designed the DUNE DAQ system around a flexible \"application framework\", which provides a modular interface for specific tasks while handling the interconnections between them.","The application framework collects modules into applications which can then be interacted with as units by the control, configuration and monitoring systems.","One of the key features of the framework is its communications abstraction layer, which allows for modules to interact with both internal queues and external network connections with a single transport-agnostic interface.","We will report on the architecture and features of the framework."],"url":"http://arxiv.org/abs/2405.18583v1","category":"physics.ins-det"}
{"created":"2024-05-28 20:56:23","title":"Automatic Calibration for an Open-source Magnetic Tactile Sensor","abstract":"Tactile sensing can enable robots to perform complex, contact-rich tasks. Magnetic sensors offer accurate three-axis force measurements while using affordable materials. Calibrating such a sensor involves either manual data collection, or automated procedures with precise mounting of the sensor relative to an actuator. We present an open-source magnetic tactile sensor with an automatic, in situ, gripper-agnostic calibration method, after which the sensor is immediately ready for use. Our goal is to lower the barrier to entry for tactile sensing, fostering collaboration in robotics. Design files and readout code can be found at https://github.com/LowiekVDS/Open-source-Magnetic-Tactile-Sensor}{https://github.com/LowiekVDS/Open-source-Magnetic-Tactile-Sensor.","sentences":["Tactile sensing can enable robots to perform complex, contact-rich tasks.","Magnetic sensors offer accurate three-axis force measurements while using affordable materials.","Calibrating such a sensor involves either manual data collection, or automated procedures with precise mounting of the sensor relative to an actuator.","We present an open-source magnetic tactile sensor with an automatic, in situ, gripper-agnostic calibration method, after which the sensor is immediately ready for use.","Our goal is to lower the barrier to entry for tactile sensing, fostering collaboration in robotics.","Design files and readout code can be found at https://github.com/LowiekVDS/Open-source-Magnetic-Tactile-Sensor}{https://github.com/LowiekVDS/Open-source-Magnetic-Tactile-Sensor."],"url":"http://arxiv.org/abs/2405.18582v1","category":"cs.RO"}
{"created":"2024-05-28 20:54:47","title":"Unleashing the Potential of Text-attributed Graphs: Automatic Relation Decomposition via Large Language Models","abstract":"Recent advancements in text-attributed graphs (TAGs) have significantly improved the quality of node features by using the textual modeling capabilities of language models. Despite this success, utilizing text attributes to enhance the predefined graph structure remains largely unexplored. Our extensive analysis reveals that conventional edges on TAGs, treated as a single relation (e.g., hyperlinks) in previous literature, actually encompass mixed semantics (e.g., \"advised by\" and \"participates in\"). This simplification hinders the representation learning process of Graph Neural Networks (GNNs) on downstream tasks, even when integrated with advanced node features. In contrast, we discover that decomposing these edges into distinct semantic relations significantly enhances the performance of GNNs. Despite this, manually identifying and labeling of edges to corresponding semantic relations is labor-intensive, often requiring domain expertise. To this end, we introduce RoSE (Relation-oriented Semantic Edge-decomposition), a novel framework that leverages the capability of Large Language Models (LLMs) to decompose the graph structure by analyzing raw text attributes - in a fully automated manner. RoSE operates in two stages: (1) identifying meaningful relations using an LLM-based generator and discriminator, and (2) categorizing each edge into corresponding relations by analyzing textual contents associated with connected nodes via an LLM-based decomposer. Extensive experiments demonstrate that our model-agnostic framework significantly enhances node classification performance across various datasets, with improvements of up to 16% on the Wisconsin dataset.","sentences":["Recent advancements in text-attributed graphs (TAGs) have significantly improved the quality of node features by using the textual modeling capabilities of language models.","Despite this success, utilizing text attributes to enhance the predefined graph structure remains largely unexplored.","Our extensive analysis reveals that conventional edges on TAGs, treated as a single relation (e.g., hyperlinks) in previous literature, actually encompass mixed semantics (e.g., \"advised by\" and \"participates in\").","This simplification hinders the representation learning process of Graph Neural Networks (GNNs) on downstream tasks, even when integrated with advanced node features.","In contrast, we discover that decomposing these edges into distinct semantic relations significantly enhances the performance of GNNs.","Despite this, manually identifying and labeling of edges to corresponding semantic relations is labor-intensive, often requiring domain expertise.","To this end, we introduce RoSE (Relation-oriented Semantic Edge-decomposition), a novel framework that leverages the capability of Large Language Models (LLMs) to decompose the graph structure by analyzing raw text attributes - in a fully automated manner.","RoSE operates in two stages: (1) identifying meaningful relations using an LLM-based generator and discriminator, and (2) categorizing each edge into corresponding relations by analyzing textual contents associated with connected nodes via an LLM-based decomposer.","Extensive experiments demonstrate that our model-agnostic framework significantly enhances node classification performance across various datasets, with improvements of up to 16% on the Wisconsin dataset."],"url":"http://arxiv.org/abs/2405.18581v1","category":"cs.AI"}
{"created":"2024-05-28 20:54:41","title":"Artificial Intelligence in Industry 4.0: A Review of Integration Challenges for Industrial Systems","abstract":"In Industry 4.0, Cyber-Physical Systems (CPS) generate vast data sets that can be leveraged by Artificial Intelligence (AI) for applications including predictive maintenance and production planning. However, despite the demonstrated potential of AI, its widespread adoption in sectors like manufacturing remains limited. Our comprehensive review of recent literature, including standards and reports, pinpoints key challenges: system integration, data-related issues, managing workforce-related concerns and ensuring trustworthy AI. A quantitative analysis highlights particular challenges and topics that are important for practitioners but still need to be sufficiently investigated by academics. The paper briefly discusses existing solutions to these challenges and proposes avenues for future research. We hope that this survey serves as a resource for practitioners evaluating the cost-benefit implications of AI in CPS and for researchers aiming to address these urgent challenges.","sentences":["In Industry 4.0, Cyber-Physical Systems (CPS) generate vast data sets that can be leveraged by Artificial Intelligence (AI) for applications including predictive maintenance and production planning.","However, despite the demonstrated potential of AI, its widespread adoption in sectors like manufacturing remains limited.","Our comprehensive review of recent literature, including standards and reports, pinpoints key challenges: system integration, data-related issues, managing workforce-related concerns and ensuring trustworthy AI.","A quantitative analysis highlights particular challenges and topics that are important for practitioners but still need to be sufficiently investigated by academics.","The paper briefly discusses existing solutions to these challenges and proposes avenues for future research.","We hope that this survey serves as a resource for practitioners evaluating the cost-benefit implications of AI in CPS and for researchers aiming to address these urgent challenges."],"url":"http://arxiv.org/abs/2405.18580v1","category":"cs.AI"}
{"created":"2024-05-28 20:43:53","title":"Low-rank finetuning for LLMs: A fairness perspective","abstract":"Low-rank approximation techniques have become the de facto standard for fine-tuning Large Language Models (LLMs) due to their reduced computational and memory requirements. This paper investigates the effectiveness of these methods in capturing the shift of fine-tuning datasets from the initial pre-trained data distribution. Our findings reveal that there are cases in which low-rank fine-tuning falls short in learning such shifts. This, in turn, produces non-negligible side effects, especially when fine-tuning is adopted for toxicity mitigation in pre-trained models, or in scenarios where it is important to provide fair models. Through comprehensive empirical evidence on several models, datasets, and tasks, we show that low-rank fine-tuning inadvertently preserves undesirable biases and toxic behaviors. We also show that this extends to sequential decision-making tasks, emphasizing the need for careful evaluation to promote responsible LLMs development.","sentences":["Low-rank approximation techniques have become the de facto standard for fine-tuning Large Language Models (LLMs) due to their reduced computational and memory requirements.","This paper investigates the effectiveness of these methods in capturing the shift of fine-tuning datasets from the initial pre-trained data distribution.","Our findings reveal that there are cases in which low-rank fine-tuning falls short in learning such shifts.","This, in turn, produces non-negligible side effects, especially when fine-tuning is adopted for toxicity mitigation in pre-trained models, or in scenarios where it is important to provide fair models.","Through comprehensive empirical evidence on several models, datasets, and tasks, we show that low-rank fine-tuning inadvertently preserves undesirable biases and toxic behaviors.","We also show that this extends to sequential decision-making tasks, emphasizing the need for careful evaluation to promote responsible LLMs development."],"url":"http://arxiv.org/abs/2405.18572v1","category":"cs.LG"}
{"created":"2024-05-28 20:10:06","title":"Potential Field Based Deep Metric Learning","abstract":"Deep metric learning (DML) involves training a network to learn a semantically meaningful representation space. Many current approaches mine n-tuples of examples and model interactions within each tuplets. We present a novel, compositional DML model, inspired by electrostatic fields in physics that, instead of in tuples, represents the influence of each example (embedding) by a continuous potential field, and superposes the fields to obtain their combined global potential field. We use attractive/repulsive potential fields to represent interactions among embeddings from images of the same/different classes. Contrary to typical learning methods, where mutual influence of samples is proportional to their distance, we enforce reduction in such influence with distance, leading to a decaying field. We show that such decay helps improve performance on real world datasets with large intra-class variations and label noise. Like other proxy-based methods, we also use proxies to succinctly represent sub-populations of examples. We evaluate our method on three standard DML benchmarks- Cars-196, CUB-200-2011, and SOP datasets where it outperforms state-of-the-art baselines.","sentences":["Deep metric learning (DML) involves training a network to learn a semantically meaningful representation space.","Many current approaches mine n-tuples of examples and model interactions within each tuplets.","We present a novel, compositional DML model, inspired by electrostatic fields in physics that, instead of in tuples, represents the influence of each example (embedding) by a continuous potential field, and superposes the fields to obtain their combined global potential field.","We use attractive/repulsive potential fields to represent interactions among embeddings from images of the same/different classes.","Contrary to typical learning methods, where mutual influence of samples is proportional to their distance, we enforce reduction in such influence with distance, leading to a decaying field.","We show that such decay helps improve performance on real world datasets with large intra-class variations and label noise.","Like other proxy-based methods, we also use proxies to succinctly represent sub-populations of examples.","We evaluate our method on three standard DML benchmarks- Cars-196, CUB-200-2011, and SOP datasets where it outperforms state-of-the-art baselines."],"url":"http://arxiv.org/abs/2405.18560v1","category":"cs.CV"}
{"created":"2024-05-28 20:03:18","title":"Reinforcement Learning in Dynamic Treatment Regimes Needs Critical Reexamination","abstract":"In the rapidly changing healthcare landscape, the implementation of offline reinforcement learning (RL) in dynamic treatment regimes (DTRs) presents a mix of unprecedented opportunities and challenges. This position paper offers a critical examination of the current status of offline RL in the context of DTRs. We argue for a reassessment of applying RL in DTRs, citing concerns such as inconsistent and potentially inconclusive evaluation metrics, the absence of naive and supervised learning baselines, and the diverse choice of RL formulation in existing research. Through a case study with more than 17,000 evaluation experiments using a publicly available Sepsis dataset, we demonstrate that the performance of RL algorithms can significantly vary with changes in evaluation metrics and Markov Decision Process (MDP) formulations. Surprisingly, it is observed that in some instances, RL algorithms can be surpassed by random baselines subjected to policy evaluation methods and reward design. This calls for more careful policy evaluation and algorithm development in future DTR works. Additionally, we discussed potential enhancements toward more reliable development of RL-based dynamic treatment regimes and invited further discussion within the community. Code is available at https://github.com/GilesLuo/ReassessDTR.","sentences":["In the rapidly changing healthcare landscape, the implementation of offline reinforcement learning (RL) in dynamic treatment regimes (DTRs) presents a mix of unprecedented opportunities and challenges.","This position paper offers a critical examination of the current status of offline RL in the context of DTRs.","We argue for a reassessment of applying RL in DTRs, citing concerns such as inconsistent and potentially inconclusive evaluation metrics, the absence of naive and supervised learning baselines, and the diverse choice of RL formulation in existing research.","Through a case study with more than 17,000 evaluation experiments using a publicly available Sepsis dataset, we demonstrate that the performance of RL algorithms can significantly vary with changes in evaluation metrics and Markov Decision Process (MDP) formulations.","Surprisingly, it is observed that in some instances, RL algorithms can be surpassed by random baselines subjected to policy evaluation methods and reward design.","This calls for more careful policy evaluation and algorithm development in future DTR works.","Additionally, we discussed potential enhancements toward more reliable development of RL-based dynamic treatment regimes and invited further discussion within the community.","Code is available at https://github.com/GilesLuo/ReassessDTR."],"url":"http://arxiv.org/abs/2405.18556v1","category":"cs.LG"}
{"created":"2024-05-28 20:00:24","title":"Multigraph reconstruction via nonlinear random walk","abstract":"Over the last few years, network science has proved to be useful in modeling a variety of complex systems, composed of a large number of interconnected units. The intricate pattern of interactions often allows the system to achieve complex tasks, such as synchronization or collective motions. In this regard, the interplay between network structure and dynamics has long been recognized as a cornerstone of network science. Among dynamical processes, random walks are undoubtedly among the most studied stochastic processes. While traditionally, the random walkers are assumed to be independent, this assumption breaks down if nodes are endowed with a finite carrying capacity, a feature shared by many real-life systems. Recently, a class of nonlinear diffusion processes accounting for the finite carrying capacities of the nodes was introduced. The stationary nodes densities were shown to be nonlinearly correlated with the nodes degrees, allowing to uncover the network structure by performing a few measurements of the stationary density at the level of a single arbitrary node and by solving an inverse problem. In this work, we extend this class of nonlinear diffusion processes to the case of multigraphs, in which links between nodes carry distinct attributes. Assuming the knowledge of the pattern of interactions associated with one type of links, we show how the degree distribution of the whole multigraph can be reconstructed. The effectiveness of the reconstruction algorithm is demonstrated through simulations on various multigraph topologies.","sentences":["Over the last few years, network science has proved to be useful in modeling a variety of complex systems, composed of a large number of interconnected units.","The intricate pattern of interactions often allows the system to achieve complex tasks, such as synchronization or collective motions.","In this regard, the interplay between network structure and dynamics has long been recognized as a cornerstone of network science.","Among dynamical processes, random walks are undoubtedly among the most studied stochastic processes.","While traditionally, the random walkers are assumed to be independent, this assumption breaks down if nodes are endowed with a finite carrying capacity, a feature shared by many real-life systems.","Recently, a class of nonlinear diffusion processes accounting for the finite carrying capacities of the nodes was introduced.","The stationary nodes densities were shown to be nonlinearly correlated with the nodes degrees, allowing to uncover the network structure by performing a few measurements of the stationary density at the level of a single arbitrary node and by solving an inverse problem.","In this work, we extend this class of nonlinear diffusion processes to the case of multigraphs, in which links between nodes carry distinct attributes.","Assuming the knowledge of the pattern of interactions associated with one type of links, we show how the degree distribution of the whole multigraph can be reconstructed.","The effectiveness of the reconstruction algorithm is demonstrated through simulations on various multigraph topologies."],"url":"http://arxiv.org/abs/2405.18555v1","category":"physics.soc-ph"}
{"created":"2024-05-28 19:54:46","title":"The FAIIR Tool: A Conversational AI Agent Assistant for Youth Mental Health Service Provision","abstract":"World's healthcare systems and mental health agencies face both a growing demand for youth mental health services, alongside a simultaneous challenge of limited resources. Given these constraints, this work presents our experience in the creation and evaluation of the FAIIR (Frontline Assistant: Issue Identification and Recommendation) tool, an ensemble of domain-adapted and fine-tuned transformer models, leveraging natural language processing to identify issues that youth may be experiencing. We explore the technical development, performance, and validation processes leveraged for the FAIIR tool in application to situations of frontline crisis response via Kids Help Phone. Frontline Crisis Responders assign an issue tag from a defined list following each conversation. Assisting with the identification of issues of relevance helps reduce the burden on CRs, ensuring that appropriate resources can be provided and that active rescues and mandatory reporting can take place in critical situations requiring immediate de-escalation.","sentences":["World's healthcare systems and mental health agencies face both a growing demand for youth mental health services, alongside a simultaneous challenge of limited resources.","Given these constraints, this work presents our experience in the creation and evaluation of the FAIIR (Frontline Assistant: Issue Identification and Recommendation) tool, an ensemble of domain-adapted and fine-tuned transformer models, leveraging natural language processing to identify issues that youth may be experiencing.","We explore the technical development, performance, and validation processes leveraged for the FAIIR tool in application to situations of frontline crisis response via Kids Help Phone.","Frontline Crisis Responders assign an issue tag from a defined list following each conversation.","Assisting with the identification of issues of relevance helps reduce the burden on CRs, ensuring that appropriate resources can be provided and that active rescues and mandatory reporting can take place in critical situations requiring immediate de-escalation."],"url":"http://arxiv.org/abs/2405.18553v1","category":"cs.AI"}
{"created":"2024-05-28 19:54:26","title":"SGD method for entropy error function with smoothing l0 regularization for neural networks","abstract":"The entropy error function has been widely used in neural networks. Nevertheless, the network training based on this error function generally leads to a slow convergence rate, and can easily be trapped in a local minimum or even with the incorrect saturation problem in practice. In fact, there are many results based on entropy error function in neural network and its applications. However, the theory of such an algorithm and its convergence have not been fully studied so far. To tackle the issue, we propose a novel entropy function with smoothing l0 regularization for feed-forward neural networks. Using real-world datasets, we performed an empirical evaluation to demonstrate that the newly conceived algorithm allows us to substantially improve the prediction performance of the considered neural networks. More importantly, the experimental results also show that our proposed function brings in more precise classifications, compared to well-founded baselines. Our work is novel as it enables neural networks to learn effectively, producing more accurate predictions compared to state-of-the-art algorithms. In this respect, we expect that the algorithm will contribute to existing studies in the field, advancing research in Machine Learning and Deep Learning.","sentences":["The entropy error function has been widely used in neural networks.","Nevertheless, the network training based on this error function generally leads to a slow convergence rate, and can easily be trapped in a local minimum or even with the incorrect saturation problem in practice.","In fact, there are many results based on entropy error function in neural network and its applications.","However, the theory of such an algorithm and its convergence have not been fully studied so far.","To tackle the issue, we propose a novel entropy function with smoothing l0 regularization for feed-forward neural networks.","Using real-world datasets, we performed an empirical evaluation to demonstrate that the newly conceived algorithm allows us to substantially improve the prediction performance of the considered neural networks.","More importantly, the experimental results also show that our proposed function brings in more precise classifications, compared to well-founded baselines.","Our work is novel as it enables neural networks to learn effectively, producing more accurate predictions compared to state-of-the-art algorithms.","In this respect, we expect that the algorithm will contribute to existing studies in the field, advancing research in Machine Learning and Deep Learning."],"url":"http://arxiv.org/abs/2405.18552v1","category":"cs.LG"}
{"created":"2024-05-28 19:30:43","title":"The Computational Complexity of Formal Reasoning for Encoder-Only Transformers","abstract":"We investigate challenges and possibilities of formal reasoning for encoder-only transformers (EOT), meaning sound and complete methods for verifying or interpreting behaviour. In detail, we condense related formal reasoning tasks in the form of a naturally occurring satisfiability problem (SAT). We find that SAT is undecidable if we consider EOT, commonly considered in the expressiveness community. Furthermore, we identify practical scenarios where SAT is decidable and establish corresponding complexity bounds. Besides trivial cases, we find that quantized EOT, namely those restricted by some fixed-width arithmetic, lead to the decidability of SAT due to their limited attention capabilities. However, the problem remains difficult, as we establish those scenarios where SAT is NEXPTIME-hard and those where we can show that it is solvable in NEXPTIME for quantized EOT. To complement our theoretical results, we put our findings and their implications in the overall perspective of formal reasoning.","sentences":["We investigate challenges and possibilities of formal reasoning for encoder-only transformers (EOT), meaning sound and complete methods for verifying or interpreting behaviour.","In detail, we condense related formal reasoning tasks in the form of a naturally occurring satisfiability problem (SAT).","We find that SAT is undecidable if we consider EOT, commonly considered in the expressiveness community.","Furthermore, we identify practical scenarios where SAT is decidable and establish corresponding complexity bounds.","Besides trivial cases, we find that quantized EOT, namely those restricted by some fixed-width arithmetic, lead to the decidability of SAT due to their limited attention capabilities.","However, the problem remains difficult, as we establish those scenarios where SAT is NEXPTIME-hard and those where we can show that it is solvable in NEXPTIME for quantized EOT.","To complement our theoretical results, we put our findings and their implications in the overall perspective of formal reasoning."],"url":"http://arxiv.org/abs/2405.18548v1","category":"cs.LO"}
{"created":"2024-05-28 19:27:44","title":"Capacity-Maximizing Dynamic User Association in Double RIS-Aided Broadcast Networks","abstract":"We introduce an information-theoretic framework to dynamically pair up different reconfigurable intelligent surfaces (RISs) with wireless users with goal of maximizing the fundamental network capacity. We focus on a double RIS-aided broadcast packet network with two users. We show using a dynamic RIS-user association and an opportunistic protocol, the network capacity could be significantly enhanced and superior to other benchmarks with static associations. The results include new outer-bounds on network capacity and their achievability. We discuss the optimal RIS-user association.","sentences":["We introduce an information-theoretic framework to dynamically pair up different reconfigurable intelligent surfaces (RISs) with wireless users with goal of maximizing the fundamental network capacity.","We focus on a double RIS-aided broadcast packet network with two users.","We show using a dynamic RIS-user association and an opportunistic protocol, the network capacity could be significantly enhanced and superior to other benchmarks with static associations.","The results include new outer-bounds on network capacity and their achievability.","We discuss the optimal RIS-user association."],"url":"http://arxiv.org/abs/2405.18546v1","category":"cs.IT"}
{"created":"2024-05-28 19:17:48","title":"Automatic detection of cognitive impairment in elderly people using an entertainment chatbot with Natural Language Processing capabilities","abstract":"Previous researchers have proposed intelligent systems for therapeutic monitoring of cognitive impairments. However, most existing practical approaches for this purpose are based on manual tests. This raises issues such as excessive caretaking effort and the white-coat effect. To avoid these issues, we present an intelligent conversational system for entertaining elderly people with news of their interest that monitors cognitive impairment transparently. Automatic chatbot dialogue stages allow assessing content description skills and detecting cognitive impairment with Machine Learning algorithms. We create these dialogue flows automatically from updated news items using Natural Language Generation techniques. The system also infers the gold standard of the answers to the questions, so it can assess cognitive capabilities automatically by comparing these answers with the user responses. It employs a similarity metric with values in [0, 1], in increasing level of similarity. To evaluate the performance and usability of our approach, we have conducted field tests with a test group of 30 elderly people in the earliest stages of dementia, under the supervision of gerontologists. In the experiments, we have analysed the effect of stress and concentration in these users. Those without cognitive impairment performed up to five times better. In particular, the similarity metric varied between 0.03, for stressed and unfocused participants, and 0.36, for relaxed and focused users. Finally, we developed a Machine Learning algorithm based on textual analysis features for automatic cognitive impairment detection, which attained accuracy, F-measure and recall levels above 80%. We have thus validated the automatic approach to detect cognitive impairment in elderly people based on entertainment content.","sentences":["Previous researchers have proposed intelligent systems for therapeutic monitoring of cognitive impairments.","However, most existing practical approaches for this purpose are based on manual tests.","This raises issues such as excessive caretaking effort and the white-coat effect.","To avoid these issues, we present an intelligent conversational system for entertaining elderly people with news of their interest that monitors cognitive impairment transparently.","Automatic chatbot dialogue stages allow assessing content description skills and detecting cognitive impairment with Machine Learning algorithms.","We create these dialogue flows automatically from updated news items using Natural Language Generation techniques.","The system also infers the gold standard of the answers to the questions, so it can assess cognitive capabilities automatically by comparing these answers with the user responses.","It employs a similarity metric with values in [0, 1], in increasing level of similarity.","To evaluate the performance and usability of our approach, we have conducted field tests with a test group of 30 elderly people in the earliest stages of dementia, under the supervision of gerontologists.","In the experiments, we have analysed the effect of stress and concentration in these users.","Those without cognitive impairment performed up to five times better.","In particular, the similarity metric varied between 0.03, for stressed and unfocused participants, and 0.36, for relaxed and focused users.","Finally, we developed a Machine Learning algorithm based on textual analysis features for automatic cognitive impairment detection, which attained accuracy, F-measure and recall levels above 80%.","We have thus validated the automatic approach to detect cognitive impairment in elderly people based on entertainment content."],"url":"http://arxiv.org/abs/2405.18542v1","category":"cs.AI"}
{"created":"2024-05-28 19:14:16","title":"The Past, Present, and Future of Automation in Model-Driven Engineering","abstract":"Model-Driven Engineering (MDE) provides a huge body of knowledge of automation for many different engineering tasks, especially those involving transitioning from design to implementation. With the huge progress made on Artificial Intelligence (AI) techniques, questions arise for the future of MDE such as how existing MDE techniques and technologies can be improved or how other activities which currently lack dedicated support can also be automated. However, at the same time, it has to be revisited where and how models should be used to keep the engineers in the loop for creating, operating, and maintaining complex systems. To trigger dedicated research on these open points, we discuss the history of automation in MDE and present perspectives on how automation in MDE can be further improved and which obstacles have to be overcome in the medium and long term perspective.","sentences":["Model-Driven Engineering (MDE) provides a huge body of knowledge of automation for many different engineering tasks, especially those involving transitioning from design to implementation.","With the huge progress made on Artificial Intelligence (AI) techniques, questions arise for the future of MDE such as how existing MDE techniques and technologies can be improved or how other activities which currently lack dedicated support can also be automated.","However, at the same time, it has to be revisited where and how models should be used to keep the engineers in the loop for creating, operating, and maintaining complex systems.","To trigger dedicated research on these open points, we discuss the history of automation in MDE and present perspectives on how automation in MDE can be further improved and which obstacles have to be overcome in the medium and long term perspective."],"url":"http://arxiv.org/abs/2405.18539v1","category":"cs.SE"}
{"created":"2024-05-28 18:50:34","title":"Interacting Streams of Cognitive Active Agents in a Three-Way Intersection","abstract":"The emergent collective motion of active agents - in particular pedestrians - at a three-way intersection is studied by Langevin simulations of cognitive intelligent active Brownian particles (iABPs) with directed visual perception and self-steering avoidance. Depending on the maneuverability $\\Omega$, the goal fixation $K$, and the vision angle $\\psi$, different types of pedestrian motion emerge. At intermediate relative maneuverability $\\Delta = \\Omega/K$ and large $\\psi$, pedestrians have noisy trajectories due to multiple scattering events as they encounter other pedestrians in their field of view. For $\\psi = \\pi$ and large relative maneuverability $\\Delta$, an effectively jammed state is found, which belongs to the percolation universality class. For small $\\psi$, agents exhibit localised clustering and flocking, while for intermediate $\\psi$ self-organized rotational flows can emerge. The analysis of mean squared displacement and velocity auto-correlation of the agents reveals that the motion is well described by fractional Brownian Motion with positively correlated noise. Finally, despite the rich variety of collective behaviour, the fundamental flow diagram for the three-way-crossing setup shows a universal curve for the different vision angles. Our research provides valuable insights into the importance of vision angle and self-steering avoidance on pedestrian dynamics in semi-dense crowds.","sentences":["The emergent collective motion of active agents - in particular pedestrians - at a three-way intersection is studied by Langevin simulations of cognitive intelligent active Brownian particles (iABPs) with directed visual perception and self-steering avoidance.","Depending on the maneuverability $\\Omega$, the goal fixation $K$, and the vision angle $\\psi$, different types of pedestrian motion emerge.","At intermediate relative maneuverability $\\Delta = \\Omega/K$ and large $\\psi$, pedestrians have noisy trajectories due to multiple scattering events as they encounter other pedestrians in their field of view.","For $\\psi = \\pi$ and large relative maneuverability $\\Delta$, an effectively jammed state is found, which belongs to the percolation universality class.","For small $\\psi$, agents exhibit localised clustering and flocking, while for intermediate $\\psi$ self-organized rotational flows can emerge.","The analysis of mean squared displacement and velocity auto-correlation of the agents reveals that the motion is well described by fractional Brownian Motion with positively correlated noise.","Finally, despite the rich variety of collective behaviour, the fundamental flow diagram for the three-way-crossing setup shows a universal curve for the different vision angles.","Our research provides valuable insights into the importance of vision angle and self-steering avoidance on pedestrian dynamics in semi-dense crowds."],"url":"http://arxiv.org/abs/2405.18528v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-28 18:49:17","title":"Task-Driven Uncertainty Quantification in Inverse Problems via Conformal Prediction","abstract":"In imaging inverse problems, one seeks to recover an image from missing/corrupted measurements. Because such problems are ill-posed, there is great motivation to quantify the uncertainty induced by the measurement-and-recovery process. Motivated by applications where the recovered image is used for a downstream task, such as soft-output classification, we propose a task-centered approach to uncertainty quantification. In particular, we use conformal prediction to construct an interval that is guaranteed to contain the task output from the true image up to a user-specified probability, and we use the width of that interval to quantify the uncertainty contributed by measurement-and-recovery. For posterior-sampling-based image recovery, we construct locally adaptive prediction intervals. Furthermore, we propose to collect measurements over multiple rounds, stopping as soon as the task uncertainty falls below an acceptable level. We demonstrate our methodology on accelerated magnetic resonance imaging (MRI).","sentences":["In imaging inverse problems, one seeks to recover an image from missing/corrupted measurements.","Because such problems are ill-posed, there is great motivation to quantify the uncertainty induced by the measurement-and-recovery process.","Motivated by applications where the recovered image is used for a downstream task, such as soft-output classification, we propose a task-centered approach to uncertainty quantification.","In particular, we use conformal prediction to construct an interval that is guaranteed to contain the task output from the true image up to a user-specified probability, and we use the width of that interval to quantify the uncertainty contributed by measurement-and-recovery.","For posterior-sampling-based image recovery, we construct locally adaptive prediction intervals.","Furthermore, we propose to collect measurements over multiple rounds, stopping as soon as the task uncertainty falls below an acceptable level.","We demonstrate our methodology on accelerated magnetic resonance imaging (MRI)."],"url":"http://arxiv.org/abs/2405.18527v1","category":"cs.CV"}
{"created":"2024-05-28 18:44:15","title":"TripletMix: Triplet Data Augmentation for 3D Understanding","abstract":"Data augmentation has proven to be a vital tool for enhancing the generalization capabilities of deep learning models, especially in the context of 3D vision where traditional datasets are often limited. Despite previous advancements, existing methods primarily cater to unimodal data scenarios, leaving a gap in the augmentation of multimodal triplet data, which integrates text, images, and point clouds. Simultaneously augmenting all three modalities enhances diversity and improves alignment across modalities, resulting in more comprehensive and robust 3D representations. To address this gap, we propose TripletMix, a novel approach to address the previously unexplored issue of multimodal data augmentation in 3D understanding. TripletMix innovatively applies the principles of mixed-based augmentation to multimodal triplet data, allowing for the preservation and optimization of cross-modal connections. Our proposed TripletMix combines feature-level and input-level augmentations to achieve dual enhancement between raw data and latent features, significantly improving the model's cross-modal understanding and generalization capabilities by ensuring feature consistency and providing diverse and realistic training samples. We demonstrate that TripletMix not only improves the baseline performance of models in various learning scenarios including zero-shot and linear probing classification but also significantly enhances model generalizability. Notably, we improved the zero-shot classification accuracy on ScanObjectNN from 51.3 percent to 61.9 percent, and on Objaverse-LVIS from 46.8 percent to 51.4 percent. Our findings highlight the potential of multimodal data augmentation to significantly advance 3D object recognition and understanding.","sentences":["Data augmentation has proven to be a vital tool for enhancing the generalization capabilities of deep learning models, especially in the context of 3D vision where traditional datasets are often limited.","Despite previous advancements, existing methods primarily cater to unimodal data scenarios, leaving a gap in the augmentation of multimodal triplet data, which integrates text, images, and point clouds.","Simultaneously augmenting all three modalities enhances diversity and improves alignment across modalities, resulting in more comprehensive and robust 3D representations.","To address this gap, we propose TripletMix, a novel approach to address the previously unexplored issue of multimodal data augmentation in 3D understanding.","TripletMix innovatively applies the principles of mixed-based augmentation to multimodal triplet data, allowing for the preservation and optimization of cross-modal connections.","Our proposed TripletMix combines feature-level and input-level augmentations to achieve dual enhancement between raw data and latent features, significantly improving the model's cross-modal understanding and generalization capabilities by ensuring feature consistency and providing diverse and realistic training samples.","We demonstrate that TripletMix not only improves the baseline performance of models in various learning scenarios including zero-shot and linear probing classification but also significantly enhances model generalizability.","Notably, we improved the zero-shot classification accuracy on ScanObjectNN from 51.3 percent to 61.9 percent, and on Objaverse-LVIS from 46.8 percent to 51.4 percent.","Our findings highlight the potential of multimodal data augmentation to significantly advance 3D object recognition and understanding."],"url":"http://arxiv.org/abs/2405.18523v1","category":"cs.CV"}
{"created":"2024-05-28 18:38:46","title":"Offline-Boosted Actor-Critic: Adaptively Blending Optimal Historical Behaviors in Deep Off-Policy RL","abstract":"Off-policy reinforcement learning (RL) has achieved notable success in tackling many complex real-world tasks, by leveraging previously collected data for policy learning. However, most existing off-policy RL algorithms fail to maximally exploit the information in the replay buffer, limiting sample efficiency and policy performance. In this work, we discover that concurrently training an offline RL policy based on the shared online replay buffer can sometimes outperform the original online learning policy, though the occurrence of such performance gains remains uncertain. This motivates a new possibility of harnessing the emergent outperforming offline optimal policy to improve online policy learning. Based on this insight, we present Offline-Boosted Actor-Critic (OBAC), a model-free online RL framework that elegantly identifies the outperforming offline policy through value comparison, and uses it as an adaptive constraint to guarantee stronger policy learning performance. Our experiments demonstrate that OBAC outperforms other popular model-free RL baselines and rivals advanced model-based RL methods in terms of sample efficiency and asymptotic performance across 53 tasks spanning 6 task suites.","sentences":["Off-policy reinforcement learning (RL) has achieved notable success in tackling many complex real-world tasks, by leveraging previously collected data for policy learning.","However, most existing off-policy RL algorithms fail to maximally exploit the information in the replay buffer, limiting sample efficiency and policy performance.","In this work, we discover that concurrently training an offline RL policy based on the shared online replay buffer can sometimes outperform the original online learning policy, though the occurrence of such performance gains remains uncertain.","This motivates a new possibility of harnessing the emergent outperforming offline optimal policy to improve online policy learning.","Based on this insight, we present Offline-Boosted Actor-Critic (OBAC), a model-free online RL framework that elegantly identifies the outperforming offline policy through value comparison, and uses it as an adaptive constraint to guarantee stronger policy learning performance.","Our experiments demonstrate that OBAC outperforms other popular model-free RL baselines and rivals advanced model-based RL methods in terms of sample efficiency and asymptotic performance across 53 tasks spanning 6 task suites."],"url":"http://arxiv.org/abs/2405.18520v1","category":"cs.LG"}
{"created":"2024-05-28 18:31:14","title":"Understanding Transformer Reasoning Capabilities via Graph Algorithms","abstract":"Which transformer scaling regimes are able to perfectly solve different classes of algorithmic problems? While tremendous empirical advances have been attained by transformer-based neural networks, a theoretical understanding of their algorithmic reasoning capabilities in realistic parameter regimes is lacking. We investigate this question in terms of the network's depth, width, and number of extra tokens for algorithm execution. Our novel representational hierarchy separates 9 algorithmic reasoning problems into classes solvable by transformers in different realistic parameter scaling regimes. We prove that logarithmic depth is necessary and sufficient for tasks like graph connectivity, while single-layer transformers with small embedding dimensions can solve contextual retrieval tasks. We also support our theoretical analysis with ample empirical evidence using the GraphQA benchmark. These results show that transformers excel at many graph reasoning tasks, even outperforming specialized graph neural networks.","sentences":["Which transformer scaling regimes are able to perfectly solve different classes of algorithmic problems?","While tremendous empirical advances have been attained by transformer-based neural networks, a theoretical understanding of their algorithmic reasoning capabilities in realistic parameter regimes is lacking.","We investigate this question in terms of the network's depth, width, and number of extra tokens for algorithm execution.","Our novel representational hierarchy separates 9 algorithmic reasoning problems into classes solvable by transformers in different realistic parameter scaling regimes.","We prove that logarithmic depth is necessary and sufficient for tasks like graph connectivity, while single-layer transformers with small embedding dimensions can solve contextual retrieval tasks.","We also support our theoretical analysis with ample empirical evidence using the GraphQA benchmark.","These results show that transformers excel at many graph reasoning tasks, even outperforming specialized graph neural networks."],"url":"http://arxiv.org/abs/2405.18512v1","category":"cs.LG"}
{"created":"2024-05-28 18:26:57","title":"Improved Emotional Alignment of AI and Humans: Human Ratings of Emotions Expressed by Stable Diffusion v1, DALL-E 2, and DALL-E 3","abstract":"Generative AI systems are increasingly capable of expressing emotions via text and imagery. Effective emotional expression will likely play a major role in the efficacy of AI systems -- particularly those designed to support human mental health and wellbeing. This motivates our present research to better understand the alignment of AI expressed emotions with the human perception of emotions. When AI tries to express a particular emotion, how might we assess whether they are successful? To answer this question, we designed a survey to measure the alignment between emotions expressed by generative AI and human perceptions. Three generative image models (DALL-E 2, DALL-E 3 and Stable Diffusion v1) were used to generate 240 examples of images, each of which was based on a prompt designed to express five positive and five negative emotions across both humans and robots. 24 participants recruited from the Prolific website rated the alignment of AI-generated emotional expressions with a text prompt used to generate the emotion (i.e., \"A robot expressing the emotion amusement\"). The results of our evaluation suggest that generative AI models are indeed capable of producing emotional expressions that are well-aligned with a range of human emotions; however, we show that the alignment significantly depends upon the AI model used and the emotion itself. We analyze variations in the performance of these systems to identify gaps for future improvement. We conclude with a discussion of the implications for future AI systems designed to support mental health and wellbeing.","sentences":["Generative AI systems are increasingly capable of expressing emotions via text and imagery.","Effective emotional expression will likely play a major role in the efficacy of AI systems -- particularly those designed to support human mental health and wellbeing.","This motivates our present research to better understand the alignment of AI expressed emotions with the human perception of emotions.","When AI tries to express a particular emotion, how might we assess whether they are successful?","To answer this question, we designed a survey to measure the alignment between emotions expressed by generative AI and human perceptions.","Three generative image models (DALL-E 2, DALL-E 3 and Stable Diffusion v1) were used to generate 240 examples of images, each of which was based on a prompt designed to express five positive and five negative emotions across both humans and robots.","24 participants recruited from the Prolific website rated the alignment of AI-generated emotional expressions with a text prompt used to generate the emotion (i.e., \"A robot expressing the emotion amusement\").","The results of our evaluation suggest that generative AI models are indeed capable of producing emotional expressions that are well-aligned with a range of human emotions; however, we show that the alignment significantly depends upon the AI model used and the emotion itself.","We analyze variations in the performance of these systems to identify gaps for future improvement.","We conclude with a discussion of the implications for future AI systems designed to support mental health and wellbeing."],"url":"http://arxiv.org/abs/2405.18510v1","category":"cs.AI"}
{"created":"2024-05-28 18:24:16","title":"Injecting Hierarchical Biological Priors into Graph Neural Networks for Flow Cytometry Prediction","abstract":"In the complex landscape of hematologic samples such as peripheral blood or bone marrow derived from flow cytometry (FC) data, cell-level prediction presents profound challenges. This work explores injecting hierarchical prior knowledge into graph neural networks (GNNs) for single-cell multi-class classification of tabular cellular data. By representing the data as graphs and encoding hierarchical relationships between classes, we propose our hierarchical plug-in method to be applied to several GNN models, namely, FCHC-GNN, and effectively designed to capture neighborhood information crucial for single-cell FC domain. Extensive experiments on our cohort of 19 distinct patients, demonstrate that incorporating hierarchical biological constraints boosts performance significantly across multiple metrics compared to baseline GNNs without such priors. The proposed approach highlights the importance of structured inductive biases for gaining improved generalization in complex biological prediction tasks.","sentences":["In the complex landscape of hematologic samples such as peripheral blood or bone marrow derived from flow cytometry (FC) data, cell-level prediction presents profound challenges.","This work explores injecting hierarchical prior knowledge into graph neural networks (GNNs) for single-cell multi-class classification of tabular cellular data.","By representing the data as graphs and encoding hierarchical relationships between classes, we propose our hierarchical plug-in method to be applied to several GNN models, namely, FCHC-GNN, and effectively designed to capture neighborhood information crucial for single-cell FC domain.","Extensive experiments on our cohort of 19 distinct patients, demonstrate that incorporating hierarchical biological constraints boosts performance significantly across multiple metrics compared to baseline GNNs without such priors.","The proposed approach highlights the importance of structured inductive biases for gaining improved generalization in complex biological prediction tasks."],"url":"http://arxiv.org/abs/2405.18507v1","category":"cs.LG"}
{"created":"2024-05-28 18:08:34","title":"Capacity Results for Non-Ergodic Multi-Modal Broadcast Channels with Controllable Statistics","abstract":"Movable antennas and reconfigurable intelligent surfaces enable a new paradigm in which channel statistics can be controlled and altered. Further, the known trajectory and operation protocol of communication satellites results in networks with predictable statistics. The predictability of future changes results in a non-ergodic model for which the fundamentals are largely unknown. We consider the canonical two-user broadcast erasure channel in which channel statistics vary at a priori known points. We consider a multi-modal setting with two non-transient modes (whose lengths scale linearly with the blocklength) and an arbitrary number of transient modes. We provide a new set of outer-bounds on the capacity region of this problem when the encoder has access to causal ACK/NACK feedback. The outer-bounds reveal the significant role of the non-transient mode with higher erasure probability both on the outer and the inner bounds. We show the outer-bounds are achievable in non-trivial regimes, characterizing the capacity region for a wide range of parameters. We also discuss the regimes where the inner and outer bounds diverge and analyze the gap between the two. A key finding of this work is the significant gain of inter-modal coding over the separate treating of individual modes.","sentences":["Movable antennas and reconfigurable intelligent surfaces enable a new paradigm in which channel statistics can be controlled and altered.","Further, the known trajectory and operation protocol of communication satellites results in networks with predictable statistics.","The predictability of future changes results in a non-ergodic model for which the fundamentals are largely unknown.","We consider the canonical two-user broadcast erasure channel in which channel statistics vary at a priori known points.","We consider a multi-modal setting with two non-transient modes (whose lengths scale linearly with the blocklength) and an arbitrary number of transient modes.","We provide a new set of outer-bounds on the capacity region of this problem when the encoder has access to causal ACK/NACK feedback.","The outer-bounds reveal the significant role of the non-transient mode with higher erasure probability both on the outer and the inner bounds.","We show the outer-bounds are achievable in non-trivial regimes, characterizing the capacity region for a wide range of parameters.","We also discuss the regimes where the inner and outer bounds diverge and analyze the gap between the two.","A key finding of this work is the significant gain of inter-modal coding over the separate treating of individual modes."],"url":"http://arxiv.org/abs/2405.18497v1","category":"cs.IT"}
{"created":"2024-05-28 18:01:52","title":"LLMs and Memorization: On Quality and Specificity of Copyright Compliance","abstract":"Memorization in large language models (LLMs) is a growing concern. LLMs have been shown to easily reproduce parts of their training data, including copyrighted work. This is an important problem to solve, as it may violate existing copyright laws as well as the European AI Act. In this work, we propose a systematic analysis to quantify the extent of potential copyright infringements in LLMs using European law as an example. Unlike previous work, we evaluate instruction-finetuned models in a realistic end-user scenario. Our analysis builds on a proposed threshold of 160 characters, which we borrow from the German Copyright Service Provider Act and a fuzzy text matching algorithm to identify potentially copyright-infringing textual reproductions. The specificity of countermeasures against copyright infringement is analyzed by comparing model behavior on copyrighted and public domain data. We investigate what behaviors models show instead of producing protected text (such as refusal or hallucination) and provide a first legal assessment of these behaviors. We find that there are huge differences in copyright compliance, specificity, and appropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous perform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing a particularly low absolute number of potential copyright violations. Code will be published soon.","sentences":["Memorization in large language models (LLMs) is a growing concern.","LLMs have been shown to easily reproduce parts of their training data, including copyrighted work.","This is an important problem to solve, as it may violate existing copyright laws as well as the European AI Act.","In this work, we propose a systematic analysis to quantify the extent of potential copyright infringements in LLMs using European law as an example.","Unlike previous work, we evaluate instruction-finetuned models in a realistic end-user scenario.","Our analysis builds on a proposed threshold of 160 characters, which we borrow from the German Copyright Service Provider Act and a fuzzy text matching algorithm to identify potentially copyright-infringing textual reproductions.","The specificity of countermeasures against copyright infringement is analyzed by comparing model behavior on copyrighted and public domain data.","We investigate what behaviors models show instead of producing protected text (such as refusal or hallucination) and provide a first legal assessment of these behaviors.","We find that there are huge differences in copyright compliance, specificity, and appropriate refusal among popular LLMs.","Alpaca, GPT 4, GPT 3.5, and Luminous perform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing a particularly low absolute number of potential copyright violations.","Code will be published soon."],"url":"http://arxiv.org/abs/2405.18492v1","category":"cs.CL"}
{"created":"2024-05-28 18:00:05","title":"Collectively enhanced ground-state cooling in subwavelength atomic arrays","abstract":"Subwavelength atomic arrays in free space are becoming a leading platform for exploring emergent many-body quantum phenomena. These arrays feature strong light-induced dipole-dipole interactions, resulting in subradiant collective resonances characterized by narrowed linewidths. In this work, we present a sideband cooling scheme for atoms trapped in subwavelength arrays that utilizes these narrow collective resonances. We derive an effective master equation for the atomic motion by adiabatically eliminating the internal degrees of freedom of the atoms, and validate its prediction with numerical simulations of the full system. Our results demonstrate that subradiant resonances enable the cooling of ensembles of atoms to temperatures lower than those achievable without dipole interactions, provided the atoms have different trap frequencies. Remarkably, narrow collective resonances can be sideband-resolved even when the individual atomic transition is not. In such scenarios, ground state cooling becomes feasible solely due to light-induced dipole-dipole interactions. This approach could be utilized for future quantum technologies based on dense ensembles of emitters, and paves the way towards harnessing many-body cooperative decay for enhanced motional control.","sentences":["Subwavelength atomic arrays in free space are becoming a leading platform for exploring emergent many-body quantum phenomena.","These arrays feature strong light-induced dipole-dipole interactions, resulting in subradiant collective resonances characterized by narrowed linewidths.","In this work, we present a sideband cooling scheme for atoms trapped in subwavelength arrays that utilizes these narrow collective resonances.","We derive an effective master equation for the atomic motion by adiabatically eliminating the internal degrees of freedom of the atoms, and validate its prediction with numerical simulations of the full system.","Our results demonstrate that subradiant resonances enable the cooling of ensembles of atoms to temperatures lower than those achievable without dipole interactions, provided the atoms have different trap frequencies.","Remarkably, narrow collective resonances can be sideband-resolved even when the individual atomic transition is not.","In such scenarios, ground state cooling becomes feasible solely due to light-induced dipole-dipole interactions.","This approach could be utilized for future quantum technologies based on dense ensembles of emitters, and paves the way towards harnessing many-body cooperative decay for enhanced motional control."],"url":"http://arxiv.org/abs/2405.18482v1","category":"quant-ph"}
{"created":"2024-05-28 18:00:01","title":"Symbolic Regression for Beyond the Standard Model Physics","abstract":"We propose symbolic regression as a powerful tool for studying Beyond the Standard Model physics. As a benchmark model, we consider the so-called Constrained Minimal Supersymmetric Standard Model, which has a four-dimensional parameter space defined at the GUT scale. We provide a set of analytical expressions that reproduce three low-energy observables of interest in terms of the parameters of the theory: the Higgs mass, the contribution to the anomalous magnetic moment of the muon, and the cold dark matter relic density. To demonstrate the power of the approach, we employ the symbolic expressions in a global fits analysis to derive the posterior probability densities of the parameters, which are obtained extremely rapidly in comparison with conventional methods.","sentences":["We propose symbolic regression as a powerful tool for studying Beyond the Standard Model physics.","As a benchmark model, we consider the so-called Constrained Minimal Supersymmetric Standard Model, which has a four-dimensional parameter space defined at the GUT scale.","We provide a set of analytical expressions that reproduce three low-energy observables of interest in terms of the parameters of the theory: the Higgs mass, the contribution to the anomalous magnetic moment of the muon, and the cold dark matter relic density.","To demonstrate the power of the approach, we employ the symbolic expressions in a global fits analysis to derive the posterior probability densities of the parameters, which are obtained extremely rapidly in comparison with conventional methods."],"url":"http://arxiv.org/abs/2405.18471v1","category":"hep-ph"}
{"created":"2024-05-29 17:57:46","title":"Barium stars as tracers of s-process nucleosynthesis in AGB stars III. Systematic deviations from the AGB models","abstract":"Barium (Ba) stars help to verify asymptotic giant branch (AGB) star nucleosynthesis models since they experienced pollution from an AGB binary companion and thus their spectra carry the signatures of the slow neutron capture process (s process). For 180 Ba stars, we searched for AGB stellar models that match the observed abundance patterns. We employed three machine learning algorithms as classifiers: a Random Forest method, developed for this work, and the two classifiers used in our previous study. We studied the statistical behaviour of the s-process elements in the observational sample to investigate if the AGB models systematically under- or overpredict the abundances observed in the Ba stars and show the results in the form of violin plots of the residuals between spectroscopic abundances and model predictions. We find a significant trend in the residuals that implies an underproduction of the elements Nb, Mo, and Ru in the models relative to the observations. This may originate from a process (e.g. the intermediate neutron-capture process, i process) at the metallicity of the Ba stars not yet included in the AGB models. Correlations are found between the residuals of these elements, suggesting a common origin for the deviations. In addition, there is a weak metallicity dependence of their residuals. The s-process temperatures derived with the [Zr/Fe] - [Nb/Fe] thermometer have an unrealistic value for the majority of our stars. The most likely explanation is that at least a fraction of these elements are not produced in a steady-state s process, and instead may be due to processes not included in the AGB models. The mass distribution of the identified models confirms that our sample of Ba stars was polluted by low-mass AGB stars. Most of the matching AGB models require low accreted mass, but a few systems with high accreted mass are needed to explain the observations. (abridged)","sentences":["Barium (Ba) stars help to verify asymptotic giant branch (AGB) star nucleosynthesis models since they experienced pollution from an AGB binary companion and thus their spectra carry the signatures of the slow neutron capture process (s process).","For 180 Ba stars, we searched for AGB stellar models that match the observed abundance patterns.","We employed three machine learning algorithms as classifiers: a Random Forest method, developed for this work, and the two classifiers used in our previous study.","We studied the statistical behaviour of the s-process elements in the observational sample to investigate if the AGB models systematically under- or overpredict the abundances observed in the Ba stars and show the results in the form of violin plots of the residuals between spectroscopic abundances and model predictions.","We find a significant trend in the residuals that implies an underproduction of the elements Nb, Mo, and Ru in the models relative to the observations.","This may originate from a process (e.g. the intermediate neutron-capture process, i process) at the metallicity of the Ba stars not yet included in the AGB models.","Correlations are found between the residuals of these elements, suggesting a common origin for the deviations.","In addition, there is a weak metallicity dependence of their residuals.","The s-process temperatures derived with the [Zr/Fe] - [Nb/Fe] thermometer have an unrealistic value for the majority of our stars.","The most likely explanation is that at least a fraction of these elements are not produced in a steady-state s process, and instead may be due to processes not included in the AGB models.","The mass distribution of the identified models confirms that our sample of Ba stars was polluted by low-mass AGB stars.","Most of the matching AGB models require low accreted mass, but a few systems with high accreted mass are needed to explain the observations.","(abridged)"],"url":"http://arxiv.org/abs/2405.19330v1","category":"astro-ph.SR"}
{"created":"2024-05-29 17:30:06","title":"Uniform-in-time estimates on the size of chaos for interacting Brownian particles","abstract":"We consider a system of classical Brownian particles interacting via a smooth long-range potential in the mean-field regime, and we analyze the propagation of chaos in form of sharp, uniform-in-time estimates on many-particle correlation functions. Our results cover both the kinetic Langevin setting and the corresponding overdamped Brownian dynamics. The approach is mainly based on so-called Lions expansions, which we combine with new diagrammatic tools to capture many-particle cancellations, as well as with fine ergodic estimates on the linearized mean-field equation, and with discrete stochastic calculus with respect to initial data. In the process, we derive some new ergodic estimates for the linearized Vlasov-Fokker-Planck kinetic equation that are of independent interest. Our analysis also leads to uniform-in-time concentration estimates and to a uniform-in-time quantitative central limit theorem for the empirical measure associated with the particle dynamics.","sentences":["We consider a system of classical Brownian particles interacting via a smooth long-range potential in the mean-field regime, and we analyze the propagation of chaos in form of sharp, uniform-in-time estimates on many-particle correlation functions.","Our results cover both the kinetic Langevin setting and the corresponding overdamped Brownian dynamics.","The approach is mainly based on so-called Lions expansions, which we combine with new diagrammatic tools to capture many-particle cancellations, as well as with fine ergodic estimates on the linearized mean-field equation, and with discrete stochastic calculus with respect to initial data.","In the process, we derive some new ergodic estimates for the linearized Vlasov-Fokker-Planck kinetic equation that are of independent interest.","Our analysis also leads to uniform-in-time concentration estimates and to a uniform-in-time quantitative central limit theorem for the empirical measure associated with the particle dynamics."],"url":"http://arxiv.org/abs/2405.19306v1","category":"math.AP"}
{"created":"2024-05-29 17:29:55","title":"Real-Time Environment Condition Classification for Autonomous Vehicles","abstract":"Current autonomous driving technologies are being rolled out in geo-fenced areas with well-defined operation conditions such as time of operation, area, weather conditions and road conditions. In this way, challenging conditions as adverse weather, slippery road or densely-populated city centers can be excluded. In order to lift the geo-fenced restriction and allow a more dynamic availability of autonomous driving functions, it is necessary for the vehicle to autonomously perform an environment condition assessment in real time to identify when the system cannot operate safely and either stop operation or require the resting passenger to take control. In particular, adverse-weather challenges are a fundamental limitation as sensor performance degenerates quickly, prohibiting the use of sensors such as cameras to locate and monitor road signs, pedestrians or other vehicles. To address this issue, we train a deep learning model to identify outdoor weather and dangerous road conditions, enabling a quick reaction to new situations and environments. We achieve this by introducing an improved taxonomy and label hierarchy for a state-of-the-art adverse-weather dataset, relabelling it with a novel semi-automated labeling pipeline. Using the novel proposed dataset and hierarchy, we train RECNet, a deep learning model for the classification of environment conditions from a single RGB frame. We outperform baseline models by relative 16% in F1- Score, while maintaining a real-time capable performance of 20 Hz.","sentences":["Current autonomous driving technologies are being rolled out in geo-fenced areas with well-defined operation conditions such as time of operation, area, weather conditions and road conditions.","In this way, challenging conditions as adverse weather, slippery road or densely-populated city centers can be excluded.","In order to lift the geo-fenced restriction and allow a more dynamic availability of autonomous driving functions, it is necessary for the vehicle to autonomously perform an environment condition assessment in real time to identify when the system cannot operate safely and either stop operation or require the resting passenger to take control.","In particular, adverse-weather challenges are a fundamental limitation as sensor performance degenerates quickly, prohibiting the use of sensors such as cameras to locate and monitor road signs, pedestrians or other vehicles.","To address this issue, we train a deep learning model to identify outdoor weather and dangerous road conditions, enabling a quick reaction to new situations and environments.","We achieve this by introducing an improved taxonomy and label hierarchy for a state-of-the-art adverse-weather dataset, relabelling it with a novel semi-automated labeling pipeline.","Using the novel proposed dataset and hierarchy, we train RECNet, a deep learning model for the classification of environment conditions from a single RGB frame.","We outperform baseline models by relative 16% in F1- Score, while maintaining a real-time capable performance of 20 Hz."],"url":"http://arxiv.org/abs/2405.19305v1","category":"cs.CV"}
{"created":"2024-05-29 17:29:54","title":"Set Descriptive Complexity of Solvable Functions","abstract":"In a recent article, we introduced and studied a precise class of dynamical systems called solvable systems. These systems present a dynamic ruled by discontinuous ordinary differential equations with solvable right-hand terms and unique evolution. They correspond to a class of systems for which a transfinite method exist to compute the solution. We also presented several examples including a nontrivial one whose solution yields, at an integer time, a real encoding of the halting set for Turing machines; therefore showcasing that the behavior of solvable systems might describe ordinal Turing computations. In the current article, we study in more depth solvable systems, using tools from descriptive set theory. By establishing a correspondence with the class of well-founded trees, we construct a coanalytic ranking over the set of solvable functions and discuss its relation with other existing rankings for differentiable functions, in particular with the Kechris-Woodin, Denjoy and Zalcwasser ranking. We prove that our ranking is unbounded below the first uncountable ordinal.","sentences":["In a recent article, we introduced and studied a precise class of dynamical systems called solvable systems.","These systems present a dynamic ruled by discontinuous ordinary differential equations with solvable right-hand terms and unique evolution.","They correspond to a class of systems for which a transfinite method exist to compute the solution.","We also presented several examples including a nontrivial one whose solution yields, at an integer time, a real encoding of the halting set for Turing machines; therefore showcasing that the behavior of solvable systems might describe ordinal Turing computations.","In the current article, we study in more depth solvable systems, using tools from descriptive set theory.","By establishing a correspondence with the class of well-founded trees, we construct a coanalytic ranking over the set of solvable functions and discuss its relation with other existing rankings for differentiable functions, in particular with the Kechris-Woodin, Denjoy and Zalcwasser ranking.","We prove that our ranking is unbounded below the first uncountable ordinal."],"url":"http://arxiv.org/abs/2405.19304v1","category":"cs.CC"}
{"created":"2024-05-29 17:23:05","title":"GPU-accelerated Higher Representations of Wilson Fermions with HiRep","abstract":"We are improving one of the available lattice software packages HiRep by adding GPU acceleration supporting highly-optimized simulations on both NVIDIA and AMD GPUs. HiRep allows lattice simulations of theories with fermions in higher representations and a variable number of colors in the gauge group. The development is accompanied by an overall software quality improvement in the build system, testing, and documentation, adding features for both CPUs and GPUs. The software is available under https://github.com/claudiopica/HiRep","sentences":["We are improving one of the available lattice software packages HiRep by adding GPU acceleration supporting highly-optimized simulations on both NVIDIA and AMD GPUs.","HiRep allows lattice simulations of theories with fermions in higher representations and a variable number of colors in the gauge group.","The development is accompanied by an overall software quality improvement in the build system, testing, and documentation, adding features for both CPUs and GPUs.","The software is available under https://github.com/claudiopica/HiRep"],"url":"http://arxiv.org/abs/2405.19294v1","category":"hep-lat"}
{"created":"2024-05-29 17:21:25","title":"Act Natural! Projecting Autonomous System Trajectories Into Naturalistic Behavior Sets","abstract":"Autonomous agents operating around human actors must consider how their behaviors might affect those humans, even when not directly interacting with them. To this end, it is often beneficial to be predictable and appear naturalistic. Existing methods to address this problem use human actor intent modeling or imitation learning techniques, but these approaches rarely capture all possible motivations for human behavior or require significant amounts of data. In contrast, we propose a technique for modeling naturalistic behavior as a set of convex hulls computed over a relatively small dataset of human behavior. Given this set, we design an optimization-based filter which projects arbitrary trajectories into it to make them more naturalistic for autonomous agents to execute while also satisfying dynamics constraints. We demonstrate our methods on real-world human driving data from the inD intersection dataset (Bock et al., 2020).","sentences":["Autonomous agents operating around human actors must consider how their behaviors might affect those humans, even when not directly interacting with them.","To this end, it is often beneficial to be predictable and appear naturalistic.","Existing methods to address this problem use human actor intent modeling or imitation learning techniques, but these approaches rarely capture all possible motivations for human behavior or require significant amounts of data.","In contrast, we propose a technique for modeling naturalistic behavior as a set of convex hulls computed over a relatively small dataset of human behavior.","Given this set, we design an optimization-based filter which projects arbitrary trajectories into it to make them more naturalistic for autonomous agents to execute while also satisfying dynamics constraints.","We demonstrate our methods on real-world human driving data from the inD intersection dataset (Bock et al., 2020)."],"url":"http://arxiv.org/abs/2405.19292v1","category":"cs.MA"}
{"created":"2024-05-29 17:19:04","title":"Integrating Multi-scale Contextualized Information for Byte-based Neural Machine Translation","abstract":"Subword tokenization is a common method for vocabulary building in Neural Machine Translation (NMT) models. However, increasingly complex tasks have revealed its disadvantages. First, a vocabulary cannot be modified once it is learned, making it hard to adapt to new words. Second, in multilingual translation, the imbalance in data volumes across different languages spreads to the vocabulary, exacerbating translations involving low-resource languages. While byte-based tokenization addresses these issues, byte-based models struggle with the low information density inherent in UTF-8 byte sequences. Previous works enhance token semantics through local contextualization but fail to select an appropriate contextualizing scope based on the input. Consequently, we propose the Multi-Scale Contextualization (MSC) method, which learns contextualized information of varying scales across different hidden state dimensions. It then leverages the attention module to dynamically integrate the multi-scale contextualized information. Experiments show that MSC significantly outperforms subword-based and other byte-based methods in both multilingual and out-of-domain scenarios. Code can be found in https://github.com/ictnlp/Multiscale-Contextualization.","sentences":["Subword tokenization is a common method for vocabulary building in Neural Machine Translation (NMT) models.","However, increasingly complex tasks have revealed its disadvantages.","First, a vocabulary cannot be modified once it is learned, making it hard to adapt to new words.","Second, in multilingual translation, the imbalance in data volumes across different languages spreads to the vocabulary, exacerbating translations involving low-resource languages.","While byte-based tokenization addresses these issues, byte-based models struggle with the low information density inherent in UTF-8 byte sequences.","Previous works enhance token semantics through local contextualization but fail to select an appropriate contextualizing scope based on the input.","Consequently, we propose the Multi-Scale Contextualization (MSC) method, which learns contextualized information of varying scales across different hidden state dimensions.","It then leverages the attention module to dynamically integrate the multi-scale contextualized information.","Experiments show that MSC significantly outperforms subword-based and other byte-based methods in both multilingual and out-of-domain scenarios.","Code can be found in https://github.com/ictnlp/Multiscale-Contextualization."],"url":"http://arxiv.org/abs/2405.19290v1","category":"cs.CL"}
{"created":"2024-05-29 17:18:51","title":"Genuine topological Anderson insulator from impurity induced chirality reversal","abstract":"We investigate a model of Dirac fermions with Haldane type mass impurities which open a global topological gap even in the dilute limit. Surprisingly, we find that the chirality of this mass term, i.e., the sign of the Chern number, can be reversed by tuning the magnitude of the single-impurity scattering. Consequently, the disorder induces a phase disconnected from the clean topological phase, i.e., a genuine topological Anderson insulator. In seeming contradiction to the expectation that mass disorder is an irrelevant perturbation to the clean integer quantum Hall transition, the tri-critical point separating these two Chern insulating phases and a thermal metal phase is located at zero impurity density and connected to the appearance of a zero energy bound state in the continuum corresponding to a divergent Haldane mass impurity. Our conclusions based on the T-matrix expansion are substantiated by large scale Chebyshev-Polynomial-Green-Function numerics. We discuss possible experimental platforms.","sentences":["We investigate a model of Dirac fermions with Haldane type mass impurities which open a global topological gap even in the dilute limit.","Surprisingly, we find that the chirality of this mass term, i.e., the sign of the Chern number, can be reversed by tuning the magnitude of the single-impurity scattering.","Consequently, the disorder induces a phase disconnected from the clean topological phase, i.e., a genuine topological Anderson insulator.","In seeming contradiction to the expectation that mass disorder is an irrelevant perturbation to the clean integer quantum Hall transition, the tri-critical point separating these two Chern insulating phases and a thermal metal phase is located at zero impurity density and connected to the appearance of a zero energy bound state in the continuum corresponding to a divergent Haldane mass impurity.","Our conclusions based on the T-matrix expansion are substantiated by large scale Chebyshev-Polynomial-Green-Function numerics.","We discuss possible experimental platforms."],"url":"http://arxiv.org/abs/2405.19289v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-29 17:11:28","title":"Understanding and Minimising Outlier Features in Neural Network Training","abstract":"Outlier Features (OF) are neurons whose activation magnitudes significantly exceed the average over a neural network's (NN) width. They are well known to emerge during standard transformer training and have the undesirable effect of hindering quantisation in afflicted models. Despite their practical importance, little is known behind why OFs emerge during training, nor how one can minimise them.   Our work focuses on the above questions, first identifying several quantitative metrics, such as the kurtosis over neuron activation norms, to measure OFs. With these metrics, we study how architectural and optimisation choices influence OFs, and provide practical insights to minimise OFs during training. As highlights, we emphasise the importance of controlling signal propagation throughout training, and propose the Outlier Protected transformer block, which removes standard Pre-Norm layers to mitigate OFs, without loss of convergence speed or training stability. Overall, our findings shed new light on our understanding of, our ability to prevent, and the complexity of this important facet in NN training dynamics.","sentences":["Outlier Features (OF) are neurons whose activation magnitudes significantly exceed the average over a neural network's (NN) width.","They are well known to emerge during standard transformer training and have the undesirable effect of hindering quantisation in afflicted models.","Despite their practical importance, little is known behind why OFs emerge during training, nor how one can minimise them.   ","Our work focuses on the above questions, first identifying several quantitative metrics, such as the kurtosis over neuron activation norms, to measure OFs.","With these metrics, we study how architectural and optimisation choices influence OFs, and provide practical insights to minimise OFs during training.","As highlights, we emphasise the importance of controlling signal propagation throughout training, and propose the Outlier Protected transformer block, which removes standard Pre-Norm layers to mitigate OFs, without loss of convergence speed or training stability.","Overall, our findings shed new light on our understanding of, our ability to prevent, and the complexity of this important facet in NN training dynamics."],"url":"http://arxiv.org/abs/2405.19279v1","category":"cs.LG"}
{"created":"2024-05-29 17:07:24","title":"A Recipe for Charge Density Prediction","abstract":"In density functional theory, charge density is the core attribute of atomic systems from which all chemical properties can be derived. Machine learning methods are promising in significantly accelerating charge density prediction, yet existing approaches either lack accuracy or scalability. We propose a recipe that can achieve both. In particular, we identify three key ingredients: (1) representing the charge density with atomic and virtual orbitals (spherical fields centered at atom/virtual coordinates); (2) using expressive and learnable orbital basis sets (basis function for the spherical fields); and (3) using high-capacity equivariant neural network architecture. Our method achieves state-of-the-art accuracy while being more than an order of magnitude faster than existing methods. Furthermore, our method enables flexible efficiency-accuracy trade-offs by adjusting the model/basis sizes.","sentences":["In density functional theory, charge density is the core attribute of atomic systems from which all chemical properties can be derived.","Machine learning methods are promising in significantly accelerating charge density prediction, yet existing approaches either lack accuracy or scalability.","We propose a recipe that can achieve both.","In particular, we identify three key ingredients: (1) representing the charge density with atomic and virtual orbitals (spherical fields centered at atom/virtual coordinates); (2) using expressive and learnable orbital basis sets (basis function for the spherical fields); and (3) using high-capacity equivariant neural network architecture.","Our method achieves state-of-the-art accuracy while being more than an order of magnitude faster than existing methods.","Furthermore, our method enables flexible efficiency-accuracy trade-offs by adjusting the model/basis sizes."],"url":"http://arxiv.org/abs/2405.19276v1","category":"physics.comp-ph"}
{"created":"2024-05-29 17:02:49","title":"Rich-Observation Reinforcement Learning with Continuous Latent Dynamics","abstract":"Sample-efficiency and reliability remain major bottlenecks toward wide adoption of reinforcement learning algorithms in continuous settings with high-dimensional perceptual inputs. Toward addressing these challenges, we introduce a new theoretical framework, RichCLD (Rich-Observation RL with Continuous Latent Dynamics), in which the agent performs control based on high-dimensional observations, but the environment is governed by low-dimensional latent states and Lipschitz continuous dynamics. Our main contribution is a new algorithm for this setting that is provably statistically and computationally efficient. The core of our algorithm is a new representation learning objective; we show that prior representation learning schemes tailored to discrete dynamics do not naturally extend to the continuous setting. Our new objective is amenable to practical implementation, and empirically, we find that it compares favorably to prior schemes in a standard evaluation protocol. We further provide several insights into the statistical complexity of the RichCLD framework, in particular proving that certain notions of Lipschitzness that admit sample-efficient learning in the absence of rich observations are insufficient in the rich-observation setting.","sentences":["Sample-efficiency and reliability remain major bottlenecks toward wide adoption of reinforcement learning algorithms in continuous settings with high-dimensional perceptual inputs.","Toward addressing these challenges, we introduce a new theoretical framework, RichCLD (Rich-Observation RL with Continuous Latent Dynamics), in which the agent performs control based on high-dimensional observations, but the environment is governed by low-dimensional latent states and Lipschitz continuous dynamics.","Our main contribution is a new algorithm for this setting that is provably statistically and computationally efficient.","The core of our algorithm is a new representation learning objective; we show that prior representation learning schemes tailored to discrete dynamics do not naturally extend to the continuous setting.","Our new objective is amenable to practical implementation, and empirically, we find that it compares favorably to prior schemes in a standard evaluation protocol.","We further provide several insights into the statistical complexity of the RichCLD framework, in particular proving that certain notions of Lipschitzness that admit sample-efficient learning in the absence of rich observations are insufficient in the rich-observation setting."],"url":"http://arxiv.org/abs/2405.19269v1","category":"cs.LG"}
{"created":"2024-05-29 17:01:07","title":"Photonic bilayer Chern insulator with corner states","abstract":"Photonic Chern insulators can be implemented in gyromagnetic photonic crystals with broken time-reversal (TR) symmetry. They exhibit gapless chiral edge states (CESs), enabling unidirectional propagation and demonstrating exceptional resilience to localization even in the presence of defects or disorders. However, when two Chern insulators with opposite Chern numbers are stacked together, this one-way nature can be nullified, causing the originally gapless CESs to become gapped. Recent theoretical works have proposed achieving such a topological phase transition in condensed matter systems using antiferromagnetic thin films such as MnBi2Te4 or by coupling two quantum spin/anomalous Hall insulators, but these approaches have yet to be realized experimentally. In a bilayer gyromagnetic photonic crystal arranged in an antiferromagnetic layer configuration, our experimental observations reveal that interlayer coupling initiates a transition from a Chern insulating phase to a higher-order topological phase. This transition results in the gapping of CESs and triggers the emergence of corner states within the bandgap. The corner mode energy within the gap can be attributed to CESs interaction, forming a Jackiw-Rebbi topological domain wall mode at the corner. These states exhibit heightened resilience against defects, setting them apart from their time-reversal symmetric counterparts.","sentences":["Photonic Chern insulators can be implemented in gyromagnetic photonic crystals with broken time-reversal (TR) symmetry.","They exhibit gapless chiral edge states (CESs), enabling unidirectional propagation and demonstrating exceptional resilience to localization even in the presence of defects or disorders.","However, when two Chern insulators with opposite Chern numbers are stacked together, this one-way nature can be nullified, causing the originally gapless CESs to become gapped.","Recent theoretical works have proposed achieving such a topological phase transition in condensed matter systems using antiferromagnetic thin films such as MnBi2Te4 or by coupling two quantum spin/anomalous Hall insulators, but these approaches have yet to be realized experimentally.","In a bilayer gyromagnetic photonic crystal arranged in an antiferromagnetic layer configuration, our experimental observations reveal that interlayer coupling initiates a transition from a Chern insulating phase to a higher-order topological phase.","This transition results in the gapping of CESs and triggers the emergence of corner states within the bandgap.","The corner mode energy within the gap can be attributed to CESs interaction, forming a Jackiw-Rebbi topological domain wall mode at the corner.","These states exhibit heightened resilience against defects, setting them apart from their time-reversal symmetric counterparts."],"url":"http://arxiv.org/abs/2405.19267v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-29 16:55:32","title":"Stoichiometric reconstruction of the Al$_{2}$O$_{3}$(0001) surface","abstract":"Macroscopic properties of materials stem from fundamental atomic-scale details, yet for insulators, resolving surface structures remains a challenge. The basal (0001) plane of ${\\alpha}$-Al$_{2}$O$_{3}$ was imaged with noncontact atomic force microscopy with an atomically-defined tip apex. The surface forms a complex $({\\sqrt31} {\\times} {\\sqrt31})R{\\pm}9{\\deg}$ reconstruction. The lateral positions of the individual O and Al surface atoms come directly from experiment; how these connect to the underlying crystal bulk was determined based on computational modeling. Before the restructuring, the surface Al atoms assume an unfavorable, threefold planar coordination; the reconstruction allows a rehybridization with subsurface O that leads to a substantial energy gain. The reconstructed surface remains stoichiometric, Al$_{2}$O$_{3}$.","sentences":["Macroscopic properties of materials stem from fundamental atomic-scale details, yet for insulators, resolving surface structures remains a challenge.","The basal (0001) plane of ${\\alpha}$-Al$_{2}$O$_{3}$ was imaged with noncontact atomic force microscopy with an atomically-defined tip apex.","The surface forms a complex $({\\sqrt31} {\\times} {\\sqrt31})R{\\pm}9{\\deg}$ reconstruction.","The lateral positions of the individual O and Al surface atoms come directly from experiment; how these connect to the underlying crystal bulk was determined based on computational modeling.","Before the restructuring, the surface Al atoms assume an unfavorable, threefold planar coordination; the reconstruction allows a rehybridization with subsurface O that leads to a substantial energy gain.","The reconstructed surface remains stoichiometric, Al$_{2}$O$_{3}$."],"url":"http://arxiv.org/abs/2405.19263v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 16:53:50","title":"Hilbert Space Diffusion in Systems with Approximate Symmetries","abstract":"Random matrix theory (RMT) universality is the defining property of quantum mechanical chaotic systems, and can be probed by observables like the spectral form factor (SFF). In this paper, we describe systematic deviations from RMT behaviour at intermediate time scales in systems with approximate symmetries. At early times, the symmetries allow us to organize the Hilbert space into approximately decoupled sectors, each of which contributes independently to the SFF. At late times, the SFF transitions into the final ramp of the fully mixed chaotic Hamiltonian. For approximate continuous symmetries, the transitional behaviour is governed by a universal process that we call Hilbert space diffusion. The diffusion constant corresponding to this process is related to the relaxation rate of the associated nearly conserved charge. By implementing a chaotic sigma model for Hilbert-space diffusion, we formulate an analytic theory of this process which agrees quantitatively with our numerical results for different examples.","sentences":["Random matrix theory (RMT) universality is the defining property of quantum mechanical chaotic systems, and can be probed by observables like the spectral form factor (SFF).","In this paper, we describe systematic deviations from RMT behaviour at intermediate time scales in systems with approximate symmetries.","At early times, the symmetries allow us to organize the Hilbert space into approximately decoupled sectors, each of which contributes independently to the SFF.","At late times, the SFF transitions into the final ramp of the fully mixed chaotic Hamiltonian.","For approximate continuous symmetries, the transitional behaviour is governed by a universal process that we call Hilbert space diffusion.","The diffusion constant corresponding to this process is related to the relaxation rate of the associated nearly conserved charge.","By implementing a chaotic sigma model for Hilbert-space diffusion, we formulate an analytic theory of this process which agrees quantitatively with our numerical results for different examples."],"url":"http://arxiv.org/abs/2405.19260v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-29 16:47:38","title":"A Privacy-Preserving Graph Encryption Scheme Based on Oblivious RAM","abstract":"Graph encryption schemes play a crucial role in facilitating secure queries on encrypted graphs hosted on untrusted servers. With applications spanning navigation systems, network topology, and social networks, the need to safeguard sensitive data becomes paramount. Existing graph encryption methods, however, exhibit vulnerabilities by inadvertently revealing aspects of the graph structure and query patterns, posing threats to security and privacy. In response, we propose a novel graph encryption scheme designed to mitigate access pattern and query pattern leakage through the integration of oblivious RAM and trusted execution environment techniques, exemplified by a Trusted Execution Environment (TEE). Our solution establishes two key security objectives: (1) ensuring that adversaries, when presented with an encrypted graph, remain oblivious to any information regarding the underlying graph, and (2) achieving query indistinguishability by concealing access patterns. Additionally, we conducted experimentation to evaluate the efficiency of the proposed schemes when dealing with real-world location navigation services.","sentences":["Graph encryption schemes play a crucial role in facilitating secure queries on encrypted graphs hosted on untrusted servers.","With applications spanning navigation systems, network topology, and social networks, the need to safeguard sensitive data becomes paramount.","Existing graph encryption methods, however, exhibit vulnerabilities by inadvertently revealing aspects of the graph structure and query patterns, posing threats to security and privacy.","In response, we propose a novel graph encryption scheme designed to mitigate access pattern and query pattern leakage through the integration of oblivious RAM and trusted execution environment techniques, exemplified by a Trusted Execution Environment (TEE).","Our solution establishes two key security objectives: (1) ensuring that adversaries, when presented with an encrypted graph, remain oblivious to any information regarding the underlying graph, and (2) achieving query indistinguishability by concealing access patterns.","Additionally, we conducted experimentation to evaluate the efficiency of the proposed schemes when dealing with real-world location navigation services."],"url":"http://arxiv.org/abs/2405.19259v1","category":"cs.CR"}
{"created":"2024-05-29 16:44:09","title":"Hybrid-Parallel: Achieving High Performance and Energy Efficient Distributed Inference on Robots","abstract":"The rapid advancements in machine learning techniques have led to significant achievements in various real-world robotic tasks. These tasks heavily rely on fast and energy-efficient inference of deep neural network (DNN) models when deployed on robots. To enhance inference performance, distributed inference has emerged as a promising approach, parallelizing inference across multiple powerful GPU devices in modern data centers using techniques such as data parallelism, tensor parallelism, and pipeline parallelism. However, when deployed on real-world robots, existing parallel methods fail to provide low inference latency and meet the energy requirements due to the limited bandwidth of robotic IoT. We present Hybrid-Parallel, a high-performance distributed inference system optimized for robotic IoT. Hybrid-Parallel employs a fine-grained approach to parallelize inference at the granularity of local operators within DNN layers (i.e., operators that can be computed independently with the partial input, such as the convolution kernel in the convolution layer). By doing so, Hybrid-Parallel enables different operators of different layers to be computed and transmitted concurrently, and overlap the computation and transmission phases within the same inference task. The evaluation demonstrate that Hybrid-Parallel reduces inference time by 14.9% ~41.1% and energy consumption per inference by up to 35.3% compared to the state-of-the-art baselines.","sentences":["The rapid advancements in machine learning techniques have led to significant achievements in various real-world robotic tasks.","These tasks heavily rely on fast and energy-efficient inference of deep neural network (DNN) models when deployed on robots.","To enhance inference performance, distributed inference has emerged as a promising approach, parallelizing inference across multiple powerful GPU devices in modern data centers using techniques such as data parallelism, tensor parallelism, and pipeline parallelism.","However, when deployed on real-world robots, existing parallel methods fail to provide low inference latency and meet the energy requirements due to the limited bandwidth of robotic IoT. We present Hybrid-Parallel, a high-performance distributed inference system optimized for robotic IoT. Hybrid-Parallel employs a fine-grained approach to parallelize inference at the granularity of local operators within DNN layers (i.e., operators that can be computed independently with the partial input, such as the convolution kernel in the convolution layer).","By doing so, Hybrid-Parallel enables different operators of different layers to be computed and transmitted concurrently, and overlap the computation and transmission phases within the same inference task.","The evaluation demonstrate that Hybrid-Parallel reduces inference time by 14.9% ~41.1% and energy consumption per inference by up to 35.3% compared to the state-of-the-art baselines."],"url":"http://arxiv.org/abs/2405.19257v1","category":"cs.RO"}
{"created":"2024-05-29 16:39:01","title":"Causal Fermion Systems as an Effective Collapse Theory","abstract":"It is shown that, in the non-relativistic limit, causal fermion systems give rise to an effective collapse theory. The nonlinear and stochastic correction terms to the Schr\\\"odinger equation are derived from the causal action principle. The dynamics of the statistical operator is described by a deterministic equation of Kossakowski-Lindblad form. Moreover, the quantum state undergoes a dynamical collapse compatible with Born's rule. The effective model has similarities with the continuous spontaneous localization model, but differs from it by a conservation law for the probability integral as well as a non-locality in time on a microscopic length scale $\\ell_{\\min}$.","sentences":["It is shown that, in the non-relativistic limit, causal fermion systems give rise to an effective collapse theory.","The nonlinear and stochastic correction terms to the Schr\\\"odinger equation are derived from the causal action principle.","The dynamics of the statistical operator is described by a deterministic equation of Kossakowski-Lindblad form.","Moreover, the quantum state undergoes a dynamical collapse compatible with Born's rule.","The effective model has similarities with the continuous spontaneous localization model, but differs from it by a conservation law for the probability integral as well as a non-locality in time on a microscopic length scale $\\ell_{\\min}$."],"url":"http://arxiv.org/abs/2405.19254v1","category":"math-ph"}
{"created":"2024-05-29 16:27:50","title":"A numerical algorithm with linear complexity for Multi-marginal Optimal Transport with $L^1$ Cost","abstract":"Numerically solving multi-marginal optimal transport (MMOT) problems is computationally prohibitive, even for moderate-scale instances involving $l\\ge4$ marginals with support sizes of $N\\ge1000$. The cost in MMOT is represented as a tensor with $N^l$ elements. Even accessing each element once incurs a significant computational burden. In fact, many algorithms require direct computation of tensor-vector products, leading to a computational complexity of $O(N^l)$ or beyond. In this paper, inspired by our previous work [$Comm. \\ Math. \\ Sci.$, 20 (2022), pp. 2053 - 2057], we observe that the costly tensor-vector products in the Sinkhorn Algorithm can be computed with a recursive process by separating summations and dynamic programming. Based on this idea, we propose a fast tensor-vector product algorithm to solve the MMOT problem with $L^1$ cost, achieving a miraculous reduction in the computational cost of the entropy regularized solution to $O(N)$. Numerical experiment results confirm such high performance of this novel method which can be several orders of magnitude faster than the original Sinkhorn algorithm.","sentences":["Numerically solving multi-marginal optimal transport (MMOT) problems is computationally prohibitive, even for moderate-scale instances involving $l\\ge4$ marginals with support sizes of $N\\ge1000$.","The cost in MMOT is represented as a tensor with $N^l$ elements.","Even accessing each element once incurs a significant computational burden.","In fact, many algorithms require direct computation of tensor-vector products, leading to a computational complexity of $O(N^l)$ or beyond.","In this paper, inspired by our previous work [$Comm.","\\ Math.","\\ Sci.$, 20 (2022), pp. 2053 - 2057], we observe that the costly tensor-vector products in the Sinkhorn Algorithm can be computed with a recursive process by separating summations and dynamic programming.","Based on this idea, we propose a fast tensor-vector product algorithm to solve the MMOT problem with $L^1$ cost, achieving a miraculous reduction in the computational cost of the entropy regularized solution to $O(N)$. Numerical experiment results confirm such high performance of this novel method which can be several orders of magnitude faster than the original Sinkhorn algorithm."],"url":"http://arxiv.org/abs/2405.19246v1","category":"math.NA"}
{"created":"2024-05-29 16:26:57","title":"Efficient Optimal Control of Open Quantum Systems","abstract":"The optimal control problem for open quantum systems can be formulated as a time-dependent Lindbladian that is parameterized by a number of time-dependent control variables. Given an observable and an initial state, the goal is to tune the control variables so that the expected value of some observable with respect to the final state is maximized. In this paper, we present algorithms for solving this optimal control problem efficiently, i.e., having a poly-logarithmic dependency on the system dimension, which is exponentially faster than best-known classical algorithms. Our algorithms are hybrid, consisting of both quantum and classical components. The quantum procedure simulates time-dependent Lindblad evolution that drives the initial state to the final state, and it also provides access to the gradients of the objective function via quantum gradient estimation. The classical procedure uses the gradient information to update the control variables.   At the technical level, we provide the first (to the best of our knowledge) simulation algorithm for time-dependent Lindbladians with an $\\ell_1$-norm dependence. As an alternative, we also present a simulation algorithm in the interaction picture to improve the algorithm for the cases where the time-independent component of a Lindbladian dominates the time-dependent part. On the classical side, we heavily adapt the state-of-the-art classical optimization analysis to interface with the quantum part of our algorithms. Both the quantum simulation techniques and the classical optimization analyses might be of independent interest.","sentences":["The optimal control problem for open quantum systems can be formulated as a time-dependent Lindbladian that is parameterized by a number of time-dependent control variables.","Given an observable and an initial state, the goal is to tune the control variables so that the expected value of some observable with respect to the final state is maximized.","In this paper, we present algorithms for solving this optimal control problem efficiently, i.e., having a poly-logarithmic dependency on the system dimension, which is exponentially faster than best-known classical algorithms.","Our algorithms are hybrid, consisting of both quantum and classical components.","The quantum procedure simulates time-dependent Lindblad evolution that drives the initial state to the final state, and it also provides access to the gradients of the objective function via quantum gradient estimation.","The classical procedure uses the gradient information to update the control variables.   ","At the technical level, we provide the first (to the best of our knowledge) simulation algorithm for time-dependent Lindbladians with an $\\ell_1$-norm dependence.","As an alternative, we also present a simulation algorithm in the interaction picture to improve the algorithm for the cases where the time-independent component of a Lindbladian dominates the time-dependent part.","On the classical side, we heavily adapt the state-of-the-art classical optimization analysis to interface with the quantum part of our algorithms.","Both the quantum simulation techniques and the classical optimization analyses might be of independent interest."],"url":"http://arxiv.org/abs/2405.19245v1","category":"quant-ph"}
{"created":"2024-05-29 16:25:54","title":"Bottomed mesons and baryons in pp collisions at $\\sqrt{s}=5 \\, TeV$ LHC energy within a Coalescence plus Fragmentation approach","abstract":"Recent experimental data from $pp$ collisions have shown a significant increase in heavy baryon production leading to a baryon over meson ratio which is one order of magnitude higher than elementary collisions ($e^+e^-$, $ep$). From a theoretical point of view this large production of baryon can be explained with hadronization via quark coalescence assuming a QGP medium in $pp$ collisions. In this study, we extend this analysis to include hadrons containing bottom quarks. Employing a coalescence plus fragmentation approach, we present predictions for $p_T$ spectra and the heavy baryon/meson ratio of charmed hadrons with and without strangeness content, specifically: $\\bar{B^0}$, $B_s$, $\\Lambda_b$, $\\Xi_b^{0,-}$, $\\Omega_b$, and the $B_c$ meson. We have found that coalescence is the dominant mechanism in the B meson production, especially at low momenta, at variance with what found in the charm sector where the D meson were mainly produced via fragmentation. Our model predicts a $\\Lambda_b/\\bar{B^0}\\approx0.5\\!-\\!1$ and $\\Xi_b^0/\\bar{B^0}$ ratio around 0.3 at very low transverse momentum, which are about $1.5$ larger then those of the corresponding charmed hadron ratios at the same collision energy. Furthermore, we discuss the relative ratios between charmed and bottomed hadrons, emphasizing how these observables can provide information about the distribution of charm and bottom quarks and, if experimentally observed, would further support the idea of quark-gluon plasma formation even in small collision systems.","sentences":["Recent experimental data from $pp$ collisions have shown a significant increase in heavy baryon production leading to a baryon over meson ratio which is one order of magnitude higher than elementary collisions ($e^+e^-$, $ep$).","From a theoretical point of view this large production of baryon can be explained with hadronization via quark coalescence assuming a QGP medium in $pp$ collisions.","In this study, we extend this analysis to include hadrons containing bottom quarks.","Employing a coalescence plus fragmentation approach, we present predictions for $p_T$ spectra and the heavy baryon/meson ratio of charmed hadrons with and without strangeness content, specifically: $\\bar{B^0}$, $B_s$, $\\Lambda_b$, $\\Xi_b^{0,-}$, $\\Omega_b$, and the $B_c$ meson.","We have found that coalescence is the dominant mechanism in the B meson production, especially at low momenta, at variance with what found in the charm sector where the D meson were mainly produced via fragmentation.","Our model predicts a $\\Lambda_b/\\bar{B^0}\\approx0.5\\!-\\!1$ and $\\Xi_b^0/\\bar{B^0}$ ratio around 0.3 at very low transverse momentum, which are about $1.5$ larger then those of the corresponding charmed hadron ratios at the same collision energy.","Furthermore, we discuss the relative ratios between charmed and bottomed hadrons, emphasizing how these observables can provide information about the distribution of charm and bottom quarks and, if experimentally observed, would further support the idea of quark-gluon plasma formation even in small collision systems."],"url":"http://arxiv.org/abs/2405.19244v1","category":"hep-ph"}
{"created":"2024-05-29 16:23:32","title":"A remark on rapid mixing for hyperbolic flows","abstract":"In this short paper, we introduce a new criterion based on the temporal distance function that guarantees rapid mixing for a hyperbolic flow with respect to Gibbs measures. This criterion is probabilistically satisfied almost surely but lacks robustness. Moreover, it enables us to establish rapid mixing for a class of hyperbolic flows that do not meet the existing rapid mixing criteria. Furthermore, beyond uniform hyperbolicity, our proof also works for a significant category of non-uniformly hyperbolic flows that can be modeled by Young towers.","sentences":["In this short paper, we introduce a new criterion based on the temporal distance function that guarantees rapid mixing for a hyperbolic flow with respect to Gibbs measures.","This criterion is probabilistically satisfied almost surely but lacks robustness.","Moreover, it enables us to establish rapid mixing for a class of hyperbolic flows that do not meet the existing rapid mixing criteria.","Furthermore, beyond uniform hyperbolicity, our proof also works for a significant category of non-uniformly hyperbolic flows that can be modeled by Young towers."],"url":"http://arxiv.org/abs/2405.19241v1","category":"math.DS"}
{"created":"2024-05-29 16:21:43","title":"On geometric invariants of singular plane curves","abstract":"Given a germ of a smooth plane curve $(\\{f(x,y)=0\\},0)\\subset (\\mathbb K^2,0), \\mathbb K=\\mathbb R, \\mathbb C$, with an isolated singularity, we define two invariants $I_f$ and $V_f\\in \\mathbb N\\cup\\{\\infty\\}$ which count the number of inflections and vertices (suitably interpreted in the complex case) concentrated at the singular point; the first is an affine invariant and the second is invariant under similarities of $\\mathbb R^2$, and their analogue for $\\mathbb C^2$. We show that for almost all representations of $f$, in the sense that their complement is of infinite codimension, these invariants are finite. Indeed when the curve has no smooth components they are always finite and bounded and we can be much more explicit about the values they can attain; the set of possible values is of course an analytic invariant of $f$. We illustrate our results by computing these invariants for Arnold's $\\mathcal K$-simple singularities as well as singularities that have ${\\mathcal A}$-simple parametrisations. We also obtain a relationship between these invariants, the Milnor number of $f$ and the contact of the curve germ with its osculating circle.","sentences":["Given a germ of a smooth plane curve $(\\{f(x,y)=0\\},0)\\subset (\\mathbb K^2,0), \\mathbb K=\\mathbb R, \\mathbb C$, with an isolated singularity, we define two invariants $I_f$ and $V_f\\in \\mathbb N\\cup\\{\\infty\\}$ which count the number of inflections and vertices (suitably interpreted in the complex case) concentrated at the singular point; the first is an affine invariant and the second is invariant under similarities of $\\mathbb R^2$, and their analogue for $\\mathbb C^2$. We show that for almost all representations of $f$, in the sense that their complement is of infinite codimension, these invariants are finite.","Indeed when the curve has no smooth components they are always finite and bounded and we can be much more explicit about the values they can attain; the set of possible values is of course an analytic invariant of $f$. We illustrate our results by computing these invariants for Arnold's $\\mathcal K$-simple singularities as well as singularities that have ${\\mathcal A}$-simple parametrisations.","We also obtain a relationship between these invariants, the Milnor number of $f$ and the contact of the curve germ with its osculating circle."],"url":"http://arxiv.org/abs/2405.19239v1","category":"math.DG"}
{"created":"2024-05-29 16:07:00","title":"Motor Imagery Task Alters Dynamics of Human Body Posture","abstract":"Motor Imagery (MI) is gaining traction in both rehabilitation and sports settings, but its immediate influence on human postural control is not yet clearly understood. The focus of this study is to examine the effects of MI on the dynamics of the Center of Pressure (COP), a crucial metric for evaluating postural stability. In the experiment, thirty healthy young adults participated in four different scenarios: normal standing with both open and closed eyes, and kinesthetic motor imagery focused on mediolateral (ML) and anteroposterior (AP) sway movements. A mathematical model was developed to characterize the nonlinear dynamics of the COP and to assess the impact of MI on these dynamics. Our results show a statistically significant increase (p-value<0.05) in variables such as COP path length and Long-Range Correlation (LRC) during MI compared to the closed-eye and normal standing conditions. These observations align well with psycho-neuromuscular theory, which suggests that imagining a specific movement activates neural pathways, consequently affecting postural control. This study presents compelling evidence that motor imagery not only has a quantifiable impact on COP dynamics but also that changes in the Center of Pressure (COP) are directionally consistent with the imagined movements. This finding holds significant implications for the field of rehabilitation science, suggesting that motor imagery could be strategically utilized to induce targeted postural adjustments. Nonetheless, additional research is required to fully understand the complex mechanisms that underlie this relationship and to corroborate these results across a more diverse set of populations.","sentences":["Motor Imagery (MI) is gaining traction in both rehabilitation and sports settings, but its immediate influence on human postural control is not yet clearly understood.","The focus of this study is to examine the effects of MI on the dynamics of the Center of Pressure (COP), a crucial metric for evaluating postural stability.","In the experiment, thirty healthy young adults participated in four different scenarios: normal standing with both open and closed eyes, and kinesthetic motor imagery focused on mediolateral (ML) and anteroposterior (AP) sway movements.","A mathematical model was developed to characterize the nonlinear dynamics of the COP and to assess the impact of MI on these dynamics.","Our results show a statistically significant increase (p-value<0.05) in variables such as COP path length and Long-Range Correlation (LRC) during MI compared to the closed-eye and normal standing conditions.","These observations align well with psycho-neuromuscular theory, which suggests that imagining a specific movement activates neural pathways, consequently affecting postural control.","This study presents compelling evidence that motor imagery not only has a quantifiable impact on COP dynamics but also that changes in the Center of Pressure (COP) are directionally consistent with the imagined movements.","This finding holds significant implications for the field of rehabilitation science, suggesting that motor imagery could be strategically utilized to induce targeted postural adjustments.","Nonetheless, additional research is required to fully understand the complex mechanisms that underlie this relationship and to corroborate these results across a more diverse set of populations."],"url":"http://arxiv.org/abs/2405.19228v1","category":"q-bio.NC"}
{"created":"2024-05-29 16:06:21","title":"ContextBLIP: Doubly Contextual Alignment for Contrastive Image Retrieval from Linguistically Complex Descriptions","abstract":"Image retrieval from contextual descriptions (IRCD) aims to identify an image within a set of minimally contrastive candidates based on linguistically complex text. Despite the success of VLMs, they still significantly lag behind human performance in IRCD. The main challenges lie in aligning key contextual cues in two modalities, where these subtle cues are concealed in tiny areas of multiple contrastive images and within the complex linguistics of textual descriptions. This motivates us to propose ContextBLIP, a simple yet effective method that relies on a doubly contextual alignment scheme for challenging IRCD. Specifically, 1) our model comprises a multi-scale adapter, a matching loss, and a text-guided masking loss. The adapter learns to capture fine-grained visual cues. The two losses enable iterative supervision for the adapter, gradually highlighting the focal patches of a single image to the key textual cues. We term such a way as intra-contextual alignment. 2) Then, ContextBLIP further employs an inter-context encoder to learn dependencies among candidates, facilitating alignment between the text to multiple images. We term this step as inter-contextual alignment. Consequently, the nuanced cues concealed in each modality can be effectively aligned. Experiments on two benchmarks show the superiority of our method. We observe that ContextBLIP can yield comparable results with GPT-4V, despite involving about 7,500 times fewer parameters.","sentences":["Image retrieval from contextual descriptions (IRCD) aims to identify an image within a set of minimally contrastive candidates based on linguistically complex text.","Despite the success of VLMs, they still significantly lag behind human performance in IRCD.","The main challenges lie in aligning key contextual cues in two modalities, where these subtle cues are concealed in tiny areas of multiple contrastive images and within the complex linguistics of textual descriptions.","This motivates us to propose ContextBLIP, a simple yet effective method that relies on a doubly contextual alignment scheme for challenging IRCD.","Specifically, 1) our model comprises a multi-scale adapter, a matching loss, and a text-guided masking loss.","The adapter learns to capture fine-grained visual cues.","The two losses enable iterative supervision for the adapter, gradually highlighting the focal patches of a single image to the key textual cues.","We term such a way as intra-contextual alignment.","2) Then, ContextBLIP further employs an inter-context encoder to learn dependencies among candidates, facilitating alignment between the text to multiple images.","We term this step as inter-contextual alignment.","Consequently, the nuanced cues concealed in each modality can be effectively aligned.","Experiments on two benchmarks show the superiority of our method.","We observe that ContextBLIP can yield comparable results with GPT-4V, despite involving about 7,500 times fewer parameters."],"url":"http://arxiv.org/abs/2405.19226v1","category":"cs.CV"}
{"created":"2024-05-29 16:01:15","title":"Domain adaptation in small-scale and heterogeneous biological datasets","abstract":"Machine learning techniques are steadily becoming more important in modern biology, and are used to build predictive models, discover patterns, and investigate biological problems. However, models trained on one dataset are often not generalizable to other datasets from different cohorts or laboratories, due to differences in the statistical properties of these datasets. These could stem from technical differences, such as the measurement technique used, or from relevant biological differences between the populations studied. Domain adaptation, a type of transfer learning, can alleviate this problem by aligning the statistical distributions of features and samples among different datasets so that similar models can be applied across them. However, a majority of state-of-the-art domain adaptation methods are designed to work with large-scale data, mostly text and images, while biological datasets often suffer from small sample sizes, and possess complexities such as heterogeneity of the feature space. This Review aims to synthetically discuss domain adaptation methods in the context of small-scale and highly heterogeneous biological data. We describe the benefits and challenges of domain adaptation in biological research and critically discuss some of its objectives, strengths, and weaknesses through key representative methodologies. We argue for the incorporation of domain adaptation techniques to the computational biologist's toolkit, with further development of customized approaches.","sentences":["Machine learning techniques are steadily becoming more important in modern biology, and are used to build predictive models, discover patterns, and investigate biological problems.","However, models trained on one dataset are often not generalizable to other datasets from different cohorts or laboratories, due to differences in the statistical properties of these datasets.","These could stem from technical differences, such as the measurement technique used, or from relevant biological differences between the populations studied.","Domain adaptation, a type of transfer learning, can alleviate this problem by aligning the statistical distributions of features and samples among different datasets so that similar models can be applied across them.","However, a majority of state-of-the-art domain adaptation methods are designed to work with large-scale data, mostly text and images, while biological datasets often suffer from small sample sizes, and possess complexities such as heterogeneity of the feature space.","This Review aims to synthetically discuss domain adaptation methods in the context of small-scale and highly heterogeneous biological data.","We describe the benefits and challenges of domain adaptation in biological research and critically discuss some of its objectives, strengths, and weaknesses through key representative methodologies.","We argue for the incorporation of domain adaptation techniques to the computational biologist's toolkit, with further development of customized approaches."],"url":"http://arxiv.org/abs/2405.19221v1","category":"q-bio.QM"}
{"created":"2024-05-29 15:56:45","title":"Two dimensional potential theory with a view towards vortex motion: Energy, capacity and Green functions","abstract":"The paper reviews some parts of classical potential theory with applications to two dimensional fluid dynamics, in particular vortex motion. Energy and forces within a system of point vortices are similar to those for point charges when the vortices are kept fixed, but the dynamics is different in the case of free vortices. Starting from Bernoulli's equation we derive these laws. Letting the number of vortices tend to infinity leads in the limit to considerations of capacity, harmonic measure and many other notions in potential theory. In particular various kinds of Green functions have a central role in the paper, where we make a difference between electrostatic and hydrodynamic Green function.   We also consider the corresponding concepts in the case of closed Riemann surfaces provided with a metric. From a canonically defined monopole Green function we rederive much of the classical theory of harmonic and analytic forms. In the final section of the paper we return to the planar case, then reappearing in form of a symmetric Riemann surface, the Schottky double. Bergman kernels, electrostatic and hydrodynamic, come up naturally, and associated to the Green function the is a certain Robin function which is important for vortex motion and which also relates to capacity functions in classical potential theory.","sentences":["The paper reviews some parts of classical potential theory with applications to two dimensional fluid dynamics, in particular vortex motion.","Energy and forces within a system of point vortices are similar to those for point charges when the vortices are kept fixed, but the dynamics is different in the case of free vortices.","Starting from Bernoulli's equation we derive these laws.","Letting the number of vortices tend to infinity leads in the limit to considerations of capacity, harmonic measure and many other notions in potential theory.","In particular various kinds of Green functions have a central role in the paper, where we make a difference between electrostatic and hydrodynamic Green function.   ","We also consider the corresponding concepts in the case of closed Riemann surfaces provided with a metric.","From a canonically defined monopole Green function we rederive much of the classical theory of harmonic and analytic forms.","In the final section of the paper we return to the planar case, then reappearing in form of a symmetric Riemann surface, the Schottky double.","Bergman kernels, electrostatic and hydrodynamic, come up naturally, and associated to the Green function the is a certain Robin function which is important for vortex motion and which also relates to capacity functions in classical potential theory."],"url":"http://arxiv.org/abs/2405.19215v1","category":"math.CV"}
{"created":"2024-05-29 15:46:51","title":"Detection of entanglement by harnessing extracted work in an opto-magno-mechanics","abstract":"The connections between thermodynamics and quantum information processing are of paramount importance. Here, we address a bipartite entanglement via extracted work in a cavity magnomechanical system contained inside an yttrium iron garnet (YIG) sphere. The photons and magnons interact through an interaction between magnetic dipoles. A magnetostrictive interaction, analogous to radiation pressure, couple's phonons and magnons. The extracted work was obtained through a device similar to the Szil\\'ard engine. This engine operates by manipulating the photon-magnon as a bipartite quantum state. We employ logarithmic negativity to measure the amount of entanglement between photon and magnon modes in steady and dynamical states. We explore the extracted work, separable work, and maximum work for squeezed thermal states. We investigate the amount of work extracted from a bipartite quantum state, which can potentially determine the degree of entanglement present in that state. Numerical studies show that entanglement, as detected by the extracted work and quantified by logarithmic negativity, is in good agreement. We show the reduction of extracted work by a second measurement compared to a single measurement. Also, the efficiency of the Szilard engine in steady and dynamical states is investigated. We hope this work is of paramount importance in quantum information processing.","sentences":["The connections between thermodynamics and quantum information processing are of paramount importance.","Here, we address a bipartite entanglement via extracted work in a cavity magnomechanical system contained inside an yttrium iron garnet (YIG) sphere.","The photons and magnons interact through an interaction between magnetic dipoles.","A magnetostrictive interaction, analogous to radiation pressure, couple's phonons and magnons.","The extracted work was obtained through a device similar to the Szil\\'ard engine.","This engine operates by manipulating the photon-magnon as a bipartite quantum state.","We employ logarithmic negativity to measure the amount of entanglement between photon and magnon modes in steady and dynamical states.","We explore the extracted work, separable work, and maximum work for squeezed thermal states.","We investigate the amount of work extracted from a bipartite quantum state, which can potentially determine the degree of entanglement present in that state.","Numerical studies show that entanglement, as detected by the extracted work and quantified by logarithmic negativity, is in good agreement.","We show the reduction of extracted work by a second measurement compared to a single measurement.","Also, the efficiency of the Szilard engine in steady and dynamical states is investigated.","We hope this work is of paramount importance in quantum information processing."],"url":"http://arxiv.org/abs/2405.19205v1","category":"quant-ph"}
{"created":"2024-05-29 15:44:51","title":"Contrastive-Adversarial and Diffusion: Exploring pre-training and fine-tuning strategies for sulcal identification","abstract":"In the last decade, computer vision has witnessed the establishment of various training and learning approaches. Techniques like adversarial learning, contrastive learning, diffusion denoising learning, and ordinary reconstruction learning have become standard, representing state-of-the-art methods extensively employed for fully training or pre-training networks across various vision tasks. The exploration of fine-tuning approaches has emerged as a current focal point, addressing the need for efficient model tuning with reduced GPU memory usage and time costs while enhancing overall performance, as exemplified by methodologies like low-rank adaptation (LoRA). Key questions arise: which pre-training technique yields optimal results - adversarial, contrastive, reconstruction, or diffusion denoising? How does the performance of these approaches vary as the complexity of fine-tuning is adjusted? This study aims to elucidate the advantages of pre-training techniques and fine-tuning strategies to enhance the learning process of neural networks in independent identical distribution (IID) cohorts. We underscore the significance of fine-tuning by examining various cases, including full tuning, decoder tuning, top-level tuning, and fine-tuning of linear parameters using LoRA. Systematic summaries of model performance and efficiency are presented, leveraging metrics such as accuracy, time cost, and memory efficiency. To empirically demonstrate our findings, we focus on a multi-task segmentation-classification challenge involving the paracingulate sulcus (PCS) using different 3D Convolutional Neural Network (CNN) architectures by using the TOP-OSLO cohort comprising 596 subjects.","sentences":["In the last decade, computer vision has witnessed the establishment of various training and learning approaches.","Techniques like adversarial learning, contrastive learning, diffusion denoising learning, and ordinary reconstruction learning have become standard, representing state-of-the-art methods extensively employed for fully training or pre-training networks across various vision tasks.","The exploration of fine-tuning approaches has emerged as a current focal point, addressing the need for efficient model tuning with reduced GPU memory usage and time costs while enhancing overall performance, as exemplified by methodologies like low-rank adaptation (LoRA).","Key questions arise: which pre-training technique yields optimal results - adversarial, contrastive, reconstruction, or diffusion denoising?","How does the performance of these approaches vary as the complexity of fine-tuning is adjusted?","This study aims to elucidate the advantages of pre-training techniques and fine-tuning strategies to enhance the learning process of neural networks in independent identical distribution (IID) cohorts.","We underscore the significance of fine-tuning by examining various cases, including full tuning, decoder tuning, top-level tuning, and fine-tuning of linear parameters using LoRA.","Systematic summaries of model performance and efficiency are presented, leveraging metrics such as accuracy, time cost, and memory efficiency.","To empirically demonstrate our findings, we focus on a multi-task segmentation-classification challenge involving the paracingulate sulcus (PCS) using different 3D Convolutional Neural Network (CNN) architectures by using the TOP-OSLO cohort comprising 596 subjects."],"url":"http://arxiv.org/abs/2405.19204v1","category":"eess.IV"}
{"created":"2024-05-29 15:29:16","title":"Algorithmic Transparency and Participation through the Handoff Lens: Lessons Learned from the U.S. Census Bureau's Adoption of Differential Privacy","abstract":"Emerging discussions on the responsible government use of algorithmic technologies propose transparency and public participation as key mechanisms for preserving accountability and trust. But in practice, the adoption and use of any technology shifts the social, organizational, and political context in which it is embedded. Therefore translating transparency and participation efforts into meaningful, effective accountability must take into account these shifts. We adopt two theoretical frames, Mulligan and Nissenbaum's handoff model and Star and Griesemer's boundary objects, to reveal such shifts during the U.S. Census Bureau's adoption of differential privacy (DP) in its updated disclosure avoidance system (DAS) for the 2020 census. This update preserved (and arguably strengthened) the confidentiality protections that the Bureau is mandated to uphold, and the Bureau engaged in a range of activities to facilitate public understanding of and participation in the system design process. Using publicly available documents concerning the Census' implementation of DP, this case study seeks to expand our understanding of how technical shifts implicate values, how such shifts can afford (or fail to afford) greater transparency and participation in system design, and the importance of localized expertise throughout. We present three lessons from this case study toward grounding understandings of algorithmic transparency and participation: (1) efforts towards transparency and participation in algorithmic governance must center values and policy decisions, not just technical design decisions; (2) the handoff model is a useful tool for revealing how such values may be cloaked beneath technical decisions; and (3) boundary objects alone cannot bridge distant communities without trusted experts traveling alongside to broker their adoption.","sentences":["Emerging discussions on the responsible government use of algorithmic technologies propose transparency and public participation as key mechanisms for preserving accountability and trust.","But in practice, the adoption and use of any technology shifts the social, organizational, and political context in which it is embedded.","Therefore translating transparency and participation efforts into meaningful, effective accountability must take into account these shifts.","We adopt two theoretical frames, Mulligan and Nissenbaum's handoff model and Star and Griesemer's boundary objects, to reveal such shifts during the U.S. Census Bureau's adoption of differential privacy (DP) in its updated disclosure avoidance system (DAS) for the 2020 census.","This update preserved (and arguably strengthened) the confidentiality protections that the Bureau is mandated to uphold, and the Bureau engaged in a range of activities to facilitate public understanding of and participation in the system design process.","Using publicly available documents concerning the Census' implementation of DP, this case study seeks to expand our understanding of how technical shifts implicate values, how such shifts can afford (or fail to afford) greater transparency and participation in system design, and the importance of localized expertise throughout.","We present three lessons from this case study toward grounding understandings of algorithmic transparency and participation: (1) efforts towards transparency and participation in algorithmic governance must center values and policy decisions, not just technical design decisions; (2) the handoff model is a useful tool for revealing how such values may be cloaked beneath technical decisions; and (3) boundary objects alone cannot bridge distant communities without trusted experts traveling alongside to broker their adoption."],"url":"http://arxiv.org/abs/2405.19187v1","category":"cs.CY"}
{"created":"2024-05-29 15:23:42","title":"Delay-Doppler Domain Pulse Design for OTFS-NOMA","abstract":"We address the challenge of developing an orthogonal time-frequency space (OTFS)-based non-orthogonal multiple access (NOMA) system where each user is modulated using orthogonal pulses in the delay Doppler domain. Building upon the concept of the sufficient (bi)orthogonality train-pulse [1], we extend this idea by introducing Hermite functions, known for their orthogonality properties. Simulation results demonstrate that our proposed Hermite functions outperform the traditional OTFS-NOMA schemes, including power-domain (PDM) NOMA and code-domain (CDM) NOMA, in terms of bit error rate (BER) over a high-mobility channel. The algorithm's complexity is minimal, primarily involving the demodulation of OTFS. The spectrum efficiency of Hermite-based OTFS-NOMA is K times that of OTFS-CDM-NOMA scheme, where K is the spreading length of the NOMA waveform.","sentences":["We address the challenge of developing an orthogonal time-frequency space (OTFS)-based non-orthogonal multiple access (NOMA) system where each user is modulated using orthogonal pulses in the delay Doppler domain.","Building upon the concept of the sufficient (bi)orthogonality train-pulse [1], we extend this idea by introducing Hermite functions, known for their orthogonality properties.","Simulation results demonstrate that our proposed Hermite functions outperform the traditional OTFS-NOMA schemes, including power-domain (PDM) NOMA and code-domain (CDM) NOMA, in terms of bit error rate (BER) over a high-mobility channel.","The algorithm's complexity is minimal, primarily involving the demodulation of OTFS.","The spectrum efficiency of Hermite-based OTFS-NOMA is K times that of OTFS-CDM-NOMA scheme, where K is the spreading length of the NOMA waveform."],"url":"http://arxiv.org/abs/2405.19182v1","category":"eess.SP"}
{"created":"2024-05-29 15:14:28","title":"Greedy Kernel Methods for Approximating Breakthrough Curves for Reactive Flow from 3D Porous Geometry Data","abstract":"We address the challenging application of 3D pore scale reactive flow under varying geometry parameters. The task is to predict time-dependent integral quantities, i.e., breakthrough curves, from the given geometries. As the 3D reactive flow simulation is highly complex and computationally expensive, we are interested in data-based surrogates that can give a rapid prediction of the target quantities of interest. This setting is an example of an application with scarce data, i.e., only having available few data samples, while the input and output dimensions are high. In this scarce data setting, standard machine learning methods are likely to ail. Therefore, we resort to greedy kernel approximation schemes that have shown to be efficient meshless approximation techniques for multivariate functions. We demonstrate that such methods can efficiently be used in the high-dimensional input/output case under scarce data. Especially, we show that the vectorial kernel orthogonal greedy approximation (VKOGA) procedure with a data-adapted two-layer kernel yields excellent predictors for learning from 3D geometry voxel data via both morphological descriptors or principal component analysis.","sentences":["We address the challenging application of 3D pore scale reactive flow under varying geometry parameters.","The task is to predict time-dependent integral quantities, i.e., breakthrough curves, from the given geometries.","As the 3D reactive flow simulation is highly complex and computationally expensive, we are interested in data-based surrogates that can give a rapid prediction of the target quantities of interest.","This setting is an example of an application with scarce data, i.e., only having available few data samples, while the input and output dimensions are high.","In this scarce data setting, standard machine learning methods are likely to ail.","Therefore, we resort to greedy kernel approximation schemes that have shown to be efficient meshless approximation techniques for multivariate functions.","We demonstrate that such methods can efficiently be used in the high-dimensional input/output case under scarce data.","Especially, we show that the vectorial kernel orthogonal greedy approximation (VKOGA) procedure with a data-adapted two-layer kernel yields excellent predictors for learning from 3D geometry voxel data via both morphological descriptors or principal component analysis."],"url":"http://arxiv.org/abs/2405.19170v1","category":"math.NA"}
{"created":"2024-05-29 15:14:10","title":"Measuring differential particle correlations in relativistic nuclear collisions","abstract":"This study explores the transverse momentum ($p_T$) dependencies of Symmetric and Asymmetric Correlations (SC and ASC) with one and two particles of interest in Au+Au collisions at 200 GeV. Leveraging the AMPT model, the investigation delves into the sensitivity of these correlations to the final state effects, providing valuable insights into their potential for constraining the final state effects' $p_T$ dependencies. The HIJING model is employed as a benchmark for non-flow correlations, shedding light on their impact on interpreting SC and ASC data. Moreover, the study points out that differential SC and ASC with one and two particles of interest (POIs) typically incorporate contributions from event-plane angle fluctuations. Consequently, this work highlights the significance of SC and ASC with one and two POIs as valuable tools for investigating the $p_T$ nature of the final state effects and advocates for comprehensive experimental measurements across various beam energies and system sizes to enhance our understanding and provide additional constraints for theoretical models.","sentences":["This study explores the transverse momentum ($p_T$) dependencies of Symmetric and Asymmetric Correlations (SC and ASC) with one and two particles of interest in Au+Au collisions at 200 GeV. Leveraging the AMPT model, the investigation delves into the sensitivity of these correlations to the final state effects, providing valuable insights into their potential for constraining the final state effects' $p_T$ dependencies.","The HIJING model is employed as a benchmark for non-flow correlations, shedding light on their impact on interpreting SC and ASC data.","Moreover, the study points out that differential SC and ASC with one and two particles of interest (POIs) typically incorporate contributions from event-plane angle fluctuations.","Consequently, this work highlights the significance of SC and ASC with one and two POIs as valuable tools for investigating the $p_T$ nature of the final state effects and advocates for comprehensive experimental measurements across various beam energies and system sizes to enhance our understanding and provide additional constraints for theoretical models."],"url":"http://arxiv.org/abs/2405.19169v1","category":"hep-ph"}
{"created":"2024-05-29 15:00:17","title":"Fate of non-Hermitian free fermions with Wannier-Stark ladder","abstract":"The Wannier-Stark ladder (WSL) dynamically alters the entanglement behavior of non-Hermitian free fermions. Using the single-particle correlation matrix, we studied the effective Hamiltonian of these fermions with WSL. By examining the half-chain entanglement entropy (EE) under open boundary conditions (OBCs), we identified two different area law regions and an algebraic scaling region. Finite-size scaling revealed the critical scaling behavior of the half-chain EE. This system also shows distinct entanglement behavior under periodic boundary conditions (PBCs), differing from (1+1)D conformal field theory (CFT) under Anderson localizations scenarios. Our work paves the way for exploring the intriguing entanglement phases arising from the interplay between the non-Hermitian Skin effect (NHSE) and Wannier-Stark localization.","sentences":["The Wannier-Stark ladder (WSL) dynamically alters the entanglement behavior of non-Hermitian free fermions.","Using the single-particle correlation matrix, we studied the effective Hamiltonian of these fermions with WSL.","By examining the half-chain entanglement entropy (EE) under open boundary conditions (OBCs), we identified two different area law regions and an algebraic scaling region.","Finite-size scaling revealed the critical scaling behavior of the half-chain EE.","This system also shows distinct entanglement behavior under periodic boundary conditions (PBCs), differing from (1+1)D conformal field theory (CFT) under Anderson localizations scenarios.","Our work paves the way for exploring the intriguing entanglement phases arising from the interplay between the non-Hermitian Skin effect (NHSE) and Wannier-Stark localization."],"url":"http://arxiv.org/abs/2405.19155v1","category":"quant-ph"}
{"created":"2024-05-29 14:48:49","title":"DeepOKAN: Deep Operator Network Based on Kolmogorov Arnold Networks for Mechanics Problems","abstract":"The modern digital engineering design often requires costly repeated simulations for different scenarios. The prediction capability of neural networks (NNs) makes them suitable surrogates for providing design insights. However, only a few NNs can efficiently handle complex engineering scenario predictions. We introduce a new version of the neural operators called DeepOKAN, which utilizes Kolmogorov Arnold networks (KANs) rather than the conventional neural network architectures. Our DeepOKAN uses Gaussian radial basis functions (RBFs) rather than the B-splines. The DeepOKAN is used to develop surrogates for different mechanics problems. This approach should pave the way for further improving the performance of neural operators. Based on the current investigations, we observe that DeepOKANs require a smaller number of learnable parameters than current MLP-based DeepONets to achieve comparable accuracy.","sentences":["The modern digital engineering design often requires costly repeated simulations for different scenarios.","The prediction capability of neural networks (NNs) makes them suitable surrogates for providing design insights.","However, only a few NNs can efficiently handle complex engineering scenario predictions.","We introduce a new version of the neural operators called DeepOKAN, which utilizes Kolmogorov Arnold networks (KANs) rather than the conventional neural network architectures.","Our DeepOKAN uses Gaussian radial basis functions (RBFs) rather than the B-splines.","The DeepOKAN is used to develop surrogates for different mechanics problems.","This approach should pave the way for further improving the performance of neural operators.","Based on the current investigations, we observe that DeepOKANs require a smaller number of learnable parameters than current MLP-based DeepONets to achieve comparable accuracy."],"url":"http://arxiv.org/abs/2405.19143v1","category":"cs.CE"}
{"created":"2024-05-29 14:47:37","title":"Resilience of mobility network to dynamic population response across COVID-19 interventions: evidences from Chile","abstract":"The COVID19 pandemic highlighted the importance of non-traditional data sources, such as mobile phone data, to inform effective public health interventions and monitor adherence to such measures. Previous studies showed how socioeconomic characteristics shaped population response during restrictions and how repeated interventions eroded adherence over time. Less is known about how different population strata changed their response to repeated interventions and how this impacted the resulting mobility network. We study population response during the first and second infection waves of the COVID-19 pandemic in Chile and Spain. Via spatial lag and regression models, we investigate the adherence to mobility interventions at the municipality level in Chile, highlighting the significant role of wealth, labor structure, COVID-19 incidence, and network metrics characterizing business-as-usual municipality connectivity in shaping mobility changes during the two waves. We assess network structural similarities in the two periods by defining mobility hotspots and traveling probabilities in the two countries. As a proof of concept, we simulate and compare outcomes of an epidemic diffusion occurring in the two waves. Our analysis reveals the resilience of the mobility network across waves. We test the robustness of our findings recovering similar results for Spain. Finally, epidemic modeling suggests that historical mobility data from past waves can be leveraged to inform future disease spatial invasion models in repeated interventions. This study highlights the value of historical mobile phone data for building pandemic preparedness and lessens the need for real-time data streams for risk assessment and outbreak response. Our work provides valuable insights into the complex interplay of factors driving mobility across repeated interventions, aiding in developing targeted mitigation strategies.","sentences":["The COVID19 pandemic highlighted the importance of non-traditional data sources, such as mobile phone data, to inform effective public health interventions and monitor adherence to such measures.","Previous studies showed how socioeconomic characteristics shaped population response during restrictions and how repeated interventions eroded adherence over time.","Less is known about how different population strata changed their response to repeated interventions and how this impacted the resulting mobility network.","We study population response during the first and second infection waves of the COVID-19 pandemic in Chile and Spain.","Via spatial lag and regression models, we investigate the adherence to mobility interventions at the municipality level in Chile, highlighting the significant role of wealth, labor structure, COVID-19 incidence, and network metrics characterizing business-as-usual municipality connectivity in shaping mobility changes during the two waves.","We assess network structural similarities in the two periods by defining mobility hotspots and traveling probabilities in the two countries.","As a proof of concept, we simulate and compare outcomes of an epidemic diffusion occurring in the two waves.","Our analysis reveals the resilience of the mobility network across waves.","We test the robustness of our findings recovering similar results for Spain.","Finally, epidemic modeling suggests that historical mobility data from past waves can be leveraged to inform future disease spatial invasion models in repeated interventions.","This study highlights the value of historical mobile phone data for building pandemic preparedness and lessens the need for real-time data streams for risk assessment and outbreak response.","Our work provides valuable insights into the complex interplay of factors driving mobility across repeated interventions, aiding in developing targeted mitigation strategies."],"url":"http://arxiv.org/abs/2405.19141v1","category":"physics.soc-ph"}
{"created":"2024-05-29 14:42:06","title":"Towards the understanding of heavy quarks hadronization: from leptonic to heavy-ion collisions","abstract":"The formation of hadrons is a fundamental process in nature that can be investigated at particle colliders. As several recent findings demonstrate, with \\ensuremath{\\mathrm{e^+e^-}}\\xspace collisions as a \"vacuum-like\" reference at one extreme, and central AA as a dense, extended-size system characterized by flow and local equilibrium at the opposite extreme, different collision systems offer a lever arm that can be exploited to probe with a range of heavy-flavour hadron species the onset of various hadronization processes. In this review, we present an overview of the theoretical and experimental developments. The focus is on open-heavy-flavour measurements. The comparison with model predictions and connections among the results in electron-positron, proton--proton, proton--nucleus, nucleus--nucleus collisions are discussed. After reviewing the current state, we suggest some prospects and future developments.","sentences":["The formation of hadrons is a fundamental process in nature that can be investigated at particle colliders.","As several recent findings demonstrate, with \\ensuremath{\\mathrm{e^+e^-}}\\xspace collisions as a \"vacuum-like\" reference at one extreme, and central AA as a dense, extended-size system characterized by flow and local equilibrium at the opposite extreme, different collision systems offer a lever arm that can be exploited to probe with a range of heavy-flavour hadron species the onset of various hadronization processes.","In this review, we present an overview of the theoretical and experimental developments.","The focus is on open-heavy-flavour measurements.","The comparison with model predictions and connections among the results in electron-positron, proton--proton, proton--nucleus, nucleus--nucleus collisions are discussed.","After reviewing the current state, we suggest some prospects and future developments."],"url":"http://arxiv.org/abs/2405.19137v1","category":"hep-ph"}
{"created":"2024-05-29 14:37:48","title":"Learning Interpretable Scheduling Algorithms for Data Processing Clusters","abstract":"Workloads in data processing clusters are often represented in the form of DAG (Directed Acyclic Graph) jobs. Scheduling DAG jobs is challenging. Simple heuristic scheduling algorithms are often adopted in practice in production data centres. There is much room for scheduling performance optimisation for cost saving. Recently, reinforcement learning approaches (like decima) have been attempted to optimise DAG job scheduling and demonstrate clear performance gain in comparison to traditional algorithms. However, reinforcement learning (RL) approaches face their own problems in real-world deployment. In particular, their black-box decision making processes and generalizability in unseen workloads may add a non-trivial burden to the cluster administrators. Moreover, adapting RL models on unseen workloads often requires significant amount of training data, which leaves edge cases run in a sub-optimal mode. To fill the gap, we propose a new method to distill a simple scheduling policy based on observations of the behaviours of a complex deep learning model. The simple model not only provides interpretability of scheduling decisions, but also adaptive to edge cases easily through tuning. We show that our method achieves high fidelity to the decisions made by deep learning models and outperforms these models when additional heuristics are taken into account.","sentences":["Workloads in data processing clusters are often represented in the form of DAG (Directed Acyclic Graph) jobs.","Scheduling DAG jobs is challenging.","Simple heuristic scheduling algorithms are often adopted in practice in production data centres.","There is much room for scheduling performance optimisation for cost saving.","Recently, reinforcement learning approaches (like decima) have been attempted to optimise DAG job scheduling and demonstrate clear performance gain in comparison to traditional algorithms.","However, reinforcement learning (RL) approaches face their own problems in real-world deployment.","In particular, their black-box decision making processes and generalizability in unseen workloads may add a non-trivial burden to the cluster administrators.","Moreover, adapting RL models on unseen workloads often requires significant amount of training data, which leaves edge cases run in a sub-optimal mode.","To fill the gap, we propose a new method to distill a simple scheduling policy based on observations of the behaviours of a complex deep learning model.","The simple model not only provides interpretability of scheduling decisions, but also adaptive to edge cases easily through tuning.","We show that our method achieves high fidelity to the decisions made by deep learning models and outperforms these models when additional heuristics are taken into account."],"url":"http://arxiv.org/abs/2405.19131v1","category":"cs.DC"}
{"created":"2024-05-29 14:30:06","title":"ACCSAMS: Automatic Conversion of Exam Documents to Accessible Learning Material for Blind and Visually Impaired","abstract":"Exam documents are essential educational materials for exam preparation. However, they pose a significant academic barrier for blind and visually impaired students, as they are often created without accessibility considerations. Typically, these documents are incompatible with screen readers, contain excessive white space, and lack alternative text for visual elements. This situation frequently requires intervention by experienced sighted individuals to modify the format and content for accessibility. We propose ACCSAMS, a semi-automatic system designed to enhance the accessibility of exam documents. Our system offers three key contributions: (1) creating an accessible layout and removing unnecessary white space, (2) adding navigational structures, and (3) incorporating alternative text for visual elements that were previously missing. Additionally, we present the first multilingual manually annotated dataset, comprising 1,293 German and 900 English exam documents which could serve as a good training source for deep learning models.","sentences":["Exam documents are essential educational materials for exam preparation.","However, they pose a significant academic barrier for blind and visually impaired students, as they are often created without accessibility considerations.","Typically, these documents are incompatible with screen readers, contain excessive white space, and lack alternative text for visual elements.","This situation frequently requires intervention by experienced sighted individuals to modify the format and content for accessibility.","We propose ACCSAMS, a semi-automatic system designed to enhance the accessibility of exam documents.","Our system offers three key contributions: (1) creating an accessible layout and removing unnecessary white space, (2) adding navigational structures, and (3) incorporating alternative text for visual elements that were previously missing.","Additionally, we present the first multilingual manually annotated dataset, comprising 1,293 German and 900 English exam documents which could serve as a good training source for deep learning models."],"url":"http://arxiv.org/abs/2405.19124v1","category":"cs.CV"}
{"created":"2024-05-29 14:26:24","title":"Can Graph Learning Improve Task Planning?","abstract":"Task planning is emerging as an important research topic alongside the development of large language models (LLMs). It aims to break down complex user requests into solvable sub-tasks, thereby fulfilling the original requests. In this context, the sub-tasks can be naturally viewed as a graph, where the nodes represent the sub-tasks, and the edges denote the dependencies among them. Consequently, task planning is a decision-making problem that involves selecting a connected path or subgraph within the corresponding graph and invoking it. In this paper, we explore graph learning-based methods for task planning, a direction that is orthogonal to the prevalent focus on prompt design. Our interest in graph learning stems from a theoretical discovery: the biases of attention and auto-regressive loss impede LLMs' ability to effectively navigate decision-making on graphs, which is adeptly addressed by graph neural networks (GNNs). This theoretical insight led us to integrate GNNs with LLMs to enhance overall performance. Extensive experiments demonstrate that GNN-based methods surpass existing solutions even without training, and minimal training can further enhance their performance. Additionally, our approach complements prompt engineering and fine-tuning techniques, with performance further enhanced by improved prompts or a fine-tuned model.","sentences":["Task planning is emerging as an important research topic alongside the development of large language models (LLMs).","It aims to break down complex user requests into solvable sub-tasks, thereby fulfilling the original requests.","In this context, the sub-tasks can be naturally viewed as a graph, where the nodes represent the sub-tasks, and the edges denote the dependencies among them.","Consequently, task planning is a decision-making problem that involves selecting a connected path or subgraph within the corresponding graph and invoking it.","In this paper, we explore graph learning-based methods for task planning, a direction that is orthogonal to the prevalent focus on prompt design.","Our interest in graph learning stems from a theoretical discovery: the biases of attention and auto-regressive loss impede LLMs' ability to effectively navigate decision-making on graphs, which is adeptly addressed by graph neural networks (GNNs).","This theoretical insight led us to integrate GNNs with LLMs to enhance overall performance.","Extensive experiments demonstrate that GNN-based methods surpass existing solutions even without training, and minimal training can further enhance their performance.","Additionally, our approach complements prompt engineering and fine-tuning techniques, with performance further enhanced by improved prompts or a fine-tuned model."],"url":"http://arxiv.org/abs/2405.19119v1","category":"cs.LG"}
{"created":"2024-05-29 14:24:21","title":"Full Asymptotic Expansion of Monodromy Data for the First Painlev\u00e9 Transcendent: Applications to Connection Problems","abstract":"We study the full asymptotic expansion of the monodromy data ({\\it i.e.}, Stokes multipliers) for the first Painlev\\'{e} transcendent (PI) with large initial data or large pole parameters. Our primary approach involves refining the complex WKB method, also known as the method of uniform asymptotics, to approximate the second-order ODEs derived from PI's Lax pair with higher-order accuracy. As an application, we provide a rigorous proof of the full asymptotic expansion of the nonlinear eigenvalues proposed numerically by Bender, Komijani, and Wang. Additionally, we present the full asymptotic expansion for the pole parameters $(p_{n}, H_{n})$ corresponding to the $n$-th pole of the real tritronqu\\'{e}e solution of the PI equation as $n \\to +\\infty$.","sentences":["We study the full asymptotic expansion of the monodromy data ({\\it i.e.}, Stokes multipliers) for the first Painlev\\'{e} transcendent (PI) with large initial data or large pole parameters.","Our primary approach involves refining the complex WKB method, also known as the method of uniform asymptotics, to approximate the second-order ODEs derived from PI's Lax pair with higher-order accuracy.","As an application, we provide a rigorous proof of the full asymptotic expansion of the nonlinear eigenvalues proposed numerically by Bender, Komijani, and Wang.","Additionally, we present the full asymptotic expansion for the pole parameters $(p_{n}, H_{n})$ corresponding to the $n$-th pole of the real tritronqu\\'{e}e solution of the PI equation as $n \\to +\\infty$."],"url":"http://arxiv.org/abs/2405.19115v1","category":"nlin.SI"}
{"created":"2024-05-29 14:05:19","title":"DataSafe: Copyright Protection with PUF Watermarking and Blockchain Tracking","abstract":"Digital watermarking methods are commonly used to safeguard digital media copyrights by confirming ownership and deterring unauthorized use. However, without reliable third-party oversight, these methods risk security vulnerabilities during watermark extraction. Furthermore, digital media lacks tangible ownership attributes, posing challenges for secure copyright transfer and tracing. This study introduces DataSafe, a copyright protection scheme that combines physical unclonable functions (PUFs) and blockchain technology. PUF devices use their unique fingerprints for blockchain registration. Subsequently, these devices incorporate invisible watermarking techniques to embed digital watermarks into media for copyright protection. The watermark verification process is confined within the devices, preserving confidentiality during extraction, validating identities during copyright exchanges, and facilitating blockchain-based traceability of copyright transfers. The implementation of a prototype system on the LPC55S69-EVK development board is detailed, illustrating the practicality and effectiveness of the proposed solution.","sentences":["Digital watermarking methods are commonly used to safeguard digital media copyrights by confirming ownership and deterring unauthorized use.","However, without reliable third-party oversight, these methods risk security vulnerabilities during watermark extraction.","Furthermore, digital media lacks tangible ownership attributes, posing challenges for secure copyright transfer and tracing.","This study introduces DataSafe, a copyright protection scheme that combines physical unclonable functions (PUFs) and blockchain technology.","PUF devices use their unique fingerprints for blockchain registration.","Subsequently, these devices incorporate invisible watermarking techniques to embed digital watermarks into media for copyright protection.","The watermark verification process is confined within the devices, preserving confidentiality during extraction, validating identities during copyright exchanges, and facilitating blockchain-based traceability of copyright transfers.","The implementation of a prototype system on the LPC55S69-EVK development board is detailed, illustrating the practicality and effectiveness of the proposed solution."],"url":"http://arxiv.org/abs/2405.19099v1","category":"cs.CR"}
{"created":"2024-05-29 13:51:43","title":"Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions","abstract":"Recent advancements in large multimodal language models have demonstrated remarkable proficiency across a wide range of tasks. Yet, these models still struggle with understanding the nuances of human humor through juxtaposition, particularly when it involves nonlinear narratives that underpin many jokes and humor cues. This paper investigates this challenge by focusing on comics with contradictory narratives, where each comic consists of two panels that create a humorous contradiction. We introduce the YesBut benchmark, which comprises tasks of varying difficulty aimed at assessing AI's capabilities in recognizing and interpreting these comics, ranging from literal content comprehension to deep narrative reasoning. Through extensive experimentation and analysis of recent commercial or open-sourced large (vision) language models, we assess their capability to comprehend the complex interplay of the narrative humor inherent in these comics. Our results show that even state-of-the-art models still lag behind human performance on this task. Our findings offer insights into the current limitations and potential improvements for AI in understanding human creative expressions.","sentences":["Recent advancements in large multimodal language models have demonstrated remarkable proficiency across a wide range of tasks.","Yet, these models still struggle with understanding the nuances of human humor through juxtaposition, particularly when it involves nonlinear narratives that underpin many jokes and humor cues.","This paper investigates this challenge by focusing on comics with contradictory narratives, where each comic consists of two panels that create a humorous contradiction.","We introduce the YesBut benchmark, which comprises tasks of varying difficulty aimed at assessing AI's capabilities in recognizing and interpreting these comics, ranging from literal content comprehension to deep narrative reasoning.","Through extensive experimentation and analysis of recent commercial or open-sourced large (vision) language models, we assess their capability to comprehend the complex interplay of the narrative humor inherent in these comics.","Our results show that even state-of-the-art models still lag behind human performance on this task.","Our findings offer insights into the current limitations and potential improvements for AI in understanding human creative expressions."],"url":"http://arxiv.org/abs/2405.19088v1","category":"cs.CL"}
{"created":"2024-05-29 13:36:47","title":"Uniform vs. Lognormal Kinematics in Robots: Perceptual Preferences for Robotic Movements","abstract":"Collaborative robots or cobots interact with humans in a common work environment. In cobots, one under investigated but important issue is related to their movement and how it is perceived by humans. This paper tries to analyze whether humans prefer a robot moving in a human or in a robotic fashion. To this end, the present work lays out what differentiates the movement performed by an industrial robotic arm from that performed by a human one. The main difference lies in the fact that the robotic movement has a trapezoidal speed profile, while for the human arm, the speed profile is bell-shaped and during complex movements, it can be considered as a sum of superimposed bell-shaped movements. Based on the lognormality principle, a procedure was developed for a robotic arm to perform human-like movements. Both speed profiles were implemented in two industrial robots, namely, an ABB IRB 120 and a Universal Robot UR3. Three tests were used to study the subjects' preference when seeing both movements and another analyzed the same when interacting with the robot by touching its ends with their fingers.","sentences":["Collaborative robots or cobots interact with humans in a common work environment.","In cobots, one under investigated but important issue is related to their movement and how it is perceived by humans.","This paper tries to analyze whether humans prefer a robot moving in a human or in a robotic fashion.","To this end, the present work lays out what differentiates the movement performed by an industrial robotic arm from that performed by a human one.","The main difference lies in the fact that the robotic movement has a trapezoidal speed profile, while for the human arm, the speed profile is bell-shaped and during complex movements, it can be considered as a sum of superimposed bell-shaped movements.","Based on the lognormality principle, a procedure was developed for a robotic arm to perform human-like movements.","Both speed profiles were implemented in two industrial robots, namely, an ABB IRB 120 and a Universal Robot UR3.","Three tests were used to study the subjects' preference when seeing both movements and another analyzed the same when interacting with the robot by touching its ends with their fingers."],"url":"http://arxiv.org/abs/2405.19081v1","category":"cs.RO"}
{"created":"2024-05-29 13:34:51","title":"Diagrammatic Representations of Higher-Dimensional Topological Orders","abstract":"In 3D spacetime, topologically ordered phases of matter feature emergent particles called anyons, whose properties like fusion rules and braiding statistics are schematically represented by diagrams. Important consistency conditions, such as pentagon and hexagon relations, are encoded in diagrams. In 4D and higher spacetimes, topological orders support not only point-like particles but also spatially extended excitations, such as loops and membranes, allowing for diverse topological data about braiding, fusion, and shrinking processes. Recently, these topological data have been explored through the path-integral formalism of topological quantum field theory. Analogous to the counterpart in 3D, in this work, we construct diagrammatic representations of higher-dimensional (4D and 5D) topological orders. We introduce basic fusion and shrinking diagrams and treat them as vectors in corresponding spaces, then build complex diagrams by stacking these basic diagrams. Within the same vector spaces, we use $F$-, $\\Delta$-, and $\\Delta^2$-symbols to transform between different bases. From these transformations, we derive consistency conditions like \\textit{pentagon equations} and \\textit{(hierarchical) shrinking-fusion hexagon equations}, which describe the consistent coexistence of fusion and shrinking data. We conjecture that all anomaly-free higher-dimensional topological orders must satisfy these conditions, with violations indicating a quantum anomaly. This work opens several promising avenues for future research such as the exploration of diagrammatic representations of braiding processes in higher dimensions as well as implications for noninvertible symmetries and Symmetry Topological Field Theory (SymTFT).","sentences":["In 3D spacetime, topologically ordered phases of matter feature emergent particles called anyons, whose properties like fusion rules and braiding statistics are schematically represented by diagrams.","Important consistency conditions, such as pentagon and hexagon relations, are encoded in diagrams.","In 4D and higher spacetimes, topological orders support not only point-like particles but also spatially extended excitations, such as loops and membranes, allowing for diverse topological data about braiding, fusion, and shrinking processes.","Recently, these topological data have been explored through the path-integral formalism of topological quantum field theory.","Analogous to the counterpart in 3D, in this work, we construct diagrammatic representations of higher-dimensional (4D and 5D) topological orders.","We introduce basic fusion and shrinking diagrams and treat them as vectors in corresponding spaces, then build complex diagrams by stacking these basic diagrams.","Within the same vector spaces, we use $F$-, $\\Delta$-, and $\\Delta^2$-symbols to transform between different bases.","From these transformations, we derive consistency conditions like \\textit{pentagon equations} and \\textit{(hierarchical) shrinking-fusion hexagon equations}, which describe the consistent coexistence of fusion and shrinking data.","We conjecture that all anomaly-free higher-dimensional topological orders must satisfy these conditions, with violations indicating a quantum anomaly.","This work opens several promising avenues for future research such as the exploration of diagrammatic representations of braiding processes in higher dimensions as well as implications for noninvertible symmetries and Symmetry Topological Field Theory (SymTFT)."],"url":"http://arxiv.org/abs/2405.19077v1","category":"hep-th"}
{"created":"2024-05-29 13:08:50","title":"Fracture metamaterials with on-demand crack paths enabled by bending","abstract":"In many scenarios -- when we bite food or during a crash -- fracture is inevitable. Finding solutions to steer fracture to mitigate its impact or turn it into a purposeful functionality, is therefore crucial. Strategies using composites, changes in chemical composition or crystal orientation, have proven to be very efficient, but the crack path control remains limited and has not been achieved in load-bearing structures. Here, we introduce fracture metamaterials consisting of slender elements whose bending enables large elastic deformation as fracture propagates. This interplay between bending and fracture enables tunable energy dissipation and the design of on-demand crack paths of arbitrary complexity. To this end, we use topology optimisation to create unit cells with anisotropic fracture energy, which we then tile up to realize fracture metamaterials with uniform density that we 3D-print. The thin ligaments that constitute the unit cells confer them a strikingly distinct response in tension and shear, and we show that by controlling the orientation and layout of the unit cells the sequential progress of the crack can be controlled, making the fracture path arbitrarily tortuous. This tortuosity increases the energy dissipation of the metamaterial without changing its stiffness. Using bespoke arrangements of unit cells, metamaterials can have on-demand fracture paths of arbitrary complexity. Our findings bring a new perspective on inelastic deformations in mechanical metamaterials, with potential applications in areas as diverse as the food industry, structural design, and for shock and impact damping.","sentences":["In many scenarios -- when we bite food or during a crash -- fracture is inevitable.","Finding solutions to steer fracture to mitigate its impact or turn it into a purposeful functionality, is therefore crucial.","Strategies using composites, changes in chemical composition or crystal orientation, have proven to be very efficient, but the crack path control remains limited and has not been achieved in load-bearing structures.","Here, we introduce fracture metamaterials consisting of slender elements whose bending enables large elastic deformation as fracture propagates.","This interplay between bending and fracture enables tunable energy dissipation and the design of on-demand crack paths of arbitrary complexity.","To this end, we use topology optimisation to create unit cells with anisotropic fracture energy, which we then tile up to realize fracture metamaterials with uniform density that we 3D-print.","The thin ligaments that constitute the unit cells confer them a strikingly distinct response in tension and shear, and we show that by controlling the orientation and layout of the unit cells the sequential progress of the crack can be controlled, making the fracture path arbitrarily tortuous.","This tortuosity increases the energy dissipation of the metamaterial without changing its stiffness.","Using bespoke arrangements of unit cells, metamaterials can have on-demand fracture paths of arbitrary complexity.","Our findings bring a new perspective on inelastic deformations in mechanical metamaterials, with potential applications in areas as diverse as the food industry, structural design, and for shock and impact damping."],"url":"http://arxiv.org/abs/2405.19061v1","category":"cond-mat.soft"}
{"created":"2024-05-29 12:56:18","title":"Neural Scene Baking for Permutation Invariant Transparency Rendering with Real-time Global Illumination","abstract":"Neural rendering provides a fundamentally new way to render photorealistic images. Similar to traditional light-baking methods, neural rendering utilizes neural networks to bake representations of scenes, materials, and lights into latent vectors learned from path-tracing ground truths. However, existing neural rendering algorithms typically use G-buffers to provide position, normal, and texture information of scenes, which are prone to occlusion by transparent surfaces, leading to distortions and loss of detail in the rendered images. To address this limitation, we propose a novel neural rendering pipeline that accurately renders the scene behind transparent surfaces with global illumination and variable scenes. Our method separates the G-buffers of opaque and transparent objects, retaining G-buffer information behind transparent objects. Additionally, to render the transparent objects with permutation invariance, we designed a new permutation-invariant neural blending function. We integrate our algorithm into an efficient custom renderer to achieve real-time performance. Our results show that our method is capable of rendering photorealistic images with variable scenes and viewpoints, accurately capturing complex transparent structures along with global illumination. Our renderer can achieve real-time performance ($256\\times 256$ at 63 FPS and $512\\times 512$ at 32 FPS) on scenes with multiple variable transparent objects.","sentences":["Neural rendering provides a fundamentally new way to render photorealistic images.","Similar to traditional light-baking methods, neural rendering utilizes neural networks to bake representations of scenes, materials, and lights into latent vectors learned from path-tracing ground truths.","However, existing neural rendering algorithms typically use G-buffers to provide position, normal, and texture information of scenes, which are prone to occlusion by transparent surfaces, leading to distortions and loss of detail in the rendered images.","To address this limitation, we propose a novel neural rendering pipeline that accurately renders the scene behind transparent surfaces with global illumination and variable scenes.","Our method separates the G-buffers of opaque and transparent objects, retaining G-buffer information behind transparent objects.","Additionally, to render the transparent objects with permutation invariance, we designed a new permutation-invariant neural blending function.","We integrate our algorithm into an efficient custom renderer to achieve real-time performance.","Our results show that our method is capable of rendering photorealistic images with variable scenes and viewpoints, accurately capturing complex transparent structures along with global illumination.","Our renderer can achieve real-time performance ($256\\times 256$ at 63 FPS and $512\\times 512$ at 32 FPS) on scenes with multiple variable transparent objects."],"url":"http://arxiv.org/abs/2405.19056v1","category":"cs.GR"}
{"created":"2024-05-29 12:56:11","title":"FUSU: A Multi-temporal-source Land Use Change Segmentation Dataset for Fine-grained Urban Semantic Understanding","abstract":"Fine urban change segmentation using multi-temporal remote sensing images is essential for understanding human-environment interactions. Despite advances in remote sensing data for urban monitoring, coarse-grained classification systems and the lack of continuous temporal observations hinder the application of deep learning to urban change analysis. To address this, we introduce FUSU, a multi-source, multi-temporal change segmentation dataset for fine-grained urban semantic understanding. FUSU features the most detailed land use classification system to date, with 17 classes and 30 billion pixels of annotations. It includes bi-temporal high-resolution satellite images with 20-50 cm ground sample distance and monthly optical and radar satellite time series, covering 847 km2 across five urban areas in China. The fine-grained pixel-wise annotations and high spatial-temporal resolution data provide a robust foundation for deep learning models to understand urbanization and land use changes. To fully leverage FUSU, we propose a unified time-series architecture for both change detection and segmentation and benchmark FUSU on various methods for several tasks. Dataset and code will be available at: https://github.com/yuanshuai0914/FUSU.","sentences":["Fine urban change segmentation using multi-temporal remote sensing images is essential for understanding human-environment interactions.","Despite advances in remote sensing data for urban monitoring, coarse-grained classification systems and the lack of continuous temporal observations hinder the application of deep learning to urban change analysis.","To address this, we introduce FUSU, a multi-source, multi-temporal change segmentation dataset for fine-grained urban semantic understanding.","FUSU features the most detailed land use classification system to date, with 17 classes and 30 billion pixels of annotations.","It includes bi-temporal high-resolution satellite images with 20-50 cm ground sample distance and monthly optical and radar satellite time series, covering 847 km2 across five urban areas in China.","The fine-grained pixel-wise annotations and high spatial-temporal resolution data provide a robust foundation for deep learning models to understand urbanization and land use changes.","To fully leverage FUSU, we propose a unified time-series architecture for both change detection and segmentation and benchmark FUSU on various methods for several tasks.","Dataset and code will be available at: https://github.com/yuanshuai0914/FUSU."],"url":"http://arxiv.org/abs/2405.19055v1","category":"cs.CV"}
{"created":"2024-05-29 12:43:39","title":"Continual Collaborative Distillation for Recommender System","abstract":"Knowledge distillation (KD) has emerged as a promising technique for addressing the computational challenges associated with deploying large-scale recommender systems. KD transfers the knowledge of a massive teacher system to a compact student model, to reduce the huge computational burdens for inference while retaining high accuracy. The existing KD studies primarily focus on one-time distillation in static environments, leaving a substantial gap in their applicability to real-world scenarios dealing with continuously incoming users, items, and their interactions. In this work, we delve into a systematic approach to operating the teacher-student KD in a non-stationary data stream. Our goal is to enable efficient deployment through a compact student, which preserves the high performance of the massive teacher, while effectively adapting to continuously incoming data. We propose Continual Collaborative Distillation (CCD) framework, where both the teacher and the student continually and collaboratively evolve along the data stream. CCD facilitates the student in effectively adapting to new data, while also enabling the teacher to fully leverage accumulated knowledge. We validate the effectiveness of CCD through extensive quantitative, ablative, and exploratory experiments on two real-world datasets. We expect this research direction to contribute to narrowing the gap between existing KD studies and practical applications, thereby enhancing the applicability of KD in real-world systems.","sentences":["Knowledge distillation (KD) has emerged as a promising technique for addressing the computational challenges associated with deploying large-scale recommender systems.","KD transfers the knowledge of a massive teacher system to a compact student model, to reduce the huge computational burdens for inference while retaining high accuracy.","The existing KD studies primarily focus on one-time distillation in static environments, leaving a substantial gap in their applicability to real-world scenarios dealing with continuously incoming users, items, and their interactions.","In this work, we delve into a systematic approach to operating the teacher-student KD in a non-stationary data stream.","Our goal is to enable efficient deployment through a compact student, which preserves the high performance of the massive teacher, while effectively adapting to continuously incoming data.","We propose Continual Collaborative Distillation (CCD) framework, where both the teacher and the student continually and collaboratively evolve along the data stream.","CCD facilitates the student in effectively adapting to new data, while also enabling the teacher to fully leverage accumulated knowledge.","We validate the effectiveness of CCD through extensive quantitative, ablative, and exploratory experiments on two real-world datasets.","We expect this research direction to contribute to narrowing the gap between existing KD studies and practical applications, thereby enhancing the applicability of KD in real-world systems."],"url":"http://arxiv.org/abs/2405.19046v1","category":"cs.IR"}
{"created":"2024-05-29 12:38:01","title":"On adaptive stochastic extended iterative methods for solving least squares","abstract":"In this paper, we propose a novel adaptive stochastic extended iterative method, which can be viewed as an improved extension of the randomized extended Kaczmarz (REK) method, for finding the unique minimum Euclidean norm least-squares solution of a given linear system. In particular, we introduce three equivalent stochastic reformulations of the linear least-squares problem: stochastic unconstrained and constrained optimization problems, and the stochastic multiobjective optimization problem. We then alternately employ the adaptive variants of the stochastic heavy ball momentum (SHBM) method, which utilize iterative information to update the parameters, to solve the stochastic reformulations. We prove that our method converges linearly in expectation, addressing an open problem in the literature related to designing theoretically supported adaptive SHBM methods. Numerical experiments show that our adaptive stochastic extended iterative method has strong advantages over the non-adaptive one.","sentences":["In this paper, we propose a novel adaptive stochastic extended iterative method, which can be viewed as an improved extension of the randomized extended Kaczmarz (REK) method, for finding the unique minimum Euclidean norm least-squares solution of a given linear system.","In particular, we introduce three equivalent stochastic reformulations of the linear least-squares problem: stochastic unconstrained and constrained optimization problems, and the stochastic multiobjective optimization problem.","We then alternately employ the adaptive variants of the stochastic heavy ball momentum (SHBM) method, which utilize iterative information to update the parameters, to solve the stochastic reformulations.","We prove that our method converges linearly in expectation, addressing an open problem in the literature related to designing theoretically supported adaptive SHBM methods.","Numerical experiments show that our adaptive stochastic extended iterative method has strong advantages over the non-adaptive one."],"url":"http://arxiv.org/abs/2405.19044v1","category":"math.NA"}
{"created":"2024-05-29 12:18:32","title":"SynerGraph: An Integrated Graph Convolution Network for Multimodal Recommendation","abstract":"This article presents a novel approach to multimodal recommendation systems, focusing on integrating and purifying multimodal data. Our methodology starts by developing a filter to remove noise from various types of data, making the recommendations more reliable. We studied the impact of top-K sparsification on different datasets, finding optimal values that strike a balance between underfitting and overfitting concerns. The study emphasizes the significant role of textual information compared to visual data in providing a deep understanding of items. We conducted sensitivity analyses to understand how different modalities and the use of purifier circle loss affect the efficiency of the model. The findings indicate that systems that incorporate multiple modalities perform better than those relying on just one modality. Our approach highlights the importance of modality purifiers in filtering out irrelevant data, ensuring that user preferences remain relevant. Models without modality purifiers showed reduced performance, emphasizing the need for effective integration of pre-extracted features. The proposed model, which includes an novel self supervised auxiliary task, shows promise in accurately capturing user preferences. The main goal of the fusion technique is to enhance the modeling of user preferences by combining knowledge with item information, utilizing sophisticated language models. Extensive experiments show that our model produces better results than the existing state-of-the-art multimodal recommendation systems.","sentences":["This article presents a novel approach to multimodal recommendation systems, focusing on integrating and purifying multimodal data.","Our methodology starts by developing a filter to remove noise from various types of data, making the recommendations more reliable.","We studied the impact of top-K sparsification on different datasets, finding optimal values that strike a balance between underfitting and overfitting concerns.","The study emphasizes the significant role of textual information compared to visual data in providing a deep understanding of items.","We conducted sensitivity analyses to understand how different modalities and the use of purifier circle loss affect the efficiency of the model.","The findings indicate that systems that incorporate multiple modalities perform better than those relying on just one modality.","Our approach highlights the importance of modality purifiers in filtering out irrelevant data, ensuring that user preferences remain relevant.","Models without modality purifiers showed reduced performance, emphasizing the need for effective integration of pre-extracted features.","The proposed model, which includes an novel self supervised auxiliary task, shows promise in accurately capturing user preferences.","The main goal of the fusion technique is to enhance the modeling of user preferences by combining knowledge with item information, utilizing sophisticated language models.","Extensive experiments show that our model produces better results than the existing state-of-the-art multimodal recommendation systems."],"url":"http://arxiv.org/abs/2405.19031v1","category":"cs.IR"}
{"created":"2024-05-29 12:18:09","title":"Exploring Frustration Effects of Strongly Interacting Bosons via the Hall Response","abstract":"We investigate the Hall response of hardcore bosonic atoms on a triangular ladder in a magnetic field. We show that the behavior of the Hall polarization, both in its saturation value and in the short-time dynamics, correlates with the features of the underlying phase diagram. In particular, it exhibits a strong negative response at the Meissner to vortex phase transitions, while at large magnetic fluxes, after changing sign, it shows a strong positive response due to non-trivial vortex commensurability effects stemming from the interplay of interactions and geometric frustration. Thus, one can employ the Hall response as a sensitive probe of the many-body chiral quantum phases present in the system.","sentences":["We investigate the Hall response of hardcore bosonic atoms on a triangular ladder in a magnetic field.","We show that the behavior of the Hall polarization, both in its saturation value and in the short-time dynamics, correlates with the features of the underlying phase diagram.","In particular, it exhibits a strong negative response at the Meissner to vortex phase transitions, while at large magnetic fluxes, after changing sign, it shows a strong positive response due to non-trivial vortex commensurability effects stemming from the interplay of interactions and geometric frustration.","Thus, one can employ the Hall response as a sensitive probe of the many-body chiral quantum phases present in the system."],"url":"http://arxiv.org/abs/2405.19030v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-29 11:54:11","title":"Distributed Management of Fluctuating Energy Resources in Dynamic Networked Systems","abstract":"Modern power systems integrate renewable distributed energy resources (DERs) as an environment-friendly enhancement to meet the ever-increasing demands. However, the inherent unreliability of renewable energy renders developing DER management algorithms imperative. We study the energy-sharing problem in a system consisting of several DERs. Each agent harvests and distributes renewable energy in its neighborhood to optimize the network's performance while minimizing energy waste. We model this problem as a bandit convex optimization problem with constraints that correspond to each node's limitations for energy production. We propose distributed decision-making policies to solve the formulated problem, where we utilize the notion of dynamic regret as the performance metric. We also include an adjustment strategy in our developed algorithm to reduce the constraint violations. Besides, we design a policy that deals with the non-stationary environment. Theoretical analysis shows the effectiveness of our proposed algorithm. Numerical experiments using a real-world dataset show superior performance of our proposal compared to state-of-the-art methods.","sentences":["Modern power systems integrate renewable distributed energy resources (DERs) as an environment-friendly enhancement to meet the ever-increasing demands.","However, the inherent unreliability of renewable energy renders developing DER management algorithms imperative.","We study the energy-sharing problem in a system consisting of several DERs.","Each agent harvests and distributes renewable energy in its neighborhood to optimize the network's performance while minimizing energy waste.","We model this problem as a bandit convex optimization problem with constraints that correspond to each node's limitations for energy production.","We propose distributed decision-making policies to solve the formulated problem, where we utilize the notion of dynamic regret as the performance metric.","We also include an adjustment strategy in our developed algorithm to reduce the constraint violations.","Besides, we design a policy that deals with the non-stationary environment.","Theoretical analysis shows the effectiveness of our proposed algorithm.","Numerical experiments using a real-world dataset show superior performance of our proposal compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.19015v1","category":"eess.SY"}
{"created":"2024-05-29 11:52:53","title":"On Dissipativity of Cross-Entropy Loss in Training ResNets","abstract":"The training of ResNets and neural ODEs can be formulated and analyzed from the perspective of optimal control. This paper proposes a dissipative formulation of the training of ResNets and neural ODEs for classification problems by including a variant of the cross-entropy as a regularization in the stage cost. Based on the dissipative formulation of the training, we prove that the trained ResNet exhibit the turnpike phenomenon. We then illustrate that the training exhibits the turnpike phenomenon by training on the two spirals and MNIST datasets. This can be used to find very shallow networks suitable for a given classification task.","sentences":["The training of ResNets and neural ODEs can be formulated and analyzed from the perspective of optimal control.","This paper proposes a dissipative formulation of the training of ResNets and neural ODEs for classification problems by including a variant of the cross-entropy as a regularization in the stage cost.","Based on the dissipative formulation of the training, we prove that the trained ResNet exhibit the turnpike phenomenon.","We then illustrate that the training exhibits the turnpike phenomenon by training on the two spirals and MNIST datasets.","This can be used to find very shallow networks suitable for a given classification task."],"url":"http://arxiv.org/abs/2405.19013v1","category":"cs.LG"}
{"created":"2024-05-29 11:37:36","title":"A structure-preserving scheme for computing effective diffusivity and anomalous diffusion phenomena of random flows","abstract":"This paper aims to investigate the diffusion behavior of particles moving in stochastic flows under a structure-preserving scheme. We compute the effective diffusivity for normal diffusive random flows and establish the power law between spatial and temporal variables for cases with anomalous diffusion phenomena. From a Lagrangian approach, we separate the corresponding stochastic differential equations (SDEs) into sub-problems and construct a one-step structure-preserving method to solve them. Then by modified equation systems, the convergence analysis in calculating the effective diffusivity is provided and compared between the structure-preserving scheme and the Euler-Maruyama scheme. Also, we provide the error estimate for the structure-preserving scheme in calculating the power law for a series of super-diffusive random flows. Finally, we calculate the effective diffusivity and anomalous diffusion phenomena for a series of 2D and 3D random fields.","sentences":["This paper aims to investigate the diffusion behavior of particles moving in stochastic flows under a structure-preserving scheme.","We compute the effective diffusivity for normal diffusive random flows and establish the power law between spatial and temporal variables for cases with anomalous diffusion phenomena.","From a Lagrangian approach, we separate the corresponding stochastic differential equations (SDEs) into sub-problems and construct a one-step structure-preserving method to solve them.","Then by modified equation systems, the convergence analysis in calculating the effective diffusivity is provided and compared between the structure-preserving scheme and the Euler-Maruyama scheme.","Also, we provide the error estimate for the structure-preserving scheme in calculating the power law for a series of super-diffusive random flows.","Finally, we calculate the effective diffusivity and anomalous diffusion phenomena for a series of 2D and 3D random fields."],"url":"http://arxiv.org/abs/2405.19003v1","category":"math.NA"}
{"created":"2024-05-29 11:25:31","title":"Derandomized Non-Abelian Homomorphism Testing in Low Soundness Regime","abstract":"We give a randomness-efficient homomorphism test in the low soundness regime for functions, $f: G\\to \\mathbb{U}_t$, from an arbitrary finite group $G$ to $t\\times t$ unitary matrices. We show that if such a function passes a derandomized Blum--Luby--Rubinfeld (BLR) test (using small-bias sets), then (i) it correlates with a function arising from a genuine homomorphism, and (ii) it has a non-trivial Fourier mass on a low-dimensional irreducible representation.   In the full randomness regime, such a test for matrix-valued functions on finite groups implicitly appears in the works of Gowers and Hatami [Sbornik: Mathematics '17], and Moore and Russell [SIAM Journal on Discrete Mathematics '15]. Thus, our work can be seen as a near-optimal derandomization of their results. Our key technical contribution is a \"degree-2 expander mixing lemma'' that shows that Gowers' $\\mathrm{U}^2$ norm can be efficiently estimated by restricting it to a small-bias subset. Another corollary is a \"derandomized'' version of a useful lemma due to Babai, Nikolov, and Pyber [SODA'08].","sentences":["We give a randomness-efficient homomorphism test in the low soundness regime for functions, $f: G\\to \\mathbb{U}_t$, from an arbitrary finite group $G$ to $t\\times t$ unitary matrices.","We show that if such a function passes a derandomized Blum--Luby--Rubinfeld (BLR) test (using small-bias sets), then (i) it correlates with a function arising from a genuine homomorphism, and (ii) it has a non-trivial Fourier mass on a low-dimensional irreducible representation.   ","In the full randomness regime, such a test for matrix-valued functions on finite groups implicitly appears in the works of Gowers and Hatami","[Sbornik: Mathematics '17], and Moore and Russell","[SIAM Journal on Discrete Mathematics '15].","Thus, our work can be seen as a near-optimal derandomization of their results.","Our key technical contribution is a \"degree-2 expander mixing lemma'' that shows that Gowers' $\\mathrm{U}^2$ norm can be efficiently estimated by restricting it to a small-bias subset.","Another corollary is a \"derandomized'' version of a useful lemma due to Babai, Nikolov, and Pyber","[SODA'08]."],"url":"http://arxiv.org/abs/2405.18998v1","category":"cs.CC"}
{"created":"2024-05-29 11:17:26","title":"Best Ergodic Averages via Optimal Graph Filters in Reversible Markov Chains","abstract":"In this paper, we address the problem of finding the best ergodic or Birkhoff averages in the ergodic theorem to ensure rapid convergence to a desired value, using graph filters. Our approach begins by representing a function on the state space as a graph signal, where the (directed) graph is formed by the transition probabilities of a reversible Markov chain. We introduce a concept of graph variation, enabling the definition of the graph Fourier transform for graph signals on this directed graph. Viewing the iteration in the ergodic theorem as a graph filter, we recognize its non-optimality and propose three optimization problems aimed at determining optimal graph filters. These optimization problems yield the Bernstein, Chebyshev, and Legendre filters. Numerical testing reveals that while the Bernstein filter performs slightly better than the traditional ergodic average, the Chebyshev and Legendre filters significantly outperform the ergodic average, demonstrating rapid convergence to the desired value.","sentences":["In this paper, we address the problem of finding the best ergodic or Birkhoff averages in the ergodic theorem to ensure rapid convergence to a desired value, using graph filters.","Our approach begins by representing a function on the state space as a graph signal, where the (directed) graph is formed by the transition probabilities of a reversible Markov chain.","We introduce a concept of graph variation, enabling the definition of the graph Fourier transform for graph signals on this directed graph.","Viewing the iteration in the ergodic theorem as a graph filter, we recognize its non-optimality and propose three optimization problems aimed at determining optimal graph filters.","These optimization problems yield the Bernstein, Chebyshev, and Legendre filters.","Numerical testing reveals that while the Bernstein filter performs slightly better than the traditional ergodic average, the Chebyshev and Legendre filters significantly outperform the ergodic average, demonstrating rapid convergence to the desired value."],"url":"http://arxiv.org/abs/2405.18995v1","category":"eess.SY"}
{"created":"2024-05-29 11:16:53","title":"Tuning load redistribution and damage near heterogeneous interfaces","abstract":"We investigate interface failure of model materials representing architected thin films in contact with heterogeneous substrates. We find that, while systems with statistically isotropic distributions of impurities derive their fracture strength from the ability to develop rough detachment fronts, materials with hierarchical microstructures confine failure near a prescribed surface, where crack growth is arrested and crack surface correlations are suppressed. We develop a theory of network Green's functions for the systems at hand, and we find that the ability of hierarchical microstructures to control failure mode and locations comes at no performance cost in terms of peak stress and specific work of failure and derives from the quenched local anistotropy of the elastic interaction kernel.","sentences":["We investigate interface failure of model materials representing architected thin films in contact with heterogeneous substrates.","We find that, while systems with statistically isotropic distributions of impurities derive their fracture strength from the ability to develop rough detachment fronts, materials with hierarchical microstructures confine failure near a prescribed surface, where crack growth is arrested and crack surface correlations are suppressed.","We develop a theory of network Green's functions for the systems at hand, and we find that the ability of hierarchical microstructures to control failure mode and locations comes at no performance cost in terms of peak stress and specific work of failure and derives from the quenched local anistotropy of the elastic interaction kernel."],"url":"http://arxiv.org/abs/2405.18994v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 11:14:07","title":"A Digital Beamforming Receiver Architecture Implemented on a FPGA for Space Applications","abstract":"The burgeoning interest within the space community in digital beamforming is largely attributable to the superior flexibility that satellites with active antenna systems offer for a wide range of applications, notably in communication services. This paper delves into the analysis and practical implementation of a Digital Beamforming and Digital Down Conversion (DDC) chain, leveraging a high-speed Analog-to-Digital Converter (ADC) certified for space applications alongside a high-performance Field-Programmable Gate Array (FPGA). The proposed design strategy focuses on optimizing resource efficiency and minimizing power consumption by strategically sequencing the beamformer processor ahead of the complex down-conversion operation. This innovative approach entails the application of demodulation and low-pass filtering exclusively to the aggregated beam channel, culminating in a marked reduction in the requisite digital signal processing resources relative to traditional, more resource-intensive digital beamforming and DDC architectures. In the experimental validation, an evaluation board integrating a high-speed ADC and a FPGA was utilized. This setup facilitated the empirical validation of the design's efficacy by applying various RF input signals to the digital beamforming receiver system. The ADC employed is capable of high-resolution signal processing, while the FPGA provides the necessary computational flexibility and speed for real-time digital signal processing tasks. The findings underscore the potential of this design to significantly enhance the efficiency and performance of digital beamforming systems in space applications.","sentences":["The burgeoning interest within the space community in digital beamforming is largely attributable to the superior flexibility that satellites with active antenna systems offer for a wide range of applications, notably in communication services.","This paper delves into the analysis and practical implementation of a Digital Beamforming and Digital Down Conversion (DDC) chain, leveraging a high-speed Analog-to-Digital Converter (ADC) certified for space applications alongside a high-performance Field-Programmable Gate Array (FPGA).","The proposed design strategy focuses on optimizing resource efficiency and minimizing power consumption by strategically sequencing the beamformer processor ahead of the complex down-conversion operation.","This innovative approach entails the application of demodulation and low-pass filtering exclusively to the aggregated beam channel, culminating in a marked reduction in the requisite digital signal processing resources relative to traditional, more resource-intensive digital beamforming and DDC architectures.","In the experimental validation, an evaluation board integrating a high-speed ADC and a FPGA was utilized.","This setup facilitated the empirical validation of the design's efficacy by applying various RF input signals to the digital beamforming receiver system.","The ADC employed is capable of high-resolution signal processing, while the FPGA provides the necessary computational flexibility and speed for real-time digital signal processing tasks.","The findings underscore the potential of this design to significantly enhance the efficiency and performance of digital beamforming systems in space applications."],"url":"http://arxiv.org/abs/2405.18992v1","category":"eess.SP"}
{"created":"2024-05-29 11:09:16","title":"Classification analysis of transition-metal chalcogenides and oxides using quantum machine learning","abstract":"Quantum machine learning (QML) leverages the potential from machine learning to explore the subtle patterns in huge datasets of complex nature with quantum advantages. This exponentially reduces the time and resources necessary for computations. QML accelerates materials research with active screening of chemical space, identifying novel materials for practical applications and classifying structurally diverse materials given their measured properties. This study analyzes the performance of three efficient quantum machine learning algorithms viz., variational quantum eigen solver (VQE), quantum support vector machine (QSVM) and quantum neural networks (QNN) for the classification of transition metal chalcogenides and oxides (TMCs &TMOs). The analysis is performed on three datasets of different sizes containing 102, 192 and 350 materials with TMCs and TMOs labelled as +1 and -1 respectively. By employing feature selection, classical machine learning achieves 100% accuracy whereas QML achieves the highest performance of 99% and 98% for test and train data respectively on QSVC. This study establishes the competence of QML models in materials classification and explores the quantum circuits in terms of over-fitting using the circuit descriptors expressibility and entangling capability. In addition, the perspectives on QML in materials research with noisy intermediate scale quantum (NISQ) devices is given.","sentences":["Quantum machine learning (QML) leverages the potential from machine learning to explore the subtle patterns in huge datasets of complex nature with quantum advantages.","This exponentially reduces the time and resources necessary for computations.","QML accelerates materials research with active screening of chemical space, identifying novel materials for practical applications and classifying structurally diverse materials given their measured properties.","This study analyzes the performance of three efficient quantum machine learning algorithms viz., variational quantum eigen solver (VQE), quantum support vector machine (QSVM) and quantum neural networks (QNN) for the classification of transition metal chalcogenides and oxides (TMCs &TMOs).","The analysis is performed on three datasets of different sizes containing 102, 192 and 350 materials with TMCs and TMOs labelled as +1 and -1 respectively.","By employing feature selection, classical machine learning achieves 100% accuracy whereas QML achieves the highest performance of 99% and 98% for test and train data respectively on QSVC.","This study establishes the competence of QML models in materials classification and explores the quantum circuits in terms of over-fitting using the circuit descriptors expressibility and entangling capability.","In addition, the perspectives on QML in materials research with noisy intermediate scale quantum (NISQ) devices is given."],"url":"http://arxiv.org/abs/2405.18989v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 10:54:33","title":"A dynamical geography of observed trends in the global ocean","abstract":"Revealing the ongoing changes in ocean dynamics and their impact on marine ecosystems requires the joint analysis of multiple variables. Yet, global observational records only cover a few decades, posing a challenge in the separation of climatic trends from internal dynamical modes. Here, we apply an empirical stochastic model to identify the emergent patterns of trends in six fundamental components of upper ocean physics. We analyze a data-driven reconstruction of the ocean state covering the 1993-2018 period. We found that including temporal derivatives into the state vector enhances the description of the ocean's dynamical system. Once Pacific oscillations are properly accounted for, averaged surface warming appears >60% faster, and a deep reshaping of the seascape is revealed. A clustering of the trend patterns identifies the main factors that drive observed trends in chlorophyll-a concentration. This data-driven approach opens new perspectives in empirical climate modelling.","sentences":["Revealing the ongoing changes in ocean dynamics and their impact on marine ecosystems requires the joint analysis of multiple variables.","Yet, global observational records only cover a few decades, posing a challenge in the separation of climatic trends from internal dynamical modes.","Here, we apply an empirical stochastic model to identify the emergent patterns of trends in six fundamental components of upper ocean physics.","We analyze a data-driven reconstruction of the ocean state covering the 1993-2018 period.","We found that including temporal derivatives into the state vector enhances the description of the ocean's dynamical system.","Once Pacific oscillations are properly accounted for, averaged surface warming appears >60% faster, and a deep reshaping of the seascape is revealed.","A clustering of the trend patterns identifies the main factors that drive observed trends in chlorophyll-a concentration.","This data-driven approach opens new perspectives in empirical climate modelling."],"url":"http://arxiv.org/abs/2405.18981v1","category":"physics.ao-ph"}
{"created":"2024-05-29 10:42:36","title":"Comparing Lazy Constraint Selection Strategies in Train Routing with Moving Block Control","abstract":"Railroad transportation plays a vital role in the future of sustainable mobility. Besides building new infrastructure, capacity can be improved by modern train control systems, e.g., based on moving blocks. At the same time, there is only limited work on how to optimally route trains using the potential gained by these systems. Recently, an initial approach for train routing with moving block control has been proposed to address this demand. However, detailed evaluations on so-called lazy constraints are missing, and no publicly available implementation exists. In this work, we close this gap by providing an extended approach as well as a flexible open-source implementation that can use different solving strategies. Using that, we experimentally evaluate what choices should be made when implementing a lazy constraint approach. The corresponding implementation and benchmarks are publicly available as part of the Munich Train Control Toolkit (MTCT) at https://github.com/cda-tum/mtct.","sentences":["Railroad transportation plays a vital role in the future of sustainable mobility.","Besides building new infrastructure, capacity can be improved by modern train control systems, e.g., based on moving blocks.","At the same time, there is only limited work on how to optimally route trains using the potential gained by these systems.","Recently, an initial approach for train routing with moving block control has been proposed to address this demand.","However, detailed evaluations on so-called lazy constraints are missing, and no publicly available implementation exists.","In this work, we close this gap by providing an extended approach as well as a flexible open-source implementation that can use different solving strategies.","Using that, we experimentally evaluate what choices should be made when implementing a lazy constraint approach.","The corresponding implementation and benchmarks are publicly available as part of the Munich Train Control Toolkit (MTCT) at https://github.com/cda-tum/mtct."],"url":"http://arxiv.org/abs/2405.18977v1","category":"eess.SY"}
{"created":"2024-05-29 10:31:53","title":"Mitigate Position Bias with Coupled Ranking Bias on CTR Prediction","abstract":"Position bias, i.e., users' preference of an item is affected by its placing position, is well studied in the recommender system literature. However, most existing methods ignore the widely coupled ranking bias, which is also related to the placing position of the item. Using both synthetic and industrial datasets, we first show how this widely coexisted ranking bias deteriorates the performance of the existing position bias estimation methods. To mitigate the position bias with the presence of the ranking bias, we propose a novel position bias estimation method, namely gradient interpolation, which fuses two estimation methods using a fusing weight. We further propose an adaptive method to automatically determine the optimal fusing weight. Extensive experiments on both synthetic and industrial datasets demonstrate the superior performance of the proposed methods.","sentences":["Position bias, i.e., users' preference of an item is affected by its placing position, is well studied in the recommender system literature.","However, most existing methods ignore the widely coupled ranking bias, which is also related to the placing position of the item.","Using both synthetic and industrial datasets, we first show how this widely coexisted ranking bias deteriorates the performance of the existing position bias estimation methods.","To mitigate the position bias with the presence of the ranking bias, we propose a novel position bias estimation method, namely gradient interpolation, which fuses two estimation methods using a fusing weight.","We further propose an adaptive method to automatically determine the optimal fusing weight.","Extensive experiments on both synthetic and industrial datasets demonstrate the superior performance of the proposed methods."],"url":"http://arxiv.org/abs/2405.18971v1","category":"cs.IR"}
{"created":"2024-05-29 10:27:48","title":"Global and local observability of hypergraphs","abstract":"This paper studies observability for non-uniform hypergraphs with inputs and outputs. To capture higher-order interactions, we define a canonical non-homogeneous dynamical system with nonlinear outputs on hypergraphs. We then construct algebraic necessary and sufficient conditions based on polynomial ideals and varieties for global observability at an initial state of hypergraphs. An example is given to illustrate the proposed criteria for observability. Further, necessary and sufficient conditions for local observability are derived based on rank conditions of observability matrices, which provide a framework to study local observability for non-uniform hypergraphs. Finally, the similarity of observability for hypergraphs is proposed using similarity of tensors, which reveals the relation of observability between two hypergraphs and helps to check the observability intuitively.","sentences":["This paper studies observability for non-uniform hypergraphs with inputs and outputs.","To capture higher-order interactions, we define a canonical non-homogeneous dynamical system with nonlinear outputs on hypergraphs.","We then construct algebraic necessary and sufficient conditions based on polynomial ideals and varieties for global observability at an initial state of hypergraphs.","An example is given to illustrate the proposed criteria for observability.","Further, necessary and sufficient conditions for local observability are derived based on rank conditions of observability matrices, which provide a framework to study local observability for non-uniform hypergraphs.","Finally, the similarity of observability for hypergraphs is proposed using similarity of tensors, which reveals the relation of observability between two hypergraphs and helps to check the observability intuitively."],"url":"http://arxiv.org/abs/2405.18969v1","category":"eess.SY"}
{"created":"2024-05-29 10:23:52","title":"Beyond the fundamental lemma: from finite time series to linear system","abstract":"We state necessary and sufficient conditions to uniquely identify (modulo state isomorphism) a linear time-invariant minimal input-state-output system from finite input-output data and upper- and lower bounds on lag and state space dimension.","sentences":["We state necessary and sufficient conditions to uniquely identify (modulo state isomorphism) a linear time-invariant minimal input-state-output system from finite input-output data and upper- and lower bounds on lag and state space dimension."],"url":"http://arxiv.org/abs/2405.18962v1","category":"math.OC"}
{"created":"2024-05-29 10:21:10","title":"Zero-echo-time sequences in highly inhomogeneous fields","abstract":"Zero echo time (ZTE) sequences have proven a powerful tool for Magnetic Resonance Imaging (MRI) of ultrashort T2 tissues, but they fail to produce useful images in the presence of strong field inhomogeneities. Here we present a method to correct artifacts induced by strong B0 inhomogeneities in non-Cartesian sequences, based on magnetic field maps obtained from the phase difference between two fast pointwise acquisitions. These are free of geometric distortions and can be used for model based image reconstruction with iterative algebraic techniques. In this paper, we show that distortions and artifacts coming from inhomogeneites in B0 are largely reverted by our method, as opposed to standard Conjugate Phase reconstructions. We base this technique on a Single-Point Double-Shot (SPDS) approach, and we have shown it to work even for intra-voxel bandwidths (determined by B0 inhomogeneities) comparable to the encoding bandwidth (determined by the gradient fields). We benchmark the performance of SPDS for ZTE acquisitions, which can be exploited for e.g. dental imaging in affordable low-field MRI systems. Furthermore, SPDS can be used for arbitrary pulse sequences and it may prove useful for extreme magnet geometries, as in e.g. single-sided MRI.","sentences":["Zero echo time (ZTE) sequences have proven a powerful tool for Magnetic Resonance Imaging (MRI) of ultrashort T2 tissues, but they fail to produce useful images in the presence of strong field inhomogeneities.","Here we present a method to correct artifacts induced by strong B0 inhomogeneities in non-Cartesian sequences, based on magnetic field maps obtained from the phase difference between two fast pointwise acquisitions.","These are free of geometric distortions and can be used for model based image reconstruction with iterative algebraic techniques.","In this paper, we show that distortions and artifacts coming from inhomogeneites in B0 are largely reverted by our method, as opposed to standard Conjugate Phase reconstructions.","We base this technique on a Single-Point Double-Shot (SPDS) approach, and we have shown it to work even for intra-voxel bandwidths (determined by B0 inhomogeneities) comparable to the encoding bandwidth (determined by the gradient fields).","We benchmark the performance of SPDS for ZTE acquisitions, which can be exploited for e.g. dental imaging in affordable low-field MRI systems.","Furthermore, SPDS can be used for arbitrary pulse sequences and it may prove useful for extreme magnet geometries, as in e.g. single-sided MRI."],"url":"http://arxiv.org/abs/2405.18960v1","category":"physics.med-ph"}
{"created":"2024-05-29 10:15:36","title":"Pessimism of the Will, Optimism of the Intellect: Fair Protocols with Malicious but Rational Agents","abstract":"Fairness is a desirable and crucial property of many protocols that handle, for instance, exchanges of message.   It states that if at least one agent engaging in the protocol is honest, then either the protocol will unfold correctly and fulfill its intended goal for all participants, or it will fail for everyone.   In this work, we present a game-based framework for the study of fairness protocols, that does not define a priori an attacker model.   It is based on the notion of strong secure equilibria, and leverages the conceptual and algorithmic toolbox of game theory.   In the case of finite games, we provide decision procedures with tight complexity bounds for determining whether a protocol is immune to nefarious attacks from a coalition of participants, and whether such a protocol could exist based on the underlying graph structure and objectives.","sentences":["Fairness is a desirable and crucial property of many protocols that handle, for instance, exchanges of message.   ","It states that if at least one agent engaging in the protocol is honest, then either the protocol will unfold correctly and fulfill its intended goal for all participants, or it will fail for everyone.   ","In this work, we present a game-based framework for the study of fairness protocols, that does not define a priori an attacker model.   ","It is based on the notion of strong secure equilibria, and leverages the conceptual and algorithmic toolbox of game theory.   ","In the case of finite games, we provide decision procedures with tight complexity bounds for determining whether a protocol is immune to nefarious attacks from a coalition of participants, and whether such a protocol could exist based on the underlying graph structure and objectives."],"url":"http://arxiv.org/abs/2405.18958v1","category":"cs.GT"}
{"created":"2024-05-29 10:14:46","title":"Donnan equilibrium in charged slit-pores from a hybrid nonequilibrium Molecular Dynamics / Monte Carlo method with ions and solvent exchange","abstract":"Ion partitioning between different compartments (\\emph{e.g.} a porous material and a bulk solution reservoir), known as Donnan equilibrium, plays a fundamental role in various contexts such as energy, environment, or water treatment. The linearized Poisson-Boltzmann (PB) equation, capturing the thermal motion of the ions with mean-field electrostatic interactions, is practically useful to understand and predict ion partitioning, despite its limited applicability to conditions of low salt concentrations and surface charge densities. Here, we investigate the Donnan equilibrium of coarse-grained dilute electrolytes confined in charged slit-pores in equilibrium with a reservoir of ions and solvent. We introduce and use an extension to confined systems of a recently developed hybrid nonequilibrium molecular dynamics / grand canonical Monte Carlo simulation method (\"H4D\"), which enhances the efficiency of solvent and ion-pair exchange via a fourth spatial dimension. We show that the validity range of linearized PB theory to predict the Donnan equilibrium of dilute electrolytes can be extended to highly charged pores, by simply considering \\textit{renormalized} surface charge densities. We compare with simulations of implicit solvent models of electrolytes and show that in the low salt concentrations and thin electric double layer limit considered here, an explicit solvent has a limited effect on the Donnan equilibrium and that the main limitations of the analytical predictions are not due to the breakdown of the mean-field description, but rather to the charge renormalization approximation, because it only focuses on the behavior far from the surfaces.","sentences":["Ion partitioning between different compartments (\\emph{e.g.} a porous material and a bulk solution reservoir), known as Donnan equilibrium, plays a fundamental role in various contexts such as energy, environment, or water treatment.","The linearized Poisson-Boltzmann (PB) equation, capturing the thermal motion of the ions with mean-field electrostatic interactions, is practically useful to understand and predict ion partitioning, despite its limited applicability to conditions of low salt concentrations and surface charge densities.","Here, we investigate the Donnan equilibrium of coarse-grained dilute electrolytes confined in charged slit-pores in equilibrium with a reservoir of ions and solvent.","We introduce and use an extension to confined systems of a recently developed hybrid nonequilibrium molecular dynamics / grand canonical Monte Carlo simulation method (\"H4D\"), which enhances the efficiency of solvent and ion-pair exchange via a fourth spatial dimension.","We show that the validity range of linearized PB theory to predict the Donnan equilibrium of dilute electrolytes can be extended to highly charged pores, by simply considering \\textit{renormalized} surface charge densities.","We compare with simulations of implicit solvent models of electrolytes and show that in the low salt concentrations and thin electric double layer limit considered here, an explicit solvent has a limited effect on the Donnan equilibrium and that the main limitations of the analytical predictions are not due to the breakdown of the mean-field description, but rather to the charge renormalization approximation, because it only focuses on the behavior far from the surfaces."],"url":"http://arxiv.org/abs/2405.18957v1","category":"physics.chem-ph"}
{"created":"2024-05-29 10:11:10","title":"MAGIC: Modular Auto-encoder for Generalisable Model Inversion with Bias Corrections","abstract":"Scientists often model physical processes to understand the natural world and uncover the causation behind observations. Due to unavoidable simplification, discrepancies often arise between model predictions and actual observations, in the form of systematic biases, whose impact varies with model completeness. Classical model inversion methods such as Bayesian inference or regressive neural networks tend either to overlook biases or make assumptions about their nature during data preprocessing, potentially leading to implausible results. Inspired by recent work in inverse graphics, we replace the decoder stage of a standard autoencoder with a physical model followed by a bias-correction layer. This generalisable approach simultaneously inverts the model and corrects its biases in an end-to-end manner without making strong assumptions about the nature of the biases. We demonstrate the effectiveness of our approach using two physical models from disparate domains: a complex radiative transfer model from remote sensing; and a volcanic deformation model from geodesy. Our method matches or surpasses results from classical approaches without requiring biases to be explicitly filtered out, suggesting an effective pathway for understanding the causation of various physical processes.","sentences":["Scientists often model physical processes to understand the natural world and uncover the causation behind observations.","Due to unavoidable simplification, discrepancies often arise between model predictions and actual observations, in the form of systematic biases, whose impact varies with model completeness.","Classical model inversion methods such as Bayesian inference or regressive neural networks tend either to overlook biases or make assumptions about their nature during data preprocessing, potentially leading to implausible results.","Inspired by recent work in inverse graphics, we replace the decoder stage of a standard autoencoder with a physical model followed by a bias-correction layer.","This generalisable approach simultaneously inverts the model and corrects its biases in an end-to-end manner without making strong assumptions about the nature of the biases.","We demonstrate the effectiveness of our approach using two physical models from disparate domains: a complex radiative transfer model from remote sensing; and a volcanic deformation model from geodesy.","Our method matches or surpasses results from classical approaches without requiring biases to be explicitly filtered out, suggesting an effective pathway for understanding the causation of various physical processes."],"url":"http://arxiv.org/abs/2405.18953v1","category":"cs.LG"}
{"created":"2024-05-29 10:05:06","title":"The GAPS programme at TNG. LVII. TOI-5076b: A warm sub-Neptune planet orbiting a thin-to-thick-disk transition star in a wide binary system","abstract":"Aims. We report the confirmation of a new transiting exoplanet orbiting the star TOI-5076. Methods. We present our vetting procedure and follow-up observations which led to the confirmation of the exoplanet TOI-5076b. In particular, we employed high-precision {\\it TESS} photometry, high-angular-resolution imaging from several telescopes, and high-precision radial velocities from HARPS-N. Results. From the HARPS-N spectroscopy, we determined the spectroscopic parameters of the host star: T$\\rm_{eff}$=(5070$\\pm$143) K, log~g=(4.6$\\pm$0.3), [Fe/H]=(+0.20$\\pm$0.08), and [$\\alpha$/Fe]=0.05$\\pm$0.06. The transiting planet is a warm sub-Neptune with a mass m$\\rm_p=$(16$\\pm$2) M$\\rm_{\\oplus}$, a radius r$\\rm_p=$(3.2$\\pm$0.1)~R$\\rm_{\\oplus}$ yielding a density $\\rho_p$=(2.8$\\pm$0.5) g cm$^{-3}$. It revolves around its star approximately every 23.445 days. Conclusions. The host star is a metal-rich, K2V dwarf, located at about 82 pc from the Sun with a radius of R$_{\\star}$=(0.78$\\pm$0.01) R$_{\\odot}$ and a mass of M$_{\\star}$=(0.80$\\pm$0.07) M$_{\\odot}$. It forms a common proper motion pair with an M-dwarf companion star located at a projected separation of 2178 au. The chemical analysis of the host-star and the Galactic-space velocities indicate that TOI-5076 belongs to the old population of thin-to-thick-disk transition stars. The density of TOI-5076b suggests the presence of a large fraction by volume of volatiles overlying a massive core. We found that a circular orbit solution is marginally favored with respect to an eccentric orbit solution for TOI-5076b.","sentences":["Aims.","We report the confirmation of a new transiting exoplanet orbiting the star TOI-5076. Methods.","We present our vetting procedure and follow-up observations which led to the confirmation of the exoplanet TOI-5076b.","In particular, we employed high-precision {\\it TESS} photometry, high-angular-resolution imaging from several telescopes, and high-precision radial velocities from HARPS-N. Results.","From the HARPS-N spectroscopy, we determined the spectroscopic parameters of the host star: T$\\rm_{eff}$=(5070$\\pm$143) K, log~g=(4.6$\\pm$0.3), [Fe/H]=(+0.20$\\pm$0.08), and [$\\alpha$/Fe]=0.05$\\pm$0.06.","The transiting planet is a warm sub-Neptune with a mass m$\\rm_p=$(16$\\pm$2) M$\\rm_{\\oplus}$, a radius r$\\rm_p=$(3.2$\\pm$0.1)~R$\\rm_{\\oplus}$ yielding a density $\\rho_p$=(2.8$\\pm$0.5) g","cm$^{-3}$. It revolves around its star approximately every 23.445 days.","Conclusions.","The host star is a metal-rich, K2V dwarf, located at about 82 pc from the Sun with a radius of R$_{\\star}$=(0.78$\\pm$0.01) R$_{\\odot}$ and a mass of M$_{\\star}$=(0.80$\\pm$0.07)","M$_{\\odot}$.","It forms a common proper motion pair with an M-dwarf companion star located at a projected separation of 2178 au.","The chemical analysis of the host-star and the Galactic-space velocities indicate that TOI-5076 belongs to the old population of thin-to-thick-disk transition stars.","The density of TOI-5076b suggests the presence of a large fraction by volume of volatiles overlying a massive core.","We found that a circular orbit solution is marginally favored with respect to an eccentric orbit solution for TOI-5076b."],"url":"http://arxiv.org/abs/2405.18950v1","category":"astro-ph.EP"}
{"created":"2024-05-29 10:04:07","title":"Multipacting mitigation by atomic layer deposition: the case study of Titanium Nitride","abstract":"This study investigates the use of Atomic Layer deposition (ALD) to mitigate multipacting phenomena inside superconducting radio frequency (SRF) cavities used in particle accelerators. The unique ALD capability to control the film thickness down to the atomic level on arbitrary complex shape objects enable the fine tuning of TiN film resistivity and total electron emission yield (TEEY) from coupons to devices. This level of control allows us to adequately choose a TiN film thickness that provide both a high resistivity to prevent Ohmic losses and low TEEY to mitigate multipacting for the application of interest. The methodology presented in this work can be scaled to other domain and devices subject to RF fields in vacuum and sensitive to multipacting or electron discharge processes with their own requirements in resistivities and TEEY values","sentences":["This study investigates the use of Atomic Layer deposition (ALD) to mitigate multipacting phenomena inside superconducting radio frequency (SRF) cavities used in particle accelerators.","The unique ALD capability to control the film thickness down to the atomic level on arbitrary complex shape objects enable the fine tuning of TiN film resistivity and total electron emission yield (TEEY) from coupons to devices.","This level of control allows us to adequately choose a TiN film thickness that provide both a high resistivity to prevent Ohmic losses and low TEEY to mitigate multipacting for the application of interest.","The methodology presented in this work can be scaled to other domain and devices subject to RF fields in vacuum and sensitive to multipacting or electron discharge processes with their own requirements in resistivities and TEEY values"],"url":"http://arxiv.org/abs/2405.18949v1","category":"physics.app-ph"}
{"created":"2024-05-29 09:56:54","title":"WTTFNet: A Weather-Time-Trajectory Fusion Network for Pedestrian Trajectory Prediction in Urban Complex","abstract":"Pedestrian trajectory modelling in an urban complex is challenging because pedestrians can have many possible destinations, such as shops, escalators, and attractions. Moreover, weather and time-of-day may affect pedestrian behavior. In this paper, a new weather-time-trajectory fusion network (WTTFNet) is proposed to improve the performance of baseline deep neural network architecture. By incorporating weather and time-of-day information as an embedding structure, a novel WTTFNet based on gate multimodal unit is used to fuse the multimodal information and deep representation of trajectories. A joint loss function based on focal loss is used to co-optimize both the deep trajectory features and final classifier, which helps to improve the accuracy in predicting the intended destination of pedestrians and hence the trajectories under possible scenarios of class imbalances. Experimental results using the Osaka Asia and Pacific Trade Center (ATC) dataset shows improved performance of the proposed approach over state-of-the-art algorithms by 23.67% increase in classification accuracy, 9.16% and 7.07% reduction of average and final displacement error. The proposed approach may serve as an attractive approach for improving existing baseline trajectory prediction models when they are applied to scenarios with influences of weather-time conditions. It can be employed in numerous applications such as pedestrian facility engineering, public space development and technology-driven retail.","sentences":["Pedestrian trajectory modelling in an urban complex is challenging because pedestrians can have many possible destinations, such as shops, escalators, and attractions.","Moreover, weather and time-of-day may affect pedestrian behavior.","In this paper, a new weather-time-trajectory fusion network (WTTFNet) is proposed to improve the performance of baseline deep neural network architecture.","By incorporating weather and time-of-day information as an embedding structure, a novel WTTFNet based on gate multimodal unit is used to fuse the multimodal information and deep representation of trajectories.","A joint loss function based on focal loss is used to co-optimize both the deep trajectory features and final classifier, which helps to improve the accuracy in predicting the intended destination of pedestrians and hence the trajectories under possible scenarios of class imbalances.","Experimental results using the Osaka Asia and Pacific Trade Center (ATC) dataset shows improved performance of the proposed approach over state-of-the-art algorithms by 23.67% increase in classification accuracy, 9.16% and 7.07% reduction of average and final displacement error.","The proposed approach may serve as an attractive approach for improving existing baseline trajectory prediction models when they are applied to scenarios with influences of weather-time conditions.","It can be employed in numerous applications such as pedestrian facility engineering, public space development and technology-driven retail."],"url":"http://arxiv.org/abs/2405.18945v1","category":"cs.CV"}
{"created":"2024-05-29 09:46:44","title":"HLOB -- Information Persistence and Structure in Limit Order Books","abstract":"We introduce a novel large-scale deep learning model for Limit Order Book mid-price changes forecasting, and we name it `HLOB'. This architecture (i) exploits the information encoded by an Information Filtering Network, namely the Triangulated Maximally Filtered Graph, to unveil deeper and non-trivial dependency structures among volume levels; and (ii) guarantees deterministic design choices to handle the complexity of the underlying system by drawing inspiration from the groundbreaking class of Homological Convolutional Neural Networks. We test our model against 9 state-of-the-art deep learning alternatives on 3 real-world Limit Order Book datasets, each including 15 stocks traded on the NASDAQ exchange, and we systematically characterize the scenarios where HLOB outperforms state-of-the-art architectures. Our approach sheds new light on the spatial distribution of information in Limit Order Books and on its degradation over increasing prediction horizons, narrowing the gap between microstructural modeling and deep learning-based forecasting in high-frequency financial markets.","sentences":["We introduce a novel large-scale deep learning model for Limit Order Book mid-price changes forecasting, and we name it `HLOB'.","This architecture (i) exploits the information encoded by an Information Filtering Network, namely the Triangulated Maximally Filtered Graph, to unveil deeper and non-trivial dependency structures among volume levels; and (ii) guarantees deterministic design choices to handle the complexity of the underlying system by drawing inspiration from the groundbreaking class of Homological Convolutional Neural Networks.","We test our model against 9 state-of-the-art deep learning alternatives on 3 real-world Limit Order Book datasets, each including 15 stocks traded on the NASDAQ exchange, and we systematically characterize the scenarios where HLOB outperforms state-of-the-art architectures.","Our approach sheds new light on the spatial distribution of information in Limit Order Books and on its degradation over increasing prediction horizons, narrowing the gap between microstructural modeling and deep learning-based forecasting in high-frequency financial markets."],"url":"http://arxiv.org/abs/2405.18938v1","category":"q-fin.TR"}
{"created":"2024-05-29 09:31:55","title":"Chiral quantum heating and cooling with an optically controlled ion","abstract":"Quantum heat engines and refrigerators are open quantum systems, whose dynamics can be well understood using a non-Hermitian formalism. A prominent feature of non-Hermiticity is the existence of exceptional points (EPs), which has no counterpart in closed quantum systems. It has been shown in classical systems that dynamical encirclement in the vicinity of an EP, whether the loop includes the EP or not, could lead to chiral mode conversion. Here, we show that this is valid also for quantum systems when dynamical encircling is performed in the vicinity of their Liouvillian EPs (LEPs) which include the effects of quantum jumps and associated noise - an important quantum feature not present in previous works. We demonstrate, using a Paul-trapped ultracold ion, the first chiral quantum heating and refrigeration by dynamically encircling a closed loop in the vicinity of an LEP. We witness the cycling direction to be associated with the chirality and heat release (absorption) of the quantum heat engine (quantum refrigerator). Our experiments have revealed that not only the adiabaticity-breakdown but also the Landau-Zener-St\\\"uckelberg process play an essential role during dynamic encircling, resulting in chiral thermodynamic cycles. Our observations contributes to further understanding of chiral and topological features in non-Hermitian systems and pave a way to exploring the relation between chirality and quantum thermodynamics.","sentences":["Quantum heat engines and refrigerators are open quantum systems, whose dynamics can be well understood using a non-Hermitian formalism.","A prominent feature of non-Hermiticity is the existence of exceptional points (EPs), which has no counterpart in closed quantum systems.","It has been shown in classical systems that dynamical encirclement in the vicinity of an EP, whether the loop includes the EP or not, could lead to chiral mode conversion.","Here, we show that this is valid also for quantum systems when dynamical encircling is performed in the vicinity of their Liouvillian EPs (LEPs) which include the effects of quantum jumps and associated noise - an important quantum feature not present in previous works.","We demonstrate, using a Paul-trapped ultracold ion, the first chiral quantum heating and refrigeration by dynamically encircling a closed loop in the vicinity of an LEP.","We witness the cycling direction to be associated with the chirality and heat release (absorption) of the quantum heat engine (quantum refrigerator).","Our experiments have revealed that not only the adiabaticity-breakdown but also the Landau-Zener-St\\\"uckelberg process play an essential role during dynamic encircling, resulting in chiral thermodynamic cycles.","Our observations contributes to further understanding of chiral and topological features in non-Hermitian systems and pave a way to exploring the relation between chirality and quantum thermodynamics."],"url":"http://arxiv.org/abs/2405.18927v1","category":"quant-ph"}
{"created":"2024-05-29 09:31:45","title":"Damped Newton Method with Near-Optimal Global $\\mathcal {O}\\left(k^{-3} \\right)$ Convergence Rate","abstract":"This paper investigates the global convergence of stepsized Newton methods for convex functions. We propose several simple stepsize schedules with fast global convergence guarantees, up to $\\mathcal{O} (k^{-3})$, nearly matching lower complexity bounds $\\Omega (k^{-3.5})$ of second-order methods. For cases with multiple plausible smoothness parameterizations or an unknown smoothness constant, we introduce a stepsize backtracking procedure that ensures convergence as if the optimal smoothness parameters were known.","sentences":["This paper investigates the global convergence of stepsized Newton methods for convex functions.","We propose several simple stepsize schedules with fast global convergence guarantees, up to $\\mathcal{O} (k^{-3})$, nearly matching lower complexity bounds $\\Omega (k^{-3.5})$ of second-order methods.","For cases with multiple plausible smoothness parameterizations or an unknown smoothness constant, we introduce a stepsize backtracking procedure that ensures convergence as if the optimal smoothness parameters were known."],"url":"http://arxiv.org/abs/2405.18926v1","category":"math.OC"}
{"created":"2024-05-29 09:25:49","title":"Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective","abstract":"Neural Machine Translation (NMT) has made remarkable progress over the past years. However, under-translation and over-translation remain two challenging problems in state-of-the-art NMT systems. In this work, we conduct an in-depth analysis on the underlying cause of under-translation in NMT, providing an explanation from the perspective of decoding objective. To optimize the beam search objective, the model tends to overlook words it is less confident about, leading to the under-translation phenomenon. Correspondingly, the model's confidence in predicting the End Of Sentence (EOS) diminishes when under-translation occurs, serving as a mild penalty for under-translated candidates. Building upon this analysis, we propose employing the confidence of predicting EOS as a detector for under-translation, and strengthening the confidence-based penalty to penalize candidates with a high risk of under-translation. Experiments on both synthetic and real-world data show that our method can accurately detect and rectify under-translated outputs, with minor impact on other correct translations.","sentences":["Neural Machine Translation (NMT) has made remarkable progress over the past years.","However, under-translation and over-translation remain two challenging problems in state-of-the-art NMT systems.","In this work, we conduct an in-depth analysis on the underlying cause of under-translation in NMT, providing an explanation from the perspective of decoding objective.","To optimize the beam search objective, the model tends to overlook words it is less confident about, leading to the under-translation phenomenon.","Correspondingly, the model's confidence in predicting the End Of Sentence (EOS) diminishes when under-translation occurs, serving as a mild penalty for under-translated candidates.","Building upon this analysis, we propose employing the confidence of predicting EOS as a detector for under-translation, and strengthening the confidence-based penalty to penalize candidates with a high risk of under-translation.","Experiments on both synthetic and real-world data show that our method can accurately detect and rectify under-translated outputs, with minor impact on other correct translations."],"url":"http://arxiv.org/abs/2405.18922v1","category":"cs.CL"}
{"created":"2024-05-29 09:22:25","title":"Exploiting Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground Integrated Networks","abstract":"Space-air-ground integrated networks (SAGIN) are pivotal for achieving uninterrupted in-flight connectivity (IFC). Most existing studies, however, merely treat satellites as transparent forwarding nodes, and overlook their caching capabilities in enhancing the IFC data rate. In this paper, we consider an IFC-oriented SAGIN, where the satellites collaboratively deliver the content to airborne passengers to facilitate airborne communication. Considering the cached files instantaneously accessible via satellites, this work pioneers the integration of multiple inter-satellite links (ISLs) into the IFC framework, thereby innovating the content delivery process. To minimize the average delay of content delivery, we formulate an optimization problem and propose an exact penalty-based method to derive the satellite association scheme. Our proposed framework has a low complexity and thus paves the way for high-speed Internet connectivity to aviation passengers. Finally, simulation results are presented to demonstrate the effectiveness of our proposed IFC framework for SAGIN.","sentences":["Space-air-ground integrated networks (SAGIN) are pivotal for achieving uninterrupted in-flight connectivity (IFC).","Most existing studies, however, merely treat satellites as transparent forwarding nodes, and overlook their caching capabilities in enhancing the IFC data rate.","In this paper, we consider an IFC-oriented SAGIN, where the satellites collaboratively deliver the content to airborne passengers to facilitate airborne communication.","Considering the cached files instantaneously accessible via satellites, this work pioneers the integration of multiple inter-satellite links (ISLs) into the IFC framework, thereby innovating the content delivery process.","To minimize the average delay of content delivery, we formulate an optimization problem and propose an exact penalty-based method to derive the satellite association scheme.","Our proposed framework has a low complexity and thus paves the way for high-speed Internet connectivity to aviation passengers.","Finally, simulation results are presented to demonstrate the effectiveness of our proposed IFC framework for SAGIN."],"url":"http://arxiv.org/abs/2405.18919v1","category":"cs.IT"}
{"created":"2024-05-29 09:20:54","title":"Computing low-thrust transfers in the asteroid belt, a comparison between astrodynamical manipulations and a machine learning approach","abstract":"Low-thrust trajectories play a crucial role in optimizing scientific output and cost efficiency in asteroid belt missions. Unlike high-thrust transfers, low-thrust trajectories require solving complex optimal control problems. This complexity grows exponentially with the number of asteroids visited due to orbital mechanics intricacies. In the literature, methods for approximating low-thrust transfers without full optimization have been proposed, including analytical and machine learning techniques. In this work, we propose new analytical approximations and compare their accuracy and performance to machine learning methods. While analytical approximations leverage orbit theory to estimate trajectory costs, machine learning employs a more black-box approach, utilizing neural networks to predict optimal transfers based on various attributes. We build a dataset of about 3 million transfers, found by solving the time and fuel optimal control problems, for different time of flights, which we also release open-source. Comparison between the two methods on this database reveals the superiority of machine learning, especially for longer transfers. Despite challenges such as multi revolution transfers, both approaches maintain accuracy within a few percent in the final mass errors, on a database of trajectories involving numerous asteroids. This work contributes to the efficient exploration of mission opportunities in the asteroid belt, providing insights into the strengths and limitations of different approximation strategies.","sentences":["Low-thrust trajectories play a crucial role in optimizing scientific output and cost efficiency in asteroid belt missions.","Unlike high-thrust transfers, low-thrust trajectories require solving complex optimal control problems.","This complexity grows exponentially with the number of asteroids visited due to orbital mechanics intricacies.","In the literature, methods for approximating low-thrust transfers without full optimization have been proposed, including analytical and machine learning techniques.","In this work, we propose new analytical approximations and compare their accuracy and performance to machine learning methods.","While analytical approximations leverage orbit theory to estimate trajectory costs, machine learning employs a more black-box approach, utilizing neural networks to predict optimal transfers based on various attributes.","We build a dataset of about 3 million transfers, found by solving the time and fuel optimal control problems, for different time of flights, which we also release open-source.","Comparison between the two methods on this database reveals the superiority of machine learning, especially for longer transfers.","Despite challenges such as multi revolution transfers, both approaches maintain accuracy within a few percent in the final mass errors, on a database of trajectories involving numerous asteroids.","This work contributes to the efficient exploration of mission opportunities in the asteroid belt, providing insights into the strengths and limitations of different approximation strategies."],"url":"http://arxiv.org/abs/2405.18918v1","category":"astro-ph.EP"}
{"created":"2024-05-29 09:11:19","title":"Evaluating the efectiveness of sonifcation in science education using Edukoi","abstract":"Science, Technology, Engineering, and Mathematics classes are mainly taught using visual supports. However, the advancement of technology and the increasing eforts to equip schools with digital instrumentation have opened up the possibility of exploring new teaching avenues, such as sonifcation. We explored the efcacy of sonifcation in education using a novel interactive tool, Edukoi, in the context of astronomy, which is predominantly disseminated through spectacular images, animations, and visuals. Edukoi is a motion-sensing sonifcation tool that converts images to sound in real-time for educational applications. Our study, conducted with nearly 150 middle-school students, included a preliminary questionnaire investigating the perception, engagement, and motivation of students towards science; two sessions dedicated to testing Edukoi and assessing the potentiality of the software for the recognition of the colour and the shape of real and sketchy images; and a fnal second administration of the questionnaire to capture a possible benefcial efect of the use of the tool in the engagement towards science. Results showed the efectiveness of Edukoi in colour recognition and reasonable efcacy in shape identifcation. Although the questionnaire did not reveal an increment in science engagement over the time of the study, oral feedback from the students was positive. Edukoi presents a possible alternative teaching aid, potentially benefting diverse learners, including the visually impaired. Further developments of the software are needed to enhance its efectiveness in conveying more complex features such as composite colours or shapes.","sentences":["Science, Technology, Engineering, and Mathematics classes are mainly taught using visual supports.","However, the advancement of technology and the increasing eforts to equip schools with digital instrumentation have opened up the possibility of exploring new teaching avenues, such as sonifcation.","We explored the efcacy of sonifcation in education using a novel interactive tool, Edukoi, in the context of astronomy, which is predominantly disseminated through spectacular images, animations, and visuals.","Edukoi is a motion-sensing sonifcation tool that converts images to sound in real-time for educational applications.","Our study, conducted with nearly 150 middle-school students, included a preliminary questionnaire investigating the perception, engagement, and motivation of students towards science; two sessions dedicated to testing Edukoi and assessing the potentiality of the software for the recognition of the colour and the shape of real and sketchy images; and a fnal second administration of the questionnaire to capture a possible benefcial efect of the use of the tool in the engagement towards science.","Results showed the efectiveness of Edukoi in colour recognition and reasonable efcacy in shape identifcation.","Although the questionnaire did not reveal an increment in science engagement over the time of the study, oral feedback from the students was positive.","Edukoi presents a possible alternative teaching aid, potentially benefting diverse learners, including the visually impaired.","Further developments of the software are needed to enhance its efectiveness in conveying more complex features such as composite colours or shapes."],"url":"http://arxiv.org/abs/2405.18908v1","category":"physics.ed-ph"}
{"created":"2024-05-29 09:07:34","title":"$S$-packing colorings of distance graphs with distance sets of cardinality $2$","abstract":"For a non-decreasing sequence $S=(s_1,s_2,\\ldots)$ of positive integers, a partition of the vertex set of a graph $G$ into subsets $X_1,\\ldots, X_\\ell$, such that vertices in $X_i$ are pairwise at distance greater than $s_i$ for every $i\\in\\{1,\\ldots,\\ell\\}$, is called an $S$-packing $\\ell$-coloring of $G$. The minimum $\\ell$ for which $G$ admits an $S$-packing $\\ell$-coloring is called the $S$-packing chromatic number of $G$, denoted by $\\chi_S(G)$. In this paper, we consider $S$-packing colorings of distance graphs $G(\\mathbb{Z},\\{k,t\\})$, where $k$ and $t$ are positive integers, which are the graphs whose vertex set is $\\mathbb{Z}$, and two vertices $x,y\\in \\mathbb{Z}$ are adjacent whenever $|x-y|\\in\\{k,t\\}$. We complement partial results from two earlier papers, thus determining all values of $\\chi_S(G(\\mathbb{Z},\\{k,t\\}))$ when $S$ is any sequence with $s_i\\le 2$ for all $i$. In particular, if $S=(1,1,2,2,\\ldots)$, then the $S$-packing chromatic number is $2$ if $k+t$ is even, and $4$ otherwise, while if $S=(1,2,2,\\ldots)$, then the $S$-packing chromatic number is $5$, unless $\\{k,t\\}=\\{2,3\\}$ when it is $6$; when $S=(2,2,2,\\ldots)$, the corresponding formula is more complex.","sentences":["For a non-decreasing sequence $S=(s_1,s_2,\\ldots)$ of positive integers, a partition of the vertex set of a graph $G$ into subsets $X_1,\\ldots, X_\\ell$, such that vertices in $X_i$ are pairwise at distance greater than $s_i$ for every $i\\in\\{1,\\ldots,\\ell\\}$, is called an $S$-packing $\\ell$-coloring of $G$. The minimum $\\ell$ for which $G$ admits an $S$-packing $\\ell$-coloring is called the $S$-packing chromatic number of $G$, denoted by $\\chi_S(G)$. In this paper, we consider $S$-packing colorings of distance graphs $G(\\mathbb{Z},\\{k,t\\})$, where $k$ and $t$ are positive integers, which are the graphs whose vertex set is $\\mathbb{Z}$, and two vertices $x,y\\in \\mathbb{Z}$ are adjacent whenever $|x-y|\\in\\{k,t\\}$. We complement partial results from two earlier papers, thus determining all values of $\\chi_S(G(\\mathbb{Z},\\{k,t\\}))$ when $S$ is any sequence with $s_i\\le 2$ for all $i$. In particular, if $S=(1,1,2,2,\\ldots)$, then the $S$-packing chromatic number is $2$ if $k+t$ is even, and $4$ otherwise, while if $S=(1,2,2,\\ldots)$, then the $S$-packing chromatic number is $5$, unless $\\{k,t\\}=\\{2,3\\}$ when it is $6$; when $S=(2,2,2,\\ldots)$, the corresponding formula is more complex."],"url":"http://arxiv.org/abs/2405.18904v1","category":"math.CO"}
{"created":"2024-05-29 08:42:49","title":"4Doodle: Two-handed Gestures for Immersive Sketching of Architectural Models","abstract":"Three-dimensional immersive sketching for content creation and modeling has been studied for some time. However, research in this domain mainly focused on CAVE-like scenarios. These setups can be expensive and offer a narrow interaction space. Building more affordable setups using head-mounted displays is possible, allowing greater immersion and a larger space for user physical movements. This paper presents a fully immersive environment using bi-manual gestures to sketch and create content freely in the virtual world. This approach can be applied to many scenarios, allowing people to express their ideas or review existing designs. To cope with known motor difficulties and inaccuracy of freehand 3D sketching, we explore proxy geometry and a laser-like metaphor to draw content directly from models and create content surfaces. Our current prototype offers 24 cubic meters for movement, limited by the room size. It features infinite virtual drawing space through pan and scale techniques and is larger than the typical 6-sided cave at a fraction of the cost. In a preliminary study conducted with architects and engineers, our system showed a clear promise as a tool for sketching and 3D content creation in virtual reality with a great emphasis on bi-manual gestures.","sentences":["Three-dimensional immersive sketching for content creation and modeling has been studied for some time.","However, research in this domain mainly focused on CAVE-like scenarios.","These setups can be expensive and offer a narrow interaction space.","Building more affordable setups using head-mounted displays is possible, allowing greater immersion and a larger space for user physical movements.","This paper presents a fully immersive environment using bi-manual gestures to sketch and create content freely in the virtual world.","This approach can be applied to many scenarios, allowing people to express their ideas or review existing designs.","To cope with known motor difficulties and inaccuracy of freehand 3D sketching, we explore proxy geometry and a laser-like metaphor to draw content directly from models and create content surfaces.","Our current prototype offers 24 cubic meters for movement, limited by the room size.","It features infinite virtual drawing space through pan and scale techniques and is larger than the typical 6-sided cave at a fraction of the cost.","In a preliminary study conducted with architects and engineers, our system showed a clear promise as a tool for sketching and 3D content creation in virtual reality with a great emphasis on bi-manual gestures."],"url":"http://arxiv.org/abs/2405.18887v1","category":"cs.HC"}
{"created":"2024-05-29 08:40:11","title":"DecomCAM: Advancing Beyond Saliency Maps through Decomposition and Integration","abstract":"Interpreting complex deep networks, notably pre-trained vision-language models (VLMs), is a formidable challenge. Current Class Activation Map (CAM) methods highlight regions revealing the model's decision-making basis but lack clear saliency maps and detailed interpretability. To bridge this gap, we propose DecomCAM, a novel decomposition-and-integration method that distills shared patterns from channel activation maps. Utilizing singular value decomposition, DecomCAM decomposes class-discriminative activation maps into orthogonal sub-saliency maps (OSSMs), which are then integrated together based on their contribution to the target concept. Extensive experiments on six benchmarks reveal that DecomCAM not only excels in locating accuracy but also achieves an optimizing balance between interpretability and computational efficiency. Further analysis unveils that OSSMs correlate with discernible object components, facilitating a granular understanding of the model's reasoning. This positions DecomCAM as a potential tool for fine-grained interpretation of advanced deep learning models. The code is avaible at https://github.com/CapricornGuang/DecomCAM.","sentences":["Interpreting complex deep networks, notably pre-trained vision-language models (VLMs), is a formidable challenge.","Current Class Activation Map (CAM) methods highlight regions revealing the model's decision-making basis but lack clear saliency maps and detailed interpretability.","To bridge this gap, we propose DecomCAM, a novel decomposition-and-integration method that distills shared patterns from channel activation maps.","Utilizing singular value decomposition, DecomCAM decomposes class-discriminative activation maps into orthogonal sub-saliency maps (OSSMs), which are then integrated together based on their contribution to the target concept.","Extensive experiments on six benchmarks reveal that DecomCAM not only excels in locating accuracy but also achieves an optimizing balance between interpretability and computational efficiency.","Further analysis unveils that OSSMs correlate with discernible object components, facilitating a granular understanding of the model's reasoning.","This positions DecomCAM as a potential tool for fine-grained interpretation of advanced deep learning models.","The code is avaible at https://github.com/CapricornGuang/DecomCAM."],"url":"http://arxiv.org/abs/2405.18882v1","category":"cs.CV"}
{"created":"2024-05-29 08:32:37","title":"Are queries and keys always relevant? A case study on Transformer wave functions","abstract":"The dot product attention mechanism, originally designed for natural language processing (NLP) tasks, is a cornerstone of modern Transformers. It adeptly captures semantic relationships between word pairs in sentences by computing a similarity overlap between queries and keys. In this work, we explore the suitability of Transformers, focusing on their attention mechanisms, in the specific domain of the parametrization of variational wave functions to approximate ground states of quantum many-body spin Hamiltonians. Specifically, we perform numerical simulations on the two-dimensional $J_1$-$J_2$ Heisenberg model, a common benchmark in the field of quantum-many body systems on lattice. By comparing the performance of standard attention mechanisms with a simplified version that excludes queries and keys, relying solely on positions, we achieve competitive results while reducing computational cost and parameter usage. Furthermore, through the analysis of the attention maps generated by standard attention mechanisms, we show that the attention weights become effectively input-independent at the end of the optimization. We support the numerical results with analytical calculations, providing physical insights of why queries and keys should be, in principle, omitted from the attention mechanism when studying large systems. Interestingly, the same arguments can be extended to the NLP domain, in the limit of long input sentences.","sentences":["The dot product attention mechanism, originally designed for natural language processing (NLP) tasks, is a cornerstone of modern Transformers.","It adeptly captures semantic relationships between word pairs in sentences by computing a similarity overlap between queries and keys.","In this work, we explore the suitability of Transformers, focusing on their attention mechanisms, in the specific domain of the parametrization of variational wave functions to approximate ground states of quantum many-body spin Hamiltonians.","Specifically, we perform numerical simulations on the two-dimensional $J_1$-$J_2$ Heisenberg model, a common benchmark in the field of quantum-many body systems on lattice.","By comparing the performance of standard attention mechanisms with a simplified version that excludes queries and keys, relying solely on positions, we achieve competitive results while reducing computational cost and parameter usage.","Furthermore, through the analysis of the attention maps generated by standard attention mechanisms, we show that the attention weights become effectively input-independent at the end of the optimization.","We support the numerical results with analytical calculations, providing physical insights of why queries and keys should be, in principle, omitted from the attention mechanism when studying large systems.","Interestingly, the same arguments can be extended to the NLP domain, in the limit of long input sentences."],"url":"http://arxiv.org/abs/2405.18874v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-29 08:32:28","title":"A Return to Biased Nets: New Specifications and Approximate Bayesian Inference","abstract":"The biased net paradigm was the first general and empirically tractable scheme for parameterizing complex patterns of dependence in networks, expressing deviations from uniform random graph structure in terms of latent ``bias events,'' whose realizations enhance reciprocity, transitivity, or other structural features. Subsequent developments have introduced local specifications of biased nets, which reduce the need for approximations required in early specifications based on tracing processes. Here, we show that while one such specification leads to inconsistencies, a closely related Markovian specification both evades these difficulties and can be extended to incorporate new types of effects. We introduce the notion of inhibitory bias events, with satiation as an example, which are useful for avoiding degeneracies that can arise from closure bias terms. Although our approach does not lead to a computable likelihood, we provide a strategy for approximate Bayesian inference using random forest prevision. We demonstrate our approach on a network of friendship ties among college students, recapitulating a relationship between the sibling bias and tie strength posited in earlier work by Fararo.","sentences":["The biased net paradigm was the first general and empirically tractable scheme for parameterizing complex patterns of dependence in networks, expressing deviations from uniform random graph structure in terms of latent ``bias events,'' whose realizations enhance reciprocity, transitivity, or other structural features.","Subsequent developments have introduced local specifications of biased nets, which reduce the need for approximations required in early specifications based on tracing processes.","Here, we show that while one such specification leads to inconsistencies, a closely related Markovian specification both evades these difficulties and can be extended to incorporate new types of effects.","We introduce the notion of inhibitory bias events, with satiation as an example, which are useful for avoiding degeneracies that can arise from closure bias terms.","Although our approach does not lead to a computable likelihood, we provide a strategy for approximate Bayesian inference using random forest prevision.","We demonstrate our approach on a network of friendship ties among college students, recapitulating a relationship between the sibling bias and tie strength posited in earlier work by Fararo."],"url":"http://arxiv.org/abs/2405.18873v1","category":"stat.ME"}
{"created":"2024-05-29 08:32:06","title":"New Approaches to Old Problems? Thinking About a New Design of the AML/CFT Strategy","abstract":"The entry of new technological infrastructures into the financial markets poses serious concerns about the misuse of the economic system for illicit purposes, such as money laundering and financing of terrorism. Although there are cases in which this connection has already been discovered by malicious actors, distributed ledger technologies can nevertheless represent a powerful tool at the disposal of competent authorities to trace illicit flows and to better monitor risks in financial markets. However, this possibility may go through an interdisciplinary analysis of the phenomena. The search for alternative systems to move funds, rather than the traditional financial intermediaries, such as banks, is not a new circumstance and not necessarily for criminal purposes. Nevertheless, some of the already-known value transfer systems may benefit from the use of distributed ledger technology and make their detection more difficult. The European institutions are discussing the needed legislative packages to enforce the current regulations and to extend their application to the crypto space, as well as the establishment of a new competent authority.","sentences":["The entry of new technological infrastructures into the financial markets poses serious concerns about the misuse of the economic system for illicit purposes, such as money laundering and financing of terrorism.","Although there are cases in which this connection has already been discovered by malicious actors, distributed ledger technologies can nevertheless represent a powerful tool at the disposal of competent authorities to trace illicit flows and to better monitor risks in financial markets.","However, this possibility may go through an interdisciplinary analysis of the phenomena.","The search for alternative systems to move funds, rather than the traditional financial intermediaries, such as banks, is not a new circumstance and not necessarily for criminal purposes.","Nevertheless, some of the already-known value transfer systems may benefit from the use of distributed ledger technology and make their detection more difficult.","The European institutions are discussing the needed legislative packages to enforce the current regulations and to extend their application to the crypto space, as well as the establishment of a new competent authority."],"url":"http://arxiv.org/abs/2405.18517v1","category":"econ.TH"}
{"created":"2024-05-29 08:27:59","title":"The Structural Complexity Landscape of Finding Balance-Fair Shortest Paths","abstract":"We study the parameterized complexity of finding shortest s-t-paths with an additional fairness requirement. The task is to compute a shortest path in a vertex-colored graph where each color appears (roughly) equally often in the solution. We provide a complete picture of the parameterized complexity landscape of the problem with respect to structural parameters by showing a tetrachotomy including polynomial kernels, fixed-parameter tractability, XP-time algorithms (and W[1]-hardness), and para-NP-hardness.","sentences":["We study the parameterized complexity of finding shortest s-t-paths with an additional fairness requirement.","The task is to compute a shortest path in a vertex-colored graph where each color appears (roughly) equally often in the solution.","We provide a complete picture of the parameterized complexity landscape of the problem with respect to structural parameters by showing a tetrachotomy including polynomial kernels, fixed-parameter tractability, XP-time algorithms (and W[1]-hardness), and para-NP-hardness."],"url":"http://arxiv.org/abs/2405.18866v1","category":"cs.DS"}
{"created":"2024-05-29 08:25:18","title":"Screening and antiscreening effects in endohedral nanotubes","abstract":"Recently we investigated from first principles screening properties in systems where small molecules, characterized by a finite electronic dipole moment, are encapsulated into different nanocages. The most relevant result was the observation of an antiscreening effect in alkali-halide nanocages characterized by ionic bonds. Here we extend the study to another class of nanostructures, the nanotubes. Using first-principles techniques based on the Density Functional Theory, we studied the properties of endohedral nanotubes obtained by encapsulation of a water molecule or a linear HF molecule. A detailed analysis of the effective dipole moment of the complexes and of the electronic charge distribution suggests that screening effects crucially depend not only on the nature of the intramolecular bonds but also on the size and the shape of the nanotubes, and on the specific encapsulated molecule. As observed in endohedral nanocages, screening is maximum in covalent-bond carbon nanotubes, while it is reduced in partially-ionic nanotubes and an antiscreening effect is observed in some ionic nanotubes. However in this case the scenario is more complex than in corresponding ionic nanocages. In fact the specific geometric structure of alkali-halide nanotubes turns out to be crucial for determining the screening/antiscreening behavior: while nanotubes with octagonal transversal section can exhibit an antiscreening effect, which quantitatively depends on the number of layers in the longitudinal direction, instead nanotubes with dodecagonal section are always characterized by a reduction of the total dipole moment, so that a screening behavior is observed. Our results therefore show that, even in nanotube structures, in principle one can tune the dipole moment and generate electrostatic fields at the nanoscale without the aid of external potentials.","sentences":["Recently we investigated from first principles screening properties in systems where small molecules, characterized by a finite electronic dipole moment, are encapsulated into different nanocages.","The most relevant result was the observation of an antiscreening effect in alkali-halide nanocages characterized by ionic bonds.","Here we extend the study to another class of nanostructures, the nanotubes.","Using first-principles techniques based on the Density Functional Theory, we studied the properties of endohedral nanotubes obtained by encapsulation of a water molecule or a linear HF molecule.","A detailed analysis of the effective dipole moment of the complexes and of the electronic charge distribution suggests that screening effects crucially depend not only on the nature of the intramolecular bonds but also on the size and the shape of the nanotubes, and on the specific encapsulated molecule.","As observed in endohedral nanocages, screening is maximum in covalent-bond carbon nanotubes, while it is reduced in partially-ionic nanotubes and an antiscreening effect is observed in some ionic nanotubes.","However in this case the scenario is more complex than in corresponding ionic nanocages.","In fact the specific geometric structure of alkali-halide nanotubes turns out to be crucial for determining the screening/antiscreening behavior: while nanotubes with octagonal transversal section can exhibit an antiscreening effect, which quantitatively depends on the number of layers in the longitudinal direction, instead nanotubes with dodecagonal section are always characterized by a reduction of the total dipole moment, so that a screening behavior is observed.","Our results therefore show that, even in nanotube structures, in principle one can tune the dipole moment and generate electrostatic fields at the nanoscale without the aid of external potentials."],"url":"http://arxiv.org/abs/2405.18864v1","category":"physics.chem-ph"}
{"created":"2024-05-29 08:12:51","title":"SSGA-Net: Stepwise Spatial Global-local Aggregation Networks for for Autonomous Driving","abstract":"Visual-based perception is the key module for autonomous driving. Among those visual perception tasks, video object detection is a primary yet challenging one because of feature degradation caused by fast motion or multiple poses. Current models usually aggregate features from the neighboring frames to enhance the object representations for the task heads to generate more accurate predictions. Though getting better performance, these methods rely on the information from the future frames and suffer from high computational complexity. Meanwhile, the aggregation process is not reconfigurable during the inference time. These issues make most of the existing models infeasible for online applications. To solve these problems, we introduce a stepwise spatial global-local aggregation network. Our proposed models mainly contain three parts: 1). Multi-stage stepwise network gradually refines the predictions and object representations from the previous stage; 2). Spatial global-local aggregation fuses the local information from the neighboring frames and global semantics from the current frame to eliminate the feature degradation; 3). Dynamic aggregation strategy stops the aggregation process early based on the refinement results to remove redundancy and improve efficiency. Extensive experiments on the ImageNet VID benchmark validate the effectiveness and efficiency of our proposed models.","sentences":["Visual-based perception is the key module for autonomous driving.","Among those visual perception tasks, video object detection is a primary yet challenging one because of feature degradation caused by fast motion or multiple poses.","Current models usually aggregate features from the neighboring frames to enhance the object representations for the task heads to generate more accurate predictions.","Though getting better performance, these methods rely on the information from the future frames and suffer from high computational complexity.","Meanwhile, the aggregation process is not reconfigurable during the inference time.","These issues make most of the existing models infeasible for online applications.","To solve these problems, we introduce a stepwise spatial global-local aggregation network.","Our proposed models mainly contain three parts: 1).","Multi-stage stepwise network gradually refines the predictions and object representations from the previous stage; 2).","Spatial global-local aggregation fuses the local information from the neighboring frames and global semantics from the current frame to eliminate the feature degradation; 3).","Dynamic aggregation strategy stops the aggregation process early based on the refinement results to remove redundancy and improve efficiency.","Extensive experiments on the ImageNet VID benchmark validate the effectiveness and efficiency of our proposed models."],"url":"http://arxiv.org/abs/2405.18857v1","category":"cs.CV"}
{"created":"2024-05-29 08:03:52","title":"Supervised Contrastive Learning for Snapshot Spectral Imaging Face Anti-Spoofing","abstract":"This study reveals a cutting-edge re-balanced contrastive learning strategy aimed at strengthening face anti-spoofing capabilities within facial recognition systems, with a focus on countering the challenges posed by printed photos, and highly realistic silicone or latex masks. Leveraging the HySpeFAS dataset, which benefits from Snapshot Spectral Imaging technology to provide hyperspectral images, our approach harmonizes class-level contrastive learning with data resampling and an innovative real-face oriented reweighting technique. This method effectively mitigates dataset imbalances and reduces identity-related biases. Notably, our strategy achieved an unprecedented 0.0000\\% Average Classification Error Rate (ACER) on the HySpeFAS dataset, ranking first at the Chalearn Snapshot Spectral Imaging Face Anti-spoofing Challenge on CVPR 2024.","sentences":["This study reveals a cutting-edge re-balanced contrastive learning strategy aimed at strengthening face anti-spoofing capabilities within facial recognition systems, with a focus on countering the challenges posed by printed photos, and highly realistic silicone or latex masks.","Leveraging the HySpeFAS dataset, which benefits from Snapshot Spectral Imaging technology to provide hyperspectral images, our approach harmonizes class-level contrastive learning with data resampling and an innovative real-face oriented reweighting technique.","This method effectively mitigates dataset imbalances and reduces identity-related biases.","Notably, our strategy achieved an unprecedented 0.0000\\% Average Classification Error Rate (ACER) on the HySpeFAS dataset, ranking first at the Chalearn Snapshot Spectral Imaging Face Anti-spoofing Challenge on CVPR 2024."],"url":"http://arxiv.org/abs/2405.18853v1","category":"cs.CV"}
{"created":"2024-05-29 08:00:15","title":"SFANet: Spatial-Frequency Attention Network for Weather Forecasting","abstract":"Weather forecasting plays a critical role in various sectors, driving decision-making and risk management. However, traditional methods often struggle to capture the complex dynamics of meteorological systems, particularly in the presence of high-resolution data. In this paper, we propose the Spatial-Frequency Attention Network (SFANet), a novel deep learning framework designed to address these challenges and enhance the accuracy of spatiotemporal weather prediction. Drawing inspiration from the limitations of existing methodologies, we present an innovative approach that seamlessly integrates advanced token mixing and attention mechanisms. By leveraging both pooling and spatial mixing strategies, SFANet optimizes the processing of high-dimensional spatiotemporal sequences, preserving inter-component relational information and modeling extensive long-range relationships. To further enhance feature integration, we introduce a novel spatial-frequency attention module, enabling the model to capture intricate cross-modal correlations. Our extensive experimental evaluation on two distinct datasets, the Storm EVent ImageRy (SEVIR) and the Institute for Climate and Application Research (ICAR) - El Ni\\~{n}o Southern Oscillation (ENSO) dataset, demonstrates the remarkable performance of SFANet. Notably, SFANet achieves substantial advancements over state-of-the-art methods, showcasing its proficiency in forecasting precipitation patterns and predicting El Ni\\~{n}o events.","sentences":["Weather forecasting plays a critical role in various sectors, driving decision-making and risk management.","However, traditional methods often struggle to capture the complex dynamics of meteorological systems, particularly in the presence of high-resolution data.","In this paper, we propose the Spatial-Frequency Attention Network (SFANet), a novel deep learning framework designed to address these challenges and enhance the accuracy of spatiotemporal weather prediction.","Drawing inspiration from the limitations of existing methodologies, we present an innovative approach that seamlessly integrates advanced token mixing and attention mechanisms.","By leveraging both pooling and spatial mixing strategies, SFANet optimizes the processing of high-dimensional spatiotemporal sequences, preserving inter-component relational information and modeling extensive long-range relationships.","To further enhance feature integration, we introduce a novel spatial-frequency attention module, enabling the model to capture intricate cross-modal correlations.","Our extensive experimental evaluation on two distinct datasets, the Storm EVent ImageRy (SEVIR) and the Institute for Climate and Application Research (ICAR) - El Ni\\~{n}o Southern Oscillation (ENSO) dataset, demonstrates the remarkable performance of SFANet.","Notably, SFANet achieves substantial advancements over state-of-the-art methods, showcasing its proficiency in forecasting precipitation patterns and predicting El Ni\\~{n}o events."],"url":"http://arxiv.org/abs/2405.18849v1","category":"cs.CV"}
{"created":"2024-05-29 07:48:43","title":"Managing Human Factors in Automated Vehicle Development: Towards Challenges and Practices","abstract":"Due to the technical complexity and social impact, automated vehicle (AV) development challenges the current state of automotive engineering practice. Research shows that it is important to consider human factors (HF) knowledge when developing AVs to make them safe and accepted. This study explores the current practices and challenges of the automotive industries for incorporating HF requirements during agile AV development. We interviewed ten industry professionals from several Swedish automotive companies, including HF experts and AV engineers. Based on our qualitative analysis of the semi-structured interviews, a number of current approaches for communicating and incorporating HF knowledge into agile AV development and associated challenges are discussed. Our findings may help to focus future research on issues that are critical to effectively incorporate HF knowledge into agile AV development.","sentences":["Due to the technical complexity and social impact, automated vehicle (AV) development challenges the current state of automotive engineering practice.","Research shows that it is important to consider human factors (HF) knowledge when developing AVs to make them safe and accepted.","This study explores the current practices and challenges of the automotive industries for incorporating HF requirements during agile AV development.","We interviewed ten industry professionals from several Swedish automotive companies, including HF experts and AV engineers.","Based on our qualitative analysis of the semi-structured interviews, a number of current approaches for communicating and incorporating HF knowledge into agile AV development and associated challenges are discussed.","Our findings may help to focus future research on issues that are critical to effectively incorporate HF knowledge into agile AV development."],"url":"http://arxiv.org/abs/2405.18841v1","category":"cs.SE"}
{"created":"2024-05-29 07:37:57","title":"Requirements Strategy for Managing Human Factors in Automated Vehicle Development","abstract":"The integration of human factors (HF) knowledge is crucial when developing safety-critical systems, such as automated vehicles (AVs). Ensuring that HF knowledge is considered continuously throughout the AV development process is essential for several reasons, including efficacy, safety, and acceptance of these advanced systems. However, it is challenging to include HF as requirements in agile development. Recently, Requirements Strategies have been suggested to address requirements engineering challenges in agile development. By applying the concept of Requirements Strategies as a lens to the investigation of HF requirements in agile development of AVs, this paper arrives at three areas for investigation: a) ownership and responsibility for HF requirements, b) structure of HF requirements and information models, and c) definition of work and feature flows related to HF requirements. Based on 13 semi-structured interviews with professionals from the global automotive industry, we provide qualitative insights in these three areas. The diverse perspectives and experiences shared by the interviewees provide insightful views and helped to reason about the potential solution spaces in each area for integrating HF within the industry, highlighting the real-world practices and strategies used.","sentences":["The integration of human factors (HF) knowledge is crucial when developing safety-critical systems, such as automated vehicles (AVs).","Ensuring that HF knowledge is considered continuously throughout the AV development process is essential for several reasons, including efficacy, safety, and acceptance of these advanced systems.","However, it is challenging to include HF as requirements in agile development.","Recently, Requirements Strategies have been suggested to address requirements engineering challenges in agile development.","By applying the concept of Requirements Strategies as a lens to the investigation of HF requirements in agile development of AVs, this paper arrives at three areas for investigation: a) ownership and responsibility for HF requirements, b) structure of HF requirements and information models, and c) definition of work and feature flows related to HF requirements.","Based on 13 semi-structured interviews with professionals from the global automotive industry, we provide qualitative insights in these three areas.","The diverse perspectives and experiences shared by the interviewees provide insightful views and helped to reason about the potential solution spaces in each area for integrating HF within the industry, highlighting the real-world practices and strategies used."],"url":"http://arxiv.org/abs/2405.18838v1","category":"cs.SE"}
{"created":"2024-05-29 07:24:23","title":"Geometric Bipartite Matching is in NC","abstract":"In this work, we study the parallel complexity of the Euclidean minimum-weight perfect matching (EWPM) problem. Here our graph is the complete bipartite graph $G$ on two sets of points $A$ and $B$ in $\\mathbb{R}^2$ and the weight of each edge is the Euclidean distance between the corresponding points. The weighted perfect matching problem on general bipartite graphs is known to be in RNC [Mulmuley, Vazirani, and Vazirani, 1987], and Quasi-NC [Fenner, Gurjar, and Thierauf, 2016]. Both of these results work only when the weights are of $O(\\log n)$ bits. It is a long-standing open question to show the problem to be in NC.   First, we show that for EWPM, a linear number of bits of approximation is required to distinguish between the minimum-weight perfect matching and other perfect matchings. Next, we show that the EWPM problem that allows up to $\\frac{1}{poly(n)}$ additive error, is in NC.","sentences":["In this work, we study the parallel complexity of the Euclidean minimum-weight perfect matching (EWPM) problem.","Here our graph is the complete bipartite graph $G$ on two sets of points $A$ and $B$ in $\\mathbb{R}^2$ and the weight of each edge is the Euclidean distance between the corresponding points.","The weighted perfect matching problem on general bipartite graphs is known to be in RNC [Mulmuley, Vazirani, and Vazirani, 1987], and Quasi-NC","[Fenner, Gurjar, and Thierauf, 2016].","Both of these results work only when the weights are of $O(\\log n)$ bits.","It is a long-standing open question to show the problem to be in NC.   ","First, we show that for EWPM, a linear number of bits of approximation is required to distinguish between the minimum-weight perfect matching and other perfect matchings.","Next, we show that the EWPM problem that allows up to $\\frac{1}{poly(n)}$ additive error, is in NC."],"url":"http://arxiv.org/abs/2405.18833v1","category":"cs.CG"}
{"created":"2024-05-29 07:17:58","title":"CHANI: Correlation-based Hawkes Aggregation of Neurons with bio-Inspiration","abstract":"The present work aims at proving mathematically that a neural network inspired by biology can learn a classification task thanks to local transformations only. In this purpose, we propose a spiking neural network named CHANI (Correlation-based Hawkes Aggregation of Neurons with bio-Inspiration), whose neurons activity is modeled by Hawkes processes. Synaptic weights are updated thanks to an expert aggregation algorithm, providing a local and simple learning rule. We were able to prove that our network can learn on average and asymptotically. Moreover, we demonstrated that it automatically produces neuronal assemblies in the sense that the network can encode several classes and that a same neuron in the intermediate layers might be activated by more than one class, and we provided numerical simulations on synthetic dataset. This theoretical approach contrasts with the traditional empirical validation of biologically inspired networks and paves the way for understanding how local learning rules enable neurons to form assemblies able to represent complex concepts.","sentences":["The present work aims at proving mathematically that a neural network inspired by biology can learn a classification task thanks to local transformations only.","In this purpose, we propose a spiking neural network named CHANI (Correlation-based Hawkes Aggregation of Neurons with bio-Inspiration), whose neurons activity is modeled by Hawkes processes.","Synaptic weights are updated thanks to an expert aggregation algorithm, providing a local and simple learning rule.","We were able to prove that our network can learn on average and asymptotically.","Moreover, we demonstrated that it automatically produces neuronal assemblies in the sense that the network can encode several classes and that a same neuron in the intermediate layers might be activated by more than one class, and we provided numerical simulations on synthetic dataset.","This theoretical approach contrasts with the traditional empirical validation of biologically inspired networks and paves the way for understanding how local learning rules enable neurons to form assemblies able to represent complex concepts."],"url":"http://arxiv.org/abs/2405.18828v1","category":"math.ST"}
{"created":"2024-05-29 07:14:12","title":"Isovalent alloying assisted anomalous valley Hall effect in hexagonal antiferromagnetic monolayer","abstract":"Exploring combination of antiferromagnetic (AFM) spintronics and anomalous valley Hall effect (AVHE) is one of the most important questions for valleytronic applications. The key to address this issue is to achieve spin splitting around the valleys in AFM systems. Here, we propose a possible way for achieving AVHE in hexagonal AFM monolayer, which involves the isovalent alloying. This can break the combined symmetry ($PT$ symmetry) of spatial inversion ($P$) and time reversal ($T$), giving rise to spin splitting. More specifically, the large spin splitting around the Fermi energy level owes to $d$ orbital mismatch among these different transition metal ions. Based on first-principles calculations, the proposed way can be verified in out-of-plane AFM $\\mathrm{CrMoC_2S_6}$ monolayer, which possesses spontaneous valley polarization and spitting splitting, providing possibility to realize AVHE. It is also proved that tensile strain can strengthen the valley splitting and maintain the out-of-plane AFM ordering. Our works provide an experimentally feasible way for developing AFM valleytronic devices.","sentences":["Exploring combination of antiferromagnetic (AFM) spintronics and anomalous valley Hall effect (AVHE) is one of the most important questions for valleytronic applications.","The key to address this issue is to achieve spin splitting around the valleys in AFM systems.","Here, we propose a possible way for achieving AVHE in hexagonal AFM monolayer, which involves the isovalent alloying.","This can break the combined symmetry ($PT$ symmetry) of spatial inversion ($P$) and time reversal ($T$), giving rise to spin splitting.","More specifically, the large spin splitting around the Fermi energy level owes to $d$ orbital mismatch among these different transition metal ions.","Based on first-principles calculations, the proposed way can be verified in out-of-plane AFM $\\mathrm{CrMoC_2S_6}$ monolayer, which possesses spontaneous valley polarization and spitting splitting, providing possibility to realize AVHE.","It is also proved that tensile strain can strengthen the valley splitting and maintain the out-of-plane AFM ordering.","Our works provide an experimentally feasible way for developing AFM valleytronic devices."],"url":"http://arxiv.org/abs/2405.18826v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 06:55:04","title":"Spontaneous CP violation and partially broken modular flavor symmetries","abstract":"We study the realization of spontaneous CP violation through moduli stabilization. In modular flavor models, the source of CP violation is the vacuum expectation values of the complex structure moduli of toroidal compact space. We demonstrate that the combined effects of Type IIB flux compactifications with modular invariant couplings between the moduli and matter fields can induce spontaneous CP violation without or with supersymmetry breaking. Furthermore, some general properties of CP and modular invariant scalar potentials are presented. It is found that certain modifications or partial breakings of modular symmetry are useful in generating spontaneous CP violation.","sentences":["We study the realization of spontaneous CP violation through moduli stabilization.","In modular flavor models, the source of CP violation is the vacuum expectation values of the complex structure moduli of toroidal compact space.","We demonstrate that the combined effects of Type IIB flux compactifications with modular invariant couplings between the moduli and matter fields can induce spontaneous CP violation without or with supersymmetry breaking.","Furthermore, some general properties of CP and modular invariant scalar potentials are presented.","It is found that certain modifications or partial breakings of modular symmetry are useful in generating spontaneous CP violation."],"url":"http://arxiv.org/abs/2405.18813v1","category":"hep-ph"}
{"created":"2024-05-29 06:52:03","title":"Multiplicative Weights Update, Area Convexity and Random Coordinate Descent for Densest Subgraph Problems","abstract":"We study the densest subgraph problem and give algorithms via multiplicative weights updated area convexity that converge in $O\\left(\\frac{\\log m}{\\epsilon^{2}}\\right)$ and $O\\left(\\frac{\\log m}{\\epsilon}\\right)$ iterations, respectively, both with nearly-linear time per iteration. Compared with the work by Bahmani et al. (2014), our MWU algorithm uses a very different and much simpler procedure for recovering the dense subgraph from the fractional solution and does not employ a binary search. Compared with the work by Boob et al. (2019), our algorithm via area convexity improves the iteration complexity by a factor $\\Delta$ -- the maximum degree in the graph, and matches the fastest theoretical runtime currently known via flows (Chekuri et al., 2022) in total time. Next, we study the dense subgraph decomposition problem and give the first practical iterative algorithm with linear convergence rate $O\\left(mn\\log\\frac{1}{\\epsilon}\\right)$ via accelerated random coordinate descent. This significantly improves over $O\\left(\\frac{m\\sqrt{mn\\Delta}}{\\epsilon}\\right)$ time of the FISTA-based algorithm by Harb et al. (2022). In the high precision regime $\\epsilon\\ll\\frac{1}{n}$ where we can even recover the exact solution, our algorithm has a total runtime of $O\\left(mn\\log n\\right)$, matching the exact algorithm via parametric flows (Gallo et al., 1989). Empirically, we show that this algorithm is very practical and scales to very large graphs, and its performance is competitive with widely used methods that have significantly weaker theoretical guarantees.","sentences":["We study the densest subgraph problem and give algorithms via multiplicative weights updated area convexity that converge in $O\\left(\\frac{\\log m}{\\epsilon^{2}}\\right)$ and $O\\left(\\frac{\\log m}{\\epsilon}\\right)$ iterations, respectively, both with nearly-linear time per iteration.","Compared with the work by Bahmani et al. (2014), our MWU algorithm uses a very different and much simpler procedure for recovering the dense subgraph from the fractional solution and does not employ a binary search.","Compared with the work by Boob et al. (2019), our algorithm via area convexity improves the iteration complexity by a factor $\\Delta$ -- the maximum degree in the graph, and matches the fastest theoretical runtime currently known via flows (Chekuri et al., 2022) in total time.","Next, we study the dense subgraph decomposition problem and give the first practical iterative algorithm with linear convergence rate $O\\left(mn\\log\\frac{1}{\\epsilon}\\right)$ via accelerated random coordinate descent.","This significantly improves over $O\\left(\\frac{m\\sqrt{mn\\Delta}}{\\epsilon}\\right)$ time of the FISTA-based algorithm by Harb et al. (2022).","In the high precision regime $\\epsilon\\ll\\frac{1}{n}$ where we can even recover the exact solution, our algorithm has a total runtime of $O\\left(mn\\log n\\right)$, matching the exact algorithm via parametric flows (Gallo et al., 1989).","Empirically, we show that this algorithm is very practical and scales to very large graphs, and its performance is competitive with widely used methods that have significantly weaker theoretical guarantees."],"url":"http://arxiv.org/abs/2405.18809v1","category":"cs.DS"}
{"created":"2024-05-29 06:34:42","title":"Layered Chirp Spread Spectrum Modulations for LPWANs","abstract":"This article examines two chirp spread spectrum techniques specifically devised for low-power wide-area networks (LPWANs) to optimize energy and spectral efficiency (SE). These methods referred to as layered CSS (LCSS) and layered dual-mode CSS (LDMCSS), involves utilizing multiple layers for multiplexing symbols with varying chirp rates. These waveform designs exemplify a high degree of SE compared to existing schemes. Additionally, LDMCSS necessitates a lesser number of layers than LCSS to attain comparable SE, thereby reducing computational complexity. These proposed techniques can employ coherent and non-coherent detection and can be adjusted to achieve various spectral efficiencies by altering the number of multiplexed layers. Unlike our proposed LCSS and LDMCSS, other CSS alternatives for LPWANs cannot provide the same level of flexibility and SE. The performance of these techniques is evaluated in terms of bit error rate under different channel conditions, as well as with phase and frequency offsets.","sentences":["This article examines two chirp spread spectrum techniques specifically devised for low-power wide-area networks (LPWANs) to optimize energy and spectral efficiency (SE).","These methods referred to as layered CSS (LCSS) and layered dual-mode CSS (LDMCSS), involves utilizing multiple layers for multiplexing symbols with varying chirp rates.","These waveform designs exemplify a high degree of SE compared to existing schemes.","Additionally, LDMCSS necessitates a lesser number of layers than LCSS to attain comparable SE, thereby reducing computational complexity.","These proposed techniques can employ coherent and non-coherent detection and can be adjusted to achieve various spectral efficiencies by altering the number of multiplexed layers.","Unlike our proposed LCSS and LDMCSS, other CSS alternatives for LPWANs cannot provide the same level of flexibility and SE.","The performance of these techniques is evaluated in terms of bit error rate under different channel conditions, as well as with phase and frequency offsets."],"url":"http://arxiv.org/abs/2405.18799v1","category":"eess.SP"}
{"created":"2024-05-29 06:20:10","title":"Characterizing Novel Indium Phosphide Pad Detectors with Focused X-ray Beams and Laboratory Tests","abstract":"Future tracking systems in High Energy Physics experiments will require large instrumented areas with low radiation length. Crystalline silicon sensors have been used in tracking systems for decades, but are difficult to manufacture and costly to produce for large areas. We are exploring alternative sensor materials that are amenable to fast fabrication techniques used for thin film devices. Indium Phosphide pad sensors were fabricated at Argonne National Lab using commercially available InP:Fe 2-inch mono-crystal substrates. Current-voltage and capacitance-voltage characterizations were performed to study the basic operating characteristics of a group of sensors. Micro-focused X-ray beams at Canadian Light Source and Diamond Light Source were used to study the response to ionizing radiation, and characterize the uniformity of the response for several devices. The results show a high degree of performance uniformity in our evaluations, both within a device and between the 48 tested devices. This motivates further studies into thin film devices for future tracking detectors.","sentences":["Future tracking systems in High Energy Physics experiments will require large instrumented areas with low radiation length.","Crystalline silicon sensors have been used in tracking systems for decades, but are difficult to manufacture and costly to produce for large areas.","We are exploring alternative sensor materials that are amenable to fast fabrication techniques used for thin film devices.","Indium Phosphide pad sensors were fabricated at Argonne National Lab using commercially available InP:","Fe 2-inch mono-crystal substrates.","Current-voltage and capacitance-voltage characterizations were performed to study the basic operating characteristics of a group of sensors.","Micro-focused X-ray beams at Canadian Light Source and Diamond Light Source were used to study the response to ionizing radiation, and characterize the uniformity of the response for several devices.","The results show a high degree of performance uniformity in our evaluations, both within a device and between the 48 tested devices.","This motivates further studies into thin film devices for future tracking detectors."],"url":"http://arxiv.org/abs/2405.18794v1","category":"physics.ins-det"}
{"created":"2024-05-29 06:18:09","title":"Adaptive Discretization-based Non-Episodic Reinforcement Learning in Metric Spaces","abstract":"We study non-episodic Reinforcement Learning for Lipschitz MDPs in which state-action space is a metric space, and the transition kernel and rewards are Lipschitz functions. We develop computationally efficient UCB-based algorithm, $\\textit{ZoRL-}\\epsilon$ that adaptively discretizes the state-action space and show that their regret as compared with $\\epsilon$-optimal policy is bounded as $\\mathcal{O}(\\epsilon^{-(2 d_\\mathcal{S} + d^\\epsilon_z + 1)}\\log{(T)})$, where $d^\\epsilon_z$ is the $\\epsilon$-zooming dimension. In contrast, if one uses the vanilla $\\textit{UCRL-}2$ on a fixed discretization of the MDP, the regret w.r.t. a $\\epsilon$-optimal policy scales as $\\mathcal{O}(\\epsilon^{-(2 d_\\mathcal{S} + d + 1)}\\log{(T)})$ so that the adaptivity gains are huge when $d^\\epsilon_z \\ll d$. Note that the absolute regret of any 'uniformly good' algorithm for a large family of continuous MDPs asymptotically scales as at least $\\Omega(\\log{(T)})$. Though adaptive discretization has been shown to yield $\\mathcal{\\tilde{O}}(H^{2.5}K^\\frac{d_z + 1}{d_z + 2})$ regret in episodic RL, an attempt to extend this to the non-episodic case by employing constant duration episodes whose duration increases with $T$, is futile since $d_z \\to d$ as $T \\to \\infty$. The current work shows how to obtain adaptivity gains for non-episodic RL. The theoretical results are supported by simulations on two systems where the performance of $\\textit{ZoRL-}\\epsilon$ is compared with that of '$\\textit{UCRL-C}$,' the fixed discretization-based extension of $\\textit{UCRL-}2$ for systems with continuous state-action spaces.","sentences":["We study non-episodic Reinforcement Learning for Lipschitz MDPs in which state-action space is a metric space, and the transition kernel and rewards are Lipschitz functions.","We develop computationally efficient UCB-based algorithm, $\\textit{ZoRL-}\\epsilon$ that adaptively discretizes the state-action space and show that their regret as compared with $\\epsilon$-optimal policy is bounded as $\\mathcal{O}(\\epsilon^{-(2 d_\\mathcal{S} + d^\\epsilon_z + 1)}\\log{(T)})$, where $d^\\epsilon_z$ is the $\\epsilon$-zooming dimension.","In contrast, if one uses the vanilla $\\textit{UCRL-}2$ on a fixed discretization of the MDP, the regret w.r.t.","a $\\epsilon$-optimal policy scales as $\\mathcal{O}(\\epsilon^{-(2 d_\\mathcal{S}","+ d + 1)}\\log{(T)})$ so that the adaptivity gains are huge when $d^\\epsilon_z \\ll d$.","Note that the absolute regret of any 'uniformly good' algorithm for a large family of continuous MDPs asymptotically scales as","at least $\\Omega(\\log{(T)})$. Though adaptive discretization has been shown to yield $\\mathcal{\\tilde{O}}(H^{2.5}K^\\frac{d_z + 1}{d_z + 2})$ regret in episodic RL, an attempt to extend this to the non-episodic case by employing constant duration episodes whose duration increases with $T$, is futile since $d_z \\to d$ as $T \\to \\infty$. The current work shows how to obtain adaptivity gains for non-episodic RL.","The theoretical results are supported by simulations on two systems where the performance of $\\textit{ZoRL-}\\epsilon$ is compared with that of '$\\textit{UCRL-C}$,' the fixed discretization-based extension of $\\textit{UCRL-}2$ for systems with continuous state-action spaces."],"url":"http://arxiv.org/abs/2405.18793v1","category":"cs.LG"}
{"created":"2024-05-29 06:11:09","title":"A new platooning model for connected and autonomous vehicles to improve string stability","abstract":"This paper introduces a novel idea of coordinated vehicle platooning such that platoon followers inside the platoon communicates only to the platoon leader. A novel dynamic model is proposed to take driving safety into account when there is communication delay. Some general results of linear stability are proved mathematically, and numerical simulations are conducted to show the effect of model parameters for both ring road with an initial disturbance and infinite road with a periodic disturbance. The simulation results are consistent with theoretical analysis, and demonstrate that the proposed look-to-the-leader platooning strategy is far superior than the follow-one-vehicle-ahead or follow-two-vehicle-ahead conventional car-following (CF) strategies in stabilizing traffic flow. This paper provides a new perspective for the organization of platoons of autonomous vehicles.","sentences":["This paper introduces a novel idea of coordinated vehicle platooning such that platoon followers inside the platoon communicates only to the platoon leader.","A novel dynamic model is proposed to take driving safety into account when there is communication delay.","Some general results of linear stability are proved mathematically, and numerical simulations are conducted to show the effect of model parameters for both ring road with an initial disturbance and infinite road with a periodic disturbance.","The simulation results are consistent with theoretical analysis, and demonstrate that the proposed look-to-the-leader platooning strategy is far superior than the follow-one-vehicle-ahead or follow-two-vehicle-ahead conventional car-following (CF) strategies in stabilizing traffic flow.","This paper provides a new perspective for the organization of platoons of autonomous vehicles."],"url":"http://arxiv.org/abs/2405.18791v1","category":"eess.SY"}
{"created":"2024-05-29 06:05:47","title":"An overview of some single machine scheduling problems: polynomial algorithms, complexity and approximability","abstract":"Since the publication of the first scheduling paper in 1954, a huge number of works dealing with different types of single machine problems appeared. They addressed many heuristics and enumerative procedures, complexity results or structural properties of certain problems. Regarding surveys, often particular subjects like special objective functions are discussed, or more general scheduling problems were surveyed, where a substantial part is devoted to single machine problems. In this paper we present some results on polynomial algorithms, complexity and approximation issues, where the main focus is on results, which have been published during the last decades in papers, where at least one of the first two authors of this paper was involved. We hope that the reviewed results will stimulate further investigation in related research fields.","sentences":["Since the publication of the first scheduling paper in 1954, a huge number of works dealing with different types of single machine problems appeared.","They addressed many heuristics and enumerative procedures, complexity results or structural properties of certain problems.","Regarding surveys, often particular subjects like special objective functions are discussed, or more general scheduling problems were surveyed, where a substantial part is devoted to single machine problems.","In this paper we present some results on polynomial algorithms, complexity and approximation issues, where the main focus is on results, which have been published during the last decades in papers, where at least one of the first two authors of this paper was involved.","We hope that the reviewed results will stimulate further investigation in related research fields."],"url":"http://arxiv.org/abs/2405.18789v1","category":"cs.DS"}
{"created":"2024-05-29 05:59:15","title":"A Fast and Adaptable Algorithm for Optimal Multi-Qubit Pathfinding in Quantum Circuit Compilation","abstract":"Quantum computing has the potential to significantly enhance our ability to simulate and solve complex, classically intractable problems across various fields of research and industry. However, we are currently in the noisy intermediate-scale quantum (NISQ) era, where devices are relatively small and suffer from substantial noise levels, prohibiting large-scale computations. To achieve any quantum advantage in this regime and beyond, it is crucial to minimise the impact of noise from qubit decoherence and two-qubit gates. A direct approach is to improve the optimisation of quantum circuit compilation processes that map circuits onto physical devices, thereby reducing noisy gates and circuit execution times. This work focuses on multi-qubit pathfinding as a critical subroutine within the quantum circuit compilation mapping problem. We introduce an algorithm, modelled using binary integer linear programming, that navigates qubits on quantum hardware optimally with respect to circuit SWAP-gate depth, while also optimising for accumulated gate errors and can be flexibly adapted to various problem modifications. This multi-qubit pathfinding algorithm incorporates considerations for gate-error penalties, SWAP movement constraints, and configurable arrangements of source and target qubit locations and qubit teams. We have benchmarked the algorithm across a variety of quantum hardware layouts, assessing properties such as computational runtimes, solution SWAP depths, and accumulated SWAP-gate error rates. The results demonstrate the algorithm's practical runtimes on current quantum devices and compare its effectiveness across different hardware configurations, providing insights for future quantum hardware design.","sentences":["Quantum computing has the potential to significantly enhance our ability to simulate and solve complex, classically intractable problems across various fields of research and industry.","However, we are currently in the noisy intermediate-scale quantum (NISQ) era, where devices are relatively small and suffer from substantial noise levels, prohibiting large-scale computations.","To achieve any quantum advantage in this regime and beyond, it is crucial to minimise the impact of noise from qubit decoherence and two-qubit gates.","A direct approach is to improve the optimisation of quantum circuit compilation processes that map circuits onto physical devices, thereby reducing noisy gates and circuit execution times.","This work focuses on multi-qubit pathfinding as a critical subroutine within the quantum circuit compilation mapping problem.","We introduce an algorithm, modelled using binary integer linear programming, that navigates qubits on quantum hardware optimally with respect to circuit SWAP-gate depth, while also optimising for accumulated gate errors and can be flexibly adapted to various problem modifications.","This multi-qubit pathfinding algorithm incorporates considerations for gate-error penalties, SWAP movement constraints, and configurable arrangements of source and target qubit locations and qubit teams.","We have benchmarked the algorithm across a variety of quantum hardware layouts, assessing properties such as computational runtimes, solution SWAP depths, and accumulated SWAP-gate error rates.","The results demonstrate the algorithm's practical runtimes on current quantum devices and compare its effectiveness across different hardware configurations, providing insights for future quantum hardware design."],"url":"http://arxiv.org/abs/2405.18785v1","category":"quant-ph"}
{"created":"2024-05-29 05:42:25","title":"Principled Probabilistic Imaging using Diffusion Models as Plug-and-Play Priors","abstract":"Diffusion models (DMs) have recently shown outstanding capability in modeling complex image distributions, making them expressive image priors for solving Bayesian inverse problems. However, most existing DM-based methods rely on approximations in the generative process to be generic to different inverse problems, leading to inaccurate sample distributions that deviate from the target posterior defined within the Bayesian framework. To harness the generative power of DMs while avoiding such approximations, we propose a Markov chain Monte Carlo algorithm that performs posterior sampling for general inverse problems by reducing it to sampling the posterior of a Gaussian denoising problem. Crucially, we leverage a general DM formulation as a unified interface that allows for rigorously solving the denoising problem with a range of state-of-the-art DMs. We demonstrate the effectiveness of the proposed method on six inverse problems (three linear and three nonlinear), including a real-world black hole imaging problem. Experimental results indicate that our proposed method offers more accurate reconstructions and posterior estimation compared to existing DM-based imaging inverse methods.","sentences":["Diffusion models (DMs) have recently shown outstanding capability in modeling complex image distributions, making them expressive image priors for solving Bayesian inverse problems.","However, most existing DM-based methods rely on approximations in the generative process to be generic to different inverse problems, leading to inaccurate sample distributions that deviate from the target posterior defined within the Bayesian framework.","To harness the generative power of DMs while avoiding such approximations, we propose a Markov chain Monte Carlo algorithm that performs posterior sampling for general inverse problems by reducing it to sampling the posterior of a Gaussian denoising problem.","Crucially, we leverage a general DM formulation as a unified interface that allows for rigorously solving the denoising problem with a range of state-of-the-art DMs.","We demonstrate the effectiveness of the proposed method on six inverse problems (three linear and three nonlinear), including a real-world black hole imaging problem.","Experimental results indicate that our proposed method offers more accurate reconstructions and posterior estimation compared to existing DM-based imaging inverse methods."],"url":"http://arxiv.org/abs/2405.18782v1","category":"eess.IV"}
{"created":"2024-05-29 05:41:28","title":"On the Role of Attention Masks and LayerNorm in Transformers","abstract":"Self-attention is the key mechanism of transformers, which are the essential building blocks of modern foundation models. Recent studies have shown that pure self-attention suffers from an increasing degree of rank collapse as depth increases, limiting model expressivity and further utilization of model depth. The existing literature on rank collapse, however, has mostly overlooked other critical components in transformers that may alleviate the rank collapse issue. In this paper, we provide a general analysis of rank collapse under self-attention, taking into account the effects of attention masks and layer normalization (LayerNorm). In particular, we find that although pure masked attention still suffers from exponential collapse to a rank one subspace, local masked attention can provably slow down the collapse rate. In the case of self-attention with LayerNorm, we first show that for certain classes of value matrices, collapse to a rank one subspace still happens exponentially. However, through construction of nontrivial counterexamples, we then establish that with proper choice of value matrices, a general class of sequences may not converge to a rank one subspace, and the self-attention dynamics with LayerNorm can simultaneously possess a rich set of equilibria with any possible rank between one and full. Our result refutes the previous hypothesis that LayerNorm plays no role in the rank collapse of self-attention and suggests that self-attention with LayerNorm constitutes a much more expressive, versatile nonlinear dynamical system than what was originally thought.","sentences":["Self-attention is the key mechanism of transformers, which are the essential building blocks of modern foundation models.","Recent studies have shown that pure self-attention suffers from an increasing degree of rank collapse as depth increases, limiting model expressivity and further utilization of model depth.","The existing literature on rank collapse, however, has mostly overlooked other critical components in transformers that may alleviate the rank collapse issue.","In this paper, we provide a general analysis of rank collapse under self-attention, taking into account the effects of attention masks and layer normalization (LayerNorm).","In particular, we find that although pure masked attention still suffers from exponential collapse to a rank one subspace, local masked attention can provably slow down the collapse rate.","In the case of self-attention with LayerNorm, we first show that for certain classes of value matrices, collapse to a rank one subspace still happens exponentially.","However, through construction of nontrivial counterexamples, we then establish that with proper choice of value matrices, a general class of sequences may not converge to a rank one subspace, and the self-attention dynamics with LayerNorm can simultaneously possess a rich set of equilibria with any possible rank between one and full.","Our result refutes the previous hypothesis that LayerNorm plays no role in the rank collapse of self-attention and suggests that self-attention with LayerNorm constitutes a much more expressive, versatile nonlinear dynamical system than what was originally thought."],"url":"http://arxiv.org/abs/2405.18781v1","category":"cs.LG"}
{"created":"2024-05-29 05:36:03","title":"SPABA: A Single-Loop and Probabilistic Stochastic Bilevel Algorithm Achieving Optimal Sample Complexity","abstract":"While stochastic bilevel optimization methods have been extensively studied for addressing large-scale nested optimization problems in machine learning, it remains an open question whether the optimal complexity bounds for solving bilevel optimization are the same as those in single-level optimization. Our main result resolves this question: SPABA, an adaptation of the PAGE method for nonconvex optimization in (Li et al., 2021) to the bilevel setting, can achieve optimal sample complexity in both the finite-sum and expectation settings. We show the optimality of SPABA by proving that there is no gap in complexity analysis between stochastic bilevel and single-level optimization when implementing PAGE. Notably, as indicated by the results of (Dagr\\'eou et al., 2022), there might exist a gap in complexity analysis when implementing other stochastic gradient estimators, like SGD and SAGA. In addition to SPABA, we propose several other single-loop stochastic bilevel algorithms, that either match or improve the state-of-the-art sample complexity results, leveraging our convergence rate and complexity analysis. Numerical experiments demonstrate the superior practical performance of the proposed methods.","sentences":["While stochastic bilevel optimization methods have been extensively studied for addressing large-scale nested optimization problems in machine learning, it remains an open question whether the optimal complexity bounds for solving bilevel optimization are the same as those in single-level optimization.","Our main result resolves this question: SPABA, an adaptation of the PAGE method for nonconvex optimization in (Li et al., 2021) to the bilevel setting, can achieve optimal sample complexity in both the finite-sum and expectation settings.","We show the optimality of SPABA by proving that there is no gap in complexity analysis between stochastic bilevel and single-level optimization when implementing PAGE.","Notably, as indicated by the results of (Dagr\\'eou et al., 2022), there might exist a gap in complexity analysis when implementing other stochastic gradient estimators, like SGD and SAGA.","In addition to SPABA, we propose several other single-loop stochastic bilevel algorithms, that either match or improve the state-of-the-art sample complexity results, leveraging our convergence rate and complexity analysis.","Numerical experiments demonstrate the superior practical performance of the proposed methods."],"url":"http://arxiv.org/abs/2405.18777v1","category":"math.OC"}
{"created":"2024-05-29 05:27:32","title":"Synchronization Scheme based on Pilot Sharing in Cell-Free Massive MIMO Systems","abstract":"This paper analyzes the impact of pilot-sharing scheme on synchronization performance in a scenario where several slave access points (APs) with uncertain carrier frequency offsets (CFOs) and timing offsets (TOs) share a common pilot sequence. First, the Cramer-Rao bound (CRB) with pilot contamination is derived for pilot-pairing estimation. Furthermore, a maximum likelihood algorithm is presented to estimate the CFO and TO among the pairing APs. Then, to minimize the sum of CRBs, we devise a synchronization strategy based on a pilot-sharing scheme by jointly optimizing the cluster classification, synchronization overhead, and pilot-sharing scheme, while simultaneously considering the overhead and each AP's synchronization requirements. To solve this NP-hard problem, we simplify it into two sub-problems, namely cluster classification problem and the pilot sharing problem. To strike a balance between synchronization performance and overhead, we first classify the clusters by using the K-means algorithm, and propose a criteria to find a good set of master APs. Then, the pilot-sharing scheme is obtained by using the swap-matching operations. Simulation results validate the accuracy of our derivations and demonstrate the effectiveness of the proposed scheme over the benchmark schemes.","sentences":["This paper analyzes the impact of pilot-sharing scheme on synchronization performance in a scenario where several slave access points (APs) with uncertain carrier frequency offsets (CFOs) and timing offsets (TOs) share a common pilot sequence.","First, the Cramer-Rao bound (CRB) with pilot contamination is derived for pilot-pairing estimation.","Furthermore, a maximum likelihood algorithm is presented to estimate the CFO and TO among the pairing APs.","Then, to minimize the sum of CRBs, we devise a synchronization strategy based on a pilot-sharing scheme by jointly optimizing the cluster classification, synchronization overhead, and pilot-sharing scheme, while simultaneously considering the overhead and each AP's synchronization requirements.","To solve this NP-hard problem, we simplify it into two sub-problems, namely cluster classification problem and the pilot sharing problem.","To strike a balance between synchronization performance and overhead, we first classify the clusters by using the K-means algorithm, and propose a criteria to find a good set of master APs.","Then, the pilot-sharing scheme is obtained by using the swap-matching operations.","Simulation results validate the accuracy of our derivations and demonstrate the effectiveness of the proposed scheme over the benchmark schemes."],"url":"http://arxiv.org/abs/2405.18775v1","category":"eess.SP"}
{"created":"2024-05-29 05:12:16","title":"OUS: Scene-Guided Dynamic Facial Expression Recognition","abstract":"Dynamic Facial Expression Recognition (DFER) is crucial for affective computing but often overlooks the impact of scene context. We have identified a significant issue in current DFER tasks: human annotators typically integrate emotions from various angles, including environmental cues and body language, whereas existing DFER methods tend to consider the scene as noise that needs to be filtered out, focusing solely on facial information. We refer to this as the Rigid Cognitive Problem. The Rigid Cognitive Problem can lead to discrepancies between the cognition of annotators and models in some samples. To align more closely with the human cognitive paradigm of emotions, we propose an Overall Understanding of the Scene DFER method (OUS). OUS effectively integrates scene and facial features, combining scene-specific emotional knowledge for DFER. Extensive experiments on the two largest datasets in the DFER field, DFEW and FERV39k, demonstrate that OUS significantly outperforms existing methods. By analyzing the Rigid Cognitive Problem, OUS successfully understands the complex relationship between scene context and emotional expression, closely aligning with human emotional understanding in real-world scenarios.","sentences":["Dynamic Facial Expression Recognition (DFER) is crucial for affective computing but often overlooks the impact of scene context.","We have identified a significant issue in current DFER tasks: human annotators typically integrate emotions from various angles, including environmental cues and body language, whereas existing DFER methods tend to consider the scene as noise that needs to be filtered out, focusing solely on facial information.","We refer to this as the Rigid Cognitive Problem.","The Rigid Cognitive Problem can lead to discrepancies between the cognition of annotators and models in some samples.","To align more closely with the human cognitive paradigm of emotions, we propose an Overall Understanding of the Scene DFER method (OUS).","OUS effectively integrates scene and facial features, combining scene-specific emotional knowledge for DFER.","Extensive experiments on the two largest datasets in the DFER field, DFEW and FERV39k, demonstrate that OUS significantly outperforms existing methods.","By analyzing the Rigid Cognitive Problem, OUS successfully understands the complex relationship between scene context and emotional expression, closely aligning with human emotional understanding in real-world scenarios."],"url":"http://arxiv.org/abs/2405.18769v1","category":"cs.CV"}
{"created":"2024-05-29 04:58:05","title":"Local nature of 0.1 Hz oscillations in microcirculation is confirmed by imaging photoplethysmography","abstract":"Low-frequency oscillations in the human circulatory system is important for basic physiology and practical applications in clinical medicine. Our objective was to study which mechanism (central or local) is responsible for changes in blood flow fluctuations at around 0.1 Hz. We used the method of imaging photoplethysmography synchronized with electrocardiography to measure blood-flow response to local forearm heating of 18 healthy male volunteers. The dynamics of peripheral perfusion was revealed by a correlation processing of photoplethysmography data, and the central hemodynamics was assessed from the electrocardiogram. Wavelet analysis was used to estimate the dynamics of spectral components. Our results show that skin heating leads to multiple increase in local perfusion accompanied by drop in blood flow oscillations at 0.1 Hz, whereas no changes in heart rate variability was observed. After switching off the heating, perfusion remains at the high level, regardless decrease in skin temperature. The 0.1 Hz oscillations are smoothly recovered to the base level. In conclusion, we confirm the local nature of fluctuations in peripheral blood flow in the frequency band of about 0.1 Hz. A significant, but time-delayed, recovery of fluctuation energy in this frequency range after cessation of the skin warming was discovered. This study reveals a novel factor involved in the regulation microcirculatory vascular tone. A comprehensive study of hemodynamics using the new technique of imaging photoplethysmography synchronized with electrocardiography is a prerequisite for development of a valuable diagnostic tool.","sentences":["Low-frequency oscillations in the human circulatory system is important for basic physiology and practical applications in clinical medicine.","Our objective was to study which mechanism (central or local) is responsible for changes in blood flow fluctuations at around 0.1 Hz.","We used the method of imaging photoplethysmography synchronized with electrocardiography to measure blood-flow response to local forearm heating of 18 healthy male volunteers.","The dynamics of peripheral perfusion was revealed by a correlation processing of photoplethysmography data, and the central hemodynamics was assessed from the electrocardiogram.","Wavelet analysis was used to estimate the dynamics of spectral components.","Our results show that skin heating leads to multiple increase in local perfusion accompanied by drop in blood flow oscillations at 0.1 Hz, whereas no changes in heart rate variability was observed.","After switching off the heating, perfusion remains at the high level, regardless decrease in skin temperature.","The 0.1 Hz oscillations are smoothly recovered to the base level.","In conclusion, we confirm the local nature of fluctuations in peripheral blood flow in the frequency band of about 0.1 Hz.","A significant, but time-delayed, recovery of fluctuation energy in this frequency range after cessation of the skin warming was discovered.","This study reveals a novel factor involved in the regulation microcirculatory vascular tone.","A comprehensive study of hemodynamics using the new technique of imaging photoplethysmography synchronized with electrocardiography is a prerequisite for development of a valuable diagnostic tool."],"url":"http://arxiv.org/abs/2405.18760v1","category":"q-bio.TO"}
{"created":"2024-05-29 04:37:19","title":"Confronting the Reproducibility Crisis: A Case Study in Validating Certified Robustness","abstract":"Reproducibility is a cornerstone of scientific research, enabling validation, extension, and progress. However, the rapidly evolving nature of software and dependencies poses significant challenges to reproducing research results, particularly in fields like adversarial robustness for deep neural networks, where complex codebases and specialized toolkits are utilized. This paper presents a case study of attempting to validate the results on certified adversarial robustness in \"SoK: Certified Robustness for Deep Neural Networks\" using the VeriGauge toolkit. Despite following the documented methodology, numerous software and hardware compatibility issues were encountered, including outdated or unavailable dependencies, version conflicts, and driver incompatibilities. While a subset of the original results could be run, key findings related to the empirical robust accuracy of various verification methods proved elusive due to these technical obstacles, as well as slight discrepancies in the test results. This practical experience sheds light on the reproducibility crisis afflicting adversarial robustness research, where a lack of reproducibility threatens scientific integrity and hinders progress. The paper discusses the broader implications of this crisis, proposing potential solutions such as containerization, software preservation, and comprehensive documentation practices. Furthermore, it highlights the need for collaboration and standardization efforts within the research community to develop robust frameworks for reproducible research. By addressing the reproducibility crisis head-on, this work aims to contribute to the ongoing discourse on scientific reproducibility and advocate for best practices that ensure the reliability and validity of research findings within not only adversarial robustness, but security and technology research as a whole.","sentences":["Reproducibility is a cornerstone of scientific research, enabling validation, extension, and progress.","However, the rapidly evolving nature of software and dependencies poses significant challenges to reproducing research results, particularly in fields like adversarial robustness for deep neural networks, where complex codebases and specialized toolkits are utilized.","This paper presents a case study of attempting to validate the results on certified adversarial robustness in \"SoK: Certified Robustness for Deep Neural Networks\" using the VeriGauge toolkit.","Despite following the documented methodology, numerous software and hardware compatibility issues were encountered, including outdated or unavailable dependencies, version conflicts, and driver incompatibilities.","While a subset of the original results could be run, key findings related to the empirical robust accuracy of various verification methods proved elusive due to these technical obstacles, as well as slight discrepancies in the test results.","This practical experience sheds light on the reproducibility crisis afflicting adversarial robustness research, where a lack of reproducibility threatens scientific integrity and hinders progress.","The paper discusses the broader implications of this crisis, proposing potential solutions such as containerization, software preservation, and comprehensive documentation practices.","Furthermore, it highlights the need for collaboration and standardization efforts within the research community to develop robust frameworks for reproducible research.","By addressing the reproducibility crisis head-on, this work aims to contribute to the ongoing discourse on scientific reproducibility and advocate for best practices that ensure the reliability and validity of research findings within not only adversarial robustness, but security and technology research as a whole."],"url":"http://arxiv.org/abs/2405.18753v1","category":"cs.LG"}
{"created":"2024-05-29 04:32:28","title":"Resilient Average Consensus with Adversaries via Distributed Detection and Recovery","abstract":"We study the problem of resilient average consensus in multi-agent systems where some of the agents are subject to failures or attacks. The objective of resilient average consensus is for non-faulty/normal agents to converge to the average of their initial values despite the erroneous effects from malicious agents. To this end, we propose a successful distributed iterative resilient average consensus algorithm for the multi-agent networks with general directed topologies. The proposed algorithm has two parts at each iteration: detection and averaging. For the detection part, we propose two distributed algorithms and one of them can detect malicious agents with only the information from direct in-neighbors. For the averaging part, we extend the applicability of an existing averaging algorithm where normal agents can remove the effects from malicious agents so far, after they are detected. Another important feature of our method is that it can handle the case where malicious agents are neighboring and collaborating with each other to mislead the normal ones from averaging. This case cannot be solved by existing detection approaches in related literature. Moreover, our algorithm is efficient in storage usage especially for large-scale networks as each agent only requires the values of neighbors within two hops. Lastly, numerical examples are given to verify the efficacy of the proposed algorithms.","sentences":["We study the problem of resilient average consensus in multi-agent systems where some of the agents are subject to failures or attacks.","The objective of resilient average consensus is for non-faulty/normal agents to converge to the average of their initial values despite the erroneous effects from malicious agents.","To this end, we propose a successful distributed iterative resilient average consensus algorithm for the multi-agent networks with general directed topologies.","The proposed algorithm has two parts at each iteration: detection and averaging.","For the detection part, we propose two distributed algorithms and one of them can detect malicious agents with only the information from direct in-neighbors.","For the averaging part, we extend the applicability of an existing averaging algorithm where normal agents can remove the effects from malicious agents so far, after they are detected.","Another important feature of our method is that it can handle the case where malicious agents are neighboring and collaborating with each other to mislead the normal ones from averaging.","This case cannot be solved by existing detection approaches in related literature.","Moreover, our algorithm is efficient in storage usage especially for large-scale networks as each agent only requires the values of neighbors within two hops.","Lastly, numerical examples are given to verify the efficacy of the proposed algorithms."],"url":"http://arxiv.org/abs/2405.18752v1","category":"cs.MA"}
{"created":"2024-05-29 04:22:18","title":"A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody Language Models","abstract":"Antibodies are crucial proteins produced by the immune system to eliminate harmful foreign substances and have become pivotal therapeutic agents for treating human diseases. To accelerate the discovery of antibody therapeutics, there is growing interest in constructing language models using antibody sequences. However, the applicability of pre-trained language models for antibody discovery has not been thoroughly evaluated due to the scarcity of labeled datasets. To overcome these limitations, we introduce AVIDa-SARS-CoV-2, a dataset featuring the antigen-variable domain of heavy chain of heavy chain antibody (VHH) interactions obtained from two alpacas immunized with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins. AVIDa-SARS-CoV-2 includes binary labels indicating the binding or non-binding of diverse VHH sequences to 12 SARS-CoV-2 mutants, such as the Delta and Omicron variants. Furthermore, we release VHHCorpus-2M, a pre-training dataset for antibody language models, containing over two million VHH sequences. We report benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT pre-trained on VHHCorpus-2M and existing general protein and antibody-specific pre-trained language models. These results confirm that AVIDa-SARS-CoV-2 provides valuable benchmarks for evaluating the representation capabilities of antibody language models for binding prediction, thereby facilitating the development of AI-driven antibody discovery. The datasets are available at https://datasets.cognanous.com.","sentences":["Antibodies are crucial proteins produced by the immune system to eliminate harmful foreign substances and have become pivotal therapeutic agents for treating human diseases.","To accelerate the discovery of antibody therapeutics, there is growing interest in constructing language models using antibody sequences.","However, the applicability of pre-trained language models for antibody discovery has not been thoroughly evaluated due to the scarcity of labeled datasets.","To overcome these limitations, we introduce AVIDa-SARS-CoV-2, a dataset featuring the antigen-variable domain of heavy chain of heavy chain antibody (VHH) interactions obtained from two alpacas immunized with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins.","AVIDa-SARS-CoV-2 includes binary labels indicating the binding or non-binding of diverse VHH sequences to 12 SARS-CoV-2 mutants, such as the Delta and Omicron variants.","Furthermore, we release VHHCorpus-2M, a pre-training dataset for antibody language models, containing over two million VHH sequences.","We report benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT pre-trained on VHHCorpus-2M and existing general protein and antibody-specific pre-trained language models.","These results confirm that AVIDa-SARS-CoV-2 provides valuable benchmarks for evaluating the representation capabilities of antibody language models for binding prediction, thereby facilitating the development of AI-driven antibody discovery.","The datasets are available at https://datasets.cognanous.com."],"url":"http://arxiv.org/abs/2405.18749v1","category":"cs.LG"}
{"created":"2024-05-29 04:21:30","title":"Equity Implications of Net-Zero Emissions: A Multi-Model Analysis of Energy Expenditures Across Income Classes Under Economy-Wide Deep Decarbonization Policies","abstract":"With companies, states, and countries targeting net-zero emissions around midcentury, there are questions about how these targets alter household welfare and finances, including distributional effects across income groups. This paper examines the distributional dimensions of technology transitions and net-zero policies with a focus on welfare impacts across household incomes. The analysis uses a model intercomparison with a range of energy-economy models using harmonized policy scenarios reaching economy-wide, net-zero CO2 emissions across the United States in 2050. We employ a novel linking approach that connects output from detailed energy system models with survey microdata on energy expenditures across income classes to provide distributional analysis of net-zero policies. Although there are differences in model structure and input assumptions, we find broad agreement in qualitative trends in policy incidence and energy burdens across income groups. Models generally agree that direct energy expenditures for many households will likely decline over time with reference and net-zero policies. However, there is variation in the extent of changes relative to current levels, energy burdens relative to reference levels, and electricity expenditures. Policy design, primarily how climate policy revenues are used, has first-order impacts on distributional outcomes. Net-zero policy costs, in both absolute and relative terms, are unevenly distributed across households, and relative increases in energy expenditures are higher for lowest-income households. However, we also find that recycled revenues from climate policies have countervailing effects when rebated on a per-capita basis, offsetting higher energy burdens and potentially even leading to net progressive outcomes.","sentences":["With companies, states, and countries targeting net-zero emissions around midcentury, there are questions about how these targets alter household welfare and finances, including distributional effects across income groups.","This paper examines the distributional dimensions of technology transitions and net-zero policies with a focus on welfare impacts across household incomes.","The analysis uses a model intercomparison with a range of energy-economy models using harmonized policy scenarios reaching economy-wide, net-zero CO2 emissions across the United States in 2050.","We employ a novel linking approach that connects output from detailed energy system models with survey microdata on energy expenditures across income classes to provide distributional analysis of net-zero policies.","Although there are differences in model structure and input assumptions, we find broad agreement in qualitative trends in policy incidence and energy burdens across income groups.","Models generally agree that direct energy expenditures for many households will likely decline over time with reference and net-zero policies.","However, there is variation in the extent of changes relative to current levels, energy burdens relative to reference levels, and electricity expenditures.","Policy design, primarily how climate policy revenues are used, has first-order impacts on distributional outcomes.","Net-zero policy costs, in both absolute and relative terms, are unevenly distributed across households, and relative increases in energy expenditures are higher for lowest-income households.","However, we also find that recycled revenues from climate policies have countervailing effects when rebated on a per-capita basis, offsetting higher energy burdens and potentially even leading to net progressive outcomes."],"url":"http://arxiv.org/abs/2405.18748v1","category":"physics.soc-ph"}
{"created":"2024-05-29 03:53:52","title":"FlocOff: Data Heterogeneity Resilient Federated Learning with Communication-Efficient Edge Offloading","abstract":"Federated Learning (FL) has emerged as a fundamental learning paradigm to harness massive data scattered at geo-distributed edge devices in a privacy-preserving way. Given the heterogeneous deployment of edge devices, however, their data are usually Non-IID, introducing significant challenges to FL including degraded training accuracy, intensive communication costs, and high computing complexity. Towards that, traditional approaches typically utilize adaptive mechanisms, which may suffer from scalability issues, increased computational overhead, and limited adaptability to diverse edge environments. To address that, this paper instead leverages the observation that the computation offloading involves inherent functionalities such as node matching and service correlation to achieve data reshaping and proposes Federated learning based on computing Offloading (FlocOff) framework, to address data heterogeneity and resource-constrained challenges. Specifically, FlocOff formulates the FL process with Non-IID data in edge scenarios and derives rigorous analysis on the impact of imbalanced data distribution. Based on this, FlocOff decouples the optimization in two steps, namely : (1) Minimizes the Kullback-Leibler (KL) divergence via Computation Offloading scheduling (MKL-CO); (2) Minimizes the Communication Cost through Resource Allocation (MCC-RA). Extensive experimental results demonstrate that the proposed FlocOff effectively improves model convergence and accuracy by 14.3\\%-32.7\\% while reducing data heterogeneity under various data distributions.","sentences":["Federated Learning (FL) has emerged as a fundamental learning paradigm to harness massive data scattered at geo-distributed edge devices in a privacy-preserving way.","Given the heterogeneous deployment of edge devices, however, their data are usually Non-IID, introducing significant challenges to FL including degraded training accuracy, intensive communication costs, and high computing complexity.","Towards that, traditional approaches typically utilize adaptive mechanisms, which may suffer from scalability issues, increased computational overhead, and limited adaptability to diverse edge environments.","To address that, this paper instead leverages the observation that the computation offloading involves inherent functionalities such as node matching and service correlation to achieve data reshaping and proposes Federated learning based on computing Offloading (FlocOff) framework, to address data heterogeneity and resource-constrained challenges.","Specifically, FlocOff formulates the FL process with Non-IID data in edge scenarios and derives rigorous analysis on the impact of imbalanced data distribution.","Based on this, FlocOff decouples the optimization in two steps, namely : (1) Minimizes the Kullback-Leibler (KL) divergence via Computation Offloading scheduling (MKL-CO); (2) Minimizes the Communication Cost through Resource Allocation (MCC-RA).","Extensive experimental results demonstrate that the proposed FlocOff effectively improves model convergence and accuracy by 14.3\\%-32.7\\% while reducing data heterogeneity under various data distributions."],"url":"http://arxiv.org/abs/2405.18739v1","category":"cs.NI"}
{"created":"2024-05-29 03:32:06","title":"A two-phase model of galaxy formation: III. The formation of globular clusters","abstract":"We develop a model of globular cluster (GC) formation within the cosmological hierarchy of structure formation. The model is rooted in the `two-phase' scenario of galaxy formation developed in Paper-I, where the fast accretion of dark matter halos at high redshift leads to the formation of self-gravitating, turbulent gas clouds that subsequently fragment into dynamically hot systems of dense sub-clouds with masses $\\sim 10^6$-$10^7 M_\\odot$. Here we elaborate on the formation, evolution, and fate of these sub-clouds, and show that some of the sub-clouds can be compactified via two distinctive channels into a 'supernova-free' regime to form two distinct populations of GCs. The model is simple, characterized by a small number of free parameters underpinned by physical considerations, and can be efficiently implemented into cosmological N-body simulations to generate a coherent sample of halos, galaxies, and GCs. Our model can reproduce a range of observations on GCs, including the mass function, the size-mass relation, the frequency per unit host galaxy/halo mass, the bimodal metallicity distribution, and the spatial profile. Predictions for GCs are made for both the local Universe and for redshift up to $z \\approx 10$, and can be tested by upcoming observations.","sentences":["We develop a model of globular cluster (GC) formation within the cosmological hierarchy of structure formation.","The model is rooted in the `two-phase' scenario of galaxy formation developed in Paper-I, where the fast accretion of dark matter halos at high redshift leads to the formation of self-gravitating, turbulent gas clouds that subsequently fragment into dynamically hot systems of dense sub-clouds with masses $\\sim 10^6$-$10^7 M_\\odot$. Here we elaborate on the formation, evolution, and fate of these sub-clouds, and show that some of the sub-clouds can be compactified via two distinctive channels into a 'supernova-free' regime to form two distinct populations of GCs.","The model is simple, characterized by a small number of free parameters underpinned by physical considerations, and can be efficiently implemented into cosmological N-body simulations to generate a coherent sample of halos, galaxies, and GCs.","Our model can reproduce a range of observations on GCs, including the mass function, the size-mass relation, the frequency per unit host galaxy/halo mass, the bimodal metallicity distribution, and the spatial profile.","Predictions for GCs are made for both the local Universe and for redshift up to $z \\approx 10$, and can be tested by upcoming observations."],"url":"http://arxiv.org/abs/2405.18735v1","category":"astro-ph.GA"}
{"created":"2024-05-29 03:28:16","title":"PillarHist: A Quantization-aware Pillar Feature Encoder based on Height-aware Histogram","abstract":"Real-time and high-performance 3D object detection plays a critical role in autonomous driving and robotics. Recent pillar-based 3D object detectors have gained significant attention due to their compact representation and low computational overhead, making them suitable for onboard deployment and quantization. However, existing pillar-based detectors still suffer from information loss along height dimension and large numerical distribution difference during pillar feature encoding (PFE), which severely limits their performance and quantization potential. To address above issue, we first unveil the importance of different input information during PFE and identify the height dimension as a key factor in enhancing 3D detection performance. Motivated by this observation, we propose a height-aware pillar feature encoder named PillarHist. Specifically, PillarHist statistics the discrete distribution of points at different heights within one pillar. This simple yet effective design greatly preserves the information along the height dimension while significantly reducing the computation overhead of the PFE. Meanwhile, PillarHist also constrains the arithmetic distribution of PFE input to a stable range, making it quantization-friendly. Notably, PillarHist operates exclusively within the PFE stage to enhance performance, enabling seamless integration into existing pillar-based methods without introducing complex operations. Extensive experiments show the effectiveness of PillarHist in terms of both efficiency and performance.","sentences":["Real-time and high-performance 3D object detection plays a critical role in autonomous driving and robotics.","Recent pillar-based 3D object detectors have gained significant attention due to their compact representation and low computational overhead, making them suitable for onboard deployment and quantization.","However, existing pillar-based detectors still suffer from information loss along height dimension and large numerical distribution difference during pillar feature encoding (PFE), which severely limits their performance and quantization potential.","To address above issue, we first unveil the importance of different input information during PFE and identify the height dimension as a key factor in enhancing 3D detection performance.","Motivated by this observation, we propose a height-aware pillar feature encoder named PillarHist.","Specifically, PillarHist statistics the discrete distribution of points at different heights within one pillar.","This simple yet effective design greatly preserves the information along the height dimension while significantly reducing the computation overhead of the PFE.","Meanwhile, PillarHist also constrains the arithmetic distribution of PFE input to a stable range, making it quantization-friendly.","Notably, PillarHist operates exclusively within the PFE stage to enhance performance, enabling seamless integration into existing pillar-based methods without introducing complex operations.","Extensive experiments show the effectiveness of PillarHist in terms of both efficiency and performance."],"url":"http://arxiv.org/abs/2405.18734v1","category":"cs.CV"}
{"created":"2024-05-29 03:20:46","title":"Development of a Novel Impedance-Controlled Quasi-Direct-Drive Robotic Hand","abstract":"Most robotic hands and grippers rely on actuators with large gearboxes and force sensors for controlling gripping force. However, this might not be ideal for tasks that require the robot to interact with an unstructured and unknown environment. In this paper, we introduce a novel quasi-direct-drive two-fingered robotic hand with variable impedance control in the joint space and Cartesian space. The hand has a total of four degrees of freedom, backdrivable differential gear trains, and four brushless direct current (BLDC) motors. Motor torque is controlled through Field-Oriented Control (FOC) with current sensing. Variable impedance control enables the robotic hand to execute dexterous manipulation tasks safely during environment-robot and human-robot interactions. The quasi-direct-drive actuators eliminate the need for complex tactile/force sensors or precise motion planning when handling environmental contact. A majority-3D-printed assembly makes this a low-cost research platform built with affordable, readily available off-the-shelf components. Experimental validation demonstrates the robotic hand's capability for stable force-closure and form-closure grasps in the presence of disturbances, reliable in-hand manipulation, and safe dynamic manipulations despite contact with the environment.","sentences":["Most robotic hands and grippers rely on actuators with large gearboxes and force sensors for controlling gripping force.","However, this might not be ideal for tasks that require the robot to interact with an unstructured and unknown environment.","In this paper, we introduce a novel quasi-direct-drive two-fingered robotic hand with variable impedance control in the joint space and Cartesian space.","The hand has a total of four degrees of freedom, backdrivable differential gear trains, and four brushless direct current (BLDC) motors.","Motor torque is controlled through Field-Oriented Control (FOC) with current sensing.","Variable impedance control enables the robotic hand to execute dexterous manipulation tasks safely during environment-robot and human-robot interactions.","The quasi-direct-drive actuators eliminate the need for complex tactile/force sensors or precise motion planning when handling environmental contact.","A majority-3D-printed assembly makes this a low-cost research platform built with affordable, readily available off-the-shelf components.","Experimental validation demonstrates the robotic hand's capability for stable force-closure and form-closure grasps in the presence of disturbances, reliable in-hand manipulation, and safe dynamic manipulations despite contact with the environment."],"url":"http://arxiv.org/abs/2405.18730v1","category":"cs.RO"}
{"created":"2024-05-29 03:16:14","title":"Reverse the auditory processing pathway: Coarse-to-fine audio reconstruction from fMRI","abstract":"Drawing inspiration from the hierarchical processing of the human auditory system, which transforms sound from low-level acoustic features to high-level semantic understanding, we introduce a novel coarse-to-fine audio reconstruction method. Leveraging non-invasive functional Magnetic Resonance Imaging (fMRI) data, our approach mimics the inverse pathway of auditory processing. Initially, we utilize CLAP to decode fMRI data coarsely into a low-dimensional semantic space, followed by a fine-grained decoding into the high-dimensional AudioMAE latent space guided by semantic features. These fine-grained neural features serve as conditions for audio reconstruction through a Latent Diffusion Model (LDM). Validation on three public fMRI datasets-Brain2Sound, Brain2Music, and Brain2Speech-underscores the superiority of our coarse-to-fine decoding method over stand-alone fine-grained approaches, showcasing state-of-the-art performance in metrics like FD, FAD, and KL. Moreover, by employing semantic prompts during decoding, we enhance the quality of reconstructed audio when semantic features are suboptimal. The demonstrated versatility of our model across diverse stimuli highlights its potential as a universal brain-to-audio framework. This research contributes to the comprehension of the human auditory system, pushing boundaries in neural decoding and audio reconstruction methodologies.","sentences":["Drawing inspiration from the hierarchical processing of the human auditory system, which transforms sound from low-level acoustic features to high-level semantic understanding, we introduce a novel coarse-to-fine audio reconstruction method.","Leveraging non-invasive functional Magnetic Resonance Imaging (fMRI) data, our approach mimics the inverse pathway of auditory processing.","Initially, we utilize CLAP to decode fMRI data coarsely into a low-dimensional semantic space, followed by a fine-grained decoding into the high-dimensional AudioMAE latent space guided by semantic features.","These fine-grained neural features serve as conditions for audio reconstruction through a Latent Diffusion Model (LDM).","Validation on three public fMRI datasets-Brain2Sound, Brain2Music, and Brain2Speech-underscores the superiority of our coarse-to-fine decoding method over stand-alone fine-grained approaches, showcasing state-of-the-art performance in metrics like FD, FAD, and KL.","Moreover, by employing semantic prompts during decoding, we enhance the quality of reconstructed audio when semantic features are suboptimal.","The demonstrated versatility of our model across diverse stimuli highlights its potential as a universal brain-to-audio framework.","This research contributes to the comprehension of the human auditory system, pushing boundaries in neural decoding and audio reconstruction methodologies."],"url":"http://arxiv.org/abs/2405.18726v1","category":"cs.SD"}
{"created":"2024-05-29 03:16:12","title":"Can We Enhance the Quality of Mobile Crowdsensing Data Without Ground Truth?","abstract":"Mobile crowdsensing (MCS) has emerged as a prominent trend across various domains. However, ensuring the quality of the sensing data submitted by mobile users (MUs) remains a complex and challenging problem. To address this challenge, an advanced method is required to detect low-quality sensing data and identify malicious MUs that may disrupt the normal operations of an MCS system. Therefore, this article proposes a prediction- and reputation-based truth discovery (PRBTD) framework, which can separate low-quality data from high-quality data in sensing tasks. First, we apply a correlation-focused spatial-temporal transformer network to predict the ground truth of the input sensing data. Then, we extract the sensing errors of the data as features based on the prediction results to calculate the implications among the data. Finally, we design a reputation-based truth discovery (TD) module for identifying low-quality data with their implications. Given sensing data submitted by MUs, PRBTD can eliminate the data with heavy noise and identify malicious MUs with high accuracy. Extensive experimental results demonstrate that PRBTD outperforms the existing methods in terms of identification accuracy and data quality enhancement.","sentences":["Mobile crowdsensing (MCS) has emerged as a prominent trend across various domains.","However, ensuring the quality of the sensing data submitted by mobile users (MUs) remains a complex and challenging problem.","To address this challenge, an advanced method is required to detect low-quality sensing data and identify malicious MUs that may disrupt the normal operations of an MCS system.","Therefore, this article proposes a prediction- and reputation-based truth discovery (PRBTD) framework, which can separate low-quality data from high-quality data in sensing tasks.","First, we apply a correlation-focused spatial-temporal transformer network to predict the ground truth of the input sensing data.","Then, we extract the sensing errors of the data as features based on the prediction results to calculate the implications among the data.","Finally, we design a reputation-based truth discovery (TD) module for identifying low-quality data with their implications.","Given sensing data submitted by MUs, PRBTD can eliminate the data with heavy noise and identify malicious MUs with high accuracy.","Extensive experimental results demonstrate that PRBTD outperforms the existing methods in terms of identification accuracy and data quality enhancement."],"url":"http://arxiv.org/abs/2405.18725v1","category":"cs.LG"}
{"created":"2024-05-29 17:36:46","title":"Causal Inference for Balanced Incomplete Block Designs","abstract":"Researchers often turn to block randomization to increase the precision of their inference or due to practical considerations, such as in multi-site trials. However, if the number of treatments under consideration is large it might not be practical or even feasible to assign all treatments within each block. We develop novel inference results under the finite-population design-based framework for a natural alternative to the complete block design that does not require reducing the number of treatment arms, the balanced incomplete block design (BIBD). This includes deriving the properties of two estimators for BIBDs and proposing conservative variance estimators. To assist practitioners in understanding the trade-offs of using BIBDs over other designs, the precisions of resulting estimators are compared to standard estimators for the complete block design, the cluster-randomized design, and the completely randomized design. Simulations and a data illustration demonstrate the strengths and weaknesses of using BIBDs. This work highlights BIBDs as practical and currently underutilized designs.","sentences":["Researchers often turn to block randomization to increase the precision of their inference or due to practical considerations, such as in multi-site trials.","However, if the number of treatments under consideration is large it might not be practical or even feasible to assign all treatments within each block.","We develop novel inference results under the finite-population design-based framework for a natural alternative to the complete block design that does not require reducing the number of treatment arms, the balanced incomplete block design (BIBD).","This includes deriving the properties of two estimators for BIBDs and proposing conservative variance estimators.","To assist practitioners in understanding the trade-offs of using BIBDs over other designs, the precisions of resulting estimators are compared to standard estimators for the complete block design, the cluster-randomized design, and the completely randomized design.","Simulations and a data illustration demonstrate the strengths and weaknesses of using BIBDs.","This work highlights BIBDs as practical and currently underutilized designs."],"url":"http://arxiv.org/abs/2405.19312v1","category":"stat.ME"}
{"created":"2024-05-29 17:35:55","title":"The Effect of Tornadic Supercell Thunderstorms on the Atmospheric Muon Flux","abstract":"Tornadoes are severe weather phenomena characterized by a violently rotating column of air connecting the ground to a parent storm. Within the United States, hundreds of tornadoes occur every year. Despite this, the dynamics of tornado formation and propagation are not particularly well understood, in part due to the challenge of instrumentation: many existing instruments for measuring atmospheric properties are in-situ detectors, making deployment in or near an active or developing tornado difficult. Here, we combine local atmospheric and cosmic ray air shower simulation to explore the potential for remote measurement of the pressure field within tornado-producing supercell thunderstorms by examining directional variations of the atmospheric muon flux.","sentences":["Tornadoes are severe weather phenomena characterized by a violently rotating column of air connecting the ground to a parent storm.","Within the United States, hundreds of tornadoes occur every year.","Despite this, the dynamics of tornado formation and propagation are not particularly well understood, in part due to the challenge of instrumentation: many existing instruments for measuring atmospheric properties are in-situ detectors, making deployment in or near an active or developing tornado difficult.","Here, we combine local atmospheric and cosmic ray air shower simulation to explore the potential for remote measurement of the pressure field within tornado-producing supercell thunderstorms by examining directional variations of the atmospheric muon flux."],"url":"http://arxiv.org/abs/2405.19311v1","category":"astro-ph.HE"}
{"created":"2024-05-29 17:33:34","title":"SDPRLayers: Certifiable Backpropagation Through Polynomial Optimization Problems in Robotics","abstract":"Differentiable optimization is a powerful new paradigm capable of reconciling model-based and learning-based approaches in robotics. However, the majority of robotics optimization problems are non-convex and current differentiable optimization techniques are therefore prone to convergence to local minima. When this occurs, the gradients provided by these existing solvers can be wildly inaccurate and will ultimately corrupt the training process. On the other hand, any non-convex robotics problems can be framed as polynomial optimization problems and, in turn, admit convex relaxations that can be used to recover a global solution via so-called certifiably correct methods. We present SDPRLayers, an approach that leverages these methods as well as state-of-the-art convex implicit differentiation techniques to provide certifiably correct gradients throughout the training process. We introduce this approach and showcase theoretical results that provide conditions under which correctness of the gradients is guaranteed. We demonstrate our approach on two simple-but-demonstrative simulated examples, which expose the potential pitfalls of existing, state-of-the-art, differentiable optimization methods. We apply our method in a real-world application: we train a deep neural network to detect image keypoints for robot localization in challenging lighting conditions. An open-source, PyTorch implementation of SDPRLayers will be made available upon paper acceptance.","sentences":["Differentiable optimization is a powerful new paradigm capable of reconciling model-based and learning-based approaches in robotics.","However, the majority of robotics optimization problems are non-convex and current differentiable optimization techniques are therefore prone to convergence to local minima.","When this occurs, the gradients provided by these existing solvers can be wildly inaccurate and will ultimately corrupt the training process.","On the other hand, any non-convex robotics problems can be framed as polynomial optimization problems and, in turn, admit convex relaxations that can be used to recover a global solution via so-called certifiably correct methods.","We present SDPRLayers, an approach that leverages these methods as well as state-of-the-art convex implicit differentiation techniques to provide certifiably correct gradients throughout the training process.","We introduce this approach and showcase theoretical results that provide conditions under which correctness of the gradients is guaranteed.","We demonstrate our approach on two simple-but-demonstrative simulated examples, which expose the potential pitfalls of existing, state-of-the-art, differentiable optimization methods.","We apply our method in a real-world application: we train a deep neural network to detect image keypoints for robot localization in challenging lighting conditions.","An open-source, PyTorch implementation of SDPRLayers will be made available upon paper acceptance."],"url":"http://arxiv.org/abs/2405.19309v1","category":"cs.RO"}
{"created":"2024-05-29 17:27:30","title":"Safe and Efficient Estimation for Robotics through the Optimal Use of Resources","abstract":"In order to operate in and interact with the physical world, robots need to have estimates of the current and future state of the environment. We thus equip robots with sensors and build models and algorithms that, given some measurements, produce estimates of the current or future states. Environments can be unpredictable and sensors are not perfect. Therefore, it is important to both use all information available, and to do so optimally: making sure that we get the best possible answer from the amount of information we have. However, in prevalent research, uncommon sensors, such as sound or radio-frequency signals, are commonly ignored for state estimation; and the most popular solvers employed to produce state estimates are only of local nature, meaning they may produce suboptimal estimates for the typically non-convex estimation problems. My research aims to use resources more optimally, by building on 1) multi-modality: using ubiquitous RF transceivers and microphones to support state estimation, 2) building certifiably optimal solvers and 3) learning and improving adequate models from data.","sentences":["In order to operate in and interact with the physical world, robots need to have estimates of the current and future state of the environment.","We thus equip robots with sensors and build models and algorithms that, given some measurements, produce estimates of the current or future states.","Environments can be unpredictable and sensors are not perfect.","Therefore, it is important to both use all information available, and to do so optimally: making sure that we get the best possible answer from the amount of information we have.","However, in prevalent research, uncommon sensors, such as sound or radio-frequency signals, are commonly ignored for state estimation; and the most popular solvers employed to produce state estimates are only of local nature, meaning they may produce suboptimal estimates for the typically non-convex estimation problems.","My research aims to use resources more optimally, by building on 1) multi-modality: using ubiquitous RF transceivers and microphones to support state estimation, 2) building certifiably optimal solvers and 3) learning and improving adequate models from data."],"url":"http://arxiv.org/abs/2405.19301v1","category":"cs.RO"}
{"created":"2024-05-29 17:25:59","title":"Genuine Retrieval of the AGN Host Stellar Population (GRAHSP)","abstract":"The assembly and co-evolution of supermassive black holes (SMBH) and their host galaxy stellar population is a key open questions in galaxy evolution. Stellar mass ($M_\\star$) and star formation rate (SFR), are inferred by modeling the spectral energy distribution (SED). For galaxies triggering SMBH activity, the active galactic nucleus (AGN) contaminates the light at all wavelengths, hampering the inference of galaxy parameters. Incomplete AGN templates can lead to systematic overestimates of the stellar mass, biasing our understanding of AGN-galaxy co-evolution. This challenge has gained further impetus with the advent of sensitive wide-area surveys with millions of luminous AGN, including by eROSITA, Euclid and LSST. We aim to estimate the accuracy and bias of AGN host galaxy parameters and improve upon existing techniques. This work makes two contributions: 1) a new SED fitting code, GRAHSP, with a flexible, empirically motivated AGN model including a power law continuum emission lines, a FeII forest and a flexible infrared torus. We verify that our model reproduces published X-ray to infrared SEDs of AGN to better than 20\\% accuracy. A fully Bayesian fit with nested sampling includes uncertainties in the model and the data, making the inference highly robust. 2) we created a benchmark photometric dataset where pure quasars are merged with non-AGN pure galaxies into a hybrid (Chimera) object but with known galaxy and AGN properties. Comparing the true and retrieved $M_\\star$, SFR and AGN luminosities shows that previous codes systematically over-estimate $M_\\star$ and SFR by 0.5 dex with a wide scatter of 0.7 dex, at AGN luminosities above 10^44 erg/s. In contrast, GRAHSP shows no bias on $M_\\star$ and SFR. GRAHSP also estimates more realistic uncertainties. GRAHSP enables characterization of the environmental conditions conducive to black hole growth. (abridged)","sentences":["The assembly and co-evolution of supermassive black holes (SMBH) and their host galaxy stellar population is a key open questions in galaxy evolution.","Stellar mass ($M_\\star$) and star formation rate (SFR), are inferred by modeling the spectral energy distribution (SED).","For galaxies triggering SMBH activity, the active galactic nucleus (AGN) contaminates the light at all wavelengths, hampering the inference of galaxy parameters.","Incomplete AGN templates can lead to systematic overestimates of the stellar mass, biasing our understanding of AGN-galaxy co-evolution.","This challenge has gained further impetus with the advent of sensitive wide-area surveys with millions of luminous AGN, including by eROSITA, Euclid and LSST.","We aim to estimate the accuracy and bias of AGN host galaxy parameters and improve upon existing techniques.","This work makes two contributions: 1) a new SED fitting code, GRAHSP, with a flexible, empirically motivated AGN model including a power law continuum emission lines, a FeII forest and a flexible infrared torus.","We verify that our model reproduces published X-ray to infrared SEDs of AGN to better than 20\\% accuracy.","A fully Bayesian fit with nested sampling includes uncertainties in the model and the data, making the inference highly robust.","2) we created a benchmark photometric dataset where pure quasars are merged with non-AGN pure galaxies into a hybrid (Chimera) object but with known galaxy and AGN properties.","Comparing the true and retrieved $M_\\star$, SFR and AGN luminosities shows that previous codes systematically over-estimate $M_\\star$ and SFR by 0.5 dex with a wide scatter of 0.7 dex, at AGN luminosities above 10^44 erg/s. In contrast, GRAHSP shows no bias on $M_\\star$ and SFR.","GRAHSP also estimates more realistic uncertainties.","GRAHSP enables characterization of the environmental conditions conducive to black hole growth.","(abridged)"],"url":"http://arxiv.org/abs/2405.19297v1","category":"astro-ph.GA"}
{"created":"2024-05-29 17:13:22","title":"An $\\textit{ab initio}$ recipe for taming nuclear-structure dependence of $ V_{ud} $: the $ {}^{10}\\mathrm{C} \\rightarrow {}^{10}\\mathrm{B} $ superallowed transition","abstract":"We report the first ab initio calculation of the nuclear-structure-dependent radiative correction $ \\delta_{ \\mathrm{NS} } $ to the $ {}^{10}\\mathrm{C} \\rightarrow {}^{10}\\mathrm{B} $ superallowed transition, computed with the no-core shell model and chiral effective field theory. We obtain $\\delta_{ \\mathrm{NS} } = - 0.400 (29)_{ \\mathrm{nuc} } (12)_{n,\\mathrm{el} } $ with a $1.7$-times reduction in the nuclear uncertainty when compared to the current literature estimate based on the shell model and Fermi gas picture. This work paves the way for a precise determination of $V_{ud}$ from superallowed beta decays.","sentences":["We report the first ab initio calculation of the nuclear-structure-dependent radiative correction $ \\delta_{ \\mathrm{NS} } $ to the $ {}^{10}\\mathrm{C} \\rightarrow {}^{10}\\mathrm{B} $ superallowed transition, computed with the no-core shell model and chiral effective field theory.","We obtain $\\delta_{ \\mathrm{NS} } = - 0.400 (29)_{ \\mathrm{nuc} } (12)_{n,\\mathrm{el} } $ with a $1.7$-times reduction in the nuclear uncertainty when compared to the current literature estimate based on the shell model and Fermi gas picture.","This work paves the way for a precise determination of $V_{ud}$ from superallowed beta decays."],"url":"http://arxiv.org/abs/2405.19281v1","category":"nucl-th"}
{"created":"2024-05-29 17:03:31","title":"Mitigating Disparate Impact of Differential Privacy in Federated Learning through Robust Clustering","abstract":"Federated Learning (FL) is a decentralized machine learning (ML) approach that keeps data localized and often incorporates Differential Privacy (DP) to enhance privacy guarantees. Similar to previous work on DP in ML, we observed that differentially private federated learning (DPFL) introduces performance disparities, particularly affecting minority groups. Recent work has attempted to address performance fairness in vanilla FL through clustering, but this method remains sensitive and prone to errors, which are further exacerbated by the DP noise in DPFL. To fill this gap, in this paper, we propose a novel clustered DPFL algorithm designed to effectively identify clients' clusters in highly heterogeneous settings while maintaining high accuracy with DP guarantees. To this end, we propose to cluster clients based on both their model updates and training loss values. Our proposed approach also addresses the server's uncertainties in clustering clients' model updates by employing larger batch sizes along with Gaussian Mixture Model (GMM) to alleviate the impact of noise and potential clustering errors, especially in privacy-sensitive scenarios. We provide theoretical analysis of the effectiveness of our proposed approach. We also extensively evaluate our approach across diverse data distributions and privacy budgets and show its effectiveness in mitigating the disparate impact of DP in FL settings with a small computational cost.","sentences":["Federated Learning (FL) is a decentralized machine learning (ML) approach that keeps data localized and often incorporates Differential Privacy (DP) to enhance privacy guarantees.","Similar to previous work on DP in ML, we observed that differentially private federated learning (DPFL) introduces performance disparities, particularly affecting minority groups.","Recent work has attempted to address performance fairness in vanilla FL through clustering, but this method remains sensitive and prone to errors, which are further exacerbated by the DP noise in DPFL.","To fill this gap, in this paper, we propose a novel clustered DPFL algorithm designed to effectively identify clients' clusters in highly heterogeneous settings while maintaining high accuracy with DP guarantees.","To this end, we propose to cluster clients based on both their model updates and training loss values.","Our proposed approach also addresses the server's uncertainties in clustering clients' model updates by employing larger batch sizes along with Gaussian Mixture Model (GMM) to alleviate the impact of noise and potential clustering errors, especially in privacy-sensitive scenarios.","We provide theoretical analysis of the effectiveness of our proposed approach.","We also extensively evaluate our approach across diverse data distributions and privacy budgets and show its effectiveness in mitigating the disparate impact of DP in FL settings with a small computational cost."],"url":"http://arxiv.org/abs/2405.19272v1","category":"cs.LG"}
{"created":"2024-05-29 16:07:39","title":"Valid Conformal Prediction for Dynamic GNNs","abstract":"Graph neural networks (GNNs) are powerful black-box models which have shown impressive empirical performance. However, without any form of uncertainty quantification, it can be difficult to trust such models in high-risk scenarios. Conformal prediction aims to address this problem, however, an assumption of exchangeability is required for its validity which has limited its applicability to static graphs and transductive regimes. We propose to use unfolding, which allows any existing static GNN to output a dynamic graph embedding with exchangeability properties. Using this, we extend the validity of conformal prediction to dynamic GNNs in both transductive and semi-inductive regimes. We provide a theoretical guarantee of valid conformal prediction in these cases and demonstrate the empirical validity, as well as the performance gains, of unfolded GNNs against standard GNN architectures on both simulated and real datasets.","sentences":["Graph neural networks (GNNs) are powerful black-box models which have shown impressive empirical performance.","However, without any form of uncertainty quantification, it can be difficult to trust such models in high-risk scenarios.","Conformal prediction aims to address this problem, however, an assumption of exchangeability is required for its validity which has limited its applicability to static graphs and transductive regimes.","We propose to use unfolding, which allows any existing static GNN to output a dynamic graph embedding with exchangeability properties.","Using this, we extend the validity of conformal prediction to dynamic GNNs in both transductive and semi-inductive regimes.","We provide a theoretical guarantee of valid conformal prediction in these cases and demonstrate the empirical validity, as well as the performance gains, of unfolded GNNs against standard GNN architectures on both simulated and real datasets."],"url":"http://arxiv.org/abs/2405.19230v1","category":"stat.ML"}
{"created":"2024-05-29 16:00:19","title":"LoByITFL: Low Communication Secure and Private Federated Learning","abstract":"Federated Learning (FL) faces several challenges, such as the privacy of the clients data and security against Byzantine clients. Existing works treating privacy and security jointly make sacrifices on the privacy guarantee. In this work, we introduce LoByITFL, the first communication-efficient Information-Theoretic (IT) private and secure FL scheme that makes no sacrifices on the privacy guarantees while ensuring security against Byzantine adversaries. The key ingredients are a small and representative dataset available to the federator, a careful transformation of the FLTrust algorithm and the use of a trusted third party only in a one-time preprocessing phase before the start of the learning algorithm. We provide theoretical guarantees on privacy and Byzantine-resilience, and provide convergence guarantee and experimental results validating our theoretical findings.","sentences":["Federated Learning (FL) faces several challenges, such as the privacy of the clients data and security against Byzantine clients.","Existing works treating privacy and security jointly make sacrifices on the privacy guarantee.","In this work, we introduce LoByITFL, the first communication-efficient Information-Theoretic (IT) private and secure FL scheme that makes no sacrifices on the privacy guarantees while ensuring security against Byzantine adversaries.","The key ingredients are a small and representative dataset available to the federator, a careful transformation of the FLTrust algorithm and the use of a trusted third party only in a one-time preprocessing phase before the start of the learning algorithm.","We provide theoretical guarantees on privacy and Byzantine-resilience, and provide convergence guarantee and experimental results validating our theoretical findings."],"url":"http://arxiv.org/abs/2405.19217v1","category":"cs.IT"}
{"created":"2024-05-29 15:21:45","title":"Observation of Significant Photosynthesis in Garden Cress and Cyanobacteria under Simulated Illumination from a K Dwarf Star","abstract":"Stars with about 45 to 80% the mass of the Sun, so-called K dwarf stars, have previously been proposed as optimal host stars in the search for habitable extrasolar worlds. These stars are abundant, have stable luminosities over billions of years longer than Sun-like stars, and offer favorable space environmental conditions. So far, the theoretical and experimental focus on exoplanet habitability has been on even less massive, though potentially less hospitable red dwarf stars. Here we present the first experimental data on the responses of photosynthetic organisms to a simulated K dwarf spectrum. We find that garden cress Lepidium sativum under K-dwarf radiation exhibits comparable growth and photosynthetic efficiency as under solar illumination on Earth. The cyanobacterium Chroococcidiopsis sp. CCMEE 029 exhibits significantly higher photosynthetic efficiency and culture growth under K dwarf radiation compared to solar conditions. Our findings of the affirmative responses of these two photosynthetic organisms to K dwarf radiation suggest that exoplanets in the habitable zones around such stars deserve high priority in the search for extrasolar life.","sentences":["Stars with about 45 to 80% the mass of the Sun, so-called K dwarf stars, have previously been proposed as optimal host stars in the search for habitable extrasolar worlds.","These stars are abundant, have stable luminosities over billions of years longer than Sun-like stars, and offer favorable space environmental conditions.","So far, the theoretical and experimental focus on exoplanet habitability has been on even less massive, though potentially less hospitable red dwarf stars.","Here we present the first experimental data on the responses of photosynthetic organisms to a simulated K dwarf spectrum.","We find that garden cress Lepidium sativum under K-dwarf radiation exhibits comparable growth and photosynthetic efficiency as under solar illumination on Earth.","The cyanobacterium Chroococcidiopsis sp.","CCMEE 029 exhibits significantly higher photosynthetic efficiency and culture growth under K dwarf radiation compared to solar conditions.","Our findings of the affirmative responses of these two photosynthetic organisms to K dwarf radiation suggest that exoplanets in the habitable zones around such stars deserve high priority in the search for extrasolar life."],"url":"http://arxiv.org/abs/2405.19180v1","category":"astro-ph.EP"}
{"created":"2024-05-29 15:17:24","title":"Strong solution of the three-dimensional $(3D)$ incompressible magneto-hydrodynamic $(MHD)$ equationss with a modified damping","abstract":"This study delves into a comprehensive examination of the three-dimensional $(3D)$ incompressible magneto-hydrodynamic $(MHD)$ equations in $H^{1}(\\R^{3})$. The modification involves incorporating a power term in the nonlinear convection component, a particularly relevant adjustment in porous media scenarios, especially when the fluid adheres to the Darcy-Forchheimer law instead of the conventional Darcy law. Our main contributions include establishing global existence over time and demonstrating the uniqueness of solutions. It is important to note that these achievements are obtained with smallness conditions on the initial data, but under the condition that $\\beta >3$ and $\\alpha>0$. However, when $\\beta=3$, the problem is limited to the case $0<\\alpha<\\frac{1}{2}$ as the above inequality is unsolvable for these values of $\\alpha$ using our method. To support our statement, we will add a \"slight disturbance\" of the function f of the type $f(z)=log(e+z^{2})$ or $\\log(\\log(e^{e}+z^{2}))$ or even $\\log(\\log(\\log((e^{e})^{e}+z^{2})))$.","sentences":["This study delves into a comprehensive examination of the three-dimensional $(3D)$ incompressible magneto-hydrodynamic $(MHD)$ equations in $H^{1}(\\R^{3})$. The modification involves incorporating a power term in the nonlinear convection component, a particularly relevant adjustment in porous media scenarios, especially when the fluid adheres to the Darcy-Forchheimer law instead of the conventional Darcy law.","Our main contributions include establishing global existence over time and demonstrating the uniqueness of solutions.","It is important to note that these achievements are obtained with smallness conditions on the initial data, but under the condition that $\\beta >3$ and $\\alpha>0$. However, when $\\beta=3$, the problem is limited to the case $0<\\alpha<\\frac{1}{2}$ as the above inequality is unsolvable for these values of $\\alpha$ using our method.","To support our statement, we will add a \"slight disturbance\" of the function f of the type $f(z)=log(e+z^{2})$ or $\\log(\\log(e^{e}+z^{2}))$ or even $\\log(\\log(\\log((e^{e})^{e}+z^{2})))$."],"url":"http://arxiv.org/abs/2405.19174v1","category":"math.AP"}
{"created":"2024-05-29 15:02:44","title":"Origin of the density wave instability in trilayer nickelate La4Ni3O10 revealed by optical and ultrafast spectroscopy","abstract":"Here we employed optical spectroscopy and ultrafast reflectivity measurements to investigate the density wave instability of trilayer nickelate La4Ni3O10 at ambient pressure. Our optical spectroscopy measurements indicate that La4Ni3O10 is metallic with a large plasma frequency at room temperature. As the temperature decreases, we observe the formation of an energy gap in reflectivity below TDW, signaling the charge/spin density wave transition. The Drude component was largely removed due to the gap opening in the Fermi surface. Our Drude-Lorentz analysis reveals that the energy gap in La4Ni3O10 is approximately 61 meV, which is three times larger than that obtained from ARPES measurements. The density wave gap feature is more prominent than that observed in bilayer nickelate La3Ni2O7, suggesting more carriers are gapped at the Fermi surface across the density wave transition. By comparing the measured plasma frequency with the first-principles calculation, we categorize La4Ni3O10 as a moderately electronic correlation material, similar to the parent compound of iron-based superconductors, however, being weaker than the bilayer nickelate La3Ni2O7. Our ultrafast pump-probe experiments also show that the relaxation time diverges near the transition temperature. By analyzing the amplitude and relaxation time with the Rothwarf-Taylor model, we estimate the energy gap to be 58 meV, which agrees with the result of optical spectroscopy. The more prominent gap feature and weaker electronic correlation might be the cause of a lower superconductivity transition temperature in La4Ni3O10 under high pressure. These findings significantly contribute to understanding the origin of density wave and superconductivity in trilayer nickelate La4Ni3O10.","sentences":["Here we employed optical spectroscopy and ultrafast reflectivity measurements to investigate the density wave instability of trilayer nickelate La4Ni3O10 at ambient pressure.","Our optical spectroscopy measurements indicate that La4Ni3O10 is metallic with a large plasma frequency at room temperature.","As the temperature decreases, we observe the formation of an energy gap in reflectivity below TDW, signaling the charge/spin density wave transition.","The Drude component was largely removed due to the gap opening in the Fermi surface.","Our Drude-Lorentz analysis reveals that the energy gap in La4Ni3O10 is approximately 61 meV, which is three times larger than that obtained from ARPES measurements.","The density wave gap feature is more prominent than that observed in bilayer nickelate La3Ni2O7, suggesting more carriers are gapped at the Fermi surface across the density wave transition.","By comparing the measured plasma frequency with the first-principles calculation, we categorize La4Ni3O10 as a moderately electronic correlation material, similar to the parent compound of iron-based superconductors, however, being weaker than the bilayer nickelate La3Ni2O7.","Our ultrafast pump-probe experiments also show that the relaxation time diverges near the transition temperature.","By analyzing the amplitude and relaxation time with the Rothwarf-Taylor model, we estimate the energy gap to be 58 meV, which agrees with the result of optical spectroscopy.","The more prominent gap feature and weaker electronic correlation might be the cause of a lower superconductivity transition temperature in La4Ni3O10 under high pressure.","These findings significantly contribute to understanding the origin of density wave and superconductivity in trilayer nickelate La4Ni3O10."],"url":"http://arxiv.org/abs/2405.19161v1","category":"cond-mat.str-el"}
{"created":"2024-05-29 15:00:19","title":"Beyond Discrepancy: A Closer Look at the Theory of Distribution Shift","abstract":"Many machine learning models appear to deploy effortlessly under distribution shift, and perform well on a target distribution that is considerably different from the training distribution. Yet, learning theory of distribution shift bounds performance on the target distribution as a function of the discrepancy between the source and target, rarely guaranteeing high target accuracy. Motivated by this gap, this work takes a closer look at the theory of distribution shift for a classifier from a source to a target distribution. Instead of relying on the discrepancy, we adopt an Invariant-Risk-Minimization (IRM)-like assumption connecting the distributions, and characterize conditions under which data from a source distribution is sufficient for accurate classification of the target. When these conditions are not met, we show when only unlabeled data from the target is sufficient, and when labeled target data is needed. In all cases, we provide rigorous theoretical guarantees in the large sample regime.","sentences":["Many machine learning models appear to deploy effortlessly under distribution shift, and perform well on a target distribution that is considerably different from the training distribution.","Yet, learning theory of distribution shift bounds performance on the target distribution as a function of the discrepancy between the source and target, rarely guaranteeing high target accuracy.","Motivated by this gap, this work takes a closer look at the theory of distribution shift for a classifier from a source to a target distribution.","Instead of relying on the discrepancy, we adopt an Invariant-Risk-Minimization (IRM)-like assumption connecting the distributions, and characterize conditions under which data from a source distribution is sufficient for accurate classification of the target.","When these conditions are not met, we show when only unlabeled data from the target is sufficient, and when labeled target data is needed.","In all cases, we provide rigorous theoretical guarantees in the large sample regime."],"url":"http://arxiv.org/abs/2405.19156v1","category":"cs.LG"}
{"created":"2024-05-29 14:51:19","title":"L-Estimation in Instrumental Variables Regression for Censored Data in Presence of Endogeneity and Dependent Errors","abstract":"In this article, we propose L-estimators of the unknown parameters in the instrumental variables regression in the presence of censored data under endogeneity. We allow the random errors involved in the model to be dependent. The proposed estimation procedure is a two-stage procedure, and the large sample properties of the proposed estimators are established. The utility of the proposed methodology is demonstrated for various simulated data and a benchmark real data set.","sentences":["In this article, we propose L-estimators of the unknown parameters in the instrumental variables regression in the presence of censored data under endogeneity.","We allow the random errors involved in the model to be dependent.","The proposed estimation procedure is a two-stage procedure, and the large sample properties of the proposed estimators are established.","The utility of the proposed methodology is demonstrated for various simulated data and a benchmark real data set."],"url":"http://arxiv.org/abs/2405.19145v1","category":"stat.ME"}
{"created":"2024-05-29 14:40:24","title":"A quantum implementation of high-order power method for estimating geometric entanglement of pure states","abstract":"Entanglement is one of the fundamental properties of a quantum state and is a crucial differentiator between classical and quantum computation. There are many ways to define entanglement and its measure, depending on the problem or application under consideration. Each of these measures may be computed or approximated by multiple methods. However, hardly any of these methods can be run on near-term quantum hardware. This work presents a quantum adaptation of the iterative higher-order power method for estimating the geometric measure of entanglement of multi-qubit pure states using rank-1 tensor approximation. This method is executable on current (hybrid) quantum hardware and does not depend on quantum memory. We study the effect of noise on the algorithm using a simple theoretical model based on the standard depolarising channel. This model allows us to post hoc mitigate the effects of noise on the results of the computation.","sentences":["Entanglement is one of the fundamental properties of a quantum state and is a crucial differentiator between classical and quantum computation.","There are many ways to define entanglement and its measure, depending on the problem or application under consideration.","Each of these measures may be computed or approximated by multiple methods.","However, hardly any of these methods can be run on near-term quantum hardware.","This work presents a quantum adaptation of the iterative higher-order power method for estimating the geometric measure of entanglement of multi-qubit pure states using rank-1 tensor approximation.","This method is executable on current (hybrid) quantum hardware and does not depend on quantum memory.","We study the effect of noise on the algorithm using a simple theoretical model based on the standard depolarising channel.","This model allows us to post hoc mitigate the effects of noise on the results of the computation."],"url":"http://arxiv.org/abs/2405.19134v1","category":"quant-ph"}
{"created":"2024-05-29 14:35:57","title":"Federated Assemblies","abstract":"A citizens' assembly is a group of people who are randomly selected to represent a larger population in a deliberation. While this approach has successfully strengthened democracy, it has certain limitations that suggest the need for assemblies to form and associate more organically. In response, we propose federated assemblies, where assemblies are interconnected, and each parent assembly is selected from members of its child assemblies. The main technical challenge is to develop random selection algorithms that meet new representation constraints inherent in this hierarchical structure. We design and analyze several algorithms that provide different representation guarantees under various assumptions on the structure of the underlying graph.","sentences":["A citizens' assembly is a group of people who are randomly selected to represent a larger population in a deliberation.","While this approach has successfully strengthened democracy, it has certain limitations that suggest the need for assemblies to form and associate more organically.","In response, we propose federated assemblies, where assemblies are interconnected, and each parent assembly is selected from members of its child assemblies.","The main technical challenge is to develop random selection algorithms that meet new representation constraints inherent in this hierarchical structure.","We design and analyze several algorithms that provide different representation guarantees under various assumptions on the structure of the underlying graph."],"url":"http://arxiv.org/abs/2405.19129v1","category":"cs.GT"}
{"created":"2024-05-29 14:31:39","title":"Early Detection of Critical Urban Events using Mobile Phone Network Data","abstract":"Network Signalling Data (NSD) have the potential to provide continuous spatio-temporal information about the presence, mobility, and usage patterns of cell phone services by individuals. Such information is invaluable for monitoring large urban areas and supporting the implementation of decision-making services. When analyzed in real time, NSD can enable the early detection of critical urban events, including fires, large accidents, stampedes, terrorist attacks, and sports and leisure gatherings, especially if these events significantly impact mobile phone network activity in the affected areas. This paper presents empirical evidence that advanced NSD can detect anomalies in mobile traffic service consumption, attributable to critical urban events, with fine spatial and temporal resolutions. We introduce two methodologies for real-time anomaly detection from multivariate time series extracted from large-scale NSD, utilizing a range of algorithms adapted from the state-of-the-art in unsupervised machine learning techniques for anomaly detection. Our research includes a comprehensive quantitative evaluation of these algorithms on a large-scale dataset of NSD service consumption for the Paris region. The evaluation uses an original dataset of documented critical or unusual urban events. This dataset has been built as a ground truth basis for assessing the algorithms performance. The obtained results demonstrate that our framework can detect unusual events almost instantaneously and locate the affected areas with high precision, largely outperforming random classifiers. This efficiency and effectiveness underline the potential of NSD-based anomaly detection in significantly enhancing emergency response strategies and urban planning.","sentences":["Network Signalling Data (NSD) have the potential to provide continuous spatio-temporal information about the presence, mobility, and usage patterns of cell phone services by individuals.","Such information is invaluable for monitoring large urban areas and supporting the implementation of decision-making services.","When analyzed in real time, NSD can enable the early detection of critical urban events, including fires, large accidents, stampedes, terrorist attacks, and sports and leisure gatherings, especially if these events significantly impact mobile phone network activity in the affected areas.","This paper presents empirical evidence that advanced NSD can detect anomalies in mobile traffic service consumption, attributable to critical urban events, with fine spatial and temporal resolutions.","We introduce two methodologies for real-time anomaly detection from multivariate time series extracted from large-scale NSD, utilizing a range of algorithms adapted from the state-of-the-art in unsupervised machine learning techniques for anomaly detection.","Our research includes a comprehensive quantitative evaluation of these algorithms on a large-scale dataset of NSD service consumption for the Paris region.","The evaluation uses an original dataset of documented critical or unusual urban events.","This dataset has been built as a ground truth basis for assessing the algorithms performance.","The obtained results demonstrate that our framework can detect unusual events almost instantaneously and locate the affected areas with high precision, largely outperforming random classifiers.","This efficiency and effectiveness underline the potential of NSD-based anomaly detection in significantly enhancing emergency response strategies and urban planning."],"url":"http://arxiv.org/abs/2405.19125v1","category":"cs.CY"}
{"created":"2024-05-29 14:24:35","title":"MHD simulations of the space weather in Proxima b: Habitability conditions and radio emission","abstract":"The habitability of exoplanets hosted by M-dwarf stars dramatically depends on their space weather. We present 3D magneto-hydrodynamic simulations to characterise the magneto-plasma environment and thus the habitability of the Earth-like planet Proxima b when it is subject to both calm and extreme (CME-like) space weather conditions. We study the role of the stellar wind and planetary magnetic field, and determine the radio emission arising from the interaction between the stellar wind of Proxima and the magnetosphere of its planet Proxima b. We find that if Prox b has a magnetic field similar to that of the Earth ($B_{\\rm p} = B_\\oplus \\approx 0.32$ G) or larger, the magnetopause standoff distance is large enough to shield the surface from the stellar wind for essentially any planetary tilt but the most extreme values (close to $90^{\\circ} $), under a calm space weather. Even if Proxima b is subject to more extreme space weather conditions, the planet is well shielded by an Earth-like magnetosphere ($B_{\\rm p} \\approx B_\\oplus$; $ \\approx 23.5^{\\circ}$), or if it has tilt smaller than that of the Earth. For calm space weather conditions, the radio emission caused by the day-side reconnection regions can be as high as 7$\\times10^{19}$ erg s$^{-1}$ in the super-Alfv\\'enic regime, and is on average almost an order of magnitude larger than the radio emission in the sub-Alfv\\'enic cases, due to the much larger contribution of the bow shock. We also find that the energy dissipation at the bow shock is independent of the angle between the planet's magnetic dipole and the incident stellar wind flow. If Prox b is subject to extreme space weather conditions, the radio emission is more than two orders of magnitude larger than under calm space weather conditions. This result yields expectations for a direct detection--from Earth--in radio of giant planets in close-in orbits.","sentences":["The habitability of exoplanets hosted by M-dwarf stars dramatically depends on their space weather.","We present 3D magneto-hydrodynamic simulations to characterise the magneto-plasma environment and thus the habitability of the Earth-like planet Proxima b when it is subject to both calm and extreme (CME-like) space weather conditions.","We study the role of the stellar wind and planetary magnetic field, and determine the radio emission arising from the interaction between the stellar wind of Proxima and the magnetosphere of its planet Proxima b.","We find that if Prox b has a magnetic field similar to that of the Earth ($B_{\\rm p} =","B_\\oplus","\\approx 0.32$ G) or larger, the magnetopause standoff distance is large enough to shield the surface from the stellar wind for essentially any planetary tilt but the most extreme values (close to $90^{\\circ} $), under a calm space weather.","Even if Proxima b is subject to more extreme space weather conditions, the planet is well shielded by an Earth-like magnetosphere ($B_{\\rm p} \\approx B_\\oplus$; $ \\approx 23.5^{\\circ}$), or if it has tilt smaller than that of the Earth.","For calm space weather conditions, the radio emission caused by the day-side reconnection regions can be as high as 7$\\times10^{19}$ erg s$^{-1}$ in the super-Alfv\\'enic regime, and is on average almost an order of magnitude larger than the radio emission in the sub-Alfv\\'enic cases, due to the much larger contribution of the bow shock.","We also find that the energy dissipation at the bow shock is independent of the angle between the planet's magnetic dipole and the incident stellar wind flow.","If Prox b is subject to extreme space weather conditions, the radio emission is more than two orders of magnitude larger than under calm space weather conditions.","This result yields expectations for a direct detection--from Earth--in radio of giant planets in close-in orbits."],"url":"http://arxiv.org/abs/2405.19116v1","category":"astro-ph.EP"}
{"created":"2024-05-29 14:07:03","title":"Annealed Calder\u00f3n-Zygmund estimates for elliptic operators with random coefficients on $C^{1}$ domains","abstract":"Concerned with elliptic operators with stationary random coefficients governed by linear or nonlinear mixing conditions and bounded (or unbounded) $C^1$ domains, this paper mainly studies (weighted) annealed Calder\\'on-Zygmund estimates, some of which are new even in a periodic setting. Stronger than some classical results derived by a perturbation argument in the deterministic case, our results own a scaling-invariant property, which additionally requires the non-perturbation method (based upon a quantitative homogenization theory and a set of functional analysis techniques) recently developed by M. Joisen and F. Otto \\cite{Josien-Otto22}. To handle boundary estimates in certain UMD (unconditional martingale differences) spaces, we hand them over to Shen's real arguments \\cite{Shen05, Shen23} instead of using Mikhlin's theorem. As a by-product, we also established ``resolvent estimates''. The potentially attractive part is to show how the two powerful kernel-free methods work together to make the results clean and robust.","sentences":["Concerned with elliptic operators with stationary random coefficients governed by linear or nonlinear mixing conditions and bounded (or unbounded) $C^1$ domains, this paper mainly studies (weighted) annealed Calder\\'on-Zygmund estimates, some of which are new even in a periodic setting.","Stronger than some classical results derived by a perturbation argument in the deterministic case, our results own a scaling-invariant property, which additionally requires the non-perturbation method (based upon a quantitative homogenization theory and a set of functional analysis techniques) recently developed by M. Joisen and F. Otto \\cite{Josien-Otto22}.","To handle boundary estimates in certain UMD (unconditional martingale differences) spaces, we hand them over to Shen's real arguments \\cite{Shen05, Shen23} instead of using Mikhlin's theorem.","As a by-product, we also established ``resolvent estimates''.","The potentially attractive part is to show how the two powerful kernel-free methods work together to make the results clean and robust."],"url":"http://arxiv.org/abs/2405.19102v1","category":"math.AP"}
{"created":"2024-05-29 13:56:43","title":"Groupoidal Realizability for Intensional Type Theory","abstract":"We develop realizability models of intensional type theory, based on groupoids, wherein realizers themselves carry non-trivial (non-discrete) homotopical structure. In the spirit of realizability, this is intended to formalize a homotopical BHK interpretation, whereby evidence for an identification is a path. Specifically, we study partitioned groupoidal assemblies. Categories of such are parameterised by \"realizer categories\" (instead of the usual partial combinatory algebras) that come equipped with an interval qua internal cogroupoid. The interval furnishes a notion of homotopy as well as a fundamental groupoid construction. Objects in a base groupoid are realized by points in the fundamental groupoid of some object from the realizer category; isomorphisms in the base groupoid are realized by paths in said fundamental groupoid. The main result is that, under mild conditions on the realizer category, the ensuing category of partitioned groupoidal assemblies models intensional (1-truncated) type theory without function extensionality. Moreover, when the underlying realizer category is \"untyped\", there exists an impredicative universe of 1-types (the modest fibrations). This is a groupoidal analogue of the traditional situation.","sentences":["We develop realizability models of intensional type theory, based on groupoids, wherein realizers themselves carry non-trivial (non-discrete) homotopical structure.","In the spirit of realizability, this is intended to formalize a homotopical BHK interpretation, whereby evidence for an identification is a path.","Specifically, we study partitioned groupoidal assemblies.","Categories of such are parameterised by \"realizer categories\" (instead of the usual partial combinatory algebras) that come equipped with an interval qua internal cogroupoid.","The interval furnishes a notion of homotopy as well as a fundamental groupoid construction.","Objects in a base groupoid are realized by points in the fundamental groupoid of some object from the realizer category; isomorphisms in the base groupoid are realized by paths in said fundamental groupoid.","The main result is that, under mild conditions on the realizer category, the ensuing category of partitioned groupoidal assemblies models intensional (1-truncated) type theory without function extensionality.","Moreover, when the underlying realizer category is \"untyped\", there exists an impredicative universe of 1-types (the modest fibrations).","This is a groupoidal analogue of the traditional situation."],"url":"http://arxiv.org/abs/2405.19095v1","category":"cs.LO"}
{"created":"2024-05-29 13:39:48","title":"Multiscale simulation of spatially correlated microstructure via a latent space representation","abstract":"When deformation gradients act on the scale of the microstructure of a part due to geometry and loading, spatial correlations and finite-size effects in simulation cells cannot be neglected. We propose a multiscale method that accounts for these effects using a variational autoencoder to encode the structure-property map of the stochastic volume elements making up the statistical description of the part. In this paradigm the autoencoder can be used to directly encode the microstructure or, alternatively, its latent space can be sampled to provide likely realizations. We demonstrate the method on three examples using the common additively manufactured material AlSi10Mg in: (a) a comparison with direct numerical simulation of the part microstructure, (b) a push forward of microstructural uncertainty to performance quantities of interest, and (c) a simulation of functional gradation of a part with stochastic microstructure.","sentences":["When deformation gradients act on the scale of the microstructure of a part due to geometry and loading, spatial correlations and finite-size effects in simulation cells cannot be neglected.","We propose a multiscale method that accounts for these effects using a variational autoencoder to encode the structure-property map of the stochastic volume elements making up the statistical description of the part.","In this paradigm the autoencoder can be used to directly encode the microstructure or, alternatively, its latent space can be sampled to provide likely realizations.","We demonstrate the method on three examples using the common additively manufactured material AlSi10Mg in: (a) a comparison with direct numerical simulation of the part microstructure, (b) a push forward of microstructural uncertainty to performance quantities of interest, and (c) a simulation of functional gradation of a part with stochastic microstructure."],"url":"http://arxiv.org/abs/2405.19082v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 13:25:49","title":"Relevance-aware Algorithmic Recourse","abstract":"As machine learning continues to gain prominence, transparency and explainability are increasingly critical. Without an understanding of these models, they can replicate and worsen human bias, adversely affecting marginalized communities. Algorithmic recourse emerges as a tool for clarifying decisions made by predictive models, providing actionable insights to alter outcomes. They answer, 'What do I have to change?' to achieve the desired result. Despite their importance, current algorithmic recourse methods treat all domain values equally, which is unrealistic in real-world settings. In this paper, we propose a novel framework, Relevance-Aware Algorithmic Recourse (RAAR), that leverages the concept of relevance in applying algorithmic recourse to regression tasks. We conducted multiple experiments on 15 datasets to outline how relevance influences recourses. Results show that relevance contributes algorithmic recourses comparable to well-known baselines, with greater efficiency and lower relative costs.","sentences":["As machine learning continues to gain prominence, transparency and explainability are increasingly critical.","Without an understanding of these models, they can replicate and worsen human bias, adversely affecting marginalized communities.","Algorithmic recourse emerges as a tool for clarifying decisions made by predictive models, providing actionable insights to alter outcomes.","They answer, 'What do I have to change?'","to achieve the desired result.","Despite their importance, current algorithmic recourse methods treat all domain values equally, which is unrealistic in real-world settings.","In this paper, we propose a novel framework, Relevance-Aware Algorithmic Recourse (RAAR), that leverages the concept of relevance in applying algorithmic recourse to regression tasks.","We conducted multiple experiments on 15 datasets to outline how relevance influences recourses.","Results show that relevance contributes algorithmic recourses comparable to well-known baselines, with greater efficiency and lower relative costs."],"url":"http://arxiv.org/abs/2405.19072v1","category":"cs.LG"}
{"created":"2024-05-29 13:23:57","title":"Quantum Optimal Control of Squeezing in Cavity Optomechanics","abstract":"Squeezing is a non-classical feature of quantum states that is a useful resource, for example in quantum sensing of mechanical forces. Here, we show how to use optimal control theory to maximize squeezing in an optomechanical setup with two external drives and determine how fast the mechanical mode can be squeezed. For the autonomous drives considered here, we find the inverse cavity decay to lower-bound the protocol duration. At and above this limit, we identify a family of protocols leveraging a two-stage control strategy, where the mechanical mode is cooled before it is squeezed. Identification of the control strategy allows for two important insights - to determine the factors that limit squeezing and to simplify the time-dependence of the external drives, making our protocol readily applicable in experiments.","sentences":["Squeezing is a non-classical feature of quantum states that is a useful resource, for example in quantum sensing of mechanical forces.","Here, we show how to use optimal control theory to maximize squeezing in an optomechanical setup with two external drives and determine how fast the mechanical mode can be squeezed.","For the autonomous drives considered here, we find the inverse cavity decay to lower-bound the protocol duration.","At and above this limit, we identify a family of protocols leveraging a two-stage control strategy, where the mechanical mode is cooled before it is squeezed.","Identification of the control strategy allows for two important insights - to determine the factors that limit squeezing and to simplify the time-dependence of the external drives, making our protocol readily applicable in experiments."],"url":"http://arxiv.org/abs/2405.19070v1","category":"quant-ph"}
{"created":"2024-05-29 12:58:57","title":"Participation bias in the estimation of heritability and genetic correlation","abstract":"It is increasingly recognized that participation bias can pose problems for genetic studies. Recently, to overcome the challenge that genetic information of non-participants is unavailable, it is shown that by comparing the IBD (identity by descent) shared and not-shared segments among the participants, one can estimate the genetic component underlying participation. That, however, does not directly address how to adjust estimates of heritability and genetic correlation for phenotypes correlated with participation. Here, for phenotypes whose mean differences between population and sample are known, we demonstrate a way to do so by adopting a statistical framework that separates out the genetic and non-genetic correlations between participation and these phenotypes. Crucially, our method avoids making the assumption that the effect of the genetic component underlying participation is manifested entirely through these other phenotypes. Applying the method to 12 UK Biobank phenotypes, we found 8 have significant genetic correlations with participation, including body mass index, educational attainment, and smoking status. For most of these phenotypes, without adjustments, estimates of heritability and the absolute value of genetic correlation would have underestimation biases.","sentences":["It is increasingly recognized that participation bias can pose problems for genetic studies.","Recently, to overcome the challenge that genetic information of non-participants is unavailable, it is shown that by comparing the IBD (identity by descent) shared and not-shared segments among the participants, one can estimate the genetic component underlying participation.","That, however, does not directly address how to adjust estimates of heritability and genetic correlation for phenotypes correlated with participation.","Here, for phenotypes whose mean differences between population and sample are known, we demonstrate a way to do so by adopting a statistical framework that separates out the genetic and non-genetic correlations between participation and these phenotypes.","Crucially, our method avoids making the assumption that the effect of the genetic component underlying participation is manifested entirely through these other phenotypes.","Applying the method to 12 UK Biobank phenotypes, we found 8 have significant genetic correlations with participation, including body mass index, educational attainment, and smoking status.","For most of these phenotypes, without adjustments, estimates of heritability and the absolute value of genetic correlation would have underestimation biases."],"url":"http://arxiv.org/abs/2405.19058v1","category":"stat.ME"}
{"created":"2024-05-29 12:57:50","title":"Metadata-guided Feature Disentanglement for Functional Genomics","abstract":"With the development of high-throughput technologies, genomics datasets rapidly grow in size, including functional genomics data. This has allowed the training of large Deep Learning (DL) models to predict epigenetic readouts, such as protein binding or histone modifications, from genome sequences. However, large dataset sizes come at a price of data consistency, often aggregating results from a large number of studies, conducted under varying experimental conditions. While data from large-scale consortia are useful as they allow studying the effects of different biological conditions, they can also contain unwanted biases from confounding experimental factors. Here, we introduce Metadata-guided Feature Disentanglement (MFD) - an approach that allows disentangling biologically relevant features from potential technical biases. MFD incorporates target metadata into model training, by conditioning weights of the model output layer on different experimental factors. It then separates the factors into disjoint groups and enforces independence of the corresponding feature subspaces with an adversarially learned penalty. We show that the metadata-driven disentanglement approach allows for better model introspection, by connecting latent features to experimental factors, without compromising, or even improving performance in downstream tasks, such as enhancer prediction, or genetic variant discovery. The code for our implemementation is available at https://github.com/HealthML/MFD","sentences":["With the development of high-throughput technologies, genomics datasets rapidly grow in size, including functional genomics data.","This has allowed the training of large Deep Learning (DL) models to predict epigenetic readouts, such as protein binding or histone modifications, from genome sequences.","However, large dataset sizes come at a price of data consistency, often aggregating results from a large number of studies, conducted under varying experimental conditions.","While data from large-scale consortia are useful as they allow studying the effects of different biological conditions, they can also contain unwanted biases from confounding experimental factors.","Here, we introduce Metadata-guided Feature Disentanglement (MFD) - an approach that allows disentangling biologically relevant features from potential technical biases.","MFD incorporates target metadata into model training, by conditioning weights of the model output layer on different experimental factors.","It then separates the factors into disjoint groups and enforces independence of the corresponding feature subspaces with an adversarially learned penalty.","We show that the metadata-driven disentanglement approach allows for better model introspection, by connecting latent features to experimental factors, without compromising, or even improving performance in downstream tasks, such as enhancer prediction, or genetic variant discovery.","The code for our implemementation is available at https://github.com/HealthML/MFD"],"url":"http://arxiv.org/abs/2405.19057v1","category":"q-bio.GN"}
{"created":"2024-05-29 12:23:09","title":"Flow-distribution dependent SDEs and Navier-Stokes equations with $\\mathbf f$B$\\mathbf m$","abstract":"Motivated by the probabilistic representation of the Navier-Stokes equations, we introduce a novel class of stochastic differential equations that depend on flow distribution. We establish the existence and uniqueness of both strong and weak solutions under one-sided Lipschitz conditions and singular drifts. These newly proposed flow-distribution dependent stochastic differential equations are closely connected to quasilinear backward Kolmogorov equations and forward Fokker-Planck equations. Furthermore, we investigate a stochastic version of the 2D-Navier-Stokes equation associated with fractional Brownian noise. We demonstrate the global well-posedness and smoothness of solutions when the Hurst parameter $H$ lies in the range $(0, \\frac12)$ and the initial vorticity is a finite signed measure.","sentences":["Motivated by the probabilistic representation of the Navier-Stokes equations, we introduce a novel class of stochastic differential equations that depend on flow distribution.","We establish the existence and uniqueness of both strong and weak solutions under one-sided Lipschitz conditions and singular drifts.","These newly proposed flow-distribution dependent stochastic differential equations are closely connected to quasilinear backward Kolmogorov equations and forward Fokker-Planck equations.","Furthermore, we investigate a stochastic version of the 2D-Navier-Stokes equation associated with fractional Brownian noise.","We demonstrate the global well-posedness and smoothness of solutions when the Hurst parameter $H$ lies in the range $(0, \\frac12)$ and the initial vorticity is a finite signed measure."],"url":"http://arxiv.org/abs/2405.19034v1","category":"math.PR"}
{"created":"2024-05-29 12:14:36","title":"On the Galactic rotation curve inferred from the Jeans equations Assessing its robustness using Gaia DR3 and cosmological simulations","abstract":"Several works have recently applied Jeans modelling to Gaia-based datasets to infer the circular velocity curve for the Milky Way. Such works have consistently found evidence for a continuous decline in the rotation curve beyond $\\sim$15kpc possibly indicative of a light dark matter halo. We used Gaia DR3 RVS data, supplemented with Bayesian distances to determine the radial variation of the second moments of the velocity distribution for stars close to the Galactic plane. We have used these profiles to determine the rotation curve using the Jeans equations under the assumption of axisymmetry and explored how they vary with azimuth and above and below the Galactic disk plane. We have applied the same methodology to an N-body simulation of a Milky Way-like galaxy impacted by a satellite akin the Sagittarius dwarf and to the Auriga suite of cosmological simulations. We reveal evidence of disequilibrium and deviations from axisymmetry closer in. We find that the second moment of $V_R$ flattens out at $R \\gtrsim 12.5$kpc, and that the second moment of $V_{\\phi}$ is different above and below the plane for $R \\gtrsim 11$kpc. The simulations indicate that these features are typical of galaxies that have been perturbed by external satellites. They also suggest that the difference between the true circular velocity curve and that inferred from Jeans equations can be as high as 15$\\%$, but is likely of order 10$\\%$ for the Milky Way. This is of larger amplitude than the systematics inherent to Jeans equations. However, if the density of the tracer population were truncated at large radii, the erroneous conclusion of a steeply declining rotation curve can be reached. We find that steady-state axisymmetric Jeans modelling becomes less robust at large radii, indicating that particular caution is needed when interpreting the rotation curve inferred in those regions.","sentences":["Several works have recently applied Jeans modelling to Gaia-based datasets to infer the circular velocity curve for the Milky Way.","Such works have consistently found evidence for a continuous decline in the rotation curve beyond $\\sim$15kpc possibly indicative of a light dark matter halo.","We used Gaia DR3 RVS data, supplemented with Bayesian distances to determine the radial variation of the second moments of the velocity distribution for stars close to the Galactic plane.","We have used these profiles to determine the rotation curve using the Jeans equations under the assumption of axisymmetry and explored how they vary with azimuth and above and below the Galactic disk plane.","We have applied the same methodology to an N-body simulation of a Milky Way-like galaxy impacted by a satellite akin the Sagittarius dwarf and to the Auriga suite of cosmological simulations.","We reveal evidence of disequilibrium and deviations from axisymmetry closer in.","We find that the second moment of $V_R$ flattens out at $R \\gtrsim 12.5$kpc, and that the second moment of $V_{\\phi}$ is different above and below the plane for $R \\gtrsim 11$kpc.","The simulations indicate that these features are typical of galaxies that have been perturbed by external satellites.","They also suggest that the difference between the true circular velocity curve and that inferred from Jeans equations can be as high as 15$\\%$, but is likely of order 10$\\%$ for the Milky Way.","This is of larger amplitude than the systematics inherent to Jeans equations.","However, if the density of the tracer population were truncated at large radii, the erroneous conclusion of a steeply declining rotation curve can be reached.","We find that steady-state axisymmetric Jeans modelling becomes less robust at large radii, indicating that particular caution is needed when interpreting the rotation curve inferred in those regions."],"url":"http://arxiv.org/abs/2405.19028v1","category":"astro-ph.GA"}
{"created":"2024-05-29 12:03:15","title":"Calibration of MAJIS (Moons And Jupiter Imaging Spectrometer): III. Spectral Calibration","abstract":"The Moons And Jupiter Imaging Spectrometer (MAJIS) is the visible and near-infrared imaging spectrometer onboard ESA s Jupiter Icy Moons Explorer (JUICE) mission. Before its integration into the spacecraft, the instrument undergoes an extensive ground calibration to establish its baseline performances. This process prepares the imaging spectrometer for flight operations by characterizing the behavior of the instrument under various operative conditions and uncovering instrumental distortions that may depend on instrumental commands. Two steps of the on-ground calibration campaigns were held at the instrument level to produce the data. Additional in-flight measurements have recently been obtained after launch during the Near-Earth Commissioning Phase. In this article, we present the analyses of these datasets, focusing on the characterization of the spectral performances. First, we describe and analyze the spectral calibration datasets obtained using both monochromatic sources and polychromatic sources coupled with solid and gas samples. Then, we derive the spectral sampling and the spectral response function over the entire field of view. These spectral characteristics are quantified for various operational parameters of MAJIS, such as temperature and spectral binning. The derived on-ground performances are then compared with in-flight measurements obtained after launch and presented in the framework of the MAJIS performance requirements.","sentences":["The Moons And Jupiter Imaging Spectrometer (MAJIS) is the visible and near-infrared imaging spectrometer onboard ESA s Jupiter Icy Moons Explorer (JUICE) mission.","Before its integration into the spacecraft, the instrument undergoes an extensive ground calibration to establish its baseline performances.","This process prepares the imaging spectrometer for flight operations by characterizing the behavior of the instrument under various operative conditions and uncovering instrumental distortions that may depend on instrumental commands.","Two steps of the on-ground calibration campaigns were held at the instrument level to produce the data.","Additional in-flight measurements have recently been obtained after launch during the Near-Earth Commissioning Phase.","In this article, we present the analyses of these datasets, focusing on the characterization of the spectral performances.","First, we describe and analyze the spectral calibration datasets obtained using both monochromatic sources and polychromatic sources coupled with solid and gas samples.","Then, we derive the spectral sampling and the spectral response function over the entire field of view.","These spectral characteristics are quantified for various operational parameters of MAJIS, such as temperature and spectral binning.","The derived on-ground performances are then compared with in-flight measurements obtained after launch and presented in the framework of the MAJIS performance requirements."],"url":"http://arxiv.org/abs/2405.19021v1","category":"astro-ph.IM"}
{"created":"2024-05-29 12:01:08","title":"Neutron skin impurity from Coulomb core-polarization in $^{208}\\text{Pb}$: Insights from PREX-II and validation via $(^{3}\\text{He}, t)$IAS reaction","abstract":"We investigate the impurity in the neutron skin induced by Coulomb core-polarization, highlighting its impact on uncertainties in neutron skin measurements from PREX-II. This effect is validated using the $(^{3}\\text{He}, t)$IAS reaction at 420 MeV. Our findings reveal that the Coulomb boundary radius, where core-polarization becomes negligible, is critical for accurately probing the neutron skin in the target nucleus. Notably, the $(^{3}\\text{He}, t)$IAS experiment conducted by Zegers et al. [Phys. Rev. Lett. 99, 202501 (2007)] primarily detects neutron excess at zero scattering angle due to the core-polarization phenomenon, rather than the neutron skin. We propose extending this experiment to measure cross sections at a 3$^\\circ$ angle to more precisely determine the neutron skin thickness in $^{208}\\text{Pb}$. Additionally, uncertainties in PREX-II may arise from electrons initially probing neutron excess instead of the neutron skin, potentially leading to an overestimation of neutron skin thickness due to the mixing effect where protons are displaced into the neutron skin region.","sentences":["We investigate the impurity in the neutron skin induced by Coulomb core-polarization, highlighting its impact on uncertainties in neutron skin measurements from PREX-II.","This effect is validated using the $(^{3}\\text{He}, t)$IAS reaction at 420 MeV.","Our findings reveal that the Coulomb boundary radius, where core-polarization becomes negligible, is critical for accurately probing the neutron skin in the target nucleus.","Notably, the $(^{3}\\text{He}, t)$IAS experiment conducted by Zegers et al.","[Phys. Rev. Lett.","99, 202501 (2007)] primarily detects neutron excess at zero scattering angle due to the core-polarization phenomenon, rather than the neutron skin.","We propose extending this experiment to measure cross sections at a 3$^\\circ$ angle to more precisely determine the neutron skin thickness in $^{208}\\text{Pb}$. Additionally, uncertainties in PREX-II may arise from electrons initially probing neutron excess instead of the neutron skin, potentially leading to an overestimation of neutron skin thickness due to the mixing effect where protons are displaced into the neutron skin region."],"url":"http://arxiv.org/abs/2405.19018v1","category":"nucl-th"}
{"created":"2024-05-29 11:59:56","title":"Efficient Exploration in Average-Reward Constrained Reinforcement Learning: Achieving Near-Optimal Regret With Posterior Sampling","abstract":"We present a new algorithm based on posterior sampling for learning in Constrained Markov Decision Processes (CMDP) in the infinite-horizon undiscounted setting. The algorithm achieves near-optimal regret bounds while being advantageous empirically compared to the existing algorithms. Our main theoretical result is a Bayesian regret bound for each cost component of $\\tilde{O} (DS\\sqrt{AT})$ for any communicating CMDP with $S$ states, $A$ actions, and diameter $D$. This regret bound matches the lower bound in order of time horizon $T$ and is the best-known regret bound for communicating CMDPs achieved by a computationally tractable algorithm. Empirical results show that our posterior sampling algorithm outperforms the existing algorithms for constrained reinforcement learning.","sentences":["We present a new algorithm based on posterior sampling for learning in Constrained Markov Decision Processes (CMDP) in the infinite-horizon undiscounted setting.","The algorithm achieves near-optimal regret bounds while being advantageous empirically compared to the existing algorithms.","Our main theoretical result is a Bayesian regret bound for each cost component of $\\tilde{O} (DS\\sqrt{AT})$ for any communicating CMDP with $S$ states, $A$ actions, and diameter $D$. This regret bound matches the lower bound in order of time horizon $T$ and is the best-known regret bound for communicating CMDPs achieved by a computationally tractable algorithm.","Empirical results show that our posterior sampling algorithm outperforms the existing algorithms for constrained reinforcement learning."],"url":"http://arxiv.org/abs/2405.19017v1","category":"cs.LG"}
{"created":"2024-05-29 11:57:04","title":"Adaptive posterior concentration rates for sparse high-dimensional linear regression with random design and unknown error variance","abstract":"This paper investigates sparse high-dimensional linear regression, particularly examining the properties of the posterior under conditions of random design and unknown error variance. We provide consistency results for the posterior and analyze its concentration rates, demonstrating adaptiveness to the unknown sparsity level of the regression coefficient vector. Furthermore, we extend our investigation to establish concentration outcomes for parameter estimation using specific distance measures. These findings are in line with recent discoveries in frequentist studies. Additionally, by employing techniques to address model misspecification through a fractional posterior, we broaden our analysis through oracle inequalities to encompass the critical aspect of model misspecification for the regular posterior. Our novel findings are demonstrated using two different types of sparsity priors: a shrinkage prior and a spike-and-slab prior.","sentences":["This paper investigates sparse high-dimensional linear regression, particularly examining the properties of the posterior under conditions of random design and unknown error variance.","We provide consistency results for the posterior and analyze its concentration rates, demonstrating adaptiveness to the unknown sparsity level of the regression coefficient vector.","Furthermore, we extend our investigation to establish concentration outcomes for parameter estimation using specific distance measures.","These findings are in line with recent discoveries in frequentist studies.","Additionally, by employing techniques to address model misspecification through a fractional posterior, we broaden our analysis through oracle inequalities to encompass the critical aspect of model misspecification for the regular posterior.","Our novel findings are demonstrated using two different types of sparsity priors: a shrinkage prior and a spike-and-slab prior."],"url":"http://arxiv.org/abs/2405.19016v1","category":"math.ST"}
{"created":"2024-05-29 11:53:07","title":"Trust the Model Where It Trusts Itself -- Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption","abstract":"Dyna-style model-based reinforcement learning (MBRL) combines model-free agents with predictive transition models through model-based rollouts. This combination raises a critical question: 'When to trust your model?'; i.e., which rollout length results in the model providing useful data? Janner et al. (2019) address this question by gradually increasing rollout lengths throughout the training. While theoretically tempting, uniform model accuracy is a fallacy that collapses at the latest when extrapolating. Instead, we propose asking the question 'Where to trust your model?'. Using inherent model uncertainty to consider local accuracy, we obtain the Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption (MACURA) algorithm. We propose an easy-to-tune rollout mechanism and demonstrate substantial improvements in data efficiency and performance compared to state-of-the-art deep MBRL methods on the MuJoCo benchmark.","sentences":["Dyna-style model-based reinforcement learning (MBRL) combines model-free agents with predictive transition models through model-based rollouts.","This combination raises a critical question: 'When to trust your model?'; i.e., which rollout length results in the model providing useful data?","Janner et al. (2019) address this question by gradually increasing rollout lengths throughout the training.","While theoretically tempting, uniform model accuracy is a fallacy that collapses at the latest when extrapolating.","Instead, we propose asking the question 'Where to trust your model?'.","Using inherent model uncertainty to consider local accuracy, we obtain the Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption (MACURA) algorithm.","We propose an easy-to-tune rollout mechanism and demonstrate substantial improvements in data efficiency and performance compared to state-of-the-art deep MBRL methods on the MuJoCo benchmark."],"url":"http://arxiv.org/abs/2405.19014v1","category":"cs.LG"}
{"created":"2024-05-29 11:07:46","title":"Survival probability and position distribution of a run and tumble particle in $U(x)=\u03b1|x|$ potential with an absorbing boundary","abstract":"We study the late time exponential decay of the survival probability $S_\\pm(t,a|x_0)\\sim e^{-\\theta(a)t}$, of a one-dimensional run and tumble particle starting from $x_0<a$ with an initial orientation $\\sigma(0)=\\pm 1$, under a confining potential $U(x)=\\alpha|x|$ with an absorbing boundary at $x=a>0$. We find that the decay rate $\\theta(a)$ of the survival probability has strong dependence on the location $a$ of the absorbing boundary, which undergoes a freezing transition at a critical value $a=a_c=(v_0-\\alpha)\\sqrt{v_0^2-\\alpha^2}/(2\\alpha\\gamma)$, where $v_0>\\alpha$ is the self-propulsion speed and $\\gamma$ is the tumbling rate of the particle. For $a>a_c$, the value of $\\theta(a)$ increases monotonically from zero, as $a$ decreases from infinity, till it attains the maximum value $\\theta(a_c)$ at $a=a_c$. For $0<a<a_c$, the value of $\\theta(a)$ freezes to the value $\\theta(a)=\\theta(a_c)$. We also obtain the propagator with the absorbing boundary condition at $x=a$. Our analytical results are supported by numerical simulations.","sentences":["We study the late time exponential decay of the survival probability $S_\\pm(t,a|x_0)\\sim e^{-\\theta(a)t}$, of a one-dimensional run and tumble particle starting from $x_0<a$ with an initial orientation $\\sigma(0)=\\pm 1$, under a confining potential $U(x)=\\alpha|x|$ with an absorbing boundary at $x=a>0$. We find that the decay rate $\\theta(a)$ of the survival probability has strong dependence on the location $a$ of the absorbing boundary, which undergoes a freezing transition at a critical value $a=a_c=(v_0-\\alpha)\\sqrt{v_0^2-\\alpha^2}/(2\\alpha\\gamma)$, where $v_0>\\alpha$ is the self-propulsion speed and $\\gamma$ is the tumbling rate of the particle.","For $a>a_c$, the value of $\\theta(a)$ increases monotonically from zero, as $a$ decreases from infinity, till it attains the maximum value $\\theta(a_c)$ at $a=a_c$. For $0<a<a_c$, the value of $\\theta(a)$ freezes to the value $\\theta(a)=\\theta(a_c)$. We also obtain the propagator with the absorbing boundary condition at $x=a$.","Our analytical results are supported by numerical simulations."],"url":"http://arxiv.org/abs/2405.18988v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-29 10:42:39","title":"Reliability of in-band and broadband spectral index measurement: systematic study of the effect of signal to noise for uGMRT data","abstract":"Low radio frequency spectral index measurements are a powerful tool to distinguish between different emission mechanisms and, in turn, to understand the nature of the sources. Besides the standard method of estimating the ``broadband\" spectral index of sources from observations in two different frequency ``bands\", if the observations were made with large instantaneous bandwidth, the ``in-band\" spectral index can be determined, either using images of emission at multiple frequency ranges within a band or using the novel Multi Term-Multi Frequency Synthesis (MT-MFS) imaging algorithm. Here, using simulated upgraded Giant Metrewave Radio Telescope (uGMRT) data, we have systematically studied the reliability of various methods of spectral index estimation for sources with a wide range of signal-to-noise ratio (SNR). It is found that, for synthetic uGMRT point source data, the MT-MFS imaging algorithm produces in-band spectral indices for SNR~$\\lesssim100$ that have errors $\\gtrsim 0.2$, making them unreliable. However, at a similar SNR, the sub-band splitting method produces errors $\\lesssim 0.2$, which are more accurate and unbiased in-band spectral indices. The broadband spectral indices produce errors $\\lesssim 0.2$ even for SNR $\\gtrsim 15$, and hence, they are most reliable if there are no higher-order variations in the spectral index. These results may be used to improve the uGMRT observation and data analysis strategies depending on the brightness of the target source.","sentences":["Low radio frequency spectral index measurements are a powerful tool to distinguish between different emission mechanisms and, in turn, to understand the nature of the sources.","Besides the standard method of estimating the ``broadband\" spectral index of sources from observations in two different frequency ``bands\", if the observations were made with large instantaneous bandwidth, the ``in-band\" spectral index can be determined, either using images of emission at multiple frequency ranges within a band or using the novel Multi Term-Multi Frequency Synthesis (MT-MFS) imaging algorithm.","Here, using simulated upgraded Giant Metrewave Radio Telescope (uGMRT) data, we have systematically studied the reliability of various methods of spectral index estimation for sources with a wide range of signal-to-noise ratio (SNR).","It is found that, for synthetic uGMRT point source data, the MT-MFS imaging algorithm produces in-band spectral indices for SNR~$\\lesssim100$ that have errors $\\gtrsim 0.2$, making them unreliable.","However, at a similar SNR, the sub-band splitting method produces errors $\\lesssim 0.2$, which are more accurate and unbiased in-band spectral indices.","The broadband spectral indices produce errors $\\lesssim 0.2$ even for SNR $\\gtrsim 15$, and hence, they are most reliable if there are no higher-order variations in the spectral index.","These results may be used to improve the uGMRT observation and data analysis strategies depending on the brightness of the target source."],"url":"http://arxiv.org/abs/2405.18978v1","category":"astro-ph.IM"}
{"created":"2024-05-29 10:24:24","title":"Diagonalization-Based Parallel-in-Time Preconditioners for Instationary Fluid Flow Control Problems","abstract":"We derive a new parallel-in-time approach for solving large-scale optimization problems constrained by time-dependent partial differential equations arising from fluid dynamics. The solver involves the use of a block circulant approximation of the original matrices, enabling parallelization-in-time via the use of fast Fourier transforms, and we devise bespoke matrix approximations which may be applied within this framework. These make use of permutations, saddle-point approximations, commutator arguments, as well as inner solvers such as the Uzawa method, Chebyshev semi-iteration, and multigrid. Theoretical results underpin our strategy of applying a block circulant strategy, and numerical experiments demonstrate the effectiveness and robustness of our approach on Stokes and Oseen problems. Noteably, satisfying results for the strong and weak scaling of our methods are provided within a fully parallel architecture.","sentences":["We derive a new parallel-in-time approach for solving large-scale optimization problems constrained by time-dependent partial differential equations arising from fluid dynamics.","The solver involves the use of a block circulant approximation of the original matrices, enabling parallelization-in-time via the use of fast Fourier transforms, and we devise bespoke matrix approximations which may be applied within this framework.","These make use of permutations, saddle-point approximations, commutator arguments, as well as inner solvers such as the Uzawa method, Chebyshev semi-iteration, and multigrid.","Theoretical results underpin our strategy of applying a block circulant strategy, and numerical experiments demonstrate the effectiveness and robustness of our approach on Stokes and Oseen problems.","Noteably, satisfying results for the strong and weak scaling of our methods are provided within a fully parallel architecture."],"url":"http://arxiv.org/abs/2405.18964v1","category":"math.NA"}
{"created":"2024-05-29 09:48:17","title":"NbSe$_{2}$'s charge density wave collapse in the (LaSe)$_{1.14}$(NbSe$_{2}$)$_{2}$ misfit layer compound","abstract":"Misfit layer compounds, heterostructures composed by a regular alternating stacking of rocksalt monochalcogenides bilayers and few-layer transition metal dichalchogenides, are an emergent platform to investigate highly doped transition metal dichalcogenides. Among them, (LaSe)$_{1.14}$(NbSe$_2$)$_2$ displays Ising superconductivity, while the presence of a charge density wave (CDW) in the material is still under debate. Here, by using polarized Raman spectroscopy and first-principles calculations, we show that NbSe$_2$ undergoes a doping-driven collapse of the CDW ordering within the misfit, and no signature of the CDW is detected down to 8~K. We provide a complete experimental and theoretical description of the lattice dynamics of this misfit compound. We show that the vibrational properties are obtained from those of the two subunits, namely the LaSe unit and the NbSe$_2$ bilayer, in the presence of a suitable field-effect doping, and then highlight the 2D nature of the lattice dynamics of NbSe$_2$ within the (LaSe)$_{1.14}$(NbSe$_2$)$_2$ 3D structure.","sentences":["Misfit layer compounds, heterostructures composed by a regular alternating stacking of rocksalt monochalcogenides bilayers and few-layer transition metal dichalchogenides, are an emergent platform to investigate highly doped transition metal dichalcogenides.","Among them, (LaSe)$_{1.14}$(NbSe$_2$)$_2$ displays Ising superconductivity, while the presence of a charge density wave (CDW) in the material is still under debate.","Here, by using polarized Raman spectroscopy and first-principles calculations, we show that NbSe$_2$ undergoes a doping-driven collapse of the CDW ordering within the misfit, and no signature of the CDW is detected down to 8~K. We provide a complete experimental and theoretical description of the lattice dynamics of this misfit compound.","We show that the vibrational properties are obtained from those of the two subunits, namely the LaSe unit and the NbSe$_2$ bilayer, in the presence of a suitable field-effect doping, and then highlight the 2D nature of the lattice dynamics of NbSe$_2$ within the (LaSe)$_{1.14}$(NbSe$_2$)$_2$ 3D structure."],"url":"http://arxiv.org/abs/2405.18939v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 09:36:57","title":"A Mallows-like Criterion for Anomaly Detection with Random Forest Implementation","abstract":"The effectiveness of anomaly signal detection can be significantly undermined by the inherent uncertainty of relying on one specified model. Under the framework of model average methods, this paper proposes a novel criterion to select the weights on aggregation of multiple models, wherein the focal loss function accounts for the classification of extremely imbalanced data. This strategy is further integrated into Random Forest algorithm by replacing the conventional voting method. We have evaluated the proposed method on benchmark datasets across various domains, including network intrusion. The findings indicate that our proposed method not only surpasses the model averaging with typical loss functions but also outstrips common anomaly detection algorithms in terms of accuracy and robustness.","sentences":["The effectiveness of anomaly signal detection can be significantly undermined by the inherent uncertainty of relying on one specified model.","Under the framework of model average methods, this paper proposes a novel criterion to select the weights on aggregation of multiple models, wherein the focal loss function accounts for the classification of extremely imbalanced data.","This strategy is further integrated into Random Forest algorithm by replacing the conventional voting method.","We have evaluated the proposed method on benchmark datasets across various domains, including network intrusion.","The findings indicate that our proposed method not only surpasses the model averaging with typical loss functions but also outstrips common anomaly detection algorithms in terms of accuracy and robustness."],"url":"http://arxiv.org/abs/2405.18932v1","category":"stat.ML"}
{"created":"2024-05-29 09:00:32","title":"BRST Covariant Phase Space and Holographic Ward Identities","abstract":"This paper develops an enlarged BRST framework to treat the large gauge transformations of a given quantum field theory. It determines the associated infinitely many Noether charges stemming from a gauge fixed and BRST invariant Lagrangian, a result that cannot be obtained from Noether's second theorem. The geometrical significance of this result is highlighted by the construction of a trigraded BRST covariant phase space, allowing a BRST invariant gauge fixing procedure. This provides an appropriate framework for determining the conserved BRST Noether current of the global BRST symmetry and the associated global Noether charges. The latter are found to be equivalent with the usual classical corner charges of large gauge transformations. It allows one to prove the gauge independence of their physical effects at the perturbative quantum level. In particular, the underlying BRST fundamental canonical relation provides the same graded symplectic brackets as in the classical covariant phase space. A unified Lagrangian Ward identity for small and large gauge transformations is built. It consistently decouples into a bulk part for small gauge transformations, which is the standard BRST--BV quantum master equation, and a boundary part for large gauge transformations. The boundary part provides a perturbation theory origin for the invariance of the Hamiltonian physical $\\mathcal{S}$-matrix under asymptotic symmetries. Holographic anomalies for the boundary Ward identity are studied and found to be solutions of a codimension one Wess--Zumino consistency condition. Such solutions are studied in the context of extended BMS symmetry. Their existence clarifies the status of the $1$-loop correction to the subleading soft graviton theorem.","sentences":["This paper develops an enlarged BRST framework to treat the large gauge transformations of a given quantum field theory.","It determines the associated infinitely many Noether charges stemming from a gauge fixed and BRST invariant Lagrangian, a result that cannot be obtained from Noether's second theorem.","The geometrical significance of this result is highlighted by the construction of a trigraded BRST covariant phase space, allowing a BRST invariant gauge fixing procedure.","This provides an appropriate framework for determining the conserved BRST Noether current of the global BRST symmetry and the associated global Noether charges.","The latter are found to be equivalent with the usual classical corner charges of large gauge transformations.","It allows one to prove the gauge independence of their physical effects at the perturbative quantum level.","In particular, the underlying BRST fundamental canonical relation provides the same graded symplectic brackets as in the classical covariant phase space.","A unified Lagrangian Ward identity for small and large gauge transformations is built.","It consistently decouples into a bulk part for small gauge transformations, which is the standard BRST--BV quantum master equation, and a boundary part for large gauge transformations.","The boundary part provides a perturbation theory origin for the invariance of the Hamiltonian physical $\\mathcal{S}$-matrix under asymptotic symmetries.","Holographic anomalies for the boundary Ward identity are studied and found to be solutions of a codimension one Wess--Zumino consistency condition.","Such solutions are studied in the context of extended BMS symmetry.","Their existence clarifies the status of the $1$-loop correction to the subleading soft graviton theorem."],"url":"http://arxiv.org/abs/2405.18898v1","category":"hep-th"}
{"created":"2024-05-29 08:57:00","title":"Unit-Aware Genetic Programming for the Development of Empirical Equations","abstract":"When developing empirical equations, domain experts require these to be accurate and adhere to physical laws. Often, constants with unknown units need to be discovered alongside the equations. Traditional unit-aware genetic programming (GP) approaches cannot be used when unknown constants with undetermined units are included. This paper presents a method for dimensional analysis that propagates unknown units as ''jokers'' and returns the magnitude of unit violations. We propose three methods, namely evolutive culling, a repair mechanism, and a multi-objective approach, to integrate the dimensional analysis in the GP algorithm. Experiments on datasets with ground truth demonstrate comparable performance of evolutive culling and the multi-objective approach to a baseline without dimensional analysis. Extensive analysis of the results on datasets without ground truth reveals that the unit-aware algorithms make only low sacrifices in accuracy, while producing unit-adherent solutions. Overall, we presented a promising novel approach for developing unit-adherent empirical equations.","sentences":["When developing empirical equations, domain experts require these to be accurate and adhere to physical laws.","Often, constants with unknown units need to be discovered alongside the equations.","Traditional unit-aware genetic programming (GP) approaches cannot be used when unknown constants with undetermined units are included.","This paper presents a method for dimensional analysis that propagates unknown units as ''jokers'' and returns the magnitude of unit violations.","We propose three methods, namely evolutive culling, a repair mechanism, and a multi-objective approach, to integrate the dimensional analysis in the GP algorithm.","Experiments on datasets with ground truth demonstrate comparable performance of evolutive culling and the multi-objective approach to a baseline without dimensional analysis.","Extensive analysis of the results on datasets without ground truth reveals that the unit-aware algorithms make only low sacrifices in accuracy, while producing unit-adherent solutions.","Overall, we presented a promising novel approach for developing unit-adherent empirical equations."],"url":"http://arxiv.org/abs/2405.18896v1","category":"cs.LG"}
{"created":"2024-05-29 08:52:48","title":"EVM Analysis of Distributed Massive MIMO with 1-Bit Radio-Over-Fiber Fronthaul","abstract":"We analyze the uplink performance of a distributed massive multiple-input multiple-output (MIMO) architecture in which the remotely located access points (APs) are connected to a central processing unit via a fiber-optical fronthaul carrying a dithered and 1-bit quantized version of the received radio-frequency (RF) signal. The innovative feature of the proposed architecture is that no down-conversion is performed at the APs. This eliminates the need to equip the APs with local oscillators, which may be difficult to synchronize. Under the assumption that a constraint is imposed on the amount of data that can be exchanged across the fiber-optical fronthaul, we investigate the tradeoff between spatial oversampling, defined in terms of the total number of APs, and temporal oversampling, defined in terms of the oversampling factor selected at the central processing unit, to facilitate the recovery of the transmitted signal from 1-bit samples of the RF received signal. Using the so-called error-vector magnitude (EVM) as performance metric, we shed light on the optimal design of the dither signal, and quantify, for a given number of APs, the minimum fronthaul rate required for our proposed distributed massive MIMO architecture to outperform a standard co-located massive MIMO architecture in terms of EVM.","sentences":["We analyze the uplink performance of a distributed massive multiple-input multiple-output (MIMO) architecture in which the remotely located access points (APs) are connected to a central processing unit via a fiber-optical fronthaul carrying a dithered and 1-bit quantized version of the received radio-frequency (RF) signal.","The innovative feature of the proposed architecture is that no down-conversion is performed at the APs.","This eliminates the need to equip the APs with local oscillators, which may be difficult to synchronize.","Under the assumption that a constraint is imposed on the amount of data that can be exchanged across the fiber-optical fronthaul, we investigate the tradeoff between spatial oversampling, defined in terms of the total number of APs, and temporal oversampling, defined in terms of the oversampling factor selected at the central processing unit, to facilitate the recovery of the transmitted signal from 1-bit samples of the RF received signal.","Using the so-called error-vector magnitude (EVM) as performance metric, we shed light on the optimal design of the dither signal, and quantify, for a given number of APs, the minimum fronthaul rate required for our proposed distributed massive MIMO architecture to outperform a standard co-located massive MIMO architecture in terms of EVM."],"url":"http://arxiv.org/abs/2405.18892v1","category":"cs.IT"}
{"created":"2024-05-29 08:46:21","title":"Locally Estimated Global Perturbations are Better than Local Perturbations for Federated Sharpness-aware Minimization","abstract":"In federated learning (FL), the multi-step update and data heterogeneity among clients often lead to a loss landscape with sharper minima, degenerating the performance of the resulted global model. Prevalent federated approaches incorporate sharpness-aware minimization (SAM) into local training to mitigate this problem. However, the local loss landscapes may not accurately reflect the flatness of global loss landscape in heterogeneous environments; as a result, minimizing local sharpness and calculating perturbations on client data might not align the efficacy of SAM in FL with centralized training. To overcome this challenge, we propose FedLESAM, a novel algorithm that locally estimates the direction of global perturbation on client side as the difference between global models received in the previous active and current rounds. Besides the improved quality, FedLESAM also speed up federated SAM-based approaches since it only performs once backpropagation in each iteration. Theoretically, we prove a slightly tighter bound than its original FedSAM by ensuring consistent perturbation. Empirically, we conduct comprehensive experiments on four federated benchmark datasets under three partition strategies to demonstrate the superior performance and efficiency of FedLESAM.","sentences":["In federated learning (FL), the multi-step update and data heterogeneity among clients often lead to a loss landscape with sharper minima, degenerating the performance of the resulted global model.","Prevalent federated approaches incorporate sharpness-aware minimization (SAM) into local training to mitigate this problem.","However, the local loss landscapes may not accurately reflect the flatness of global loss landscape in heterogeneous environments; as a result, minimizing local sharpness and calculating perturbations on client data might not align the efficacy of SAM in FL with centralized training.","To overcome this challenge, we propose FedLESAM, a novel algorithm that locally estimates the direction of global perturbation on client side as the difference between global models received in the previous active and current rounds.","Besides the improved quality, FedLESAM also speed up federated SAM-based approaches since it only performs once backpropagation in each iteration.","Theoretically, we prove a slightly tighter bound than its original FedSAM by ensuring consistent perturbation.","Empirically, we conduct comprehensive experiments on four federated benchmark datasets under three partition strategies to demonstrate the superior performance and efficiency of FedLESAM."],"url":"http://arxiv.org/abs/2405.18890v1","category":"cs.LG"}
{"created":"2024-05-29 08:41:08","title":"Learning Mixture-of-Experts for General-Purpose Black-Box Discrete Optimization","abstract":"Real-world applications involve various discrete optimization problems. Designing a specialized optimizer for each of these problems is challenging, typically requiring significant domain knowledge and human efforts. Hence, developing general-purpose optimizers as an off-the-shelf tool for a wide range of problems has been a long-standing research target. This article introduces MEGO, a novel general-purpose neural optimizer trained through a fully data-driven learning-to-optimize (L2O) approach. MEGO consists of a mixture-of-experts trained on experiences from solving training problems and can be viewed as a foundation model for optimization problems with binary decision variables. When presented with a problem to solve, MEGO actively selects relevant expert models to generate high-quality solutions. MEGO can be used as a standalone sample-efficient optimizer or in conjunction with existing search methods as an initial solution generator. The generality of MEGO is validated across six problem classes, including three classic problem classes and three problem classes arising from real-world applications in compilers, network analysis, and 3D reconstruction. Trained solely on classic problem classes, MEGO performs very well on all six problem classes, significantly surpassing widely used general-purpose optimizers in both solution quality and efficiency. In some cases, MEGO even surpasses specialized state-of-the-art optimizers. Additionally, MEGO provides a similarity measure between problems, yielding a new perspective for problem classification. In the pursuit of general-purpose optimizers through L2O, MEGO represents an initial yet significant step forward.","sentences":["Real-world applications involve various discrete optimization problems.","Designing a specialized optimizer for each of these problems is challenging, typically requiring significant domain knowledge and human efforts.","Hence, developing general-purpose optimizers as an off-the-shelf tool for a wide range of problems has been a long-standing research target.","This article introduces MEGO, a novel general-purpose neural optimizer trained through a fully data-driven learning-to-optimize (L2O) approach.","MEGO consists of a mixture-of-experts trained on experiences from solving training problems and can be viewed as a foundation model for optimization problems with binary decision variables.","When presented with a problem to solve, MEGO actively selects relevant expert models to generate high-quality solutions.","MEGO can be used as a standalone sample-efficient optimizer or in conjunction with existing search methods as an initial solution generator.","The generality of MEGO is validated across six problem classes, including three classic problem classes and three problem classes arising from real-world applications in compilers, network analysis, and 3D reconstruction.","Trained solely on classic problem classes, MEGO performs very well on all six problem classes, significantly surpassing widely used general-purpose optimizers in both solution quality and efficiency.","In some cases, MEGO even surpasses specialized state-of-the-art optimizers.","Additionally, MEGO provides a similarity measure between problems, yielding a new perspective for problem classification.","In the pursuit of general-purpose optimizers through L2O, MEGO represents an initial yet significant step forward."],"url":"http://arxiv.org/abs/2405.18884v1","category":"cs.NE"}
{"created":"2024-05-29 08:35:37","title":"On Fairness Concerns in the Blockchain Ecosystem","abstract":"Blockchains revolutionized centralized sectors like banking and finance by promoting decentralization and transparency. In a blockchain, information is transmitted through transactions issued by participants or applications. Miners crucially select, order, and validate pending transactions for block inclusion, prioritizing those with higher incentives or fees. The order in which transactions are included can impact the blockchain final state. Moreover, applications running on top of a blockchain often rely on governance protocols to decentralize the decision-making power to make changes to their core functionality. These changes can affect how participants interact with these applications. Since one token equals one vote, participants holding multiple tokens have a higher voting power to support or reject the proposed changes. The extent to which this voting power is distributed is questionable and if highly concentrated among a few holders can lead to governance attacks. In this thesis, we audit the Bitcoin and Ethereum blockchains to investigate the norms followed by miners in determining the transaction prioritization. We also audit decentralized governance protocols such as Compound to evaluate whether the voting power is fairly distributed among the participants. Our findings have significant implications for future developments of blockchains and decentralized applications.","sentences":["Blockchains revolutionized centralized sectors like banking and finance by promoting decentralization and transparency.","In a blockchain, information is transmitted through transactions issued by participants or applications.","Miners crucially select, order, and validate pending transactions for block inclusion, prioritizing those with higher incentives or fees.","The order in which transactions are included can impact the blockchain final state.","Moreover, applications running on top of a blockchain often rely on governance protocols to decentralize the decision-making power to make changes to their core functionality.","These changes can affect how participants interact with these applications.","Since one token equals one vote, participants holding multiple tokens have a higher voting power to support or reject the proposed changes.","The extent to which this voting power is distributed is questionable and if highly concentrated among a few holders can lead to governance attacks.","In this thesis, we audit the Bitcoin and Ethereum blockchains to investigate the norms followed by miners in determining the transaction prioritization.","We also audit decentralized governance protocols such as Compound to evaluate whether the voting power is fairly distributed among the participants.","Our findings have significant implications for future developments of blockchains and decentralized applications."],"url":"http://arxiv.org/abs/2405.18876v1","category":"cs.CR"}
{"created":"2024-05-29 08:30:34","title":"Towards Data-Driven Electricity Management: Multi-Region Harmonized Data and Knowledge Graph","abstract":"Due to growing population and technological advances, global electricity consumption, and consequently also CO2 emissions are increasing. The residential sector makes up 25% of global electricity consumption and has great potential to increase efficiency and reduce CO2 footprint without sacrificing comfort. However, a lack of uniform consumption data at the household level spanning multiple regions hinders large-scale studies and robust multi-region model development. This paper introduces a multi-region dataset compiled from publicly available sources and presented in a uniform format. This data enables machine learning tasks such as disaggregation, demand forecasting, appliance ON/OFF classification, etc. Furthermore, we develop an RDF knowledge graph that characterizes the electricity consumption of the households and contextualizes it with household related properties enabling semantic queries and interoperability with other open knowledge bases like Wikidata and DBpedia. This structured data can be utilized to inform various stakeholders towards data-driven policy and business development.","sentences":["Due to growing population and technological advances, global electricity consumption, and consequently also CO2 emissions are increasing.","The residential sector makes up 25% of global electricity consumption and has great potential to increase efficiency and reduce CO2 footprint without sacrificing comfort.","However, a lack of uniform consumption data at the household level spanning multiple regions hinders large-scale studies and robust multi-region model development.","This paper introduces a multi-region dataset compiled from publicly available sources and presented in a uniform format.","This data enables machine learning tasks such as disaggregation, demand forecasting, appliance ON/OFF classification, etc.","Furthermore, we develop an RDF knowledge graph that characterizes the electricity consumption of the households and contextualizes it with household related properties enabling semantic queries and interoperability with other open knowledge bases like Wikidata and DBpedia.","This structured data can be utilized to inform various stakeholders towards data-driven policy and business development."],"url":"http://arxiv.org/abs/2405.18869v1","category":"cs.LG"}
{"created":"2024-05-29 08:22:33","title":"Domain-Inspired Sharpness-Aware Minimization Under Domain Shifts","abstract":"This paper presents a Domain-Inspired Sharpness-Aware Minimization (DISAM) algorithm for optimization under domain shifts. It is motivated by the inconsistent convergence degree of SAM across different domains, which induces optimization bias towards certain domains and thus impairs the overall convergence. To address this issue, we consider the domain-level convergence consistency in the sharpness estimation to prevent the overwhelming (deficient) perturbations for less (well) optimized domains. Specifically, DISAM introduces the constraint of minimizing variance in the domain loss, which allows the elastic gradient calibration in perturbation generation: when one domain is optimized above the averaging level \\textit{w.r.t.} loss, the gradient perturbation towards that domain will be weakened automatically, and vice versa. Under this mechanism, we theoretically show that DISAM can achieve faster overall convergence and improved generalization in principle when inconsistent convergence emerges. Extensive experiments on various domain generalization benchmarks show the superiority of DISAM over a range of state-of-the-art methods. Furthermore, we show the superior efficiency of DISAM in parameter-efficient fine-tuning combined with the pretraining models. The source code is released at https://github.com/MediaBrain-SJTU/DISAM.","sentences":["This paper presents a Domain-Inspired Sharpness-Aware Minimization (DISAM) algorithm for optimization under domain shifts.","It is motivated by the inconsistent convergence degree of SAM across different domains, which induces optimization bias towards certain domains and thus impairs the overall convergence.","To address this issue, we consider the domain-level convergence consistency in the sharpness estimation to prevent the overwhelming (deficient) perturbations for less (well) optimized domains.","Specifically, DISAM introduces the constraint of minimizing variance in the domain loss, which allows the elastic gradient calibration in perturbation generation: when one domain is optimized above the averaging level \\textit{w.r.t.}","loss, the gradient perturbation towards that domain will be weakened automatically, and vice versa.","Under this mechanism, we theoretically show that DISAM can achieve faster overall convergence and improved generalization in principle when inconsistent convergence emerges.","Extensive experiments on various domain generalization benchmarks show the superiority of DISAM over a range of state-of-the-art methods.","Furthermore, we show the superior efficiency of DISAM in parameter-efficient fine-tuning combined with the pretraining models.","The source code is released at https://github.com/MediaBrain-SJTU/DISAM."],"url":"http://arxiv.org/abs/2405.18861v1","category":"cs.CV"}
{"created":"2024-05-29 08:10:29","title":"Inference under covariate-adaptive randomization with many strata","abstract":"Covariate-adaptive randomization is widely employed to balance baseline covariates in interventional studies such as clinical trials and experiments in development economics. Recent years have witnessed substantial progress in inference under covariate-adaptive randomization with a fixed number of strata. However, concerns have been raised about the impact of a large number of strata on its design and analysis, which is a common scenario in practice, such as in multicenter randomized clinical trials. In this paper, we propose a general framework for inference under covariate-adaptive randomization, which extends the seminal works of Bugni et al. (2018, 2019) by allowing for a diverging number of strata. Furthermore, we introduce a novel weighted regression adjustment that ensures efficiency improvement. On top of establishing the asymptotic theory, practical algorithms for handling situations involving an extremely large number of strata are also developed. Moreover, by linking design balance and inference robustness, we highlight the advantages of stratified block randomization, which enforces better covariate balance within strata compared to simple randomization. This paper offers a comprehensive landscape of inference under covariate-adaptive randomization, spanning from fixed to diverging to extremely large numbers of strata.","sentences":["Covariate-adaptive randomization is widely employed to balance baseline covariates in interventional studies such as clinical trials and experiments in development economics.","Recent years have witnessed substantial progress in inference under covariate-adaptive randomization with a fixed number of strata.","However, concerns have been raised about the impact of a large number of strata on its design and analysis, which is a common scenario in practice, such as in multicenter randomized clinical trials.","In this paper, we propose a general framework for inference under covariate-adaptive randomization, which extends the seminal works of Bugni et al.","(2018, 2019) by allowing for a diverging number of strata.","Furthermore, we introduce a novel weighted regression adjustment that ensures efficiency improvement.","On top of establishing the asymptotic theory, practical algorithms for handling situations involving an extremely large number of strata are also developed.","Moreover, by linking design balance and inference robustness, we highlight the advantages of stratified block randomization, which enforces better covariate balance within strata compared to simple randomization.","This paper offers a comprehensive landscape of inference under covariate-adaptive randomization, spanning from fixed to diverging to extremely large numbers of strata."],"url":"http://arxiv.org/abs/2405.18856v1","category":"stat.ME"}
{"created":"2024-05-29 07:49:15","title":"Descriptive Image Quality Assessment in the Wild","abstract":"With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce Depicted image Quality Assessment in the Wild (DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Datasets and codes will be released in https://depictqa.github.io/depictqa-wild/.","sentences":["With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks.","However, current methods are still far from practical usage.","First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications.","Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality.","To overcome these challenges, we introduce Depicted image Quality Assessment in the Wild (DepictQA-Wild).","Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios.","We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework.","Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses.","Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks.","Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images.","Datasets and codes will be released in https://depictqa.github.io/depictqa-wild/."],"url":"http://arxiv.org/abs/2405.18842v1","category":"cs.CV"}
{"created":"2024-05-29 07:40:31","title":"MEGA: Masked Generative Autoencoder for Human Mesh Recovery","abstract":"Human Mesh Recovery (HMR) from a single RGB image is a highly ambiguous problem, as similar 2D projections can correspond to multiple 3D interpretations. Nevertheless, most HMR methods overlook this ambiguity and make a single prediction without accounting for the associated uncertainty. A few approaches generate a distribution of human meshes, enabling the sampling of multiple predictions; however, none of them is competitive with the latest single-output model when making a single prediction. This work proposes a new approach based on masked generative modeling. By tokenizing the human pose and shape, we formulate the HMR task as generating a sequence of discrete tokens conditioned on an input image. We introduce MEGA, a MaskEd Generative Autoencoder trained to recover human meshes from images and partial human mesh token sequences. Given an image, our flexible generation scheme allows us to predict a single human mesh in deterministic mode or to generate multiple human meshes in stochastic mode. MEGA enables us to propose multiple outputs and to evaluate the uncertainty of the predictions. Experiments on in-the-wild benchmarks show that MEGA achieves state-of-the-art performance in deterministic and stochastic modes, outperforming single-output and multi-output approaches.","sentences":["Human Mesh Recovery (HMR) from a single RGB image is a highly ambiguous problem, as similar 2D projections can correspond to multiple 3D interpretations.","Nevertheless, most HMR methods overlook this ambiguity and make a single prediction without accounting for the associated uncertainty.","A few approaches generate a distribution of human meshes, enabling the sampling of multiple predictions; however, none of them is competitive with the latest single-output model when making a single prediction.","This work proposes a new approach based on masked generative modeling.","By tokenizing the human pose and shape, we formulate the HMR task as generating a sequence of discrete tokens conditioned on an input image.","We introduce MEGA, a MaskEd Generative Autoencoder trained to recover human meshes from images and partial human mesh token sequences.","Given an image, our flexible generation scheme allows us to predict a single human mesh in deterministic mode or to generate multiple human meshes in stochastic mode.","MEGA enables us to propose multiple outputs and to evaluate the uncertainty of the predictions.","Experiments on in-the-wild benchmarks show that MEGA achieves state-of-the-art performance in deterministic and stochastic modes, outperforming single-output and multi-output approaches."],"url":"http://arxiv.org/abs/2405.18839v1","category":"cs.CV"}
{"created":"2024-05-29 07:34:07","title":"Transformer for Parameterized Quantum Circuits Expressibility Prediction","abstract":"With the exponentially faster computation for certain problems, quantum computing has garnered significant attention in recent years. Variational Quantum Algorithm (VQA) is a crucial method to implement quantum computing, and an appropriate task-specific ansatz can effectively enhance the quantum advantage of VQAs. However, the vast search space makes it challenging to find the optimal task-specific ansatz. Expressibility, quantifying the diversity of quantum states to explore the Hilbert space effectively, can be used to evaluate whether one ansatz is superior than another. This study investigates the effectiveness of the Transformer model in predicting the expressibility of parameterized quantum circuits. We construct two datasets containing noiseless circuits generated by the gatewise method, varying in qubits, gate numbers and depths. The circuits are transformed into graphs, and then their expressibility are calculated using KL-divergence and Relative KL-divergence. A Transformer model is trained on these datasets to capture the intricate relationships between circuit characteristics and expressibility. Five evaluation metrics are calculated, and experimental results demonstrate that the trained model achieves high performance and robustness across various expressibility calculation methods. This research provides ideas for efficient quantum circuit design and can contribute to the advancement of quantum architecture search methods.","sentences":["With the exponentially faster computation for certain problems, quantum computing has garnered significant attention in recent years.","Variational Quantum Algorithm (VQA) is a crucial method to implement quantum computing, and an appropriate task-specific ansatz can effectively enhance the quantum advantage of VQAs.","However, the vast search space makes it challenging to find the optimal task-specific ansatz.","Expressibility, quantifying the diversity of quantum states to explore the Hilbert space effectively, can be used to evaluate whether one ansatz is superior than another.","This study investigates the effectiveness of the Transformer model in predicting the expressibility of parameterized quantum circuits.","We construct two datasets containing noiseless circuits generated by the gatewise method, varying in qubits, gate numbers and depths.","The circuits are transformed into graphs, and then their expressibility are calculated using KL-divergence and Relative KL-divergence.","A Transformer model is trained on these datasets to capture the intricate relationships between circuit characteristics and expressibility.","Five evaluation metrics are calculated, and experimental results demonstrate that the trained model achieves high performance and robustness across various expressibility calculation methods.","This research provides ideas for efficient quantum circuit design and can contribute to the advancement of quantum architecture search methods."],"url":"http://arxiv.org/abs/2405.18837v1","category":"quant-ph"}
{"created":"2024-05-29 07:25:12","title":"Exploring Exotic Decays of the Higgs Boson to Multi-Photons at the LHC via Multimodal Learning Approaches","abstract":"The Standard Model (SM) Higgs boson, the most recently discovered elementary particle, may still serve as a mediator between the SM sector and a new physics sector related to dark matter (DM). The Large Hadron Collider (LHC) has not yet fully constrained the physics associated with the Higgs boson, leaving room for such possibilities. Among the various potential mass scales of the dark sector, the sub-GeV mass range is particularly intriguing. This parameter space presents significant challenges for DM direct detection experiments that rely on nuclear recoils. Various innovative experimental methods are currently under investigation to explore this sub-GeV dark sector. The LHC, functioning as a Higgs factory, could explore this sector once the challenge of identifying DM signals is resolved. Due to the significantly lower mass of particles in the dark sector compared to the Higgs boson, these particles are expected to be highly boosted following the Higgs boson's decay. However, detecting and identifying these highly boosted particles remains a considerable challenge at the LHC, despite their eventual decay into SM particles. We employ a well-motivated leptophobic $Z^{\\prime}_B$ model as a prototype to analyze the distinctive signatures from Higgs boson exotic decays into multi-photons. These signatures consist of collimated photons that fail to meet the photon isolation criteria, forming jet-like objects. Conventional analyses relying solely on the purity of energy deposits in the electromagnetic calorimeter would fail to detect these signatures, as they would be overwhelmed by background events from Quantum Chromodynamics. To effectively distinguish between such novel signal signatures and SM background events, we leverage advanced machine learning techniques, specifically the transformer encoder in a multimodal network structure.","sentences":["The Standard Model (SM) Higgs boson, the most recently discovered elementary particle, may still serve as a mediator between the SM sector and a new physics sector related to dark matter (DM).","The Large Hadron Collider (LHC) has not yet fully constrained the physics associated with the Higgs boson, leaving room for such possibilities.","Among the various potential mass scales of the dark sector, the sub-GeV mass range is particularly intriguing.","This parameter space presents significant challenges for DM direct detection experiments that rely on nuclear recoils.","Various innovative experimental methods are currently under investigation to explore this sub-GeV dark sector.","The LHC, functioning as a Higgs factory, could explore this sector once the challenge of identifying DM signals is resolved.","Due to the significantly lower mass of particles in the dark sector compared to the Higgs boson, these particles are expected to be highly boosted following the Higgs boson's decay.","However, detecting and identifying these highly boosted particles remains a considerable challenge at the LHC, despite their eventual decay into SM particles.","We employ a well-motivated leptophobic $Z^{\\prime}_B$ model as a prototype to analyze the distinctive signatures from Higgs boson exotic decays into multi-photons.","These signatures consist of collimated photons that fail to meet the photon isolation criteria, forming jet-like objects.","Conventional analyses relying solely on the purity of energy deposits in the electromagnetic calorimeter would fail to detect these signatures, as they would be overwhelmed by background events from Quantum Chromodynamics.","To effectively distinguish between such novel signal signatures and SM background events, we leverage advanced machine learning techniques, specifically the transformer encoder in a multimodal network structure."],"url":"http://arxiv.org/abs/2405.18834v1","category":"hep-ph"}
{"created":"2024-05-29 07:19:44","title":"Visual Servoing Based on 3D Features: Design and Implementation for Robotic Insertion Tasks","abstract":"This paper proposes a feature-based Visual Servoing (VS) method for insertion task skills. A camera mounted on the robot's end-effector provides the pose relative to a cylinder (hole), allowing a contact-free and damage-free search of the hole and avoiding uncertainties emerging when the pose is computed via robot kinematics. Two points located on the hole's principal axis and three mutually orthogonal planes defining the flange's reference frame are associated with the pose of the hole and the flange, respectively. The proposed VS drives to zero the distance between the two points and the three planes aligning the robot's flange with the hole's direction. Compared with conventional VS where the Jacobian is difficult to compute in practice, the proposed featured-based uses a Jacobian easily calculated from the measured hole pose. Furthermore, the feature-based VS design considers the robot's maximum cartesian velocity. The VS method is implemented in an industrial robot and the experimental results support its usefulness.","sentences":["This paper proposes a feature-based Visual Servoing (VS) method for insertion task skills.","A camera mounted on the robot's end-effector provides the pose relative to a cylinder (hole), allowing a contact-free and damage-free search of the hole and avoiding uncertainties emerging when the pose is computed via robot kinematics.","Two points located on the hole's principal axis and three mutually orthogonal planes defining the flange's reference frame are associated with the pose of the hole and the flange, respectively.","The proposed VS drives to zero the distance between the two points and the three planes aligning the robot's flange with the hole's direction.","Compared with conventional VS where the Jacobian is difficult to compute in practice, the proposed featured-based uses a Jacobian easily calculated from the measured hole pose.","Furthermore, the feature-based VS design considers the robot's maximum cartesian velocity.","The VS method is implemented in an industrial robot and the experimental results support its usefulness."],"url":"http://arxiv.org/abs/2405.18830v1","category":"cs.RO"}
{"created":"2024-05-29 07:19:09","title":"On the stationary solution of the Landau-Lifshitz-Gilbert equation on a nanowire with constant external magnetic field","abstract":"We consider an infinite ferromagnetic nanowire, with an energy functional $E$ with easy-axis in the direction $e_1$ and a constant external magnetic field $E_{ext} = h_0 e_1$ along the same direction. The evolution of its magnetization is governed by the Landau-Lifshitz-Gilbert equation (LLG) associated to $E$. Under some assumptions on $h_0$, we prove the existence of stationary solutions with the same limits at infinity, their uniqueness up to the invariances of the equation and the instability of their orbits with respect to the flow. This property gives interesting new insights of the behavior of the solutions of (LLG), which are completed by some numerical simulations and discussed afterwards, in particular regarding the stability of 2-domain wall structures proven in [4] and more generally the interactions between domain walls.","sentences":["We consider an infinite ferromagnetic nanowire, with an energy functional $E$ with easy-axis in the direction $e_1$ and a constant external magnetic field $E_{ext} = h_0 e_1$ along the same direction.","The evolution of its magnetization is governed by the Landau-Lifshitz-Gilbert equation (LLG) associated to $E$. Under some assumptions on $h_0$, we prove the existence of stationary solutions with the same limits at infinity, their uniqueness up to the invariances of the equation and the instability of their orbits with respect to the flow.","This property gives interesting new insights of the behavior of the solutions of (LLG), which are completed by some numerical simulations and discussed afterwards, in particular regarding the stability of 2-domain wall structures proven in [4] and more generally the interactions between domain walls."],"url":"http://arxiv.org/abs/2405.18829v1","category":"math.AP"}
{"created":"2024-05-29 07:11:58","title":"Theoretical insights and an experimental comparison of tango trees and multi-splay trees","abstract":"The tango tree is the first proven $O(\\lg \\lg n)$-competitive binary search tree (BST). We present the first ever experimental implementation of tango trees and compare the running time of the tango tree with the multi-splay tree and the splay tree on a variety of families of access sequences. We construct access sequences that are intended to test specific properties of BSTs. The results of the other experiments demonstrate the optimality of the splay tree and multi-splay tree on these accesses, while simultaneously demonstrating the tango trees inability to achieve optimality. We prove that the running time of tango trees on the sequential access is $\\Theta(n \\lg \\lg n)$, which provides insight into why the $\\Theta(\\lg \\lg n)$ slow down exists on many access sequences. Motivated by experimental results, we conduct a deeper analysis of the working set access on multi-splay trees, leading to new insights about multi-splay tree behavior. Finally, all of the experiments also reveal insights about large constants and lower order terms in the multi-splay tree, which make it less practical than the splay tree, even though its proven competitive bound is tighter.","sentences":["The tango tree is the first proven $O(\\lg \\lg n)$-competitive binary search tree (BST).","We present the first ever experimental implementation of tango trees and compare the running time of the tango tree with the multi-splay tree and the splay tree on a variety of families of access sequences.","We construct access sequences that are intended to test specific properties of BSTs.","The results of the other experiments demonstrate the optimality of the splay tree and multi-splay tree on these accesses, while simultaneously demonstrating the tango trees inability to achieve optimality.","We prove that the running time of tango trees on the sequential access is $\\Theta(n \\lg \\lg n)$, which provides insight into why the $\\Theta(\\lg \\lg n)$ slow down exists on many access sequences.","Motivated by experimental results, we conduct a deeper analysis of the working set access on multi-splay trees, leading to new insights about multi-splay tree behavior.","Finally, all of the experiments also reveal insights about large constants and lower order terms in the multi-splay tree, which make it less practical than the splay tree, even though its proven competitive bound is tighter."],"url":"http://arxiv.org/abs/2405.18825v1","category":"cs.DS"}
{"created":"2024-05-29 07:03:31","title":"Toxicity Detection for Free","abstract":"Current LLMs are generally aligned to follow safety requirements and tend to refuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be overcautious and refuse benign examples. In addition, state-of-the-art toxicity detectors have low TPRs at low FPR, incurring high costs in real-world applications where toxic examples are rare. In this paper, we explore Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves. We found significant gaps between benign and toxic prompts in the distribution of alternative refusal responses and in the distribution of the first response token's logits. These gaps can be used to detect toxicities: We show that a toy model based on the logits of specific starting tokens gets reliable performance, while requiring no training or additional computational cost. We build a more robust detector using a sparse logistic regression model on the first response token logits, which greatly exceeds SOTA detectors under multiple metrics.","sentences":["Current LLMs are generally aligned to follow safety requirements and tend to refuse toxic prompts.","However, LLMs can fail to refuse toxic prompts or be overcautious and refuse benign examples.","In addition, state-of-the-art toxicity detectors have low TPRs at low FPR, incurring high costs in real-world applications where toxic examples are rare.","In this paper, we explore Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves.","We found significant gaps between benign and toxic prompts in the distribution of alternative refusal responses and in the distribution of the first response token's logits.","These gaps can be used to detect toxicities: We show that a toy model based on the logits of specific starting tokens gets reliable performance, while requiring no training or additional computational cost.","We build a more robust detector using a sparse logistic regression model on the first response token logits, which greatly exceeds SOTA detectors under multiple metrics."],"url":"http://arxiv.org/abs/2405.18822v1","category":"cs.CL"}
{"created":"2024-05-29 06:56:12","title":"Flow Priors for Linear Inverse Problems via Iterative Corrupted Trajectory Matching","abstract":"Generative models based on flow matching have attracted significant attention for their simplicity and superior performance in high-resolution image synthesis. By leveraging the instantaneous change-of-variables formula, one can directly compute image likelihoods from a learned flow, making them enticing candidates as priors for downstream tasks such as inverse problems. In particular, a natural approach would be to incorporate such image probabilities in a maximum-a-posteriori (MAP) estimation problem. A major obstacle, however, lies in the slow computation of the log-likelihood, as it requires backpropagating through an ODE solver, which can be prohibitively slow for high-dimensional problems. In this work, we propose an iterative algorithm to approximate the MAP estimator efficiently to solve a variety of linear inverse problems. Our algorithm is mathematically justified by the observation that the MAP objective can be approximated by a sum of $N$ ``local MAP'' objectives, where $N$ is the number of function evaluations. By leveraging Tweedie's formula, we show that we can perform gradient steps to sequentially optimize these objectives. We validate our approach for various linear inverse problems, such as super-resolution, deblurring, inpainting, and compressed sensing, and demonstrate that we can outperform other methods based on flow matching.","sentences":["Generative models based on flow matching have attracted significant attention for their simplicity and superior performance in high-resolution image synthesis.","By leveraging the instantaneous change-of-variables formula, one can directly compute image likelihoods from a learned flow, making them enticing candidates as priors for downstream tasks such as inverse problems.","In particular, a natural approach would be to incorporate such image probabilities in a maximum-a-posteriori (MAP) estimation problem.","A major obstacle, however, lies in the slow computation of the log-likelihood, as it requires backpropagating through an ODE solver, which can be prohibitively slow for high-dimensional problems.","In this work, we propose an iterative algorithm to approximate the MAP estimator efficiently to solve a variety of linear inverse problems.","Our algorithm is mathematically justified by the observation that the MAP objective can be approximated by a sum of $N$ ``local MAP'' objectives, where $N$ is the number of function evaluations.","By leveraging Tweedie's formula, we show that we can perform gradient steps to sequentially optimize these objectives.","We validate our approach for various linear inverse problems, such as super-resolution, deblurring, inpainting, and compressed sensing, and demonstrate that we can outperform other methods based on flow matching."],"url":"http://arxiv.org/abs/2405.18816v1","category":"cs.CV"}
{"created":"2024-05-29 06:43:49","title":"SketchTriplet: Self-Supervised Scenarized Sketch-Text-Image Triplet Generation","abstract":"The scarcity of free-hand sketch presents a challenging problem. Despite the emergence of some large-scale sketch datasets, these datasets primarily consist of sketches at the single-object level. There continues to be a lack of large-scale paired datasets for scene sketches. In this paper, we propose a self-supervised method for scene sketch generation that does not rely on any existing scene sketch, enabling the transformation of single-object sketches into scene sketches. To accomplish this, we introduce a method for vector sketch captioning and sketch semantic expansion. Additionally, we design a sketch generation network that incorporates a fusion of multi-modal perceptual constraints, suitable for application in zero-shot image-to-sketch downstream task, demonstrating state-of-the-art performance through experimental validation. Finally, leveraging our proposed sketch-to-sketch generation method, we contribute a large-scale dataset centered around scene sketches, comprising highly semantically consistent \"text-sketch-image\" triplets. Our research confirms that this dataset can significantly enhance the capabilities of existing models in sketch-based image retrieval and sketch-controlled image synthesis tasks. We will make our dataset and code publicly available.","sentences":["The scarcity of free-hand sketch presents a challenging problem.","Despite the emergence of some large-scale sketch datasets, these datasets primarily consist of sketches at the single-object level.","There continues to be a lack of large-scale paired datasets for scene sketches.","In this paper, we propose a self-supervised method for scene sketch generation that does not rely on any existing scene sketch, enabling the transformation of single-object sketches into scene sketches.","To accomplish this, we introduce a method for vector sketch captioning and sketch semantic expansion.","Additionally, we design a sketch generation network that incorporates a fusion of multi-modal perceptual constraints, suitable for application in zero-shot image-to-sketch downstream task, demonstrating state-of-the-art performance through experimental validation.","Finally, leveraging our proposed sketch-to-sketch generation method, we contribute a large-scale dataset centered around scene sketches, comprising highly semantically consistent \"text-sketch-image\" triplets.","Our research confirms that this dataset can significantly enhance the capabilities of existing models in sketch-based image retrieval and sketch-controlled image synthesis tasks.","We will make our dataset and code publicly available."],"url":"http://arxiv.org/abs/2405.18801v1","category":"cs.CV"}
{"created":"2024-05-29 06:33:29","title":"Effective Potential and Topological Photon Spheres: A Novel Approach to Black Hole Parameter Classification","abstract":"In this article, we anchor our analysis on the premise that the presence of a photon sphere is an intrinsic characteristic of any ultra-compact gravitational structure with spherical symmetry. Utilizing the concept of a topological photon sphere, we categorize the behaviors of diverse gravitational models based on the structure of their photon spheres. This innovative approach enables us to define boundaries for black hole parameters, which subsequently allows us to classify the model as either a black hole or a naked singularity. Indeed, we will demonstrate that the existence of this reciprocal relationship between the gravitational structure and the presence of a photon sphere offers a unique advantage that can be directly and inversely harnessed. Furthermore, we present a precise correlation between the behavior of the defined effective potential and the total topological charge for each model. Through an independent examination of the effective potential, we show that a gravitational model typically exhibits naked singularity behavior when at least one effective potential minimum (a stable photon sphere) protrudes beyond the event horizon. Finally, we will compare the effect of adding perfect fluid dark matter to the models under study and observe the impact of this quantity on the behavior of the photon sphere and the extent of their dominance.","sentences":["In this article, we anchor our analysis on the premise that the presence of a photon sphere is an intrinsic characteristic of any ultra-compact gravitational structure with spherical symmetry.","Utilizing the concept of a topological photon sphere, we categorize the behaviors of diverse gravitational models based on the structure of their photon spheres.","This innovative approach enables us to define boundaries for black hole parameters, which subsequently allows us to classify the model as either a black hole or a naked singularity.","Indeed, we will demonstrate that the existence of this reciprocal relationship between the gravitational structure and the presence of a photon sphere offers a unique advantage that can be directly and inversely harnessed.","Furthermore, we present a precise correlation between the behavior of the defined effective potential and the total topological charge for each model.","Through an independent examination of the effective potential, we show that a gravitational model typically exhibits naked singularity behavior when at least one effective potential minimum (a stable photon sphere) protrudes beyond the event horizon.","Finally, we will compare the effect of adding perfect fluid dark matter to the models under study and observe the impact of this quantity on the behavior of the photon sphere and the extent of their dominance."],"url":"http://arxiv.org/abs/2405.18798v1","category":"gr-qc"}
{"created":"2024-05-29 06:33:25","title":"User Association and Channel Allocation in 5G Mobile Asymmetric Multi-band Heterogeneous Networks","abstract":"With the proliferation of mobile terminals and the continuous upgrading of services, 4G LTE networks are showing signs of weakness. To enhance the capacity of wireless networks, millimeter waves are introduced to drive the evolution of networks towards multi-band 5G heterogeneous networks. The distinct propagation characteristics of mmWaves and microwaves, as well as the vastly different hardware configurations of heterogeneous base stations, make traditional access strategies no longer effective. Therefore, to narrowing the gap between theory and practice, we investigate the access strategy in multi-band 5G heterogeneous networks, taking into account the characteristics of mobile users, asynchronous switching between uplink and downlink of pico base stations, asymmetric service requirements, and user communication continuity. We formulate the problem as integer nonlinear programming and prove its intractability. Thereby, we decouple it into three subproblems: user association, switch point selection, and subchannel allocation, and design an algorithm based on optimal matching and spectral clustering to solve it efficiently. The simulation results show that the proposed algorithm outperforms the comparison methods in terms of overall data rate, effective data rate, and number of satisfied users.","sentences":["With the proliferation of mobile terminals and the continuous upgrading of services, 4G LTE networks are showing signs of weakness.","To enhance the capacity of wireless networks, millimeter waves are introduced to drive the evolution of networks towards multi-band 5G heterogeneous networks.","The distinct propagation characteristics of mmWaves and microwaves, as well as the vastly different hardware configurations of heterogeneous base stations, make traditional access strategies no longer effective.","Therefore, to narrowing the gap between theory and practice, we investigate the access strategy in multi-band 5G heterogeneous networks, taking into account the characteristics of mobile users, asynchronous switching between uplink and downlink of pico base stations, asymmetric service requirements, and user communication continuity.","We formulate the problem as integer nonlinear programming and prove its intractability.","Thereby, we decouple it into three subproblems: user association, switch point selection, and subchannel allocation, and design an algorithm based on optimal matching and spectral clustering to solve it efficiently.","The simulation results show that the proposed algorithm outperforms the comparison methods in terms of overall data rate, effective data rate, and number of satisfied users."],"url":"http://arxiv.org/abs/2405.18797v1","category":"cs.NI"}
{"created":"2024-05-29 06:26:52","title":"Federated Q-Learning with Reference-Advantage Decomposition: Almost Optimal Regret and Logarithmic Communication Cost","abstract":"In this paper, we consider model-free federated reinforcement learning for tabular episodic Markov decision processes. Under the coordination of a central server, multiple agents collaboratively explore the environment and learn an optimal policy without sharing their raw data. Despite recent advances in federated Q-learning algorithms achieving near-linear regret speedup with low communication cost, existing algorithms only attain suboptimal regrets compared to the information bound. We propose a novel model-free federated Q-learning algorithm, termed FedQ-Advantage. Our algorithm leverages reference-advantage decomposition for variance reduction and operates under two distinct mechanisms: synchronization between the agents and the server, and policy update, both triggered by events. We prove that our algorithm not only requires a lower logarithmic communication cost but also achieves an almost optimal regret, reaching the information bound up to a logarithmic factor and near-linear regret speedup compared to its single-agent counterpart when the time horizon is sufficiently large.","sentences":["In this paper, we consider model-free federated reinforcement learning for tabular episodic Markov decision processes.","Under the coordination of a central server, multiple agents collaboratively explore the environment and learn an optimal policy without sharing their raw data.","Despite recent advances in federated Q-learning algorithms achieving near-linear regret speedup with low communication cost, existing algorithms only attain suboptimal regrets compared to the information bound.","We propose a novel model-free federated Q-learning algorithm, termed FedQ-Advantage.","Our algorithm leverages reference-advantage decomposition for variance reduction and operates under two distinct mechanisms: synchronization between the agents and the server, and policy update, both triggered by events.","We prove that our algorithm not only requires a lower logarithmic communication cost but also achieves an almost optimal regret, reaching the information bound up to a logarithmic factor and near-linear regret speedup compared to its single-agent counterpart when the time horizon is sufficiently large."],"url":"http://arxiv.org/abs/2405.18795v1","category":"stat.ML"}
{"created":"2024-05-29 05:58:34","title":"LP-3DGS: Learning to Prune 3D Gaussian Splatting","abstract":"Recently, 3D Gaussian Splatting (3DGS) has become one of the mainstream methodologies for novel view synthesis (NVS) due to its high quality and fast rendering speed. However, as a point-based scene representation, 3DGS potentially generates a large number of Gaussians to fit the scene, leading to high memory usage. Improvements that have been proposed require either an empirical and preset pruning ratio or importance score threshold to prune the point cloud. Such hyperparamter requires multiple rounds of training to optimize and achieve the maximum pruning ratio, while maintaining the rendering quality for each scene. In this work, we propose learning-to-prune 3DGS (LP-3DGS), where a trainable binary mask is applied to the importance score that can find optimal pruning ratio automatically. Instead of using the traditional straight-through estimator (STE) method to approximate the binary mask gradient, we redesign the masking function to leverage the Gumbel-Sigmoid method, making it differentiable and compatible with the existing training process of 3DGS. Extensive experiments have shown that LP-3DGS consistently produces a good balance that is both efficient and high quality.","sentences":["Recently, 3D Gaussian Splatting (3DGS) has become one of the mainstream methodologies for novel view synthesis (NVS) due to its high quality and fast rendering speed.","However, as a point-based scene representation, 3DGS potentially generates a large number of Gaussians to fit the scene, leading to high memory usage.","Improvements that have been proposed require either an empirical and preset pruning ratio or importance score threshold to prune the point cloud.","Such hyperparamter requires multiple rounds of training to optimize and achieve the maximum pruning ratio, while maintaining the rendering quality for each scene.","In this work, we propose learning-to-prune 3DGS (LP-3DGS), where a trainable binary mask is applied to the importance score that can find optimal pruning ratio automatically.","Instead of using the traditional straight-through estimator (STE) method to approximate the binary mask gradient, we redesign the masking function to leverage the Gumbel-Sigmoid method, making it differentiable and compatible with the existing training process of 3DGS.","Extensive experiments have shown that LP-3DGS consistently produces a good balance that is both efficient and high quality."],"url":"http://arxiv.org/abs/2405.18784v1","category":"cs.CV"}
{"created":"2024-05-29 05:09:38","title":"Kinetic temperature of massive star-forming molecular clumps measured with formaldehyde V. The massive filament DR21","abstract":"The kinetic temperature structure of the massive filament DR21 has been mapped using the IRAM 30 m telescope. This mapping employed the para-H$_2$CO triplet ($J_{\\rm K_aK_c}$ = 3$_{03}$--2$_{02}$, 3$_{22}$--2$_{21}$, and 3$_{21}$--2$_{20}$) on a scale of $\\sim$0.1 pc. By modeling the averaged line ratios of para-H$_{2}$CO with RADEX under non-LTE assumptions, the kinetic temperature of the dense gas was derived at a density of $n$(H$_{2}$) = 10$^{5}$ cm$^{-3}$. The para-H$_2$CO lines reveal significantly higher temperatures than NH$_3$ (1,1)/(2,2) and FIR wavelengths. The dense clumps appear to correlate with the notable kinetic temperature. Among the four dense cores (N44, N46, N48, and N54), temperature gradients are observed on a scale of $\\sim$0.1-0.3 pc. This suggests that the warm dense gas is influenced by internal star formation activity. With the exception of N54, the temperature profiles of these cores were fitted with power-law indices ranging from $-$0.3 to $-$0.5. This indicates that the warm dense gas is heated by radiation emitted from internally embedded protostar(s) and/or clusters. While there is no direct evidence supporting the idea that the dense gas is heated by shocks resulting from a past explosive event in the DR21 region, our measurements toward the DR21W1 region provide compelling evidence that the dense gas is indeed heated by shocks originating from the western DR21 flow. Higher temperatures appear to be associated with turbulence. The physical parameters of the dense gas in the DR21 filament exhibit a remarkable similarity to the results obtained in OMC-1 and N113. This may imply that the physical mechanisms governing the dynamics and thermodynamics of dense gas traced by H$_{2}$CO in diverse star formation regions may be dominated by common underlying principles despite variations in specific environmental conditions. (abbreviated)","sentences":["The kinetic temperature structure of the massive filament DR21 has been mapped using the IRAM 30 m telescope.","This mapping employed the para-H$_2$CO triplet ($J_{\\rm K_aK_c}$ = 3$_{03}$--2$_{02}$, 3$_{22}$--2$_{21}$, and 3$_{21}$--2$_{20}$) on a scale of $\\sim$0.1 pc.","By modeling the averaged line ratios of para-H$_{2}$CO with RADEX under non-LTE assumptions, the kinetic temperature of the dense gas was derived at a density of $n$(H$_{2}$) = 10$^{5}$ cm$^{-3}$.","The para-H$_2$CO lines reveal significantly higher temperatures than NH$_3$ (1,1)/(2,2) and FIR wavelengths.","The dense clumps appear to correlate with the notable kinetic temperature.","Among the four dense cores (N44, N46, N48, and N54), temperature gradients are observed on a scale of $\\sim$0.1-0.3 pc.","This suggests that the warm dense gas is influenced by internal star formation activity.","With the exception of N54, the temperature profiles of these cores were fitted with power-law indices ranging from $-$0.3 to $-$0.5.","This indicates that the warm dense gas is heated by radiation emitted from internally embedded protostar(s) and/or clusters.","While there is no direct evidence supporting the idea that the dense gas is heated by shocks resulting from a past explosive event in the DR21 region, our measurements toward the DR21W1 region provide compelling evidence that the dense gas is indeed heated by shocks originating from the western DR21 flow.","Higher temperatures appear to be associated with turbulence.","The physical parameters of the dense gas in the DR21 filament exhibit a remarkable similarity to the results obtained in OMC-1 and N113.","This may imply that the physical mechanisms governing the dynamics and thermodynamics of dense gas traced by H$_{2}$CO in diverse star formation regions may be dominated by common underlying principles despite variations in specific environmental conditions.","(abbreviated)"],"url":"http://arxiv.org/abs/2405.18767v1","category":"astro-ph.GA"}
{"created":"2024-05-29 05:00:50","title":"FDQN: A Flexible Deep Q-Network Framework for Game Automation","abstract":"In reinforcement learning, it is often difficult to automate high-dimensional, rapid decision-making in dynamic environments, especially when domains require real-time online interaction and adaptive strategies such as web-based games. This work proposes a state-of-the-art Flexible Deep Q-Network (FDQN) framework that can address this challenge with a selfadaptive approach that is processing high-dimensional sensory data in realtime using a CNN and dynamically adapting the model architecture to varying action spaces of different gaming environments and outperforming previous baseline models in various Atari games and the Chrome Dino game as baselines. Using the epsilon-greedy policy, it effectively balances the new learning and exploitation for improved performance, and it has been designed with a modular structure that it can be easily adapted to other HTML-based games without touching the core part of the framework. It is demonstrated that the FDQN framework can successfully solve a well-defined task in a laboratory condition, but more importantly it also discusses potential applications to more challenging real-world cases and serve as the starting point for future further exploration into automated game play and beyond.","sentences":["In reinforcement learning, it is often difficult to automate high-dimensional, rapid decision-making in dynamic environments, especially when domains require real-time online interaction and adaptive strategies such as web-based games.","This work proposes a state-of-the-art Flexible Deep Q-Network (FDQN) framework that can address this challenge with a selfadaptive approach that is processing high-dimensional sensory data in realtime using a CNN and dynamically adapting the model architecture to varying action spaces of different gaming environments and outperforming previous baseline models in various Atari games and the Chrome Dino game as baselines.","Using the epsilon-greedy policy, it effectively balances the new learning and exploitation for improved performance, and it has been designed with a modular structure that it can be easily adapted to other HTML-based games without touching the core part of the framework.","It is demonstrated that the FDQN framework can successfully solve a well-defined task in a laboratory condition, but more importantly it also discusses potential applications to more challenging real-world cases and serve as the starting point for future further exploration into automated game play and beyond."],"url":"http://arxiv.org/abs/2405.18761v1","category":"cs.LG"}
{"created":"2024-05-29 04:50:53","title":"Multi-objective Cross-task Learning via Goal-conditioned GPT-based Decision Transformers for Surgical Robot Task Automation","abstract":"Surgical robot task automation has been a promising research topic for improving surgical efficiency and quality. Learning-based methods have been recognized as an interesting paradigm and been increasingly investigated. However, existing approaches encounter difficulties in long-horizon goal-conditioned tasks due to the intricate compositional structure, which requires decision-making for a sequence of sub-steps and understanding of inherent dynamics of goal-reaching tasks. In this paper, we propose a new learning-based framework by leveraging the strong reasoning capability of the GPT-based architecture to automate surgical robotic tasks. The key to our approach is developing a goal-conditioned decision transformer to achieve sequential representations with goal-aware future indicators in order to enhance temporal reasoning. Moreover, considering to exploit a general understanding of dynamics inherent in manipulations, thus making the model's reasoning ability to be task-agnostic, we also design a cross-task pretraining paradigm that uses multiple training objectives associated with data from diverse tasks. We have conducted extensive experiments on 10 tasks using the surgical robot learning simulator SurRoL~\\cite{long2023human}. The results show that our new approach achieves promising performance and task versatility compared to existing methods. The learned trajectories can be deployed on the da Vinci Research Kit (dVRK) for validating its practicality in real surgical robot settings. Our project website is at: https://med-air.github.io/SurRoL.","sentences":["Surgical robot task automation has been a promising research topic for improving surgical efficiency and quality.","Learning-based methods have been recognized as an interesting paradigm and been increasingly investigated.","However, existing approaches encounter difficulties in long-horizon goal-conditioned tasks due to the intricate compositional structure, which requires decision-making for a sequence of sub-steps and understanding of inherent dynamics of goal-reaching tasks.","In this paper, we propose a new learning-based framework by leveraging the strong reasoning capability of the GPT-based architecture to automate surgical robotic tasks.","The key to our approach is developing a goal-conditioned decision transformer to achieve sequential representations with goal-aware future indicators in order to enhance temporal reasoning.","Moreover, considering to exploit a general understanding of dynamics inherent in manipulations, thus making the model's reasoning ability to be task-agnostic, we also design a cross-task pretraining paradigm that uses multiple training objectives associated with data from diverse tasks.","We have conducted extensive experiments on 10 tasks using the surgical robot learning simulator SurRoL~\\cite{long2023human}.","The results show that our new approach achieves promising performance and task versatility compared to existing methods.","The learned trajectories can be deployed on the da Vinci Research Kit (dVRK) for validating its practicality in real surgical robot settings.","Our project website is at: https://med-air.github.io/SurRoL."],"url":"http://arxiv.org/abs/2405.18757v1","category":"cs.RO"}
{"created":"2024-05-29 04:06:50","title":"PermLLM: Private Inference of Large Language Models within 3 Seconds under WAN","abstract":"The emergence of ChatGPT marks the arrival of the large language model (LLM) era. While LLMs demonstrate their power in a variety of fields, they also raise serious privacy concerns as the users' queries are sent to the model provider. On the other side, deploying the LLM on the user's device will also leak all the model data. Existing methods based on secure multiparty computation (MPC) managed to protect both the privacy of the model parameters and user queries. However, they require gigabytes of data transfer and several minutes to generate just one token, making them impractical for most real-world applications. To improve the efficiency of private LLM inference, we propose PermLLM, which accelerates the evaluation of non-linear functions using secure random permutation. Along with the optimized secret sharing protocols and homomorphic encryption, PermLLM achieves two-party private inference of the ChatGLM-6B model at the speed of around 3s/token, under a realistic network setting (10ms RTT and 1Gbps bandwidth), which is magnitudes faster than existing MPC solutions.","sentences":["The emergence of ChatGPT marks the arrival of the large language model (LLM) era.","While LLMs demonstrate their power in a variety of fields, they also raise serious privacy concerns as the users' queries are sent to the model provider.","On the other side, deploying the LLM on the user's device will also leak all the model data.","Existing methods based on secure multiparty computation (MPC) managed to protect both the privacy of the model parameters and user queries.","However, they require gigabytes of data transfer and several minutes to generate just one token, making them impractical for most real-world applications.","To improve the efficiency of private LLM inference, we propose PermLLM, which accelerates the evaluation of non-linear functions using secure random permutation.","Along with the optimized secret sharing protocols and homomorphic encryption, PermLLM achieves two-party private inference of the ChatGLM-6B model at the speed of around 3s/token, under a realistic network setting (10ms RTT and 1Gbps bandwidth), which is magnitudes faster than existing MPC solutions."],"url":"http://arxiv.org/abs/2405.18744v1","category":"cs.CR"}
{"created":"2024-05-29 04:00:41","title":"Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs","abstract":"Despite impressive advances in recent multimodal large language models (MLLMs), state-of-the-art models such as from the GPT-4 suite still struggle with knowledge-intensive tasks. To address this, we consider Reverse Image Retrieval (RIR) augmented generation, a simple yet effective strategy to augment MLLMs with web-scale reverse image search results. RIR robustly improves knowledge-intensive visual question answering (VQA) of GPT-4V by 37-43%, GPT-4 Turbo by 25-27%, and GPT-4o by 18-20% in terms of open-ended VQA evaluation metrics. To our surprise, we discover that RIR helps the model to better access its own world knowledge. Concretely, our experiments suggest that RIR augmentation helps by providing further visual and textual cues without necessarily containing the direct answer to a query. In addition, we elucidate cases in which RIR can hurt performance and conduct a human evaluation. Finally, we find that the overall advantage of using RIR makes it difficult for an agent that can choose to use RIR to perform better than an approach where RIR is the default setting.","sentences":["Despite impressive advances in recent multimodal large language models (MLLMs), state-of-the-art models such as from the GPT-4 suite still struggle with knowledge-intensive tasks.","To address this, we consider Reverse Image Retrieval (RIR) augmented generation, a simple yet effective strategy to augment MLLMs with web-scale reverse image search results.","RIR robustly improves knowledge-intensive visual question answering (VQA) of GPT-4V by 37-43%, GPT-4 Turbo by 25-27%, and GPT-4o by 18-20% in terms of open-ended VQA evaluation metrics.","To our surprise, we discover that RIR helps the model to better access its own world knowledge.","Concretely, our experiments suggest that RIR augmentation helps by providing further visual and textual cues without necessarily containing the direct answer to a query.","In addition, we elucidate cases in which RIR can hurt performance and conduct a human evaluation.","Finally, we find that the overall advantage of using RIR makes it difficult for an agent that can choose to use RIR to perform better than an approach where RIR is the default setting."],"url":"http://arxiv.org/abs/2405.18740v1","category":"cs.CL"}
{"created":"2024-05-29 03:18:48","title":"A Tick-by-Tick Solution for Concentrated Liquidity Provisioning","abstract":"Automated market makers with concentrated liquidity capabilities are programmable at the tick level. The maximization of earned fees, plus depreciated reserves, is a convex optimization problem whose vector solution gives the best provision of liquidity at each tick under a given set of parameter estimates for swap volume and price volatility. Surprisingly, early results show that concentrating liquidity around the current price is usually not the best strategy.","sentences":["Automated market makers with concentrated liquidity capabilities are programmable at the tick level.","The maximization of earned fees, plus depreciated reserves, is a convex optimization problem whose vector solution gives the best provision of liquidity at each tick under a given set of parameter estimates for swap volume and price volatility.","Surprisingly, early results show that concentrating liquidity around the current price is usually not the best strategy."],"url":"http://arxiv.org/abs/2405.18728v1","category":"q-fin.PM"}
{"created":"2024-05-29 02:53:59","title":"SketchDeco: Decorating B&W Sketches with Colour","abstract":"This paper introduces a novel approach to sketch colourisation, inspired by the universal childhood activity of colouring and its professional applications in design and story-boarding. Striking a balance between precision and convenience, our method utilises region masks and colour palettes to allow intuitive user control, steering clear of the meticulousness of manual colour assignments or the limitations of textual prompts. By strategically combining ControlNet and staged generation, incorporating Stable Diffusion v1.5, and leveraging BLIP-2 text prompts, our methodology facilitates faithful image generation and user-directed colourisation. Addressing challenges of local and global consistency, we employ inventive solutions such as an inversion scheme, guided sampling, and a self-attention mechanism with a scaling factor. The resulting tool is not only fast and training-free but also compatible with consumer-grade Nvidia RTX 4090 Super GPUs, making it a valuable asset for both creative professionals and enthusiasts in various fields. Project Page: \\url{https://chaitron.github.io/SketchDeco/}","sentences":["This paper introduces a novel approach to sketch colourisation, inspired by the universal childhood activity of colouring and its professional applications in design and story-boarding.","Striking a balance between precision and convenience, our method utilises region masks and colour palettes to allow intuitive user control, steering clear of the meticulousness of manual colour assignments or the limitations of textual prompts.","By strategically combining ControlNet and staged generation, incorporating Stable Diffusion v1.5, and leveraging BLIP-2 text prompts, our methodology facilitates faithful image generation and user-directed colourisation.","Addressing challenges of local and global consistency, we employ inventive solutions such as an inversion scheme, guided sampling, and a self-attention mechanism with a scaling factor.","The resulting tool is not only fast and training-free but also compatible with consumer-grade Nvidia RTX 4090 Super GPUs, making it a valuable asset for both creative professionals and enthusiasts in various fields.","Project Page: \\url{https://chaitron.github.io/SketchDeco/}"],"url":"http://arxiv.org/abs/2405.18716v1","category":"cs.CV"}
{"created":"2024-05-29 02:53:40","title":"NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild","abstract":"Neural Radiance Fields (NeRFs) have shown remarkable success in synthesizing photorealistic views from multi-view images of static scenes, but face challenges in dynamic, real-world environments with distractors like moving objects, shadows, and lighting changes. Existing methods manage controlled environments and low occlusion ratios but fall short in render quality, especially under high occlusion scenarios. In this paper, we introduce NeRF On-the-go, a simple yet effective approach that enables the robust synthesis of novel views in complex, in-the-wild scenes from only casually captured image sequences. Delving into uncertainty, our method not only efficiently eliminates distractors, even when they are predominant in captures, but also achieves a notably faster convergence speed. Through comprehensive experiments on various scenes, our method demonstrates a significant improvement over state-of-the-art techniques. This advancement opens new avenues for NeRF in diverse and dynamic real-world applications.","sentences":["Neural Radiance Fields (NeRFs) have shown remarkable success in synthesizing photorealistic views from multi-view images of static scenes, but face challenges in dynamic, real-world environments with distractors like moving objects, shadows, and lighting changes.","Existing methods manage controlled environments and low occlusion ratios but fall short in render quality, especially under high occlusion scenarios.","In this paper, we introduce NeRF On-the-go, a simple yet effective approach that enables the robust synthesis of novel views in complex, in-the-wild scenes from only casually captured image sequences.","Delving into uncertainty, our method not only efficiently eliminates distractors, even when they are predominant in captures, but also achieves a notably faster convergence speed.","Through comprehensive experiments on various scenes, our method demonstrates a significant improvement over state-of-the-art techniques.","This advancement opens new avenues for NeRF in diverse and dynamic real-world applications."],"url":"http://arxiv.org/abs/2405.18715v1","category":"cs.CV"}
{"created":"2024-05-29 02:45:07","title":"Identifying the Most Influential Driver Nodes for Pinning Control of Multi-Agent Systems with Time-Varying Topology","abstract":"Identifying the most influential driver nodes to guarantee the fastest synchronization speed is a key topic in pinning control of multi-agent systems. This paper develops a methodology to find the most influential pinning nodes under time-varying topologies. First, we provide the pinning control synchronization conditions of multi-agent systems. Second, a method is proposed to identify the best driver nodes that can guarantee the fastest synchronization speed under periodically switched systems. We show that the determination of the best driver nodes is independent of the system matrix under certain conditions. Finally, we develop a method to estimate the switching frequency threshold that can make the selected best driver nodes remain the same as the average system. Numerical simulations reveal the feasibility of these methods.","sentences":["Identifying the most influential driver nodes to guarantee the fastest synchronization speed is a key topic in pinning control of multi-agent systems.","This paper develops a methodology to find the most influential pinning nodes under time-varying topologies.","First, we provide the pinning control synchronization conditions of multi-agent systems.","Second, a method is proposed to identify the best driver nodes that can guarantee the fastest synchronization speed under periodically switched systems.","We show that the determination of the best driver nodes is independent of the system matrix under certain conditions.","Finally, we develop a method to estimate the switching frequency threshold that can make the selected best driver nodes remain the same as the average system.","Numerical simulations reveal the feasibility of these methods."],"url":"http://arxiv.org/abs/2405.18712v1","category":"eess.SY"}
{"created":"2024-05-29 02:27:47","title":"Bridging the Gap between Partially Observable Stochastic Games and Sparse POMDP Methods","abstract":"Many real-world decision problems involve interaction of multiple self-interested agents with limited sensing ability. The partially observable stochastic game (POSG) provides a mathematical framework for posing these problems, however solving a POSG requires difficult reasoning about two critical factors: (1) information revealed by partial observations and (2) decisions other agents make. In the single agent case, partially observable Markov decision process (POMDP) planning can efficiently address partial observability with particle filtering. In the multi-agent case, imperfect information game solution methods account for other agent's decisions, but preclude belief approximation. We propose a unifying framework that combines POMDP-inspired state distribution approximation and game-theoretic equilibrium search on information sets. This approach enables online planning in POSGs with very large state spaces, paving the way for reliable autonomous interaction in real-world physical environments and complementing offline multi-agent reinforcement learning. Experiments in several zero-sum examples show that the new framework computes solutions for problems with both small and large state spaces.","sentences":["Many real-world decision problems involve interaction of multiple self-interested agents with limited sensing ability.","The partially observable stochastic game (POSG) provides a mathematical framework for posing these problems, however solving a POSG requires difficult reasoning about two critical factors: (1) information revealed by partial observations and (2) decisions other agents make.","In the single agent case, partially observable Markov decision process (POMDP) planning can efficiently address partial observability with particle filtering.","In the multi-agent case, imperfect information game solution methods account for other agent's decisions, but preclude belief approximation.","We propose a unifying framework that combines POMDP-inspired state distribution approximation and game-theoretic equilibrium search on information sets.","This approach enables online planning in POSGs with very large state spaces, paving the way for reliable autonomous interaction in real-world physical environments and complementing offline multi-agent reinforcement learning.","Experiments in several zero-sum examples show that the new framework computes solutions for problems with both small and large state spaces."],"url":"http://arxiv.org/abs/2405.18703v1","category":"cs.GT"}
{"created":"2024-05-29 02:08:58","title":"Signal-Comparison-Based Distributed Estimation Under Decaying Average Bit Rate Communications","abstract":"The paper investigates the distributed estimation problem under low bit rate communications. Based on the signal-comparison (SC) consensus protocol under binary-valued communications, a new consensus+innovations type distributed estimation algorithm is proposed. Firstly, the high-dimensional estimates are compressed into binary-valued messages by using a periodic compressive strategy, dithered noises and a sign function. Next, based on the dithered noises and expanding triggering thresholds, a new stochastic event-triggered mechanism is proposed to reduce the communication frequency. Then, a modified SC consensus protocol is applied to fuse the neighborhood information. Finally, a stochastic approximation estimation algorithm is used to process innovations. The proposed SC-based algorithm has the advantages of high effectiveness and low communication cost. For the effectiveness, the estimates of the SC-based algorithm converge to the true value in the almost sure and mean square sense. A polynomial almost sure convergence rate is also obtained. For the communication cost, the local and global average bit rates for communications decay to zero at a polynomial rate. The trade-off between the convergence rate and the communication cost is established through event-triggered coefficients. A better convergence rate can be achieved by decreasing event-triggered coefficients, while lower communication cost can be achieved by increasing event-triggered coefficients. A simulation example is given to demonstrate the theoretical results.","sentences":["The paper investigates the distributed estimation problem under low bit rate communications.","Based on the signal-comparison (SC) consensus protocol under binary-valued communications, a new consensus+innovations type distributed estimation algorithm is proposed.","Firstly, the high-dimensional estimates are compressed into binary-valued messages by using a periodic compressive strategy, dithered noises and a sign function.","Next, based on the dithered noises and expanding triggering thresholds, a new stochastic event-triggered mechanism is proposed to reduce the communication frequency.","Then, a modified SC consensus protocol is applied to fuse the neighborhood information.","Finally, a stochastic approximation estimation algorithm is used to process innovations.","The proposed SC-based algorithm has the advantages of high effectiveness and low communication cost.","For the effectiveness, the estimates of the SC-based algorithm converge to the true value in the almost sure and mean square sense.","A polynomial almost sure convergence rate is also obtained.","For the communication cost, the local and global average bit rates for communications decay to zero at a polynomial rate.","The trade-off between the convergence rate and the communication cost is established through event-triggered coefficients.","A better convergence rate can be achieved by decreasing event-triggered coefficients, while lower communication cost can be achieved by increasing event-triggered coefficients.","A simulation example is given to demonstrate the theoretical results."],"url":"http://arxiv.org/abs/2405.18694v1","category":"eess.SY"}
{"created":"2024-05-29 01:46:50","title":"Advancing Household Robotics: Deep Interactive Reinforcement Learning for Efficient Training and Enhanced Performance","abstract":"The market for domestic robots made to perform household chores is growing as these robots relieve people of everyday responsibilities. Domestic robots are generally welcomed for their role in easing human labor, in contrast to industrial robots, which are frequently criticized for displacing human workers. But before these robots can carry out domestic chores, they need to become proficient in several minor activities, such as recognizing their surroundings, making decisions, and picking up on human behaviors. Reinforcement learning, or RL, has emerged as a key robotics technology that enables robots to interact with their environment and learn how to optimize their actions to maximize rewards. However, the goal of Deep Reinforcement Learning is to address more complicated, continuous action-state spaces in real-world settings by combining RL with Neural Networks. The efficacy of DeepRL can be further augmented through interactive feedback, in which a trainer offers real-time guidance to expedite the robot's learning process. Nevertheless, the current methods have drawbacks, namely the transient application of guidance that results in repeated learning under identical conditions. Therefore, we present a novel method to preserve and reuse information and advice via Deep Interactive Reinforcement Learning, which utilizes a persistent rule-based system. This method not only expedites the training process but also lessens the number of repetitions that instructors will have to carry out. This study has the potential to advance the development of household robots and improve their effectiveness and efficiency as learners.","sentences":["The market for domestic robots made to perform household chores is growing as these robots relieve people of everyday responsibilities.","Domestic robots are generally welcomed for their role in easing human labor, in contrast to industrial robots, which are frequently criticized for displacing human workers.","But before these robots can carry out domestic chores, they need to become proficient in several minor activities, such as recognizing their surroundings, making decisions, and picking up on human behaviors.","Reinforcement learning, or RL, has emerged as a key robotics technology that enables robots to interact with their environment and learn how to optimize their actions to maximize rewards.","However, the goal of Deep Reinforcement Learning is to address more complicated, continuous action-state spaces in real-world settings by combining RL with Neural Networks.","The efficacy of DeepRL can be further augmented through interactive feedback, in which a trainer offers real-time guidance to expedite the robot's learning process.","Nevertheless, the current methods have drawbacks, namely the transient application of guidance that results in repeated learning under identical conditions.","Therefore, we present a novel method to preserve and reuse information and advice via Deep Interactive Reinforcement Learning, which utilizes a persistent rule-based system.","This method not only expedites the training process but also lessens the number of repetitions that instructors will have to carry out.","This study has the potential to advance the development of household robots and improve their effectiveness and efficiency as learners."],"url":"http://arxiv.org/abs/2405.18687v1","category":"cs.RO"}
{"created":"2024-05-29 01:32:17","title":"Rejection via Learning Density Ratios","abstract":"Classification with rejection emerges as a learning paradigm which allows models to abstain from making predictions. The predominant approach is to alter the supervised learning pipeline by augmenting typical loss functions, letting model rejection incur a lower loss than an incorrect prediction. Instead, we propose a different distributional perspective, where we seek to find an idealized data distribution which maximizes a pretrained model's performance. This can be formalized via the optimization of a loss's risk with a $ \\phi$-divergence regularization term. Through this idealized distribution, a rejection decision can be made by utilizing the density ratio between this distribution and the data distribution. We focus on the setting where our $ \\phi $-divergences are specified by the family of $ \\alpha $-divergence. Our framework is tested empirically over clean and noisy datasets.","sentences":["Classification with rejection emerges as a learning paradigm which allows models to abstain from making predictions.","The predominant approach is to alter the supervised learning pipeline by augmenting typical loss functions, letting model rejection incur a lower loss than an incorrect prediction.","Instead, we propose a different distributional perspective, where we seek to find an idealized data distribution which maximizes a pretrained model's performance.","This can be formalized via the optimization of a loss's risk with a $ \\phi$-divergence regularization term.","Through this idealized distribution, a rejection decision can be made by utilizing the density ratio between this distribution and the data distribution.","We focus on the setting where our $ \\phi $-divergences are specified by the family of $ \\alpha $-divergence.","Our framework is tested empirically over clean and noisy datasets."],"url":"http://arxiv.org/abs/2405.18686v1","category":"stat.ML"}
{"created":"2024-05-29 01:30:22","title":"Low-Mass Galaxy Interactions Trigger Black Hole Activity","abstract":"The existence of high-$z$ over-massive supermassive black holes represents a major conundrum in our understanding of black hole evolution. In this paper, we probe from the observational point of view how early Universe environmental conditions could have acted as an evolutionary mechanism for the accelerated growth of the first black holes. Under the assumption that the early Universe is dominated by dwarf galaxies, we investigate the hypothesis that dwarf-dwarf galaxy interactions trigger black hole accretion. We present the discovery of 82 dwarf-dwarf galaxy pairs and 11 dwarf galaxy groups using the Hubble Space Telescope, doubling existing samples. The dwarf systems span a redshift range of 0.13$<$z$<$1.5, and a stellar mass range of 7.24$<$log(M$_*$/\\(M_\\odot\\))$<$9.73. We performed an X-ray study of a subset of these dwarf systems with Chandra and detected six new AGN, increasing the number of known dwarf-dwarf-merger-related AGN from one to seven. We then compared the frequency of these AGN in grouped/paired dwarfs to that of isolated dwarfs and found a statistically significant enhancement (4$\\sigma$-6$\\sigma$) in the interacting sample. This study, the first of its kind at the lowest mass scales, implies that the presence of a nearby dwarf neighbor is efficient in triggering black hole accretion. These results open new avenues for indirect studies of the emergence of the first supermassive black holes.","sentences":["The existence of high-$z$ over-massive supermassive black holes represents a major conundrum in our understanding of black hole evolution.","In this paper, we probe from the observational point of view how early Universe environmental conditions could have acted as an evolutionary mechanism for the accelerated growth of the first black holes.","Under the assumption that the early Universe is dominated by dwarf galaxies, we investigate the hypothesis that dwarf-dwarf galaxy interactions trigger black hole accretion.","We present the discovery of 82 dwarf-dwarf galaxy pairs and 11 dwarf galaxy groups using the Hubble Space Telescope, doubling existing samples.","The dwarf systems span a redshift range of 0.13$<$z$<$1.5, and a stellar mass range of 7.24$<$log(M$_*$/\\(M_\\odot\\))$<$9.73.","We performed an X-ray study of a subset of these dwarf systems with Chandra and detected six new AGN, increasing the number of known dwarf-dwarf-merger-related AGN from one to seven.","We then compared the frequency of these AGN in grouped/paired dwarfs to that of isolated dwarfs and found a statistically significant enhancement (4$\\sigma$-6$\\sigma$) in the interacting sample.","This study, the first of its kind at the lowest mass scales, implies that the presence of a nearby dwarf neighbor is efficient in triggering black hole accretion.","These results open new avenues for indirect studies of the emergence of the first supermassive black holes."],"url":"http://arxiv.org/abs/2405.18685v1","category":"astro-ph.GA"}
{"created":"2024-05-29 01:07:26","title":"Navigable Graphs for High-Dimensional Nearest Neighbor Search: Constructions and Limits","abstract":"There has been significant recent interest in graph-based nearest neighbor search methods, many of which are centered on the construction of navigable graphs over high-dimensional point sets. A graph is navigable if we can successfully move from any starting node to any target node using a greedy routing strategy where we always move to the neighbor that is closest to the destination according to a given distance function. The complete graph is navigable for any point set, but the important question for applications is if sparser graphs can be constructed. While this question is fairly well understood in low-dimensions, we establish some of the first upper and lower bounds for high-dimensional point sets. First, we give a simple and efficient way to construct a navigable graph with average degree $O(\\sqrt{n \\log n })$ for any set of $n$ points, in any dimension, for any distance function. We compliment this result with a nearly matching lower bound: even under the Euclidean metric in $O(\\log n)$ dimensions, a random point set has no navigable graph with average degree $O(n^{\\alpha})$ for any $\\alpha < 1/2$. Our lower bound relies on sharp anti-concentration bounds for binomial random variables, which we use to show that the near-neighborhoods of a set of random points do not overlap significantly, forcing any navigable graph to have many edges.","sentences":["There has been significant recent interest in graph-based nearest neighbor search methods, many of which are centered on the construction of navigable graphs over high-dimensional point sets.","A graph is navigable if we can successfully move from any starting node to any target node using a greedy routing strategy where we always move to the neighbor that is closest to the destination according to a given distance function.","The complete graph is navigable for any point set, but the important question for applications is if sparser graphs can be constructed.","While this question is fairly well understood in low-dimensions, we establish some of the first upper and lower bounds for high-dimensional point sets.","First, we give a simple and efficient way to construct a navigable graph with average degree $O(\\sqrt{n \\log n })$ for any set of $n$ points, in any dimension, for any distance function.","We compliment this result with a nearly matching lower bound: even under the Euclidean metric in $O(\\log n)$ dimensions, a random point set has no navigable graph with average degree $O(n^{\\alpha})$ for any $\\alpha < 1/2$. Our lower bound relies on sharp anti-concentration bounds for binomial random variables, which we use to show that the near-neighborhoods of a set of random points do not overlap significantly, forcing any navigable graph to have many edges."],"url":"http://arxiv.org/abs/2405.18680v1","category":"cs.DS"}
{"created":"2024-05-29 00:42:10","title":"Light-induced topological phase transition with tunable layer Hall effect in axion antiferromagnets","abstract":"The intricate interplay between light and matter provides effective tools for manipulating topological phenomena. Here, we theoretically propose and computationally show that circularly polarized light hold the potential to transform the axion insulating phase into quantum anomalous Hall state in MnBi2Te4 thin films, featuring tunable Chern numbers (ranging up to 2). In particular, we reveal the spatial rearrangement of the hidden layer-resolved anomalous Hall effect under light driven Floquet-engineering. Notably, upon Bi2Te3 layer intercalation, the anomalous Hall conductance predominantly localizes in the nonmagnetic Bi2Te3 layers that hold zero Berry curvature in the intact state, suggesting significant magnetic proximity effect. Additionally, we estimate variations in the magneto-optical Kerr effect, giving a contactless method for detecting topological transitions. Our work not only presents a strategy to investigate emergent topological phases, but also sheds light on the possible applications of the layer Hall effect in topological antiferromagnetic spintronics.","sentences":["The intricate interplay between light and matter provides effective tools for manipulating topological phenomena.","Here, we theoretically propose and computationally show that circularly polarized light hold the potential to transform the axion insulating phase into quantum anomalous Hall state in MnBi2Te4 thin films, featuring tunable Chern numbers (ranging up to 2).","In particular, we reveal the spatial rearrangement of the hidden layer-resolved anomalous Hall effect under light driven Floquet-engineering.","Notably, upon Bi2Te3 layer intercalation, the anomalous Hall conductance predominantly localizes in the nonmagnetic Bi2Te3 layers that hold zero Berry curvature in the intact state, suggesting significant magnetic proximity effect.","Additionally, we estimate variations in the magneto-optical Kerr effect, giving a contactless method for detecting topological transitions.","Our work not only presents a strategy to investigate emergent topological phases, but also sheds light on the possible applications of the layer Hall effect in topological antiferromagnetic spintronics."],"url":"http://arxiv.org/abs/2405.18675v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-29 00:36:56","title":"LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained Image Classification","abstract":"Recent advancements in interpretable models for vision-language tasks have achieved competitive performance; however, their interpretability often suffers due to the reliance on unstructured text outputs from large language models (LLMs). This introduces randomness and compromises both transparency and reliability, which are essential for addressing safety issues in AI systems. We introduce \\texttt{Hi-CoDe} (Hierarchical Concept Decomposition), a novel framework designed to enhance model interpretability through structured concept analysis. Our approach consists of two main components: (1) We use GPT-4 to decompose an input image into a structured hierarchy of visual concepts, thereby forming a visual concept tree. (2) We then employ an ensemble of simple linear classifiers that operate on concept-specific features derived from CLIP to perform classification. Our approach not only aligns with the performance of state-of-the-art models but also advances transparency by providing clear insights into the decision-making process and highlighting the importance of various concepts. This allows for a detailed analysis of potential failure modes and improves model compactness, therefore setting a new benchmark in interpretability without compromising the accuracy.","sentences":["Recent advancements in interpretable models for vision-language tasks have achieved competitive performance; however, their interpretability often suffers due to the reliance on unstructured text outputs from large language models (LLMs).","This introduces randomness and compromises both transparency and reliability, which are essential for addressing safety issues in AI systems.","We introduce \\texttt{Hi-CoDe} (Hierarchical Concept Decomposition), a novel framework designed to enhance model interpretability through structured concept analysis.","Our approach consists of two main components: (1) We use GPT-4 to decompose an input image into a structured hierarchy of visual concepts, thereby forming a visual concept tree.","(2) We then employ an ensemble of simple linear classifiers that operate on concept-specific features derived from CLIP to perform classification.","Our approach not only aligns with the performance of state-of-the-art models but also advances transparency by providing clear insights into the decision-making process and highlighting the importance of various concepts.","This allows for a detailed analysis of potential failure modes and improves model compactness, therefore setting a new benchmark in interpretability without compromising the accuracy."],"url":"http://arxiv.org/abs/2405.18672v1","category":"cs.CV"}
{"created":"2024-05-29 00:07:41","title":"Non-Driving-Related Tasks Influencing Drivers' Takeover Time: A Meta-Analysis","abstract":"Before the era of fully automated vehicles, human is consistently an indispensable part of the driving system. Various studies have investigated drivers' cooperation with the vehicle under different conditions. In this article, we analyzed how non-driving-related tasks (NDRT) influence takeover time (TOT) by conducting a meta-analysis on 37 related papers. NDRTs were transcoded into combinations of five basic dimensions to unify and demonstrate their effects on drivers. In order to interpret experimental data comprehensively, we implemented three methods. A synthetical analysis was conducted to compare the effect size between each study and subgroup. Studies with eligible control groups have been examined by the two-group analysis, followed by moderator analysis on seven variables. The results from the two-group analysis showed that both visual-mental-motoric and visual-mental tasks have significant negative effects on the takeover time and the previous type had a larger effect than the latter one. Moreover, the subgroup comparison and meta-regression in the meta-analysis part revealed the correlation between moderators and the effect size, in which the Driving Experience and the Automation Level affected the relation between NDRT and TOT. The findings of this paper can contribute to the improvement and new directions for further scientific research and engineering design.","sentences":["Before the era of fully automated vehicles, human is consistently an indispensable part of the driving system.","Various studies have investigated drivers' cooperation with the vehicle under different conditions.","In this article, we analyzed how non-driving-related tasks (NDRT) influence takeover time (TOT) by conducting a meta-analysis on 37 related papers.","NDRTs were transcoded into combinations of five basic dimensions to unify and demonstrate their effects on drivers.","In order to interpret experimental data comprehensively, we implemented three methods.","A synthetical analysis was conducted to compare the effect size between each study and subgroup.","Studies with eligible control groups have been examined by the two-group analysis, followed by moderator analysis on seven variables.","The results from the two-group analysis showed that both visual-mental-motoric and visual-mental tasks have significant negative effects on the takeover time and the previous type had a larger effect than the latter one.","Moreover, the subgroup comparison and meta-regression in the meta-analysis part revealed the correlation between moderators and the effect size, in which the Driving Experience and the Automation Level affected the relation between NDRT and TOT.","The findings of this paper can contribute to the improvement and new directions for further scientific research and engineering design."],"url":"http://arxiv.org/abs/2405.18667v1","category":"cs.HC"}
{"created":"2024-05-28 23:54:44","title":"Understanding Intrinsic Socioeconomic Biases in Large Language Models","abstract":"Large Language Models (LLMs) are increasingly integrated into critical decision-making processes, such as loan approvals and visa applications, where inherent biases can lead to discriminatory outcomes. In this paper, we examine the nuanced relationship between demographic attributes and socioeconomic biases in LLMs, a crucial yet understudied area of fairness in LLMs. We introduce a novel dataset of one million English sentences to systematically quantify socioeconomic biases across various demographic groups. Our findings reveal pervasive socioeconomic biases in both established models such as GPT-2 and state-of-the-art models like Llama 2 and Falcon. We demonstrate that these biases are significantly amplified when considering intersectionality, with LLMs exhibiting a remarkable capacity to extract multiple demographic attributes from names and then correlate them with specific socioeconomic biases. This research highlights the urgent necessity for proactive and robust bias mitigation techniques to safeguard against discriminatory outcomes when deploying these powerful models in critical real-world applications.","sentences":["Large Language Models (LLMs) are increasingly integrated into critical decision-making processes, such as loan approvals and visa applications, where inherent biases can lead to discriminatory outcomes.","In this paper, we examine the nuanced relationship between demographic attributes and socioeconomic biases in LLMs, a crucial yet understudied area of fairness in LLMs.","We introduce a novel dataset of one million English sentences to systematically quantify socioeconomic biases across various demographic groups.","Our findings reveal pervasive socioeconomic biases in both established models such as GPT-2 and state-of-the-art models like Llama 2 and Falcon.","We demonstrate that these biases are significantly amplified when considering intersectionality, with LLMs exhibiting a remarkable capacity to extract multiple demographic attributes from names and then correlate them with specific socioeconomic biases.","This research highlights the urgent necessity for proactive and robust bias mitigation techniques to safeguard against discriminatory outcomes when deploying these powerful models in critical real-world applications."],"url":"http://arxiv.org/abs/2405.18662v1","category":"cs.CL"}
{"created":"2024-05-28 23:31:21","title":"A Dynamical Systems Approach to Bots and Online Political Communication","abstract":"Bots have become increasingly prevalent in the digital sphere and have taken up a proactive role in shaping democratic processes. While previous studies have focused on their influence at the individual level, their potential macro-level impact on communication dynamics is still little understood. This study adopts an information theoretic approach from dynamical systems theory to examine the role of political bots shaping the dynamics of an online political discussion on Twitter. We quantify the components of this dynamic process in terms of its complexity, predictability, and the remaining uncertainty. Our findings suggest that bot activity is associated with increased complexity and uncertainty in the structural dynamics of online political communication. This work serves as a showcase for the use of information-theoretic measures from dynamical systems theory in modeling human-bot dynamics as a computational process that unfolds over time.","sentences":["Bots have become increasingly prevalent in the digital sphere and have taken up a proactive role in shaping democratic processes.","While previous studies have focused on their influence at the individual level, their potential macro-level impact on communication dynamics is still little understood.","This study adopts an information theoretic approach from dynamical systems theory to examine the role of political bots shaping the dynamics of an online political discussion on Twitter.","We quantify the components of this dynamic process in terms of its complexity, predictability, and the remaining uncertainty.","Our findings suggest that bot activity is associated with increased complexity and uncertainty in the structural dynamics of online political communication.","This work serves as a showcase for the use of information-theoretic measures from dynamical systems theory in modeling human-bot dynamics as a computational process that unfolds over time."],"url":"http://arxiv.org/abs/2405.18652v1","category":"cs.CY"}
{"created":"2024-05-28 22:38:33","title":"Structure evolution of nanoparticulate Fe2O3","abstract":"The atomic structure and properties of nanoparticulate Fe2O3 are characterized starting from its smallest Fe2O3 building unit through (Fe2O3)n clusters to nanometer-sized Fe2O3 particles. This is achieved by combining global structure optimizations at the density functional theory level, molecular dynamics simulations by employing tailored, ab initio parameterized interatomic potential functions and experiments. With the exception of nearly tetrahedral, adamantane-like (Fe2O3)2 small (Fe2O3)n clusters assume compact, virtually amorphous structures with little or no symmetry. For n = 2-5 (Fe2O3)n clusters consist mainly of two- and three-membered Fe-O rings. Starting from n = 5 they increasingly assume tetrahedral shape with the adamantane-like (Fe2O3)2 unit as the main building block. However, the small energy differences between different isomers of the same cluster-size make precise structural assignment for larger (Fe2O3)n clusters difficult. The tetrahedral morphology persists for Fe2O3 nanoparticles with up to 3 nm in diameter. Simulated crystallization of larger nanoparticles with diameters of about 5 nm demonstrates pronounced melting point depression and leads to formation of {\\epsilon}-Fe2O3 single crystals with hexagonal morphology. This finding is in excellent agreement with the results obtained for Fe2O3 nanopowders generated by laser vaporization and provides the first direct indication that {\\epsilon}-Fe2O3 may be thermodynamically the most stable phase in this size regime.","sentences":["The atomic structure and properties of nanoparticulate Fe2O3 are characterized starting from its smallest Fe2O3 building unit through (Fe2O3)n clusters to nanometer-sized Fe2O3 particles.","This is achieved by combining global structure optimizations at the density functional theory level, molecular dynamics simulations by employing tailored, ab initio parameterized interatomic potential functions and experiments.","With the exception of nearly tetrahedral, adamantane-like (Fe2O3)2 small (Fe2O3)n clusters assume compact, virtually amorphous structures with little or no symmetry.","For n = 2-5 (Fe2O3)n clusters consist mainly of two- and three-membered Fe-O rings.","Starting from n = 5 they increasingly assume tetrahedral shape with the adamantane-like (Fe2O3)2 unit as the main building block.","However, the small energy differences between different isomers of the same cluster-size make precise structural assignment for larger (Fe2O3)n clusters difficult.","The tetrahedral morphology persists for Fe2O3 nanoparticles with up to 3 nm in diameter.","Simulated crystallization of larger nanoparticles with diameters of about 5 nm demonstrates pronounced melting point depression and leads to formation of {\\epsilon}-Fe2O3 single crystals with hexagonal morphology.","This finding is in excellent agreement with the results obtained for Fe2O3 nanopowders generated by laser vaporization and provides the first direct indication that {\\epsilon}-Fe2O3 may be thermodynamically the most stable phase in this size regime."],"url":"http://arxiv.org/abs/2405.18637v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-28 22:33:02","title":"A Theoretical Understanding of Self-Correction through In-context Alignment","abstract":"Going beyond mimicking limited human experiences, recent studies show initial evidence that, like humans, large language models (LLMs) are capable of improving their abilities purely by self-correction, i.e., correcting previous responses through self-examination, in certain circumstances. Nevertheless, little is known about how such capabilities arise. In this work, based on a simplified setup akin to an alignment task, we theoretically analyze self-correction from an in-context learning perspective, showing that when LLMs give relatively accurate self-examinations as rewards, they are capable of refining responses in an in-context way. Notably, going beyond previous theories on over-simplified linear transformers, our theoretical construction underpins the roles of several key designs of realistic transformers for self-correction: softmax attention, multi-head attention, and the MLP block. We validate these findings extensively on synthetic datasets. Inspired by these findings, we also illustrate novel applications of self-correction, such as defending against LLM jailbreaks, where a simple self-correction step does make a large difference. We believe that these findings will inspire further research on understanding, exploiting, and enhancing self-correction for building better foundation models.","sentences":["Going beyond mimicking limited human experiences, recent studies show initial evidence that, like humans, large language models (LLMs) are capable of improving their abilities purely by self-correction, i.e., correcting previous responses through self-examination, in certain circumstances.","Nevertheless, little is known about how such capabilities arise.","In this work, based on a simplified setup akin to an alignment task, we theoretically analyze self-correction from an in-context learning perspective, showing that when LLMs give relatively accurate self-examinations as rewards, they are capable of refining responses in an in-context way.","Notably, going beyond previous theories on over-simplified linear transformers, our theoretical construction underpins the roles of several key designs of realistic transformers for self-correction: softmax attention, multi-head attention, and the MLP block.","We validate these findings extensively on synthetic datasets.","Inspired by these findings, we also illustrate novel applications of self-correction, such as defending against LLM jailbreaks, where a simple self-correction step does make a large difference.","We believe that these findings will inspire further research on understanding, exploiting, and enhancing self-correction for building better foundation models."],"url":"http://arxiv.org/abs/2405.18634v1","category":"cs.LG"}
{"created":"2024-05-28 21:42:35","title":"GLOCON Database: Design Decisions and User Manual (v1.0)","abstract":"GLOCON is a database of contentious events automatically extracted from national news sources from various countries in multiple languages. National news sources are utilized, and complete news archives are processed to create an event list for each source. Automation is achieved using a gold standard corpus sampled randomly from complete news archives (Y\\\"or\\\"uk et al. 2022) and all annotated by at least two domain experts based on the event definition provided in Duru\\c{s}an et al. (2022).","sentences":["GLOCON is a database of contentious events automatically extracted from national news sources from various countries in multiple languages.","National news sources are utilized, and complete news archives are processed to create an event list for each source.","Automation is achieved using a gold standard corpus sampled randomly from complete news archives (Y\\\"or\\\"uk et al. 2022) and all annotated by at least two domain experts based on the event definition provided in Duru\\c{s}an et al. (2022)."],"url":"http://arxiv.org/abs/2405.18613v1","category":"cs.CL"}
{"created":"2024-05-28 21:39:29","title":"Actuators \u00c0 La Mode: Modal Actuations for Soft Body Locomotion","abstract":"Traditional character animation specializes in characters with a rigidly articulated skeleton and a bipedal/quadripedal morphology. This assumption simplifies many aspects for designing physically based animations, like locomotion, but comes with the price of excluding characters of arbitrary deformable geometries. To remedy this, our framework makes use of a spatio-temporal actuation subspace built off of the natural vibration modes of the character geometry. The resulting actuation is coupled to a reduced fast soft body simulation, allowing us to formulate a locomotion optimization problem that is tractable for a wide variety of high resolution deformable characters.","sentences":["Traditional character animation specializes in characters with a rigidly articulated skeleton and a bipedal/quadripedal morphology.","This assumption simplifies many aspects for designing physically based animations, like locomotion, but comes with the price of excluding characters of arbitrary deformable geometries.","To remedy this, our framework makes use of a spatio-temporal actuation subspace built off of the natural vibration modes of the character geometry.","The resulting actuation is coupled to a reduced fast soft body simulation, allowing us to formulate a locomotion optimization problem that is tractable for a wide variety of high resolution deformable characters."],"url":"http://arxiv.org/abs/2405.18609v1","category":"cs.GR"}
{"created":"2024-05-28 21:36:54","title":"Optimizaci\u00f3n del sistema de iluminaci\u00f3n y proyecci\u00f3n para un sensor multiparam\u00e9trico acusto-\u00f3ptico","abstract":"In previous works we presented a novel multiparametric sensor to simultaneously measure the refractive index and the speed of sound in a liquid by means of the acousto-optic effect. The sensor requires an illumination system that expands the laser beam so that it interacts effectively with the liquid under study. Also, a projection system is necessary in order to adequately capture the diffraction pattern at the cell exit. In this article, we present the optimization of the sensor illumination and projection system using the optical design computational tool Zemax OpticStudio. The considered range of refractive indices of the liquid samples is between 1.33 and 1.51. The results show a correct expansion of the incident beam on the cell, and the projection system achieves an image with very few aberrations and an adequate angular separation between the maxima of the diffraction pattern. From the resulting dimensions of the sensor after the illumination and projection system optimization, we affirm that it is compact and portable.","sentences":["In previous works we presented a novel multiparametric sensor to simultaneously measure the refractive index and the speed of sound in a liquid by means of the acousto-optic effect.","The sensor requires an illumination system that expands the laser beam so that it interacts effectively with the liquid under study.","Also, a projection system is necessary in order to adequately capture the diffraction pattern at the cell exit.","In this article, we present the optimization of the sensor illumination and projection system using the optical design computational tool Zemax OpticStudio.","The considered range of refractive indices of the liquid samples is between 1.33 and 1.51.","The results show a correct expansion of the incident beam on the cell, and the projection system achieves an image with very few aberrations and an adequate angular separation between the maxima of the diffraction pattern.","From the resulting dimensions of the sensor after the illumination and projection system optimization, we affirm that it is compact and portable."],"url":"http://arxiv.org/abs/2405.18607v1","category":"physics.optics"}
{"created":"2024-05-28 21:33:12","title":"From Conformal Predictions to Confidence Regions","abstract":"Conformal prediction methodologies have significantly advanced the quantification of uncertainties in predictive models. Yet, the construction of confidence regions for model parameters presents a notable challenge, often necessitating stringent assumptions regarding data distribution or merely providing asymptotic guarantees. We introduce a novel approach termed CCR, which employs a combination of conformal prediction intervals for the model outputs to establish confidence regions for model parameters. We present coverage guarantees under minimal assumptions on noise and that is valid in finite sample regime. Our approach is applicable to both split conformal predictions and black-box methodologies including full or cross-conformal approaches. In the specific case of linear models, the derived confidence region manifests as the feasible set of a Mixed-Integer Linear Program (MILP), facilitating the deduction of confidence intervals for individual parameters and enabling robust optimization. We empirically compare CCR to recent advancements in challenging settings such as with heteroskedastic and non-Gaussian noise.","sentences":["Conformal prediction methodologies have significantly advanced the quantification of uncertainties in predictive models.","Yet, the construction of confidence regions for model parameters presents a notable challenge, often necessitating stringent assumptions regarding data distribution or merely providing asymptotic guarantees.","We introduce a novel approach termed CCR, which employs a combination of conformal prediction intervals for the model outputs to establish confidence regions for model parameters.","We present coverage guarantees under minimal assumptions on noise and that is valid in finite sample regime.","Our approach is applicable to both split conformal predictions and black-box methodologies including full or cross-conformal approaches.","In the specific case of linear models, the derived confidence region manifests as the feasible set of a Mixed-Integer Linear Program (MILP), facilitating the deduction of confidence intervals for individual parameters and enabling robust optimization.","We empirically compare CCR to recent advancements in challenging settings such as with heteroskedastic and non-Gaussian noise."],"url":"http://arxiv.org/abs/2405.18601v1","category":"stat.ML"}
{"created":"2024-05-28 21:14:01","title":"Metaheuristic approaches to the placement of suicide bomber detectors","abstract":"Suicide bombing is an infamous form of terrorism that is becoming increasingly prevalent in the current era of global terror warfare. We consider the case of targeted attacks of this kind, and the use of detectors distributed over the area under threat as a protective countermeasure. Such detectors are non-fully reliable, and must be strategically placed in order to maximize the chances of detecting the attack, hence minimizing the expected number of casualties. To this end, different metaheuristic approaches based on local search and on population-based search are considered and benchmarked against a powerful greedy heuristic from the literature. We conduct an extensive empirical evaluation on synthetic instances featuring very diverse properties. Most metaheuristics outperform the greedy algorithm, and a hill-climber is shown to be superior to remaining approaches. This hill-climber is subsequently subject to a sensitivity analysis to determine which problem features make it stand above the greedy approach, and is finally deployed on a number of problem instances built after realistic scenarios, corroborating the good performance of the heuristic.","sentences":["Suicide bombing is an infamous form of terrorism that is becoming increasingly prevalent in the current era of global terror warfare.","We consider the case of targeted attacks of this kind, and the use of detectors distributed over the area under threat as a protective countermeasure.","Such detectors are non-fully reliable, and must be strategically placed in order to maximize the chances of detecting the attack, hence minimizing the expected number of casualties.","To this end, different metaheuristic approaches based on local search and on population-based search are considered and benchmarked against a powerful greedy heuristic from the literature.","We conduct an extensive empirical evaluation on synthetic instances featuring very diverse properties.","Most metaheuristics outperform the greedy algorithm, and a hill-climber is shown to be superior to remaining approaches.","This hill-climber is subsequently subject to a sensitivity analysis to determine which problem features make it stand above the greedy approach, and is finally deployed on a number of problem instances built after realistic scenarios, corroborating the good performance of the heuristic."],"url":"http://arxiv.org/abs/2405.18593v1","category":"cs.NE"}
{"created":"2024-05-28 21:10:12","title":"Large Scale Linear Magnetic Holes with Magnetic Mirror Properties in Hybrid Simulations of Solar Wind Turbulence","abstract":"Magnetic holes (MHs) are coherent magnetic field dips whose size ranges from fluid to kinetic scale, ubiquitously observed in the heliosphere and in planetary environments. Despite the longstanding effort in interpreting the abundance of observations, the origin and properties of MHs are still debated. In this letter, we investigate the interplay between plasma turbulence and MHs, using a 2D hybrid simulation initialized with solar wind parameters. We show that fully developed turbulence exhibits localized elongated magnetic depressions, whose properties are consistent with linear MHs frequently encountered in space. The observed MHs develop self-consistently from the initial magnetic field perturbations, by trapping hot ions with large pitch angles. Ion trapping produces an enhanced perpendicular temperature anysotropy that makes MHs stable for hundreds of ion gyroperiods, despite the surrounding turbulence. We introduce a new quantity, based on local magnetic field and ion temperature values, to measure the efficiency of ion trapping, with potential applications to the detection of MHs in satellite measurements. We complement this method by analyzing the ion velocity distribution functions inside MHs. Our diagnostics reveal the presence of trapped gyrotropic ion populations, whose velocity distribution is consistent with a loss cone, as expected for the motion of particles inside a magnetic mirror. Our results have potential implications for the theoretical and numerical modelling of MHs.","sentences":["Magnetic holes (MHs) are coherent magnetic field dips whose size ranges from fluid to kinetic scale, ubiquitously observed in the heliosphere and in planetary environments.","Despite the longstanding effort in interpreting the abundance of observations, the origin and properties of MHs are still debated.","In this letter, we investigate the interplay between plasma turbulence and MHs, using a 2D hybrid simulation initialized with solar wind parameters.","We show that fully developed turbulence exhibits localized elongated magnetic depressions, whose properties are consistent with linear MHs frequently encountered in space.","The observed MHs develop self-consistently from the initial magnetic field perturbations, by trapping hot ions with large pitch angles.","Ion trapping produces an enhanced perpendicular temperature anysotropy that makes MHs stable for hundreds of ion gyroperiods, despite the surrounding turbulence.","We introduce a new quantity, based on local magnetic field and ion temperature values, to measure the efficiency of ion trapping, with potential applications to the detection of MHs in satellite measurements.","We complement this method by analyzing the ion velocity distribution functions inside MHs.","Our diagnostics reveal the presence of trapped gyrotropic ion populations, whose velocity distribution is consistent with a loss cone, as expected for the motion of particles inside a magnetic mirror.","Our results have potential implications for the theoretical and numerical modelling of MHs."],"url":"http://arxiv.org/abs/2405.18591v1","category":"astro-ph.SR"}
{"created":"2024-05-28 20:54:31","title":"Public Technologies Transforming Work of the Public and the Public Sector","abstract":"Technologies adopted by the public sector have transformed the work practices of employees in public agencies by creating different means of communication and decision-making. Although much of the recent research in the future of work domain has concentrated on the effects of technological advancements on public sector employees, the influence on work practices of external stakeholders engaging with this sector remains under-explored. In this paper, we focus on a digital platform called OneStop which is deployed by several building departments across the U.S. and aims to integrate various steps and services into a single point of online contact between public sector employees and the public. Drawing on semi-structured interviews with 22 stakeholders, including local business owners, experts involved in the construction process, community representatives, and building department employees, we investigate how this technology transition has impacted the work of these different stakeholders. We observe a multifaceted perspective and experience caused by the adoption of OneStop. OneStop exacerbated inequitable practices for local business owners due to a lack of face-to-face interactions with the department employees. For the public sector employees, OneStop standardized the work practices, representing the building department's priorities and values. Based on our findings, we discuss tensions around standardization, equality, and equity in technology transition, as well as design implications for equitable practices in the public sector.","sentences":["Technologies adopted by the public sector have transformed the work practices of employees in public agencies by creating different means of communication and decision-making.","Although much of the recent research in the future of work domain has concentrated on the effects of technological advancements on public sector employees, the influence on work practices of external stakeholders engaging with this sector remains under-explored.","In this paper, we focus on a digital platform called OneStop which is deployed by several building departments across the U.S. and aims to integrate various steps and services into a single point of online contact between public sector employees and the public.","Drawing on semi-structured interviews with 22 stakeholders, including local business owners, experts involved in the construction process, community representatives, and building department employees, we investigate how this technology transition has impacted the work of these different stakeholders.","We observe a multifaceted perspective and experience caused by the adoption of OneStop.","OneStop exacerbated inequitable practices for local business owners due to a lack of face-to-face interactions with the department employees.","For the public sector employees, OneStop standardized the work practices, representing the building department's priorities and values.","Based on our findings, we discuss tensions around standardization, equality, and equity in technology transition, as well as design implications for equitable practices in the public sector."],"url":"http://arxiv.org/abs/2405.18579v1","category":"cs.CY"}
{"created":"2024-05-29 17:39:42","title":"Matryoshka Query Transformer for Large Vision-Language Models","abstract":"Large Vision-Language Models (LVLMs) typically encode an image into a fixed number of visual tokens (e.g., 576) and process these tokens with a language model. Despite their strong performance, LVLMs face challenges in adapting to varying computational constraints. This raises the question: can we achieve flexibility in the number of visual tokens to suit different tasks and computational resources? We answer this with an emphatic yes. Inspired by Matryoshka Representation Learning, we introduce the Matryoshka Query Transformer (MQT), capable of encoding an image into m visual tokens during inference, where m can be any number up to a predefined maximum. This is achieved by employing a query transformer with M latent query tokens to compress the visual embeddings. During each training step, we randomly select m <= M latent query tokens and train the model using only these first m tokens, discarding the rest. Combining MQT with LLaVA, we train a single model once, and flexibly and drastically reduce the number of inference-time visual tokens while maintaining similar or better performance compared to training independent models for each number of tokens. Our model, MQT-LLAVA, matches LLaVA-1.5 performance across 11 benchmarks using a maximum of 256 tokens instead of LLaVA's fixed 576. Reducing to 16 tokens (8x less TFLOPs) only sacrifices the performance by 2.4 points on MMBench. On certain tasks such as ScienceQA and MMMU, we can even go down to only 2 visual tokens with performance drops of just 3% and 6% each. Our exploration of the trade-off between the accuracy and computational cost brought about by the number of visual tokens facilitates future research to achieve the best of both worlds.","sentences":["Large Vision-Language Models (LVLMs) typically encode an image into a fixed number of visual tokens (e.g., 576) and process these tokens with a language model.","Despite their strong performance, LVLMs face challenges in adapting to varying computational constraints.","This raises the question: can we achieve flexibility in the number of visual tokens to suit different tasks and computational resources?","We answer this with an emphatic yes.","Inspired by Matryoshka Representation Learning, we introduce the Matryoshka Query Transformer (MQT), capable of encoding an image into m visual tokens during inference, where m can be any number up to a predefined maximum.","This is achieved by employing a query transformer with M latent query tokens to compress the visual embeddings.","During each training step, we randomly select m <= M latent query tokens and train the model using only these first m tokens, discarding the rest.","Combining MQT with LLaVA, we train a single model once, and flexibly and drastically reduce the number of inference-time visual tokens while maintaining similar or better performance compared to training independent models for each number of tokens.","Our model, MQT-LLAVA, matches LLaVA-1.5 performance across 11 benchmarks using a maximum of 256 tokens instead of LLaVA's fixed 576.","Reducing to 16 tokens (8x less TFLOPs) only sacrifices the performance by 2.4 points on MMBench.","On certain tasks such as ScienceQA and MMMU, we can even go down to only 2 visual tokens with performance drops of just 3% and 6% each.","Our exploration of the trade-off between the accuracy and computational cost brought about by the number of visual tokens facilitates future research to achieve the best of both worlds."],"url":"http://arxiv.org/abs/2405.19315v1","category":"cs.CV"}
{"created":"2024-05-29 16:31:55","title":"Uniform Inviscid Damping and Inviscid Limit of the 2D Navier-Stokes equation with Navier Boundary Conditions","abstract":"We consider the 2D, incompressible Navier-Stokes equations near the Couette flow, $\\omega^{(NS)} = 1 + \\epsilon \\omega$, set on the channel $\\mathbb{T} \\times [-1, 1]$, supplemented with Navier boundary conditions on the perturbation, $\\omega|_{y = \\pm 1} = 0$. We are simultaneously interested in two asymptotic regimes that are classical in hydrodynamic stability: the long time, $t \\rightarrow \\infty$, stability of background shear flows, and the inviscid limit, $\\nu \\rightarrow 0$ in the presence of boundaries. Given small ($\\epsilon \\ll 1$, but independent of $\\nu$) Gevrey 2- datum, $\\omega_0^{(\\nu)}(x, y)$, that is supported away from the boundaries $y = \\pm 1$, we prove the following results: \\begin{align*} & \\|\\omega^{(\\nu)}(t) - \\frac{1}{2\\pi}\\int \\omega^{(\\nu)}(t) dx \\|_{L^2} \\lesssim \\epsilon e^{-\\delta \\nu^{1/3} t}, & \\text{(Enhanced Dissipation)} \\\\ & \\langle t \\rangle \\|u_1^{(\\nu)}(t) - \\frac{1}{2\\pi} \\int u_1^{(\\nu)}(t) dx\\|_{L^2} + \\langle t \\rangle^2 \\|u_2^{(\\nu)}(t)\\|_{L^2} \\lesssim \\epsilon e^{-\\delta \\nu^{1/3} t}, & \\text{(Inviscid Damping)} \\\\ &\\| \\omega^{(\\nu)} - \\omega^{(0)} \\|_{L^\\infty} \\lesssim \\epsilon \\nu t^{3+\\eta}, \\quad\\quad t \\lesssim \\nu^{-1/(3+\\eta)} & \\text{(Long-time Inviscid Limit)} \\end{align*} This is the first nonlinear asymptotic stability result of its type, which combines three important physical phenomena at the nonlinear level: inviscid damping, enhanced dissipation, and long-time inviscid limit in the presence of boundaries. The techniques we develop represent a major departure from prior works on nonlinear inviscid damping as physical space techniques necessarily play a central role. In this paper, we focus on the primary nonlinear result, while tools for handling the linearized parabolic and elliptic equations are developed in our separate, companion work.","sentences":["We consider the 2D, incompressible Navier-Stokes equations near the Couette flow, $\\omega^{(NS)} = 1 + \\epsilon \\omega$, set on the channel $\\mathbb{T} \\times","[-1, 1]$, supplemented with Navier boundary conditions on the perturbation, $\\omega|_{y = \\pm 1} = 0$.","We are simultaneously interested in two asymptotic regimes that are classical in hydrodynamic stability: the long time, $t \\rightarrow \\infty$, stability of background shear flows, and the inviscid limit, $\\nu \\rightarrow 0$ in the presence of boundaries.","Given small ($\\epsilon \\ll 1$, but independent of $\\nu$)","Gevrey 2- datum, $\\omega_0^{(\\nu)}(x, y)$, that is supported away from the boundaries $y = \\pm 1$, we prove the following results: \\begin{align*} & \\|\\omega^{(\\nu)}(t) - \\frac{1}{2\\pi}\\int \\omega^{(\\nu)}(t) dx \\|_{L^2} \\lesssim \\epsilon e^{-\\delta \\nu^{1/3} t}, & \\text{(Enhanced Dissipation)} \\\\ & \\langle t \\rangle \\|u_1^{(\\nu)}(t) - \\frac{1}{2\\pi} \\int u_1^{(\\nu)}(t) dx\\|_{L^2} +","\\langle t \\rangle^2 \\|u_2^{(\\nu)}(t)\\|_{L^2} \\lesssim \\epsilon e^{-\\delta \\nu^{1/3} t}, & \\text{(Inviscid Damping)} \\\\ &\\| \\omega^{(\\nu)} - \\omega^{(0)} \\|_{L^\\infty} \\lesssim \\epsilon \\nu t^{3+\\eta}, \\quad\\quad t \\lesssim \\nu^{-1/(3+\\eta)} & \\text{(Long-time Inviscid Limit)} \\end{align*} This is the first nonlinear asymptotic stability result of its type, which combines three important physical phenomena at the nonlinear level: inviscid damping, enhanced dissipation, and long-time inviscid limit in the presence of boundaries.","The techniques we develop represent a major departure from prior works on nonlinear inviscid damping as physical space techniques necessarily play a central role.","In this paper, we focus on the primary nonlinear result, while tools for handling the linearized parabolic and elliptic equations are developed in our separate, companion work."],"url":"http://arxiv.org/abs/2405.19249v1","category":"math.AP"}
{"created":"2024-05-29 16:13:43","title":"Pseudo-Gevrey Smoothing for the Passive Scalar Equations near Couette","abstract":"In this article, we study the regularity theory for two linear equations that are important in fluid dynamics: the passive scalar equation for (time-varying) shear flows close to Couette in $\\mathbb T \\times [-1,1]$ with vanishing diffusivity $\\nu \\to 0$ and the Poisson equation with right-hand side behaving in similar function spaces to such a passive scalar. The primary motivation for this work is to develop some of the main technical tools required for our treatment of the (nonlinear) 2D Navier-Stokes equations, carried out in our companion work. Both equations are studied with homogeneous Dirichlet conditions (the analogue of a Navier slip-type boundary condition) and the initial condition is taken to be compactly supported away from the walls. We develop smoothing estimates with the following three features:   [1] Uniform-in-$\\nu$ regularity is with respect to $\\partial_x$ and a time-dependent adapted vector-field $\\Gamma$ which approximately commutes with the passive scalar equation (as opposed to `flat' derivatives), and a scaled gradient $\\sqrt{\\nu} \\nabla$;   [2] $(\\partial_x, \\Gamma)$-regularity estimates are performed in Gevrey spaces with regularity that depends on the spatial coordinate, $y$ (what we refer to as `pseudo-Gevrey');   [3] The regularity of these pseudo-Gevrey spaces degenerates to finite regularity near the center of the channel and hence standard Gevrey product rules and other amenable properties do not hold.   Nonlinear analysis in such a delicate functional setting is one of the key ingredients to our companion paper, \\cite{BHIW24a}, which proves the full nonlinear asymptotic stability of the Couette flow with slip boundary conditions. The present article introduces new estimates for the associated linear problems in these degenerate pseudo-Gevrey spaces, which is of independent interest.","sentences":["In this article, we study the regularity theory for two linear equations that are important in fluid dynamics: the passive scalar equation for (time-varying) shear flows close to Couette in $\\mathbb T \\times","[-1,1]$ with vanishing diffusivity $\\nu \\to 0$ and the Poisson equation with right-hand side behaving in similar function spaces to such a passive scalar.","The primary motivation for this work is to develop some of the main technical tools required for our treatment of the (nonlinear) 2D Navier-Stokes equations, carried out in our companion work.","Both equations are studied with homogeneous Dirichlet conditions (the analogue of a Navier slip-type boundary condition) and the initial condition is taken to be compactly supported away from the walls.","We develop smoothing estimates with the following three features:   ","[1] Uniform-in-$\\nu$ regularity is with respect to $\\partial_x$ and a time-dependent adapted vector-field $\\Gamma$ which approximately commutes with the passive scalar equation (as opposed to `flat' derivatives), and a scaled gradient $\\sqrt{\\nu} \\nabla$;   [2] $(\\partial_x, \\Gamma)$-regularity estimates are performed in Gevrey spaces with regularity that depends on the spatial coordinate, $y$ (what we refer to as `pseudo-Gevrey');   ","[3] The regularity of these pseudo-Gevrey spaces degenerates to finite regularity near the center of the channel and hence standard Gevrey product rules and other amenable properties do not hold.   ","Nonlinear analysis in such a delicate functional setting is one of the key ingredients to our companion paper, \\cite{BHIW24a}, which proves the full nonlinear asymptotic stability of the Couette flow with slip boundary conditions.","The present article introduces new estimates for the associated linear problems in these degenerate pseudo-Gevrey spaces, which is of independent interest."],"url":"http://arxiv.org/abs/2405.19233v1","category":"math.AP"}
{"created":"2024-05-29 16:08:15","title":"Covariate Shift Corrected Conditional Randomization Test","abstract":"Conditional independence tests are crucial across various disciplines in determining the independence of an outcome variable $Y$ from a treatment variable $X$, conditioning on a set of confounders $Z$. The Conditional Randomization Test (CRT) offers a powerful framework for such testing by assuming known distributions of $X \\mid Z$; it controls the Type-I error exactly, allowing for the use of flexible, black-box test statistics. In practice, testing for conditional independence often involves using data from a source population to draw conclusions about a target population. This can be challenging due to covariate shift -- differences in the distribution of $X$, $Z$, and surrogate variables, which can affect the conditional distribution of $Y \\mid X, Z$ -- rendering traditional CRT approaches invalid. To address this issue, we propose a novel Covariate Shift Corrected Pearson Chi-squared Conditional Randomization (csPCR) test. This test adapts to covariate shifts by integrating importance weights and employing the control variates method to reduce variance in the test statistics and thus enhance power. Theoretically, we establish that the csPCR test controls the Type-I error asymptotically. Empirically, through simulation studies, we demonstrate that our method not only maintains control over Type-I errors but also exhibits superior power, confirming its efficacy and practical utility in real-world scenarios where covariate shifts are prevalent. Finally, we apply our methodology to a real-world dataset to assess the impact of a COVID-19 treatment on the 90-day mortality rate among patients.","sentences":["Conditional independence tests are crucial across various disciplines in determining the independence of an outcome variable $Y$ from a treatment variable $X$, conditioning on a set of confounders $Z$. The Conditional Randomization Test (CRT) offers a powerful framework for such testing by assuming known distributions of $X \\mid Z$; it controls the Type-I error exactly, allowing for the use of flexible, black-box test statistics.","In practice, testing for conditional independence often involves using data from a source population to draw conclusions about a target population.","This can be challenging due to covariate shift -- differences in the distribution of $X$, $Z$, and surrogate variables, which can affect the conditional distribution of $Y \\mid X, Z$ -- rendering traditional CRT approaches invalid.","To address this issue, we propose a novel Covariate Shift Corrected Pearson Chi-squared Conditional Randomization (csPCR) test.","This test adapts to covariate shifts by integrating importance weights and employing the control variates method to reduce variance in the test statistics and thus enhance power.","Theoretically, we establish that the csPCR test controls the Type-I error asymptotically.","Empirically, through simulation studies, we demonstrate that our method not only maintains control over Type-I errors but also exhibits superior power, confirming its efficacy and practical utility in real-world scenarios where covariate shifts are prevalent.","Finally, we apply our methodology to a real-world dataset to assess the impact of a COVID-19 treatment on the 90-day mortality rate among patients."],"url":"http://arxiv.org/abs/2405.19231v1","category":"stat.ME"}
{"created":"2024-05-29 15:56:36","title":"Compactly supported anomalous weak solutions for 2D Euler equations with vorticity in Hardy spaces","abstract":"In a previous work (arXiv:2306.05948), we constructed by convex integration examples of energy dissipating solutions to the 2D Euler equations on $\\mathbb{R}^2$ with vorticity in the real Hardy space $H^p(\\mathbb{R}^2)$. In the present paper, we develop tools that significantly improve that result in two ways: Firstly, we achieve vorticities in $H^p(\\mathbb{R}^2)$ in the optimal range $p\\in (0,1)$ compared to $(2/3,1)$ in our previous work. Secondly, the solutions constructed here possess compact support and in particular preserve linear and angular momenta.","sentences":["In a previous work (arXiv:2306.05948), we constructed by convex integration examples of energy dissipating solutions to the 2D Euler equations on $\\mathbb{R}^2$ with vorticity in the real Hardy space $H^p(\\mathbb{R}^2)$. In the present paper, we develop tools that significantly improve that result in two ways: Firstly, we achieve vorticities in $H^p(\\mathbb{R}^2)$ in the optimal range $p\\in (0,1)$ compared to $(2/3,1)$ in our previous work.","Secondly, the solutions constructed here possess compact support and in particular preserve linear and angular momenta."],"url":"http://arxiv.org/abs/2405.19214v1","category":"math.AP"}
{"created":"2024-05-29 13:25:30","title":"Computational bounds on randomized algorithms for online bin stretching","abstract":"A frequently studied performance measure in online optimization is competitive analysis. It corresponds to the worst-case ratio, over all possible inputs of an algorithm, between the performance of the algorithm and the optimal offline performance. However, this analysis may be too pessimistic to give valuable insight on a problem. Several workarounds exist, such as randomized algorithms. This paper aims to propose computational methods to construct randomized algorithms and to bound their performance on the classical online bin stretching problem. A game theory method is adapted to construct lower bounds on the performance of randomized online algorithms via linear programming. Another computational method is then proposed to construct randomized algorithms which perform better than the best deterministic algorithms known. Finally, another lower bound method for a restricted class of randomized algorithm for this problem is proposed.","sentences":["A frequently studied performance measure in online optimization is competitive analysis.","It corresponds to the worst-case ratio, over all possible inputs of an algorithm, between the performance of the algorithm and the optimal offline performance.","However, this analysis may be too pessimistic to give valuable insight on a problem.","Several workarounds exist, such as randomized algorithms.","This paper aims to propose computational methods to construct randomized algorithms and to bound their performance on the classical online bin stretching problem.","A game theory method is adapted to construct lower bounds on the performance of randomized online algorithms via linear programming.","Another computational method is then proposed to construct randomized algorithms which perform better than the best deterministic algorithms known.","Finally, another lower bound method for a restricted class of randomized algorithm for this problem is proposed."],"url":"http://arxiv.org/abs/2405.19071v1","category":"math.OC"}
{"created":"2024-05-29 09:56:00","title":"Predicting Many Properties of Crystals by a Single Deep Learning Model","abstract":"The use of machine learning methods for predicting the properties of crystalline materials encounters significant challenges, primarily related to input encoding, output versatility, and interpretability. Here, we introduce CrystalBERT, an adaptable transformer-based framework with novel structure that integrates space group, elemental, and unit cell information. The method's adaptability lies not only in its ability to seamlessly combine diverse features but also in its capability to accurately predict a wide range of physically important properties, including topological properties, superconducting transition temperatures, dielectric constants, and more. CrystalBERT also provides insightful physical interpretations regarding the features that most significantly influence the target properties. Our findings indicate that space group and elemental information are more important for predicting topological and superconducting properties, in contrast to some properties that primarily depend on the unit cell information. This underscores the intricate nature of topological and superconducting properties. By incorporating all these features, we achieve a high accuracy of 91% in topological classification, surpassing prior studies and identifying previously misclassified topological materials, further demonstrating the effectiveness of our model.","sentences":["The use of machine learning methods for predicting the properties of crystalline materials encounters significant challenges, primarily related to input encoding, output versatility, and interpretability.","Here, we introduce CrystalBERT, an adaptable transformer-based framework with novel structure that integrates space group, elemental, and unit cell information.","The method's adaptability lies not only in its ability to seamlessly combine diverse features but also in its capability to accurately predict a wide range of physically important properties, including topological properties, superconducting transition temperatures, dielectric constants, and more.","CrystalBERT also provides insightful physical interpretations regarding the features that most significantly influence the target properties.","Our findings indicate that space group and elemental information are more important for predicting topological and superconducting properties, in contrast to some properties that primarily depend on the unit cell information.","This underscores the intricate nature of topological and superconducting properties.","By incorporating all these features, we achieve a high accuracy of 91% in topological classification, surpassing prior studies and identifying previously misclassified topological materials, further demonstrating the effectiveness of our model."],"url":"http://arxiv.org/abs/2405.18944v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 09:13:30","title":"Exploring Human-in-the-Loop Test-Time Adaptation by Synergizing Active Learning and Model Selection","abstract":"Existing test-time adaptation (TTA) approaches often adapt models with the unlabeled testing data stream. A recent attempt relaxed the assumption by introducing limited human annotation, referred to as Human-In-the-Loop Test-Time Adaptation (HILTTA) in this study. The focus of existing HILTTA lies on selecting the most informative samples to label, a.k.a. active learning. In this work, we are motivated by a pitfall of TTA, i.e. sensitive to hyper-parameters, and propose to approach HILTTA by synergizing active learning and model selection. Specifically, we first select samples for human annotation (active learning) and then use the labeled data to select optimal hyper-parameters (model selection). A sample selection strategy is tailored for choosing samples by considering the balance between active learning and model selection purposes. We demonstrate on 4 TTA datasets that the proposed HILTTA approach is compatible with off-the-shelf TTA methods which outperform the state-of-the-art HILTTA methods and stream-based active learning methods. Importantly, our proposed method can always prevent choosing the worst hyper-parameters on all off-the-shelf TTA methods. The source code will be released upon publication.","sentences":["Existing test-time adaptation (TTA) approaches often adapt models with the unlabeled testing data stream.","A recent attempt relaxed the assumption by introducing limited human annotation, referred to as Human-In-the-Loop Test-Time Adaptation (HILTTA) in this study.","The focus of existing HILTTA lies on selecting the most informative samples to label, a.k.a. active learning.","In this work, we are motivated by a pitfall of TTA, i.e. sensitive to hyper-parameters, and propose to approach HILTTA by synergizing active learning and model selection.","Specifically, we first select samples for human annotation (active learning) and then use the labeled data to select optimal hyper-parameters (model selection).","A sample selection strategy is tailored for choosing samples by considering the balance between active learning and model selection purposes.","We demonstrate on 4 TTA datasets that the proposed HILTTA approach is compatible with off-the-shelf TTA methods which outperform the state-of-the-art HILTTA methods and stream-based active learning methods.","Importantly, our proposed method can always prevent choosing the worst hyper-parameters on all off-the-shelf TTA methods.","The source code will be released upon publication."],"url":"http://arxiv.org/abs/2405.18911v1","category":"cs.CV"}
{"created":"2024-05-29 08:57:23","title":"MLAE: Masked LoRA Experts for Parameter-Efficient Fine-Tuning","abstract":"In response to the challenges posed by the extensive parameter updates required for full fine-tuning of large-scale pre-trained models, parameter-efficient fine-tuning (PEFT) methods, exemplified by Low-Rank Adaptation (LoRA), have emerged. LoRA simplifies the fine-tuning process but may still struggle with a certain level of redundancy in low-rank matrices and limited effectiveness from merely increasing their rank. To address these issues, a natural idea is to enhance the independence and diversity of the learning process for the low-rank matrices. Therefore, we propose Masked LoRA Experts (MLAE), an innovative approach that applies the concept of masking to PEFT. Our method incorporates a cellular decomposition strategy that transforms a low-rank matrix into independent rank-1 submatrices, or ``experts'', thus enhancing independence. Additionally, we introduce a binary mask matrix that selectively activates these experts during training to promote more diverse and anisotropic learning, based on expert-level dropout strategies. Our investigations reveal that this selective activation not only enhances performance but also fosters a more diverse acquisition of knowledge with a marked decrease in parameter similarity among MLAE, significantly boosting the quality of the model while barely increasing the parameter count. Remarkably, MLAE achieves new SOTA performance with an average accuracy score of 78.8% on the VTAB-1k benchmark and 90.9% on the FGVC benchmark, demonstrating superior performance. Our code is available at https://github.com/jie040109/MLAE.","sentences":["In response to the challenges posed by the extensive parameter updates required for full fine-tuning of large-scale pre-trained models, parameter-efficient fine-tuning (PEFT) methods, exemplified by Low-Rank Adaptation (LoRA), have emerged.","LoRA simplifies the fine-tuning process but may still struggle with a certain level of redundancy in low-rank matrices and limited effectiveness from merely increasing their rank.","To address these issues, a natural idea is to enhance the independence and diversity of the learning process for the low-rank matrices.","Therefore, we propose Masked LoRA Experts (MLAE), an innovative approach that applies the concept of masking to PEFT.","Our method incorporates a cellular decomposition strategy that transforms a low-rank matrix into independent rank-1 submatrices, or ``experts'', thus enhancing independence.","Additionally, we introduce a binary mask matrix that selectively activates these experts during training to promote more diverse and anisotropic learning, based on expert-level dropout strategies.","Our investigations reveal that this selective activation not only enhances performance but also fosters a more diverse acquisition of knowledge with a marked decrease in parameter similarity among MLAE, significantly boosting the quality of the model while barely increasing the parameter count.","Remarkably, MLAE achieves new SOTA performance with an average accuracy score of 78.8% on the VTAB-1k benchmark and 90.9% on the FGVC benchmark, demonstrating superior performance.","Our code is available at https://github.com/jie040109/MLAE."],"url":"http://arxiv.org/abs/2405.18897v1","category":"cs.CV"}
{"created":"2024-05-29 08:01:18","title":"Approximation of the steady state for piecewise stable Ornstein-Uhlenbeck processes arising in queueing networks","abstract":"We shall use the Euler-Maruyama (EM) scheme with decreasing step size $\\Lambda=(\\eta_n)_{n\\in \\mathbb{N}}$ to approximate the steady state for piecewise $\\alpha$-stable Ornstein-Uhlenbeck processes arising in queue networks. These processes do not have an explicit dissipation. We prove the EM scheme converges to the steady state with a rate $\\eta^{1/\\alpha}_n$ in Wasserstein-1 distance. In addition, we utilize the Sinkhorn--Knopp algorithm to compute the Wasserstein-1 distance and conduct the simulations for several examples.","sentences":["We shall use the Euler-Maruyama (EM) scheme with decreasing step size $\\Lambda=(\\eta_n)_{n\\in \\mathbb{N}}$ to approximate the steady state for piecewise $\\alpha$-stable Ornstein-Uhlenbeck processes arising in queue networks.","These processes do not have an explicit dissipation.","We prove the EM scheme converges to the steady state with a rate $\\eta^{1/\\alpha}_n$ in Wasserstein-1 distance.","In addition, we utilize the Sinkhorn--Knopp algorithm to compute the Wasserstein-1 distance and conduct the simulations for several examples."],"url":"http://arxiv.org/abs/2405.18851v1","category":"math.PR"}
{"created":"2024-05-29 05:56:44","title":"Global optimization in variational quantum algorithms via dynamic tunneling method","abstract":"We present a global optimization routine for the variational quantum algorithms, which utilizes the dynamic tunneling flow. Originally designed to leverage information gathered by a gradient-based optimizer around local minima, we adapt the conventional dynamic tunneling flow to exploit the distance measure of quantum states, resolving issues of extrinsic degeneracy arising from the parametrization of quantum states. Our global optimization algorithm is applied to the variational quantum eigensolver for the transverse-field Ising model to demonstrate the performance of our routine while comparing it with the conventional dynamic tunneling method, which is based on the Euclidean distance measure on the parameter space.","sentences":["We present a global optimization routine for the variational quantum algorithms, which utilizes the dynamic tunneling flow.","Originally designed to leverage information gathered by a gradient-based optimizer around local minima, we adapt the conventional dynamic tunneling flow to exploit the distance measure of quantum states, resolving issues of extrinsic degeneracy arising from the parametrization of quantum states.","Our global optimization algorithm is applied to the variational quantum eigensolver for the transverse-field Ising model to demonstrate the performance of our routine while comparing it with the conventional dynamic tunneling method, which is based on the Euclidean distance measure on the parameter space."],"url":"http://arxiv.org/abs/2405.18783v1","category":"quant-ph"}
{"created":"2024-05-29 05:26:25","title":"LLaMA-Reg: Using LLaMA 2 for Unsupervised Medical Image Registration","abstract":"Medical image registration is an essential topic in medical image analysis. In this paper, we propose a method for medical image registration using a pretrained large language model. We find that using the pretrained large language model to encode deep features of the medical images in the registration model can effectively improve image registration accuracy, indicating the great potential of the large language model in medical image registration tasks. We use dual encoders to perform deep feature extraction on image pairs and then input the features into the pretrained large language model. To adapt the large language model to our registration task, the weights of the large language model are frozen in the registration model, and an adapter is utilized to fine-tune the large language model, which aims at (a) mapping the visual tokens to the language space before the large language model computing, (b) project the modeled language tokens output from the large language model to the visual space. Our method combines output features from the fine-tuned large language model with the features output from each encoder layer to gradually generate the deformation fields required for registration in the decoder. To demonstrate the effectiveness of the large prediction model in registration tasks, we conducted experiments on knee and brain MRI and achieved state-of-the-art results.","sentences":["Medical image registration is an essential topic in medical image analysis.","In this paper, we propose a method for medical image registration using a pretrained large language model.","We find that using the pretrained large language model to encode deep features of the medical images in the registration model can effectively improve image registration accuracy, indicating the great potential of the large language model in medical image registration tasks.","We use dual encoders to perform deep feature extraction on image pairs and then input the features into the pretrained large language model.","To adapt the large language model to our registration task, the weights of the large language model are frozen in the registration model, and an adapter is utilized to fine-tune the large language model, which aims at (a) mapping the visual tokens to the language space before the large language model computing, (b) project the modeled language tokens output from the large language model to the visual space.","Our method combines output features from the fine-tuned large language model with the features output from each encoder layer to gradually generate the deformation fields required for registration in the decoder.","To demonstrate the effectiveness of the large prediction model in registration tasks, we conducted experiments on knee and brain MRI and achieved state-of-the-art results."],"url":"http://arxiv.org/abs/2405.18774v1","category":"cs.CV"}
{"created":"2024-05-29 05:10:25","title":"RNAFlow: RNA Structure & Sequence Design via Inverse Folding-Based Flow Matching","abstract":"The growing significance of RNA engineering in diverse biological applications has spurred interest in developing AI methods for structure-based RNA design. While diffusion models have excelled in protein design, adapting them for RNA presents new challenges due to RNA's conformational flexibility and the computational cost of fine-tuning large structure prediction models. To this end, we propose RNAFlow, a flow matching model for protein-conditioned RNA sequence-structure design. Its denoising network integrates an RNA inverse folding model and a pre-trained RosettaFold2NA network for generation of RNA sequences and structures. The integration of inverse folding in the structure denoising process allows us to simplify training by fixing the structure prediction network. We further enhance the inverse folding model by conditioning it on inferred conformational ensembles to model dynamic RNA conformations. Evaluation on protein-conditioned RNA structure and sequence generation tasks demonstrates RNAFlow's advantage over existing RNA design methods.","sentences":["The growing significance of RNA engineering in diverse biological applications has spurred interest in developing AI methods for structure-based RNA design.","While diffusion models have excelled in protein design, adapting them for RNA presents new challenges due to RNA's conformational flexibility and the computational cost of fine-tuning large structure prediction models.","To this end, we propose RNAFlow, a flow matching model for protein-conditioned RNA sequence-structure design.","Its denoising network integrates an RNA inverse folding model and a pre-trained RosettaFold2NA network for generation of RNA sequences and structures.","The integration of inverse folding in the structure denoising process allows us to simplify training by fixing the structure prediction network.","We further enhance the inverse folding model by conditioning it on inferred conformational ensembles to model dynamic RNA conformations.","Evaluation on protein-conditioned RNA structure and sequence generation tasks demonstrates RNAFlow's advantage over existing RNA design methods."],"url":"http://arxiv.org/abs/2405.18768v1","category":"q-bio.BM"}
{"created":"2024-05-29 00:25:07","title":"Adapting Differentially Private Synthetic Data to Relational Databases","abstract":"Existing differentially private (DP) synthetic data generation mechanisms typically assume a single-source table. In practice, data is often distributed across multiple tables with relationships across tables. In this paper, we introduce the first-of-its-kind algorithm that can be combined with any existing DP mechanisms to generate synthetic relational databases. Our algorithm iteratively refines the relationship between individual synthetic tables to minimize their approximation errors in terms of low-order marginal distributions while maintaining referential integrity. Finally, we provide both DP and theoretical utility guarantees for our algorithm.","sentences":["Existing differentially private (DP) synthetic data generation mechanisms typically assume a single-source table.","In practice, data is often distributed across multiple tables with relationships across tables.","In this paper, we introduce the first-of-its-kind algorithm that can be combined with any existing DP mechanisms to generate synthetic relational databases.","Our algorithm iteratively refines the relationship between individual synthetic tables to minimize their approximation errors in terms of low-order marginal distributions while maintaining referential integrity.","Finally, we provide both DP and theoretical utility guarantees for our algorithm."],"url":"http://arxiv.org/abs/2405.18670v1","category":"cs.LG"}
{"created":"2024-05-28 23:54:01","title":"Weak (non)conservation and stochastic dynamics of angular momentum","abstract":"Angular momentum conservation influences equilibrium statistical mechanics, leading to a generalized microcanonical density for an isolated system and a generalized Gibbs density for a weakly coupled system. We study the stochastic decay of angular momentum due to weakly imperfect rotational symmetry of the external potential that confines the isolated many-particle system. We present a mesoscopic description of the system, deriving Langevin and Fokker-Planck equations, which are consistent with equilibrium statistical mechanics when rotational symmetry is maintained. When the symmetry is weakly violated, we formulate a coarse-grained stochastic differential equation governing the decay of total angular momentum over time. To validate our analytical predictions, we conduct numerical simulations of the microcanonical ensemble, an isolated system undergoing thermalization due to weak two-body interactions. Our coarse-grained Langevin equation accurately characterizes both the decay of the angular momentum and its fluctuations in a steady state. Furthermore, we estimate the parameters of our mesoscopic model directly from simulations, providing insights into the dissipative phenomenological coefficients, such as friction. More generally, this study contributes to a deeper understanding of the behavior of the integrals of motion when the corresponding symmetry is weakly violated.","sentences":["Angular momentum conservation influences equilibrium statistical mechanics, leading to a generalized microcanonical density for an isolated system and a generalized Gibbs density for a weakly coupled system.","We study the stochastic decay of angular momentum due to weakly imperfect rotational symmetry of the external potential that confines the isolated many-particle system.","We present a mesoscopic description of the system, deriving Langevin and Fokker-Planck equations, which are consistent with equilibrium statistical mechanics when rotational symmetry is maintained.","When the symmetry is weakly violated, we formulate a coarse-grained stochastic differential equation governing the decay of total angular momentum over time.","To validate our analytical predictions, we conduct numerical simulations of the microcanonical ensemble, an isolated system undergoing thermalization due to weak two-body interactions.","Our coarse-grained Langevin equation accurately characterizes both the decay of the angular momentum and its fluctuations in a steady state.","Furthermore, we estimate the parameters of our mesoscopic model directly from simulations, providing insights into the dissipative phenomenological coefficients, such as friction.","More generally, this study contributes to a deeper understanding of the behavior of the integrals of motion when the corresponding symmetry is weakly violated."],"url":"http://arxiv.org/abs/2405.18660v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-28 23:32:46","title":"Recent Advances of Foundation Language Models-based Continual Learning: A Survey","abstract":"Recently, foundation language models (LMs) have marked significant achievements in the domains of natural language processing (NLP) and computer vision (CV). Unlike traditional neural network models, foundation LMs obtain a great ability for transfer learning by acquiring rich commonsense knowledge through pre-training on extensive unsupervised datasets with a vast number of parameters. However, they still can not emulate human-like continuous learning due to catastrophic forgetting. Consequently, various continual learning (CL)-based methodologies have been developed to refine LMs, enabling them to adapt to new tasks without forgetting previous knowledge. However, a systematic taxonomy of existing approaches and a comparison of their performance are still lacking, which is the gap that our survey aims to fill. We delve into a comprehensive review, summarization, and classification of the existing literature on CL-based approaches applied to foundation language models, such as pre-trained language models (PLMs), large language models (LLMs) and vision-language models (VLMs). We divide these studies into offline CL and online CL, which consist of traditional methods, parameter-efficient-based methods, instruction tuning-based methods and continual pre-training methods. Offline CL encompasses domain-incremental learning, task-incremental learning, and class-incremental learning, while online CL is subdivided into hard task boundary and blurry task boundary settings. Additionally, we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs-based continual learning.","sentences":["Recently, foundation language models (LMs) have marked significant achievements in the domains of natural language processing (NLP) and computer vision (CV).","Unlike traditional neural network models, foundation LMs obtain a great ability for transfer learning by acquiring rich commonsense knowledge through pre-training on extensive unsupervised datasets with a vast number of parameters.","However, they still can not emulate human-like continuous learning due to catastrophic forgetting.","Consequently, various continual learning (CL)-based methodologies have been developed to refine LMs, enabling them to adapt to new tasks without forgetting previous knowledge.","However, a systematic taxonomy of existing approaches and a comparison of their performance are still lacking, which is the gap that our survey aims to fill.","We delve into a comprehensive review, summarization, and classification of the existing literature on CL-based approaches applied to foundation language models, such as pre-trained language models (PLMs), large language models (LLMs) and vision-language models (VLMs).","We divide these studies into offline CL and online CL, which consist of traditional methods, parameter-efficient-based methods, instruction tuning-based methods and continual pre-training methods.","Offline CL encompasses domain-incremental learning, task-incremental learning, and class-incremental learning, while online CL is subdivided into hard task boundary and blurry task boundary settings.","Additionally, we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs-based continual learning."],"url":"http://arxiv.org/abs/2405.18653v1","category":"cs.CL"}
{"created":"2024-05-28 22:32:17","title":"Battery Degradation Heuristics for Predictive Energy Management in Shipboard Power Systems","abstract":"The presence of Pulse Power Loads (PPLs) in the Notional Shipboard Power System (SPS) presents a challenge in the form of meeting their high ramp rate requirements. Considering the ramp rate limitations on the generators, this might hinder the power flow in the grid. Failure to meet the ramp rate requirements might cause instability. Aggregating generators with energy storage elements usually addresses the ramp requirements while ensuring the power demand is achieved. This paper proposes an energy management strategy that adaptively splits the power demand between the generators and the batteries while simultaneously considering the battery degradation and the generator's efficient operation. Since it is challenging to incorporate the battery degradation model directly into the optimization problem due to its complex structure and the degradation time scale which is not practical for real-time implementation, two reasonable heuristics in terms of minimizing the absolute battery power and minimizing the battery state of charge are proposed and compared to manage the battery degradation. A model predictive energy management strategy is then developed to coordinate the power split considering the generator efficiency and minimizing the battery degradation based on the two heuristic approaches. The designed strategy is tested via a simulation of a lumped notional shipboard power system. The results show the impact of the battery degradation heuristics for energy management strategy in mitigating battery degradation and its health management.","sentences":["The presence of Pulse Power Loads (PPLs) in the Notional Shipboard Power System (SPS) presents a challenge in the form of meeting their high ramp rate requirements.","Considering the ramp rate limitations on the generators, this might hinder the power flow in the grid.","Failure to meet the ramp rate requirements might cause instability.","Aggregating generators with energy storage elements usually addresses the ramp requirements while ensuring the power demand is achieved.","This paper proposes an energy management strategy that adaptively splits the power demand between the generators and the batteries while simultaneously considering the battery degradation and the generator's efficient operation.","Since it is challenging to incorporate the battery degradation model directly into the optimization problem due to its complex structure and the degradation time scale which is not practical for real-time implementation, two reasonable heuristics in terms of minimizing the absolute battery power and minimizing the battery state of charge are proposed and compared to manage the battery degradation.","A model predictive energy management strategy is then developed to coordinate the power split considering the generator efficiency and minimizing the battery degradation based on the two heuristic approaches.","The designed strategy is tested via a simulation of a lumped notional shipboard power system.","The results show the impact of the battery degradation heuristics for energy management strategy in mitigating battery degradation and its health management."],"url":"http://arxiv.org/abs/2405.18633v1","category":"math.OC"}
{"created":"2024-05-28 22:19:30","title":"Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference","abstract":"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\\times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.","sentences":["The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance.","While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput.","Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost.","To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours.","Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens.","This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions.","Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs.","Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%.","More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\\times$ further speed improvement.","Our code is available at https://github.com/hmarkc/parallel-prompt-decoding."],"url":"http://arxiv.org/abs/2405.18628v1","category":"cs.LG"}
{"created":"2024-05-28 22:01:50","title":"Multi-Armed Bandits with Network Interference","abstract":"Online experimentation with interference is a common challenge in modern applications such as e-commerce and adaptive clinical trials in medicine. For example, in online marketplaces, the revenue of a good depends on discounts applied to competing goods. Statistical inference with interference is widely studied in the offline setting, but far less is known about how to adaptively assign treatments to minimize regret. We address this gap by studying a multi-armed bandit (MAB) problem where a learner (e-commerce platform) sequentially assigns one of possible $\\mathcal{A}$ actions (discounts) to $N$ units (goods) over $T$ rounds to minimize regret (maximize revenue). Unlike traditional MAB problems, the reward of each unit depends on the treatments assigned to other units, i.e., there is interference across the underlying network of units. With $\\mathcal{A}$ actions and $N$ units, minimizing regret is combinatorially difficult since the action space grows as $\\mathcal{A}^N$. To overcome this issue, we study a sparse network interference model, where the reward of a unit is only affected by the treatments assigned to $s$ neighboring units. We use tools from discrete Fourier analysis to develop a sparse linear representation of the unit-specific reward $r_n: [\\mathcal{A}]^N \\rightarrow \\mathbb{R} $, and propose simple, linear regression-based algorithms to minimize regret. Importantly, our algorithms achieve provably low regret both when the learner observes the interference neighborhood for all units and when it is unknown. This significantly generalizes other works on this topic which impose strict conditions on the strength of interference on a known network, and also compare regret to a markedly weaker optimal action. Empirically, we corroborate our theoretical findings via numerical simulations.","sentences":["Online experimentation with interference is a common challenge in modern applications such as e-commerce and adaptive clinical trials in medicine.","For example, in online marketplaces, the revenue of a good depends on discounts applied to competing goods.","Statistical inference with interference is widely studied in the offline setting, but far less is known about how to adaptively assign treatments to minimize regret.","We address this gap by studying a multi-armed bandit (MAB) problem where a learner (e-commerce platform) sequentially assigns one of possible $\\mathcal{A}$ actions (discounts) to $N$ units (goods) over $T$ rounds to minimize regret (maximize revenue).","Unlike traditional MAB problems, the reward of each unit depends on the treatments assigned to other units, i.e., there is interference across the underlying network of units.","With $\\mathcal{A}$ actions and $N$ units, minimizing regret is combinatorially difficult since the action space grows as $\\mathcal{A}^N$. To overcome this issue, we study a sparse network interference model, where the reward of a unit is only affected by the treatments assigned to $s$ neighboring units.","We use tools from discrete Fourier analysis to develop a sparse linear representation of the unit-specific reward $r_n:","[\\mathcal{A}]^N \\rightarrow \\mathbb{R} $, and propose simple, linear regression-based algorithms to minimize regret.","Importantly, our algorithms achieve provably low regret both when the learner observes the interference neighborhood for all units and when it is unknown.","This significantly generalizes other works on this topic which impose strict conditions on the strength of interference on a known network, and also compare regret to a markedly weaker optimal action.","Empirically, we corroborate our theoretical findings via numerical simulations."],"url":"http://arxiv.org/abs/2405.18621v1","category":"cs.LG"}
{"created":"2024-05-28 21:58:09","title":"Stability of the Rao-Nakra sandwich beam with a dissipation of fractional derivative type: theoretical and numerical study","abstract":"This paper is devoted to the solution and stability of a one-dimensional model depicting Rao--Nakra sandwich beams, incorporating damping terms characterized by fractional derivative types within the domain, specifically a generalized Caputo derivative with exponential weight. To address existence, uniqueness, stability, and numerical results, fractional derivatives are substituted by diffusion equations relative to a new independent variable, $\\xi$, resulting in an augmented model with a dissipative semigroup operator. Polynomial decay of energy is achieved, with a decay rate depending on the fractional derivative parameters. Both the polynomial decay and its dependency on the parameters of the generalized Caputo derivative are numerically validated. To this end, an energy-conserving finite difference numerical scheme is employed.","sentences":["This paper is devoted to the solution and stability of a one-dimensional model depicting Rao--Nakra sandwich beams, incorporating damping terms characterized by fractional derivative types within the domain, specifically a generalized Caputo derivative with exponential weight.","To address existence, uniqueness, stability, and numerical results, fractional derivatives are substituted by diffusion equations relative to a new independent variable, $\\xi$, resulting in an augmented model with a dissipative semigroup operator.","Polynomial decay of energy is achieved, with a decay rate depending on the fractional derivative parameters.","Both the polynomial decay and its dependency on the parameters of the generalized Caputo derivative are numerically validated.","To this end, an energy-conserving finite difference numerical scheme is employed."],"url":"http://arxiv.org/abs/2405.18619v1","category":"math.NA"}
{"created":"2024-05-28 21:32:11","title":"OpenConvoy: Universal Platform for Real-World Testing of Cooperative Driving Systems","abstract":"Cooperative driving, enabled by communication between automated vehicle systems, promises significant benefits to fuel efficiency, road capacity, and safety over single-vehicle driver assistance systems such as adaptive cruise control (ACC). However, the responsible development and implementation of these algorithms poses substantial challenges due to the need for extensive real-world testing. We address this issue and introduce OpenConvoy, an open and extensible framework designed for the implementation and assessment of cooperative driving policies on physical connected and autonomous vehicles (CAVs). We demonstrate the capabilities of OpenConvoy through a series of experiments on a convoy of multi-scale vehicles controlled by Platooning to show the stability of our system across vehicle configurations and its ability to effectively measure convoy cohesion across driving scenarios including varying degrees of communication loss.","sentences":["Cooperative driving, enabled by communication between automated vehicle systems, promises significant benefits to fuel efficiency, road capacity, and safety over single-vehicle driver assistance systems such as adaptive cruise control (ACC).","However, the responsible development and implementation of these algorithms poses substantial challenges due to the need for extensive real-world testing.","We address this issue and introduce OpenConvoy, an open and extensible framework designed for the implementation and assessment of cooperative driving policies on physical connected and autonomous vehicles (CAVs).","We demonstrate the capabilities of OpenConvoy through a series of experiments on a convoy of multi-scale vehicles controlled by Platooning to show the stability of our system across vehicle configurations and its ability to effectively measure convoy cohesion across driving scenarios including varying degrees of communication loss."],"url":"http://arxiv.org/abs/2405.18600v1","category":"cs.RO"}
{"created":"2024-05-28 21:07:53","title":"Emergence and long-term maintenance of modularity in spiking neural networks with plasticity","abstract":"In the last three decades the field of brain connectivity has uncovered that cortical regions, interconnected via white-matter fibers, form a modular and hierarchical network. This type of organization, which has also been recognised at the microscopic level in the form of interconnected neural assemblies, is typically believed to support the coexistence of segregation (specialization) and integration (binding) of information. A prominent remaining question is to understand how the brain could possibly become such a complex network. Here, we give a first step into answering this question and propose that adaptation to various inputs could be the key driving mechanism for the formation of structural assemblies at different scales. To illustrate that, we develop a model of (QIF) spiking neurons, subjected to stimuli targetting distributed populations. The model follows several biologically plausible constraints: (i) it contains both excitatory and inhibitory neurons with two classes of plasticity: Hebbian and anti-Hebbian STDP, (ii) dynamics are not frozen after the entrainment is finished but the network is allowed to continue firing spontaneously, and (iii) plasticity remains always active, also after the learning phase. We find that only the combination of Hebbian and anti-Hebbian inhibitory plasticity allows the formation of stable modular organization in the network. Besides, given that the model continues ``alive'' after the learning, the network settles into an asynchronous irregular firing state displaying spontaneous memory recalls which, as we show, turn crucial for the long-term consolidation of the learned memories.","sentences":["In the last three decades the field of brain connectivity has uncovered that cortical regions, interconnected via white-matter fibers, form a modular and hierarchical network.","This type of organization, which has also been recognised at the microscopic level in the form of interconnected neural assemblies, is typically believed to support the coexistence of segregation (specialization) and integration (binding) of information.","A prominent remaining question is to understand how the brain could possibly become such a complex network.","Here, we give a first step into answering this question and propose that adaptation to various inputs could be the key driving mechanism for the formation of structural assemblies at different scales.","To illustrate that, we develop a model of (QIF) spiking neurons, subjected to stimuli targetting distributed populations.","The model follows several biologically plausible constraints: (i) it contains both excitatory and inhibitory neurons with two classes of plasticity: Hebbian and anti-Hebbian STDP, (ii) dynamics are not frozen after the entrainment is finished but the network is allowed to continue firing spontaneously, and (iii) plasticity remains always active, also after the learning phase.","We find that only the combination of Hebbian and anti-Hebbian inhibitory plasticity allows the formation of stable modular organization in the network.","Besides, given that the model continues ``alive'' after the learning, the network settles into an asynchronous irregular firing state displaying spontaneous memory recalls which, as we show, turn crucial for the long-term consolidation of the learned memories."],"url":"http://arxiv.org/abs/2405.18587v1","category":"q-bio.NC"}
{"created":"2024-05-28 20:53:26","title":"Smooth connectivity in real algebraic varieties","abstract":"A standard question in real algebraic geometry is to compute the number of connected components of a real algebraic variety in affine space. By adapting an approach for determining connectivity in complements of real hypersurfaces by Hong, Rohal, Safey El Din, and Schost, algorithms are presented for computing the number of connected components, the Euler characteristic, and deciding the connectivity between two points for a smooth manifold arising as the complement of a real hypersurface of a real algebraic variety. When taking such real hypersurface to be the set of singular points, this yields an approach for determining smooth connectivity in a real algebraic variety. The method is based upon gradient ascent/descent paths on the real algebraic variety and several examples are included to demonstrate the approach.","sentences":["A standard question in real algebraic geometry is to compute the number of connected components of a real algebraic variety in affine space.","By adapting an approach for determining connectivity in complements of real hypersurfaces by Hong, Rohal, Safey El Din, and Schost, algorithms are presented for computing the number of connected components, the Euler characteristic, and deciding the connectivity between two points for a smooth manifold arising as the complement of a real hypersurface of a real algebraic variety.","When taking such real hypersurface to be the set of singular points, this yields an approach for determining smooth connectivity in a real algebraic variety.","The method is based upon gradient ascent/descent paths on the real algebraic variety and several examples are included to demonstrate the approach."],"url":"http://arxiv.org/abs/2405.18578v1","category":"math.AG"}
{"created":"2024-05-28 20:28:07","title":"Its Not a Modality Gap: Characterizing and Addressing the Contrastive Gap","abstract":"Multi-modal contrastive models such as CLIP achieve state-of-the-art performance in zero-shot classification by embedding input images and texts on a joint representational space. Recently, a modality gap has been reported in two-encoder contrastive models like CLIP, meaning that the image and text embeddings reside in disjoint areas of the latent space. Previous studies suggest that this gap exists due to 1) the cone effect, 2) mismatched pairs in the dataset, and 3) insufficient training. We show that, even when accounting for all these factors, and even when using the same modality, the contrastive loss actually creates a gap during training. As a result, We propose that the modality gap is inherent to the two-encoder contrastive loss and rename it the contrastive gap. We present evidence that attributes this contrastive gap to low uniformity in CLIP space, resulting in embeddings that occupy only a small portion of the latent space. To close the gap, we adapt the uniformity and alignment properties of unimodal contrastive loss to the multi-modal setting and show that simply adding these terms to the CLIP loss distributes the embeddings more uniformly in the representational space, closing the gap. In our experiments, we show that the modified representational space achieves better performance than default CLIP loss in downstream tasks such as zero-shot image classification and multi-modal arithmetic.","sentences":["Multi-modal contrastive models such as CLIP achieve state-of-the-art performance in zero-shot classification by embedding input images and texts on a joint representational space.","Recently, a modality gap has been reported in two-encoder contrastive models like CLIP, meaning that the image and text embeddings reside in disjoint areas of the latent space.","Previous studies suggest that this gap exists due to 1) the cone effect, 2) mismatched pairs in the dataset, and 3) insufficient training.","We show that, even when accounting for all these factors, and even when using the same modality, the contrastive loss actually creates a gap during training.","As a result, We propose that the modality gap is inherent to the two-encoder contrastive loss and rename it the contrastive gap.","We present evidence that attributes this contrastive gap to low uniformity in CLIP space, resulting in embeddings that occupy only a small portion of the latent space.","To close the gap, we adapt the uniformity and alignment properties of unimodal contrastive loss to the multi-modal setting and show that simply adding these terms to the CLIP loss distributes the embeddings more uniformly in the representational space, closing the gap.","In our experiments, we show that the modified representational space achieves better performance than default CLIP loss in downstream tasks such as zero-shot image classification and multi-modal arithmetic."],"url":"http://arxiv.org/abs/2405.18570v1","category":"cs.CV"}
{"created":"2024-05-28 20:24:51","title":"Locally different models in a checkerboard pattern with mesh adaptation and error control for multiple quantities of interest","abstract":"In this work, we apply multi-goal oriented error estimation to the finite element method. In particular, we use the dual weighted residual method and apply it to a model problem. This model problem consist of locally different coercive partial differential equations in a checkerboard pattern, where the solution is continuous across the interface. In addition to the error estimation, the error can be localized using a partition of unity technique. The resulting adaptive algorithm is substantiated with a numerical example.","sentences":["In this work, we apply multi-goal oriented error estimation to the finite element method.","In particular, we use the dual weighted residual method and apply it to a model problem.","This model problem consist of locally different coercive partial differential equations in a checkerboard pattern, where the solution is continuous across the interface.","In addition to the error estimation, the error can be localized using a partition of unity technique.","The resulting adaptive algorithm is substantiated with a numerical example."],"url":"http://arxiv.org/abs/2405.18567v1","category":"math.NA"}
{"created":"2024-05-28 20:13:44","title":"Covariance Operator Estimation via Adaptive Thresholding","abstract":"This paper studies sparse covariance operator estimation for nonstationary Gaussian processes with sharply varying marginal variance and small correlation lengthscale. We introduce a covariance operator estimator that adaptively thresholds the sample covariance function using an estimate of the variance components. Building on recent results from empirical process theory, we derive an operator norm bound on the estimation error in terms of the sparsity level of the covariance and the expected supremum of the normalized process. Our theory and numerical simulations demonstrate the advantage of adaptive threshold estimators over universal threshold and sample covariance estimators in nonstationary settings.","sentences":["This paper studies sparse covariance operator estimation for nonstationary Gaussian processes with sharply varying marginal variance and small correlation lengthscale.","We introduce a covariance operator estimator that adaptively thresholds the sample covariance function using an estimate of the variance components.","Building on recent results from empirical process theory, we derive an operator norm bound on the estimation error in terms of the sparsity level of the covariance and the expected supremum of the normalized process.","Our theory and numerical simulations demonstrate the advantage of adaptive threshold estimators over universal threshold and sample covariance estimators in nonstationary settings."],"url":"http://arxiv.org/abs/2405.18562v1","category":"math.ST"}
{"created":"2024-05-28 19:16:59","title":"Low-Rank Few-Shot Adaptation of Vision-Language Models","abstract":"Recent progress in the few-shot adaptation of Vision-Language Models (VLMs) has further pushed their generalization capabilities, at the expense of just a few labeled samples within the target downstream task. However, this promising, already quite abundant few-shot literature has focused principally on prompt learning and, to a lesser extent, on adapters, overlooking the recent advances in Parameter-Efficient Fine-Tuning (PEFT). Furthermore, existing few-shot learning methods for VLMs often rely on heavy training procedures and/or carefully chosen, task-specific hyper-parameters, which might impede their applicability. In response, we introduce Low-Rank Adaptation (LoRA) in few-shot learning for VLMs, and show its potential on 11 datasets, in comparison to current state-of-the-art prompt- and adapter-based approaches. Surprisingly, our simple CLIP-LoRA method exhibits substantial improvements, while reducing the training times and keeping the same hyper-parameters in all the target tasks, i.e., across all the datasets and numbers of shots. Certainly, our surprising results do not dismiss the potential of prompt-learning and adapter-based research. However, we believe that our strong baseline could be used to evaluate progress in these emergent subjects in few-shot VLMs.","sentences":["Recent progress in the few-shot adaptation of Vision-Language Models (VLMs) has further pushed their generalization capabilities, at the expense of just a few labeled samples within the target downstream task.","However, this promising, already quite abundant few-shot literature has focused principally on prompt learning and, to a lesser extent, on adapters, overlooking the recent advances in Parameter-Efficient Fine-Tuning (PEFT).","Furthermore, existing few-shot learning methods for VLMs often rely on heavy training procedures and/or carefully chosen, task-specific hyper-parameters, which might impede their applicability.","In response, we introduce Low-Rank Adaptation (LoRA) in few-shot learning for VLMs, and show its potential on 11 datasets, in comparison to current state-of-the-art prompt- and adapter-based approaches.","Surprisingly, our simple CLIP-LoRA method exhibits substantial improvements, while reducing the training times and keeping the same hyper-parameters in all the target tasks, i.e., across all the datasets and numbers of shots.","Certainly, our surprising results do not dismiss the potential of prompt-learning and adapter-based research.","However, we believe that our strong baseline could be used to evaluate progress in these emergent subjects in few-shot VLMs."],"url":"http://arxiv.org/abs/2405.18541v1","category":"cs.CV"}
{"created":"2024-05-28 18:18:06","title":"Symmetry-protection Zeno phase transition in monitored lattice gauge theories","abstract":"Quantum measurements profoundly influence system dynamics. They lead to complex nonequilibrium phenomena like the quantum Zeno effect, and they can be used for mitigating errors in quantum simulations.   Such an ability is particularly valuable for lattice gauge theories (LGTs), which require the challenging preservation of an extensive number of local conservation laws.   While it is known that tailored quantum measurements can soften violations of gauge symmetry, the nature of this protection, and in particular the possibility of a threshold behavior, is still unexplored.   Here, we demonstrate the existence of a sharp transition, triggered by the measurement rate, between a protected gauge-theory regime resistant to simulation errors and an irregular regime.   Our results are based on the paradigmatic example of a 1+1d $\\mathbb{Z}_2$ LGT. We study in detail the protection through projective measurements of ancillary qubits coupled to the local symmetry generators, and compare this approach with analog (weak) measurement protocols.   We show that, while the resulting ensemble averages in the continuous-time limit share the same Liouvillian dynamics, different physical implementations of the stochastic gauge protection protocol yield trajectory unravelings with vastly different statistics.   Additionally, we design an on-chip feedback mechanism that corrects bit-flip errors and significantly enhances the discrete-time scheme. Our results shed light on the dissipative criticality of strongly-interacting, highly-constrained quantum systems, and they offer valuable insights into error mitigation and correction of gauge-theory quantum simulations.","sentences":["Quantum measurements profoundly influence system dynamics.","They lead to complex nonequilibrium phenomena like the quantum Zeno effect, and they can be used for mitigating errors in quantum simulations.   ","Such an ability is particularly valuable for lattice gauge theories (LGTs), which require the challenging preservation of an extensive number of local conservation laws.   ","While it is known that tailored quantum measurements can soften violations of gauge symmetry, the nature of this protection, and in particular the possibility of a threshold behavior, is still unexplored.   ","Here, we demonstrate the existence of a sharp transition, triggered by the measurement rate, between a protected gauge-theory regime resistant to simulation errors and an irregular regime.   ","Our results are based on the paradigmatic example of a 1+1d $\\mathbb{Z}_2$ LGT.","We study in detail the protection through projective measurements of ancillary qubits coupled to the local symmetry generators, and compare this approach with analog (weak) measurement protocols.   ","We show that, while the resulting ensemble averages in the continuous-time limit share the same Liouvillian dynamics, different physical implementations of the stochastic gauge protection protocol yield trajectory unravelings with vastly different statistics.   ","Additionally, we design an on-chip feedback mechanism that corrects bit-flip errors and significantly enhances the discrete-time scheme.","Our results shed light on the dissipative criticality of strongly-interacting, highly-constrained quantum systems, and they offer valuable insights into error mitigation and correction of gauge-theory quantum simulations."],"url":"http://arxiv.org/abs/2405.18504v1","category":"quant-ph"}
{"created":"2024-05-28 18:09:22","title":"The Unified Balance Theory of Second-Moment Exponential Scaling Optimizers in Visual Tasks","abstract":"We have identified a potential method for unifying first-order optimizers through the use of variable Second-Moment Exponential Scaling(SMES). We begin with back propagation, addressing classic phenomena such as gradient vanishing and explosion, as well as issues related to dataset sparsity, and introduce the theory of balance in optimization. Through this theory, we suggest that SGD and adaptive optimizers can be unified under a broader inference, employing variable moving exponential scaling to achieve a balanced approach within a generalized formula for first-order optimizers. We conducted tests on some classic datasets and networks to confirm the impact of different balance coefficients on the overall training process.","sentences":["We have identified a potential method for unifying first-order optimizers through the use of variable Second-Moment Exponential Scaling(SMES).","We begin with back propagation, addressing classic phenomena such as gradient vanishing and explosion, as well as issues related to dataset sparsity, and introduce the theory of balance in optimization.","Through this theory, we suggest that SGD and adaptive optimizers can be unified under a broader inference, employing variable moving exponential scaling to achieve a balanced approach within a generalized formula for first-order optimizers.","We conducted tests on some classic datasets and networks to confirm the impact of different balance coefficients on the overall training process."],"url":"http://arxiv.org/abs/2405.18498v1","category":"cs.LG"}
{"created":"2024-05-28 03:06:10","title":"Adaptive Multiscale Retinal Diagnosis: A Hybrid Trio-Model Approach for Comprehensive Fundus Multi-Disease Detection Leveraging Transfer Learning and Siamese Networks","abstract":"WHO has declared that more than 2.2 billion people worldwide are suffering from visual disorders, such as media haze, glaucoma, and drusen. At least 1 billion of these cases could have been either prevented or successfully treated, yet they remain unaddressed due to poverty, a lack of specialists, inaccurate ocular fundus diagnoses by ophthalmologists, or the presence of a rare disease. To address this, the research has developed the Hybrid Trio-Network Model Algorithm for accurately diagnosing 12 distinct common and rare eye diseases. This algorithm utilized the RFMiD dataset of 3,200 fundus images and the Binary Relevance Method to detect diseases separately, ensuring expandability and avoiding incorrect correlations. Each detector, incorporating finely tuned hyperparameters to optimize performance, consisted of three feature components: A classical transfer learning CNN model, a two-stage CNN model, and a Siamese Network. The diagnosis was made using features extracted through this Trio-Model with Ensembled Machine Learning algorithms. The proposed model achieved an average accuracy of 97% and an AUC score of 0.96. Compared to past benchmark studies, an increase of over 10% in the F1-score was observed for most diseases. Furthermore, using the Siamese Network, the model successfully made predictions in diseases like optic disc pallor, which past studies failed to predict due to low confidence. This diagnostic tool presents a stable, adaptive, cost-effective, efficient, accessible, and fast solution for globalizing early detection of both common and rare diseases.","sentences":["WHO has declared that more than 2.2 billion people worldwide are suffering from visual disorders, such as media haze, glaucoma, and drusen.","At least 1 billion of these cases could have been either prevented or successfully treated, yet they remain unaddressed due to poverty, a lack of specialists, inaccurate ocular fundus diagnoses by ophthalmologists, or the presence of a rare disease.","To address this, the research has developed the Hybrid Trio-Network Model Algorithm for accurately diagnosing 12 distinct common and rare eye diseases.","This algorithm utilized the RFMiD dataset of 3,200 fundus images and the Binary Relevance Method to detect diseases separately, ensuring expandability and avoiding incorrect correlations.","Each detector, incorporating finely tuned hyperparameters to optimize performance, consisted of three feature components: A classical transfer learning CNN model, a two-stage CNN model, and a Siamese Network.","The diagnosis was made using features extracted through this Trio-Model with Ensembled Machine Learning algorithms.","The proposed model achieved an average accuracy of 97% and an AUC score of 0.96.","Compared to past benchmark studies, an increase of over 10% in the F1-score was observed for most diseases.","Furthermore, using the Siamese Network, the model successfully made predictions in diseases like optic disc pallor, which past studies failed to predict due to low confidence.","This diagnostic tool presents a stable, adaptive, cost-effective, efficient, accessible, and fast solution for globalizing early detection of both common and rare diseases."],"url":"http://arxiv.org/abs/2405.18449v1","category":"eess.IV"}
{"created":"2024-05-29 17:23:51","title":"3D Neural Edge Reconstruction","abstract":"Real-world objects and environments are predominantly composed of edge features, including straight lines and curves. Such edges are crucial elements for various applications, such as CAD modeling, surface meshing, lane mapping, etc. However, existing traditional methods only prioritize lines over curves for simplicity in geometric modeling. To this end, we introduce EMAP, a new method for learning 3D edge representations with a focus on both lines and curves. Our method implicitly encodes 3D edge distance and direction in Unsigned Distance Functions (UDF) from multi-view edge maps. On top of this neural representation, we propose an edge extraction algorithm that robustly abstracts parametric 3D edges from the inferred edge points and their directions. Comprehensive evaluations demonstrate that our method achieves better 3D edge reconstruction on multiple challenging datasets. We further show that our learned UDF field enhances neural surface reconstruction by capturing more details.","sentences":["Real-world objects and environments are predominantly composed of edge features, including straight lines and curves.","Such edges are crucial elements for various applications, such as CAD modeling, surface meshing, lane mapping, etc.","However, existing traditional methods only prioritize lines over curves for simplicity in geometric modeling.","To this end, we introduce EMAP, a new method for learning 3D edge representations with a focus on both lines and curves.","Our method implicitly encodes 3D edge distance and direction in Unsigned Distance Functions (UDF) from multi-view edge maps.","On top of this neural representation, we propose an edge extraction algorithm that robustly abstracts parametric 3D edges from the inferred edge points and their directions.","Comprehensive evaluations demonstrate that our method achieves better 3D edge reconstruction on multiple challenging datasets.","We further show that our learned UDF field enhances neural surface reconstruction by capturing more details."],"url":"http://arxiv.org/abs/2405.19295v1","category":"cs.CV"}
{"created":"2024-05-29 17:12:10","title":"Parametric satellites and connected-sums in the space of Legendrian embeddings","abstract":"This article introduces two new constructions at the higher homotopy level in the space of Legendrian embeddings in $(\\mathbb{R}^3, \\xi_{\\operatorname{std}})$. We first introduce the parametric Legendrian satellite construction, showing that the satellite operation works for parametric families of Legendrian embeddings. This yields new invariants at the higher-order homotopy level.   We then introduce the parametric connected-sum construction. This operation takes as inputs two $n$-spheres based at Legendrian embeddings $K_1$ and $K_2$, respectively, and produces a new $n$-sphere based at $K_1\\# K_2$. As a main application we construct new infinite families of loops of Legendrian embeddings with non-trivial LCH monodromy invariant.","sentences":["This article introduces two new constructions at the higher homotopy level in the space of Legendrian embeddings in $(\\mathbb{R}^3, \\xi_{\\operatorname{std}})$. We first introduce the parametric Legendrian satellite construction, showing that the satellite operation works for parametric families of Legendrian embeddings.","This yields new invariants at the higher-order homotopy level.   ","We then introduce the parametric connected-sum construction.","This operation takes as inputs two $n$-spheres based at Legendrian embeddings $K_1$ and $K_2$, respectively, and produces a new $n$-sphere based at $K_1\\# K_2$.","As a main application we construct new infinite families of loops of Legendrian embeddings with non-trivial LCH monodromy invariant."],"url":"http://arxiv.org/abs/2405.19280v1","category":"math.SG"}
{"created":"2024-05-29 16:13:54","title":"Forward-Backward Knowledge Distillation for Continual Clustering","abstract":"Unsupervised Continual Learning (UCL) is a burgeoning field in machine learning, focusing on enabling neural networks to sequentially learn tasks without explicit label information. Catastrophic Forgetting (CF), where models forget previously learned tasks upon learning new ones, poses a significant challenge in continual learning, especially in UCL, where labeled information of data is not accessible. CF mitigation strategies, such as knowledge distillation and replay buffers, often face memory inefficiency and privacy issues. Although current research in UCL has endeavored to refine data representations and address CF in streaming data contexts, there is a noticeable lack of algorithms specifically designed for unsupervised clustering. To fill this gap, in this paper, we introduce the concept of Unsupervised Continual Clustering (UCC). We propose Forward-Backward Knowledge Distillation for unsupervised Continual Clustering (FBCC) to counteract CF within the context of UCC. FBCC employs a single continual learner (the ``teacher'') with a cluster projector, along with multiple student models, to address the CF issue. The proposed method consists of two phases: Forward Knowledge Distillation, where the teacher learns new clusters while retaining knowledge from previous tasks with guidance from specialized student models, and Backward Knowledge Distillation, where a student model mimics the teacher's behavior to retain task-specific knowledge, aiding the teacher in subsequent tasks. FBCC marks a pioneering approach to UCC, demonstrating enhanced performance and memory efficiency in clustering across various tasks, outperforming the application of clustering algorithms to the latent space of state-of-the-art UCL algorithms.","sentences":["Unsupervised Continual Learning (UCL) is a burgeoning field in machine learning, focusing on enabling neural networks to sequentially learn tasks without explicit label information.","Catastrophic Forgetting (CF), where models forget previously learned tasks upon learning new ones, poses a significant challenge in continual learning, especially in UCL, where labeled information of data is not accessible.","CF mitigation strategies, such as knowledge distillation and replay buffers, often face memory inefficiency and privacy issues.","Although current research in UCL has endeavored to refine data representations and address CF in streaming data contexts, there is a noticeable lack of algorithms specifically designed for unsupervised clustering.","To fill this gap, in this paper, we introduce the concept of Unsupervised Continual Clustering (UCC).","We propose Forward-Backward Knowledge Distillation for unsupervised Continual Clustering (FBCC) to counteract CF within the context of UCC.","FBCC employs a single continual learner (the ``teacher'') with a cluster projector, along with multiple student models, to address the CF issue.","The proposed method consists of two phases: Forward Knowledge Distillation, where the teacher learns new clusters while retaining knowledge from previous tasks with guidance from specialized student models, and Backward Knowledge Distillation, where a student model mimics the teacher's behavior to retain task-specific knowledge, aiding the teacher in subsequent tasks.","FBCC marks a pioneering approach to UCC, demonstrating enhanced performance and memory efficiency in clustering across various tasks, outperforming the application of clustering algorithms to the latent space of state-of-the-art UCL algorithms."],"url":"http://arxiv.org/abs/2405.19234v1","category":"cs.LG"}
{"created":"2024-05-29 16:06:23","title":"Metallicity Dependence of Pressure-Regulated Feedback-Modulated Star Formation in the TIGRESS-NCR Simulation Suite","abstract":"We present a new suite of numerical simulations of the star-forming interstellar medium (ISM) using the TIGRESS-NCR framework, covering a wide range of galactic conditions including metallicity. The TIGRESS-NCR framework is a model of the ISM in galactic disks that solves ideal MHD equations with self-gravity in a local shearing-box, including explicit treatment of cooling and heating processes coupled with ray-tracing UV radiation transfer and resolved supernova feedback. The TIGRESS-NCR suite presented in this paper covers metallicity variation $Z'\\equiv Z/Z_\\odot\\sim 0.1-3$, gas surface density $\\Sigma_{\\rm gas}\\sim5-150{\\,M_{\\odot}{\\rm pc^{-2}}}$, and stellar surface density $\\Sigma_{\\rm star}\\sim1-50{M_{\\odot}{\\rm pc^{-2}}}$, leading to emergent SFR surface density $\\Sigma_{\\rm SFR}\\sim 10^{-4}-0.5{M_{\\odot}{\\rm kpc^{-2}yr^{-1}}}$ and ISM total midplane pressure $P_{\\rm tot}/k_B=10^3-10^6 {\\rm cm^{-3}K}$, with $P_{\\rm tot}$ equal to the ISM weight $W$. In our simulation suite, $\\Sigma_{\\rm SFR} \\propto {Z'}^{0.3}$, which can be understood based on feedback physics. We present a new calibration for the components of feedback yield $\\Upsilon$, defined as ratios between pressure (thermal, turbulent, and magnetic) and $\\Sigma_{\\rm SFR}$. We find that the thermal feedback yield varies sensitively as $\\Upsilon_{\\rm th}\\propto W^{-0.46}Z'^{-0.53}$, while the combined turbulent and magnetic feedback yield shows weaker dependence $\\Upsilon_{\\rm turb+mag}\\propto W^{-0.22}Z'^{-0.18}$. The reduced $\\Sigma_{\\rm SFR}$ at low metallicity is due mainly to enhanced thermal feedback yield resulting from reduced attenuation of UV radiation. Combining vertical dynamical equilibrium, feedback yield, and effective equation of state, we provide a new metallicity-dependent subgrid star formation prescription that can be used in cosmological simulations where the ISM is unresolved.","sentences":["We present a new suite of numerical simulations of the star-forming interstellar medium (ISM) using the TIGRESS-NCR framework, covering a wide range of galactic conditions including metallicity.","The TIGRESS-NCR framework is a model of the ISM in galactic disks that solves ideal MHD equations with self-gravity in a local shearing-box, including explicit treatment of cooling and heating processes coupled with ray-tracing UV radiation transfer and resolved supernova feedback.","The TIGRESS-NCR suite presented in this paper covers metallicity variation $Z'\\equiv Z/Z_\\odot\\sim 0.1-3$, gas surface density $\\Sigma_{\\rm gas}\\sim5-150{\\,M_{\\odot}{\\rm pc^{-2}}}$, and stellar surface density $\\Sigma_{\\rm star}\\sim1-50{M_{\\odot}{\\rm pc^{-2}}}$, leading to emergent SFR surface density $\\Sigma_{\\rm SFR}\\sim 10^{-4}-0.5{M_{\\odot}{\\rm kpc^{-2}yr^{-1}}}$ and ISM total midplane pressure $P_{\\rm tot}/k_B=10^3-10^6 {\\rm cm^{-3}K}$, with $P_{\\rm tot}$ equal to the ISM weight $W$. In our simulation suite, $\\Sigma_{\\rm SFR} \\propto {Z'}^{0.3}$, which can be understood based on feedback physics.","We present a new calibration for the components of feedback yield $\\Upsilon$, defined as ratios between pressure (thermal, turbulent, and magnetic) and $\\Sigma_{\\rm SFR}$. We find that the thermal feedback yield varies sensitively as $\\Upsilon_{\\rm th}\\propto W^{-0.46}Z'^{-0.53}$, while the combined turbulent and magnetic feedback yield shows weaker dependence $\\Upsilon_{\\rm turb+mag}\\propto W^{-0.22}Z'^{-0.18}$.","The reduced $\\Sigma_{\\rm SFR}$ at low metallicity is due mainly to enhanced thermal feedback yield resulting from reduced attenuation of UV radiation.","Combining vertical dynamical equilibrium, feedback yield, and effective equation of state, we provide a new metallicity-dependent subgrid star formation prescription that can be used in cosmological simulations where the ISM is unresolved."],"url":"http://arxiv.org/abs/2405.19227v1","category":"astro-ph.GA"}
{"created":"2024-05-29 16:02:09","title":"Lower Bounds on the Expressivity of Recurrent Neural Language Models","abstract":"The recent successes and spread of large neural language models (LMs) call for a thorough understanding of their computational ability. Describing their computational abilities through LMs' \\emph{representational capacity} is a lively area of research. However, investigation into the representational capacity of neural LMs has predominantly focused on their ability to \\emph{recognize} formal languages. For example, recurrent neural networks (RNNs) with Heaviside activations are tightly linked to regular languages, i.e., languages defined by finite-state automata (FSAs). Such results, however, fall short of describing the capabilities of RNN \\emph{language models} (LMs), which are definitionally \\emph{distributions} over strings. We take a fresh look at the representational capacity of RNN LMs by connecting them to \\emph{probabilistic} FSAs and demonstrate that RNN LMs with linearly bounded precision can express arbitrary regular LMs.","sentences":["The recent successes and spread of large neural language models (LMs) call for a thorough understanding of their computational ability.","Describing their computational abilities through LMs' \\emph{representational capacity} is a lively area of research.","However, investigation into the representational capacity of neural LMs has predominantly focused on their ability to \\emph{recognize} formal languages.","For example, recurrent neural networks (RNNs) with Heaviside activations are tightly linked to regular languages, i.e., languages defined by finite-state automata (FSAs).","Such results, however, fall short of describing the capabilities of RNN \\emph{language models} (LMs), which are definitionally \\emph{distributions} over strings.","We take a fresh look at the representational capacity of RNN LMs by connecting them to \\emph{probabilistic} FSAs and demonstrate that RNN LMs with linearly bounded precision can express arbitrary regular LMs."],"url":"http://arxiv.org/abs/2405.19222v1","category":"cs.CL"}
{"created":"2024-05-29 15:41:21","title":"Exploring the redundancy of Radon transform using a set of partial derivative equations: Could we precisely reconstruct the image from a sparse-view projection without any image prior?","abstract":"In this study, we proposed a universal n-th order partial differential equation (PDE) of 2-D Radon transform to disclose the relationship of Radon transform over a neighborhood of the integral line, named as local correlation equation (LCE). It is independent to the imaging object while in present CT theory, the relationship of Radon transform over neighboring integral line had been described depended on the imaging objection. Hence, the LCE is the first PDE to reveal the universal correlation property of Radon transform. The LCE can be applied to either of 2D CT projections or any 2-D profile of 3-D CT projections. The correlation also provides the redundancy property of Radon transform. In this regard, we carried out a preliminary study on sparse-view CT reconstruction by using a discrete first order LCE to interpolate missing projections in sparse-view sampling without knowing image prior. Meanwhile, we also proposed a unified reconstruction framework that combines a regularized iterative reconstruction with the LCE based interpolation method to handle the sparse-view CT problem with higher sparsity level. The conducted experiments have credibly validated the proposed LCE, projection interpolation method, and the unified reconstruction scheme. The result of this study suggests an attractive possibility that a sparse-view projection may contain enough information of the complete projection, by which projection completeness in CT scanning may not be necessity. This possibility would bring profound changes in CT geometry designs and reconstruction algorithms. Moreover, this study initiates an appealing research topic of exploring the redundancy property of Radon transform and investigating new CT theories based on the redundancy property, which will boost the further development of CT reconstructions.","sentences":["In this study, we proposed a universal n-th order partial differential equation (PDE) of 2-D Radon transform to disclose the relationship of Radon transform over a neighborhood of the integral line, named as local correlation equation (LCE).","It is independent to the imaging object while in present CT theory, the relationship of Radon transform over neighboring integral line had been described depended on the imaging objection.","Hence, the LCE is the first PDE to reveal the universal correlation property of Radon transform.","The LCE can be applied to either of 2D CT projections or any 2-D profile of 3-D CT projections.","The correlation also provides the redundancy property of Radon transform.","In this regard, we carried out a preliminary study on sparse-view CT reconstruction by using a discrete first order LCE to interpolate missing projections in sparse-view sampling without knowing image prior.","Meanwhile, we also proposed a unified reconstruction framework that combines a regularized iterative reconstruction with the LCE based interpolation method to handle the sparse-view CT problem with higher sparsity level.","The conducted experiments have credibly validated the proposed LCE, projection interpolation method, and the unified reconstruction scheme.","The result of this study suggests an attractive possibility that a sparse-view projection may contain enough information of the complete projection, by which projection completeness in CT scanning may not be necessity.","This possibility would bring profound changes in CT geometry designs and reconstruction algorithms.","Moreover, this study initiates an appealing research topic of exploring the redundancy property of Radon transform and investigating new CT theories based on the redundancy property, which will boost the further development of CT reconstructions."],"url":"http://arxiv.org/abs/2405.19200v1","category":"physics.med-ph"}
{"created":"2024-05-29 15:13:05","title":"Poincar\u00e9 inequality for one forms on four manifolds with bounded Ricci curvature","abstract":"In this short note, we provide a quantitative global Poincar\\'e inequality for one forms on a closed Riemannian four manifold, in terms of an upper bound on the diameter, a positive lower bound on the volume, and a two-sided bound on Ricci curvature. This seems to be the first non-trivial result giving such an inequality without any higher curvature assumptions. The proof is based on a Hodge theoretic result on orbifolds, a comparison for fundamental groups, and a spectral convergence with respect to Gromov-Hausdorff convergence, via a degeneration result to orbifolds by Anderson.","sentences":["In this short note, we provide a quantitative global Poincar\\'e inequality for one forms on a closed Riemannian four manifold, in terms of an upper bound on the diameter, a positive lower bound on the volume, and a two-sided bound on Ricci curvature.","This seems to be the first non-trivial result giving such an inequality without any higher curvature assumptions.","The proof is based on a Hodge theoretic result on orbifolds, a comparison for fundamental groups, and a spectral convergence with respect to Gromov-Hausdorff convergence, via a degeneration result to orbifolds by Anderson."],"url":"http://arxiv.org/abs/2405.19168v1","category":"math.DG"}
{"created":"2024-05-29 14:51:59","title":"Dress Anyone : Automatic Physically-Based Garment Pattern Refitting","abstract":"Well-fitted clothing is essential for both real and virtual garments to enable self-expression and accurate representation for a large variety of body types. Common practice in the industry is to provide a pre-made selection of distinct garment sizes such as small, medium and large. While these may cater to certain groups of individuals that fall within this distribution, they often exclude large sections of the population. In contrast, individually tailored clothing offers a solution to obtain custom-fit garments that are tailored to each individual. However, manual tailoring is time-consuming and requires specialized knowledge, prohibiting the approach from being applied to produce fitted clothing at scale. To address this challenge, we propose a novel method leveraging differentiable simulation for refitting and draping 3D garments and their corresponding 2D pattern panels onto a new body shape, enabling a workflow where garments only need to be designed once, in a single size, and they can be automatically refitted to support numerous body size and shape variations. Our method enables downstream applications, where our optimized 3D drape can be directly ingested into game engines or other applications. Our 2D sewing patterns allow for accurate physics-based simulations and enables manufacturing clothing for the real world.","sentences":["Well-fitted clothing is essential for both real and virtual garments to enable self-expression and accurate representation for a large variety of body types.","Common practice in the industry is to provide a pre-made selection of distinct garment sizes such as small, medium and large.","While these may cater to certain groups of individuals that fall within this distribution, they often exclude large sections of the population.","In contrast, individually tailored clothing offers a solution to obtain custom-fit garments that are tailored to each individual.","However, manual tailoring is time-consuming and requires specialized knowledge, prohibiting the approach from being applied to produce fitted clothing at scale.","To address this challenge, we propose a novel method leveraging differentiable simulation for refitting and draping 3D garments and their corresponding 2D pattern panels onto a new body shape, enabling a workflow where garments only need to be designed once, in a single size, and they can be automatically refitted to support numerous body size and shape variations.","Our method enables downstream applications, where our optimized 3D drape can be directly ingested into game engines or other applications.","Our 2D sewing patterns allow for accurate physics-based simulations and enables manufacturing clothing for the real world."],"url":"http://arxiv.org/abs/2405.19148v1","category":"cs.GR"}
{"created":"2024-05-29 14:36:47","title":"Transport model study of transverse momentum distributions of Pion, kaon, and (anti-)proton production in U+U collisions at $\\sqrt{s_{NN}}$ = 193 GeV","abstract":"The transverse momentum spectra of $\\pi ^{\\pm }$, $k ^{\\pm }$ and $p(\\bar{p})$ in midrapidity ($\\left | y \\right | < 0.1$) for nine centrality classes : $0-5\\%$, $5-10\\%$, $10-20\\%$, $20-30\\%$, $30-40\\%$, $40-50\\%$, $50-60\\%$, $60-70\\%$ and $70-80\\%$ in $^{238} U$+$^{238} U$ collisions at $\\sqrt{s_{NN}}$=193 GeV are studied within the framework of the cascade and soft momentum dependent equation of state (SM-EoS) mode of the UrQMD model. Other extracted observables from $p_{T}$ spectra such as average transverse momentum ($\\left \\langle p_{T} \\right \\rangle$), particle yields ($dN/dy$) and particle ratios are also shown as functions of collision centrality. It is found that the U+U collision process is segmented. Before the collision centrality is $50-60\\%$, the experimental data are described well using cascade mode when the $p_{T} < 1.2 GeV/c$. The results are in good agreement with the experimental data using the SM-EoS mode at $p_{T} > 1.2 GeV/c$. For the case of $60-80\\%$ centrality, the SM-EoS mode describes the data better. Anti-particle to particle yield ratios indicating pair production is the dominant mechanism of particle production at RHIC energy.","sentences":["The transverse momentum spectra of $\\pi ^{\\pm }$, $k ^{\\pm }$ and $p(\\bar{p})$ in midrapidity ($\\left | y \\right | < 0.1$) for nine centrality classes : $0-5\\%$, $5-10\\%$, $10-20\\%$, $20-30\\%$, $30-40\\%$, $40-50\\%$, $50-60\\%$, $60-70\\%$ and $70-80\\%$ in $^{238} U$+$^{238} U$ collisions at $\\sqrt{s_{NN}}$=193 GeV are studied within the framework of the cascade and soft momentum dependent equation of state (SM-EoS) mode of the UrQMD model.","Other extracted observables from $p_{T}$ spectra such as average transverse momentum ($\\left \\langle p_{T} \\right \\rangle$), particle yields ($dN/dy$) and particle ratios are also shown as functions of collision centrality.","It is found that the U+U collision process is segmented.","Before the collision centrality is $50-60\\%$, the experimental data are described well using cascade mode when the $p_{T} < 1.2 GeV/c$. The results are in good agreement with the experimental data using the SM-EoS mode at $p_{T} > 1.2 GeV/c$.","For the case of $60-80\\%$ centrality, the SM-EoS mode describes the data better.","Anti-particle to particle yield ratios indicating pair production is the dominant mechanism of particle production at RHIC energy."],"url":"http://arxiv.org/abs/2405.19130v1","category":"hep-ph"}
{"created":"2024-05-29 14:08:45","title":"Reflections to set-theoretic solutions of the Yang-Baxter equation","abstract":"The main aim of this paper is to determine reflections to bijective and non-degenerate solutions of the Yang-Baxter equation, by exploring their connections with their derived solutions. This is motivated by a recent description of left non-degenerate solutions in terms of a family of automorphisms of their associated left rack. In some cases, we show that the study of reflections for bijective and non-degenerate solutions can be reduced to those of derived type. Moreover, we extend some results obtained in the literature for reflections of involutive non-degenerate solutions to more arbitrary solutions. Besides, we provide ways for defining reflections for solutions obtained by employing some classical construction techniques of solutions. Finally, we gather some numerical data on reflections for bijective non-degenerate solutions associated with skew braces of small order.","sentences":["The main aim of this paper is to determine reflections to bijective and non-degenerate solutions of the Yang-Baxter equation, by exploring their connections with their derived solutions.","This is motivated by a recent description of left non-degenerate solutions in terms of a family of automorphisms of their associated left rack.","In some cases, we show that the study of reflections for bijective and non-degenerate solutions can be reduced to those of derived type.","Moreover, we extend some results obtained in the literature for reflections of involutive non-degenerate solutions to more arbitrary solutions.","Besides, we provide ways for defining reflections for solutions obtained by employing some classical construction techniques of solutions.","Finally, we gather some numerical data on reflections for bijective non-degenerate solutions associated with skew braces of small order."],"url":"http://arxiv.org/abs/2405.19105v1","category":"math.QA"}
{"created":"2024-05-29 13:53:31","title":"Weighted Sonine conditions and application","abstract":"The Sonine kernel described by the classical Sonine condition of convolution form is an important class of kernels used in integral equations and nonlocal differential equations. This work extends this idea to introduce weighted Sonine conditions where the non-convolutional weight functions accommodate the inhomogeneity in practical applications. We characterize tight relations between classical Sonine condition and its weighted versions, which indicates that the non-degenerate weight functions may not introduce significant changes on the set of Sonine kernels. To demonstrate the application of weighted Sonine conditions, we employ them to derive equivalent but more feasible formulations of weighted integral equations and nonlocal differential equations to prove their well-posedness, and discuss possible application to corresponding partial differential equation models.","sentences":["The Sonine kernel described by the classical Sonine condition of convolution form is an important class of kernels used in integral equations and nonlocal differential equations.","This work extends this idea to introduce weighted Sonine conditions where the non-convolutional weight functions accommodate the inhomogeneity in practical applications.","We characterize tight relations between classical Sonine condition and its weighted versions, which indicates that the non-degenerate weight functions may not introduce significant changes on the set of Sonine kernels.","To demonstrate the application of weighted Sonine conditions, we employ them to derive equivalent but more feasible formulations of weighted integral equations and nonlocal differential equations to prove their well-posedness, and discuss possible application to corresponding partial differential equation models."],"url":"http://arxiv.org/abs/2405.19091v1","category":"math.CA"}
{"created":"2024-05-29 12:23:48","title":"State Space Models are Comparable to Transformers in Estimating Functions with Dynamic Smoothness","abstract":"Deep neural networks based on state space models (SSMs) are attracting much attention in sequence modeling since their computational cost is significantly smaller than that of Transformers. While the capabilities of SSMs have been primarily investigated through experimental comparisons, theoretical understanding of SSMs is still limited. In particular, there is a lack of statistical and quantitative evaluation of whether SSM can replace Transformers. In this paper, we theoretically explore in which tasks SSMs can be alternatives of Transformers from the perspective of estimating sequence-to-sequence functions. We consider the setting where the target function has direction-dependent smoothness and prove that SSMs can estimate such functions with the same convergence rate as Transformers. Additionally, we prove that SSMs can estimate the target function, even if the smoothness changes depending on the input sequence, as well as Transformers. Our results show the possibility that SSMs can replace Transformers when estimating the functions in certain classes that appear in practice.","sentences":["Deep neural networks based on state space models (SSMs) are attracting much attention in sequence modeling since their computational cost is significantly smaller than that of Transformers.","While the capabilities of SSMs have been primarily investigated through experimental comparisons, theoretical understanding of SSMs is still limited.","In particular, there is a lack of statistical and quantitative evaluation of whether SSM can replace Transformers.","In this paper, we theoretically explore in which tasks SSMs can be alternatives of Transformers from the perspective of estimating sequence-to-sequence functions.","We consider the setting where the target function has direction-dependent smoothness and prove that SSMs can estimate such functions with the same convergence rate as Transformers.","Additionally, we prove that SSMs can estimate the target function, even if the smoothness changes depending on the input sequence, as well as Transformers.","Our results show the possibility that SSMs can replace Transformers when estimating the functions in certain classes that appear in practice."],"url":"http://arxiv.org/abs/2405.19036v1","category":"stat.ML"}
{"created":"2024-05-29 12:10:57","title":"Fractional diffusion as the limit of a short range potential Rayleigh gas","abstract":"The fractional diffusion equation is rigorously derived as a scaling limit from a deterministic Rayleigh gas, where particles interact via short range potentials with support of size $\\varepsilon$ and the background is distributed in space $R^3$ according to a Poisson process with intensity $N$ and in velocity according some fat-tailed distribution. As an intermediate step a linear Boltzmann equation is obtained in the Boltzmann-Grad limit as $\\varepsilon$ tends to zero and $N$ tends to infinity with $N \\varepsilon^2 =c$. The convergence of the empiric particle dynamics to the Boltzmann-type dynamics is shown using semigroup methods to describe probability measures on collision trees associated to physical trajectories in the case of a Rayleigh gas. The fractional diffusion equation is a hydrodynamic limit for times $t \\in [0,T]$, where $T$ and inverse mean free path $c$ can both be chosen as some negative rational power $\\varepsilon^{-k}$.","sentences":["The fractional diffusion equation is rigorously derived as a scaling limit from a deterministic Rayleigh gas, where particles interact via short range potentials with support of size $\\varepsilon$ and the background is distributed in space $R^3$ according to a Poisson process with intensity $N$ and in velocity according some fat-tailed distribution.","As an intermediate step a linear Boltzmann equation is obtained in the Boltzmann-Grad limit as $\\varepsilon$ tends to zero and $N$ tends to infinity with $N \\varepsilon^2 =c$. The convergence of the empiric particle dynamics to the Boltzmann-type dynamics is shown using semigroup methods to describe probability measures on collision trees associated to physical trajectories in the case of a Rayleigh gas.","The fractional diffusion equation is a hydrodynamic limit for times $t \\in [0,T]$, where $T$ and inverse mean free path $c$ can both be chosen as some negative rational power $\\varepsilon^{-k}$."],"url":"http://arxiv.org/abs/2405.19025v1","category":"math.AP"}
{"created":"2024-05-29 12:02:39","title":"Any K\u00e4hler metric is a Fisher information metric","abstract":"The Fisher information metric or the Fisher-Rao metric corresponds to a natural Riemannian metric defined on a parameterized family of probability density functions. As in the case of Riemannian geometry, we can define a distance in terms of the Fisher information metric, called the Fisher-Rao distance. The Fisher information metric has a wide range of applications in estimation and information theories. Indeed, it provides the most informative Cramer-Rao bound for an unbiased estimator. The Goldberg conjecture is a well-known unsolved problem which states that any compact Einstein almost K\\\"ahler manifold is necessarily a K\\\"ahler-Einstein. Note that, there is also a known odd-dimensional analog of the Goldberg conjecture in the literature. The main objective of this paper is to establish a new characterization of coK\\\"ahler manifolds and K\\\"ahler manifolds; our characterization is statistical in nature. Finally, we corroborate that every, K\\\"ahler and co-K\\\"ahler manifolds, can be viewed as being a parametric family of probability density functions, whereas K\\\"ahler and coK\\\"ahler metrics can be regarded as Fisher information metrics. In particular, we prove that, when the K\\\"ahler metric is real analytic, it is always locally the Fisher information of an exponential family. We also tackle the link between K\\\"ahler potential and Kullback-Leibler divergence.","sentences":["The Fisher information metric or the Fisher-Rao metric corresponds to a natural Riemannian metric defined on a parameterized family of probability density functions.","As in the case of Riemannian geometry, we can define a distance in terms of the Fisher information metric, called the Fisher-Rao distance.","The Fisher information metric has a wide range of applications in estimation and information theories.","Indeed, it provides the most informative Cramer-Rao bound for an unbiased estimator.","The Goldberg conjecture is a well-known unsolved problem which states that any compact Einstein almost K\\\"ahler manifold is necessarily a K\\\"ahler-Einstein.","Note that, there is also a known odd-dimensional analog of the Goldberg conjecture in the literature.","The main objective of this paper is to establish a new characterization of coK\\\"ahler manifolds and K\\\"ahler manifolds; our characterization is statistical in nature.","Finally, we corroborate that every, K\\\"ahler and co-K\\\"ahler manifolds, can be viewed as being a parametric family of probability density functions, whereas K\\\"ahler and coK\\\"ahler metrics can be regarded as Fisher information metrics.","In particular, we prove that, when the K\\\"ahler metric is real analytic, it is always locally the Fisher information of an exponential family.","We also tackle the link between K\\\"ahler potential and Kullback-Leibler divergence."],"url":"http://arxiv.org/abs/2405.19020v1","category":"math.DG"}
{"created":"2024-05-29 11:47:36","title":"Mechanism and kinetics of sodium diffusion in Na-feldspar from neural network based atomistic simulations","abstract":"Alkali diffusion is a first-order control for microstructure and compositional evolution of feldspar during cooling from high temperatures of primary magmatic or metamorphic crystallization, and knowledge of the respective diffusion coefficients is crucial for reconstructing thermal histories. Our understanding of alkali diffusion in feldspar is, however, hindered by an insufficient grasp of the underlying diffusion mechanisms. We performed molecular dynamics simulations of sodium feldspar (Albite) containing different point defects using a recently developed neural network potential. A high degree of agreement between the sodium self-diffusion coefficients obtained from model simulations and those determined experimentally in earlier studies motivated a detailed investigation into the interstitial and vacancy mechanisms, corresponding jump rates, correlation factors and anisotropy. We identified a dumbbell shaped double occupancy of an alkali site as an important point defect and a correlation effect originating from the orientation of the dumbbell as a possible cause for the $\\perp\\!\\!(001) > \\, \\perp\\!\\!(010)$ diffusion anisotropy, which has been reported in a slew of feldspar cation diffusion experiments.","sentences":["Alkali diffusion is a first-order control for microstructure and compositional evolution of feldspar during cooling from high temperatures of primary magmatic or metamorphic crystallization, and knowledge of the respective diffusion coefficients is crucial for reconstructing thermal histories.","Our understanding of alkali diffusion in feldspar is, however, hindered by an insufficient grasp of the underlying diffusion mechanisms.","We performed molecular dynamics simulations of sodium feldspar (Albite) containing different point defects using a recently developed neural network potential.","A high degree of agreement between the sodium self-diffusion coefficients obtained from model simulations and those determined experimentally in earlier studies motivated a detailed investigation into the interstitial and vacancy mechanisms, corresponding jump rates, correlation factors and anisotropy.","We identified a dumbbell shaped double occupancy of an alkali site as an important point defect and a correlation effect originating from the orientation of the dumbbell as a possible cause for the $\\perp\\!\\!(001) > \\, \\perp\\!\\!(010)$ diffusion anisotropy, which has been reported in a slew of feldspar cation diffusion experiments."],"url":"http://arxiv.org/abs/2405.19008v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 11:10:06","title":"The Process $gg\\to h^0 Z^{*}$ in the Inverted Hierarchy Scenario of the 2HDM Type-I at the LHC","abstract":"While searching at the Large Hadron Collider (LHC) for the production and decay of the CP-odd scalar ($A^0$) in the 2-Higgs-Doublet Model (2HDM) with Natural Flavour Conservation (NFC) via the channels $gg\\to A^0$ (through one-loop triangle diagrams) and $A^0\\to h^0 Z^*$ (with $m_{h^0} =125$ GeV or $m_{h^0} < 125$ GeV, with $Z$ off-shell), respectively, a factorisation of the two processes is normally performed, with the $A^0$ state being on-shell. While this approach is gauge-invariant, it is not capturing the presence of either of the following two channels: $gg\\to Z^*\\to h^0Z^*$ (through one-loop triangle diagrams) or $gg\\to h^0Z^*$ (through one-loop box diagrams). As the resolution of the $A^0$ mass cannot be infinitely precise, we affirm that all such contributions should be computed simultaneously, whichever the $h^0$($Z^{*}$) decay(splitting) products, thereby including all possible interferences amongst themselves. The cross section of the ensuing complete process is significantly different from that obtained in the factorisation case, being of the order up to ten percent in either direction at the integrated level and larger (including changes in the shape of kinematical observables) at the differential level. We thus suggest that the complete calculation ought to be performed while searching for $A^0$ in this channel. We illustrate this need for the case of a 2HDM of Type-I in the inverted hierarchy scenario with $m_{h^0}<125$ GeV.","sentences":["While searching at the Large Hadron Collider (LHC) for the production and decay of the CP-odd scalar ($A^0$) in the 2-Higgs-Doublet Model (2HDM) with Natural Flavour Conservation (NFC) via the channels $gg\\to A^0$ (through one-loop triangle diagrams) and $A^0\\to h^0 Z^*$ (with $m_{h^0} =125$ GeV or $m_{h^0} < 125$ GeV, with $Z$ off-shell), respectively, a factorisation of the two processes is normally performed, with the $A^0$ state being on-shell.","While this approach is gauge-invariant, it is not capturing the presence of either of the following two channels: $gg\\to Z^*\\to h^0Z^*$ (through one-loop triangle diagrams) or $gg\\to h^0Z^*$ (through one-loop box diagrams).","As the resolution of the $A^0$ mass cannot be infinitely precise, we affirm that all such contributions should be computed simultaneously, whichever the $h^0$($Z^{*}$) decay(splitting) products, thereby including all possible interferences amongst themselves.","The cross section of the ensuing complete process is significantly different from that obtained in the factorisation case, being of the order up to ten percent in either direction at the integrated level and larger (including changes in the shape of kinematical observables) at the differential level.","We thus suggest that the complete calculation ought to be performed while searching for $A^0$ in this channel.","We illustrate this need for the case of a 2HDM of Type-I in the inverted hierarchy scenario with $m_{h^0}<125$ GeV."],"url":"http://arxiv.org/abs/2405.18990v1","category":"hep-ph"}
{"created":"2024-05-29 09:15:23","title":"Analytic NNLO QCD corrections to top quark pair production in electron-positron collisions","abstract":"We present the analytic total cross section of top quark pair production in electron-positron annihilation at next-to-next-to-leading order (NNLO) in Quantum Chromodynamics (QCD). By utilizing the optical theorem, the NNLO corrections are related to the imaginary parts of three-loop self-energy Feynman diagrams, of which the master integrals are calculated with canonical differential equations. The analytic results for the NNLO corrections are expressed in terms of multiple polylogarithms as well as elliptic functions. We discuss the asymptotic expansions near the threshold and in the high energy limit in detail. Numerical results are provided for the total cross section of top quark pair production at future lepton colliders.","sentences":["We present the analytic total cross section of top quark pair production in electron-positron annihilation at next-to-next-to-leading order (NNLO) in Quantum Chromodynamics (QCD).","By utilizing the optical theorem, the NNLO corrections are related to the imaginary parts of three-loop self-energy Feynman diagrams, of which the master integrals are calculated with canonical differential equations.","The analytic results for the NNLO corrections are expressed in terms of multiple polylogarithms as well as elliptic functions.","We discuss the asymptotic expansions near the threshold and in the high energy limit in detail.","Numerical results are provided for the total cross section of top quark pair production at future lepton colliders."],"url":"http://arxiv.org/abs/2405.18912v1","category":"hep-ph"}
{"created":"2024-05-29 08:37:48","title":"Spatiotemporal Forecasting Meets Efficiency: Causal Graph Process Neural Networks","abstract":"Graph Neural Networks (GNNs) have advanced spatiotemporal forecasting by leveraging relational inductive biases among sensors (or any other measuring scheme) represented as nodes in a graph. However, current methods often rely on Recurrent Neural Networks (RNNs), leading to increased runtimes and memory use. Moreover, these methods typically operate within 1-hop neighborhoods, exacerbating the reduction of the receptive field. Causal Graph Processes (CGPs) offer an alternative, using graph filters instead of MLP layers to reduce parameters and minimize memory consumption. This paper introduces the Causal Graph Process Neural Network (CGProNet), a non-linear model combining CGPs and GNNs for spatiotemporal forecasting. CGProNet employs higher-order graph filters, optimizing the model with fewer parameters, reducing memory usage, and improving runtime efficiency. We present a comprehensive theoretical and experimental stability analysis, highlighting key aspects of CGProNet. Experiments on synthetic and real data demonstrate CGProNet's superior efficiency, minimizing memory and time requirements while maintaining competitive forecasting performance.","sentences":["Graph Neural Networks (GNNs) have advanced spatiotemporal forecasting by leveraging relational inductive biases among sensors (or any other measuring scheme) represented as nodes in a graph.","However, current methods often rely on Recurrent Neural Networks (RNNs), leading to increased runtimes and memory use.","Moreover, these methods typically operate within 1-hop neighborhoods, exacerbating the reduction of the receptive field.","Causal Graph Processes (CGPs) offer an alternative, using graph filters instead of MLP layers to reduce parameters and minimize memory consumption.","This paper introduces the Causal Graph Process Neural Network (CGProNet), a non-linear model combining CGPs and GNNs for spatiotemporal forecasting.","CGProNet employs higher-order graph filters, optimizing the model with fewer parameters, reducing memory usage, and improving runtime efficiency.","We present a comprehensive theoretical and experimental stability analysis, highlighting key aspects of CGProNet.","Experiments on synthetic and real data demonstrate CGProNet's superior efficiency, minimizing memory and time requirements while maintaining competitive forecasting performance."],"url":"http://arxiv.org/abs/2405.18879v1","category":"cs.LG"}
{"created":"2024-05-29 08:31:54","title":"Single image super-resolution based on trainable feature matching attention network","abstract":"Convolutional Neural Networks (CNNs) have been widely employed for image Super-Resolution (SR) in recent years. Various techniques enhance SR performance by altering CNN structures or incorporating improved self-attention mechanisms. Interestingly, these advancements share a common trait. Instead of explicitly learning high-frequency details, they learn an implicit feature processing mode that utilizes weighted sums of a feature map's own elements for reconstruction, akin to convolution and non-local. In contrast, early dictionary-based approaches learn feature decompositions explicitly to match and rebuild Low-Resolution (LR) features. Building on this analysis, we introduce Trainable Feature Matching (TFM) to amalgamate this explicit feature learning into CNNs, augmenting their representation capabilities. Within TFM, trainable feature sets are integrated to explicitly learn features from training images through feature matching. Furthermore, we integrate non-local and channel attention into our proposed Trainable Feature Matching Attention Network (TFMAN) to further enhance SR performance. To alleviate the computational demands of non-local operations, we propose a streamlined variant called Same-size-divided Region-level Non-Local (SRNL). SRNL conducts non-local computations in parallel on blocks uniformly divided from the input feature map. The efficacy of TFM and SRNL is validated through ablation studies and module explorations. We employ a recurrent convolutional network as the backbone of our TFMAN to optimize parameter utilization. Comprehensive experiments on benchmark datasets demonstrate that TFMAN achieves superior results in most comparisons while using fewer parameters. The code is available at https://github.com/qizhou000/tfman.","sentences":["Convolutional Neural Networks (CNNs) have been widely employed for image Super-Resolution (SR) in recent years.","Various techniques enhance SR performance by altering CNN structures or incorporating improved self-attention mechanisms.","Interestingly, these advancements share a common trait.","Instead of explicitly learning high-frequency details, they learn an implicit feature processing mode that utilizes weighted sums of a feature map's own elements for reconstruction, akin to convolution and non-local.","In contrast, early dictionary-based approaches learn feature decompositions explicitly to match and rebuild Low-Resolution (LR) features.","Building on this analysis, we introduce Trainable Feature Matching (TFM) to amalgamate this explicit feature learning into CNNs, augmenting their representation capabilities.","Within TFM, trainable feature sets are integrated to explicitly learn features from training images through feature matching.","Furthermore, we integrate non-local and channel attention into our proposed Trainable Feature Matching Attention Network (TFMAN) to further enhance SR performance.","To alleviate the computational demands of non-local operations, we propose a streamlined variant called Same-size-divided Region-level Non-Local (SRNL).","SRNL conducts non-local computations in parallel on blocks uniformly divided from the input feature map.","The efficacy of TFM and SRNL is validated through ablation studies and module explorations.","We employ a recurrent convolutional network as the backbone of our TFMAN to optimize parameter utilization.","Comprehensive experiments on benchmark datasets demonstrate that TFMAN achieves superior results in most comparisons while using fewer parameters.","The code is available at https://github.com/qizhou000/tfman."],"url":"http://arxiv.org/abs/2405.18872v1","category":"cs.CV"}
{"created":"2024-05-29 08:27:26","title":"Curvature properties of pseudosymmetry type of some 2-quasi-Einstein manifolds","abstract":"We determine pseudosymmetry type curvature conditions of some 2-quasi-Einstein manifolds (M,g), dim M > 3, with the Riemann-Christoffel curvature tensor R expresed by a linear combination of Kulkarni-Nomizu products formed by the metric tensor g, the Ricci tensor S and its square S^2.","sentences":["We determine pseudosymmetry type curvature conditions of some 2-quasi-Einstein manifolds (M,g), dim M > 3, with the Riemann-Christoffel curvature tensor R expresed by a linear combination of Kulkarni-Nomizu products formed by the metric tensor g, the Ricci tensor S and its square S^2."],"url":"http://arxiv.org/abs/2405.18865v1","category":"math.DG"}
{"created":"2024-05-29 08:25:04","title":"Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy","abstract":"Enabling the synthesis of arbitrarily novel viewpoint images within a patient's stomach from pre-captured monocular gastroscopic images is a promising topic in stomach diagnosis. Typical methods to achieve this objective integrate traditional 3D reconstruction techniques, including structure-from-motion (SfM) and Poisson surface reconstruction. These methods produce explicit 3D representations, such as point clouds and meshes, thereby enabling the rendering of the images from novel viewpoints. However, the existence of low-texture and non-Lambertian regions within the stomach often results in noisy and incomplete reconstructions of point clouds and meshes, hindering the attainment of high-quality image rendering. In this paper, we apply the emerging technique of neural radiance fields (NeRF) to monocular gastroscopic data for synthesizing photo-realistic images for novel viewpoints. To address the performance degradation due to view sparsity in local regions of monocular gastroscopy, we incorporate geometry priors from a pre-reconstructed point cloud into the training of NeRF, which introduces a novel geometry-based loss to both pre-captured observed views and generated unobserved views. Compared to other recent NeRF methods, our approach showcases high-fidelity image renderings from novel viewpoints within the stomach both qualitatively and quantitatively.","sentences":["Enabling the synthesis of arbitrarily novel viewpoint images within a patient's stomach from pre-captured monocular gastroscopic images is a promising topic in stomach diagnosis.","Typical methods to achieve this objective integrate traditional 3D reconstruction techniques, including structure-from-motion (SfM) and Poisson surface reconstruction.","These methods produce explicit 3D representations, such as point clouds and meshes, thereby enabling the rendering of the images from novel viewpoints.","However, the existence of low-texture and non-Lambertian regions within the stomach often results in noisy and incomplete reconstructions of point clouds and meshes, hindering the attainment of high-quality image rendering.","In this paper, we apply the emerging technique of neural radiance fields (NeRF) to monocular gastroscopic data for synthesizing photo-realistic images for novel viewpoints.","To address the performance degradation due to view sparsity in local regions of monocular gastroscopy, we incorporate geometry priors from a pre-reconstructed point cloud into the training of NeRF, which introduces a novel geometry-based loss to both pre-captured observed views and generated unobserved views.","Compared to other recent NeRF methods, our approach showcases high-fidelity image renderings from novel viewpoints within the stomach both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2405.18863v1","category":"cs.CV"}
{"created":"2024-05-29 08:05:06","title":"Notes on asymptotic behavior of radial solutions for some weighted elliptic equations on the annulus","abstract":"In this paper, we study the asymptotic behavior of radial solutions for several weighted elliptic equations with power type or exponential type nonlinearities on an annulus.","sentences":["In this paper, we study the asymptotic behavior of radial solutions for several weighted elliptic equations with power type or exponential type nonlinearities on an annulus."],"url":"http://arxiv.org/abs/2405.18854v1","category":"math.AP"}
{"created":"2024-05-29 07:09:16","title":"Node Injection Attack Based on Label Propagation Against Graph Neural Network","abstract":"Graph Neural Network (GNN) has achieved remarkable success in various graph learning tasks, such as node classification, link prediction and graph classification. The key to the success of GNN lies in its effective structure information representation through neighboring aggregation. However, the attacker can easily perturb the aggregation process through injecting fake nodes, which reveals that GNN is vulnerable to the graph injection attack. Existing graph injection attack methods primarily focus on damaging the classical feature aggregation process while overlooking the neighborhood aggregation process via label propagation. To bridge this gap, we propose the label-propagation-based global injection attack (LPGIA) which conducts the graph injection attack on the node classification task. Specifically, we analyze the aggregation process from the perspective of label propagation and transform the graph injection attack problem into a global injection label specificity attack problem. To solve this problem, LPGIA utilizes a label propagation-based strategy to optimize the combinations of the nodes connected to the injected node. Then, LPGIA leverages the feature mapping to generate malicious features for injected nodes. In extensive experiments against representative GNNs, LPGIA outperforms the previous best-performing injection attack method in various datasets, demonstrating its superiority and transferability.","sentences":["Graph Neural Network (GNN) has achieved remarkable success in various graph learning tasks, such as node classification, link prediction and graph classification.","The key to the success of GNN lies in its effective structure information representation through neighboring aggregation.","However, the attacker can easily perturb the aggregation process through injecting fake nodes, which reveals that GNN is vulnerable to the graph injection attack.","Existing graph injection attack methods primarily focus on damaging the classical feature aggregation process while overlooking the neighborhood aggregation process via label propagation.","To bridge this gap, we propose the label-propagation-based global injection attack (LPGIA) which conducts the graph injection attack on the node classification task.","Specifically, we analyze the aggregation process from the perspective of label propagation and transform the graph injection attack problem into a global injection label specificity attack problem.","To solve this problem, LPGIA utilizes a label propagation-based strategy to optimize the combinations of the nodes connected to the injected node.","Then, LPGIA leverages the feature mapping to generate malicious features for injected nodes.","In extensive experiments against representative GNNs, LPGIA outperforms the previous best-performing injection attack method in various datasets, demonstrating its superiority and transferability."],"url":"http://arxiv.org/abs/2405.18824v1","category":"cs.CR"}
{"created":"2024-05-29 06:55:03","title":"MindSemantix: Deciphering Brain Visual Experiences with a Brain-Language Model","abstract":"Deciphering the human visual experience through brain activities captured by fMRI represents a compelling and cutting-edge challenge in the field of neuroscience research. Compared to merely predicting the viewed image itself, decoding brain activity into meaningful captions provides a higher-level interpretation and summarization of visual information, which naturally enhances the application flexibility in real-world situations. In this work, we introduce MindSemantix, a novel multi-modal framework that enables LLMs to comprehend visually-evoked semantic content in brain activity. Our MindSemantix explores a more ideal brain captioning paradigm by weaving LLMs into brain activity analysis, crafting a seamless, end-to-end Brain-Language Model. To effectively capture semantic information from brain responses, we propose Brain-Text Transformer, utilizing a Brain Q-Former as its core architecture. It integrates a pre-trained brain encoder with a frozen LLM to achieve multi-modal alignment of brain-vision-language and establish a robust brain-language correspondence. To enhance the generalizability of neural representations, we pre-train our brain encoder on a large-scale, cross-subject fMRI dataset using self-supervised learning techniques. MindSemantix provides more feasibility to downstream brain decoding tasks such as stimulus reconstruction. Conditioned by MindSemantix captioning, our framework facilitates this process by integrating with advanced generative models like Stable Diffusion and excels in understanding brain visual perception. MindSemantix generates high-quality captions that are deeply rooted in the visual and semantic information derived from brain activity. This approach has demonstrated substantial quantitative improvements over prior art. Our code will be released.","sentences":["Deciphering the human visual experience through brain activities captured by fMRI represents a compelling and cutting-edge challenge in the field of neuroscience research.","Compared to merely predicting the viewed image itself, decoding brain activity into meaningful captions provides a higher-level interpretation and summarization of visual information, which naturally enhances the application flexibility in real-world situations.","In this work, we introduce MindSemantix, a novel multi-modal framework that enables LLMs to comprehend visually-evoked semantic content in brain activity.","Our MindSemantix explores a more ideal brain captioning paradigm by weaving LLMs into brain activity analysis, crafting a seamless, end-to-end Brain-Language Model.","To effectively capture semantic information from brain responses, we propose Brain-Text Transformer, utilizing a Brain Q-Former as its core architecture.","It integrates a pre-trained brain encoder with a frozen LLM to achieve multi-modal alignment of brain-vision-language and establish a robust brain-language correspondence.","To enhance the generalizability of neural representations, we pre-train our brain encoder on a large-scale, cross-subject fMRI dataset using self-supervised learning techniques.","MindSemantix provides more feasibility to downstream brain decoding tasks such as stimulus reconstruction.","Conditioned by MindSemantix captioning, our framework facilitates this process by integrating with advanced generative models like Stable Diffusion and excels in understanding brain visual perception.","MindSemantix generates high-quality captions that are deeply rooted in the visual and semantic information derived from brain activity.","This approach has demonstrated substantial quantitative improvements over prior art.","Our code will be released."],"url":"http://arxiv.org/abs/2405.18812v1","category":"cs.CV"}
{"created":"2024-05-29 06:48:51","title":"Propagation of Waves from Finite Sources Arranged in Line Segments within an Infinite Triangular Lattice","abstract":"This paper examines the propagation of time harmonic waves through a two-dimensional triangular lattice with sources located on line segments. Specifically, we investigate the discrete Helmholtz equation with a wavenumber $k \\in \\left( 0,2\\sqrt{2} \\right)$, where input data is prescribed on finite rows or columns of lattice sites. We focus on two main questions: the efficacy of the numerical methods employed in evaluating the Green's function, and the necessity of the cone condition. Consistent with a continuum theory, we employ the notion of radiating solution and establish a unique solvability result and Green's representation formula using difference potentials. Finally, we propose a numerical computation method and demonstrate its efficiency through examples related to the propagation problems in the left-handed 2D inductor-capacitor metamaterial.","sentences":["This paper examines the propagation of time harmonic waves through a two-dimensional triangular lattice with sources located on line segments.","Specifically, we investigate the discrete Helmholtz equation with a wavenumber $k \\in \\left( 0,2\\sqrt{2} \\right)$, where input data is prescribed on finite rows or columns of lattice sites.","We focus on two main questions: the efficacy of the numerical methods employed in evaluating the Green's function, and the necessity of the cone condition.","Consistent with a continuum theory, we employ the notion of radiating solution and establish a unique solvability result and Green's representation formula using difference potentials.","Finally, we propose a numerical computation method and demonstrate its efficiency through examples related to the propagation problems in the left-handed 2D inductor-capacitor metamaterial."],"url":"http://arxiv.org/abs/2405.18806v1","category":"math.AP"}
{"created":"2024-05-29 06:47:45","title":"Semiring Activation in Neural Networks","abstract":"We introduce a class of trainable nonlinear operators based on semirings that are suitable for use in neural networks. These operators generalize the traditional alternation of linear operators with activation functions in neural networks. Semirings are algebraic structures that describe a generalised notation of linearity, greatly expanding the range of trainable operators that can be included in neural networks. In fact, max- or min-pooling operations are convolutions in the tropical semiring with a fixed kernel.   We perform experiments where we replace the activation functions for trainable semiring-based operators to show that these are viable operations to include in fully connected as well as convolutional neural networks (ConvNeXt). We discuss some of the challenges of replacing traditional activation functions with trainable semiring activations and the trade-offs of doing so.","sentences":["We introduce a class of trainable nonlinear operators based on semirings that are suitable for use in neural networks.","These operators generalize the traditional alternation of linear operators with activation functions in neural networks.","Semirings are algebraic structures that describe a generalised notation of linearity, greatly expanding the range of trainable operators that can be included in neural networks.","In fact, max- or min-pooling operations are convolutions in the tropical semiring with a fixed kernel.   ","We perform experiments where we replace the activation functions for trainable semiring-based operators to show that these are viable operations to include in fully connected as well as convolutional neural networks (ConvNeXt).","We discuss some of the challenges of replacing traditional activation functions with trainable semiring activations and the trade-offs of doing so."],"url":"http://arxiv.org/abs/2405.18805v1","category":"cs.LG"}
{"created":"2024-05-29 06:35:33","title":"Face processing emerges from object-trained convolutional neural networks","abstract":"Whether face processing depends on unique, domain-specific neurocognitive mechanisms or domain-general object recognition mechanisms has long been debated. Directly testing these competing hypotheses in humans has proven challenging due to extensive exposure to both faces and objects. Here, we systematically test these hypotheses by capitalizing on recent progress in convolutional neural networks (CNNs) that can be trained without face exposure (i.e., pre-trained weights). Domain-general mechanism accounts posit that face processing can emerge from a neural network without specialized pre-training on faces. Consequently, we trained CNNs solely on objects and tested their ability to recognize and represent faces as well as objects that look like faces (face pareidolia stimuli).... Due to the character limits, for more details see in attached pdf","sentences":["Whether face processing depends on unique, domain-specific neurocognitive mechanisms or domain-general object recognition mechanisms has long been debated.","Directly testing these competing hypotheses in humans has proven challenging due to extensive exposure to both faces and objects.","Here, we systematically test these hypotheses by capitalizing on recent progress in convolutional neural networks (CNNs) that can be trained without face exposure (i.e., pre-trained weights).","Domain-general mechanism accounts posit that face processing can emerge from a neural network without specialized pre-training on faces.","Consequently, we trained CNNs solely on objects and tested their ability to recognize and represent faces as well as objects that look like faces (face pareidolia stimuli)....","Due to the character limits, for more details see in attached pdf"],"url":"http://arxiv.org/abs/2405.18800v1","category":"cs.CV"}
{"created":"2024-05-29 05:32:50","title":"LMO-DP: Optimizing the Randomization Mechanism for Differentially Private Fine-Tuning (Large) Language Models","abstract":"Differentially Private Stochastic Gradient Descent (DP-SGD) and its variants have been proposed to ensure rigorous privacy for fine-tuning large-scale pre-trained language models. However, they rely heavily on the Gaussian mechanism, which may overly perturb the gradients and degrade the accuracy, especially in stronger privacy regimes (e.g., the privacy budget $\\epsilon < 3$). To address such limitations, we propose a novel Language Model-based Optimal Differential Privacy (LMO-DP) mechanism, which takes the first step to enable the tight composition of accurately fine-tuning (large) language models with a sub-optimal DP mechanism, even in strong privacy regimes (e.g., $0.1\\leq \\epsilon<3$). Furthermore, we propose a novel offline optimal noise search method to efficiently derive the sub-optimal DP that significantly reduces the noise magnitude. For instance, fine-tuning RoBERTa-large (with 300M parameters) on the SST-2 dataset can achieve an accuracy of 92.20% (given $\\epsilon=0.3$, $\\delta=10^{-10}$) by drastically outperforming the Gaussian mechanism (e.g., $\\sim 50\\%$ for small $\\epsilon$ and $\\delta$). We also draw similar findings on the text generation tasks on GPT-2. Finally, to our best knowledge, LMO-DP is also the first solution to accurately fine-tune Llama-2 with strong differential privacy guarantees. The code will be released soon and available upon request.","sentences":["Differentially Private Stochastic Gradient Descent (DP-SGD) and its variants have been proposed to ensure rigorous privacy for fine-tuning large-scale pre-trained language models.","However, they rely heavily on the Gaussian mechanism, which may overly perturb the gradients and degrade the accuracy, especially in stronger privacy regimes (e.g., the privacy budget $\\epsilon < 3$).","To address such limitations, we propose a novel Language Model-based Optimal Differential Privacy (LMO-DP) mechanism, which takes the first step to enable the tight composition of accurately fine-tuning (large) language models with a sub-optimal DP mechanism, even in strong privacy regimes (e.g., $0.1\\leq \\epsilon<3$).","Furthermore, we propose a novel offline optimal noise search method to efficiently derive the sub-optimal DP that significantly reduces the noise magnitude.","For instance, fine-tuning RoBERTa-large (with 300M parameters) on the SST-2 dataset can achieve an accuracy of 92.20% (given $\\epsilon=0.3$, $\\delta=10^{-10}$) by drastically outperforming the Gaussian mechanism (e.g., $\\sim 50\\%$ for small $\\epsilon$ and $\\delta$).","We also draw similar findings on the text generation tasks on GPT-2.","Finally, to our best knowledge, LMO-DP is also the first solution to accurately fine-tune Llama-2 with strong differential privacy guarantees.","The code will be released soon and available upon request."],"url":"http://arxiv.org/abs/2405.18776v1","category":"cs.CR"}
{"created":"2024-05-29 05:22:31","title":"Evolving Reliable Differentiating Constraints for the Chance-constrained Maximum Coverage Problem","abstract":"Chance-constrained problems involve stochastic components in the constraints which can be violated with a small probability. We investigate the impact of different types of chance constraints on the performance of iterative search algorithms and study the classical maximum coverage problem in graphs with chance constraints. Our goal is to evolve reliable chance constraint settings for a given graph where the performance of algorithms differs significantly not just in expectation but with high confidence. This allows to better learn and understand how different types of algorithms can deal with different types of constraint settings and supports automatic algorithm selection. We develop an evolutionary algorithm that provides sets of chance constraints that differentiate the performance of two stochastic search algorithms with high confidence. We initially use traditional approximation ratio as the fitness function of (1+1)~EA to evolve instances, which shows inadequacy to generate reliable instances. To address this issue, we introduce a new measure to calculate the performance difference for two algorithms, which considers variances of performance ratios. Our experiments show that our approach is highly successful in solving the instability issue of the performance ratios and leads to evolving reliable sets of chance constraints with significantly different performance for various types of algorithms.","sentences":["Chance-constrained problems involve stochastic components in the constraints which can be violated with a small probability.","We investigate the impact of different types of chance constraints on the performance of iterative search algorithms and study the classical maximum coverage problem in graphs with chance constraints.","Our goal is to evolve reliable chance constraint settings for a given graph where the performance of algorithms differs significantly not just in expectation but with high confidence.","This allows to better learn and understand how different types of algorithms can deal with different types of constraint settings and supports automatic algorithm selection.","We develop an evolutionary algorithm that provides sets of chance constraints that differentiate the performance of two stochastic search algorithms with high confidence.","We initially use traditional approximation ratio as the fitness function of (1+1)~EA to evolve instances, which shows inadequacy to generate reliable instances.","To address this issue, we introduce a new measure to calculate the performance difference for two algorithms, which considers variances of performance ratios.","Our experiments show that our approach is highly successful in solving the instability issue of the performance ratios and leads to evolving reliable sets of chance constraints with significantly different performance for various types of algorithms."],"url":"http://arxiv.org/abs/2405.18772v1","category":"cs.NE"}
{"created":"2024-05-29 05:08:16","title":"Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI","abstract":"The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks. Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM.","sentences":["The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability.","Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs).","We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training.","Then the models can be fine-tuned for different downstream tasks.","However, compared to text data, the volume of EEG datasets is generally small and the format varies widely.","For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio.","To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM).","LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches.","Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes.","We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches.","The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks.","Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields.","Our code is available at https://github.com/935963004/LaBraM."],"url":"http://arxiv.org/abs/2405.18765v1","category":"cs.LG"}
{"created":"2024-05-29 04:39:24","title":"GIST: Greedy Independent Set Thresholding for Diverse Data Summarization","abstract":"We propose a novel subset selection task called min-distance diverse data summarization ($\\textsf{MDDS}$), which has a wide variety of applications in machine learning, e.g., data sampling and feature selection. Given a set of points in a metric space, the goal is to maximize an objective that combines the total utility of the points and a diversity term that captures the minimum distance between any pair of selected points, subject to the constraint $|S| \\le k$. For example, the points may correspond to training examples in a data sampling problem, e.g., learned embeddings of images extracted from a deep neural network. This work presents the $\\texttt{GIST}$ algorithm, which achieves a $\\frac{2}{3}$-approximation guarantee for $\\textsf{MDDS}$ by approximating a series of maximum independent set problems with a bicriteria greedy algorithm. We also prove a complementary $(\\frac{2}{3}+\\varepsilon)$-hardness of approximation, for any $\\varepsilon > 0$. Finally, we provide an empirical study that demonstrates $\\texttt{GIST}$ outperforms existing methods for $\\textsf{MDDS}$ on synthetic data, and also for a real-world image classification experiment the studies single-shot subset selection for ImageNet.","sentences":["We propose a novel subset selection task called min-distance diverse data summarization ($\\textsf{MDDS}$), which has a wide variety of applications in machine learning, e.g., data sampling and feature selection.","Given a set of points in a metric space, the goal is to maximize an objective that combines the total utility of the points and a diversity term that captures the minimum distance between any pair of selected points, subject to the constraint $|S| \\le k$.","For example, the points may correspond to training examples in a data sampling problem, e.g., learned embeddings of images extracted from a deep neural network.","This work presents the $\\texttt{GIST}$ algorithm, which achieves a $\\frac{2}{3}$-approximation guarantee for $\\textsf{MDDS}$ by approximating a series of maximum independent set problems with a bicriteria greedy algorithm.","We also prove a complementary $(\\frac{2}{3}+\\varepsilon)$-hardness of approximation, for any $\\varepsilon > 0$.","Finally, we provide an empirical study that demonstrates $\\texttt{GIST}$ outperforms existing methods for $\\textsf{MDDS}$ on synthetic data, and also for a real-world image classification experiment the studies single-shot subset selection for ImageNet."],"url":"http://arxiv.org/abs/2405.18754v1","category":"cs.DS"}
{"created":"2024-05-29 04:26:17","title":"T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback","abstract":"Diffusion-based text-to-video (T2V) models have achieved significant success but continue to be hampered by the slow sampling speed of their iterative sampling processes. To address the challenge, consistency models have been proposed to facilitate fast inference, albeit at the cost of sample quality. In this work, we aim to break the quality bottleneck of a video consistency model (VCM) to achieve $\\textbf{both fast and high-quality video generation}$. We introduce T2V-Turbo, which integrates feedback from a mixture of differentiable reward models into the consistency distillation (CD) process of a pre-trained T2V model. Notably, we directly optimize rewards associated with single-step generations that arise naturally from computing the CD loss, effectively bypassing the memory constraints imposed by backpropagating gradients through an iterative sampling process. Remarkably, the 4-step generations from our T2V-Turbo achieve the highest total score on VBench, even surpassing Gen-2 and Pika. We further conduct human evaluations to corroborate the results, validating that the 4-step generations from our T2V-Turbo are preferred over the 50-step DDIM samples from their teacher models, representing more than a tenfold acceleration while improving video generation quality.","sentences":["Diffusion-based text-to-video (T2V) models have achieved significant success but continue to be hampered by the slow sampling speed of their iterative sampling processes.","To address the challenge, consistency models have been proposed to facilitate fast inference, albeit at the cost of sample quality.","In this work, we aim to break the quality bottleneck of a video consistency model (VCM) to achieve $\\textbf{both fast and high-quality video generation}$. We introduce T2V-Turbo, which integrates feedback from a mixture of differentiable reward models into the consistency distillation (CD) process of a pre-trained T2V model.","Notably, we directly optimize rewards associated with single-step generations that arise naturally from computing the CD loss, effectively bypassing the memory constraints imposed by backpropagating gradients through an iterative sampling process.","Remarkably, the 4-step generations from our T2V-Turbo achieve the highest total score on VBench, even surpassing Gen-2 and Pika.","We further conduct human evaluations to corroborate the results, validating that the 4-step generations from our T2V-Turbo are preferred over the 50-step DDIM samples from their teacher models, representing more than a tenfold acceleration while improving video generation quality."],"url":"http://arxiv.org/abs/2405.18750v1","category":"cs.CV"}
{"created":"2024-05-29 04:09:59","title":"DESI Constraints on Exponential Quintessence","abstract":"The DESI collaboration have recently analyzed their first year of data, finding a preference for thawing dark energy scenarios when using parameterized equations of state for dark energy. We investigate whether this preference persists when the data is analyzed within the context of a well-studied field theory model of thawing dark energy, exponential quintessence. No preference for this model over $\\Lambda$CDM is found, and both models are poorer fits to the data than the Chevallier-Polarski-Linder $w_0$--$w_a$ parameterization. We demonstrate that the worse fit is due to a lack of sharp features in the potential that results in a slowly-evolving dark energy equation of state that does not have enough freedom to simultaneously fit the combination of the supernovae, DESI, and cosmic microwave background data. Our analysis provides guidance for constructing dynamical dark energy models that are able to better accommodate the data.","sentences":["The DESI collaboration have recently analyzed their first year of data, finding a preference for thawing dark energy scenarios when using parameterized equations of state for dark energy.","We investigate whether this preference persists when the data is analyzed within the context of a well-studied field theory model of thawing dark energy, exponential quintessence.","No preference for this model over $\\Lambda$CDM is found, and both models are poorer fits to the data than the Chevallier-Polarski-Linder $w_0$--$w_a$ parameterization.","We demonstrate that the worse fit is due to a lack of sharp features in the potential that results in a slowly-evolving dark energy equation of state that does not have enough freedom to simultaneously fit the combination of the supernovae, DESI, and cosmic microwave background data.","Our analysis provides guidance for constructing dynamical dark energy models that are able to better accommodate the data."],"url":"http://arxiv.org/abs/2405.18747v1","category":"astro-ph.CO"}
{"created":"2024-05-29 04:09:46","title":"STIQ: Safeguarding Training and Inferencing of Quantum Neural Networks from Untrusted Cloud","abstract":"The high expenses imposed by current quantum cloud providers, coupled with the escalating need for quantum resources, may incentivize the emergence of cheaper cloud-based quantum services from potentially untrusted providers. Deploying or hosting quantum models, such as Quantum Neural Networks (QNNs), on these untrusted platforms introduces a myriad of security concerns, with the most critical one being model theft. This vulnerability stems from the cloud provider's full access to these circuits during training and/or inference. In this work, we introduce STIQ, a novel ensemble-based strategy designed to safeguard QNNs against such cloud-based adversaries. Our method innovatively trains two distinct QNNs concurrently, hosting them on same or different platforms, in a manner that each network yields obfuscated outputs rendering the individual QNNs ineffective for adversaries operating within cloud environments. However, when these outputs are combined locally (using an aggregate function), they reveal the correct result. Through extensive experiments across various QNNs and datasets, our technique has proven to effectively masks the accuracy and losses of the individually hosted models by upto 76\\%, albeit at the expense of $\\leq 2\\times$ increase in the total computational overhead. This trade-off, however, is a small price to pay for the enhanced security and integrity of QNNs in a cloud-based environment prone to untrusted adversaries. We also demonstrated STIQ's practical application by evaluating it on real 127-qubit IBM\\_Sherbrooke hardware, showing that STIQ achieves up to 60\\% obfuscation, with combined performance comparable to an unobfuscated model.","sentences":["The high expenses imposed by current quantum cloud providers, coupled with the escalating need for quantum resources, may incentivize the emergence of cheaper cloud-based quantum services from potentially untrusted providers.","Deploying or hosting quantum models, such as Quantum Neural Networks (QNNs), on these untrusted platforms introduces a myriad of security concerns, with the most critical one being model theft.","This vulnerability stems from the cloud provider's full access to these circuits during training and/or inference.","In this work, we introduce STIQ, a novel ensemble-based strategy designed to safeguard QNNs against such cloud-based adversaries.","Our method innovatively trains two distinct QNNs concurrently, hosting them on same or different platforms, in a manner that each network yields obfuscated outputs rendering the individual QNNs ineffective for adversaries operating within cloud environments.","However, when these outputs are combined locally (using an aggregate function), they reveal the correct result.","Through extensive experiments across various QNNs and datasets, our technique has proven to effectively masks the accuracy and losses of the individually hosted models by upto 76\\%, albeit at the expense of $\\leq 2\\times$ increase in the total computational overhead.","This trade-off, however, is a small price to pay for the enhanced security and integrity of QNNs in a cloud-based environment prone to untrusted adversaries.","We also demonstrated STIQ's practical application by evaluating it on real 127-qubit IBM\\_Sherbrooke hardware, showing that STIQ achieves up to 60\\% obfuscation, with combined performance comparable to an unobfuscated model."],"url":"http://arxiv.org/abs/2405.18746v1","category":"quant-ph"}
{"created":"2024-05-29 04:07:14","title":"PanoNormal: Monocular Indoor 360\u00b0 Surface Normal Estimation","abstract":"The presence of spherical distortion on the Equirectangular image is an acknowledged challenge in dense regression computer vision tasks, such as surface normal estimation. Recent advances in convolutional neural networks (CNNs) strive to mitigate spherical distortion but often fall short in capturing holistic structures effectively, primarily due to their fixed receptive field. On the other hand, vision transformers (ViTs) excel in establishing long-range dependencies through a global self-attention mechanism, yet they encounter limitations in preserving local details. We introduce \\textit{PanoNormal}, a monocular surface normal estimation architecture designed for 360{\\deg} images, which combines the strengths of CNNs and ViTs. Specifically, we employ a multi-level global self-attention scheme with the consideration of the spherical feature distribution, enhancing the comprehensive understanding of the scene. Our experimental results demonstrate that our approach achieves state-of-the-art performance across multiple popular 360{\\deg} monocular datasets. The code and models will be released.","sentences":["The presence of spherical distortion on the Equirectangular image is an acknowledged challenge in dense regression computer vision tasks, such as surface normal estimation.","Recent advances in convolutional neural networks (CNNs) strive to mitigate spherical distortion but often fall short in capturing holistic structures effectively, primarily due to their fixed receptive field.","On the other hand, vision transformers (ViTs) excel in establishing long-range dependencies through a global self-attention mechanism, yet they encounter limitations in preserving local details.","We introduce \\textit{PanoNormal}, a monocular surface normal estimation architecture designed for 360{\\deg} images, which combines the strengths of CNNs and ViTs.","Specifically, we employ a multi-level global self-attention scheme with the consideration of the spherical feature distribution, enhancing the comprehensive understanding of the scene.","Our experimental results demonstrate that our approach achieves state-of-the-art performance across multiple popular 360{\\deg} monocular datasets.","The code and models will be released."],"url":"http://arxiv.org/abs/2405.18745v1","category":"cs.CV"}
{"created":"2024-05-29 03:39:23","title":"WLC-Net: a robust and fast deep-learning wood-leaf classification method","abstract":"Wood-leaf classification is an essential and fundamental prerequisite in the analysis and estimation of forest attributes from terrestrial laser scanning (TLS) point clouds,including critical measurements such as diameter at breast height(DBH),above-ground biomass(AGB),wood volume.To address this,we introduce the Wood-Leaf Classification Network(WLC-Net),a deep learning model derived from PointNet++,designed to differentiate between wood and leaf points within tree point clouds.WLC-Net enhances classification accuracy,completeness,and speed by incorporating linearity as an inherent feature,refining the input-output framework,and optimizing the centroid sampling technique.WLC-Net was trained and assessed using three distinct tree species datasets,comprising a total of 102 individual tree point clouds:21 Chinese ash trees,21 willow trees,and 60 tropical trees.For comparative evaluation,five alternative methods,including PointNet++,DGCNN,Krishna Moorthy's method,LeWoS, and Sun's method,were also applied to these datasets.The classification accuracy of all six methods was quantified using three metrics:overall accuracy(OA),mean Intersection over Union(mIoU),and F1-score.Across all three datasets,WLC-Net demonstrated superior performance, achieving OA scores of 0.9778, 0.9712, and 0.9508;mIoU scores of 0.9761, 0.9693,and 0.9141;and F1-scores of 0.8628, 0.7938,and 0.9019,respectively.The time costs of WLC-Net were also recorded to evaluate the efficiency.The average processing time was 102.74s per million points for WLC-Net.In terms of visual inspect,accuracy evaluation and efficiency evaluation,the results suggest that WLC-Net presents a promising approach for wood-leaf classification,distinguished by its high accuracy. In addition,WLC-Net also exhibits strong applicability across various tree point clouds and holds promise for further optimization.","sentences":["Wood-leaf classification is an essential and fundamental prerequisite in the analysis and estimation of forest attributes from terrestrial laser scanning (TLS) point clouds,including critical measurements such as diameter at breast height(DBH),above-ground biomass(AGB),wood volume.","To address this,we introduce the Wood-Leaf Classification Network(WLC-Net),a deep learning model derived from PointNet++,designed to differentiate between wood and leaf points within tree point clouds.","WLC-Net enhances classification accuracy,completeness,and speed by incorporating linearity as an inherent feature,refining the input-output framework,and optimizing the centroid sampling technique.","WLC-Net was trained and assessed using three distinct tree species datasets,comprising a total of 102 individual tree point clouds:21","Chinese ash trees,21 willow trees,and 60 tropical trees.","For comparative evaluation,five alternative methods,including PointNet++,DGCNN,Krishna Moorthy's method,LeWoS, and Sun's method,were also applied to these datasets.","The classification accuracy of all six methods was quantified using three metrics:overall accuracy(OA),mean Intersection over Union(mIoU),and F1-score.","Across all three datasets,WLC-Net demonstrated superior performance, achieving OA scores of 0.9778, 0.9712, and 0.9508;mIoU scores of 0.9761, 0.9693,and 0.9141;and F1-scores of 0.8628, 0.7938,and 0.9019,respectively.","The time costs of WLC-Net were also recorded to evaluate the efficiency.","The average processing time was 102.74s per million points for WLC-Net.","In terms of visual inspect,accuracy evaluation and efficiency evaluation,the results suggest that WLC-Net presents a promising approach for wood-leaf classification,distinguished by its high accuracy.","In addition,WLC-Net also exhibits strong applicability across various tree point clouds and holds promise for further optimization."],"url":"http://arxiv.org/abs/2405.18737v1","category":"cs.CV"}
{"created":"2024-05-29 02:48:12","title":"A Converse to the Skoda $L^2$ Division Theorem","abstract":"In this paper, we present a converse to a version of Skoda's $L^2$ division theorem by investigating the solvability of $\\bar{\\partial}$ equations of a specific type.","sentences":["In this paper, we present a converse to a version of Skoda's $L^2$ division theorem by investigating the solvability of $\\bar{\\partial}$ equations of a specific type."],"url":"http://arxiv.org/abs/2405.18713v1","category":"math.CV"}
{"created":"2024-05-29 02:21:31","title":"Multi-Condition Latent Diffusion Network for Scene-Aware Neural Human Motion Prediction","abstract":"Inferring 3D human motion is fundamental in many applications, including understanding human activity and analyzing one's intention. While many fruitful efforts have been made to human motion prediction, most approaches focus on pose-driven prediction and inferring human motion in isolation from the contextual environment, thus leaving the body location movement in the scene behind. However, real-world human movements are goal-directed and highly influenced by the spatial layout of their surrounding scenes. In this paper, instead of planning future human motion in a 'dark' room, we propose a Multi-Condition Latent Diffusion network (MCLD) that reformulates the human motion prediction task as a multi-condition joint inference problem based on the given historical 3D body motion and the current 3D scene contexts. Specifically, instead of directly modeling joint distribution over the raw motion sequences, MCLD performs a conditional diffusion process within the latent embedding space, characterizing the cross-modal mapping from the past body movement and current scene context condition embeddings to the future human motion embedding. Extensive experiments on large-scale human motion prediction datasets demonstrate that our MCLD achieves significant improvements over the state-of-the-art methods on both realistic and diverse predictions.","sentences":["Inferring 3D human motion is fundamental in many applications, including understanding human activity and analyzing one's intention.","While many fruitful efforts have been made to human motion prediction, most approaches focus on pose-driven prediction and inferring human motion in isolation from the contextual environment, thus leaving the body location movement in the scene behind.","However, real-world human movements are goal-directed and highly influenced by the spatial layout of their surrounding scenes.","In this paper, instead of planning future human motion in a 'dark' room, we propose a Multi-Condition Latent Diffusion network (MCLD) that reformulates the human motion prediction task as a multi-condition joint inference problem based on the given historical 3D body motion and the current 3D scene contexts.","Specifically, instead of directly modeling joint distribution over the raw motion sequences, MCLD performs a conditional diffusion process within the latent embedding space, characterizing the cross-modal mapping from the past body movement and current scene context condition embeddings to the future human motion embedding.","Extensive experiments on large-scale human motion prediction datasets demonstrate that our MCLD achieves significant improvements over the state-of-the-art methods on both realistic and diverse predictions."],"url":"http://arxiv.org/abs/2405.18700v1","category":"cs.CV"}
{"created":"2024-05-29 02:00:41","title":"Partially invariant solution with an arbitrary surface of blow-up for the gas dynamics equations admitting pressure translation","abstract":"We applied a method of symmetry reduction to the gas dynamics equations with a special form of the equation of state. This equation of state is a pressure represented as the sum of a density and an entropy functions. The symmetry Lie algebra of the system is 12-dimensional. One, two and three-dimensional subalgebras were considered. In this article, four-dimensional subalgebras are considered for the first time. Specifically, invariants are calculated for 50 four-dimensional subalgebras. Using invariants of one of the subalgebras, a symmetry reduction of the original system is calculated. The reduced system is a partially invariant submodel because one gas-dynamic function cannot be expressed in terms of the invariants. The submodel leads to two families of exact solutions, one of which describes the isochoric motion of the media, and the other solution specifies an arbitrary blow-up surface. For the first family of solutions, the particle trajectories are parabolas or rays; for the second family of solutions, the particles move along cubic parabolas or straight lines. From each point of the blow-up surface, particles fly out at different speeds and end up on a straight line at any other fixed moment in time. A description of the motion of particles for each family of solutions is given.","sentences":["We applied a method of symmetry reduction to the gas dynamics equations with a special form of the equation of state.","This equation of state is a pressure represented as the sum of a density and an entropy functions.","The symmetry Lie algebra of the system is 12-dimensional.","One, two and three-dimensional subalgebras were considered.","In this article, four-dimensional subalgebras are considered for the first time.","Specifically, invariants are calculated for 50 four-dimensional subalgebras.","Using invariants of one of the subalgebras, a symmetry reduction of the original system is calculated.","The reduced system is a partially invariant submodel because one gas-dynamic function cannot be expressed in terms of the invariants.","The submodel leads to two families of exact solutions, one of which describes the isochoric motion of the media, and the other solution specifies an arbitrary blow-up surface.","For the first family of solutions, the particle trajectories are parabolas or rays; for the second family of solutions, the particles move along cubic parabolas or straight lines.","From each point of the blow-up surface, particles fly out at different speeds and end up on a straight line at any other fixed moment in time.","A description of the motion of particles for each family of solutions is given."],"url":"http://arxiv.org/abs/2405.18691v1","category":"math.AP"}
{"created":"2024-05-29 01:58:22","title":"Differentially-Private Distributed Model Predictive Control of Linear Discrete-Time Systems with Global Constraints","abstract":"Distributed model predictive control (DMPC) has attracted extensive attention as it can explicitly handle system constraints and achieve optimal control in a decentralized manner. However, the deployment of DMPC strategies generally requires the sharing of sensitive data among subsystems, which may violate the privacy of participating systems. In this paper, we propose a differentially-private DMPC algorithm for linear discrete-time systems subject to coupled global constraints. Specifically, we first show that a conventional distributed dual gradient algorithm can be used to address the considered DMPC problem but cannot provide strong privacy preservation. Then, to protect privacy against the eavesdropper, we incorporate a differential-privacy noise injection mechanism into the DMPC framework and prove that the resulting distributed optimization algorithm can ensure both provable convergence to a global optimal solution and rigorous $\\epsilon$-differential privacy. In addition, an implementation strategy of the DMPC is designed such that the recursive feasibility and stability of the closed-loop system are guaranteed. Numerical simulation results confirm the effectiveness of the proposed approach.","sentences":["Distributed model predictive control (DMPC) has attracted extensive attention as it can explicitly handle system constraints and achieve optimal control in a decentralized manner.","However, the deployment of DMPC strategies generally requires the sharing of sensitive data among subsystems, which may violate the privacy of participating systems.","In this paper, we propose a differentially-private DMPC algorithm for linear discrete-time systems subject to coupled global constraints.","Specifically, we first show that a conventional distributed dual gradient algorithm can be used to address the considered DMPC problem but cannot provide strong privacy preservation.","Then, to protect privacy against the eavesdropper, we incorporate a differential-privacy noise injection mechanism into the DMPC framework and prove that the resulting distributed optimization algorithm can ensure both provable convergence to a global optimal solution and rigorous $\\epsilon$-differential privacy.","In addition, an implementation strategy of the DMPC is designed such that the recursive feasibility and stability of the closed-loop system are guaranteed.","Numerical simulation results confirm the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2405.18690v1","category":"eess.SY"}
{"created":"2024-05-29 01:25:43","title":"Learning Diffeomorphism for Image Registration with Time-Continuous Networks using Semigroup Regularization","abstract":"Diffeomorphic image registration (DIR) is a critical task in 3D medical image analysis, aimed at finding topology preserving deformations between pairs of images. Focusing on the solution of the flow map differential equation as the diffeomorphic deformation, recent methods use discrete timesteps along with various regularization terms to penalize the negative determinant of Jacobian and impose smoothness of the solution vector field. In this paper, we propose a novel learning-based approach for diffeomorphic 3D-image registration which finds the diffeomorphisms in the time continuum with fewer regularization terms and no additional integration. As one of the fundamental properties of flow maps, we exploit the semigroup property as the only form of regularization, ensuring temporally continuous diffeomorphic flows between pairs of images. Leveraging this property, our method alleviates the need for additional regularization terms and scaling and squaring integration during both training and evaluation. To achieve time-continuous diffeomorphisms, we employ time-embedded UNets, a technique commonly utilized in diffusion models. The proposed method reveals that ensuring diffeomorphism in a continuous time interval leads to better registration results. Experimental results on two public datasets (OASIS and CANDI) demonstrate the superiority of our model over both learning-based and optimization-based methods.","sentences":["Diffeomorphic image registration (DIR) is a critical task in 3D medical image analysis, aimed at finding topology preserving deformations between pairs of images.","Focusing on the solution of the flow map differential equation as the diffeomorphic deformation, recent methods use discrete timesteps along with various regularization terms to penalize the negative determinant of Jacobian and impose smoothness of the solution vector field.","In this paper, we propose a novel learning-based approach for diffeomorphic 3D-image registration which finds the diffeomorphisms in the time continuum with fewer regularization terms and no additional integration.","As one of the fundamental properties of flow maps, we exploit the semigroup property as the only form of regularization, ensuring temporally continuous diffeomorphic flows between pairs of images.","Leveraging this property, our method alleviates the need for additional regularization terms and scaling and squaring integration during both training and evaluation.","To achieve time-continuous diffeomorphisms, we employ time-embedded UNets, a technique commonly utilized in diffusion models.","The proposed method reveals that ensuring diffeomorphism in a continuous time interval leads to better registration results.","Experimental results on two public datasets (OASIS and CANDI) demonstrate the superiority of our model over both learning-based and optimization-based methods."],"url":"http://arxiv.org/abs/2405.18684v1","category":"cs.CV"}
{"created":"2024-05-29 01:01:19","title":"Vim-F: Visual State Space Model Benefiting from Learning in the Frequency Domain","abstract":"In recent years, State Space Models (SSMs) with efficient hardware-aware designs, known as the Mamba deep learning models, have made significant progress in modeling long sequences such as language understanding. Therefore, building efficient and general-purpose visual backbones based on SSMs is a promising direction. Compared to traditional convolutional neural networks (CNNs) and Vision Transformers (ViTs), the performance of Vision Mamba (ViM) methods is not yet fully competitive. To enable SSMs to process image data, ViMs typically flatten 2D images into 1D sequences, inevitably ignoring some 2D local dependencies, thereby weakening the model's ability to interpret spatial relationships from a global perspective. We use Fast Fourier Transform (FFT) to obtain the spectrum of the feature map and add it to the original feature map, enabling ViM to model a unified visual representation in both frequency and spatial domains. The introduction of frequency domain information enables ViM to have a global receptive field during scanning. We propose a novel model called Vim-F, which employs pure Mamba encoders and scans in both the frequency and spatial domains. Moreover, we question the necessity of position embedding in ViM and remove it accordingly in Vim-F, which helps to fully utilize the efficient long-sequence modeling capability of ViM. Finally, we redesign a patch embedding for Vim-F, leveraging a convolutional stem to capture more local correlations, further improving the performance of Vim-F. Code is available at: \\url{https://github.com/yws-wxs/Vim-F}.","sentences":["In recent years, State Space Models (SSMs) with efficient hardware-aware designs, known as the Mamba deep learning models, have made significant progress in modeling long sequences such as language understanding.","Therefore, building efficient and general-purpose visual backbones based on SSMs is a promising direction.","Compared to traditional convolutional neural networks (CNNs) and Vision Transformers (ViTs), the performance of Vision Mamba (ViM) methods is not yet fully competitive.","To enable SSMs to process image data, ViMs typically flatten 2D images into 1D sequences, inevitably ignoring some 2D local dependencies, thereby weakening the model's ability to interpret spatial relationships from a global perspective.","We use Fast Fourier Transform (FFT) to obtain the spectrum of the feature map and add it to the original feature map, enabling ViM to model a unified visual representation in both frequency and spatial domains.","The introduction of frequency domain information enables ViM to have a global receptive field during scanning.","We propose a novel model called Vim-F, which employs pure Mamba encoders and scans in both the frequency and spatial domains.","Moreover, we question the necessity of position embedding in ViM and remove it accordingly in Vim-F, which helps to fully utilize the efficient long-sequence modeling capability of ViM. Finally, we redesign a patch embedding for Vim-F, leveraging a convolutional stem to capture more local correlations, further improving the performance of Vim-F. Code is available at: \\url{https://github.com/yws-wxs/Vim-F}."],"url":"http://arxiv.org/abs/2405.18679v1","category":"cs.CV"}
{"created":"2024-05-29 00:47:22","title":"Exploring Automated Contouring Across Institutional Boundaries: A Deep Learning Approach with Mouse Micro-CT Datasets","abstract":"Image-guided mouse irradiation is essential to understand interventions involving radiation prior to human studies. Our objective is to employ Swin UNEt Transformers (Swin UNETR) to segment native micro-CT and contrast-enhanced micro-CT scans and benchmark the results against 3D no-new-Net (nnU-Net). Swin UNETR reformulates mouse organ segmentation as a sequence-to-sequence prediction task, using a hierarchical Swin Transformer encoder to extract features at 5 resolution levels, and connects to a Fully Convolutional Neural Network (FCNN)-based decoder via skip connections. The models were trained and evaluated on open datasets, with data separation based on individual mice. Further evaluation on an external mouse dataset acquired on a different micro-CT with lower kVp and higher imaging noise was also employed to assess model robustness and generalizability. Results indicate that Swin UNETR consistently outperforms nnU-Net and AIMOS in terms of average dice similarity coefficient (DSC) and Hausdorff distance (HD95p), except in two mice of intestine contouring. This superior performance is especially evident in the external dataset, confirming the model's robustness to variations in imaging conditions, including noise and quality, thereby positioning Swin UNETR as a highly generalizable and efficient tool for automated contouring in pre-clinical workflows.","sentences":["Image-guided mouse irradiation is essential to understand interventions involving radiation prior to human studies.","Our objective is to employ Swin UNEt Transformers (Swin UNETR) to segment native micro-CT and contrast-enhanced micro-CT scans and benchmark the results against 3D no-new-Net (nnU-Net).","Swin UNETR reformulates mouse organ segmentation as a sequence-to-sequence prediction task, using a hierarchical Swin Transformer encoder to extract features at 5 resolution levels, and connects to a Fully Convolutional Neural Network (FCNN)-based decoder via skip connections.","The models were trained and evaluated on open datasets, with data separation based on individual mice.","Further evaluation on an external mouse dataset acquired on a different micro-CT with lower kVp and higher imaging noise was also employed to assess model robustness and generalizability.","Results indicate that Swin UNETR consistently outperforms nnU-Net and AIMOS in terms of average dice similarity coefficient (DSC) and Hausdorff distance (HD95p), except in two mice of intestine contouring.","This superior performance is especially evident in the external dataset, confirming the model's robustness to variations in imaging conditions, including noise and quality, thereby positioning Swin UNETR as a highly generalizable and efficient tool for automated contouring in pre-clinical workflows."],"url":"http://arxiv.org/abs/2405.18676v1","category":"physics.med-ph"}
{"created":"2024-05-29 00:38:50","title":"GAN: Dynamics","abstract":"We study quantitatively the overparametrization limit of the original Wasserstein-GAN algorithm. Effectively, we show that the algorithm is a stochastic discretization of a system of continuity equations for the parameter distributions of the generator and discriminator. We show that parameter clipping to satisfy the Lipschitz condition in the algorithm induces a discontinuous vector field in the mean field dynamics, which gives rise to blow-up in finite time of the mean field dynamics. We look into a specific toy example that shows that all solutions to the mean field equations converge in the long time limit to time periodic solutions, this helps explain the failure to converge.","sentences":["We study quantitatively the overparametrization limit of the original Wasserstein-GAN algorithm.","Effectively, we show that the algorithm is a stochastic discretization of a system of continuity equations for the parameter distributions of the generator and discriminator.","We show that parameter clipping to satisfy the Lipschitz condition in the algorithm induces a discontinuous vector field in the mean field dynamics, which gives rise to blow-up in finite time of the mean field dynamics.","We look into a specific toy example that shows that all solutions to the mean field equations converge in the long time limit to time periodic solutions, this helps explain the failure to converge."],"url":"http://arxiv.org/abs/2405.18673v1","category":"math.AP"}
{"created":"2024-05-28 23:46:34","title":"Applications of the quaternionic Jordan form to hypercomplex geometry","abstract":"We apply the quaternionic Jordan form to classify the hypercomplex nilpotent almost abelian Lie algebras in all dimensions and to carry out the complete classification of 12-dimensional hypercomplex almost abelian Lie algebras. Moreover, we determine which 12-dimensional simply connected hypercomplex almost abelian Lie groups admit lattices. Finally, for each integer $n>1$ we construct infinitely many, up to diffeomorphism, $(4n+4)$-dimensional hypercomplex almost abelian solvmanifolds which are completely solvable. These solvmanifolds arise from a distinguished family of monic integer polynomials of degree $n$.","sentences":["We apply the quaternionic Jordan form to classify the hypercomplex nilpotent almost abelian Lie algebras in all dimensions and to carry out the complete classification of 12-dimensional hypercomplex almost abelian Lie algebras.","Moreover, we determine which 12-dimensional simply connected hypercomplex almost abelian Lie groups admit lattices.","Finally, for each integer $n>1$ we construct infinitely many, up to diffeomorphism, $(4n+4)$-dimensional hypercomplex almost abelian solvmanifolds which are completely solvable.","These solvmanifolds arise from a distinguished family of monic integer polynomials of degree $n$."],"url":"http://arxiv.org/abs/2405.18656v1","category":"math.DG"}
{"created":"2024-05-28 22:14:16","title":"Higher gauge theory and integrability","abstract":"In recent years, significant progress has been made in the study of integrable systems from a gauge theoretic perspective. This development originated with the introduction of $4$d Chern-Simons theory with defects, which provided a systematic framework for constructing two-dimensional integrable systems. In this article, we propose a novel approach to studying higher-dimensional integrable models employing techniques from higher category theory. Starting with higher Chern-Simons theory on the $4$-manifold $\\mathbb{R}\\times Y$, we complexify and compactify the real line to $\\mathbb{C}P^1$ and introduce the disorder defect $\\omega=z^{-1}\\mathrm{d} z $. This procedure defines a holomorphic five-dimensional variant of higher Chern-Simons theory, which, when endowed with suitable boundary conditions, allows for the localisation to a three-dimensional theory on $Y$. The equations of motion of the resulting model are equivalent to the flatness of a $2$-connection $(L,H)$, that we then use to construct the corresponding higher holonomies. We prove that these are invariants of homotopies relative boundary, which enables the construction of conserved quantities. The latter are labelled by both the categorical characters of a Lie crossed-module and the infinite number of homotopy classes of surfaces relative boundary in $Y$. Moreover, we also demonstrate that the $3$d theory has left and right acting symmetries whose current algebra is given by an infinite dimensional centrally extended affine Lie 2-algebra. Both of these conditions are direct higher homotopy analogues of the properties satisfied by the 2d Wess-Zumino-Witten CFT, which we therefore interpret as facets of integrable structures.","sentences":["In recent years, significant progress has been made in the study of integrable systems from a gauge theoretic perspective.","This development originated with the introduction of $4$d Chern-Simons theory with defects, which provided a systematic framework for constructing two-dimensional integrable systems.","In this article, we propose a novel approach to studying higher-dimensional integrable models employing techniques from higher category theory.","Starting with higher Chern-Simons theory on the $4$-manifold $\\mathbb{R}\\times Y$, we complexify and compactify the real line to $\\mathbb{C}P^1$ and introduce the disorder defect $\\omega=z^{-1}\\mathrm{d} z $.","This procedure defines a holomorphic five-dimensional variant of higher Chern-Simons theory, which, when endowed with suitable boundary conditions, allows for the localisation to a three-dimensional theory on $Y$. The equations of motion of the resulting model are equivalent to the flatness of a $2$-connection $(L,H)$, that we then use to construct the corresponding higher holonomies.","We prove that these are invariants of homotopies relative boundary, which enables the construction of conserved quantities.","The latter are labelled by both the categorical characters of a Lie crossed-module and the infinite number of homotopy classes of surfaces relative boundary in $Y$.","Moreover, we also demonstrate that the $3$d theory has left and right acting symmetries whose current algebra is given by an infinite dimensional centrally extended affine Lie 2-algebra.","Both of these conditions are direct higher homotopy analogues of the properties satisfied by the 2d Wess-Zumino-Witten CFT, which we therefore interpret as facets of integrable structures."],"url":"http://arxiv.org/abs/2405.18625v1","category":"hep-th"}
{"created":"2024-05-28 21:41:48","title":"A semilinear problem associated to the space-time fractional heat equation in $\\mathbb{R}^N$","abstract":"We study the fully nonlocal semilinear equation $\\partial_t^\\alpha u+(-\\Delta)^\\beta u=|u|^{p-1}u$, $p\\ge1$, where $\\partial_t^\\alpha$ stands for the Caputo derivative of order $\\alpha\\in (0,1)$ and $(-\\Delta)^\\beta$, $\\beta\\in(0,1]$, is the usual $\\beta$ power of the Laplacian. We prescribe an initial datum in $L^q(\\mathbb{R}^N)$.   We give conditions ensuring the existence and uniqueness of a solution living in $L^q(\\mathbb{R}^N)$ up to a maximal existence time $T$ that may be finite or infinite. If~$T$ is finite, the $L^q$ norm of the solution becomes unbounded as time approaches $T$, and $u$ is said to blow up in $L^q$. Otherwise, the solution is global in time.   For the case of nonnegative and nontrivial solutions, we give conditions on the initial datum that ensure either blow-up or global existence. It turns out that every nonnegative nontrivial solution in $L^q$ blows up in finite time if $1<p<p_f:=1+\\frac{2\\beta}N$ whereas if $p\\ge p_f$ there are both solutions that blow up and global ones. The critical exponent $p_f$, which does not depend on $\\alpha$, coincides with the Fujita exponent for the case $\\alpha=1$, in which the time derivative is the standard (local) one. In contrast to the case $\\alpha=1$, when $\\alpha\\in(0,1)$ the critical exponent $p=p_f$ falls within the situation in which global existence may occur. Our weakest condition for global existence and our condition for blow-up are both related to the size of the mean value of the initial datum in large balls.","sentences":["We study the fully nonlocal semilinear equation $\\partial_t^\\alpha u+(-\\Delta)^\\beta u=|u|^{p-1}u$, $p\\ge1$, where $\\partial_t^\\alpha$ stands for the Caputo derivative of order $\\alpha\\in (0,1)$ and $(-\\Delta)^\\beta$, $\\beta\\in(0,1]$, is the usual $\\beta$ power of the Laplacian.","We prescribe an initial datum in $L^q(\\mathbb{R}^N)$.   We give conditions ensuring the existence and uniqueness of a solution living in $L^q(\\mathbb{R}^N)$ up to a maximal existence time $T$ that may be finite or infinite.","If~$T$ is finite, the $L^q$ norm of the solution becomes unbounded as time approaches $T$, and $u$ is said to blow up in $L^q$. Otherwise, the solution is global in time.   ","For the case of nonnegative and nontrivial solutions, we give conditions on the initial datum that ensure either blow-up or global existence.","It turns out that every nonnegative nontrivial solution in $L^q$ blows up in finite time if $1<p<p_f:=1+\\frac{2\\beta}N$ whereas if $p\\ge p_f$ there are both solutions that blow up and global ones.","The critical exponent $p_f$, which does not depend on $\\alpha$, coincides with the Fujita exponent for the case $\\alpha=1$, in which the time derivative is the standard (local) one.","In contrast to the case $\\alpha=1$, when $\\alpha\\in(0,1)$ the critical exponent $p=p_f$ falls within the situation in which global existence may occur.","Our weakest condition for global existence and our condition for blow-up are both related to the size of the mean value of the initial datum in large balls."],"url":"http://arxiv.org/abs/2405.18612v1","category":"math.AP"}
{"created":"2024-05-28 21:40:27","title":"A better bound on blow-up rate for the superconformal semilinear wave equation","abstract":"We consider the semilinear wave equation in higher dimensions with superconformal power nonlinearity. The purpose of this paper is to give a new upper bound on the blow-up rate in some space-time integral, showing a $|\\log(T-t)|^q$ improvement in comparison with previous results obtained in \\cite{HZdcds13,KSVsurc12}.","sentences":["We consider the semilinear wave equation in higher dimensions with superconformal power nonlinearity.","The purpose of this paper is to give a new upper bound on the blow-up rate in some space-time integral, showing a $|\\log(T-t)|^q$ improvement in comparison with previous results obtained in \\cite{HZdcds13,KSVsurc12}."],"url":"http://arxiv.org/abs/2405.18611v1","category":"math.AP"}
{"created":"2024-05-28 21:33:57","title":"Metric Reconstruction in Kerr Spacetime","abstract":"Metric reconstruction is the general problem of parameterizing GR in terms of its two ``true degrees of freedom'', e.g., by a complex scalar ``potential'' -- in practice mostly with the aim of simplifying the Einstein equation (EE) within perturbative approaches. In this paper, we re-analyze the metric reconstruction procedure by Green, Hollands, and Zimmerman (GHZ) [Class. Quant. Grav. \\textbf{37}, 075001 (2020)], which is a generalization of the Chrzanowski-Cohen-Kegeles (CCK) approach. Contrary to the CCK method, that by GHZ is applicable not only to the vacuum, but also to the sourced linearized Einstein equation (EE). Our main innovation is a version of the GHZ method giving an efficient integration scheme for the initial value problem of the sourced linear EE. By iteration, our scheme gives the metric to as high an order in perturbation theory around Kerr as one might wish, in principle. At each order, the metric perturbation is a sum of a corrector, obtained by solving a triangular system of transport equations, a reconstructed piece, obtained from a Hertz potential as in the CCK approach, and an algebraically special perturbation, determined by the ADM quantities. As a by-product, we determine the precise relations between the asymptotic tail of the Hertz potential in the GHZ and CCK schemes, and the quantities relevant for gravitational radiation, namely, the energy flux, news- and memory tensors, and their associated BMS-supertranslations. We also discuss ways of transforming the metric perturbation to Lorenz gauge.","sentences":["Metric reconstruction is the general problem of parameterizing GR in terms of its two ``true degrees of freedom'', e.g., by a complex scalar ``potential'' -- in practice mostly with the aim of simplifying the Einstein equation (EE) within perturbative approaches.","In this paper, we re-analyze the metric reconstruction procedure by Green, Hollands, and Zimmerman (GHZ)","[Class. Quant.","Grav. \\textbf{37}, 075001 (2020)], which is a generalization of the Chrzanowski-Cohen-Kegeles (CCK) approach.","Contrary to the CCK method, that by GHZ is applicable not only to the vacuum, but also to the sourced linearized Einstein equation (EE).","Our main innovation is a version of the GHZ method giving an efficient integration scheme for the initial value problem of the sourced linear EE.","By iteration, our scheme gives the metric to as high an order in perturbation theory around Kerr as one might wish, in principle.","At each order, the metric perturbation is a sum of a corrector, obtained by solving a triangular system of transport equations, a reconstructed piece, obtained from a Hertz potential as in the CCK approach, and an algebraically special perturbation, determined by the ADM quantities.","As a by-product, we determine the precise relations between the asymptotic tail of the Hertz potential in the GHZ and CCK schemes, and the quantities relevant for gravitational radiation, namely, the energy flux, news- and memory tensors, and their associated BMS-supertranslations.","We also discuss ways of transforming the metric perturbation to Lorenz gauge."],"url":"http://arxiv.org/abs/2405.18604v1","category":"gr-qc"}
{"created":"2024-05-28 21:33:49","title":"A constant rank theorem for special Lagrangian equations","abstract":"Constant rank theorems are obtained for saddle solutions to the special Lagrangian equation and the quadratic Hessian equation. The argument also leads to Liouville type results for the special Lagrangian equation with subcritical phase, matching the known rigidity results for semiconvex entire solutions to the quadratic Hessian equation.","sentences":["Constant rank theorems are obtained for saddle solutions to the special Lagrangian equation and the quadratic Hessian equation.","The argument also leads to Liouville type results for the special Lagrangian equation with subcritical phase, matching the known rigidity results for semiconvex entire solutions to the quadratic Hessian equation."],"url":"http://arxiv.org/abs/2405.18603v1","category":"math.AP"}
{"created":"2024-05-28 21:21:16","title":"Ergodic quasi-isometries and the cohomology of nilpotent Lie groups","abstract":"We give a simplified and self-contained proof of the following result due to Shalom, Sauer, and Gotfredsen-Kyed: two quasi-isometric simply connected nilpotent Lie groups $G$ and $H$ have isomorphic cohomology algebras. Our proof is based on considering maps which induce an ergodic measure on the space of functions from $G$ to $H$ ($\\textit{ergodic maps}$), and we show that, given an ergodic quasi-isometry, one can construct an explicit isomorphism from $H^*(H)$ to $H^*(G)$.   Specifically, when $\\psi$ is an ergodic quasi-isometry, the pullback $\\psi^*\\omega$ of a differential form $\\omega$ has a well-defined $\\textit{amenable average}$ $\\overline{\\psi^*}\\omega$, and we show that $\\overline{\\psi^*}$ is the desired isomorphism. A key observation in our proof is that quasi-isometries of nilpotent groups are coarsely volume-preserving, so the amenable average of the pullback of the volume form is always nonzero.","sentences":["We give a simplified and self-contained proof of the following result due to Shalom, Sauer, and Gotfredsen-Kyed: two quasi-isometric simply connected nilpotent Lie groups $G$ and $H$ have isomorphic cohomology algebras.","Our proof is based on considering maps which induce an ergodic measure on the space of functions from $G$ to $H$ ($\\textit{ergodic maps}$), and we show that, given an ergodic quasi-isometry, one can construct an explicit isomorphism from $H^*(H)$ to $H^*(G)$.   Specifically, when $\\psi$ is an ergodic quasi-isometry, the pullback $\\psi^*\\omega$ of a differential form $\\omega$ has a well-defined $\\textit{amenable average}$ $\\overline{\\psi^*}\\omega$, and we show that $\\overline{\\psi^*}$ is the desired isomorphism.","A key observation in our proof is that quasi-isometries of nilpotent groups are coarsely volume-preserving, so the amenable average of the pullback of the volume form is always nonzero."],"url":"http://arxiv.org/abs/2405.18598v1","category":"math.GR"}
{"created":"2024-05-28 21:19:15","title":"Causal inference in the closed-loop: marginal structural models for sequential excursion effects","abstract":"Optogenetics is widely used to study the effects of neural circuit manipulation on behavior. However, the paucity of causal inference methodological work on this topic has resulted in analysis conventions that discard information, and constrain the scientific questions that can be posed. To fill this gap, we introduce a nonparametric causal inference framework for analyzing \"closed-loop\" designs, which use dynamic policies that assign treatment based on covariates. In this setting, standard methods can introduce bias and occlude causal effects. Building on the sequentially randomized experiments literature in causal inference, our approach extends history-restricted marginal structural models for dynamic regimes. In practice, our framework can identify a wide range of causal effects of optogenetics on trial-by-trial behavior, such as, fast/slow-acting, dose-response, additive/antagonistic, and floor/ceiling. Importantly, it does so without requiring negative controls, and can estimate how causal effect magnitudes evolve across time points. From another view, our work extends \"excursion effect\" methods--popular in the mobile health literature--to enable estimation of causal contrasts for treatment sequences greater than length one, in the presence of positivity violations. We derive rigorous statistical guarantees, enabling hypothesis testing of these causal effects. We demonstrate our approach on data from a recent study of dopaminergic activity on learning, and show how our method reveals relevant effects obscured in standard analyses.","sentences":["Optogenetics is widely used to study the effects of neural circuit manipulation on behavior.","However, the paucity of causal inference methodological work on this topic has resulted in analysis conventions that discard information, and constrain the scientific questions that can be posed.","To fill this gap, we introduce a nonparametric causal inference framework for analyzing \"closed-loop\" designs, which use dynamic policies that assign treatment based on covariates.","In this setting, standard methods can introduce bias and occlude causal effects.","Building on the sequentially randomized experiments literature in causal inference, our approach extends history-restricted marginal structural models for dynamic regimes.","In practice, our framework can identify a wide range of causal effects of optogenetics on trial-by-trial behavior, such as, fast/slow-acting, dose-response, additive/antagonistic, and floor/ceiling.","Importantly, it does so without requiring negative controls, and can estimate how causal effect magnitudes evolve across time points.","From another view, our work extends \"excursion effect\" methods--popular in the mobile health literature--to enable estimation of causal contrasts for treatment sequences greater than length one, in the presence of positivity violations.","We derive rigorous statistical guarantees, enabling hypothesis testing of these causal effects.","We demonstrate our approach on data from a recent study of dopaminergic activity on learning, and show how our method reveals relevant effects obscured in standard analyses."],"url":"http://arxiv.org/abs/2405.18597v1","category":"stat.ME"}
{"created":"2024-05-28 21:08:58","title":"A Margin-based Multiclass Generalization Bound via Geometric Complexity","abstract":"There has been considerable effort to better understand the generalization capabilities of deep neural networks both as a means to unlock a theoretical understanding of their success as well as providing directions for further improvements. In this paper, we investigate margin-based multiclass generalization bounds for neural networks which rely on a recent complexity measure, the geometric complexity, developed for neural networks. We derive a new upper bound on the generalization error which scales with the margin-normalized geometric complexity of the network and which holds for a broad family of data distributions and model classes. Our generalization bound is empirically investigated for a ResNet-18 model trained with SGD on the CIFAR-10 and CIFAR-100 datasets with both original and random labels.","sentences":["There has been considerable effort to better understand the generalization capabilities of deep neural networks both as a means to unlock a theoretical understanding of their success as well as providing directions for further improvements.","In this paper, we investigate margin-based multiclass generalization bounds for neural networks which rely on a recent complexity measure, the geometric complexity, developed for neural networks.","We derive a new upper bound on the generalization error which scales with the margin-normalized geometric complexity of the network and which holds for a broad family of data distributions and model classes.","Our generalization bound is empirically investigated for a ResNet-18 model trained with SGD on the CIFAR-10 and CIFAR-100 datasets with both original and random labels."],"url":"http://arxiv.org/abs/2405.18590v1","category":"stat.ML"}
{"created":"2024-05-28 21:06:39","title":"A Verifiable Computing Scheme for Encrypted Control Systems","abstract":"The proliferation of cloud computing technologies has paved the way for deploying networked encrypted control systems, offering high performance, remote accessibility and privacy. However, in scenarios where the control algorithms run on third-party cloud service providers, the control logic might be changed by a malicious agent on the cloud. Consequently, it is imperative to verify the correctness of the control signals received from the cloud. Traditional verification methods, like zero-knowledge proof techniques, are computationally demanding in both proof generation and verification, may require several rounds of interactions between the prover and verifier and, consequently, are inapplicable in realtime control system applications. In this paper, we present a novel computationally inexpensive verifiable computing solution inspired by the probabilistic cut-and-choose approach. The proposed scheme allows the plant's actuator to validate the computations accomplished by the encrypted cloud-based networked controller without compromising the control scheme's performance. We showcase the effectiveness and real-time applicability of the proposed verifiable computation scheme using a remotely controlled Khepera IV differential-drive robot.","sentences":["The proliferation of cloud computing technologies has paved the way for deploying networked encrypted control systems, offering high performance, remote accessibility and privacy.","However, in scenarios where the control algorithms run on third-party cloud service providers, the control logic might be changed by a malicious agent on the cloud.","Consequently, it is imperative to verify the correctness of the control signals received from the cloud.","Traditional verification methods, like zero-knowledge proof techniques, are computationally demanding in both proof generation and verification, may require several rounds of interactions between the prover and verifier and, consequently, are inapplicable in realtime control system applications.","In this paper, we present a novel computationally inexpensive verifiable computing solution inspired by the probabilistic cut-and-choose approach.","The proposed scheme allows the plant's actuator to validate the computations accomplished by the encrypted cloud-based networked controller without compromising the control scheme's performance.","We showcase the effectiveness and real-time applicability of the proposed verifiable computation scheme using a remotely controlled Khepera IV differential-drive robot."],"url":"http://arxiv.org/abs/2405.18586v1","category":"eess.SY"}
{"created":"2024-05-28 21:03:15","title":"Identification of multi-component LOFAR sources with multi-modal deep learning","abstract":"Modern high-sensitivity radio telescopes are discovering an increased number of resolved sources with intricate radio structures and fainter radio emissions. These sources often present a challenge because source detectors might identify them as separate radio sources rather than components belonging to the same physically connected radio source. Currently, there are no reliable automatic methods to determine which radio components are single radio sources or part of multi-component sources. We propose a deep learning classifier to identify those sources that are part of a multi-component system and require component association on data from the LOFAR Two-Metre Sky Survey (LoTSS). We combine different types of input data using multi-modal deep learning to extract spatial and local information about the radio source components: a convolutional neural network component that processes radio images is combined with a neural network component that uses parameters measured from the radio sources and their nearest neighbours. Our model retrieves 94 per cent of the sources with multiple components on a balanced test set with 2,683 sources and achieves almost 97 per cent accuracy in the real imbalanced data (323,103 sources). The approach holds potential for integration into pipelines for automatic radio component association and cross-identification. Our work demonstrates how deep learning can be used to integrate different types of data and create an effective solution for managing modern radio surveys.","sentences":["Modern high-sensitivity radio telescopes are discovering an increased number of resolved sources with intricate radio structures and fainter radio emissions.","These sources often present a challenge because source detectors might identify them as separate radio sources rather than components belonging to the same physically connected radio source.","Currently, there are no reliable automatic methods to determine which radio components are single radio sources or part of multi-component sources.","We propose a deep learning classifier to identify those sources that are part of a multi-component system and require component association on data from the LOFAR Two-Metre Sky Survey (LoTSS).","We combine different types of input data using multi-modal deep learning to extract spatial and local information about the radio source components: a convolutional neural network component that processes radio images is combined with a neural network component that uses parameters measured from the radio sources and their nearest neighbours.","Our model retrieves 94 per cent of the sources with multiple components on a balanced test set with 2,683 sources and achieves almost 97 per cent accuracy in the real imbalanced data (323,103 sources).","The approach holds potential for integration into pipelines for automatic radio component association and cross-identification.","Our work demonstrates how deep learning can be used to integrate different types of data and create an effective solution for managing modern radio surveys."],"url":"http://arxiv.org/abs/2405.18584v1","category":"astro-ph.IM"}
{"created":"2024-05-28 19:56:53","title":"Scalable Surrogate Verification of Image-based Neural Network Control Systems using Composition and Unrolling","abstract":"Verifying safety of neural network control systems that use images as input is a difficult problem because, from a given system state, there is no known way to mathematically model what images are possible in the real-world. We build on recent work that considers a surrogate verification approach, training a conditional generative adversarial network (cGAN) as an image generator in place of the real world. This enables set-based formal analysis of the closed-loop system, providing analysis beyond simulation and testing. While existing work is effective on small examples, excessive overapproximation both within a single control period and across multiple control periods limits its scalability. We propose approaches to overcome these two sources of error. First, we overcome one-step error by composing the system's dynamics along with the cGAN and neural network controller, without losing the dependencies between input states and the control outputs as in the monotonic analysis of the system dynamics. Second, we reduce multi-step error by repeating the single-step composition, essentially unrolling multiple steps of the control loop into a large neural network. We then leverage existing network verification tools to compute accurate reachable sets for multiple steps, avoiding the accumulation of abstraction error at each step. We demonstrate the effectiveness of our approach in terms of both accuracy and scalability using two case studies: an autonomous aircraft taxiing system and an advanced emergency braking system. On the aircraft taxiing system, the converged reachable set is 175% larger using the prior baseline method compared with our proposed approach. On the emergency braking system, with 24x the number of image output variables from the cGAN, the baseline method fails to prove any states are safe, whereas our improvements enable set-based safety analysis.","sentences":["Verifying safety of neural network control systems that use images as input is a difficult problem because, from a given system state, there is no known way to mathematically model what images are possible in the real-world.","We build on recent work that considers a surrogate verification approach, training a conditional generative adversarial network (cGAN) as an image generator in place of the real world.","This enables set-based formal analysis of the closed-loop system, providing analysis beyond simulation and testing.","While existing work is effective on small examples, excessive overapproximation both within a single control period and across multiple control periods limits its scalability.","We propose approaches to overcome these two sources of error.","First, we overcome one-step error by composing the system's dynamics along with the cGAN and neural network controller, without losing the dependencies between input states and the control outputs as in the monotonic analysis of the system dynamics.","Second, we reduce multi-step error by repeating the single-step composition, essentially unrolling multiple steps of the control loop into a large neural network.","We then leverage existing network verification tools to compute accurate reachable sets for multiple steps, avoiding the accumulation of abstraction error at each step.","We demonstrate the effectiveness of our approach in terms of both accuracy and scalability using two case studies: an autonomous aircraft taxiing system and an advanced emergency braking system.","On the aircraft taxiing system, the converged reachable set is 175% larger using the prior baseline method compared with our proposed approach.","On the emergency braking system, with 24x the number of image output variables from the cGAN, the baseline method fails to prove any states are safe, whereas our improvements enable set-based safety analysis."],"url":"http://arxiv.org/abs/2405.18554v1","category":"cs.LG"}
{"created":"2024-05-28 19:39:09","title":"Unisolvence of unsymmetric random Kansa collocation by Gaussians and other analytic RBF vanishing at infinity","abstract":"We give a short proof of almost sure invertibility of unsymmetric random Kansa collocation matrices by a class of analytic RBF vanishing at infinity, for the Poisson equation with Dirichlet boundary conditions. Such a class includes popular Positive Definite instances such as Gaussians, Generalized Inverse MultiQuadrics and Matern RBF. The proof works on general domains in any dimension, with any distribution of boundary collocation points and any continuous random distribution of internal collocation points.","sentences":["We give a short proof of almost sure invertibility of unsymmetric random Kansa collocation matrices by a class of analytic RBF vanishing at infinity, for the Poisson equation with Dirichlet boundary conditions.","Such a class includes popular Positive Definite instances such as Gaussians, Generalized Inverse MultiQuadrics and Matern RBF.","The proof works on general domains in any dimension, with any distribution of boundary collocation points and any continuous random distribution of internal collocation points."],"url":"http://arxiv.org/abs/2405.18550v1","category":"math.NA"}
{"created":"2024-05-28 19:07:12","title":"Data-Driven Simulator for Mechanical Circulatory Support with Domain Adversarial Neural Process","abstract":"Mechanical Circulatory Support (MCS) devices, implemented as a probabilistic deep sequence model. Existing mechanical simulators for MCS rely on oversimplifying assumptions and are insensitive to patient-specific behavior, limiting their applicability to real-world treatment scenarios. To address these shortcomings, our model Domain Adversarial Neural Process (DANP) employs a neural process architecture, allowing it to capture the probabilistic relationship between MCS pump levels and aortic pressure measurements with uncertainty. We use domain adversarial training to combine simulation data with real-world observations, resulting in a more realistic and diverse representation of potential outcomes. Empirical results with an improvement of 19% in non-stationary trend prediction establish DANP as an effective tool for clinicians to understand and make informed decisions regarding MCS patient treatment.","sentences":["Mechanical Circulatory Support (MCS) devices, implemented as a probabilistic deep sequence model.","Existing mechanical simulators for MCS rely on oversimplifying assumptions and are insensitive to patient-specific behavior, limiting their applicability to real-world treatment scenarios.","To address these shortcomings, our model Domain Adversarial Neural Process (DANP) employs a neural process architecture, allowing it to capture the probabilistic relationship between MCS pump levels and aortic pressure measurements with uncertainty.","We use domain adversarial training to combine simulation data with real-world observations, resulting in a more realistic and diverse representation of potential outcomes.","Empirical results with an improvement of 19% in non-stationary trend prediction establish DANP as an effective tool for clinicians to understand and make informed decisions regarding MCS patient treatment."],"url":"http://arxiv.org/abs/2405.18536v1","category":"cs.LG"}
{"created":"2024-05-28 18:56:39","title":"Cardiovascular Disease Detection from Multi-View Chest X-rays with BI-Mamba","abstract":"Accurate prediction of Cardiovascular disease (CVD) risk in medical imaging is central to effective patient health management. Previous studies have demonstrated that imaging features in computed tomography (CT) can help predict CVD risk. However, CT entails notable radiation exposure, which may result in adverse health effects for patients. In contrast, chest X-ray emits significantly lower levels of radiation, offering a safer option. This rationale motivates our investigation into the feasibility of using chest X-ray for predicting CVD risk. Convolutional Neural Networks (CNNs) and Transformers are two established network architectures for computer-aided diagnosis. However, they struggle to model very high resolution chest X-ray due to the lack of large context modeling power or quadratic time complexity. Inspired by state space sequence models (SSMs), a new class of network architectures with competitive sequence modeling power as Transfomers and linear time complexity, we propose Bidirectional Image Mamba (BI-Mamba) to complement the unidirectional SSMs with opposite directional information. BI-Mamba utilizes parallel forward and backwark blocks to encode longe-range dependencies of multi-view chest X-rays. We conduct extensive experiments on images from 10,395 subjects in National Lung Screening Trail (NLST). Results show that BI-Mamba outperforms ResNet-50 and ViT-S with comparable parameter size, and saves significant amount of GPU memory during training. Besides, BI-Mamba achieves promising performance compared with previous state of the art in CT, unraveling the potential of chest X-ray for CVD risk prediction.","sentences":["Accurate prediction of Cardiovascular disease (CVD) risk in medical imaging is central to effective patient health management.","Previous studies have demonstrated that imaging features in computed tomography (CT) can help predict CVD risk.","However, CT entails notable radiation exposure, which may result in adverse health effects for patients.","In contrast, chest X-ray emits significantly lower levels of radiation, offering a safer option.","This rationale motivates our investigation into the feasibility of using chest X-ray for predicting CVD risk.","Convolutional Neural Networks (CNNs) and Transformers are two established network architectures for computer-aided diagnosis.","However, they struggle to model very high resolution chest X-ray due to the lack of large context modeling power or quadratic time complexity.","Inspired by state space sequence models (SSMs), a new class of network architectures with competitive sequence modeling power as Transfomers and linear time complexity, we propose Bidirectional Image Mamba (BI-Mamba) to complement the unidirectional SSMs with opposite directional information.","BI-Mamba utilizes parallel forward and backwark blocks to encode longe-range dependencies of multi-view chest X-rays.","We conduct extensive experiments on images from 10,395 subjects in National Lung Screening Trail (NLST).","Results show that BI-Mamba outperforms ResNet-50 and ViT-S with comparable parameter size, and saves significant amount of GPU memory during training.","Besides, BI-Mamba achieves promising performance compared with previous state of the art in CT, unraveling the potential of chest X-ray for CVD risk prediction."],"url":"http://arxiv.org/abs/2405.18533v1","category":"eess.IV"}
{"created":"2024-05-28 18:45:10","title":"REPARO: Compositional 3D Assets Generation with Differentiable 3D Layout Alignment","abstract":"Traditional image-to-3D models often struggle with scenes containing multiple objects due to biases and occlusion complexities. To address this challenge, we present REPARO, a novel approach for compositional 3D asset generation from single images. REPARO employs a two-step process: first, it extracts individual objects from the scene and reconstructs their 3D meshes using off-the-shelf image-to-3D models; then, it optimizes the layout of these meshes through differentiable rendering techniques, ensuring coherent scene composition. By integrating optimal transport-based long-range appearance loss term and high-level semantic loss term in the differentiable rendering, REPARO can effectively recover the layout of 3D assets. The proposed method can significantly enhance object independence, detail accuracy, and overall scene coherence. Extensive evaluation of multi-object scenes demonstrates that our REPARO offers a comprehensive approach to address the complexities of multi-object 3D scene generation from single images.","sentences":["Traditional image-to-3D models often struggle with scenes containing multiple objects due to biases and occlusion complexities.","To address this challenge, we present REPARO, a novel approach for compositional 3D asset generation from single images.","REPARO employs a two-step process:","first, it extracts individual objects from the scene and reconstructs their 3D meshes using off-the-shelf image-to-3D models; then, it optimizes the layout of these meshes through differentiable rendering techniques, ensuring coherent scene composition.","By integrating optimal transport-based long-range appearance loss term and high-level semantic loss term in the differentiable rendering, REPARO can effectively recover the layout of 3D assets.","The proposed method can significantly enhance object independence, detail accuracy, and overall scene coherence.","Extensive evaluation of multi-object scenes demonstrates that our REPARO offers a comprehensive approach to address the complexities of multi-object 3D scene generation from single images."],"url":"http://arxiv.org/abs/2405.18525v1","category":"cs.CV"}
{"created":"2024-05-28 18:44:42","title":"Aligning in a Compact Space: Contrastive Knowledge Distillation between Heterogeneous Architectures","abstract":"Knowledge distillation is commonly employed to compress neural networks, reducing the inference costs and memory footprint. In the scenario of homogenous architecture, feature-based methods have been widely validated for their effectiveness. However, in scenarios where the teacher and student models are of heterogeneous architectures, the inherent differences in feature representation significantly degrade the performance of these methods. Recent studies have highlighted that low-frequency components constitute the majority of image features. Motivated by this, we propose a Low-Frequency Components-based Contrastive Knowledge Distillation (LFCC) framework that significantly enhances the performance of feature-based distillation between heterogeneous architectures. Specifically, we designe a set of multi-scale low-pass filters to extract the low-frequency components of intermediate features from both the teacher and student models, aligning them in a compact space to overcome architectural disparities. Moreover, leveraging the intrinsic pairing characteristic of the teacher-student framework, we design an innovative sample-level contrastive learning framework that adeptly restructures the constraints of within-sample feature similarity and between-sample feature divergence into a contrastive learning task. This strategy enables the student model to capitalize on intra-sample feature congruence while simultaneously enhancing the discrimination of features among disparate samples. Consequently, our LFCC framework accurately captures the commonalities in feature representation across heterogeneous architectures. Extensive evaluations and empirical analyses across three architectures (CNNs, Transformers, and MLPs) demonstrate that LFCC achieves superior performance on the challenging benchmarks of ImageNet-1K and CIFAR-100. All codes will be publicly available.","sentences":["Knowledge distillation is commonly employed to compress neural networks, reducing the inference costs and memory footprint.","In the scenario of homogenous architecture, feature-based methods have been widely validated for their effectiveness.","However, in scenarios where the teacher and student models are of heterogeneous architectures, the inherent differences in feature representation significantly degrade the performance of these methods.","Recent studies have highlighted that low-frequency components constitute the majority of image features.","Motivated by this, we propose a Low-Frequency Components-based Contrastive Knowledge Distillation (LFCC) framework that significantly enhances the performance of feature-based distillation between heterogeneous architectures.","Specifically, we designe a set of multi-scale low-pass filters to extract the low-frequency components of intermediate features from both the teacher and student models, aligning them in a compact space to overcome architectural disparities.","Moreover, leveraging the intrinsic pairing characteristic of the teacher-student framework, we design an innovative sample-level contrastive learning framework that adeptly restructures the constraints of within-sample feature similarity and between-sample feature divergence into a contrastive learning task.","This strategy enables the student model to capitalize on intra-sample feature congruence while simultaneously enhancing the discrimination of features among disparate samples.","Consequently, our LFCC framework accurately captures the commonalities in feature representation across heterogeneous architectures.","Extensive evaluations and empirical analyses across three architectures (CNNs, Transformers, and MLPs) demonstrate that LFCC achieves superior performance on the challenging benchmarks of ImageNet-1K and CIFAR-100.","All codes will be publicly available."],"url":"http://arxiv.org/abs/2405.18524v1","category":"cs.CV"}
{"created":"2024-05-28 18:33:18","title":"Atlas3D: Physically Constrained Self-Supporting Text-to-3D for Simulation and Fabrication","abstract":"Existing diffusion-based text-to-3D generation methods primarily focus on producing visually realistic shapes and appearances, often neglecting the physical constraints necessary for downstream tasks. Generated models frequently fail to maintain balance when placed in physics-based simulations or 3D printed. This balance is crucial for satisfying user design intentions in interactive gaming, embodied AI, and robotics, where stable models are needed for reliable interaction. Additionally, stable models ensure that 3D-printed objects, such as figurines for home decoration, can stand on their own without requiring additional supports. To fill this gap, we introduce Atlas3D, an automatic and easy-to-implement method that enhances existing Score Distillation Sampling (SDS)-based text-to-3D tools. Atlas3D ensures the generation of self-supporting 3D models that adhere to physical laws of stability under gravity, contact, and friction. Our approach combines a novel differentiable simulation-based loss function with physically inspired regularization, serving as either a refinement or a post-processing module for existing frameworks. We verify Atlas3D's efficacy through extensive generation tasks and validate the resulting 3D models in both simulated and real-world environments.","sentences":["Existing diffusion-based text-to-3D generation methods primarily focus on producing visually realistic shapes and appearances, often neglecting the physical constraints necessary for downstream tasks.","Generated models frequently fail to maintain balance when placed in physics-based simulations or 3D printed.","This balance is crucial for satisfying user design intentions in interactive gaming, embodied AI, and robotics, where stable models are needed for reliable interaction.","Additionally, stable models ensure that 3D-printed objects, such as figurines for home decoration, can stand on their own without requiring additional supports.","To fill this gap, we introduce Atlas3D, an automatic and easy-to-implement method that enhances existing Score Distillation Sampling (SDS)-based text-to-3D tools.","Atlas3D ensures the generation of self-supporting 3D models that adhere to physical laws of stability under gravity, contact, and friction.","Our approach combines a novel differentiable simulation-based loss function with physically inspired regularization, serving as either a refinement or a post-processing module for existing frameworks.","We verify Atlas3D's efficacy through extensive generation tasks and validate the resulting 3D models in both simulated and real-world environments."],"url":"http://arxiv.org/abs/2405.18515v1","category":"cs.LG"}
{"created":"2024-05-28 18:10:45","title":"Large Margin Discriminative Loss for Classification","abstract":"In this paper, we introduce a novel discriminative loss function with large margin in the context of Deep Learning. This loss boosts the discriminative power of neural nets, represented by intra-class compactness and inter-class separability. On the one hand, the class compactness is ensured by close distance of samples of the same class to each other. On the other hand, the inter-class separability is boosted by a margin loss that ensures the minimum distance of each class to its closest boundary. All the terms in our loss have an explicit meaning, giving a direct view of the feature space obtained. We analyze mathematically the relation between compactness and margin term, giving a guideline about the impact of the hyper-parameters on the learned features. Moreover, we also analyze properties of the gradient of the loss with respect to the parameters of the neural net. Based on this, we design a strategy called partial momentum updating that enjoys simultaneously stability and consistency in training. Furthermore, we also investigate generalization errors to have better theoretical insights. Our loss function systematically boosts the test accuracy of models compared to the standard softmax loss in our experiments.","sentences":["In this paper, we introduce a novel discriminative loss function with large margin in the context of Deep Learning.","This loss boosts the discriminative power of neural nets, represented by intra-class compactness and inter-class separability.","On the one hand, the class compactness is ensured by close distance of samples of the same class to each other.","On the other hand, the inter-class separability is boosted by a margin loss that ensures the minimum distance of each class to its closest boundary.","All the terms in our loss have an explicit meaning, giving a direct view of the feature space obtained.","We analyze mathematically the relation between compactness and margin term, giving a guideline about the impact of the hyper-parameters on the learned features.","Moreover, we also analyze properties of the gradient of the loss with respect to the parameters of the neural net.","Based on this, we design a strategy called partial momentum updating that enjoys simultaneously stability and consistency in training.","Furthermore, we also investigate generalization errors to have better theoretical insights.","Our loss function systematically boosts the test accuracy of models compared to the standard softmax loss in our experiments."],"url":"http://arxiv.org/abs/2405.18499v1","category":"stat.ML"}
{"created":"2024-05-28 18:07:44","title":"Black hole-neutron star mergers in Einstein-scalar-Gauss-Bonnet gravity","abstract":"Gravitational wave observations of black hole-neutron star binaries, particularly those where the black hole has a lower mass compared to other observed systems, have the potential to place strong constraints on modifications to general relativity that arise at small curvature length scales. Here we study the dynamics of black hole-neutron star mergers in shift-symmetric Einstein-scalar-Gauss-Bonnet gravity, a representative example of such a theory, by numerically evolving the full equations of motion. We consider quasi-circular binaries with different mass-ratios that are consistent with recent gravitational wave observations, including cases with and without tidal disruption of the star, and quantify the impact of varying the coupling controlling deviations from general relativity on the gravitational wave signal and scalar radiation. We find that the main effect on the late inspiral is the accelerated frequency evolution compared to general relativity, and that--even considering Gauss-Bonnet coupling values approaching those where the theory breaks down--the impact on the merger gravitational wave signal is mild, predominately manifesting as a small change in the amplitude of the ringdown. We compare our results to current post-Newtonian calculations and find consistency throughout the inspiral.","sentences":["Gravitational wave observations of black hole-neutron star binaries, particularly those where the black hole has a lower mass compared to other observed systems, have the potential to place strong constraints on modifications to general relativity that arise at small curvature length scales.","Here we study the dynamics of black hole-neutron star mergers in shift-symmetric Einstein-scalar-Gauss-Bonnet gravity, a representative example of such a theory, by numerically evolving the full equations of motion.","We consider quasi-circular binaries with different mass-ratios that are consistent with recent gravitational wave observations, including cases with and without tidal disruption of the star, and quantify the impact of varying the coupling controlling deviations from general relativity on the gravitational wave signal and scalar radiation.","We find that the main effect on the late inspiral is the accelerated frequency evolution compared to general relativity, and that--even considering Gauss-Bonnet coupling values approaching those where the theory breaks down--the impact on the merger gravitational wave signal is mild, predominately manifesting as a small change in the amplitude of the ringdown.","We compare our results to current post-Newtonian calculations and find consistency throughout the inspiral."],"url":"http://arxiv.org/abs/2405.18496v1","category":"gr-qc"}
{"created":"2024-05-28 18:04:33","title":"Constraining the primordial black hole abundance through Big-Bang nucleosynthesis","abstract":"We investigate the scenario in which primordial black holes (PBHs) with masses Mpbh < 10^9 g undergo Hawking evaporation, around the Big-Bang nucleosynthesis (BBN) epoch. The evaporation process modifies the Universe's expansion rate and the baryon-to-photon ratio, leading to an alteration of the primordial abundance of light nuclei. We present numerical solutions for the set of equations describing this physics, considering different values of PBH masses and abundances at their formation, showing how their evaporation impacts the abundances of light nuclei, obtained by incorporating the non-standard Hubble rate and baryon-to-photon ratio into the BBN code PArthENoPE. The results are then used to place upper bounds for the PBH relative abundance at formation in the range 10^8 g < Mpbh < 10^9 g, providing the strongest constraints existing to-date in this mass range.","sentences":["We investigate the scenario in which primordial black holes (PBHs) with masses Mpbh < 10^9 g undergo Hawking evaporation, around the Big-Bang nucleosynthesis (BBN) epoch.","The evaporation process modifies the Universe's expansion rate and the baryon-to-photon ratio, leading to an alteration of the primordial abundance of light nuclei.","We present numerical solutions for the set of equations describing this physics, considering different values of PBH masses and abundances at their formation, showing how their evaporation impacts the abundances of light nuclei, obtained by incorporating the non-standard Hubble rate and baryon-to-photon ratio into the BBN code PArthENoPE.","The results are then used to place upper bounds for the PBH relative abundance at formation in the range 10^8 g <","Mpbh < 10^9 g, providing the strongest constraints existing to-date in this mass range."],"url":"http://arxiv.org/abs/2405.18493v1","category":"astro-ph.CO"}
{"created":"2024-05-28 18:00:32","title":"Predicting Ground State Properties: Constant Sample Complexity and Deep Learning Algorithms","abstract":"A fundamental problem in quantum many-body physics is that of finding ground states of local Hamiltonians. A number of recent works gave provably efficient machine learning (ML) algorithms for learning ground states. Specifically, [Huang et al. Science 2022], introduced an approach for learning properties of the ground state of an $n$-qubit gapped local Hamiltonian $H$ from only $n^{\\mathcal{O}(1)}$ data points sampled from Hamiltonians in the same phase of matter. This was subsequently improved by [Lewis et al. Nature Communications 2024], to $\\mathcal{O}(\\log n)$ samples when the geometry of the $n$-qubit system is known. In this work, we introduce two approaches that achieve a constant sample complexity, independent of system size $n$, for learning ground state properties. Our first algorithm consists of a simple modification of the ML model used by Lewis et al. and applies to a property of interest known beforehand. Our second algorithm, which applies even if a description of the property is not known, is a deep neural network model. While empirical results showing the performance of neural networks have been demonstrated, to our knowledge, this is the first rigorous sample complexity bound on a neural network model for predicting ground state properties. We also perform numerical experiments that confirm the improved scaling of our approach compared to earlier results.","sentences":["A fundamental problem in quantum many-body physics is that of finding ground states of local Hamiltonians.","A number of recent works gave provably efficient machine learning (ML) algorithms for learning ground states.","Specifically, [Huang et al.","Science 2022], introduced an approach for learning properties of the ground state of an $n$-qubit gapped local Hamiltonian $H$ from only $n^{\\mathcal{O}(1)}$ data points sampled from Hamiltonians in the same phase of matter.","This was subsequently improved by [Lewis et al.","Nature Communications 2024], to $\\mathcal{O}(\\log n)$ samples when the geometry of the $n$-qubit system is known.","In this work, we introduce two approaches that achieve a constant sample complexity, independent of system size $n$, for learning ground state properties.","Our first algorithm consists of a simple modification of the ML model used by Lewis et al. and applies to a property of interest known beforehand.","Our second algorithm, which applies even if a description of the property is not known, is a deep neural network model.","While empirical results showing the performance of neural networks have been demonstrated, to our knowledge, this is the first rigorous sample complexity bound on a neural network model for predicting ground state properties.","We also perform numerical experiments that confirm the improved scaling of our approach compared to earlier results."],"url":"http://arxiv.org/abs/2405.18489v1","category":"quant-ph"}
{"created":"2024-05-28 18:00:09","title":"Emergent Time in Hamiltonian General Relativity","abstract":"In this paper we introduce a definition of time that emerges in terms of the geometry of the configuration space of a dynamical system. We illustrate this, using the Hamilton-Jacobi equation, in various examples: particle mechanics on a fixed energy surface; non-Abelian gauge theories for compact semi-simple Lie groups where the Gauss law presents new features; and General Relativity in $d+1$ dimensions with $d$ the dimension of space. The discussion in General Relativity is like the non-abelian gauge theory case except for the indefiniteness of the de Witt metric in the Einstein-Hamilton-Jacobi equation, which we discuss in some detail. We illustrate the general formula for the emergent time in various examples including de Sitter spacetime and asymptotically AdS spacetimes.","sentences":["In this paper we introduce a definition of time that emerges in terms of the geometry of the configuration space of a dynamical system.","We illustrate this, using the Hamilton-Jacobi equation, in various examples: particle mechanics on a fixed energy surface; non-Abelian gauge theories for compact semi-simple Lie groups where the Gauss law presents new features; and General Relativity in $d+1$ dimensions with $d$ the dimension of space.","The discussion in General Relativity is like the non-abelian gauge theory case except for the indefiniteness of the de Witt metric in the Einstein-Hamilton-Jacobi equation, which we discuss in some detail.","We illustrate the general formula for the emergent time in various examples including de Sitter spacetime and asymptotically AdS spacetimes."],"url":"http://arxiv.org/abs/2405.18486v1","category":"hep-th"}
{"created":"2024-05-28 18:00:00","title":"Dust dynamics in RAMSES -- II. Equilibrium drift velocity distributions of charged dust grains","abstract":"We investigate the gas-grain relative drift velocity distributions of charged astrophysical dust grains in MHD turbulence. We do this using a range of MHD-PIC simulations spanning different plasma-$\\beta$, sonic/Alfv\\'en Mach number, and with grains of varying size and charge-to-mass ratio. We find that the root-mean-square drift velocity is a strong function of the grain size, following a power law with a 1/2 slope. The r.m.s. value has only a very weak dependence on the charge-to-mass ratio. On the other hand, the shape of the distribution is a strong function of the grain charge-to-mass ratio, and in compressible turbulence, also the grain size. We then compare these results to simple analytic models based upon time-domain quasi-linear theory and solutions to the Fokker-Planck equation. These models explain qualitatively the r.m.s. drift velocity's lack of charge-to-mass ratio dependence, as well as why the shape of the distribution changes as the charge-to-mass ratio increases. Finally we scale our results to astrophysical conditions. As an example, at a length scale of one parsec in the cold neutral medium, 0.1 $\\mu$m grains should be drifting at roughly 40% of the turbulent velocity dispersion. These findings may serve as a basis for a model for grain velocities in the context of grain-grain collisions, non-thermal sputtering, and accretion of metals. These findings also have implications for the transport of grains through the galaxy, suggesting that grains may have non-negligible random motions at length-scales that many modern galaxy simulations approach.","sentences":["We investigate the gas-grain relative drift velocity distributions of charged astrophysical dust grains in MHD turbulence.","We do this using a range of MHD-PIC simulations spanning different plasma-$\\beta$, sonic/Alfv\\'en","Mach number, and with grains of varying size and charge-to-mass ratio.","We find that the root-mean-square drift velocity is a strong function of the grain size, following a power law with a 1/2 slope.","The r.m.s. value has only a very weak dependence on the charge-to-mass ratio.","On the other hand, the shape of the distribution is a strong function of the grain charge-to-mass ratio, and in compressible turbulence, also the grain size.","We then compare these results to simple analytic models based upon time-domain quasi-linear theory and solutions to the Fokker-Planck equation.","These models explain qualitatively the r.m.s.","drift velocity's lack of charge-to-mass ratio dependence, as well as why the shape of the distribution changes as the charge-to-mass ratio increases.","Finally we scale our results to astrophysical conditions.","As an example, at a length scale of one parsec in the cold neutral medium, 0.1 $\\mu$m grains should be drifting at roughly 40% of the turbulent velocity dispersion.","These findings may serve as a basis for a model for grain velocities in the context of grain-grain collisions, non-thermal sputtering, and accretion of metals.","These findings also have implications for the transport of grains through the galaxy, suggesting that grains may have non-negligible random motions at length-scales that many modern galaxy simulations approach."],"url":"http://arxiv.org/abs/2405.18463v1","category":"astro-ph.GA"}
{"created":"2024-05-28 17:27:20","title":"Asymmetrical estimator for training grey-box deep photonic neural networks","abstract":"Physical neural networks (PNNs) are emerging paradigms for neural network acceleration due to their high-bandwidth, in-propagation analogue processing. Despite the advantages of PNN for inference, training remains a challenge. The imperfect information of the physical transformation means the failure of conventional gradient-based updates from backpropagation (BP). Here, we present the asymmetrical training (AT) method, which treats the PNN structure as a grey box. AT performs training while only knowing the last layer output and neuron topological connectivity of a deep neural network structure, not requiring information about the physical control-transformation mapping. We experimentally demonstrated the AT method on deep grey-box PNNs implemented by uncalibrated photonic integrated circuits (PICs), improving the classification accuracy of Iris flower and modified MNIST hand-written digits from random guessing to near theoretical maximum. We also showcased the consistently enhanced performance of AT over BP for different datasets, including MNIST, fashion-MNIST, and Kuzushiji-MNIST. The AT method demonstrated successful training with minimal hardware overhead and reduced computational overhead, serving as a robust light-weight training alternative to fully explore the advantages of physical computation.","sentences":["Physical neural networks (PNNs) are emerging paradigms for neural network acceleration due to their high-bandwidth, in-propagation analogue processing.","Despite the advantages of PNN for inference, training remains a challenge.","The imperfect information of the physical transformation means the failure of conventional gradient-based updates from backpropagation (BP).","Here, we present the asymmetrical training (AT) method, which treats the PNN structure as a grey box.","AT performs training while only knowing the last layer output and neuron topological connectivity of a deep neural network structure, not requiring information about the physical control-transformation mapping.","We experimentally demonstrated the AT method on deep grey-box PNNs implemented by uncalibrated photonic integrated circuits (PICs), improving the classification accuracy of Iris flower and modified MNIST hand-written digits from random guessing to near theoretical maximum.","We also showcased the consistently enhanced performance of AT over BP for different datasets, including MNIST, fashion-MNIST, and Kuzushiji-MNIST.","The AT method demonstrated successful training with minimal hardware overhead and reduced computational overhead, serving as a robust light-weight training alternative to fully explore the advantages of physical computation."],"url":"http://arxiv.org/abs/2405.18458v1","category":"cs.LG"}
{"created":"2024-05-29 16:28:12","title":"Comparative Study of Neighbor-based Methods for Local Outlier Detection","abstract":"The neighbor-based method has become a powerful tool to handle the outlier detection problem, which aims to infer the abnormal degree of the sample based on the compactness of the sample and its neighbors. However, the existing methods commonly focus on designing different processes to locate outliers in the dataset, while the contributions of different types neighbors to outlier detection has not been well discussed. To this end, this paper studies the neighbor in the existing outlier detection algorithms and a taxonomy is introduced, which uses the three-level components of information, neighbor and methodology to define hybrid methods. This taxonomy can serve as a paradigm where a novel neighbor-based outlier detection method can be proposed by combining different components in this taxonomy. A large number of comparative experiments were conducted on synthetic and real-world datasets in terms of performance comparison and case study, and the results show that reverse K-nearest neighbor based methods achieve promising performance and dynamic selection method is suitable for working in high-dimensional space. Notably, it is verified that rationally selecting components from this taxonomy may create an algorithms superior to existing methods.","sentences":["The neighbor-based method has become a powerful tool to handle the outlier detection problem, which aims to infer the abnormal degree of the sample based on the compactness of the sample and its neighbors.","However, the existing methods commonly focus on designing different processes to locate outliers in the dataset, while the contributions of different types neighbors to outlier detection has not been well discussed.","To this end, this paper studies the neighbor in the existing outlier detection algorithms and a taxonomy is introduced, which uses the three-level components of information, neighbor and methodology to define hybrid methods.","This taxonomy can serve as a paradigm where a novel neighbor-based outlier detection method can be proposed by combining different components in this taxonomy.","A large number of comparative experiments were conducted on synthetic and real-world datasets in terms of performance comparison and case study, and the results show that reverse K-nearest neighbor based methods achieve promising performance and dynamic selection method is suitable for working in high-dimensional space.","Notably, it is verified that rationally selecting components from this taxonomy may create an algorithms superior to existing methods."],"url":"http://arxiv.org/abs/2405.19247v1","category":"cs.LG"}
{"created":"2024-05-29 16:05:57","title":"Synthetic Potential Outcomes for Mixtures of Treatment Effects","abstract":"Modern data analysis frequently relies on the use of large datasets, often constructed as amalgamations of diverse populations or data-sources. Heterogeneity across these smaller datasets constitutes two major challenges for causal inference: (1) the source of each sample can introduce latent confounding between treatment and effect, and (2) diverse populations may respond differently to the same treatment, giving rise to heterogeneous treatment effects (HTEs). The issues of latent confounding and HTEs have been studied separately but not in conjunction. In particular, previous works only report the conditional average treatment effect (CATE) among similar individuals (with respect to the measured covariates). CATEs cannot resolve mixtures of potential treatment effects driven by latent heterogeneity, which we call mixtures of treatment effects (MTEs). Inspired by method of moment approaches to mixture models, we propose \"synthetic potential outcomes\" (SPOs). Our new approach deconfounds heterogeneity while also guaranteeing the identifiability of MTEs. This technique bypasses full recovery of a mixture, which significantly simplifies its requirements for identifiability. We demonstrate the efficacy of SPOs on synthetic data.","sentences":["Modern data analysis frequently relies on the use of large datasets, often constructed as amalgamations of diverse populations or data-sources.","Heterogeneity across these smaller datasets constitutes two major challenges for causal inference: (1) the source of each sample can introduce latent confounding between treatment and effect, and (2) diverse populations may respond differently to the same treatment, giving rise to heterogeneous treatment effects (HTEs).","The issues of latent confounding and HTEs have been studied separately but not in conjunction.","In particular, previous works only report the conditional average treatment effect (CATE) among similar individuals (with respect to the measured covariates).","CATEs cannot resolve mixtures of potential treatment effects driven by latent heterogeneity, which we call mixtures of treatment effects (MTEs).","Inspired by method of moment approaches to mixture models, we propose \"synthetic potential outcomes\" (SPOs).","Our new approach deconfounds heterogeneity while also guaranteeing the identifiability of MTEs.","This technique bypasses full recovery of a mixture, which significantly simplifies its requirements for identifiability.","We demonstrate the efficacy of SPOs on synthetic data."],"url":"http://arxiv.org/abs/2405.19225v1","category":"cs.LG"}
{"created":"2024-05-29 15:53:23","title":"Gone but Not Forgotten: Improved Benchmarks for Machine Unlearning","abstract":"Machine learning models are vulnerable to adversarial attacks, including attacks that leak information about the model's training data. There has recently been an increase in interest about how to best address privacy concerns, especially in the presence of data-removal requests. Machine unlearning algorithms aim to efficiently update trained models to comply with data deletion requests while maintaining performance and without having to resort to retraining the model from scratch, a costly endeavor. Several algorithms in the machine unlearning literature demonstrate some level of privacy gains, but they are often evaluated only on rudimentary membership inference attacks, which do not represent realistic threats. In this paper we describe and propose alternative evaluation methods for three key shortcomings in the current evaluation of unlearning algorithms. We show the utility of our alternative evaluations via a series of experiments of state-of-the-art unlearning algorithms on different computer vision datasets, presenting a more detailed picture of the state of the field.","sentences":["Machine learning models are vulnerable to adversarial attacks, including attacks that leak information about the model's training data.","There has recently been an increase in interest about how to best address privacy concerns, especially in the presence of data-removal requests.","Machine unlearning algorithms aim to efficiently update trained models to comply with data deletion requests while maintaining performance and without having to resort to retraining the model from scratch, a costly endeavor.","Several algorithms in the machine unlearning literature demonstrate some level of privacy gains, but they are often evaluated only on rudimentary membership inference attacks, which do not represent realistic threats.","In this paper we describe and propose alternative evaluation methods for three key shortcomings in the current evaluation of unlearning algorithms.","We show the utility of our alternative evaluations via a series of experiments of state-of-the-art unlearning algorithms on different computer vision datasets, presenting a more detailed picture of the state of the field."],"url":"http://arxiv.org/abs/2405.19211v1","category":"cs.LG"}
{"created":"2024-05-29 15:17:53","title":"Online Linear Regression in Dynamic Environments via Discounting","abstract":"We develop algorithms for online linear regression which achieve optimal static and dynamic regret guarantees \\emph{even in the complete absence of prior knowledge}. We present a novel analysis showing that a discounted variant of the Vovk-Azoury-Warmuth forecaster achieves dynamic regret of the form $R_{T}(\\vec{u})\\le O\\left(d\\log(T)\\vee \\sqrt{dP_{T}^{\\gamma}(\\vec{u})T}\\right)$, where $P_{T}^{\\gamma}(\\vec{u})$ is a measure of variability of the comparator sequence, and show that the discount factor achieving this result can be learned on-the-fly. We show that this result is optimal by providing a matching lower bound. We also extend our results to \\emph{strongly-adaptive} guarantees which hold over every sub-interval $[a,b]\\subseteq[1,T]$ simultaneously.","sentences":["We develop algorithms for online linear regression which achieve optimal static and dynamic regret guarantees \\emph{even in the complete absence of prior knowledge}.","We present a novel analysis showing that a discounted variant of the Vovk-Azoury-Warmuth forecaster achieves dynamic regret of the form $R_{T}(\\vec{u})\\le O\\left(d\\log(T)\\vee \\sqrt{dP_{T}^{\\gamma}(\\vec{u})T}\\right)$, where $P_{T}^{\\gamma}(\\vec{u})$ is a measure of variability of the comparator sequence, and show that the discount factor achieving this result can be learned on-the-fly.","We show that this result is optimal by providing a matching lower bound.","We also extend our results to \\emph{strongly-adaptive} guarantees which hold over every sub-interval $[a,b]\\subseteq[1,T]$ simultaneously."],"url":"http://arxiv.org/abs/2405.19175v1","category":"cs.LG"}
{"created":"2024-05-29 14:51:41","title":"I Bet You Did Not Mean That: Testing Semantic Importance via Betting","abstract":"Recent works have extended notions of feature importance to \\emph{semantic concepts} that are inherently interpretable to the users interacting with a black-box predictive model. Yet, precise statistical guarantees, such as false positive rate control, are needed to communicate findings transparently and to avoid unintended consequences in real-world scenarios. In this paper, we formalize the global (i.e., over a population) and local (i.e., for a sample) statistical importance of semantic concepts for the predictions of opaque models, by means of conditional independence, which allows for rigorous testing. We use recent ideas of sequential kernelized testing (SKIT) to induce a rank of importance across concepts, and showcase the effectiveness and flexibility of our framework on synthetic datasets as well as on image classification tasks using vision-language models such as CLIP.","sentences":["Recent works have extended notions of feature importance to \\emph{semantic concepts} that are inherently interpretable to the users interacting with a black-box predictive model.","Yet, precise statistical guarantees, such as false positive rate control, are needed to communicate findings transparently and to avoid unintended consequences in real-world scenarios.","In this paper, we formalize the global (i.e., over a population) and local (i.e., for a sample) statistical importance of semantic concepts for the predictions of opaque models, by means of conditional independence, which allows for rigorous testing.","We use recent ideas of sequential kernelized testing (SKIT) to induce a rank of importance across concepts, and showcase the effectiveness and flexibility of our framework on synthetic datasets as well as on image classification tasks using vision-language models such as CLIP."],"url":"http://arxiv.org/abs/2405.19146v1","category":"stat.ML"}
{"created":"2024-05-29 13:00:10","title":"Robust Entropy Search for Safe Efficient Bayesian Optimization","abstract":"The practical use of Bayesian Optimization (BO) in engineering applications imposes special requirements: high sampling efficiency on the one hand and finding a robust solution on the other hand. We address the case of adversarial robustness, where all parameters are controllable during the optimization process, but a subset of them is uncontrollable or even adversely perturbed at the time of application. To this end, we develop an efficient information-based acquisition function that we call Robust Entropy Search (RES). We empirically demonstrate its benefits in experiments on synthetic and real-life data. The results showthat RES reliably finds robust optima, outperforming state-of-the-art algorithms.","sentences":["The practical use of Bayesian Optimization (BO) in engineering applications imposes special requirements: high sampling efficiency on the one hand and finding a robust solution on the other hand.","We address the case of adversarial robustness, where all parameters are controllable during the optimization process, but a subset of them is uncontrollable or even adversely perturbed at the time of application.","To this end, we develop an efficient information-based acquisition function that we call Robust Entropy Search (RES).","We empirically demonstrate its benefits in experiments on synthetic and real-life data.","The results showthat RES reliably finds robust optima, outperforming state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2405.19059v1","category":"cs.LG"}
{"created":"2024-05-29 12:24:34","title":"PointNetPGAP-SLC: A 3D LiDAR-based Place Recognition Approach with Segment-level Consistency Training for Mobile Robots in Horticulture","abstract":"This paper addresses robotic place recognition in horticultural environments using 3D-LiDAR technology and deep learning. Three main contributions are proposed: (i) a novel model called PointNetPGAP, which combines a global average pooling aggregator and a pairwise feature interaction aggregator; (ii) a Segment-Level Consistency (SLC) model, used only during training, with the goal of augmenting the contrastive loss with a context-specific training signal to enhance descriptors; and (iii) a novel dataset named HORTO-3DLM featuring sequences from orchards and strawberry plantations. The experimental evaluation, conducted on the new HORTO-3DLM dataset, compares PointNetPGAP at the sequence- and segment-level with state-of-the-art (SOTA) models, including OverlapTransformer, PointNetVLAD, and LOGG3D. Additionally, all models were trained and evaluated using the SLC. Empirical results obtained through a cross-validation evaluation protocol demonstrate the superiority of PointNetPGAP compared to existing SOTA models. PointNetPGAP emerges as the best model in retrieving the top-1 candidate, outperforming PointNetVLAD (the second-best model). Moreover, when comparing the impact of training with the SLC model, performance increased on four out of the five evaluated models, indicating that adding a context-specific signal to the contrastive loss leads to improved descriptors.","sentences":["This paper addresses robotic place recognition in horticultural environments using 3D-LiDAR technology and deep learning.","Three main contributions are proposed: (i) a novel model called PointNetPGAP, which combines a global average pooling aggregator and a pairwise feature interaction aggregator; (ii) a Segment-Level Consistency (SLC) model, used only during training, with the goal of augmenting the contrastive loss with a context-specific training signal to enhance descriptors; and (iii) a novel dataset named HORTO-3DLM featuring sequences from orchards and strawberry plantations.","The experimental evaluation, conducted on the new HORTO-3DLM dataset, compares PointNetPGAP at the sequence- and segment-level with state-of-the-art (SOTA) models, including OverlapTransformer, PointNetVLAD, and LOGG3D.","Additionally, all models were trained and evaluated using the SLC.","Empirical results obtained through a cross-validation evaluation protocol demonstrate the superiority of PointNetPGAP compared to existing SOTA models.","PointNetPGAP emerges as the best model in retrieving the top-1 candidate, outperforming PointNetVLAD (the second-best model).","Moreover, when comparing the impact of training with the SLC model, performance increased on four out of the five evaluated models, indicating that adding a context-specific signal to the contrastive loss leads to improved descriptors."],"url":"http://arxiv.org/abs/2405.19038v1","category":"cs.RO"}
{"created":"2024-05-29 12:23:29","title":"A Good Foundation is Worth Many Labels: Label-Efficient Panoptic Segmentation","abstract":"A key challenge for the widespread application of learning-based models for robotic perception is to significantly reduce the required amount of annotated training data while achieving accurate predictions. This is essential not only to decrease operating costs but also to speed up deployment time. In this work, we address this challenge for PAnoptic SegmenTation with fEw Labels (PASTEL) by exploiting the groundwork paved by visual foundation models. We leverage descriptive image features from such a model to train two lightweight network heads for semantic segmentation and object boundary detection, using very few annotated training samples. We then merge their predictions via a novel fusion module that yields panoptic maps based on normalized cut. To further enhance the performance, we utilize self-training on unlabeled images selected by a feature-driven similarity scheme. We underline the relevance of our approach by employing PASTEL to important robot perception use cases from autonomous driving and agricultural robotics. In extensive experiments, we demonstrate that PASTEL significantly outperforms previous methods for label-efficient segmentation even when using fewer annotations. The code of our work is publicly available at http://pastel.cs.uni-freiburg.de.","sentences":["A key challenge for the widespread application of learning-based models for robotic perception is to significantly reduce the required amount of annotated training data while achieving accurate predictions.","This is essential not only to decrease operating costs but also to speed up deployment time.","In this work, we address this challenge for PAnoptic SegmenTation with fEw Labels (PASTEL) by exploiting the groundwork paved by visual foundation models.","We leverage descriptive image features from such a model to train two lightweight network heads for semantic segmentation and object boundary detection, using very few annotated training samples.","We then merge their predictions via a novel fusion module that yields panoptic maps based on normalized cut.","To further enhance the performance, we utilize self-training on unlabeled images selected by a feature-driven similarity scheme.","We underline the relevance of our approach by employing PASTEL to important robot perception use cases from autonomous driving and agricultural robotics.","In extensive experiments, we demonstrate that PASTEL significantly outperforms previous methods for label-efficient segmentation even when using fewer annotations.","The code of our work is publicly available at http://pastel.cs.uni-freiburg.de."],"url":"http://arxiv.org/abs/2405.19035v1","category":"cs.RO"}
{"created":"2024-05-29 11:48:17","title":"Enhancing Vision-Language Model with Unmasked Token Alignment","abstract":"Contrastive pre-training on image-text pairs, exemplified by CLIP, becomes a standard technique for learning multi-modal visual-language representations. Although CLIP has demonstrated remarkable performance, training it from scratch on noisy web-scale datasets is computationally demanding. On the other hand, mask-then-predict pre-training approaches, like Masked Image Modeling (MIM), offer efficient self-supervised learning for single-modal representations. This paper introduces Unmasked Token Alignment (UTA), a method that leverages existing CLIP models to further enhance its vision-language representations. UTA trains a Vision Transformer (ViT) by aligning unmasked visual tokens to the corresponding image tokens from a frozen CLIP vision encoder, which automatically aligns the ViT model with the CLIP text encoder. The pre-trained ViT can be directly applied for zero-shot evaluation even without training on image-text pairs. Compared to MIM approaches, UTA does not suffer from training-finetuning inconsistency and is much more training-efficient by avoiding using the extra [MASK] tokens. Extensive experimental results demonstrate that UTA can enhance CLIP models and outperform existing MIM methods on various uni- and multi-modal benchmarks. Code and models are available at https://github.com/jihaonew/UTA.","sentences":["Contrastive pre-training on image-text pairs, exemplified by CLIP, becomes a standard technique for learning multi-modal visual-language representations.","Although CLIP has demonstrated remarkable performance, training it from scratch on noisy web-scale datasets is computationally demanding.","On the other hand, mask-then-predict pre-training approaches, like Masked Image Modeling (MIM), offer efficient self-supervised learning for single-modal representations.","This paper introduces Unmasked Token Alignment (UTA), a method that leverages existing CLIP models to further enhance its vision-language representations.","UTA trains a Vision Transformer (ViT) by aligning unmasked visual tokens to the corresponding image tokens from a frozen CLIP vision encoder, which automatically aligns the ViT model with the CLIP text encoder.","The pre-trained ViT can be directly applied for zero-shot evaluation even without training on image-text pairs.","Compared to MIM approaches, UTA does not suffer from training-finetuning inconsistency and is much more training-efficient by avoiding using the extra [MASK] tokens.","Extensive experimental results demonstrate that UTA can enhance CLIP models and outperform existing MIM methods on various uni-","and multi-modal benchmarks.","Code and models are available at https://github.com/jihaonew/UTA."],"url":"http://arxiv.org/abs/2405.19009v1","category":"cs.CV"}
{"created":"2024-05-29 11:31:15","title":"Dynamic Throwing with Robotic Material Handling Machines","abstract":"Automation of hydraulic material handling machinery is currently limited to semi-static pick-and-place cycles. Dynamic throwing motions which utilize the passive joints, can greatly improve time efficiency as well as increase the dumping workspace. In this work, we use Reinforcement Learning (RL) to design dynamic controllers for material handlers with underactuated arms as commonly used in logistics. The controllers are tested both in simulation and in real-world experiments on a 12-ton test platform. The method is able to exploit the passive joints of the gripper to perform dynamic throwing motions. With the proposed controllers, the machine is able to throw individual objects to targets outside the static reachability zone with good accuracy for its practical applications. The work demonstrates the possibility of using RL to perform highly dynamic tasks with heavy machinery, suggesting a potential for improving the efficiency and precision of autonomous material handling tasks.","sentences":["Automation of hydraulic material handling machinery is currently limited to semi-static pick-and-place cycles.","Dynamic throwing motions which utilize the passive joints, can greatly improve time efficiency as well as increase the dumping workspace.","In this work, we use Reinforcement Learning (RL) to design dynamic controllers for material handlers with underactuated arms as commonly used in logistics.","The controllers are tested both in simulation and in real-world experiments on a 12-ton test platform.","The method is able to exploit the passive joints of the gripper to perform dynamic throwing motions.","With the proposed controllers, the machine is able to throw individual objects to targets outside the static reachability zone with good accuracy for its practical applications.","The work demonstrates the possibility of using RL to perform highly dynamic tasks with heavy machinery, suggesting a potential for improving the efficiency and precision of autonomous material handling tasks."],"url":"http://arxiv.org/abs/2405.19001v1","category":"cs.RO"}
{"created":"2024-05-29 11:21:25","title":"Kernel Semi-Implicit Variational Inference","abstract":"Semi-implicit variational inference (SIVI) extends traditional variational families with semi-implicit distributions defined in a hierarchical manner. Due to the intractable densities of semi-implicit distributions, classical SIVI often resorts to surrogates of evidence lower bound (ELBO) that would introduce biases for training. A recent advancement in SIVI, named SIVI-SM, utilizes an alternative score matching objective made tractable via a minimax formulation, albeit requiring an additional lower-level optimization. In this paper, we propose kernel SIVI (KSIVI), a variant of SIVI-SM that eliminates the need for lower-level optimization through kernel tricks. Specifically, we show that when optimizing over a reproducing kernel Hilbert space (RKHS), the lower-level problem has an explicit solution. This way, the upper-level objective becomes the kernel Stein discrepancy (KSD), which is readily computable for stochastic gradient descent due to the hierarchical structure of semi-implicit variational distributions. An upper bound for the variance of the Monte Carlo gradient estimators of the KSD objective is derived, which allows us to establish novel convergence guarantees of KSIVI. We demonstrate the effectiveness and efficiency of KSIVI on both synthetic distributions and a variety of real data Bayesian inference tasks.","sentences":["Semi-implicit variational inference (SIVI) extends traditional variational families with semi-implicit distributions defined in a hierarchical manner.","Due to the intractable densities of semi-implicit distributions, classical SIVI often resorts to surrogates of evidence lower bound (ELBO) that would introduce biases for training.","A recent advancement in SIVI, named SIVI-SM, utilizes an alternative score matching objective made tractable via a minimax formulation, albeit requiring an additional lower-level optimization.","In this paper, we propose kernel SIVI (KSIVI), a variant of SIVI-SM that eliminates the need for lower-level optimization through kernel tricks.","Specifically, we show that when optimizing over a reproducing kernel Hilbert space (RKHS), the lower-level problem has an explicit solution.","This way, the upper-level objective becomes the kernel Stein discrepancy (KSD), which is readily computable for stochastic gradient descent due to the hierarchical structure of semi-implicit variational distributions.","An upper bound for the variance of the Monte Carlo gradient estimators of the KSD objective is derived, which allows us to establish novel convergence guarantees of KSIVI.","We demonstrate the effectiveness and efficiency of KSIVI on both synthetic distributions and a variety of real data Bayesian inference tasks."],"url":"http://arxiv.org/abs/2405.18997v1","category":"stat.ML"}
{"created":"2024-05-29 10:19:11","title":"Transcending Fusion: A Multi-Scale Alignment Method for Remote Sensing Image-Text Retrieval","abstract":"Remote Sensing Image-Text Retrieval (RSITR) is pivotal for knowledge services and data mining in the remote sensing (RS) domain. Considering the multi-scale representations in image content and text vocabulary can enable the models to learn richer representations and enhance retrieval. Current multi-scale RSITR approaches typically align multi-scale fused image features with text features, but overlook aligning image-text pairs at distinct scales separately. This oversight restricts their ability to learn joint representations suitable for effective retrieval. We introduce a novel Multi-Scale Alignment (MSA) method to overcome this limitation. Our method comprises three key innovations: (1) Multi-scale Cross-Modal Alignment Transformer (MSCMAT), which computes cross-attention between single-scale image features and localized text features, integrating global textual context to derive a matching score matrix within a mini-batch, (2) a multi-scale cross-modal semantic alignment loss that enforces semantic alignment across scales, and (3) a cross-scale multi-modal semantic consistency loss that uses the matching matrix from the largest scale to guide alignment at smaller scales. We evaluated our method across multiple datasets, demonstrating its efficacy with various visual backbones and establishing its superiority over existing state-of-the-art methods. The GitHub URL for our project is: https://github.com/yr666666/MSA","sentences":["Remote Sensing Image-Text Retrieval (RSITR) is pivotal for knowledge services and data mining in the remote sensing (RS) domain.","Considering the multi-scale representations in image content and text vocabulary can enable the models to learn richer representations and enhance retrieval.","Current multi-scale RSITR approaches typically align multi-scale fused image features with text features, but overlook aligning image-text pairs at distinct scales separately.","This oversight restricts their ability to learn joint representations suitable for effective retrieval.","We introduce a novel Multi-Scale Alignment (MSA) method to overcome this limitation.","Our method comprises three key innovations: (1) Multi-scale Cross-Modal Alignment Transformer (MSCMAT), which computes cross-attention between single-scale image features and localized text features, integrating global textual context to derive a matching score matrix within a mini-batch, (2) a multi-scale cross-modal semantic alignment loss that enforces semantic alignment across scales, and (3) a cross-scale multi-modal semantic consistency loss that uses the matching matrix from the largest scale to guide alignment at smaller scales.","We evaluated our method across multiple datasets, demonstrating its efficacy with various visual backbones and establishing its superiority over existing state-of-the-art methods.","The GitHub URL for our project is: https://github.com/yr666666/MSA"],"url":"http://arxiv.org/abs/2405.18959v1","category":"cs.CV"}
{"created":"2024-05-29 10:03:57","title":"Learning to Recover from Plan Execution Errors during Robot Manipulation: A Neuro-symbolic Approach","abstract":"Automatically detecting and recovering from failures is an important but challenging problem for autonomous robots. Most of the recent work on learning to plan from demonstrations lacks the ability to detect and recover from errors in the absence of an explicit state representation and/or a (sub-) goal check function. We propose an approach (blending learning with symbolic search) for automated error discovery and recovery, without needing annotated data of failures. Central to our approach is a neuro-symbolic state representation, in the form of dense scene graph, structured based on the objects present within the environment. This enables efficient learning of the transition function and a discriminator that not only identifies failures but also localizes them facilitating fast re-planning via computation of heuristic distance function. We also present an anytime version of our algorithm, where instead of recovering to the last correct state, we search for a sub-goal in the original plan minimizing the total distance to the goal given a re-planning budget. Experiments on a physics simulator with a variety of simulated failures show the effectiveness of our approach compared to existing baselines, both in terms of efficiency as well as accuracy of our recovery mechanism.","sentences":["Automatically detecting and recovering from failures is an important but challenging problem for autonomous robots.","Most of the recent work on learning to plan from demonstrations lacks the ability to detect and recover from errors in the absence of an explicit state representation and/or a (sub-) goal check function.","We propose an approach (blending learning with symbolic search) for automated error discovery and recovery, without needing annotated data of failures.","Central to our approach is a neuro-symbolic state representation, in the form of dense scene graph, structured based on the objects present within the environment.","This enables efficient learning of the transition function and a discriminator that not only identifies failures but also localizes them facilitating fast re-planning via computation of heuristic distance function.","We also present an anytime version of our algorithm, where instead of recovering to the last correct state, we search for a sub-goal in the original plan minimizing the total distance to the goal given a re-planning budget.","Experiments on a physics simulator with a variety of simulated failures show the effectiveness of our approach compared to existing baselines, both in terms of efficiency as well as accuracy of our recovery mechanism."],"url":"http://arxiv.org/abs/2405.18948v1","category":"cs.RO"}
{"created":"2024-05-29 08:36:42","title":"Privacy Preserving Data Imputation via Multi-party Computation for Medical Applications","abstract":"Handling missing data is crucial in machine learning, but many datasets contain gaps due to errors or non-response. Unlike traditional methods such as listwise deletion, which are simple but inadequate, the literature offers more sophisticated and effective methods, thereby improving sample size and accuracy. However, these methods require accessing the whole dataset, which contradicts the privacy regulations when the data is distributed among multiple sources. Especially in the medical and healthcare domain, such access reveals sensitive information about patients. This study addresses privacy-preserving imputation methods for sensitive data using secure multi-party computation, enabling secure computations without revealing any party's sensitive information. In this study, we realized the mean, median, regression, and kNN imputation methods in a privacy-preserving way. We specifically target the medical and healthcare domains considering the significance of protection of the patient data, showcasing our methods on a diabetes dataset. Experiments on the diabetes dataset validated the correctness of our privacy-preserving imputation methods, yielding the largest error around $3 \\times 10^{-3}$, closely matching plaintext methods. We also analyzed the scalability of our methods to varying numbers of samples, showing their applicability to real-world healthcare problems. Our analysis demonstrated that all our methods scale linearly with the number of samples. Except for kNN, the runtime of all our methods indicates that they can be utilized for large datasets.","sentences":["Handling missing data is crucial in machine learning, but many datasets contain gaps due to errors or non-response.","Unlike traditional methods such as listwise deletion, which are simple but inadequate, the literature offers more sophisticated and effective methods, thereby improving sample size and accuracy.","However, these methods require accessing the whole dataset, which contradicts the privacy regulations when the data is distributed among multiple sources.","Especially in the medical and healthcare domain, such access reveals sensitive information about patients.","This study addresses privacy-preserving imputation methods for sensitive data using secure multi-party computation, enabling secure computations without revealing any party's sensitive information.","In this study, we realized the mean, median, regression, and kNN imputation methods in a privacy-preserving way.","We specifically target the medical and healthcare domains considering the significance of protection of the patient data, showcasing our methods on a diabetes dataset.","Experiments on the diabetes dataset validated the correctness of our privacy-preserving imputation methods, yielding the largest error around $3 \\times 10^{-3}$, closely matching plaintext methods.","We also analyzed the scalability of our methods to varying numbers of samples, showing their applicability to real-world healthcare problems.","Our analysis demonstrated that all our methods scale linearly with the number of samples.","Except for kNN, the runtime of all our methods indicates that they can be utilized for large datasets."],"url":"http://arxiv.org/abs/2405.18878v1","category":"cs.CR"}
{"created":"2024-05-29 08:31:34","title":"DFAMiner: Mining minimal separating DFAs from labelled samples","abstract":"We propose DFAMiner, a passive learning tool for learning minimal separating deterministic finite automata (DFA) from a set of labelled samples. Separating automata are an interesting class of automata that occurs generally in regular model checking and has raised interest in foundational questions of parity game solving. We first propose a simple and linear-time algorithm that incrementally constructs a three-valued DFA (3DFA) from a set of labelled samples given in the usual lexicographical order. This 3DFA has accepting and rejecting states as well as don't-care states, so that it can exactly recognise the labelled examples. We then apply our tool to mining a minimal separating DFA for the labelled samples by minimising the constructed automata via a reduction to solving SAT problems. Empirical evaluation shows that our tool outperforms current state-of-the-art tools significantly on standard benchmarks for learning minimal separating DFAs from samples. Progress in the efficient construction of separating DFAs can also lead to finding the lower bound of parity game solving, where we show that DFAMiner can create optimal separating automata for simple languages with up to 7 colours. Future improvements might offer inroads to better data structures.","sentences":["We propose DFAMiner, a passive learning tool for learning minimal separating deterministic finite automata (DFA) from a set of labelled samples.","Separating automata are an interesting class of automata that occurs generally in regular model checking and has raised interest in foundational questions of parity game solving.","We first propose a simple and linear-time algorithm that incrementally constructs a three-valued DFA (3DFA) from a set of labelled samples given in the usual lexicographical order.","This 3DFA has accepting and rejecting states as well as don't-care states, so that it can exactly recognise the labelled examples.","We then apply our tool to mining a minimal separating DFA for the labelled samples by minimising the constructed automata via a reduction to solving SAT problems.","Empirical evaluation shows that our tool outperforms current state-of-the-art tools significantly on standard benchmarks for learning minimal separating DFAs from samples.","Progress in the efficient construction of separating DFAs can also lead to finding the lower bound of parity game solving, where we show that DFAMiner can create optimal separating automata for simple languages with up to 7 colours.","Future improvements might offer inroads to better data structures."],"url":"http://arxiv.org/abs/2405.18871v1","category":"cs.FL"}
{"created":"2024-05-29 07:31:18","title":"Do Finetti: On Causal Effects for Exchangeable Data","abstract":"We study causal effect estimation in a setting where the data are not i.i.d. (independent and identically distributed). We focus on exchangeable data satisfying an assumption of independent causal mechanisms. Traditional causal effect estimation frameworks, e.g., relying on structural causal models and do-calculus, are typically limited to i.i.d. data and do not extend to more general exchangeable generative processes, which naturally arise in multi-environment data. To address this gap, we develop a generalized framework for exchangeable data and introduce a truncated factorization formula that facilitates both the identification and estimation of causal effects in our setting. To illustrate potential applications, we introduce a causal P\\'olya urn model and demonstrate how intervention propagates effects in exchangeable data settings. Finally, we develop an algorithm that performs simultaneous causal discovery and effect estimation given multi-environment data.","sentences":["We study causal effect estimation in a setting where the data are not i.i.d.","(independent and identically distributed).","We focus on exchangeable data satisfying an assumption of independent causal mechanisms.","Traditional causal effect estimation frameworks, e.g., relying on structural causal models and do-calculus, are typically limited to i.i.d. data and do not extend to more general exchangeable generative processes, which naturally arise in multi-environment data.","To address this gap, we develop a generalized framework for exchangeable data and introduce a truncated factorization formula that facilitates both the identification and estimation of causal effects in our setting.","To illustrate potential applications, we introduce a causal P\\'olya urn model and demonstrate how intervention propagates effects in exchangeable data settings.","Finally, we develop an algorithm that performs simultaneous causal discovery and effect estimation given multi-environment data."],"url":"http://arxiv.org/abs/2405.18836v1","category":"stat.ME"}
{"created":"2024-05-29 07:20:28","title":"Evaluating Zero-Shot GPT-4V Performance on 3D Visual Question Answering Benchmarks","abstract":"As interest in \"reformulating\" the 3D Visual Question Answering (VQA) problem in the context of foundation models grows, it is imperative to assess how these new paradigms influence existing closed-vocabulary datasets. In this case study, we evaluate the zero-shot performance of foundational models (GPT-4 Vision and GPT-4) on well-established 3D VQA benchmarks, namely 3D-VQA and ScanQA. We provide an investigation to contextualize the performance of GPT-based agents relative to traditional modeling approaches. We find that GPT-based agents without any fine-tuning perform on par with the closed vocabulary approaches. Our findings corroborate recent results that \"blind\" models establish a surprisingly strong baseline in closed-vocabulary settings. We demonstrate that agents benefit significantly from scene-specific vocabulary via in-context textual grounding. By presenting a preliminary comparison with previous baselines, we hope to inform the community's ongoing efforts to refine multi-modal 3D benchmarks.","sentences":["As interest in \"reformulating\" the 3D Visual Question Answering (VQA) problem in the context of foundation models grows, it is imperative to assess how these new paradigms influence existing closed-vocabulary datasets.","In this case study, we evaluate the zero-shot performance of foundational models (GPT-4 Vision and GPT-4) on well-established 3D VQA benchmarks, namely 3D-VQA and ScanQA.","We provide an investigation to contextualize the performance of GPT-based agents relative to traditional modeling approaches.","We find that GPT-based agents without any fine-tuning perform on par with the closed vocabulary approaches.","Our findings corroborate recent results that \"blind\" models establish a surprisingly strong baseline in closed-vocabulary settings.","We demonstrate that agents benefit significantly from scene-specific vocabulary via in-context textual grounding.","By presenting a preliminary comparison with previous baselines, we hope to inform the community's ongoing efforts to refine multi-modal 3D benchmarks."],"url":"http://arxiv.org/abs/2405.18831v1","category":"cs.CV"}
{"created":"2024-05-29 06:09:34","title":"Opinion-Unaware Blind Image Quality Assessment using Multi-Scale Deep Feature Statistics","abstract":"Deep learning-based methods have significantly influenced the blind image quality assessment (BIQA) field, however, these methods often require training using large amounts of human rating data. In contrast, traditional knowledge-based methods are cost-effective for training but face challenges in effectively extracting features aligned with human visual perception. To bridge these gaps, we propose integrating deep features from pre-trained visual models with a statistical analysis model into a Multi-scale Deep Feature Statistics (MDFS) model for achieving opinion-unaware BIQA (OU-BIQA), thereby eliminating the reliance on human rating data and significantly improving training efficiency. Specifically, we extract patch-wise multi-scale features from pre-trained vision models, which are subsequently fitted into a multivariate Gaussian (MVG) model. The final quality score is determined by quantifying the distance between the MVG model derived from the test image and the benchmark MVG model derived from the high-quality image set. A comprehensive series of experiments conducted on various datasets show that our proposed model exhibits superior consistency with human visual perception compared to state-of-the-art BIQA models. Furthermore, it shows improved generalizability across diverse target-specific BIQA tasks. Our code is available at: https://github.com/eezkni/MDFS","sentences":["Deep learning-based methods have significantly influenced the blind image quality assessment (BIQA) field, however, these methods often require training using large amounts of human rating data.","In contrast, traditional knowledge-based methods are cost-effective for training but face challenges in effectively extracting features aligned with human visual perception.","To bridge these gaps, we propose integrating deep features from pre-trained visual models with a statistical analysis model into a Multi-scale Deep Feature Statistics (MDFS) model for achieving opinion-unaware BIQA (OU-BIQA), thereby eliminating the reliance on human rating data and significantly improving training efficiency.","Specifically, we extract patch-wise multi-scale features from pre-trained vision models, which are subsequently fitted into a multivariate Gaussian (MVG) model.","The final quality score is determined by quantifying the distance between the MVG model derived from the test image and the benchmark MVG model derived from the high-quality image set.","A comprehensive series of experiments conducted on various datasets show that our proposed model exhibits superior consistency with human visual perception compared to state-of-the-art BIQA models.","Furthermore, it shows improved generalizability across diverse target-specific BIQA tasks.","Our code is available at: https://github.com/eezkni/MDFS"],"url":"http://arxiv.org/abs/2405.18790v1","category":"cs.CV"}
{"created":"2024-05-29 05:59:52","title":"MOKD: Cross-domain Finetuning for Few-shot Classification via Maximizing Optimized Kernel Dependence","abstract":"In cross-domain few-shot classification, \\emph{nearest centroid classifier} (NCC) aims to learn representations to construct a metric space where few-shot classification can be performed by measuring the similarities between samples and the prototype of each class. An intuition behind NCC is that each sample is pulled closer to the class centroid it belongs to while pushed away from those of other classes. However, in this paper, we find that there exist high similarities between NCC-learned representations of two samples from different classes. In order to address this problem, we propose a bi-level optimization framework, \\emph{maximizing optimized kernel dependence} (MOKD) to learn a set of class-specific representations that match the cluster structures indicated by labeled data of the given task. Specifically, MOKD first optimizes the kernel adopted in \\emph{Hilbert-Schmidt independence criterion} (HSIC) to obtain the optimized kernel HSIC (opt-HSIC) that can capture the dependence more precisely. Then, an optimization problem regarding the opt-HSIC is addressed to simultaneously maximize the dependence between representations and labels and minimize the dependence among all samples. Extensive experiments on Meta-Dataset demonstrate that MOKD can not only achieve better generalization performance on unseen domains in most cases but also learn better data representation clusters. The project repository of MOKD is available at: \\href{https://github.com/tmlr-group/MOKD}{https://github.com/tmlr-group/MOKD}.","sentences":["In cross-domain few-shot classification, \\emph{nearest centroid classifier} (NCC) aims to learn representations to construct a metric space where few-shot classification can be performed by measuring the similarities between samples and the prototype of each class.","An intuition behind NCC is that each sample is pulled closer to the class centroid it belongs to while pushed away from those of other classes.","However, in this paper, we find that there exist high similarities between NCC-learned representations of two samples from different classes.","In order to address this problem, we propose a bi-level optimization framework, \\emph{maximizing optimized kernel dependence} (MOKD) to learn a set of class-specific representations that match the cluster structures indicated by labeled data of the given task.","Specifically, MOKD first optimizes the kernel adopted in \\emph{Hilbert-Schmidt independence criterion} (HSIC) to obtain the optimized kernel HSIC (opt-HSIC) that can capture the dependence more precisely.","Then, an optimization problem regarding the opt-HSIC is addressed to simultaneously maximize the dependence between representations and labels and minimize the dependence among all samples.","Extensive experiments on Meta-Dataset demonstrate that MOKD can not only achieve better generalization performance on unseen domains in most cases but also learn better data representation clusters.","The project repository of MOKD is available at: \\href{https://github.com/tmlr-group/MOKD}{https://github.com/tmlr-group/MOKD}."],"url":"http://arxiv.org/abs/2405.18786v1","category":"cs.LG"}
{"created":"2024-05-29 17:27:50","title":"Multi-qubit circuit synthesis and Hermitian lattices","abstract":"We present new optimal and heuristic algorithms for exact synthesis of multi-qubit unitaries and isometries. For example, our algorithms find Clifford and T circuits for unitaries with entries in $\\mathbb{Z}[i,1/\\sqrt{2}]$. The optimal algorithms are the A* search instantiated with a new data structure for graph vertices and new consistent heuristic functions. We also prove that for some gate sets, best-first search synthesis relying on the same heuristic is efficient. For example, for two-qubit Clifford and T circuits, our best-first search runtime is proportional to the T-count of the unitary. Our algorithms rely on Hermite and Smith Normal Forms of matrices with entries in a ring of integers of a number field, and we leverage the theory of and algorithms for Hermitian lattices over number fields to prove efficiency. These new techniques are of independent interest for future work on multi-qubit exact circuit synthesis and related questions.","sentences":["We present new optimal and heuristic algorithms for exact synthesis of multi-qubit unitaries and isometries.","For example, our algorithms find Clifford and T circuits for unitaries with entries in $\\mathbb{Z}[i,1/\\sqrt{2}]$. The optimal algorithms are the A* search instantiated with a new data structure for graph vertices and new consistent heuristic functions.","We also prove that for some gate sets, best-first search synthesis relying on the same heuristic is efficient.","For example, for two-qubit Clifford and T circuits, our best-first search runtime is proportional to the T-count of the unitary.","Our algorithms rely on Hermite and Smith Normal Forms of matrices with entries in a ring of integers of a number field, and we leverage the theory of and algorithms for Hermitian lattices over number fields to prove efficiency.","These new techniques are of independent interest for future work on multi-qubit exact circuit synthesis and related questions."],"url":"http://arxiv.org/abs/2405.19302v1","category":"quant-ph"}
{"created":"2024-05-29 17:18:08","title":"Efficiently manipulating Pauli strings with PauliArray","abstract":"Pauli matrices and Pauli strings are widely used in quantum computing. These mathematical objects are useful to describe or manipulate the quantum state of qubits. They offer a convenient basis to express operators and observables used in different problem instances such as molecular simulation and combinatorial optimization. Therefore, it is important to have a well-rounded, versatile and efficient tool to handle a large number of Pauli strings and operators expressed in this basis. This is the objective behind the development of the PauliArray library presented in this work. This library introduces data structures to represent arrays of Pauli strings and operators as well as various methods to modify and combine them. Built using NumPy, PauliArray offers fast operations and the ability to use broadcasting to easily carry out otherwise cumbersome manipulations. Applications to the fermion-to-qubit mapping, to the estimation of expectation values and to the computation of commutators are considered to illustrate how PauliArray can simplify some relevant tasks and accomplish them faster than current libraries.","sentences":["Pauli matrices and Pauli strings are widely used in quantum computing.","These mathematical objects are useful to describe or manipulate the quantum state of qubits.","They offer a convenient basis to express operators and observables used in different problem instances such as molecular simulation and combinatorial optimization.","Therefore, it is important to have a well-rounded, versatile and efficient tool to handle a large number of Pauli strings and operators expressed in this basis.","This is the objective behind the development of the PauliArray library presented in this work.","This library introduces data structures to represent arrays of Pauli strings and operators as well as various methods to modify and combine them.","Built using NumPy, PauliArray offers fast operations and the ability to use broadcasting to easily carry out otherwise cumbersome manipulations.","Applications to the fermion-to-qubit mapping, to the estimation of expectation values and to the computation of commutators are considered to illustrate how PauliArray can simplify some relevant tasks and accomplish them faster than current libraries."],"url":"http://arxiv.org/abs/2405.19287v1","category":"quant-ph"}
{"created":"2024-05-29 15:35:09","title":"LOGO: Video Text Spotting with Language Collaboration and Glyph Perception Model","abstract":"Video text spotting aims to simultaneously localize, recognize and track text instances in videos. To address the limited recognition capability of end-to-end methods, tracking the zero-shot results of state-of-the-art image text spotters directly can achieve impressive performance. However, owing to the domain gap between different datasets, these methods usually obtain limited tracking trajectories on extreme dataset. Fine-tuning transformer-based text spotters on specific datasets could yield performance enhancements, albeit at the expense of considerable training resources. In this paper, we propose a Language Collaboration and Glyph Perception Model, termed LOGO to enhance the performance of conventional text spotters through the integration of a synergy module. To achieve this goal, a language synergy classifier (LSC) is designed to explicitly discern text instances from background noise in the recognition stage. Specially, the language synergy classifier can output text content or background code based on the legibility of text regions, thus computing language scores. Subsequently, fusion scores are computed by taking the average of detection scores and language scores, and are utilized to re-score the detection results before tracking. By the re-scoring mechanism, the proposed LSC facilitates the detection of low-resolution text instances while filtering out text-like regions. Besides, the glyph supervision and visual position mixture module are proposed to enhance the recognition accuracy of noisy text regions, and acquire more discriminative tracking features, respectively. Extensive experiments on public benchmarks validate the effectiveness of the proposed method.","sentences":["Video text spotting aims to simultaneously localize, recognize and track text instances in videos.","To address the limited recognition capability of end-to-end methods, tracking the zero-shot results of state-of-the-art image text spotters directly can achieve impressive performance.","However, owing to the domain gap between different datasets, these methods usually obtain limited tracking trajectories on extreme dataset.","Fine-tuning transformer-based text spotters on specific datasets could yield performance enhancements, albeit at the expense of considerable training resources.","In this paper, we propose a Language Collaboration and Glyph Perception Model, termed LOGO to enhance the performance of conventional text spotters through the integration of a synergy module.","To achieve this goal, a language synergy classifier (LSC) is designed to explicitly discern text instances from background noise in the recognition stage.","Specially, the language synergy classifier can output text content or background code based on the legibility of text regions, thus computing language scores.","Subsequently, fusion scores are computed by taking the average of detection scores and language scores, and are utilized to re-score the detection results before tracking.","By the re-scoring mechanism, the proposed LSC facilitates the detection of low-resolution text instances while filtering out text-like regions.","Besides, the glyph supervision and visual position mixture module are proposed to enhance the recognition accuracy of noisy text regions, and acquire more discriminative tracking features, respectively.","Extensive experiments on public benchmarks validate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2405.19194v1","category":"cs.CV"}
{"created":"2024-05-29 14:51:54","title":"Homomorphism Counts to Trees","abstract":"We construct a pair of non-isomorphic, bipartite graphs which are not distinguished by counting the number of homomorphisms to any tree. This answers a question raised by Atserias et al. (LICS 2021). In order to establish the construction, we analyse the equivalence relations induced by counting homomorphisms to trees of diameter two and three and obtain necessary and sufficient conditions for two graphs to be equivalent. We show that three is the optimal diameter for our construction.","sentences":["We construct a pair of non-isomorphic, bipartite graphs which are not distinguished by counting the number of homomorphisms to any tree.","This answers a question raised by Atserias et al. (LICS 2021).","In order to establish the construction, we analyse the equivalence relations induced by counting homomorphisms to trees of diameter two and three and obtain necessary and sufficient conditions for two graphs to be equivalent.","We show that three is the optimal diameter for our construction."],"url":"http://arxiv.org/abs/2405.19147v1","category":"cs.DM"}
{"created":"2024-05-29 13:36:16","title":"On the Influence of Smoothness Constraints in Computed Tomography Motion Compensation","abstract":"Computed tomography (CT) relies on precise patient immobilization during image acquisition. Nevertheless, motion artifacts in the reconstructed images can persist. Motion compensation methods aim to correct such artifacts post-acquisition, often incorporating temporal smoothness constraints on the estimated motion patterns. This study analyzes the influence of a spline-based motion model within an existing rigid motion compensation algorithm for cone-beam CT on the recoverable motion frequencies. Results demonstrate that the choice of motion model crucially influences recoverable frequencies. The optimization-based motion compensation algorithm is able to accurately fit the spline nodes for frequencies almost up to the node-dependent theoretical limit according to the Nyquist-Shannon theorem. Notably, a higher node count does not compromise reconstruction performance for slow motion patterns, but can extend the range of recoverable high frequencies for the investigated algorithm. Eventually, the optimal motion model is dependent on the imaged anatomy, clinical use case, and scanning protocol and should be tailored carefully to the expected motion frequency spectrum to ensure accurate motion compensation.","sentences":["Computed tomography (CT) relies on precise patient immobilization during image acquisition.","Nevertheless, motion artifacts in the reconstructed images can persist.","Motion compensation methods aim to correct such artifacts post-acquisition, often incorporating temporal smoothness constraints on the estimated motion patterns.","This study analyzes the influence of a spline-based motion model within an existing rigid motion compensation algorithm for cone-beam CT on the recoverable motion frequencies.","Results demonstrate that the choice of motion model crucially influences recoverable frequencies.","The optimization-based motion compensation algorithm is able to accurately fit the spline nodes for frequencies almost up to the node-dependent theoretical limit according to the Nyquist-Shannon theorem.","Notably, a higher node count does not compromise reconstruction performance for slow motion patterns, but can extend the range of recoverable high frequencies for the investigated algorithm.","Eventually, the optimal motion model is dependent on the imaged anatomy, clinical use case, and scanning protocol and should be tailored carefully to the expected motion frequency spectrum to ensure accurate motion compensation."],"url":"http://arxiv.org/abs/2405.19079v1","category":"eess.IV"}
{"created":"2024-05-29 12:13:03","title":"A Dual-functional Blockchain Framework for Solving Distributed Optimization","abstract":"Proof of Work (PoW) has been extensively utilized as the foundation of blockchain's security, consistency, and tamper-resistance. However, long has it been criticized for its tremendous and inefficient utilization of computational power and energy. In this work, we design a dual-functional blockchain framework that uses solving optimization problems to reach consensus as an alternative to PoW, channeling wasted resources into useful work. We model and analyze our framework by developing discrete Markov chains, and derive the security conditions to ensure that selfish miners behave honestly. Based on the security conditions, we derive a lower bound for the security overhead and analyze the trade-off between useful work efficiency and PoW safeguard. We further dive deep into the reward function design for the proposed dual-functional blockchain and provide practical design guidelines for reward functions assuming concavity and linearity respectively. Finally, simulation results are presented to validate and illustrate our analytical results.","sentences":["Proof of Work (PoW) has been extensively utilized as the foundation of blockchain's security, consistency, and tamper-resistance.","However, long has it been criticized for its tremendous and inefficient utilization of computational power and energy.","In this work, we design a dual-functional blockchain framework that uses solving optimization problems to reach consensus as an alternative to PoW, channeling wasted resources into useful work.","We model and analyze our framework by developing discrete Markov chains, and derive the security conditions to ensure that selfish miners behave honestly.","Based on the security conditions, we derive a lower bound for the security overhead and analyze the trade-off between useful work efficiency and PoW safeguard.","We further dive deep into the reward function design for the proposed dual-functional blockchain and provide practical design guidelines for reward functions assuming concavity and linearity respectively.","Finally, simulation results are presented to validate and illustrate our analytical results."],"url":"http://arxiv.org/abs/2405.19027v1","category":"cs.DC"}
{"created":"2024-05-29 11:41:38","title":"An implementation of tensor product patch smoothers on GPU","abstract":"We present a GPU implementation of vertex-patch smoothers for higher order finite element methods in two and three dimensions. Analysis shows that they are not memory bound with respect to GPU DRAM, but with respect to on-chip scratchpad memory. Multigrid operations are optimized through localization and reorganized local operations in on-chip memory, achieving minimal global data transfer and a conflict free memory access pattern. Performance tests demonstrate that the optimized kernel is at least 2 times faster than the straightforward implementation for the Poisson problem, across various polynomial degrees in 2D and 3D, achieving up to 36% of the peak performance in both single and double precision on Nvidia A100 GPU.","sentences":["We present a GPU implementation of vertex-patch smoothers for higher order finite element methods in two and three dimensions.","Analysis shows that they are not memory bound with respect to GPU DRAM, but with respect to on-chip scratchpad memory.","Multigrid operations are optimized through localization and reorganized local operations in on-chip memory, achieving minimal global data transfer and a conflict free memory access pattern.","Performance tests demonstrate that the optimized kernel is at least 2 times faster than the straightforward implementation for the Poisson problem, across various polynomial degrees in 2D and 3D, achieving up to 36% of the peak performance in both single and double precision on Nvidia A100 GPU."],"url":"http://arxiv.org/abs/2405.19004v1","category":"math.NA"}
{"created":"2024-05-29 10:55:52","title":"Multilevel Interior Penalty Methods on GPUs","abstract":"We present a matrix-free multigrid method for high-order discontinuous Galerkin (DG) finite element methods with GPU acceleration. A performance analysis is conducted, comparing various data and compute layouts. Smoother implementations are optimized through localization and fast diagonalization techniques. Leveraging conflict-free access patterns in shared memory, arithmetic throughput of up to 39% of the peak performance on Nvidia A100 GPUs are achieved. Experimental results affirm the effectiveness of mixed-precision approaches and MPI parallelization in accelerating algorithms. Furthermore, an assessment of solver efficiency and robustness is provided across both two and three dimensions, with applications to Poisson problems.","sentences":["We present a matrix-free multigrid method for high-order discontinuous Galerkin (DG) finite element methods with GPU acceleration.","A performance analysis is conducted, comparing various data and compute layouts.","Smoother implementations are optimized through localization and fast diagonalization techniques.","Leveraging conflict-free access patterns in shared memory, arithmetic throughput of up to 39% of the peak performance on Nvidia A100 GPUs are achieved.","Experimental results affirm the effectiveness of mixed-precision approaches and MPI parallelization in accelerating algorithms.","Furthermore, an assessment of solver efficiency and robustness is provided across both two and three dimensions, with applications to Poisson problems."],"url":"http://arxiv.org/abs/2405.18982v1","category":"math.NA"}
{"created":"2024-05-29 10:48:15","title":"A novel mechanical design of a bolometric array for the CROSS double-beta decay experiment","abstract":"The CROSS experiment will search for neutrinoless double-beta decay using a specific mechanical structure to hold thermal detectors. The design of the structure was tuned to minimize the background contribution, keeping an optimal detector performance. A single module of the structure holds two scintillating bolometers (with a crystal size of 45x45x45 mm and a Ge slab facing the crystal's upper side) in the Cu frame, allowing for a modular construction of a large-scale array. Two designs are released: the initial Thick version contains around 15% of Cu over the crystal mass (lithium molybdate, LMO), while this ratio is reduced to ~6% in a finer (Slim) design. Both designs were tested extensively at aboveground (IJCLab, France) and underground (LSC, Spain) laboratories. In particular, at LSC we used a pulse-tube-based CROSS facility to operate a 6-crystal array of LMOs enriched/depleted in $^{100}$Mo. The tested LMOs show high spectrometric performance in both designs; notably, the measured energy resolution is 5--7 keV FWHM at 2615 keV $\\gamma$s, nearby the Q-value of $^{100}$Mo (3034 keV). Due to the absence of a reflective cavity around LMOs, a low scintillation signal is detected by Ge bolometers: ~0.3 keV (~150 photons) for 1-MeV $\\gamma$($\\beta$) LMO-event. Despite that, an acceptable separation between $\\alpha$ and $\\gamma$($\\beta$) events is achieved with most devices. The highest efficiency is reached with light detectors in the Thick design thanks to a lower baseline noise width (0.05--0.09 keV RMS) when compared to that obtained in the Slim version (0.10--0.35 keV RMS). Given the pivotal role of bolometric photodetectors for particle identification and random coincidences rejection, we will use the structure here described with upgraded light detectors, featuring thermal signal amplification via the Neganov-Trofimov-Luke effect, as also demonstrated in the present work.","sentences":["The CROSS experiment will search for neutrinoless double-beta decay using a specific mechanical structure to hold thermal detectors.","The design of the structure was tuned to minimize the background contribution, keeping an optimal detector performance.","A single module of the structure holds two scintillating bolometers (with a crystal size of 45x45x45 mm and a Ge slab facing the crystal's upper side) in the Cu frame, allowing for a modular construction of a large-scale array.","Two designs are released: the initial Thick version contains around 15% of Cu over the crystal mass (lithium molybdate, LMO), while this ratio is reduced to ~6% in a finer (Slim) design.","Both designs were tested extensively at aboveground (IJCLab, France) and underground (LSC, Spain) laboratories.","In particular, at LSC we used a pulse-tube-based CROSS facility to operate a 6-crystal array of LMOs enriched/depleted in $^{100}$Mo.","The tested LMOs show high spectrometric performance in both designs; notably, the measured energy resolution is 5--7 keV FWHM at 2615 keV $\\gamma$s, nearby the Q-value of $^{100}$Mo (3034 keV).","Due to the absence of a reflective cavity around LMOs, a low scintillation signal is detected by Ge bolometers: ~0.3 keV (~150 photons) for 1-MeV $\\gamma$($\\beta$) LMO-event.","Despite that, an acceptable separation between $\\alpha$ and $\\gamma$($\\beta$) events is achieved with most devices.","The highest efficiency is reached with light detectors in the Thick design thanks to a lower baseline noise width (0.05--0.09 keV RMS) when compared to that obtained in the Slim version (0.10--0.35 keV RMS).","Given the pivotal role of bolometric photodetectors for particle identification and random coincidences rejection, we will use the structure here described with upgraded light detectors, featuring thermal signal amplification via the Neganov-Trofimov-Luke effect, as also demonstrated in the present work."],"url":"http://arxiv.org/abs/2405.18980v1","category":"physics.ins-det"}
{"created":"2024-05-29 09:41:31","title":"Optimizing Broker Performance Evaluation through Intraday Modeling of Execution Cost","abstract":"Minimizing execution costs for large orders is a fundamental challenge in finance. Firms often depend on brokers to manage their trades due to limited internal resources for optimizing trading strategies. This paper presents a methodology for evaluating the effectiveness of broker execution algorithms using trading data. We focus on two primary cost components: a linear cost that quantifies short-term execution quality and a quadratic cost associated with the price impact of trades. Using a model with transient price impact, we derive analytical formulas for estimating these costs. Furthermore, we enhance estimation accuracy by introducing novel methods such as weighting price changes based on their expected impact content. Our results demonstrate substantial improvements in estimating both linear and impact costs, providing a robust and efficient framework for selecting the most cost-effective brokers.","sentences":["Minimizing execution costs for large orders is a fundamental challenge in finance.","Firms often depend on brokers to manage their trades due to limited internal resources for optimizing trading strategies.","This paper presents a methodology for evaluating the effectiveness of broker execution algorithms using trading data.","We focus on two primary cost components: a linear cost that quantifies short-term execution quality and a quadratic cost associated with the price impact of trades.","Using a model with transient price impact, we derive analytical formulas for estimating these costs.","Furthermore, we enhance estimation accuracy by introducing novel methods such as weighting price changes based on their expected impact content.","Our results demonstrate substantial improvements in estimating both linear and impact costs, providing a robust and efficient framework for selecting the most cost-effective brokers."],"url":"http://arxiv.org/abs/2405.18936v1","category":"q-fin.TR"}
{"created":"2024-05-29 09:26:50","title":"The BlackGEM telescope array I: Overview","abstract":"The main science aim of the BlackGEM array is to detect optical counterparts to gravitational wave mergers. Additionally, the array will perform a set of synoptic surveys to detect Local Universe transients and short time-scale variability in stars and binaries, as well as a six-filter all-sky survey down to ~22nd mag. The BlackGEM Phase-I array consists of three optical wide-field unit telescopes. Each unit uses an f/5.5 modified Dall-Kirkham (Harmer-Wynne) design with a triplet corrector lens, and a 65cm primary mirror, coupled with a 110Mpix CCD detector, that provides an instantaneous field-of-view of 2.7~square degrees, sampled at 0.564\\arcsec/pixel. The total field-of-view for the array is 8.2 square degrees. Each telescope is equipped with a six-slot filter wheel containing an optimised Sloan set (BG-u, BG-g, BG-r, BG-i, BG-z) and a wider-band 440-720 nm (BG-q) filter. Each unit telescope is independent from the others. Cloud-based data processing is done in real time, and includes a transient-detection routine as well as a full-source optimal-photometry module. BlackGEM has been installed at the ESO La Silla observatory as of October 2019. After a prolonged COVID-19 hiatus, science operations started on April 1, 2023 and will run for five years. Aside from its core scientific program, BlackGEM will give rise to a multitude of additional science cases in multi-colour time-domain astronomy, to the benefit of a variety of topics in astrophysics, such as infant supernovae, luminous red novae, asteroseismology of post-main-sequence objects, (ultracompact) binary stars, and the relation between gravitational wave counterparts and other classes of transients","sentences":["The main science aim of the BlackGEM array is to detect optical counterparts to gravitational wave mergers.","Additionally, the array will perform a set of synoptic surveys to detect Local Universe transients and short time-scale variability in stars and binaries, as well as a six-filter all-sky survey down to ~22nd mag.","The BlackGEM Phase-I array consists of three optical wide-field unit telescopes.","Each unit uses an f/5.5 modified Dall-Kirkham (Harmer-Wynne) design with a triplet corrector lens, and a 65cm primary mirror, coupled with a 110Mpix CCD detector, that provides an instantaneous field-of-view of 2.7~square degrees, sampled at 0.564\\arcsec/pixel.","The total field-of-view for the array is 8.2 square degrees.","Each telescope is equipped with a six-slot filter wheel containing an optimised Sloan set (BG-u, BG-g, BG-r, BG-i, BG-z) and a wider-band 440-720 nm (BG-q) filter.","Each unit telescope is independent from the others.","Cloud-based data processing is done in real time, and includes a transient-detection routine as well as a full-source optimal-photometry module.","BlackGEM has been installed at the ESO La Silla observatory as of October 2019.","After a prolonged COVID-19 hiatus, science operations started on April 1, 2023 and will run for five years.","Aside from its core scientific program, BlackGEM will give rise to a multitude of additional science cases in multi-colour time-domain astronomy, to the benefit of a variety of topics in astrophysics, such as infant supernovae, luminous red novae, asteroseismology of post-main-sequence objects, (ultracompact) binary stars, and the relation between gravitational wave counterparts and other classes of transients"],"url":"http://arxiv.org/abs/2405.18923v1","category":"astro-ph.IM"}
{"created":"2024-05-29 09:17:50","title":"Global Optimization for Trajectory Design via Invariant Manifolds in the Earth-Moon Circular Restricted Three-Body Problem","abstract":"This study addresses optimal impulsive trajectory design within the Circular Restricted Three-Body Problem (CR3BP), presenting a global optimization-based approach to identify minimum $\\Delta V$ transfers between periodic orbits, including heteroclinic connections. By combining a Monotonic Basin Hopping (MBH) algorithm with a sequential quadratic solver in a parallel optimization framework, a wide range of minimum $\\Delta V$ transfers are efficiently found. To validate this approach, known connections from the literature are reproduced. Consequently, three-dimensional periodic orbits are explored and a systematic search for minimum propellant trajectories is conducted within a selected interval of Jacobi constants and a maximum time of flight. Analysis of the results reveals the presence of very low $\\Delta V$ solutions and showcases the algorithm's effectiveness across various mission scenarios.","sentences":["This study addresses optimal impulsive trajectory design within the Circular Restricted Three-Body Problem (CR3BP), presenting a global optimization-based approach to identify minimum $\\Delta V$ transfers between periodic orbits, including heteroclinic connections.","By combining a Monotonic Basin Hopping (MBH) algorithm with a sequential quadratic solver in a parallel optimization framework, a wide range of minimum $\\Delta V$ transfers are efficiently found.","To validate this approach, known connections from the literature are reproduced.","Consequently, three-dimensional periodic orbits are explored and a systematic search for minimum propellant trajectories is conducted within a selected interval of Jacobi constants and a maximum time of flight.","Analysis of the results reveals the presence of very low $\\Delta V$ solutions and showcases the algorithm's effectiveness across various mission scenarios."],"url":"http://arxiv.org/abs/2405.18916v1","category":"physics.space-ph"}
{"created":"2024-05-29 09:00:47","title":"The Near-Infrared Gatherer of Helium Transits (NIGHT)","abstract":"This paper provides a comprehensive overview of the subsystems of the NIGHT instrument. NIGHT (the Near Infrared Gatherer of Helium Transits) is a narrowband, high-resolution spectrograph, marking the first dedicated survey instrument for exoplanetary atmosphere observations. Developed through a collaboration between the Observatory of Geneva and the Universite de Montreal, NIGHT aims to conduct an extensive statistical survey of helium atmospheres around 100+ exoplanets over several years. The instrument will report new detections of helium in exoplanet atmospheres and perform temporal monitoring of a subset of these.   NIGHT measures absorption from the metastable helium state during exoplanet transits, observable in a triplet of lines around 1083nm. The instrument comprises a vacuum enclosure housing the spectrograph, a front end unit for fiber injection at the telescope's focal plane, and a calibration and control rack containing calibration light sources and control hardware.   The spectrograph is optimized for efficiency, achieving a uniform throughput of approximately 71%. The primary disperser employs a VPH grating in a unique double-pass configuration, enabling a spectral resolution of 75,000 while maintaining high throughput. The detector is a HAWAII-1 infrared array, cooled to 85K, with the spectrograph operating at room temperature. Thanks to its relatively high throughput, NIGHT on a 2m class telescope is predicted to be as sensitive as existing instruments on 4m class telescopes.   The front end unit injects starlight and sky background into two separate fibers leading to the spectrograph. It also performs near-infrared guiding and includes a mechanism for injecting calibration light.   The assembly and optical alignment of NIGHT's spectrograph and front end unit are scheduled for July to September 2024, with the first light anticipated before early 2025.","sentences":["This paper provides a comprehensive overview of the subsystems of the NIGHT instrument.","NIGHT (the Near Infrared Gatherer of Helium Transits) is a narrowband, high-resolution spectrograph, marking the first dedicated survey instrument for exoplanetary atmosphere observations.","Developed through a collaboration between the Observatory of Geneva and the Universite de Montreal, NIGHT aims to conduct an extensive statistical survey of helium atmospheres around 100+ exoplanets over several years.","The instrument will report new detections of helium in exoplanet atmospheres and perform temporal monitoring of a subset of these.   ","NIGHT measures absorption from the metastable helium state during exoplanet transits, observable in a triplet of lines around 1083nm.","The instrument comprises a vacuum enclosure housing the spectrograph, a front end unit for fiber injection at the telescope's focal plane, and a calibration and control rack containing calibration light sources and control hardware.   ","The spectrograph is optimized for efficiency, achieving a uniform throughput of approximately 71%.","The primary disperser employs a VPH grating in a unique double-pass configuration, enabling a spectral resolution of 75,000 while maintaining high throughput.","The detector is a HAWAII-1 infrared array, cooled to 85K, with the spectrograph operating at room temperature.","Thanks to its relatively high throughput, NIGHT on a 2m class telescope is predicted to be as sensitive as existing instruments on 4m class telescopes.   ","The front end unit injects starlight and sky background into two separate fibers leading to the spectrograph.","It also performs near-infrared guiding and includes a mechanism for injecting calibration light.   ","The assembly and optical alignment of NIGHT's spectrograph and front end unit are scheduled for July to September 2024, with the first light anticipated before early 2025."],"url":"http://arxiv.org/abs/2405.18899v1","category":"astro-ph.IM"}
{"created":"2024-05-29 08:13:48","title":"Distributed Bilevel Optimization with Communication Compression","abstract":"Stochastic bilevel optimization tackles challenges involving nested optimization structures. Its fast-growing scale nowadays necessitates efficient distributed algorithms. In conventional distributed bilevel methods, each worker must transmit full-dimensional stochastic gradients to the server every iteration, leading to significant communication overhead and thus hindering efficiency and scalability. To resolve this issue, we introduce the first family of distributed bilevel algorithms with communication compression. The primary challenge in algorithmic development is mitigating bias in hypergradient estimation caused by the nested structure. We first propose C-SOBA, a simple yet effective approach with unbiased compression and provable linear speedup convergence. However, it relies on strong assumptions on bounded gradients. To address this limitation, we explore the use of moving average, error feedback, and multi-step compression in bilevel optimization, resulting in a series of advanced algorithms with relaxed assumptions and improved convergence properties. Numerical experiments show that our compressed bilevel algorithms can achieve $10\\times$ reduction in communication overhead without severe performance degradation.","sentences":["Stochastic bilevel optimization tackles challenges involving nested optimization structures.","Its fast-growing scale nowadays necessitates efficient distributed algorithms.","In conventional distributed bilevel methods, each worker must transmit full-dimensional stochastic gradients to the server every iteration, leading to significant communication overhead and thus hindering efficiency and scalability.","To resolve this issue, we introduce the first family of distributed bilevel algorithms with communication compression.","The primary challenge in algorithmic development is mitigating bias in hypergradient estimation caused by the nested structure.","We first propose C-SOBA, a simple yet effective approach with unbiased compression and provable linear speedup convergence.","However, it relies on strong assumptions on bounded gradients.","To address this limitation, we explore the use of moving average, error feedback, and multi-step compression in bilevel optimization, resulting in a series of advanced algorithms with relaxed assumptions and improved convergence properties.","Numerical experiments show that our compressed bilevel algorithms can achieve $10\\times$ reduction in communication overhead without severe performance degradation."],"url":"http://arxiv.org/abs/2405.18858v1","category":"math.OC"}
{"created":"2024-05-29 02:57:07","title":"Efficient Model-agnostic Alignment via Bayesian Persuasion","abstract":"With recent advancements in large language models (LLMs), alignment has emerged as an effective technique for keeping LLMs consensus with human intent. Current methods primarily involve direct training through Supervised Fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), both of which require substantial computational resources and extensive ground truth data. This paper explores an efficient method for aligning black-box large models using smaller models, introducing a model-agnostic and lightweight Bayesian Persuasion Alignment framework. We formalize this problem as an optimization of the signaling strategy from the small model's perspective. In the persuasion process, the small model (Advisor) observes the information item (i.e., state) and persuades large models (Receiver) to elicit improved responses. The Receiver then generates a response based on the input, the signal from the Advisor, and its updated belief about the information item. Through training using our framework, we demonstrate that the Advisor can significantly enhance the performance of various Receivers across a range of tasks. We theoretically analyze our persuasion framework and provide an upper bound on the Advisor's regret, confirming its effectiveness in learning the optimal signaling strategy. Our Empirical results demonstrates that GPT-2 can significantly improve the performance of various models, achieving an average enhancement of 16.1% in mathematical reasoning ability and 13.7% in code generation. We hope our work can provide an initial step toward rethinking the alignment framework from the Bayesian Persuasion perspective.","sentences":["With recent advancements in large language models (LLMs), alignment has emerged as an effective technique for keeping LLMs consensus with human intent.","Current methods primarily involve direct training through Supervised Fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), both of which require substantial computational resources and extensive ground truth data.","This paper explores an efficient method for aligning black-box large models using smaller models, introducing a model-agnostic and lightweight Bayesian Persuasion Alignment framework.","We formalize this problem as an optimization of the signaling strategy from the small model's perspective.","In the persuasion process, the small model (Advisor) observes the information item (i.e., state) and persuades large models (Receiver) to elicit improved responses.","The Receiver then generates a response based on the input, the signal from the Advisor, and its updated belief about the information item.","Through training using our framework, we demonstrate that the Advisor can significantly enhance the performance of various Receivers across a range of tasks.","We theoretically analyze our persuasion framework and provide an upper bound on the Advisor's regret, confirming its effectiveness in learning the optimal signaling strategy.","Our Empirical results demonstrates that GPT-2 can significantly improve the performance of various models, achieving an average enhancement of 16.1% in mathematical reasoning ability and 13.7% in code generation.","We hope our work can provide an initial step toward rethinking the alignment framework from the Bayesian Persuasion perspective."],"url":"http://arxiv.org/abs/2405.18718v1","category":"cs.CL"}
{"created":"2024-05-29 02:28:24","title":"A simple inverse power method for balanced graph cut","abstract":"The existing inverse power ($\\mathbf{IP}$) method for solving the balanced graph cut lacks local convergence and its inner subproblem requires a nonsmooth convex solver. To address these issues, we develop a simple inverse power ($\\mathbf{SIP}$) method using a novel equivalent continuous formulation of the balanced graph cut, and its inner subproblem allows an explicit analytic solution, which is the biggest advantage over $\\mathbf{IP}$ and constitutes the main reason why we call it $\\mathit{simple}$. By fully exploiting the closed-form of the inner subproblem solution, we design a boundary-detected subgradient selection with which $\\mathbf{SIP}$ is proved to be locally converged. We show that $\\mathbf{SIP}$ is also applicable to a new ternary valued $\\theta$-balanced cut which reduces to the balanced cut when $\\theta=1$. When $\\mathbf{SIP}$ reaches its local optimum, we seamlessly transfer to solve the $\\theta$-balanced cut within exactly the same iteration algorithm framework and thus obtain $\\mathbf{SIP}$-$\\mathbf{perturb}$ -- an efficient local breakout improvement of $\\mathbf{SIP}$, which transforms some ``partitioned\" vertices back to the ``un-partitioned\" ones through the adjustable $\\theta$. Numerical experiments on G-set for Cheeger cut and Sparsest cut demonstrate that $\\mathbf{SIP}$ is significantly faster than $\\mathbf{IP}$ while maintaining approximate solutions of comparable quality, and $\\mathbf{SIP}$-$\\mathbf{perturb}$ outperforms $\\mathtt{Gurobi}$ in terms of both computational cost and solution quality.","sentences":["The existing inverse power ($\\mathbf{IP}$) method for solving the balanced graph cut lacks local convergence and its inner subproblem requires a nonsmooth convex solver.","To address these issues, we develop a simple inverse power ($\\mathbf{SIP}$) method using a novel equivalent continuous formulation of the balanced graph cut, and its inner subproblem allows an explicit analytic solution, which is the biggest advantage over $\\mathbf{IP}$ and constitutes the main reason why we call it $\\mathit{simple}$. By fully exploiting the closed-form of the inner subproblem solution, we design a boundary-detected subgradient selection with which $\\mathbf{SIP}$ is proved to be locally converged.","We show that $\\mathbf{SIP}$ is also applicable to a new ternary valued $\\theta$-balanced cut which reduces to the balanced cut when $\\theta=1$. When $\\mathbf{SIP}$ reaches its local optimum, we seamlessly transfer to solve the $\\theta$-balanced cut within exactly the same iteration algorithm framework and thus obtain $\\mathbf{SIP}$-$\\mathbf{perturb}$ -- an efficient local breakout improvement of $\\mathbf{SIP}$, which transforms some ``partitioned\" vertices back to the ``un-partitioned\" ones through the adjustable $\\theta$. Numerical experiments on G-set for Cheeger cut and Sparsest cut demonstrate that $\\mathbf{SIP}$ is significantly faster than $\\mathbf{IP}$ while maintaining approximate solutions of comparable quality, and $\\mathbf{SIP}$-$\\mathbf{perturb}$ outperforms $\\mathtt{Gurobi}$ in terms of both computational cost and solution quality."],"url":"http://arxiv.org/abs/2405.18705v1","category":"math.OC"}
{"created":"2024-05-29 02:10:35","title":"Data-Efficient Approach to Humanoid Control via Fine-Tuning a Pre-Trained GPT on Action Data","abstract":"There are several challenges in developing a model for multi-tasking humanoid control. Reinforcement learning and imitation learning approaches are quite popular in this domain. However, there is a trade-off between the two. Reinforcement learning is not the best option for training a humanoid to perform multiple behaviors due to training time and model size, and imitation learning using kinematics data alone is not appropriate to realize the actual physics of the motion. Training models to perform multiple complex tasks take long training time due to high DoF and complexities of the movements. Although training models offline would be beneficial, another issue is the size of the dataset, usually being quite large to encapsulate multiple movements. Many papers have implemented state of the art deep learning models such as transformers to control humanoid characters and predict their motion based on a large dataset of recorded/reference motion. In this paper, we train a GPT on a large dataset of noisy expert policy rollout observations from a humanoid motion dataset as a pre-trained model and fine tune that model on a smaller dataset of noisy expert policy rollout observations and actions to autoregressively generate physically plausible motion trajectories. We show that it is possible to train a GPT-based foundation model on a smaller dataset in shorter training time to control a humanoid in a realistic physics environment to perform human-like movements.","sentences":["There are several challenges in developing a model for multi-tasking humanoid control.","Reinforcement learning and imitation learning approaches are quite popular in this domain.","However, there is a trade-off between the two.","Reinforcement learning is not the best option for training a humanoid to perform multiple behaviors due to training time and model size, and imitation learning using kinematics data alone is not appropriate to realize the actual physics of the motion.","Training models to perform multiple complex tasks take long training time due to high DoF and complexities of the movements.","Although training models offline would be beneficial, another issue is the size of the dataset, usually being quite large to encapsulate multiple movements.","Many papers have implemented state of the art deep learning models such as transformers to control humanoid characters and predict their motion based on a large dataset of recorded/reference motion.","In this paper, we train a GPT on a large dataset of noisy expert policy rollout observations from a humanoid motion dataset as a pre-trained model and fine tune that model on a smaller dataset of noisy expert policy rollout observations and actions to autoregressively generate physically plausible motion trajectories.","We show that it is possible to train a GPT-based foundation model on a smaller dataset in shorter training time to control a humanoid in a realistic physics environment to perform human-like movements."],"url":"http://arxiv.org/abs/2405.18695v1","category":"cs.RO"}
{"created":"2024-05-29 02:04:36","title":"Movable Antenna Empowered Downlink NOMA Systems: Power Allocation and Antenna Position Optimization","abstract":"This paper investigates a novel communication paradigm employing movable antennas (MAs) within a multiple-input single-output (MISO) non-orthogonal multiple access (NOMA) downlink framework, where users are equipped with MAs. Initially, leveraging the far-field response, we delineate the channel characteristics concerning both the power allocation coefficient and positions of MAs. Subsequently, we endeavor to maximize the channel capacity by jointly optimizing power allocation and antenna positions. To tackle the resultant non-convex problem, we propose an alternating optimization (AO) scheme underpinned by successive convex approximation (SCA) to converge towards a stationary point. Through numerical simulations, our findings substantiate the superiority of the MA-assisted NOMA system over both orthogonal multiple access (OMA) and conventional NOMA configurations in terms of average sum rate and outage probability.","sentences":["This paper investigates a novel communication paradigm employing movable antennas (MAs) within a multiple-input single-output (MISO) non-orthogonal multiple access (NOMA) downlink framework, where users are equipped with MAs.","Initially, leveraging the far-field response, we delineate the channel characteristics concerning both the power allocation coefficient and positions of MAs.","Subsequently, we endeavor to maximize the channel capacity by jointly optimizing power allocation and antenna positions.","To tackle the resultant non-convex problem, we propose an alternating optimization (AO) scheme underpinned by successive convex approximation (SCA) to converge towards a stationary point.","Through numerical simulations, our findings substantiate the superiority of the MA-assisted NOMA system over both orthogonal multiple access (OMA) and conventional NOMA configurations in terms of average sum rate and outage probability."],"url":"http://arxiv.org/abs/2405.18692v1","category":"cs.IT"}
{"created":"2024-05-29 00:02:03","title":"Refinement of global coronal and interplanetary magnetic field extrapolations constrained by remote-sensing and in-situ observations at the solar minimum","abstract":"Solar magnetic fields are closely related to various physical phenomena on the sun, which can be extrapolated with different models from photospheric magnetograms. However, the Open Flux Problem (OFP), the underestimation of the magnetic field derived from the extrapolated model, is still unsolved. To minimize the impact of the OFP, we propose three evaluation parameters to quantitatively evaluate magnetic field models and determine the optimal free parameters in the models by constraining the coronal magnetic fields (CMFs) and the interplanetary magnetic fields (IMFs) with real observations. Although the OFP still exists, we find that magnetic field lines traced from the coronal models effectively capture the intricate topological configurations observed in the corona, including streamers and plumes. The OFP is lessened by using the HMI synoptic map instead of the GONG daily synoptic maps, and the PFSS+PFCS model instead of the CSSS model. For Carrington Rotation (CR) 2231 at the solar minimum, we suggest that the optimal parameters for the PFSS+PFCS model are $R_{\\mathrm{ss}} = 2.2-2.5\\ R_{\\mathrm{sun}}$ and $R_{\\mathrm{scs}} = 10.5-14.0\\ R_{\\mathrm{sun}}$, as well as for the CSSS model are $R_{\\mathrm{cs}} = 2.0 - 2.4\\ R_{\\mathrm{sun}}$, $R_{\\mathrm{ss}} = 11.0 - 14.7\\ R_{\\mathrm{sun}}$ and $a = 1.0\\ R_{\\mathrm{sun}}$. Despite the IMFs at 1 AU being consistent with the measurements by artificially increasing the polar magnetic fields, the IMFs near the sun are still underestimated. The OFP might be advanced by improving the accuracy of both the weak magnetic fields and polar magnetic fields, especially considering magnetic activities arising from interplanetary physical processes.","sentences":["Solar magnetic fields are closely related to various physical phenomena on the sun, which can be extrapolated with different models from photospheric magnetograms.","However, the Open Flux Problem (OFP), the underestimation of the magnetic field derived from the extrapolated model, is still unsolved.","To minimize the impact of the OFP, we propose three evaluation parameters to quantitatively evaluate magnetic field models and determine the optimal free parameters in the models by constraining the coronal magnetic fields (CMFs) and the interplanetary magnetic fields (IMFs) with real observations.","Although the OFP still exists, we find that magnetic field lines traced from the coronal models effectively capture the intricate topological configurations observed in the corona, including streamers and plumes.","The OFP is lessened by using the HMI synoptic map instead of the GONG daily synoptic maps, and the PFSS+PFCS model instead of the CSSS model.","For Carrington Rotation (CR) 2231 at the solar minimum, we suggest that the optimal parameters for the PFSS+PFCS model are $R_{\\mathrm{ss}} = 2.2-2.5\\ R_{\\mathrm{sun}}$ and $R_{\\mathrm{scs}} = 10.5-14.0\\ R_{\\mathrm{sun}}$, as well as for the CSSS model are $R_{\\mathrm{cs}} = 2.0 - 2.4\\ R_{\\mathrm{sun}}$, $R_{\\mathrm{ss}} = 11.0 - 14.7\\ R_{\\mathrm{sun}}$ and $a = 1.0\\ R_{\\mathrm{sun}}$. Despite the IMFs at 1 AU being consistent with the measurements by artificially increasing the polar magnetic fields, the IMFs near the sun are still underestimated.","The OFP might be advanced by improving the accuracy of both the weak magnetic fields and polar magnetic fields, especially considering magnetic activities arising from interplanetary physical processes."],"url":"http://arxiv.org/abs/2405.18665v1","category":"astro-ph.SR"}
{"created":"2024-05-28 23:11:09","title":"Thermodynamic compatibility of actives encapsulated into PEG-PLA nanoparticles: In Silico predictions and experimental verification","abstract":"Achieving optimal solubility of active substances in polymeric carriers is of fundamental importance for a number of industrial applications, including targeted drug delivery within the growing field of nanomedicine. However, its experimental optimization using a trial-and-error approach is cumbersome and time-consuming. Here, an approach based on molecular dynamics (MD) simulations and the Flory-Huggins theory is proposed for rapid prediction of thermodynamic compatibility between active species and copolymers comprising hydrophilic and hydrophobic segments. In contrast to similar methods, our approach offers high computational efficiency by employing MD simulations that avoid explicit consideration of the actual copolymer chains. The accuracy of the method is demonstrated for compatibility predictions between pyrene and nile red as model dyes as well as indomethacin as model drug and copolymers containing blocks of poly(ethylene glycol) and poly(lactic acid) in different ratios. The results of the simulations are directly verified by comparison with the observed encapsulation efficiency of nanoparticles prepared by nanoprecipitation.","sentences":["Achieving optimal solubility of active substances in polymeric carriers is of fundamental importance for a number of industrial applications, including targeted drug delivery within the growing field of nanomedicine.","However, its experimental optimization using a trial-and-error approach is cumbersome and time-consuming.","Here, an approach based on molecular dynamics (MD) simulations and the Flory-Huggins theory is proposed for rapid prediction of thermodynamic compatibility between active species and copolymers comprising hydrophilic and hydrophobic segments.","In contrast to similar methods, our approach offers high computational efficiency by employing MD simulations that avoid explicit consideration of the actual copolymer chains.","The accuracy of the method is demonstrated for compatibility predictions between pyrene and nile red as model dyes as well as indomethacin as model drug and copolymers containing blocks of poly(ethylene glycol) and poly(lactic acid) in different ratios.","The results of the simulations are directly verified by comparison with the observed encapsulation efficiency of nanoparticles prepared by nanoprecipitation."],"url":"http://arxiv.org/abs/2405.18646v1","category":"cond-mat.soft"}
{"created":"2024-05-28 22:53:43","title":"Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning","abstract":"Recent studies show that Large Language Models (LLMs) with safety alignment can be jail-broken by fine-tuning on a dataset mixed with harmful data. First time in the literature, we show that the jail-broken effect can be mitigated by separating states in the finetuning stage to optimize the alignment and user datasets. Unfortunately, our subsequent study shows that this simple Bi-State Optimization (BSO) solution experiences convergence instability when steps invested in its alignment state is too small, leading to downgraded alignment performance. By statistical analysis, we show that the \\textit{excess drift} towards consensus could be a probable reason for the instability. To remedy this issue, we propose \\textbf{L}azy(\\textbf{i}) \\textbf{s}afety \\textbf{a}lignment (\\textbf{Lisa}), which introduces a proximal term to constraint the drift of each state. Theoretically, the benefit of the proximal term is supported by the convergence analysis, wherein we show that a sufficient large proximal factor is necessary to guarantee Lisa's convergence. Empirically, our results on four downstream finetuning tasks show that Lisa with a proximal term can significantly increase alignment performance while maintaining the LLM's accuracy on the user tasks. Code is available at \\url{https://github.com/git-disl/Lisa}.","sentences":["Recent studies show that Large Language Models (LLMs) with safety alignment can be jail-broken by fine-tuning on a dataset mixed with harmful data.","First time in the literature, we show that the jail-broken effect can be mitigated by separating states in the finetuning stage to optimize the alignment and user datasets.","Unfortunately, our subsequent study shows that this simple Bi-State Optimization (BSO) solution experiences convergence instability when steps invested in its alignment state is too small, leading to downgraded alignment performance.","By statistical analysis, we show that the \\textit{excess drift} towards consensus could be a probable reason for the instability.","To remedy this issue, we propose \\textbf{L}azy(\\textbf{i}) \\textbf{s}afety \\textbf{a}lignment (\\textbf{Lisa}), which introduces a proximal term to constraint the drift of each state.","Theoretically, the benefit of the proximal term is supported by the convergence analysis, wherein we show that a sufficient large proximal factor is necessary to guarantee Lisa's convergence.","Empirically, our results on four downstream finetuning tasks show that Lisa with a proximal term can significantly increase alignment performance while maintaining the LLM's accuracy on the user tasks.","Code is available at \\url{https://github.com/git-disl/Lisa}."],"url":"http://arxiv.org/abs/2405.18641v1","category":"cs.LG"}
{"created":"2024-05-28 22:25:21","title":"A linear bound for the size of the finite terminal assembly of a directed non-cooperative tile assembly system","abstract":"The abstract tile assembly model (aTam) is a model of DNA self-assembly. Most of the studies focus on cooperative aTam where a form of synchronization between the tiles is possible. Simulating Turing machines is achievable in this context. Few results and constructions are known for the non-cooperative case (a variant of Wang tilings where assemblies do not need to cover the whole plane and some mismatches may occur).   Introduced by P.E. Meunier and D. Regnault, efficient paths are a non-trivial construction for non-cooperative aTam. These paths of width nlog(n) are designed with n different tile types. Assembling them relies heavily on a form of ``non-determinism''. Indeed, the set of tiles may produced different finite terminal assemblies but they all contain the same efficient path. Directed non-cooperative aTam does not allow this non-determinism as only one assembly may be produced by a tile assembly system. This variant of aTam is the only one who was shown to be decidable.   In this paper, we show that if the terminal assembly of a directed non-cooperative tile assembly system is finite then its width and length are of linear size according to the size of the tile assembly system. This result implies that the construction of efficient paths cannot be generalized to the directed case and that some computation must rely on a competition between different paths. It also implies that the construction of a square of width n using 2n-1 tiles types is asymptotically optimal. Moreover, we hope that the techniques introduced here will lead to a better comprehension of the non-directed case.","sentences":["The abstract tile assembly model (aTam) is a model of DNA self-assembly.","Most of the studies focus on cooperative aTam where a form of synchronization between the tiles is possible.","Simulating Turing machines is achievable in this context.","Few results and constructions are known for the non-cooperative case (a variant of Wang tilings where assemblies do not need to cover the whole plane and some mismatches may occur).   ","Introduced by P.E. Meunier and D. Regnault, efficient paths are a non-trivial construction for non-cooperative aTam.","These paths of width nlog(n) are designed with n different tile types.","Assembling them relies heavily on a form of ``non-determinism''.","Indeed, the set of tiles may produced different finite terminal assemblies but they all contain the same efficient path.","Directed non-cooperative aTam does not allow this non-determinism as only one assembly may be produced by a tile assembly system.","This variant of aTam is the only one who was shown to be decidable.   ","In this paper, we show that if the terminal assembly of a directed non-cooperative tile assembly system is finite then its width and length are of linear size according to the size of the tile assembly system.","This result implies that the construction of efficient paths cannot be generalized to the directed case and that some computation must rely on a competition between different paths.","It also implies that the construction of a square of width n using 2n-1 tiles types is asymptotically optimal.","Moreover, we hope that the techniques introduced here will lead to a better comprehension of the non-directed case."],"url":"http://arxiv.org/abs/2405.18630v1","category":"cs.CC"}
{"created":"2024-05-28 21:50:34","title":"Unraveling the Spin-to-Charge Current Conversion Mechanism and Charge Transfer Dynamics at Interface of Graphene/WS$_2$ Heterostructures at Room Temperature","abstract":"We report experimental investigations of spin-to-charge current conversion and charge transfer dynamics (CT) at the interface of graphene/WS$_2$ van der Waals heterostructure. Pure spin current was produced by the spin precession in the microwave-driven ferromagnetic resonance of a permalloy film (Py-Ni$_{81}$Fe$_{19}$) and injected into the graphene/WS$_2$ heterostructure through the spin pumping process. The observed spin-to-charge current conversion in the heterostructure is attributed to inverse Rashba-Edelstein effect (IREE) at the graphene/WS$_2$ interface. Interfacial CT dynamics in this heterostructure was investigated based on the framework of core-hole-clock (CHC) approach. The results obtained from spin pumping and CHC studies show that the spin-to-charge current conversion and charge transfer process are more efficient in the graphene/WS$_2$ heterostructure compared to isolated WS2 and graphene films. The results show that the presence of WS$_2$ flakes improves the current conversion efficiency. These experimental results are corroborated by density functional theory (DFT) calculations, which reveal (i) Rashba spin-orbit splitting of graphene orbitals and (ii) electronic coupling between graphene and WS$_2$ orbitals. This study provides valuable insights for optimizing the design and performance of spintronic devices.","sentences":["We report experimental investigations of spin-to-charge current conversion and charge transfer dynamics (CT) at the interface of graphene/WS$_2$ van der Waals heterostructure.","Pure spin current was produced by the spin precession in the microwave-driven ferromagnetic resonance of a permalloy film (Py-Ni$_{81}$Fe$_{19}$) and injected into the graphene/WS$_2$ heterostructure through the spin pumping process.","The observed spin-to-charge current conversion in the heterostructure is attributed to inverse Rashba-Edelstein effect (IREE) at the graphene/WS$_2$ interface.","Interfacial CT dynamics in this heterostructure was investigated based on the framework of core-hole-clock (CHC) approach.","The results obtained from spin pumping and CHC studies show that the spin-to-charge current conversion and charge transfer process are more efficient in the graphene/WS$_2$ heterostructure compared to isolated WS2 and graphene films.","The results show that the presence of WS$_2$ flakes improves the current conversion efficiency.","These experimental results are corroborated by density functional theory (DFT) calculations, which reveal (i) Rashba spin-orbit splitting of graphene orbitals and (ii) electronic coupling between graphene and WS$_2$ orbitals.","This study provides valuable insights for optimizing the design and performance of spintronic devices."],"url":"http://arxiv.org/abs/2405.18617v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-28 21:37:10","title":"Estimates for smooth Weyl sums on major arcs","abstract":"We present estimates for smooth Weyl sums of use on sets of major arcs in applications of the Hardy-Littlewood method. In particular, we derive mean value estimates on major arcs for smooth Weyl sums of degree $k$ delivering essentially optimal bounds for moments of order $u$ whenever $u>2\\lfloor k/2\\rfloor +4$.","sentences":["We present estimates for smooth Weyl sums of use on sets of major arcs in applications of the Hardy-Littlewood method.","In particular, we derive mean value estimates on major arcs for smooth Weyl sums of degree $k$ delivering essentially optimal bounds for moments of order $u$ whenever $u>2\\lfloor k/2\\rfloor +4$."],"url":"http://arxiv.org/abs/2405.18608v1","category":"math.NT"}
{"created":"2024-05-28 21:26:51","title":"Anomalous and linear holographic hard wall models for light unflavored mesons","abstract":"In this work we consider anomalous and linear holographic hard wall (HW) models for light unflavored mesons inspired by the AdS/CFT correspondence. The anomalous dimensions depend on the logarithm of the spin $S$ of the meson state and come from a semiclassical analysis of gauge/string duality. The anomalous HW model produces very good masses and good Regge trajectories for mesons compared with PDG data. Inspired by this anomalous HW model we also propose a phenomenological modification of the dimension of the boundary operators such that the model produces asymptotic linear Regge trajectories.","sentences":["In this work we consider anomalous and linear holographic hard wall (HW) models for light unflavored mesons inspired by the AdS/CFT correspondence.","The anomalous dimensions depend on the logarithm of the spin $S$ of the meson state and come from a semiclassical analysis of gauge/string duality.","The anomalous HW model produces very good masses and good Regge trajectories for mesons compared with PDG data.","Inspired by this anomalous HW model we also propose a phenomenological modification of the dimension of the boundary operators such that the model produces asymptotic linear Regge trajectories."],"url":"http://arxiv.org/abs/2405.18599v1","category":"hep-ph"}
{"created":"2024-05-28 20:52:46","title":"Single-loop Stochastic Algorithms for Difference of Max-Structured Weakly Convex Functions","abstract":"In this paper, we study a class of non-smooth non-convex problems in the form of $\\min_{x}[\\max_{y\\in Y}\\phi(x, y) - \\max_{z\\in Z}\\psi(x, z)]$, where both $\\Phi(x) = \\max_{y\\in Y}\\phi(x, y)$ and $\\Psi(x)=\\max_{z\\in Z}\\psi(x, z)$ are weakly convex functions, and $\\phi(x, y), \\psi(x, z)$ are strongly concave functions in terms of $y$ and $z$, respectively. It covers two families of problems that have been studied but are missing single-loop stochastic algorithms, i.e., difference of weakly convex functions and weakly convex strongly-concave min-max problems. We propose a stochastic Moreau envelope approximate gradient method dubbed SMAG, the first single-loop algorithm for solving these problems, and provide a state-of-the-art non-asymptotic convergence rate. The key idea of the design is to compute an approximate gradient of the Moreau envelopes of $\\Phi, \\Psi$ using only one step of stochastic gradient update of the primal and dual variables. Empirically, we conduct experiments on positive-unlabeled (PU) learning and partial area under ROC curve (pAUC) optimization with an adversarial fairness regularizer to validate the effectiveness of our proposed algorithms.","sentences":["In this paper, we study a class of non-smooth non-convex problems in the form of $\\min_{x}[\\max_{y\\in Y}\\phi(x, y) - \\max_{z\\in Z}\\psi(x, z)]$, where both $\\Phi(x) = \\max_{y\\in Y}\\phi(x, y)$ and $\\Psi(x)=\\max_{z\\in Z}\\psi(x, z)$ are weakly convex functions, and $\\phi(x, y), \\psi(x, z)$ are strongly concave functions in terms of $y$ and $z$, respectively.","It covers two families of problems that have been studied but are missing single-loop stochastic algorithms, i.e., difference of weakly convex functions and weakly convex strongly-concave min-max problems.","We propose a stochastic Moreau envelope approximate gradient method dubbed SMAG, the first single-loop algorithm for solving these problems, and provide a state-of-the-art non-asymptotic convergence rate.","The key idea of the design is to compute an approximate gradient of the Moreau envelopes of $\\Phi, \\Psi$ using only one step of stochastic gradient update of the primal and dual variables.","Empirically, we conduct experiments on positive-unlabeled (PU) learning and partial area under ROC curve (pAUC) optimization with an adversarial fairness regularizer to validate the effectiveness of our proposed algorithms."],"url":"http://arxiv.org/abs/2405.18577v1","category":"math.OC"}
{"created":"2024-05-28 20:26:05","title":"Warm-starting Push-Relabel","abstract":"Push-Relabel is one of the most celebrated network flow algorithms. Maintaining a pre-flow that saturates a cut, it enjoys better theoretical and empirical running time than other flow algorithms, such as Ford-Fulkerson. In practice, Push-Relabel is even faster than what theoretical guarantees can promise, in part because of the use of good heuristics for seeding and updating the iterative algorithm. However, it remains unclear how to run Push-Relabel on an arbitrary initialization that is not necessarily a pre-flow or cut-saturating. We provide the first theoretical guarantees for warm-starting Push-Relabel with a predicted flow, where our learning-augmented version benefits from fast running time when the predicted flow is close to an optimal flow, while maintaining robust worst-case guarantees. Interestingly, our algorithm uses the gap relabeling heuristic, which has long been employed in practice, even though prior to our work there was no rigorous theoretical justification for why it can lead to run-time improvements. We then provide experiments that show our warm-started Push-Relabel also works well in practice.","sentences":["Push-Relabel is one of the most celebrated network flow algorithms.","Maintaining a pre-flow that saturates a cut, it enjoys better theoretical and empirical running time than other flow algorithms, such as Ford-Fulkerson.","In practice, Push-Relabel is even faster than what theoretical guarantees can promise, in part because of the use of good heuristics for seeding and updating the iterative algorithm.","However, it remains unclear how to run Push-Relabel on an arbitrary initialization that is not necessarily a pre-flow or cut-saturating.","We provide the first theoretical guarantees for warm-starting Push-Relabel with a predicted flow, where our learning-augmented version benefits from fast running time when the predicted flow is close to an optimal flow, while maintaining robust worst-case guarantees.","Interestingly, our algorithm uses the gap relabeling heuristic, which has long been employed in practice, even though prior to our work there was no rigorous theoretical justification for why it can lead to run-time improvements.","We then provide experiments that show our warm-started Push-Relabel also works well in practice."],"url":"http://arxiv.org/abs/2405.18568v1","category":"cs.DS"}
{"created":"2024-05-28 20:20:41","title":"A faster heuristic for the Traveling Salesman Problem with Drone","abstract":"Given a set of customers, the Flying Sidekick Traveling Salesman Problem (FSTSP) consists of using one truck and one drone to perform deliveries to them. The drone is limited to delivering to one customer at a time, after which it returns to the truck, from where it can be launched again. The goal is to minimize the time required to service all customers and return both vehicles to the depot. In the literature, we can find heuristics for this problem that follow the order-first split-second approach: find a Hamiltonian cycle h with all customers, and then remove some customers to be handled by the drone while deciding from where the drone will be launched and where it will be retrieved. Indeed, they optimally solve the h-FSTSP, which is a variation that consists of solving the FSTSP while respecting a given initial cycle h. We present the Lazy Drone Property, which guarantees that only some combinations of nodes for launch and retrieval of the drone need to be considered by algorithms for the h-FSTSP. We also present an algorithm that uses the property, and we show experimental results which corroborate its effectiveness in decreasing the running time of such algorithms. Our algorithm was shown to be more than 84 times faster than the previously best-known ones over the literature benchmark. Moreover, on average, it considered a number of launch and retrieval pairs that is linear on the number of customers, indicating that the algorithm's performance should be sustainable for larger instances.","sentences":["Given a set of customers, the Flying Sidekick Traveling Salesman Problem (FSTSP) consists of using one truck and one drone to perform deliveries to them.","The drone is limited to delivering to one customer at a time, after which it returns to the truck, from where it can be launched again.","The goal is to minimize the time required to service all customers and return both vehicles to the depot.","In the literature, we can find heuristics for this problem that follow the order-first split-second approach: find a Hamiltonian cycle h with all customers, and then remove some customers to be handled by the drone while deciding from where the drone will be launched and where it will be retrieved.","Indeed, they optimally solve the h-FSTSP, which is a variation that consists of solving the FSTSP while respecting a given initial cycle h. We present the Lazy Drone Property, which guarantees that only some combinations of nodes for launch and retrieval of the drone need to be considered by algorithms for the h-FSTSP.","We also present an algorithm that uses the property, and we show experimental results which corroborate its effectiveness in decreasing the running time of such algorithms.","Our algorithm was shown to be more than 84 times faster than the previously best-known ones over the literature benchmark.","Moreover, on average, it considered a number of launch and retrieval pairs that is linear on the number of customers, indicating that the algorithm's performance should be sustainable for larger instances."],"url":"http://arxiv.org/abs/2405.18566v1","category":"cs.DS"}
{"created":"2024-05-28 20:19:38","title":"Video2MR: Automatically Generating Mixed Reality 3D Instructions by Augmenting Extracted Motion from 2D Videos","abstract":"This paper introduces Video2MR, a mixed reality system that automatically generates 3D sports and exercise instructions from 2D videos. Mixed reality instructions have great potential for physical training, but existing works require substantial time and cost to create these 3D experiences. Video2MR overcomes this limitation by transforming arbitrary instructional videos available online into MR 3D avatars with AI-enabled motion capture (DeepMotion). Then, it automatically enhances the avatar motion through the following augmentation techniques: 1) contrasting and highlighting differences between the user and avatar postures, 2) visualizing key trajectories and movements of specific body parts, 3) manipulation of time and speed using body motion, and 4) spatially repositioning avatars for different perspectives. Developed on Hololens 2 and Azure Kinect, we showcase various use cases, including yoga, dancing, soccer, tennis, and other physical exercises. The study results confirm that Video2MR provides more engaging and playful learning experiences, compared to existing 2D video instructions.","sentences":["This paper introduces Video2MR, a mixed reality system that automatically generates 3D sports and exercise instructions from 2D videos.","Mixed reality instructions have great potential for physical training, but existing works require substantial time and cost to create these 3D experiences.","Video2MR overcomes this limitation by transforming arbitrary instructional videos available online into MR 3D avatars with AI-enabled motion capture (DeepMotion).","Then, it automatically enhances the avatar motion through the following augmentation techniques: 1) contrasting and highlighting differences between the user and avatar postures, 2) visualizing key trajectories and movements of specific body parts, 3) manipulation of time and speed using body motion, and 4) spatially repositioning avatars for different perspectives.","Developed on Hololens 2 and Azure Kinect, we showcase various use cases, including yoga, dancing, soccer, tennis, and other physical exercises.","The study results confirm that Video2MR provides more engaging and playful learning experiences, compared to existing 2D video instructions."],"url":"http://arxiv.org/abs/2405.18565v1","category":"cs.HC"}
{"created":"2024-05-28 20:06:01","title":"Improving cosmological analyses of HI clustering by reducing stochastic noise","abstract":"High-number-density tracers of large-scale structure, such as the HI-rich galaxies measured by 21 cm intensity mapping, have low sampling noise, making them particularly promising as cosmological probes. At large scales, this sampling noise can be subdominant to other scale-independent contributions to the power spectrum, arising from nonlinear bias. This has important consequences for cosmological constraints obtained from such tracers, since it indicates that using the power spectrum does not lead to optimal constraints even in the linear regime. In this paper, we provide a conservative estimate of the possible improvement in constraining power of a 21cm survey if one were to use an optimal analysis strategy (such as field-level analysis), where only the true sampling noise enters the error budget. We find that improvements in uncertainties on some cosmological parameters can be as large as 50%, depending on redshift, foreground cleaning efficiency, scales used in the analysis, and instrumental noise. One byproduct of our work is measurements of bias parameters and stochasticity for neutral hydrogen in the IllustrisTNG simulation over a wide range of redshifts; we provide simple fitting formulas for these measurements. Our results motivate further exploration of new optimal analysis techniques and provide important insights into the constraining power of current and future 21 cm surveys.","sentences":["High-number-density tracers of large-scale structure, such as the HI-rich galaxies measured by 21 cm intensity mapping, have low sampling noise, making them particularly promising as cosmological probes.","At large scales, this sampling noise can be subdominant to other scale-independent contributions to the power spectrum, arising from nonlinear bias.","This has important consequences for cosmological constraints obtained from such tracers, since it indicates that using the power spectrum does not lead to optimal constraints even in the linear regime.","In this paper, we provide a conservative estimate of the possible improvement in constraining power of a 21cm survey if one were to use an optimal analysis strategy (such as field-level analysis), where only the true sampling noise enters the error budget.","We find that improvements in uncertainties on some cosmological parameters can be as large as 50%, depending on redshift, foreground cleaning efficiency, scales used in the analysis, and instrumental noise.","One byproduct of our work is measurements of bias parameters and stochasticity for neutral hydrogen in the IllustrisTNG simulation over a wide range of redshifts; we provide simple fitting formulas for these measurements.","Our results motivate further exploration of new optimal analysis techniques and provide important insights into the constraining power of current and future 21 cm surveys."],"url":"http://arxiv.org/abs/2405.18559v1","category":"astro-ph.CO"}
{"created":"2024-05-28 19:36:55","title":"Learning from Uncertain Data: From Possible Worlds to Possible Models","abstract":"We introduce an efficient method for learning linear models from uncertain data, where uncertainty is represented as a set of possible variations in the data, leading to predictive multiplicity. Our approach leverages abstract interpretation and zonotopes, a type of convex polytope, to compactly represent these dataset variations, enabling the symbolic execution of gradient descent on all possible worlds simultaneously. We develop techniques to ensure that this process converges to a fixed point and derive closed-form solutions for this fixed point. Our method provides sound over-approximations of all possible optimal models and viable prediction ranges. We demonstrate the effectiveness of our approach through theoretical and empirical analysis, highlighting its potential to reason about model and prediction uncertainty due to data quality issues in training data.","sentences":["We introduce an efficient method for learning linear models from uncertain data, where uncertainty is represented as a set of possible variations in the data, leading to predictive multiplicity.","Our approach leverages abstract interpretation and zonotopes, a type of convex polytope, to compactly represent these dataset variations, enabling the symbolic execution of gradient descent on all possible worlds simultaneously.","We develop techniques to ensure that this process converges to a fixed point and derive closed-form solutions for this fixed point.","Our method provides sound over-approximations of all possible optimal models and viable prediction ranges.","We demonstrate the effectiveness of our approach through theoretical and empirical analysis, highlighting its potential to reason about model and prediction uncertainty due to data quality issues in training data."],"url":"http://arxiv.org/abs/2405.18549v1","category":"cs.LG"}
{"created":"2024-05-28 19:22:16","title":"Novel materials for next-generation accelerator target facilities","abstract":"As beam power continues to increase in next-generation accelerator facilities, high-power target systems face crucial challenges. Components like beam windows and particle-production targets must endure significantly higher levels of particle fluence. The primary beam's energy deposition causes rapid heating (thermal shock) and induces microstructural changes (radiation damage) within the target material. These effects ultimately deteriorate the components' properties and lifespan. With conventional materials already stretched to their limits, we are exploring novel materials including High-Entropy Alloys and Electrospun Nanofibers that offer a fresh approach to enhancing tolerance against thermal shock and radiation damage. Following an introduction to the challenges facing high-power target systems, we will give an overview of the promising advancements we have made so far in customizing the compositions and microstructures of these pioneering materials. Our focus is on optimizing their in-beam thermomechanical and physics performance. Additionally, we will outline our ongoing plans for in-beam irradiation experiments and advanced material characterizations. The primary goal of this research is to push the frontiers of target materials, thereby enabling future multi-MW facilities that will benefit various programs in high-energy physics and beyond.","sentences":["As beam power continues to increase in next-generation accelerator facilities, high-power target systems face crucial challenges.","Components like beam windows and particle-production targets must endure significantly higher levels of particle fluence.","The primary beam's energy deposition causes rapid heating (thermal shock) and induces microstructural changes (radiation damage) within the target material.","These effects ultimately deteriorate the components' properties and lifespan.","With conventional materials already stretched to their limits, we are exploring novel materials including High-Entropy Alloys and Electrospun Nanofibers that offer a fresh approach to enhancing tolerance against thermal shock and radiation damage.","Following an introduction to the challenges facing high-power target systems, we will give an overview of the promising advancements we have made so far in customizing the compositions and microstructures of these pioneering materials.","Our focus is on optimizing their in-beam thermomechanical and physics performance.","Additionally, we will outline our ongoing plans for in-beam irradiation experiments and advanced material characterizations.","The primary goal of this research is to push the frontiers of target materials, thereby enabling future multi-MW facilities that will benefit various programs in high-energy physics and beyond."],"url":"http://arxiv.org/abs/2405.18545v1","category":"physics.acc-ph"}
{"created":"2024-05-28 19:11:08","title":"A Framework for Balancing Power Grid Efficiency and Risk with Bi-objective Stochastic Integer Optimization","abstract":"Power grid expansion planning requires making large investment decisions in the present that will impact the future cost and reliability of a system exposed to wide-ranging uncertainties. Extreme temperatures can pose significant challenges to providing power by increasing demand and decreasing supply and have contributed to recent major power outages. We propose to address a modeling challenge of such high-impact, low-frequency events with a bi-objective stochastic integer optimization model that finds solutions with different trade-offs between efficiency in normal conditions and risk to extreme events. We propose a conditional sampling approach paired with a risk measure to address the inherent challenge in approximating the risk of low-frequency events within a sampling based approach. We present a model for spatially correlated, county-specific temperatures and a method to generate both unconditional and conditionally extreme temperature samples from this model efficiently. These models are investigated within an extensive case study with realistic data that demonstrates the effectiveness of the bi-objective approach and the conditional sampling technique. We find that spatial correlations in the temperature samples are essential to finding good solutions and that modeling generator temperature dependence is an important consideration for finding efficient, low-risk solutions.","sentences":["Power grid expansion planning requires making large investment decisions in the present that will impact the future cost and reliability of a system exposed to wide-ranging uncertainties.","Extreme temperatures can pose significant challenges to providing power by increasing demand and decreasing supply and have contributed to recent major power outages.","We propose to address a modeling challenge of such high-impact, low-frequency events with a bi-objective stochastic integer optimization model that finds solutions with different trade-offs between efficiency in normal conditions and risk to extreme events.","We propose a conditional sampling approach paired with a risk measure to address the inherent challenge in approximating the risk of low-frequency events within a sampling based approach.","We present a model for spatially correlated, county-specific temperatures and a method to generate both unconditional and conditionally extreme temperature samples from this model efficiently.","These models are investigated within an extensive case study with realistic data that demonstrates the effectiveness of the bi-objective approach and the conditional sampling technique.","We find that spatial correlations in the temperature samples are essential to finding good solutions and that modeling generator temperature dependence is an important consideration for finding efficient, low-risk solutions."],"url":"http://arxiv.org/abs/2405.18538v1","category":"math.OC"}
{"created":"2024-05-28 19:02:30","title":"Individualized Privacy Accounting via Subsampling with Applications in Combinatorial Optimization","abstract":"In this work, we give a new technique for analyzing individualized privacy accounting via the following simple observation: if an algorithm is one-sided add-DP, then its subsampled variant satisfies two-sided DP. From this, we obtain several improved algorithms for private combinatorial optimization problems, including decomposable submodular maximization and set cover. Our error guarantees are asymptotically tight and our algorithm satisfies pure-DP while previously known algorithms (Gupta et al., 2010; Chaturvedi et al., 2021) are approximate-DP. We also show an application of our technique beyond combinatorial optimization by giving a pure-DP algorithm for the shifting heavy hitter problem in a stream; previously, only an approximateDP algorithm was known (Kaplan et al., 2021; Cohen & Lyu, 2023).","sentences":["In this work, we give a new technique for analyzing individualized privacy accounting via the following simple observation: if an algorithm is one-sided add-DP, then its subsampled variant satisfies two-sided DP.","From this, we obtain several improved algorithms for private combinatorial optimization problems, including decomposable submodular maximization and set cover.","Our error guarantees are asymptotically tight and our algorithm satisfies pure-DP while previously known algorithms (Gupta et al., 2010; Chaturvedi et al., 2021) are approximate-DP.","We also show an application of our technique beyond combinatorial optimization by giving a pure-DP algorithm for the shifting heavy hitter problem in a stream; previously, only an approximateDP algorithm was known (Kaplan et al., 2021; Cohen & Lyu, 2023)."],"url":"http://arxiv.org/abs/2405.18534v1","category":"cs.DS"}
{"created":"2024-05-28 18:55:44","title":"Automatic Forward Model Parameterization with Bayesian Inference of Conformational Populations","abstract":"To quantify how well theoretical predictions of structural ensembles agree with experimental measurements, we depend on the accuracy of forward models. These models are computational frameworks that generate observable quantities from molecular configurations based on empirical relationships linking specific molecular properties to experimental measurements. Bayesian Inference of Conformational Populations (BICePs) is a reweighting algorithm that reconciles simulated ensembles with ensemble-averaged experimental observations, even when such observations are sparse and/or noisy. This is achieved by sampling the posterior distribution of conformational populations under experimental restraints as well as sampling the posterior distribution of uncertainties due to random and systematic error. In this study, we enhance the algorithm for the refinement of empirical forward model (FM) parameters. We introduce and evaluate two novel methods for optimizing FM parameters. The first method treats FM parameters as nuisance parameters, integrating over them in the full posterior distribution. The second method employs variational minimization of a quantity called the BICePs score that reports the free energy of `turning on` the experimental restraints. This technique, coupled with improved likelihood functions for handling experimental outliers, facilitates force field validation and optimization, as illustrated in recent studies (Raddi et al. 2023, 2024). Using this approach, we refine parameters that modulate the Karplus relation, crucial for accurate predictions of J-coupling constants based on dihedral angles between interacting nuclei. We validate this approach first with a toy model system, and then for human ubiquitin, predicting six sets of Karplus parameters. This approach, which does not rely on predetermined parameters, enhances predictive accuracy and can be used for many applications.","sentences":["To quantify how well theoretical predictions of structural ensembles agree with experimental measurements, we depend on the accuracy of forward models.","These models are computational frameworks that generate observable quantities from molecular configurations based on empirical relationships linking specific molecular properties to experimental measurements.","Bayesian Inference of Conformational Populations (BICePs) is a reweighting algorithm that reconciles simulated ensembles with ensemble-averaged experimental observations, even when such observations are sparse and/or noisy.","This is achieved by sampling the posterior distribution of conformational populations under experimental restraints as well as sampling the posterior distribution of uncertainties due to random and systematic error.","In this study, we enhance the algorithm for the refinement of empirical forward model (FM) parameters.","We introduce and evaluate two novel methods for optimizing FM parameters.","The first method treats FM parameters as nuisance parameters, integrating over them in the full posterior distribution.","The second method employs variational minimization of a quantity called the BICePs score that reports the free energy of `turning on` the experimental restraints.","This technique, coupled with improved likelihood functions for handling experimental outliers, facilitates force field validation and optimization, as illustrated in recent studies (Raddi et al. 2023, 2024).","Using this approach, we refine parameters that modulate the Karplus relation, crucial for accurate predictions of J-coupling constants based on dihedral angles between interacting nuclei.","We validate this approach first with a toy model system, and then for human ubiquitin, predicting six sets of Karplus parameters.","This approach, which does not rely on predetermined parameters, enhances predictive accuracy and can be used for many applications."],"url":"http://arxiv.org/abs/2405.18532v1","category":"physics.bio-ph"}
{"created":"2024-05-28 18:39:54","title":"Falsifiable Test Design in Coordination Games","abstract":"A principal can propose a project to an agent, who then decides whether to accept. Their payoffs from launching the project depend on an unknown binary state. The principal can obtain more precise information about the state through a test at no cost, but crucially, it is common knowledge that she can falsify the test result. In the most interesting case where players have conflicted interests, the optimal test is a binary lemon-detecting test. We also find that coordination is possible when the principal is pessimistic but not when the agent is pessimistic. Moreover, when the agent has private information about the state, a single binary lemon-detecting test remains optimal even though the principal has the option to screen the agent by providing a menu of tests. Our finding is consistent with observed tests in real practice.","sentences":["A principal can propose a project to an agent, who then decides whether to accept.","Their payoffs from launching the project depend on an unknown binary state.","The principal can obtain more precise information about the state through a test at no cost, but crucially, it is common knowledge that she can falsify the test result.","In the most interesting case where players have conflicted interests, the optimal test is a binary lemon-detecting test.","We also find that coordination is possible when the principal is pessimistic but not when the agent is pessimistic.","Moreover, when the agent has private information about the state, a single binary lemon-detecting test remains optimal even though the principal has the option to screen the agent by providing a menu of tests.","Our finding is consistent with observed tests in real practice."],"url":"http://arxiv.org/abs/2405.18521v1","category":"econ.TH"}
{"created":"2024-05-28 18:14:52","title":"SoundCTM: Uniting Score-based and Consistency Models for Text-to-Sound Generation","abstract":"Sound content is an indispensable element for multimedia works such as video games, music, and films. Recent high-quality diffusion-based sound generation models can serve as valuable tools for the creators. However, despite producing high-quality sounds, these models often suffer from slow inference speeds. This drawback burdens creators, who typically refine their sounds through trial and error to align them with their artistic intentions. To address this issue, we introduce Sound Consistency Trajectory Models (SoundCTM). Our model enables flexible transitioning between high-quality 1-step sound generation and superior sound quality through multi-step generation. This allows creators to initially control sounds with 1-step samples before refining them through multi-step generation. While CTM fundamentally achieves flexible 1-step and multi-step generation, its impressive performance heavily depends on an additional pretrained feature extractor and an adversarial loss, which are expensive to train and not always available in other domains. Thus, we reframe CTM's training framework and introduce a novel feature distance by utilizing the teacher's network for a distillation loss. Additionally, while distilling classifier-free guided trajectories, we train conditional and unconditional student models simultaneously and interpolate between these models during inference. We also propose training-free controllable frameworks for SoundCTM, leveraging its flexible sampling capability. SoundCTM achieves both promising 1-step and multi-step real-time sound generation without using any extra off-the-shelf networks. Furthermore, we demonstrate SoundCTM's capability of controllable sound generation in a training-free manner.","sentences":["Sound content is an indispensable element for multimedia works such as video games, music, and films.","Recent high-quality diffusion-based sound generation models can serve as valuable tools for the creators.","However, despite producing high-quality sounds, these models often suffer from slow inference speeds.","This drawback burdens creators, who typically refine their sounds through trial and error to align them with their artistic intentions.","To address this issue, we introduce Sound Consistency Trajectory Models (SoundCTM).","Our model enables flexible transitioning between high-quality 1-step sound generation and superior sound quality through multi-step generation.","This allows creators to initially control sounds with 1-step samples before refining them through multi-step generation.","While CTM fundamentally achieves flexible 1-step and multi-step generation, its impressive performance heavily depends on an additional pretrained feature extractor and an adversarial loss, which are expensive to train and not always available in other domains.","Thus, we reframe CTM's training framework and introduce a novel feature distance by utilizing the teacher's network for a distillation loss.","Additionally, while distilling classifier-free guided trajectories, we train conditional and unconditional student models simultaneously and interpolate between these models during inference.","We also propose training-free controllable frameworks for SoundCTM, leveraging its flexible sampling capability.","SoundCTM achieves both promising 1-step and multi-step real-time sound generation without using any extra off-the-shelf networks.","Furthermore, we demonstrate SoundCTM's capability of controllable sound generation in a training-free manner."],"url":"http://arxiv.org/abs/2405.18503v1","category":"cs.SD"}
{"created":"2024-05-28 18:00:13","title":"Genuine LFUV observables in $\u03c4-\u03bc$ sector in $B \\to (K,\\,K^*) \\ell \\ell $ decays","abstract":"It was previously shown that unlike the ratios $R_K^{\\mu e} \\equiv R_K \\equiv \\Gamma(B \\to K \\mu^+ \\mu^-)/\\Gamma(B \\to K e^+ e^-)$ and $R_{K^*}^{\\mu e} \\equiv R_{K^*} \\equiv \\Gamma(B \\to K^* \\mu^+ \\mu^-)/\\Gamma(B \\to K^* e^+ e^-)$, the ratios $R_K^{\\tau \\mu}$ and $R_{K^*}^{\\tau \\mu}$ can deviate from their Standard Model (SM) predictions even with universal new physics couplings. This observation highlights the critical need to identify and establish genuine lepton flavor universality violating (LFUV) observables in the $\\tau-\\mu$ sector. This work embarks on establishing genuine LFUV ratio observables in \\(B \\to K \\ell \\ell\\) and \\(B \\to K^* \\ell \\ell\\) decays through comprehensive analysis of their angular distributions. We find that like $R_{K^*}^{\\tau \\mu}$, the ratios $R_{A_{FB}}^{\\tau \\mu}$ and $R_{f_L}^{\\tau \\mu}$ do not qualify as genuine LFUV observables, whereas the ratios of all optimized observables in $B \\to K^* \\ell \\ell$ decays within the $\\tau-\\mu$ sector definitively do. In the case of $B \\to K \\ell \\ell$ decays, similar to $R_K^{\\tau \\mu}$, the ratio $R_{F_H}$ is influenced by mass effects and therefore cannot be considered a genuine LFUV observable in the $\\tau-\\mu$ sector. However, the ratio $\\Gamma_\\tau(1-F_{H}^{\\tau})/\\Gamma_\\mu(1-F_{H}^{\\mu})$ stands as the sole genuine LFUV observable in $B \\to K \\ell \\ell$ decays. Furthermore, by making use of new physics Lorentz structures which provide a better fit to the current $b \\to s \\ell \\ell$ data as compared to the SM, we demonstrate how the non-genuine LFU ratios $R_{A_{FB}}^{\\tau \\mu}$ and $R_{f_L}^{\\tau \\mu}$ can be employed to distinguish between framework with solely universal lepton couplings and those with both universal and non-universal couplings.","sentences":["It was previously shown that unlike the ratios $R_K^{\\mu e} \\equiv R_K \\equiv \\Gamma(B \\to K \\mu^+ \\mu^-)/\\Gamma(B \\to K e^+ e^-)$ and $R_{K^*}^{\\mu e} \\equiv R_{K^*} \\equiv \\Gamma(B \\to K^* \\mu^+ \\mu^-)/\\Gamma(B \\to K^* e^+ e^-)$, the ratios $R_K^{\\tau \\mu}$ and $R_{K^*}^{\\tau \\mu}$ can deviate from their Standard Model (SM) predictions even with universal new physics couplings.","This observation highlights the critical need to identify and establish genuine lepton flavor universality violating (LFUV) observables in the $\\tau-\\mu$ sector.","This work embarks on establishing genuine LFUV ratio observables in \\(B \\to K \\ell \\ell\\) and \\(B \\to K^* \\ell \\ell\\) decays through comprehensive analysis of their angular distributions.","We find that like $R_{K^*}^{\\tau \\mu}$, the ratios $R_{A_{FB}}^{\\tau \\mu}$ and $R_{f_L}^{\\tau \\mu}$ do not qualify as genuine LFUV observables, whereas the ratios of all optimized observables in $B \\to K^* \\ell \\ell$ decays within the $\\tau-\\mu$ sector definitively do.","In the case of $B \\to K \\ell \\ell$ decays, similar to $R_K^{\\tau \\mu}$, the ratio $R_{F_H}$ is influenced by mass effects and therefore cannot be considered a genuine LFUV observable in the $\\tau-\\mu$ sector.","However, the ratio $\\Gamma_\\tau(1-F_{H}^{\\tau})/\\Gamma_\\mu(1-F_{H}^{\\mu})$ stands as the sole genuine LFUV observable in $B \\to K \\ell \\ell$ decays.","Furthermore, by making use of new physics Lorentz structures which provide a better fit to the current $b \\to s \\ell \\ell$ data as compared to the SM, we demonstrate how the non-genuine LFU ratios $R_{A_{FB}}^{\\tau \\mu}$ and $R_{f_L}^{\\tau \\mu}$ can be employed to distinguish between framework with solely universal lepton couplings and those with both universal and non-universal couplings."],"url":"http://arxiv.org/abs/2405.18488v1","category":"hep-ph"}
{"created":"2024-05-28 18:00:00","title":"On the deep superstring spectrum","abstract":"We propose a covariant method of constructing entire trajectories of physical states in superstring theory in the critical dimension. It is inspired by a recently developed covariant technology of excavating bosonic string trajectories, that is facilitated by the observation that the Virasoro constraints can be written as linear combinations of lowering operators of a bigger algebra, namely a symplectic algebra, which is Howe dual to the spacetime Lorentz algebra. For superstrings, it is the orthosymplectic algebra that appears instead, with its lowest weight states forming the simplest class of physical trajectories in the NS sector. To construct the simplest class in the R sector, the lowest weight states need to be supplemented with other states, which we determine. Deeper trajectories are then constructed by acting with suitable combinations of the raising operators of the orthosymplectic algebra, which we illustrate with several examples.","sentences":["We propose a covariant method of constructing entire trajectories of physical states in superstring theory in the critical dimension.","It is inspired by a recently developed covariant technology of excavating bosonic string trajectories, that is facilitated by the observation that the Virasoro constraints can be written as linear combinations of lowering operators of a bigger algebra, namely a symplectic algebra, which is Howe dual to the spacetime Lorentz algebra.","For superstrings, it is the orthosymplectic algebra that appears instead, with its lowest weight states forming the simplest class of physical trajectories in the NS sector.","To construct the simplest class in the R sector, the lowest weight states need to be supplemented with other states, which we determine.","Deeper trajectories are then constructed by acting with suitable combinations of the raising operators of the orthosymplectic algebra, which we illustrate with several examples."],"url":"http://arxiv.org/abs/2405.18467v1","category":"hep-th"}
{"created":"2024-05-29 17:54:59","title":"X-ray and Radio campaign of the Z-source GX 340+0: discovery of X-ray polarization and its implications","abstract":"We present the discovery of X-ray polarization from the neutron star low-mass X-ray binary and Z-source, GX~340$+$0, using an Imaging X-ray Polarimetry Explorer (IXPE) observation in March 2024. Along with the IXPE observation, we conducted an extensive X-ray and radio monitoring campaign to ascertain the source properties during and around the IXPE observation. The source was within the horizontal branch throughout the multiwavelength campaign. We measured a significant X-ray polarization in 2--8 keV with polarization degree (PD) = $4.02 \\pm 0.35$% and polarization angle (PA) = $37.6 \\pm 2.5^\\circ$. The energy-dependent polarization indicates that in the 2-2.5 keV energy range, the PA is much lower, $\\sim9\\pm8^\\circ$, while other energy bands are consistent with the PA found over 2.5--8 keV. The simultaneous AstroSat-IXPE spectro-polarimetric observations provide some evidence for independent polarization from various spectral components, hinting at a disparity in the PA from the accretion disk and the Comptonized emission, while suggesting an unpolarized emission from the blackbody component. Radio observations in the 0.7--9 GHz frequency range reveal a non-detection of radio emission in 0.7-1.5 GHz and a significant detection in 5.5--9 GHz, suggesting the presence of a spectral break in 1.5-5.5 GHz. Using ATCA observation we place upper limits on the radio polarization at $<$6% on the linear polarization and $<$4% on the circular polarization at 3$\\sigma$ level. We discuss the origin of the X-ray polarization and its implications on the geometry of the spectral components.","sentences":["We present the discovery of X-ray polarization from the neutron star low-mass X-ray binary and Z-source, GX~340$+$0, using an Imaging X-ray Polarimetry Explorer (IXPE) observation in March 2024.","Along with the IXPE observation, we conducted an extensive X-ray and radio monitoring campaign to ascertain the source properties during and around the IXPE observation.","The source was within the horizontal branch throughout the multiwavelength campaign.","We measured a significant X-ray polarization in 2--8 keV with polarization degree (PD) = $4.02 \\pm 0.35$% and polarization angle (PA)","= $37.6 \\pm 2.5^\\circ$.","The energy-dependent polarization indicates that in the 2-2.5 keV energy range, the PA is much lower, $\\sim9\\pm8^\\circ$, while other energy bands are consistent with the PA found over 2.5--8 keV.","The simultaneous AstroSat-IXPE spectro-polarimetric observations provide some evidence for independent polarization from various spectral components, hinting at a disparity in the PA from the accretion disk and the Comptonized emission, while suggesting an unpolarized emission from the blackbody component.","Radio observations in the 0.7--9 GHz frequency range reveal a non-detection of radio emission in 0.7-1.5 GHz and a significant detection in 5.5--9 GHz, suggesting the presence of a spectral break in 1.5-5.5 GHz.","Using ATCA observation we place upper limits on the radio polarization at $<$6% on the linear polarization and $<$4% on the circular polarization at 3$\\sigma$ level.","We discuss the origin of the X-ray polarization and its implications on the geometry of the spectral components."],"url":"http://arxiv.org/abs/2405.19324v1","category":"astro-ph.HE"}
{"created":"2024-05-29 17:37:43","title":"Higgs Physics at a $\\sqrt{s}=3$ TeV Muon Collider with detailed detector simulation","abstract":"The Muon Collider is one of the most promising future collider facilities with the potential to reach multi-TeV center-of-mass energy and high luminosity. Due to the significant Higgs boson production cross section in muon collisions at these high energies, the collider can be considered a Higgs factory. It holds the capability to significantly advance our understanding of the Higgs sector to an unprecedented level of precision. However, the presence of beam-induced background resulting from the decay of the beam muons poses unique challenges for detector development and event reconstruction. In this paper, the prospects for various measurements of the Higgs boson production cross sections at a $\\sqrt{s}=3$ TeV collider are presented using a detailed detector simulation in a realistic environment. The study demonstrates the feasibility of achieving high precision measurements of the Higgs boson production cross sections with the current state-of-the-art detector design. In addition, the paper discusses the detector requirements necessary for obtaining such resolutions and for measuring the Higgs trilinear self-coupling.","sentences":["The Muon Collider is one of the most promising future collider facilities with the potential to reach multi-TeV center-of-mass energy and high luminosity.","Due to the significant Higgs boson production cross section in muon collisions at these high energies, the collider can be considered a Higgs factory.","It holds the capability to significantly advance our understanding of the Higgs sector to an unprecedented level of precision.","However, the presence of beam-induced background resulting from the decay of the beam muons poses unique challenges for detector development and event reconstruction.","In this paper, the prospects for various measurements of the Higgs boson production cross sections at a $\\sqrt{s}=3$ TeV collider are presented using a detailed detector simulation in a realistic environment.","The study demonstrates the feasibility of achieving high precision measurements of the Higgs boson production cross sections with the current state-of-the-art detector design.","In addition, the paper discusses the detector requirements necessary for obtaining such resolutions and for measuring the Higgs trilinear self-coupling."],"url":"http://arxiv.org/abs/2405.19314v1","category":"hep-ex"}
{"created":"2024-05-29 16:57:05","title":"Measurement of the presence of $\\mathrm{a}_1(1420)$ and $\u03c9(782)$ in $\u03c4^-\\to\u03c0^-\u03c0^-\u03c0^+\u03bd_\u03c4$ at Belle","abstract":"We present preliminary results of a partial-wave analysis of $\\tau^-\\to\\pi^-\\pi^-\\pi^+\\nu_\\tau$ using data from the Belle experiment at the KEKB $\\mathrm{e}^+\\mathrm{e}^-$ collider. We validate our analysis with a model-independent one. We see an $\\mathrm{a}_1(1420)$ resonance and a G-parity violating $1^-[\\omega(782)\\pi]_\\mathrm{P}]$ wave in tauon decays. Our results will improve models used in simulation studies necessary for measuring the tauon electric and magnetic dipole moments and Michel parameters.","sentences":["We present preliminary results of a partial-wave analysis of $\\tau^-\\to\\pi^-\\pi^-\\pi^+\\nu_\\tau$ using data from the Belle experiment at the KEKB $\\mathrm{e}^+\\mathrm{e}^-$ collider.","We validate our analysis with a model-independent one.","We see an $\\mathrm{a}_1(1420)$ resonance and a G-parity violating $1^-[\\omega(782)\\pi]_\\mathrm{P}]$ wave in tauon decays.","Our results will improve models used in simulation studies necessary for measuring the tauon electric and magnetic dipole moments and Michel parameters."],"url":"http://arxiv.org/abs/2405.19264v1","category":"hep-ex"}
{"created":"2024-05-29 16:37:25","title":"Seasonal and longitudinal variability in Io's SO2 atmosphere from 22 years of IRTF/TEXES observations","abstract":"Between 2001 and 2023, we obtained high spectral resolution mid-infrared observations of Io using the TEXES instrument at NASA's Infrared Telescope Facility. These observations were centered at 529.8 cm-1 (18.88 {\\mu}m) and include several SO2 absorption lines. By modeling the shapes and strengths of these absorption lines, we are able to determine how Io's SO2 atmospheric density varies over the 22-year time period, covering nearly two Jovian years. Previous analysis has shown that the density of Io's atmosphere on the anti-Jovian hemisphere exhibits clear seasonal temporal variability, which can be modeled as the sum of a seasonally-varying frost sublimation component and a constant component, assumed to be volcanic. The new data show that the seasonal pattern repeats during the second Jovian year, confirming the importance of sublimation support. The considerable longitudinal variability in Io's atmospheric density found in previous work is also stable over the second Jovian year with the SO2 column density on the Jupiter-facing hemisphere being 5--8 times lower than the anti-Jovian hemisphere. For the first time, we detect seasonal variability on the Jupiter-facing hemisphere as well. This can also be modeled as a combination of sublimation and a small constant source. The lower atmospheric density on the Jupiter-facing hemisphere can plausibly be explained by the daily Jupiter eclipses, which decrease the surface temperature and therefore reduce the sublimation-driven component of the atmosphere, combined with a lower level of volcanic activity directly emitting SO2 into the atmosphere.","sentences":["Between 2001 and 2023, we obtained high spectral resolution mid-infrared observations of Io using the TEXES instrument at NASA's Infrared Telescope Facility.","These observations were centered at 529.8 cm-1 (18.88 {\\mu}m) and include several SO2 absorption lines.","By modeling the shapes and strengths of these absorption lines, we are able to determine how Io's SO2 atmospheric density varies over the 22-year time period, covering nearly two Jovian years.","Previous analysis has shown that the density of Io's atmosphere on the anti-Jovian hemisphere exhibits clear seasonal temporal variability, which can be modeled as the sum of a seasonally-varying frost sublimation component and a constant component, assumed to be volcanic.","The new data show that the seasonal pattern repeats during the second Jovian year, confirming the importance of sublimation support.","The considerable longitudinal variability in Io's atmospheric density found in previous work is also stable over the second Jovian year with the SO2 column density on the Jupiter-facing hemisphere being 5--8 times lower than the anti-Jovian hemisphere.","For the first time, we detect seasonal variability on the Jupiter-facing hemisphere as well.","This can also be modeled as a combination of sublimation and a small constant source.","The lower atmospheric density on the Jupiter-facing hemisphere can plausibly be explained by the daily Jupiter eclipses, which decrease the surface temperature and therefore reduce the sublimation-driven component of the atmosphere, combined with a lower level of volcanic activity directly emitting SO2 into the atmosphere."],"url":"http://arxiv.org/abs/2405.19253v1","category":"astro-ph.EP"}
{"created":"2024-05-29 16:00:22","title":"The $SL_2(\\mathbb{R})$ duality and the non-invertible $U(1)$ symmetry of Maxwell theory","abstract":"Recent proposals for the Symmetry Topological Field Theory (SymTFT) of Maxwell theory admit a 0-form symmetry compatible with the classical $SL_2(\\mathbb{R})$ duality of electromagnetism. We describe how to realize these automorphisms of the SymTFT in terms of its operators and we detail their effects on the dynamical theory and its global variants. In the process, we show that the classical $U(1)$ symmetry, corresponding to the stabilizer of $SL_2(\\mathbb{R})$, can be restored as a non-invertible one, by means of an infinite series of discrete gauging. This provides an example of the reemergence of a classical symmetry in the quantum regime, which was not broken by anomalies, but rather by the quantization of electromagnetic fluxes. However, this procedure comes at the price of introducing \"continuous\" condensates that trivialize all line operators.","sentences":["Recent proposals for the Symmetry Topological Field Theory (SymTFT) of Maxwell theory admit a 0-form symmetry compatible with the classical $SL_2(\\mathbb{R})$ duality of electromagnetism.","We describe how to realize these automorphisms of the SymTFT in terms of its operators and we detail their effects on the dynamical theory and its global variants.","In the process, we show that the classical $U(1)$ symmetry, corresponding to the stabilizer of $SL_2(\\mathbb{R})$, can be restored as a non-invertible one, by means of an infinite series of discrete gauging.","This provides an example of the reemergence of a classical symmetry in the quantum regime, which was not broken by anomalies, but rather by the quantization of electromagnetic fluxes.","However, this procedure comes at the price of introducing \"continuous\" condensates that trivialize all line operators."],"url":"http://arxiv.org/abs/2405.19218v1","category":"hep-th"}
{"created":"2024-05-29 15:58:14","title":"Central Limit Theorem for Tensor Products of Free Variables via Bi-Free Independence","abstract":"In this paper, a connection between bi-free probability and the asymptotics of random quantum channels and tensor products of random matrices is established. Using bi-free matrix models, it is demonstrated that the spectral distribution of certain self-adjoint quantum channels and tensor products of random matrices tend to a distribution that can be obtained by an averaged sum of products of bi-freely independent pairs. Subsequently, using bi-free techniques, a Central Limit Theorem for such operator is established.","sentences":["In this paper, a connection between bi-free probability and the asymptotics of random quantum channels and tensor products of random matrices is established.","Using bi-free matrix models, it is demonstrated that the spectral distribution of certain self-adjoint quantum channels and tensor products of random matrices tend to a distribution that can be obtained by an averaged sum of products of bi-freely independent pairs.","Subsequently, using bi-free techniques, a Central Limit Theorem for such operator is established."],"url":"http://arxiv.org/abs/2405.19216v1","category":"math.OA"}
{"created":"2024-05-29 15:35:09","title":"Quantum Tunneling Could Enable Proton Transfer Reactions on Titan","abstract":"The surface of Titan, Saturn's largest moon, is rich in organics and is often suggested to model early Earth environments. Titan's surface is cold, at a temperature of approximately 90 K, which prohibits most thermally activated chemical reactions. However, quantum effects become more important at low temperatures and reactions that are classically prohibited can often proceed through quantum mechanical pathways. Using path integral molecular dynamics simulations, we investigate nuclear quantum effects on the thermodynamics of model proton transfer reactions in liquid ethane. We find that proton transfer can occur at Titan surface conditions through quantum tunneling. Consequently, we estimate that nuclear quantum effects can enhance reaction rates by many orders of magnitude. Our results suggest that nuclear quantum effects could facilitate prebiotic chemistry on Titan, and quantum effects should be considered in future investigations.","sentences":["The surface of Titan, Saturn's largest moon, is rich in organics and is often suggested to model early Earth environments.","Titan's surface is cold, at a temperature of approximately 90 K, which prohibits most thermally activated chemical reactions.","However, quantum effects become more important at low temperatures and reactions that are classically prohibited can often proceed through quantum mechanical pathways.","Using path integral molecular dynamics simulations, we investigate nuclear quantum effects on the thermodynamics of model proton transfer reactions in liquid ethane.","We find that proton transfer can occur at Titan surface conditions through quantum tunneling.","Consequently, we estimate that nuclear quantum effects can enhance reaction rates by many orders of magnitude.","Our results suggest that nuclear quantum effects could facilitate prebiotic chemistry on Titan, and quantum effects should be considered in future investigations."],"url":"http://arxiv.org/abs/2405.19195v1","category":"physics.chem-ph"}
{"created":"2024-05-29 15:34:27","title":"Ultraclean carbon nanotube-based Josephson junctions","abstract":"We present a technique for integrating ultraclean carbon nanotubes into superconducting circuits, aiming to realize Josephson junctions based on one-dimensional elementary quantum conductors. This technique primarily involves depositing the nanotube in the final step, thus preserving it from the inherent contaminations of nanofabrication and maintaining contact solely with superconducting electrodes and a crystalline hBN substrate. Through transport measurements performed in both the normal and superconducting states, we demonstrate that our method yields high-quality junctions with Josephson energies suitable for quantum device applications, such as carbon nanotube-based superconducting qubits.","sentences":["We present a technique for integrating ultraclean carbon nanotubes into superconducting circuits, aiming to realize Josephson junctions based on one-dimensional elementary quantum conductors.","This technique primarily involves depositing the nanotube in the final step, thus preserving it from the inherent contaminations of nanofabrication and maintaining contact solely with superconducting electrodes and a crystalline hBN substrate.","Through transport measurements performed in both the normal and superconducting states, we demonstrate that our method yields high-quality junctions with Josephson energies suitable for quantum device applications, such as carbon nanotube-based superconducting qubits."],"url":"http://arxiv.org/abs/2405.19192v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-29 15:26:15","title":"Benford's law in atomic spectra and opacity databases","abstract":"The intriguing law of anomalous numbers, also named Benford's law, states that the significant digits of data follow a logarithmic distribution favoring the smallest values. In this work, we test the compliance with this law of the atomic databases developed at the National Institute of Standards and Technology (NIST) focusing on line energies, oscillator strengths, Einstein coefficients and radiative opacities. The considered databases are the Atomic Spectra Database (ASD) and the NIST-LANL (Los Alamos National Laboratory) Lanthanide/Actinide Opacity Database. The present study is not limited to the first digit and the case of multipole lines is also considered. The fact that the law is verified with a high accuracy means that the occurrence of digits reflects the constraints induced, in a given angular-momentum coupling, by the selection rules for atomic processes. As a consequence, Benford's law may be of great interest to detect inconsistencies in atomic databases.","sentences":["The intriguing law of anomalous numbers, also named Benford's law, states that the significant digits of data follow a logarithmic distribution favoring the smallest values.","In this work, we test the compliance with this law of the atomic databases developed at the National Institute of Standards and Technology (NIST) focusing on line energies, oscillator strengths, Einstein coefficients and radiative opacities.","The considered databases are the Atomic Spectra Database (ASD) and the NIST-LANL (Los Alamos National Laboratory)","Lanthanide/Actinide Opacity Database.","The present study is not limited to the first digit and the case of multipole lines is also considered.","The fact that the law is verified with a high accuracy means that the occurrence of digits reflects the constraints induced, in a given angular-momentum coupling, by the selection rules for atomic processes.","As a consequence, Benford's law may be of great interest to detect inconsistencies in atomic databases."],"url":"http://arxiv.org/abs/2405.19185v1","category":"physics.atom-ph"}
{"created":"2024-05-29 15:02:28","title":"Precision microfluidic control of neuronal ensembles in cultured cortical networks","abstract":"In vitro neuronal culture is an important research platform in cellular and network neuroscience. However, neurons cultured on a homogeneous scaffold form dense, randomly connected networks and display excessively synchronized activity; this phenomenon has limited their applications in network-level studies, such as studies of neuronal ensembles, or coordinated activity by a group of neurons. Herein, we develop polydimethylsiloxane-based microfluidic devices to create small neuronal networks exhibiting a hierarchically modular structure resembling the connectivity observed in the mammalian cortex. The strength of intermodular coupling was manipulated by varying the width and height of the microchannels that connect the modules. Using fluorescent calcium imaging, we observe that the spontaneous activity in networks with smaller microchannels (2.2$-$5.5 $\\mu$m$^2$) had lower synchrony and exhibit a threefold variety of neuronal ensembles. Optogenetic stimulation demonstrates that a reduction in intermodular coupling enriches evoked neuronal activity patterns and that repeated stimulation induces plasticity in neuronal ensembles in these networks. These findings suggest that cell engineering technologies based on microfluidic devices enable in vitro reconstruction of the intricate dynamics of neuronal ensembles, thus providing a robust platform for studying neuronal ensembles in a well-defined physicochemical environment.","sentences":["In vitro neuronal culture is an important research platform in cellular and network neuroscience.","However, neurons cultured on a homogeneous scaffold form dense, randomly connected networks and display excessively synchronized activity; this phenomenon has limited their applications in network-level studies, such as studies of neuronal ensembles, or coordinated activity by a group of neurons.","Herein, we develop polydimethylsiloxane-based microfluidic devices to create small neuronal networks exhibiting a hierarchically modular structure resembling the connectivity observed in the mammalian cortex.","The strength of intermodular coupling was manipulated by varying the width and height of the microchannels that connect the modules.","Using fluorescent calcium imaging, we observe that the spontaneous activity in networks with smaller microchannels (2.2$-$5.5 $\\mu$m$^2$) had lower synchrony and exhibit a threefold variety of neuronal ensembles.","Optogenetic stimulation demonstrates that a reduction in intermodular coupling enriches evoked neuronal activity patterns and that repeated stimulation induces plasticity in neuronal ensembles in these networks.","These findings suggest that cell engineering technologies based on microfluidic devices enable in vitro reconstruction of the intricate dynamics of neuronal ensembles, thus providing a robust platform for studying neuronal ensembles in a well-defined physicochemical environment."],"url":"http://arxiv.org/abs/2405.19159v1","category":"q-bio.NC"}
{"created":"2024-05-29 14:25:27","title":"String Theory and the Early Universe: Constraints and Opportunities","abstract":"This is a short account, based on a talk given at the 2024 Moriond Cosmology Conference, of where and why string theory matters in early universe cosmology. It is written for a cosmology audience predisposed to be at best sceptical, and at worst contemptuous, of the notion that either quantum gravity or string theory has any relevance for their discipline. I cover inflation, CMB tensor modes, extended kination or tracker epochs and reheating.","sentences":["This is a short account, based on a talk given at the 2024 Moriond Cosmology Conference, of where and why string theory matters in early universe cosmology.","It is written for a cosmology audience predisposed to be at best sceptical, and at worst contemptuous, of the notion that either quantum gravity or string theory has any relevance for their discipline.","I cover inflation, CMB tensor modes, extended kination or tracker epochs and reheating."],"url":"http://arxiv.org/abs/2405.19118v1","category":"astro-ph.CO"}
{"created":"2024-05-29 14:10:46","title":"The discovery and significance of fast radio bursts","abstract":"In 2007 we were part of a team that discovered the so-called ``Lorimer Burst'', the first example of a new class of objects now known as fast radio bursts (FRBs). These enigmatic events are only a few ms in duration and occur at random locations on the sky at a rate of a few thousand per day. Several thousand FRBs are currently known. While it is now well established that they have a cosmological origin, and about 10\\% of all currently known sources have been seen to exhibit multiple bursts, the origins of these enigmatic sources are currently poorly understood. In this article, we review the discovery of FRBs and present some of the highlights from the vast body of work by an international community. Following a brief overview of the scale of the visible Universe in \\S 1, we describe the key moments in radio astronomy (\\S 2) that led up to the discovery of the Lorimer burst (\\S 3). Early efforts to find more FRBs are described in \\S 4 which led to the discovery of the first repeating source (\\S 5). In \\S 6, as we close out on the second decade of FRBs, we outline some of the many open questions in the field and look ahead to the coming years where many surprises are surely in store.","sentences":["In 2007 we were part of a team that discovered the so-called ``Lorimer Burst'', the first example of a new class of objects now known as fast radio bursts (FRBs).","These enigmatic events are only a few ms in duration and occur at random locations on the sky at a rate of a few thousand per day.","Several thousand FRBs are currently known.","While it is now well established that they have a cosmological origin, and about 10\\% of all currently known sources have been seen to exhibit multiple bursts, the origins of these enigmatic sources are currently poorly understood.","In this article, we review the discovery of FRBs and present some of the highlights from the vast body of work by an international community.","Following a brief overview of the scale of the visible Universe in \\S 1, we describe the key moments in radio astronomy (\\S 2) that led up to the discovery of the Lorimer burst (\\S 3).","Early efforts to find more FRBs are described in \\S 4 which led to the discovery of the first repeating source (\\S 5).","In \\S 6, as we close out on the second decade of FRBs, we outline some of the many open questions in the field and look ahead to the coming years where many surprises are surely in store."],"url":"http://arxiv.org/abs/2405.19106v1","category":"astro-ph.HE"}
{"created":"2024-05-29 13:50:22","title":"Impacts of ALP on the Constraints of Dark Photon","abstract":"Dark sector may exist and interact with Standard Model (SM) through the $U(1)$ kinetic mixing. Through this portal-type interaction, dark photon from dark sector couples to SM fermions, and may explain the discrepancy between experimental data and SM calculations on muon anomalous magnetic moment, muon $g-2$. However, current searches for dark photon impose stringent constraints on the mixing parameter $\\varepsilon$ for various dark photon masses, excluding the favorite parameter space for muon $g-2$. In this paper, we study the case where a global $U(1)$ in dark sector is spontaneously broken, resulting a light pseudo-Goldstone, axion-like particle (ALP) $a$, which couples to dark photon and SM photon, $g_{a\\gamma\\gamma'}$. Through this interaction, dark photon may decay into photon and ALP when this channel is kinematically allowed. As a result, the experimental constraints on dark photon change significantly, and dark photon is able to explain the muon $g-2$ anomaly when its mass is heavier than $10$ GeV.","sentences":["Dark sector may exist and interact with Standard Model (SM) through the $U(1)$ kinetic mixing.","Through this portal-type interaction, dark photon from dark sector couples to SM fermions, and may explain the discrepancy between experimental data and SM calculations on muon anomalous magnetic moment, muon $g-2$. However, current searches for dark photon impose stringent constraints on the mixing parameter $\\varepsilon$ for various dark photon masses, excluding the favorite parameter space for muon $g-2$. In this paper, we study the case where a global $U(1)$ in dark sector is spontaneously broken, resulting a light pseudo-Goldstone, axion-like particle (ALP) $a$, which couples to dark photon and SM photon, $g_{a\\gamma\\gamma'}$. Through this interaction, dark photon may decay into photon and ALP when this channel is kinematically allowed.","As a result, the experimental constraints on dark photon change significantly, and dark photon is able to explain the muon $g-2$ anomaly when its mass is heavier than $10$ GeV."],"url":"http://arxiv.org/abs/2405.19087v1","category":"hep-ph"}
{"created":"2024-05-29 13:43:08","title":"Triple-gauge couplings in LHC diboson production: a SMEFT view from every angle","abstract":"This study explores fully leptonic WZ and WW production at the LHC within the SMEFT framework at NLO in QCD, focusing on both CP-even and CP-odd triple-gauge-coupling dimension-six operators. We investigate the off-shell processes, contrasting our findings in inclusive setups with those in the presence of realistic fiducial selections. Alongside the conventional kinematic observables, we examine polarisation-sensitive observables and angular coefficients. Moreover, we assess potential SMEFT effects on asymmetry observables. Through a sensitivity analysis, we identify critical LHC observables that are particularly sensitive to SMEFT-induced modifications, thereby shedding light on potential avenues for new-physics searches in diboson production at the LHC.","sentences":["This study explores fully leptonic WZ and WW production at the LHC within the SMEFT framework at NLO in QCD, focusing on both CP-even and CP-odd triple-gauge-coupling dimension-six operators.","We investigate the off-shell processes, contrasting our findings in inclusive setups with those in the presence of realistic fiducial selections.","Alongside the conventional kinematic observables, we examine polarisation-sensitive observables and angular coefficients.","Moreover, we assess potential SMEFT effects on asymmetry observables.","Through a sensitivity analysis, we identify critical LHC observables that are particularly sensitive to SMEFT-induced modifications, thereby shedding light on potential avenues for new-physics searches in diboson production at the LHC."],"url":"http://arxiv.org/abs/2405.19083v1","category":"hep-ph"}
{"created":"2024-05-29 13:23:09","title":"UPC physics with ALICE in Run 3","abstract":"The ALICE experiment has undergone a major detector upgrade for Run 3, expanding its detection capabilities for a wide variety of studies. The new continuous readout has significantly enhanced the physics potential for ultra-peripheral collision analyses. In this talk, we discussed some of the physics analyses that can be carried out in ultra-peripheral collisions using the Run 3 data and presented some of the first physics performance plots in both proton-proton and heavy-ion collisions.","sentences":["The ALICE experiment has undergone a major detector upgrade for Run 3, expanding its detection capabilities for a wide variety of studies.","The new continuous readout has significantly enhanced the physics potential for ultra-peripheral collision analyses.","In this talk, we discussed some of the physics analyses that can be carried out in ultra-peripheral collisions using the Run 3 data and presented some of the first physics performance plots in both proton-proton and heavy-ion collisions."],"url":"http://arxiv.org/abs/2405.19069v1","category":"hep-ex"}
{"created":"2024-05-29 12:34:59","title":"Vacuum Condensates on the Coulomb Branch","abstract":"We study correlation functions on the Coulomb branch of planar $\\mathcal{N} = 4$ super-Yang- Mills theory (SYM), and their relationship with integrability, the operator product expansion (OPE), the sum rule, the large charge expansion, and holography. First, we compute one-point functions of arbitrary scalar operators at weak coupling and derive a compact spin-chain representation. We next study the two-point functions of chiral primaries at one loop and find that the radius of convergence of OPE in position space is infinite. We estimate the asymptotic growth of the OPE data based on this finding. Finally, we propose a concrete nonperturbative formula that connects the correlation functions on the Coulomb branch with the correlation functions with large charge insertions at the conformal point and provide a holographic interpretation based on a large D3-brane in AdS. The formula extends the known connection between the large charge sector and the Coulomb branch for rank-1 theories to the large $N$ limit.","sentences":["We study correlation functions on the Coulomb branch of planar $\\mathcal{N} = 4$ super-Yang- Mills theory (SYM), and their relationship with integrability, the operator product expansion (OPE), the sum rule, the large charge expansion, and holography.","First, we compute one-point functions of arbitrary scalar operators at weak coupling and derive a compact spin-chain representation.","We next study the two-point functions of chiral primaries at one loop and find that the radius of convergence of OPE in position space is infinite.","We estimate the asymptotic growth of the OPE data based on this finding.","Finally, we propose a concrete nonperturbative formula that connects the correlation functions on the Coulomb branch with the correlation functions with large charge insertions at the conformal point and provide a holographic interpretation based on a large D3-brane in AdS.","The formula extends the known connection between the large charge sector and the Coulomb branch for rank-1 theories to the large $N$ limit."],"url":"http://arxiv.org/abs/2405.19043v1","category":"hep-th"}
{"created":"2024-05-29 12:26:54","title":"Heavy baryons in the relativized quark model with chromodynamics","abstract":"Following the work of Capstick and Isgur [\\href{https://doi.org/10.1103/PhysRevD.34.2809}{Phys.~Rev.~D~34,~2809~(1986)}], we systematically study the mass spectrum of the heavy baryons in the relativized quark potential model with chromodynamics. Besides the original Godfrey-Isgur (GI) model, we also adopt a modified GI model which replaces the linear confinement by a screened one. The two models give similar results in our work. All heavy baryons observed so far can be explained as three-quark states. In particular, we identify the $\\Omega_{c}(3000)$/$\\Omega_{b}(6316)$, $\\Omega_{c}(3050)$/$\\Omega_{b}(6330)$, $\\Omega_{c}(3065)$/$\\Omega_{b}(6340)$ and $\\Omega_{c}(3090)$/$\\Omega_{b}(6350)$ states as the $p_{\\lambda}$ excitations with quantum numbers $1/2^{-}$, $3/2^{-}$, $3/2^{-}$ and $5/2^{-}$. The $\\Omega_{c}(3120)$ is a $3/2^{-}$ state with the $p_{\\rho}$ excitation, whose bottom partner is predicted to be $\\Omega_{b}(6446/6457,3/2^{-})$. The higher state $\\Omega_{c}(3188)$ is the $2s_{\\lambda}$ excitation with quantum numbers $1/2^{+}$, and $\\Omega_{c}(3327)$ is a $d_{\\lambda}$ excitation with quantum numbers $3/2^{+}$ or $5/2^{+}$. In addition, the $\\Lambda_{c}(2940)$ with quantum numbers $J^{P}=3/2^{-}$ could be explained as the $p_{\\rho}$ excitation.","sentences":["Following the work of Capstick and Isgur [\\href{https://doi.org/10.1103/PhysRevD.34.2809}{Phys.~Rev.~D~34,~2809~(1986)}], we systematically study the mass spectrum of the heavy baryons in the relativized quark potential model with chromodynamics.","Besides the original Godfrey-Isgur (GI) model, we also adopt a modified GI model which replaces the linear confinement by a screened one.","The two models give similar results in our work.","All heavy baryons observed so far can be explained as three-quark states.","In particular, we identify the $\\Omega_{c}(3000)$/$\\Omega_{b}(6316)$, $\\Omega_{c}(3050)$/$\\Omega_{b}(6330)$, $\\Omega_{c}(3065)$/$\\Omega_{b}(6340)$ and $\\Omega_{c}(3090)$/$\\Omega_{b}(6350)$ states as the $p_{\\lambda}$ excitations with quantum numbers $1/2^{-}$, $3/2^{-}$, $3/2^{-}$ and $5/2^{-}$. The $\\Omega_{c}(3120)$ is a $3/2^{-}$ state with the $p_{\\rho}$ excitation, whose bottom partner is predicted to be $\\Omega_{b}(6446/6457,3/2^{-})$. The higher state $\\Omega_{c}(3188)$ is the $2s_{\\lambda}$ excitation with quantum numbers $1/2^{+}$, and $\\Omega_{c}(3327)$ is a $d_{\\lambda}$ excitation with quantum numbers $3/2^{+}$ or $5/2^{+}$. In addition, the $\\Lambda_{c}(2940)$ with quantum numbers $J^{P}=3/2^{-}$ could be explained as the $p_{\\rho}$ excitation."],"url":"http://arxiv.org/abs/2405.19039v1","category":"hep-ph"}
{"created":"2024-05-29 09:16:11","title":"Coexistence of Antiferromagnetic Cubic and Ferromagnetic Tetragonal Polymorphs in Epitaxial CuMnSb","abstract":"High-resolution transmission electron microscopy and superconducting quantum interference device magnetometry shows that epitaxial CuMnSb films exhibit a coexistence of two magnetic phases, coherently intertwined in nanometric scales. The dominant $\\alpha$~phase is half-Heusler cubic antiferromagnet with the N\\'{e}el temperature of 62~K, the equilibrium structure of bulk CuMnSb. The secondary phase is its ferromagnetic tetragonal $\\beta$ polymorph with the Curie temperature of about 100~K. First principles calculations provide a consistent interpretation of experiment, since (i) total energy of $\\beta$--CuMnSb is higher than that of $\\alpha$--CuMnSb only by 0.12~eV per formula unit, which allows for epitaxial stabilization of this phase, (ii) the metallic character of $\\beta$--CuMnSb favors the Ruderman-Kittel-Kasuya-Yoshida ferromagnetic coupling, and (iii) the calculated effective Curie-Weiss magnetic moment of Mn ions in both phases is about $5.5~\\mu_\\mathrm{B}$, favorably close to the measured value. Calculated properties of all point native defects indicate that the most likely to occur are $\\mathrm{Mn}_\\mathrm{Cu}$ antisites. They affect magnetic properties of epilayers, but they cannot induce the ferromagnetic order in CuMnSb. Combined, the findings highlight a practical route towards fabrication of functional materials in which coexisting polymorphs provide complementing functionalities in one host.","sentences":["High-resolution transmission electron microscopy and superconducting quantum interference device magnetometry shows that epitaxial CuMnSb films exhibit a coexistence of two magnetic phases, coherently intertwined in nanometric scales.","The dominant $\\alpha$~phase is half-Heusler cubic antiferromagnet with the N\\'{e}el temperature of 62~K, the equilibrium structure of bulk CuMnSb.","The secondary phase is its ferromagnetic tetragonal $\\beta$ polymorph with the Curie temperature of about 100~K. First principles calculations provide a consistent interpretation of experiment, since (i) total energy of $\\beta$--CuMnSb is higher than that of $\\alpha$--CuMnSb only by 0.12~eV per formula unit, which allows for epitaxial stabilization of this phase, (ii) the metallic character of $\\beta$--CuMnSb favors the Ruderman-Kittel-Kasuya-Yoshida ferromagnetic coupling, and (iii) the calculated effective Curie-Weiss magnetic moment of Mn ions in both phases is about $5.5~\\mu_\\mathrm{B}$, favorably close to the measured value.","Calculated properties of all point native defects indicate that the most likely to occur are $\\mathrm{Mn}_\\mathrm{Cu}$ antisites.","They affect magnetic properties of epilayers, but they cannot induce the ferromagnetic order in CuMnSb.","Combined, the findings highlight a practical route towards fabrication of functional materials in which coexisting polymorphs provide complementing functionalities in one host."],"url":"http://arxiv.org/abs/2405.18914v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 09:10:34","title":"Development of the X-ray polarimeter using CMOS imager: polarization sensitivity of a $1.5~{\\rm \u03bcm}$ pixel CMOS sensor","abstract":"We are developing an imaging polarimeter by combining a fine-pixel CMOS image sensor with a coded aperture mask as part of the cipher project, aiming to achieve X-ray polarimetry in the energy range of $10$$\\unicode{x2013}$$30~\\mathrm{keV}$. A successful proof-of-concept experiment was conducted using a fine-pixel CMOS sensor with a $2.5~\\mathrm{\\mu m}$ pixel size. In this study, we conducted beam experiments to assess the modulation factor (MF) of the CMOS sensor with a $1.5~\\mathrm{\\mu m}$ pixel size manufactured by Canon and to determine if there was any improvement in the MF. The measured MF was $8.32\\% \\pm 0.34\\%$ at $10~\\mathrm{keV}$ and $16.10\\% \\pm 0.68\\%$ at $22~\\mathrm{keV}$, exceeding those of the $2.5~\\mathrm{\\mu m}$ sensor in the $6$$\\unicode{x2013}$$22~\\mathrm{keV}$ range. We also evaluated the quantum efficiency of the sensor, inferring a detection layer thickness of $2.67 \\pm 0.48~{\\rm \\mu m}$. To develop a more sensitive polarimeter, a sensor with a thicker detection layer, smaller pixel size, and reduced thermal diffusion effect is desirable.","sentences":["We are developing an imaging polarimeter by combining a fine-pixel CMOS image sensor with a coded aperture mask as part of the cipher project, aiming to achieve X-ray polarimetry in the energy range of $10$$\\unicode{x2013}$$30~\\mathrm{keV}$. A successful proof-of-concept experiment was conducted using a fine-pixel CMOS sensor with a $2.5~\\mathrm{\\mu m}$ pixel size.","In this study, we conducted beam experiments to assess the modulation factor (MF) of the CMOS sensor with a $1.5~\\mathrm{\\mu m}$ pixel size manufactured by Canon and to determine if there was any improvement in the MF.","The measured MF was $8.32\\% \\pm 0.34\\%$ at $10~\\mathrm{keV}$ and $16.10\\% \\pm 0.68\\%$ at $22~\\mathrm{keV}$, exceeding those of the $2.5~\\mathrm{\\mu m}$ sensor in the $6$$\\unicode{x2013}$$22~\\mathrm{keV}$ range.","We also evaluated the quantum efficiency of the sensor, inferring a detection layer thickness of $2.67 \\pm 0.48~{\\rm \\mu m}$. To develop a more sensitive polarimeter, a sensor with a thicker detection layer, smaller pixel size, and reduced thermal diffusion effect is desirable."],"url":"http://arxiv.org/abs/2405.18907v1","category":"astro-ph.IM"}
{"created":"2024-05-29 08:29:40","title":"Quantum gravity signatures in gravitational wave detectors placed inside a harmonic trap potential","abstract":"In this work, we consider a general gravitational wave detector of gravitational wave interacting with an incoming gravitational wave carrying plus polarization only placed inside a harmonic trap. This model can be well acquainted with the description of a resonant detector of gravitational wave as well. The well known detector-gravitational wave interaction scenario uses the method of a semi classical approach where the detector is treated quantum mechanically but the gravitational wave is considered at a classical level. In our analysis, we use a discrete mode decomposition of the gravitational wave perturbation which results in a Hamiltonian involving the position and momentum operators corresponding to the gravitational wave and the harmonic oscillator. We have then calculated the transition probability for the harmonic oscillator-gravitational wave tensor product state for going from an initial state to some unknown final state. Using the energy flux relation of the gravitational waves, we observe that if we consider the total energy as a combination of the number of gravitons in the initial state of the detector then the transition probability for the resonant absorption case scenario takes the analytical form which is exactly similar to the semi-classical absorption case. In case of the emission scenario, we observe a spontaneous emission of a single graviton which was completely absent in the semi-classical analouge of this model. This therefore gives a direct signature of linearized quantum gravity.","sentences":["In this work, we consider a general gravitational wave detector of gravitational wave interacting with an incoming gravitational wave carrying plus polarization only placed inside a harmonic trap.","This model can be well acquainted with the description of a resonant detector of gravitational wave as well.","The well known detector-gravitational wave interaction scenario uses the method of a semi classical approach where the detector is treated quantum mechanically but the gravitational wave is considered at a classical level.","In our analysis, we use a discrete mode decomposition of the gravitational wave perturbation which results in a Hamiltonian involving the position and momentum operators corresponding to the gravitational wave and the harmonic oscillator.","We have then calculated the transition probability for the harmonic oscillator-gravitational wave tensor product state for going from an initial state to some unknown final state.","Using the energy flux relation of the gravitational waves, we observe that if we consider the total energy as a combination of the number of gravitons in the initial state of the detector then the transition probability for the resonant absorption case scenario takes the analytical form which is exactly similar to the semi-classical absorption case.","In case of the emission scenario, we observe a spontaneous emission of a single graviton which was completely absent in the semi-classical analouge of this model.","This therefore gives a direct signature of linearized quantum gravity."],"url":"http://arxiv.org/abs/2405.18868v1","category":"hep-th"}
{"created":"2024-05-29 08:10:14","title":"Electric Field Control of Molecular Charge State in a Single-Component 2D Organic Nanoarray","abstract":"Quantum dots (QD) with electric-field-controlled charge state are promising for electronics applications, e.g., digital information storage, single-electron transistors and quantum computing. Inorganic QDs consisting of semiconductor nanostructures or heterostructures often offer limited control on size and composition distribution, as well as low potential for scalability and/or nanoscale miniaturization. Owing to their tunability and self-assembly capability, using organic molecules as building nano-units can allow for bottom-up synthesis of two-dimensional (2D) nanoarrays of QDs. However, 2D molecular self-assembly protocols are often applicable on metals surfaces, where electronic hybridization and Fermi level pinning can hinder electric-field control of the QD charge state. Here, we demonstrate the synthesis of a single-component self-assembled 2D array of molecules [9, 10-dicyanoanthracene (DCA)] that exhibit electric-field-controlled spatially periodic charging on a noble metal surface, Ag(111). The charge state of DCA can be altered (between neutral and negative), depending on its adsorption site, by the local electric field induced by a scanning tunneling microscope tip. Limited metal-molecule interactions result in an effective tunneling barrier between DCA and Ag(111) that enables electric-field-induced electron population of the lowest unoccupied molecular orbital (LUMO) and hence charging of the molecule. Subtle site-dependent variation of the molecular adsorption height translates into a significant spatial modulation of the molecular polarizability, dielectric constant and LUMO energy level alignment, giving rise to a spatially dependent effective molecule-surface tunneling barrier and likelihood of charging. This work offers potential for high-density 2D self-assembled nanoarrays of identical QDs whose charge states can be addressed individually with an electric field.","sentences":["Quantum dots (QD) with electric-field-controlled charge state are promising for electronics applications, e.g., digital information storage, single-electron transistors and quantum computing.","Inorganic QDs consisting of semiconductor nanostructures or heterostructures often offer limited control on size and composition distribution, as well as low potential for scalability and/or nanoscale miniaturization.","Owing to their tunability and self-assembly capability, using organic molecules as building nano-units can allow for bottom-up synthesis of two-dimensional (2D) nanoarrays of QDs.","However, 2D molecular self-assembly protocols are often applicable on metals surfaces, where electronic hybridization and Fermi level pinning can hinder electric-field control of the QD charge state.","Here, we demonstrate the synthesis of a single-component self-assembled 2D array of molecules [9, 10-dicyanoanthracene (DCA)] that exhibit electric-field-controlled spatially periodic charging on a noble metal surface, Ag(111).","The charge state of DCA can be altered (between neutral and negative), depending on its adsorption site, by the local electric field induced by a scanning tunneling microscope tip.","Limited metal-molecule interactions result in an effective tunneling barrier between DCA and Ag(111) that enables electric-field-induced electron population of the lowest unoccupied molecular orbital (LUMO) and hence charging of the molecule.","Subtle site-dependent variation of the molecular adsorption height translates into a significant spatial modulation of the molecular polarizability, dielectric constant and LUMO energy level alignment, giving rise to a spatially dependent effective molecule-surface tunneling barrier and likelihood of charging.","This work offers potential for high-density 2D self-assembled nanoarrays of identical QDs whose charge states can be addressed individually with an electric field."],"url":"http://arxiv.org/abs/2405.18855v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 07:41:34","title":"Parameter-efficient Fine-tuning in Hyperspherical Space for Open-vocabulary Semantic Segmentation","abstract":"Open-vocabulary semantic segmentation seeks to label each pixel in an image with arbitrary text descriptions. Vision-language foundation models, especially CLIP, have recently emerged as powerful tools for acquiring open-vocabulary capabilities. However, fine-tuning CLIP to equip it with pixel-level prediction ability often suffers three issues: 1) high computational cost, 2) misalignment between the two inherent modalities of CLIP, and 3) degraded generalization ability on unseen categories. To address these issues, we propose H-CLIP a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both of the two CLIP modalities. Specifically, the PEFT strategy is achieved by a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices. Since the PEFT strategy is conducted symmetrically to the two CLIP modalities, the misalignment between them is mitigated. Furthermore, we apply an additional constraint to PEFT on the CLIP text encoder according to the hyperspherical energy principle, i.e., minimizing hyperspherical energy during fine-tuning preserves the intrinsic structure of the original parameter space, to prevent the destruction of the generalization ability offered by the CLIP text encoder. Extensive evaluations across various benchmarks show that H-CLIP achieves new SOTA open-vocabulary semantic segmentation results while only requiring updating approximately 4% of the total parameters of CLIP.","sentences":["Open-vocabulary semantic segmentation seeks to label each pixel in an image with arbitrary text descriptions.","Vision-language foundation models, especially CLIP, have recently emerged as powerful tools for acquiring open-vocabulary capabilities.","However, fine-tuning CLIP to equip it with pixel-level prediction ability often suffers three issues: 1) high computational cost, 2) misalignment between the two inherent modalities of CLIP, and 3) degraded generalization ability on unseen categories.","To address these issues, we propose H-CLIP a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both of the two CLIP modalities.","Specifically, the PEFT strategy is achieved by a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices.","Since the PEFT strategy is conducted symmetrically to the two CLIP modalities, the misalignment between them is mitigated.","Furthermore, we apply an additional constraint to PEFT on the CLIP text encoder according to the hyperspherical energy principle, i.e., minimizing hyperspherical energy during fine-tuning preserves the intrinsic structure of the original parameter space, to prevent the destruction of the generalization ability offered by the CLIP text encoder.","Extensive evaluations across various benchmarks show that H-CLIP achieves new SOTA open-vocabulary semantic segmentation results while only requiring updating approximately 4% of the total parameters of CLIP."],"url":"http://arxiv.org/abs/2405.18840v1","category":"cs.CV"}
{"created":"2024-05-29 06:59:37","title":"Encoding innumerable charge density waves of FeGe into polymorphs of LiFe6Ge6","abstract":"Kagome metals exhibit rich quantum states by the intertwining of lattice, charge, orbital and spin degrees of freedom. Recently, a novel charge density wave (CDW) ground state was discovered in kagome magnet FeGe and was revealed to be driven by lowering magnetic energy via large Ge1-dimerization. Here, based on DFT calculations, we show that such mechanism will yield infinitely many metastable CDWs in FeGe due to different ways to arrange the Ge1-dimerization in enlarged superstructures. Intriguingly, utilizing these metastable CDWs, innumerable polymorphs of kagome magnet LiFe6Ge6 can be stabilized by filling Li atoms in the voids right above/below the dimerized Ge1-sites in the CDW superstructures. Such polymorphs are very stable due to the presence of magnetic-energy-saving mechanism, in sharp contrast to the non-magnetic \"166\" kagome compounds. In this way, a one-to-one mapping of the metastable CDWs of FeGe to stable polymorphs of LiFe6Ge6 is established. On one hand, the fingerprints of these metastable CDWs, i.e., the induced in-plane atomic distortions and band gaps, are encoded into the corresponding stable polymorphs of LiFe6Ge6, such that further study of their properties becomes possible. On the other hand, such innumerable polymorphs of LiFe6Ge6 offer great degrees of freedom to explore the rich physics of magnetic kagome metals. We thus reveal a novel connection between the unusually abundant CDWs and structural polymorphism in magnetic kagome materials, and establish a new route to obtain structural polymorphism on top of CDW states.","sentences":["Kagome metals exhibit rich quantum states by the intertwining of lattice, charge, orbital and spin degrees of freedom.","Recently, a novel charge density wave (CDW) ground state was discovered in kagome magnet FeGe and was revealed to be driven by lowering magnetic energy via large Ge1-dimerization.","Here, based on DFT calculations, we show that such mechanism will yield infinitely many metastable CDWs in FeGe due to different ways to arrange the Ge1-dimerization in enlarged superstructures.","Intriguingly, utilizing these metastable CDWs, innumerable polymorphs of kagome magnet LiFe6Ge6 can be stabilized by filling Li atoms in the voids right above/below the dimerized Ge1-sites in the CDW superstructures.","Such polymorphs are very stable due to the presence of magnetic-energy-saving mechanism, in sharp contrast to the non-magnetic \"166\" kagome compounds.","In this way, a one-to-one mapping of the metastable CDWs of FeGe to stable polymorphs of LiFe6Ge6 is established.","On one hand, the fingerprints of these metastable CDWs, i.e., the induced in-plane atomic distortions and band gaps, are encoded into the corresponding stable polymorphs of LiFe6Ge6, such that further study of their properties becomes possible.","On the other hand, such innumerable polymorphs of LiFe6Ge6 offer great degrees of freedom to explore the rich physics of magnetic kagome metals.","We thus reveal a novel connection between the unusually abundant CDWs and structural polymorphism in magnetic kagome materials, and establish a new route to obtain structural polymorphism on top of CDW states."],"url":"http://arxiv.org/abs/2405.18819v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-29 06:58:06","title":"Dark energy due to quantum corrections to effective potential","abstract":"In this paper we calculate quantum corrections to the effective potential in different models of inflationary cosmology. We show that quantum corrections lead to a modification of the initial potential uplifting its value at the minimum, which can be interpreted as a cosmological constant/dark energy. We concentrate on the models of $\\alpha$-attractors and show that one can naturally get small values of the cosmological constant. In the model of quintessence, the same mechanism modifies the value of dark energy at the present time.","sentences":["In this paper we calculate quantum corrections to the effective potential in different models of inflationary cosmology.","We show that quantum corrections lead to a modification of the initial potential uplifting its value at the minimum, which can be interpreted as a cosmological constant/dark energy.","We concentrate on the models of $\\alpha$-attractors and show that one can naturally get small values of the cosmological constant.","In the model of quintessence, the same mechanism modifies the value of dark energy at the present time."],"url":"http://arxiv.org/abs/2405.18818v1","category":"hep-th"}
{"created":"2024-05-29 05:37:35","title":"On the moments of averages of quadratic twists of the M\u00f6bius function","abstract":"We consider the moment of quadratic twists of the M\\\"obius function of the form \\[   S_k(X,Y) = \\sum_{d\\leq X} \\left( \\sum_{n\\leq Y} \\left(\\frac{8d}{n}\\right) \\mu(n)\\right)^k, \\] where $\\left(\\frac{8d}{\\cdot}\\right)$ is the Kronecker symbol and $d$ runs over positive, odd and square-free integers. We give unconditional results for their asymptotic behaviors.","sentences":["We consider the moment of quadratic twists of the M\\\"obius function of the form \\[   S_k(X,Y) = \\sum_{d\\leq X} \\left( \\sum_{n\\leq Y} \\left(\\frac{8d}{n}\\right) \\mu(n)\\right)^k, \\] where $\\left(\\frac{8d}{\\cdot}\\right)$ is the Kronecker symbol and $d$ runs over positive, odd and square-free integers.","We give unconditional results for their asymptotic behaviors."],"url":"http://arxiv.org/abs/2405.18778v1","category":"math.NT"}
{"created":"2024-05-29 04:53:36","title":"Ponderomotive electron physics captured in single-fluid extended MHD model","abstract":"The well-known ponderomotive force, arising from the interaction of matter and light, has critical implications across a broad range of fields from laser fusion and astrophysics to laser diagnostics and even pulsed-power experiments. This pseudo-potential pushes electrons, which through coulomb forces causes ion density modulations that can steepen with profound implications. When used intentionally, density modulations can be used for plasma gratings, which are essential for optical components operating in extreme conditions for next generation lasers. They can also be important for plasma confinement and particle trapping, which can even impact magnetic confinement in fusion devices. The ponderomotive potential also leads to laser self-focusing, complicating laser diagnostics. In laser fusion, the force exacerbates challenges posed by stimulated Brillouin scattering (SBS) and crossed beam energy transfer (CBET), both of which destabilize the fusion process. It even plays an astrophysical role in the filamentation of fast radio bursts in the relativistic winds of magnetars. Since the ponderomotive force primarily effects electron dynamics, multi-fluid/particle codes or additional ansatz are required to include its effects. This paper demonstrates that by including electron effects on an ion timescale with a 1-fluid, 2-energy extended magnetohydrodynamics (XMHD) model, ponderomotive effects are also naturally present. We introduce the theory for these dynamics and demonstrate their presence with 1-D pencil-like simulations.","sentences":["The well-known ponderomotive force, arising from the interaction of matter and light, has critical implications across a broad range of fields from laser fusion and astrophysics to laser diagnostics and even pulsed-power experiments.","This pseudo-potential pushes electrons, which through coulomb forces causes ion density modulations that can steepen with profound implications.","When used intentionally, density modulations can be used for plasma gratings, which are essential for optical components operating in extreme conditions for next generation lasers.","They can also be important for plasma confinement and particle trapping, which can even impact magnetic confinement in fusion devices.","The ponderomotive potential also leads to laser self-focusing, complicating laser diagnostics.","In laser fusion, the force exacerbates challenges posed by stimulated Brillouin scattering (SBS) and crossed beam energy transfer (CBET), both of which destabilize the fusion process.","It even plays an astrophysical role in the filamentation of fast radio bursts in the relativistic winds of magnetars.","Since the ponderomotive force primarily effects electron dynamics, multi-fluid/particle codes or additional ansatz are required to include its effects.","This paper demonstrates that by including electron effects on an ion timescale with a 1-fluid, 2-energy extended magnetohydrodynamics (XMHD) model, ponderomotive effects are also naturally present.","We introduce the theory for these dynamics and demonstrate their presence with 1-D pencil-like simulations."],"url":"http://arxiv.org/abs/2405.18759v1","category":"physics.plasm-ph"}
{"created":"2024-05-29 04:41:54","title":"Feynman integrals and Fox functions","abstract":"In this work we discuss the connection between Feynman integrals and Fox functions. Illustrative examples are given.","sentences":["In this work we discuss the connection between Feynman integrals and Fox functions.","Illustrative examples are given."],"url":"http://arxiv.org/abs/2405.18755v1","category":"hep-ph"}
{"created":"2024-05-29 03:47:00","title":"Coherent synchrotron radiation instability in low-emittance electron storage rings","abstract":"Longitudinal impedances at high frequencies, which extend far beyond the width of the beam spectrum, can pose a threat to the performance of modern low-emittance electron storage rings, as they can establish a relatively low threshold for microwave instability. In such rings, coherent synchrotron radiation (CSR) emerges as a prominent contributor to these high-frequency impedances. This paper undertakes a systematic investigation into the effects of CSR on electron rings, utilizing Elettra 2.0, a ring of fourth-generation light sources, and the SuperKEKB low-energy ring, a ring of $e^+e^-$ circular colliders, as illustrative examples. Our work revisits theories of microwave instability driven by CSR impedance, extending the analysis to encompass other high-frequency impedances such as resistive wall and coherent wiggler radiation. Through instability analysis and numerical simulations conducted on the two aforementioned rings, the study explored the impact of high-frequency impedances and their interactions with broadband impedances from discontinuities in vacuum chambers.","sentences":["Longitudinal impedances at high frequencies, which extend far beyond the width of the beam spectrum, can pose a threat to the performance of modern low-emittance electron storage rings, as they can establish a relatively low threshold for microwave instability.","In such rings, coherent synchrotron radiation (CSR) emerges as a prominent contributor to these high-frequency impedances.","This paper undertakes a systematic investigation into the effects of CSR on electron rings, utilizing Elettra 2.0, a ring of fourth-generation light sources, and the SuperKEKB low-energy ring, a ring of $e^+e^-$ circular colliders, as illustrative examples.","Our work revisits theories of microwave instability driven by CSR impedance, extending the analysis to encompass other high-frequency impedances such as resistive wall and coherent wiggler radiation.","Through instability analysis and numerical simulations conducted on the two aforementioned rings, the study explored the impact of high-frequency impedances and their interactions with broadband impedances from discontinuities in vacuum chambers."],"url":"http://arxiv.org/abs/2405.18738v1","category":"physics.acc-ph"}
{"created":"2024-05-29 03:38:09","title":"Effects of alloying elements on carbon diffusion in the austenite (f.c.c.) and ferrite (b.c.c.) phases","abstract":"TThe effects of alloying elements on diffusion pathways and migration energies of interstitial carbon in austenite (f.c.c.) and ferrite (b.c.c.) are studied using density functional theory first-principles calculations. The binding energies between carbon and alloying elements are determined through 6th nearest-neighbor (NN) distances. The elements studied are Ni, Mo, V, Cr, Mn, Cu, Al, Ti, and Si, relevant to most high-strength steels. Nickel, Mn, Al, and Si have repulsive binding energies; Mo, V, Cr, Cu, and Ti have attractive binding energies in austenite and ferrite. Alloying elements at 1st NN sites of a C atom in an octahedral site introduce asymmetry into the minimum energy diffusion pathway, causing up to about 1 eV changes in saddle-point energies. This pathway goes from one octahedral site to another via intermediate energy states, differing for austenite and ferrite. We find that the elements with attractive binding energies increase the energy barrier for C migration resulting in decelerated carbon diffusion, while the elements with repulsive binding energies decrease the energy barrier for C migration leading to accelerated C diffusion. The magnitude of changes in C migration energies is proportional to the binding energies between C and alloying elements. Among the three austenite stabilizers, Ni and Mn are C diffusion accelerators, while Cu decelerates C diffusion in austenite. Among the four ferrite stabilizers, Si is a C diffusion accelerator, while V and Ti serve as C diffusion decelerators in ferrite. Aluminum has no significant effect on C's diffusivity, while Mo and Cr decelerate C diffusion.","sentences":["TThe effects of alloying elements on diffusion pathways and migration energies of interstitial carbon in austenite (f.c.c.) and ferrite (b.c.c.) are studied using density functional theory first-principles calculations.","The binding energies between carbon and alloying elements are determined through 6th nearest-neighbor (NN) distances.","The elements studied are Ni, Mo, V, Cr, Mn, Cu, Al, Ti, and Si, relevant to most high-strength steels.","Nickel, Mn, Al, and Si have repulsive binding energies; Mo, V, Cr, Cu, and Ti have attractive binding energies in austenite and ferrite.","Alloying elements at 1st NN sites of a C atom in an octahedral site introduce asymmetry into the minimum energy diffusion pathway, causing up to about 1 eV changes in saddle-point energies.","This pathway goes from one octahedral site to another via intermediate energy states, differing for austenite and ferrite.","We find that the elements with attractive binding energies increase the energy barrier for C migration resulting in decelerated carbon diffusion, while the elements with repulsive binding energies decrease the energy barrier for C migration leading to accelerated C diffusion.","The magnitude of changes in C migration energies is proportional to the binding energies between C and alloying elements.","Among the three austenite stabilizers, Ni and Mn are C diffusion accelerators, while Cu decelerates C diffusion in austenite.","Among the four ferrite stabilizers, Si is a C diffusion accelerator, while V and Ti serve as C diffusion decelerators in ferrite.","Aluminum has no significant effect on C's diffusivity, while Mo and Cr decelerate C diffusion."],"url":"http://arxiv.org/abs/2405.18736v1","category":"physics.atom-ph"}
{"created":"2024-05-29 02:59:27","title":"Machine-Learning based photon counting for PMT waveforms and its application to the improvement of the energy resolution in large liquid scintillator detectors","abstract":"Photomultiplier tubes (PMTs) are widely used in particle experiments for photon detection. PMT waveform analysis is crucial for high-precision measurement of the position and energy of incident particles in liquid scintillator (LS) detectors. A key factor contributing to the energy resolution in large liquid scintillator detectors with PMTs is the charge smearing of PMTs. This paper presents a machine-learning-based photon counting method for PMT waveforms and its application to the energy reconstruction, using the JUNO experiment as an example. The results indicate that leveraging the photon counting information from the machine learning model can partially mitigate the impact of PMT charge smearing and lead to a relative 2.0% to 2.8% improvement on the energy resolution at different energies.","sentences":["Photomultiplier tubes (PMTs) are widely used in particle experiments for photon detection.","PMT waveform analysis is crucial for high-precision measurement of the position and energy of incident particles in liquid scintillator (LS) detectors.","A key factor contributing to the energy resolution in large liquid scintillator detectors with PMTs is the charge smearing of PMTs.","This paper presents a machine-learning-based photon counting method for PMT waveforms and its application to the energy reconstruction, using the JUNO experiment as an example.","The results indicate that leveraging the photon counting information from the machine learning model can partially mitigate the impact of PMT charge smearing and lead to a relative 2.0% to 2.8% improvement on the energy resolution at different energies."],"url":"http://arxiv.org/abs/2405.18720v1","category":"physics.ins-det"}
{"created":"2024-05-29 02:54:18","title":"Silicon-integrated scandium-doped aluminum nitride electro-optic modulator","abstract":"Scandium-doped aluminum nitride (AlScN) with an asymmetric hexagonal wurtzite structure exhibits enhanced second-order nonlinear and piezoelectric properties compared to aluminum nitride (AlN), while maintaining a relatively large bandgap. It provides a promising platform for photonic integration and facilitates the seamless integration of passive and active functional devices. Here, we present the design, fabrication, and characterization of AlScN EO micro-ring modulators, introducing active functionalities to the chip-scale AlScN platform. These waveguide-integrated EO modulators employ sputtered AlScN thin films as the light-guiding medium, and the entire fabrication process is compatible with complementary metal oxide semiconductor (CMOS) technology. We characterize the high-frequency performance of an AlScN modulator for the first time, extracting a maximum in-device effective EO coefficient of 2.86 pm/V at 12 GHz. The devices show a minimum half-wave voltage-length product of 3.12 V*cm and a 3-dB modulation bandwidth of approximately 22 GHz. Our work provides a promising modulation scheme for cost-effective silicon-integrated photonics systems.","sentences":["Scandium-doped aluminum nitride (AlScN) with an asymmetric hexagonal wurtzite structure exhibits enhanced second-order nonlinear and piezoelectric properties compared to aluminum nitride (AlN), while maintaining a relatively large bandgap.","It provides a promising platform for photonic integration and facilitates the seamless integration of passive and active functional devices.","Here, we present the design, fabrication, and characterization of AlScN EO micro-ring modulators, introducing active functionalities to the chip-scale AlScN platform.","These waveguide-integrated EO modulators employ sputtered AlScN thin films as the light-guiding medium, and the entire fabrication process is compatible with complementary metal oxide semiconductor (CMOS) technology.","We characterize the high-frequency performance of an AlScN modulator for the first time, extracting a maximum in-device effective EO coefficient of 2.86 pm/V at 12 GHz.","The devices show a minimum half-wave voltage-length product of 3.12 V*cm and a 3-dB modulation bandwidth of approximately 22 GHz.","Our work provides a promising modulation scheme for cost-effective silicon-integrated photonics systems."],"url":"http://arxiv.org/abs/2405.18717v1","category":"physics.app-ph"}
{"created":"2024-05-29 02:51:30","title":"Eighth-Order Foldy-Wouthuysen Transformation","abstract":"The calculation of higher-order binding corrections to bound systems is a fundamental problem of theoretical physics. For any nonrelativistic expansion, one needs the Foldy-Wouthuysen Transformation which disentangles the particle and the antiparticle degrees of freedom. This transformation is carried out here to eighth order in the momenta, or, to eighth order in the momentum operators, which is equivalent to the eighth order of the fine-structure constant. Matrix elements of the eighth-order terms are evaluated for F_5/2 and F_7/2 states in hydrogenlike ions and compared with the Dirac-Coulomb energy levels.","sentences":["The calculation of higher-order binding corrections to bound systems is a fundamental problem of theoretical physics.","For any nonrelativistic expansion, one needs the Foldy-Wouthuysen Transformation which disentangles the particle and the antiparticle degrees of freedom.","This transformation is carried out here to eighth order in the momenta, or, to eighth order in the momentum operators, which is equivalent to the eighth order of the fine-structure constant.","Matrix elements of the eighth-order terms are evaluated for F_5/2 and F_7/2 states in hydrogenlike ions and compared with the Dirac-Coulomb energy levels."],"url":"http://arxiv.org/abs/2405.18714v1","category":"physics.atom-ph"}
{"created":"2024-05-29 02:34:13","title":"FocSAM: Delving Deeply into Focused Objects in Segmenting Anything","abstract":"The Segment Anything Model (SAM) marks a notable milestone in segmentation models, highlighted by its robust zero-shot capabilities and ability to handle diverse prompts. SAM follows a pipeline that separates interactive segmentation into image preprocessing through a large encoder and interactive inference via a lightweight decoder, ensuring efficient real-time performance. However, SAM faces stability issues in challenging samples upon this pipeline. These issues arise from two main factors. Firstly, the image preprocessing disables SAM from dynamically using image-level zoom-in strategies to refocus on the target object during interaction. Secondly, the lightweight decoder struggles to sufficiently integrate interactive information with image embeddings. To address these two limitations, we propose FocSAM with a pipeline redesigned on two pivotal aspects. First, we propose Dynamic Window Multi-head Self-Attention (Dwin-MSA) to dynamically refocus SAM's image embeddings on the target object. Dwin-MSA localizes attention computations around the target object, enhancing object-related embeddings with minimal computational overhead. Second, we propose Pixel-wise Dynamic ReLU (P-DyReLU) to enable sufficient integration of interactive information from a few initial clicks that have significant impacts on the overall segmentation results. Experimentally, FocSAM augments SAM's interactive segmentation performance to match the existing state-of-the-art method in segmentation quality, requiring only about 5.6% of this method's inference time on CPUs.","sentences":["The Segment Anything Model (SAM) marks a notable milestone in segmentation models, highlighted by its robust zero-shot capabilities and ability to handle diverse prompts.","SAM follows a pipeline that separates interactive segmentation into image preprocessing through a large encoder and interactive inference via a lightweight decoder, ensuring efficient real-time performance.","However, SAM faces stability issues in challenging samples upon this pipeline.","These issues arise from two main factors.","Firstly, the image preprocessing disables SAM from dynamically using image-level zoom-in strategies to refocus on the target object during interaction.","Secondly, the lightweight decoder struggles to sufficiently integrate interactive information with image embeddings.","To address these two limitations, we propose FocSAM with a pipeline redesigned on two pivotal aspects.","First, we propose Dynamic Window Multi-head Self-Attention (Dwin-MSA) to dynamically refocus SAM's image embeddings on the target object.","Dwin-MSA localizes attention computations around the target object, enhancing object-related embeddings with minimal computational overhead.","Second, we propose Pixel-wise Dynamic ReLU (P-DyReLU) to enable sufficient integration of interactive information from a few initial clicks that have significant impacts on the overall segmentation results.","Experimentally, FocSAM augments SAM's interactive segmentation performance to match the existing state-of-the-art method in segmentation quality, requiring only about 5.6% of this method's inference time on CPUs."],"url":"http://arxiv.org/abs/2405.18706v1","category":"cs.CV"}
{"created":"2024-05-29 02:24:17","title":"Two classes of Majorana neutrinos in the seesaw model","abstract":"The commonly used pseudo-C symmetry $(\\nu_{L})^{c}=C\\overline{\\nu_{L}}^{T}$ is not defined in Lagrangian field theory. In general, there exist two classes of Majorana fermions; the first is associated with the chiral fermion with the conventional C and P symmetries, and the second is associated with the Weyl fermion defined by CP symmetry only and formally characterised by the pseudo-C symmetry. Taking the seesaw model as an example, it is shown that a generalized Pauli--G\\\"{u}rsey (or Bogoliubov-type) canonical transformation converts the neutrino defined by the Weyl fermion to the neutrino defined by the chiral fermion and thus to the conventional Majorana fermion, while preserving the anti-commutation relations. The mixing angles in the weak lepton sector are not modified by this generalized Pauli--G\\\"{u}rsey transformation.","sentences":["The commonly used pseudo-C symmetry $(\\nu_{L})^{c}=C\\overline{\\nu_{L}}^{T}$ is not defined in Lagrangian field theory.","In general, there exist two classes of Majorana fermions; the first is associated with the chiral fermion with the conventional C and P symmetries, and the second is associated with the Weyl fermion defined by CP symmetry only and formally characterised by the pseudo-C symmetry.","Taking the seesaw model as an example, it is shown that a generalized Pauli--G\\\"{u}rsey (or Bogoliubov-type) canonical transformation converts the neutrino defined by the Weyl fermion to the neutrino defined by the chiral fermion and thus to the conventional Majorana fermion, while preserving the anti-commutation relations.","The mixing angles in the weak lepton sector are not modified by this generalized Pauli--G\\\"{u}rsey transformation."],"url":"http://arxiv.org/abs/2405.18702v1","category":"hep-ph"}
