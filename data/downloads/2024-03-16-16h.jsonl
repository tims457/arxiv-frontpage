{"created":"2024-03-14 17:59:55","title":"SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior","abstract":"Semantic image synthesis (SIS) shows good promises for sensor simulation. However, current best practices in this field, based on GANs, have not yet reached the desired level of quality. As latent diffusion models make significant strides in image generation, we are prompted to evaluate ControlNet, a notable method for its dense control capabilities. Our investigation uncovered two primary issues with its results: the presence of weird sub-structures within large semantic areas and the misalignment of content with the semantic mask. Through empirical study, we pinpointed the cause of these problems as a mismatch between the noised training data distribution and the standard normal prior applied at the inference stage. To address this challenge, we developed specific noise priors for SIS, encompassing spatial, categorical, and a novel spatial-categorical joint prior for inference. This approach, which we have named SCP-Diff, has yielded exceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on ADE20K.The code and models can be accessed via the project page.","sentences":["Semantic image synthesis (SIS) shows good promises for sensor simulation.","However, current best practices in this field, based on GANs, have not yet reached the desired level of quality.","As latent diffusion models make significant strides in image generation, we are prompted to evaluate ControlNet, a notable method for its dense control capabilities.","Our investigation uncovered two primary issues with its results: the presence of weird sub-structures within large semantic areas and the misalignment of content with the semantic mask.","Through empirical study, we pinpointed the cause of these problems as a mismatch between the noised training data distribution and the standard normal prior applied at the inference stage.","To address this challenge, we developed specific noise priors for SIS, encompassing spatial, categorical, and a novel spatial-categorical joint prior for inference.","This approach, which we have named SCP-Diff, has yielded exceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on ADE20K.The code and models can be accessed via the project page."],"url":"http://arxiv.org/abs/2403.09638v1","category":"cs.CV"}
{"created":"2024-03-14 17:59:46","title":"GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping","abstract":"Constructing a 3D scene capable of accommodating open-ended language queries, is a pivotal pursuit, particularly within the domain of robotics. Such technology facilitates robots in executing object manipulations based on human language directives. To tackle this challenge, some research efforts have been dedicated to the development of language-embedded implicit fields. However, implicit fields (e.g. NeRF) encounter limitations due to the necessity of processing a large number of input views for reconstruction, coupled with their inherent inefficiencies in inference. Thus, we present the GaussianGrasper, which utilizes 3D Gaussian Splatting to explicitly represent the scene as a collection of Gaussian primitives. Our approach takes a limited set of RGB-D views and employs a tile-based splatting technique to create a feature field. In particular, we propose an Efficient Feature Distillation (EFD) module that employs contrastive learning to efficiently and accurately distill language embeddings derived from foundational models. With the reconstructed geometry of the Gaussian field, our method enables the pre-trained grasping model to generate collision-free grasp pose candidates. Furthermore, we propose a normal-guided grasp module to select the best grasp pose. Through comprehensive real-world experiments, we demonstrate that GaussianGrasper enables robots to accurately query and grasp objects with language instructions, providing a new solution for language-guided manipulation tasks. Data and codes can be available at https://github.com/MrSecant/GaussianGrasper.","sentences":["Constructing a 3D scene capable of accommodating open-ended language queries, is a pivotal pursuit, particularly within the domain of robotics.","Such technology facilitates robots in executing object manipulations based on human language directives.","To tackle this challenge, some research efforts have been dedicated to the development of language-embedded implicit fields.","However, implicit fields (e.g. NeRF) encounter limitations due to the necessity of processing a large number of input views for reconstruction, coupled with their inherent inefficiencies in inference.","Thus, we present the GaussianGrasper, which utilizes 3D Gaussian Splatting to explicitly represent the scene as a collection of Gaussian primitives.","Our approach takes a limited set of RGB-D views and employs a tile-based splatting technique to create a feature field.","In particular, we propose an Efficient Feature Distillation (EFD) module that employs contrastive learning to efficiently and accurately distill language embeddings derived from foundational models.","With the reconstructed geometry of the Gaussian field, our method enables the pre-trained grasping model to generate collision-free grasp pose candidates.","Furthermore, we propose a normal-guided grasp module to select the best grasp pose.","Through comprehensive real-world experiments, we demonstrate that GaussianGrasper enables robots to accurately query and grasp objects with language instructions, providing a new solution for language-guided manipulation tasks.","Data and codes can be available at https://github.com/MrSecant/GaussianGrasper."],"url":"http://arxiv.org/abs/2403.09637v1","category":"cs.RO"}
{"created":"2024-03-14 17:59:26","title":"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference","abstract":"Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time. Most importantly, the model learns to apply different compression rates in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA). GQA and DMC can be even combined to obtain compounded gains. As a result DMC fits longer contexts and larger batches within any given memory budget.","sentences":["Transformers have emerged as the backbone of large language models (LLMs).","However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size.","As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time.","Most importantly, the model learns to apply different compression rates in different heads and layers.","We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU.","DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters.","We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA).","GQA and DMC can be even combined to obtain compounded gains.","As a result DMC fits longer contexts and larger batches within any given memory budget."],"url":"http://arxiv.org/abs/2403.09636v1","category":"cs.CL"}
{"created":"2024-03-14 17:59:14","title":"Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models","abstract":"In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These improvements also translate into improved performance on downstream Question Answering tasks and improved robustness for image classification.","sentences":["In spite of their huge success, transformer models remain difficult to scale in depth.","In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model.","Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores.","We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers.","We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes.","These improvements also translate into improved performance on downstream Question Answering tasks and improved robustness for image classification."],"url":"http://arxiv.org/abs/2403.09635v1","category":"cs.CL"}
{"created":"2024-03-14 17:59:13","title":"OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning","abstract":"Visual object tracking aims to localize the target object of each frame based on its initial appearance in the first frame. Depending on the input modility, tracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and RGB+D) tracking. Despite the different input modalities, the core aspect of tracking is the temporal matching. Based on this common ground, we present a general framework to unify various tracking tasks, termed as OneTracker. OneTracker first performs a large-scale pre-training on a RGB tracker called Foundation Tracker. This pretraining phase equips the Foundation Tracker with a stable ability to estimate the location of the target object. Then we regard other modality information as prompt and build Prompt Tracker upon Foundation Tracker. Through freezing the Foundation Tracker and only adjusting some additional trainable parameters, Prompt Tracker inhibits the strong localization ability from Foundation Tracker and achieves parameter-efficient finetuning on downstream RGB+X tracking tasks. To evaluate the effectiveness of our general framework OneTracker, which is consisted of Foundation Tracker and Prompt Tracker, we conduct extensive experiments on 6 popular tracking tasks across 11 benchmarks and our OneTracker outperforms other models and achieves state-of-the-art performance.","sentences":["Visual object tracking aims to localize the target object of each frame based on its initial appearance in the first frame.","Depending on the input modility, tracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and RGB+D) tracking.","Despite the different input modalities, the core aspect of tracking is the temporal matching.","Based on this common ground, we present a general framework to unify various tracking tasks, termed as OneTracker.","OneTracker first performs a large-scale pre-training on a RGB tracker called Foundation Tracker.","This pretraining phase equips the Foundation Tracker with a stable ability to estimate the location of the target object.","Then we regard other modality information as prompt and build Prompt Tracker upon Foundation Tracker.","Through freezing the Foundation Tracker and only adjusting some additional trainable parameters, Prompt Tracker inhibits the strong localization ability from Foundation Tracker and achieves parameter-efficient finetuning on downstream RGB+X tracking tasks.","To evaluate the effectiveness of our general framework OneTracker, which is consisted of Foundation Tracker and Prompt Tracker, we conduct extensive experiments on 6 popular tracking tasks across 11 benchmarks and our OneTracker outperforms other models and achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.09634v1","category":"cs.CV"}
{"created":"2024-03-14 17:58:56","title":"Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image","abstract":"At the core of portrait photography is the search for ideal lighting and viewpoint. The process often requires advanced knowledge in photography and an elaborate studio setup. In this work, we propose Holo-Relighting, a volumetric relighting method that is capable of synthesizing novel viewpoints, and novel lighting from a single image. Holo-Relighting leverages the pretrained 3D GAN (EG3D) to reconstruct geometry and appearance from an input portrait as a set of 3D-aware features. We design a relighting module conditioned on a given lighting to process these features, and predict a relit 3D representation in the form of a tri-plane, which can render to an arbitrary viewpoint through volume rendering. Besides viewpoint and lighting control, Holo-Relighting also takes the head pose as a condition to enable head-pose-dependent lighting effects. With these novel designs, Holo-Relighting can generate complex non-Lambertian lighting effects (e.g., specular highlights and cast shadows) without using any explicit physical lighting priors. We train Holo-Relighting with data captured with a light stage, and propose two data-rendering techniques to improve the data quality for training the volumetric relighting system. Through quantitative and qualitative experiments, we demonstrate Holo-Relighting can achieve state-of-the-arts relighting quality with better photorealism, 3D consistency and controllability.","sentences":["At the core of portrait photography is the search for ideal lighting and viewpoint.","The process often requires advanced knowledge in photography and an elaborate studio setup.","In this work, we propose Holo-Relighting, a volumetric relighting method that is capable of synthesizing novel viewpoints, and novel lighting from a single image.","Holo-Relighting leverages the pretrained 3D GAN (EG3D) to reconstruct geometry and appearance from an input portrait as a set of 3D-aware features.","We design a relighting module conditioned on a given lighting to process these features, and predict a relit 3D representation in the form of a tri-plane, which can render to an arbitrary viewpoint through volume rendering.","Besides viewpoint and lighting control, Holo-Relighting also takes the head pose as a condition to enable head-pose-dependent lighting effects.","With these novel designs, Holo-Relighting can generate complex non-Lambertian lighting effects (e.g., specular highlights and cast shadows) without using any explicit physical lighting priors.","We train Holo-Relighting with data captured with a light stage, and propose two data-rendering techniques to improve the data quality for training the volumetric relighting system.","Through quantitative and qualitative experiments, we demonstrate Holo-Relighting can achieve state-of-the-arts relighting quality with better photorealism, 3D consistency and controllability."],"url":"http://arxiv.org/abs/2403.09632v1","category":"cs.CV"}
{"created":"2024-03-14 17:58:41","title":"3D-VLA: A 3D Vision-Language-Action Generative World Model","abstract":"Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.","sentences":["Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world.","Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics.","In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly.","To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model.","Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment.","Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds.","To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets.","Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications."],"url":"http://arxiv.org/abs/2403.09631v1","category":"cs.CV"}
{"created":"2024-03-14 17:58:33","title":"Generalized Predictive Model for Autonomous Driving","abstract":"In this paper, we introduce the first large-scale video prediction model in the autonomous driving discipline. To eliminate the restriction of high-cost data collection and empower the generalization ability of our model, we acquire massive data from the web and pair it with diverse and high-quality text descriptions. The resultant dataset accumulates over 2000 hours of driving videos, spanning areas all over the world with diverse weather conditions and traffic scenarios. Inheriting the merits from recent latent diffusion models, our model, dubbed GenAD, handles the challenging dynamics in driving scenes with novel temporal reasoning blocks. We showcase that it can generalize to various unseen driving datasets in a zero-shot manner, surpassing general or driving-specific video prediction counterparts. Furthermore, GenAD can be adapted into an action-conditioned prediction model or a motion planner, holding great potential for real-world driving applications.","sentences":["In this paper, we introduce the first large-scale video prediction model in the autonomous driving discipline.","To eliminate the restriction of high-cost data collection and empower the generalization ability of our model, we acquire massive data from the web and pair it with diverse and high-quality text descriptions.","The resultant dataset accumulates over 2000 hours of driving videos, spanning areas all over the world with diverse weather conditions and traffic scenarios.","Inheriting the merits from recent latent diffusion models, our model, dubbed GenAD, handles the challenging dynamics in driving scenes with novel temporal reasoning blocks.","We showcase that it can generalize to various unseen driving datasets in a zero-shot manner, surpassing general or driving-specific video prediction counterparts.","Furthermore, GenAD can be adapted into an action-conditioned prediction model or a motion planner, holding great potential for real-world driving applications."],"url":"http://arxiv.org/abs/2403.09630v1","category":"cs.CV"}
{"created":"2024-03-14 17:58:16","title":"Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking","abstract":"When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.","sentences":["When writing and talking, people sometimes pause to think.","Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text.","For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation.","In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer.","This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text.","We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions.","We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens.","To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique.","Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions.","In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%$\\rightarrow$10.9%) and CommonsenseQA","(36.3%$\\rightarrow$47.2%) and observe a perplexity improvement of difficult tokens in natural text.","Crucially, these improvements require no fine-tuning on these tasks.","Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way."],"url":"http://arxiv.org/abs/2403.09629v1","category":"cs.CL"}
{"created":"2024-03-14 17:57:04","title":"Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation","abstract":"Recent years have witnessed the strong power of 3D generation models, which offer a new level of creative flexibility by allowing users to guide the 3D content generation process through a single image or natural language. However, it remains challenging for existing 3D generation methods to create subject-driven 3D content across diverse prompts. In this paper, we introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 minutes. Our key insight is to harmonize the distributions of a multi-view diffusion model and an identity-specific 2D generative model, aligning them with the distribution of the desired 3D subject. Specifically, we design a co-evolution framework to reduce the variance of distributions, where each model undergoes a process of learning from the other through identity-aware optimization and subject-prior optimization, respectively. Extensive experiments demonstrate that our method can produce high-quality, consistent, and subject-specific 3D content with text-driven modifications that are unseen in subject image.","sentences":["Recent years have witnessed the strong power of 3D generation models, which offer a new level of creative flexibility by allowing users to guide the 3D content generation process through a single image or natural language.","However, it remains challenging for existing 3D generation methods to create subject-driven 3D content across diverse prompts.","In this paper, we introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 minutes.","Our key insight is to harmonize the distributions of a multi-view diffusion model and an identity-specific 2D generative model, aligning them with the distribution of the desired 3D subject.","Specifically, we design a co-evolution framework to reduce the variance of distributions, where each model undergoes a process of learning from the other through identity-aware optimization and subject-prior optimization, respectively.","Extensive experiments demonstrate that our method can produce high-quality, consistent, and subject-specific 3D content with text-driven modifications that are unseen in subject image."],"url":"http://arxiv.org/abs/2403.09625v1","category":"cs.CV"}
{"created":"2024-03-14 17:55:33","title":"Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering","abstract":"Visual text rendering poses a fundamental challenge for contemporary text-to-image generation models, with the core problem lying in text encoder deficiencies. To achieve accurate text rendering, we identify two crucial requirements for text encoders: character awareness and alignment with glyphs. Our solution involves crafting a series of customized text encoder, Glyph-ByT5, by fine-tuning the character-aware ByT5 encoder using a meticulously curated paired glyph-text dataset. We present an effective method for integrating Glyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for design image generation. This significantly enhances text rendering accuracy, improving it from less than $20\\%$ to nearly $90\\%$ on our design image benchmark. Noteworthy is Glyph-SDXL's newfound ability for text paragraph rendering, achieving high spelling accuracy for tens to hundreds of characters with automated multi-line layouts. Finally, through fine-tuning Glyph-SDXL with a small set of high-quality, photorealistic images featuring visual text, we showcase a substantial improvement in scene text rendering capabilities in open-domain real images. These compelling outcomes aim to encourage further exploration in designing customized text encoders for diverse and challenging tasks.","sentences":["Visual text rendering poses a fundamental challenge for contemporary text-to-image generation models, with the core problem lying in text encoder deficiencies.","To achieve accurate text rendering, we identify two crucial requirements for text encoders: character awareness and alignment with glyphs.","Our solution involves crafting a series of customized text encoder, Glyph-ByT5, by fine-tuning the character-aware ByT5 encoder using a meticulously curated paired glyph-text dataset.","We present an effective method for integrating Glyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for design image generation.","This significantly enhances text rendering accuracy, improving it from less than $20\\%$ to nearly $90\\%$ on our design image benchmark.","Noteworthy is Glyph-SDXL's newfound ability for text paragraph rendering, achieving high spelling accuracy for tens to hundreds of characters with automated multi-line layouts.","Finally, through fine-tuning Glyph-SDXL with a small set of high-quality, photorealistic images featuring visual text, we showcase a substantial improvement in scene text rendering capabilities in open-domain real images.","These compelling outcomes aim to encourage further exploration in designing customized text encoders for diverse and challenging tasks."],"url":"http://arxiv.org/abs/2403.09622v1","category":"cs.CV"}
{"created":"2024-03-14 17:55:10","title":"Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning","abstract":"Distributionally robust offline reinforcement learning (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces. However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation. Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust offline RL. Our results uncover that function approximation in robust offline RL is essentially distinct from and probably harder than that in standard offline RL. Our algorithms and theoretical results crucially depend on a variety of new techniques, involving a novel function approximation mechanism incorporating variance information, a new procedure of suboptimality and estimation uncertainty decomposition, a quantification of the robust value function shrinkage, and a meticulously designed family of hard instances, which might be of independent interest.","sentences":["Distributionally robust offline reinforcement learning (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces.","However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation.","Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust offline RL.","Our results uncover that function approximation in robust offline RL is essentially distinct from and probably harder than that in standard offline RL.","Our algorithms and theoretical results crucially depend on a variety of new techniques, involving a novel function approximation mechanism incorporating variance information, a new procedure of suboptimality and estimation uncertainty decomposition, a quantification of the robust value function shrinkage, and a meticulously designed family of hard instances, which might be of independent interest."],"url":"http://arxiv.org/abs/2403.09621v1","category":"cs.LG"}
{"created":"2024-03-14 17:55:03","title":"PosSAM: Panoptic Open-vocabulary Segment Anything","abstract":"In this paper, we introduce an open-vocabulary panoptic segmentation model that effectively unifies the strengths of the Segment Anything Model (SAM) with the vision-language CLIP model in an end-to-end framework. While SAM excels in generating spatially-aware masks, it's decoder falls short in recognizing object class information and tends to oversegment without additional guidance. Existing approaches address this limitation by using multi-stage techniques and employing separate models to generate class-aware prompts, such as bounding boxes or segmentation masks. Our proposed method, PosSAM is an end-to-end model which leverages SAM's spatially rich features to produce instance-aware masks and harnesses CLIP's semantically discriminative features for effective instance classification. Specifically, we address the limitations of SAM and propose a novel Local Discriminative Pooling (LDP) module leveraging class-agnostic SAM and class-aware CLIP features for unbiased open-vocabulary classification. Furthermore, we introduce a Mask-Aware Selective Ensembling (MASE) algorithm that adaptively enhances the quality of generated masks and boosts the performance of open-vocabulary classification during inference for each image. We conducted extensive experiments to demonstrate our methods strong generalization properties across multiple datasets, achieving state-of-the-art performance with substantial improvements over SOTA open-vocabulary panoptic segmentation methods. In both COCO to ADE20K and ADE20K to COCO settings, PosSAM outperforms the previous state-of-the-art methods by a large margin, 2.4 PQ and 4.6 PQ, respectively. Project Website: https://vibashan.github.io/possam-web/.","sentences":["In this paper, we introduce an open-vocabulary panoptic segmentation model that effectively unifies the strengths of the Segment Anything Model (SAM) with the vision-language CLIP model in an end-to-end framework.","While SAM excels in generating spatially-aware masks, it's decoder falls short in recognizing object class information and tends to oversegment without additional guidance.","Existing approaches address this limitation by using multi-stage techniques and employing separate models to generate class-aware prompts, such as bounding boxes or segmentation masks.","Our proposed method, PosSAM is an end-to-end model which leverages SAM's spatially rich features to produce instance-aware masks and harnesses CLIP's semantically discriminative features for effective instance classification.","Specifically, we address the limitations of SAM and propose a novel Local Discriminative Pooling (LDP) module leveraging class-agnostic SAM and class-aware CLIP features for unbiased open-vocabulary classification.","Furthermore, we introduce a Mask-Aware Selective Ensembling (MASE) algorithm that adaptively enhances the quality of generated masks and boosts the performance of open-vocabulary classification during inference for each image.","We conducted extensive experiments to demonstrate our methods strong generalization properties across multiple datasets, achieving state-of-the-art performance with substantial improvements over SOTA open-vocabulary panoptic segmentation methods.","In both COCO to ADE20K and ADE20K to COCO settings, PosSAM outperforms the previous state-of-the-art methods by a large margin, 2.4 PQ and 4.6 PQ, respectively.","Project Website: https://vibashan.github.io/possam-web/."],"url":"http://arxiv.org/abs/2403.09620v1","category":"cs.CV"}
{"created":"2024-03-14 17:54:27","title":"Dynamics of Pseudoentanglement","abstract":"The dynamics of quantum entanglement plays a central role in explaining the emergence of thermal equilibrium in isolated many-body systems. However, entanglement is notoriously hard to measure, and can in fact be forged: recent works have introduced a notion of pseudoentanglement describing ensembles of many-body states that, while only weakly entangled, cannot be efficiently distinguished from states with much higher entanglement, such as random states in the Hilbert space. In this work we initiate the study of the dynamical generation and propagation of pseudoentanglement. As generic quantum dynamics tends to maximize actual entanglement, we consider constrained models of time evolution: automaton (i.e. reversible classical) circuits that, when fed suitable input states, provably produce the \"standard models\" of pseudoentangled ensembles--uniformly random subset(-phase) states--at late times, a phenomenon we name 'pseudothermalization'. We examine (i) how a pseudoentangled ensemble on a small subsystem spreads to the whole system as a function of time, and (ii) how a pseudoentangled ensemble is generated from an initial product state. We map the above problems onto a family of classical Markov chains on subsets of the computational basis. The mixing times of such Markov chains are related to the time scales at which the states produced from the dynamics become indistinguishable from Haar-random states at the level of each statistical moment (or number of copies). Based on a combination of rigorous bounds and conjectures supported by numerics, we argue that each Markov chain's relaxation time and mixing time have different asymptotic behavior in the limit of large system size. This is a necessary condition for a cutoff phenomenon: an abrupt dynamical transition to equilibrium. We thus conjecture that our random circuits give rise to asymptotically sharp pseudothermalization transitions.","sentences":["The dynamics of quantum entanglement plays a central role in explaining the emergence of thermal equilibrium in isolated many-body systems.","However, entanglement is notoriously hard to measure, and can in fact be forged: recent works have introduced a notion of pseudoentanglement describing ensembles of many-body states that, while only weakly entangled, cannot be efficiently distinguished from states with much higher entanglement, such as random states in the Hilbert space.","In this work we initiate the study of the dynamical generation and propagation of pseudoentanglement.","As generic quantum dynamics tends to maximize actual entanglement, we consider constrained models of time evolution: automaton (i.e. reversible classical) circuits that, when fed suitable input states, provably produce the \"standard models\" of pseudoentangled ensembles--uniformly random subset(-phase) states--at late times, a phenomenon we name 'pseudothermalization'.","We examine (i) how a pseudoentangled ensemble on a small subsystem spreads to the whole system as a function of time, and (ii) how a pseudoentangled ensemble is generated from an initial product state.","We map the above problems onto a family of classical Markov chains on subsets of the computational basis.","The mixing times of such Markov chains are related to the time scales at which the states produced from the dynamics become indistinguishable from Haar-random states at the level of each statistical moment (or number of copies).","Based on a combination of rigorous bounds and conjectures supported by numerics, we argue that each Markov chain's relaxation time and mixing time have different asymptotic behavior in the limit of large system size.","This is a necessary condition for a cutoff phenomenon: an abrupt dynamical transition to equilibrium.","We thus conjecture that our random circuits give rise to asymptotically sharp pseudothermalization transitions."],"url":"http://arxiv.org/abs/2403.09619v1","category":"quant-ph"}
{"created":"2024-03-14 17:54:20","title":"Generating functional of correlators of twist-$2$ operators in $\\mathcal{N} = 1$ SUSY Yang-Mills theory, I","abstract":"Extending our previous work in pure Yang-Mills (YM) theory, we compute the generating functional of correlators of collinear twist-$2$ operators that enter the components of balanced superfields -- i.e., superfields with an equal number of dotted and undotted indices in their spinor representation -- in $\\mathcal{N} = 1$ SUSY SU($N$) YM theory in Minkowskian and Euclidean space-time, in the conformal limit and renormalization-group improved form, and to the leading and next-to-leading order in the large-$N$ expansion. The latter calculation sets strong UV asymptotic constraints on the nonperturbative solution of large-$N$ $\\mathcal{N} = 1$ SUSY YM theory that may be a pivotal guide for the search of such a solution.","sentences":["Extending our previous work in pure Yang-Mills (YM) theory, we compute the generating functional of correlators of collinear twist-$2$ operators that enter the components of balanced superfields -- i.e., superfields with an equal number of dotted and undotted indices in their spinor representation -- in $\\mathcal{N} = 1$ SUSY SU($N$) YM theory in Minkowskian and Euclidean space-time, in the conformal limit and renormalization-group improved form, and to the leading and next-to-leading order in the large-$N$ expansion.","The latter calculation sets strong UV asymptotic constraints on the nonperturbative solution of large-$N$ $\\mathcal{N} = 1$ SUSY YM theory that may be a pivotal guide for the search of such a solution."],"url":"http://arxiv.org/abs/2403.09617v1","category":"hep-th"}
{"created":"2024-03-14 17:52:31","title":"Explore In-Context Segmentation via Latent Diffusion Models","abstract":"In-context segmentation has drawn more attention with the introduction of vision foundation models. Most existing approaches adopt metric learning or masked image modeling to build the correlation between visual prompts and input image queries. In this work, we explore this problem from a new perspective, using one representative generation model, the latent diffusion model (LDM). We observe a task gap between generation and segmentation in diffusion models, but LDM is still an effective minimalist for in-context segmentation. In particular, we propose two meta-architectures and correspondingly design several output alignment and optimization strategies. We have conducted comprehensive ablation studies and empirically found that the segmentation quality counts on output alignment and in-context instructions. Moreover, we build a new and fair in-context segmentation benchmark that includes both image and video datasets. Experiments validate the efficiency of our approach, demonstrating comparable or even stronger results than previous specialist models or visual foundation models. Our study shows that LDMs can also achieve good enough results for challenging in-context segmentation tasks.","sentences":["In-context segmentation has drawn more attention with the introduction of vision foundation models.","Most existing approaches adopt metric learning or masked image modeling to build the correlation between visual prompts and input image queries.","In this work, we explore this problem from a new perspective, using one representative generation model, the latent diffusion model (LDM).","We observe a task gap between generation and segmentation in diffusion models, but LDM is still an effective minimalist for in-context segmentation.","In particular, we propose two meta-architectures and correspondingly design several output alignment and optimization strategies.","We have conducted comprehensive ablation studies and empirically found that the segmentation quality counts on output alignment and in-context instructions.","Moreover, we build a new and fair in-context segmentation benchmark that includes both image and video datasets.","Experiments validate the efficiency of our approach, demonstrating comparable or even stronger results than previous specialist models or visual foundation models.","Our study shows that LDMs can also achieve good enough results for challenging in-context segmentation tasks."],"url":"http://arxiv.org/abs/2403.09616v1","category":"cs.CV"}
{"created":"2024-03-14 17:52:17","title":"PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation","abstract":"Generative text-to-image models, which allow users to create appealing images through a text prompt, have seen a dramatic increase in popularity in recent years. However, most users have a limited understanding of how such models work and it often requires many trials and errors to achieve satisfactory results. The prompt history contains a wealth of information that could provide users with insights into what have been explored and how the prompt changes impact the output image, yet little research attention has been paid to the visual analysis of such process to support users. We propose the Image Variant Graph, a novel visual representation designed to support comparing prompt-image pairs and exploring the editing history. The Image Variant Graph models prompt differences as edges between corresponding images and presents the distances between images through projection. Based on the graph, we developed the PrompTHis system through co-design with artists. Besides Image Variant Graph, PrompTHis also incorporates a detailed prompt-image history and a navigation mini-map. Based on the review and analysis of the prompting history, users can better understand the impact of prompt changes and have a more effective control of image generation. A quantitative user study with eleven amateur participants and qualitative interviews with five professionals and one amateur user were conducted to evaluate the effectiveness of PrompTHis. The results demonstrate PrompTHis can help users review the prompt history, make sense of the model, and plan their creative process.","sentences":["Generative text-to-image models, which allow users to create appealing images through a text prompt, have seen a dramatic increase in popularity in recent years.","However, most users have a limited understanding of how such models work and it often requires many trials and errors to achieve satisfactory results.","The prompt history contains a wealth of information that could provide users with insights into what have been explored and how the prompt changes impact the output image, yet little research attention has been paid to the visual analysis of such process to support users.","We propose the Image Variant Graph, a novel visual representation designed to support comparing prompt-image pairs and exploring the editing history.","The Image Variant Graph models prompt differences as edges between corresponding images and presents the distances between images through projection.","Based on the graph, we developed the PrompTHis system through co-design with artists.","Besides Image Variant Graph, PrompTHis also incorporates a detailed prompt-image history and a navigation mini-map.","Based on the review and analysis of the prompting history, users can better understand the impact of prompt changes and have a more effective control of image generation.","A quantitative user study with eleven amateur participants and qualitative interviews with five professionals and one amateur user were conducted to evaluate the effectiveness of PrompTHis.","The results demonstrate PrompTHis can help users review the prompt history, make sense of the model, and plan their creative process."],"url":"http://arxiv.org/abs/2403.09615v1","category":"cs.HC"}
{"created":"2024-03-14 17:50:24","title":"Generative reconstruction of 3D volume elements for Ti-6Al-4V basketweave microstructure by optimization of CNN-based microstructural descriptors","abstract":"We present a methodology for the generative reconstruction of 3D Volume Elements (VE) for numerical multiscale analysis of Ti-6Al-4V processed by Additive Manufacturing (AM). The basketweave morphology, which is typically dominant in AM-processed Ti-6Al-4V, is analyzed in conventional Electron Backscatter Diffusion (EBSD) micrographs. Prior \\b{eta}-grain reconstruction is performed to obtain the out-of-plane orientation of the observed grains leveraging Burgers orientation relationship. Convolutional Neural Network (CNN) - based microstructure descriptors are extracted from the 2D data, and used for cross-section-based optimization of pixel values on orthogonal planes in 3D, using the Microstructure Characterization and Reconstruction (MCR) implementation MCRpy [16]. In order to utilize MCRpy, which performs best for binary systems, the basketweave microstructure, which consists of up to twelve distinct grain orientations, is decomposed into several separate two-phase systems. Our reconstructions capture key characteristics of the titanium basketweave morphology and show qualitative resemblance to experimentally obtained 3D data. The preservation of volume fraction during assembly of the reconstruction remains an unadressed challenge at this stage.","sentences":["We present a methodology for the generative reconstruction of 3D Volume Elements (VE) for numerical multiscale analysis of Ti-6Al-4V processed by Additive Manufacturing (AM).","The basketweave morphology, which is typically dominant in AM-processed Ti-6Al-4V, is analyzed in conventional Electron Backscatter Diffusion (EBSD) micrographs.","Prior \\b{eta}-grain reconstruction is performed to obtain the out-of-plane orientation of the observed grains leveraging Burgers orientation relationship.","Convolutional Neural Network (CNN) - based microstructure descriptors are extracted from the 2D data, and used for cross-section-based optimization of pixel values on orthogonal planes in 3D, using the Microstructure Characterization and Reconstruction (MCR) implementation MCRpy","[16].","In order to utilize MCRpy, which performs best for binary systems, the basketweave microstructure, which consists of up to twelve distinct grain orientations, is decomposed into several separate two-phase systems.","Our reconstructions capture key characteristics of the titanium basketweave morphology and show qualitative resemblance to experimentally obtained 3D data.","The preservation of volume fraction during assembly of the reconstruction remains an unadressed challenge at this stage."],"url":"http://arxiv.org/abs/2403.09609v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-14 17:49:58","title":"Interaction-Driven Instabilities in the Random-Field XXZ Chain","abstract":"Despite enormous efforts devoted to the study of the many-body localization (MBL) phenomenon, the nature of the high-energy behavior of the Heisenberg spin chain in a strong random magnetic field is lacking consensus. Here, we take a step back by exploring the weak interaction limit starting from the Anderson localized (AL) insulator. Through shift-invert diagonalization, we find that below a certain disorder threshold $h^*$, weak interactions necessarily lead to ergodic instability, whereas at strong disorder the AL insulator directly turns into MBL. This agrees with a simple interpretation of the avalanche theory for restoration of ergodicity. We further map the phase diagram for the generic XXZ model in the disorder $h$ -- interaction $\\Delta$ plane. Taking advantage of the total magnetization conservation, our results unveil the remarkable behavior of the spin-spin correlation functions: in the regime indicated as MBL by standard observables, their exponential decay undergoes a unique inversion of orientation $\\xi_z>\\xi_x$. We find that the longitudinal length $\\xi_z$ is a key quantity for capturing ergodic instabilities, as it increases with system size near the thermal phase, in sharp contrast to its transverse counterpart $\\xi_x$.","sentences":["Despite enormous efforts devoted to the study of the many-body localization (MBL) phenomenon, the nature of the high-energy behavior of the Heisenberg spin chain in a strong random magnetic field is lacking consensus.","Here, we take a step back by exploring the weak interaction limit starting from the Anderson localized (AL) insulator.","Through shift-invert diagonalization, we find that below a certain disorder threshold $h^*$, weak interactions necessarily lead to ergodic instability, whereas at strong disorder the AL insulator directly turns into MBL.","This agrees with a simple interpretation of the avalanche theory for restoration of ergodicity.","We further map the phase diagram for the generic XXZ model in the disorder $h$ -- interaction $\\Delta$ plane.","Taking advantage of the total magnetization conservation, our results unveil the remarkable behavior of the spin-spin correlation functions: in the regime indicated as MBL by standard observables, their exponential decay undergoes a unique inversion of orientation $\\xi_z>\\xi_x$. We find that the longitudinal length $\\xi_z$ is a key quantity for capturing ergodic instabilities, as it increases with system size near the thermal phase, in sharp contrast to its transverse counterpart $\\xi_x$."],"url":"http://arxiv.org/abs/2403.09608v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-14 17:47:20","title":"Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey","abstract":"Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective potential to further the development of more advanced and equitable artificial intelligence systems.","sentences":["Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables.","The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities.","This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality.","Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations.","This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective potential to further the development of more advanced and equitable artificial intelligence systems."],"url":"http://arxiv.org/abs/2403.09606v1","category":"cs.CL"}
{"created":"2024-03-14 17:47:01","title":"Counterfactual contrastive learning: robust representations via causal image synthesis","abstract":"Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic information while destroying domain-specific information. Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead? In this work, we show how to utilise recent progress in counterfactual image generation to this effect. We propose CF-SimCLR, a counterfactual contrastive learning approach which leverages approximate counterfactual inference for positive pair creation. Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- and out-of-distribution data, particularly for domains which are under-represented during training.","sentences":["Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings.","However, it is sensitive to the choice of augmentation pipeline.","Positive pairs should preserve semantic information while destroying domain-specific information.","Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead?","In this work, we show how to utilise recent progress in counterfactual image generation to this effect.","We propose CF-SimCLR, a counterfactual contrastive learning approach which leverages approximate counterfactual inference for positive pair creation.","Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- and out-of-distribution data, particularly for domains which are under-represented during training."],"url":"http://arxiv.org/abs/2403.09605v1","category":"cs.CV"}
{"created":"2024-03-14 17:44:35","title":"Optimistic Verifiable Training by Controlling Hardware Nondeterminism","abstract":"The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and \"optimistic\" methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thresholding procedure, to successfully control for nondeterminism. Across three different NVIDIA GPUs (A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32 precision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2 (117M) models. Our verifiable training scheme significantly decreases the storage and time costs compared to proof-based systems.","sentences":["The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources.","However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges.","Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and \"optimistic\" methods that consider a trusted third-party auditor who replicates the training process.","A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust.","We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thresholding procedure, to successfully control for nondeterminism.","Across three different NVIDIA GPUs (A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32 precision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2 (117M) models.","Our verifiable training scheme significantly decreases the storage and time costs compared to proof-based systems."],"url":"http://arxiv.org/abs/2403.09603v1","category":"cs.CR"}
{"created":"2024-03-14 17:44:22","title":"Parafermions with symmetry-protected non-Abelian statistics","abstract":"Non-Abelian anyons have garnered extensive attention for obeying exotic non-Abelian statistics and potential applications to fault-tolerant quantum computation. Although the prior research has predominantly focused on non-Abelian statistics without the necessity of symmetry protection, recent progresses have shown that symmetries can play essential roles and bring about a notion of the symmetry-protected non-Abelian (SPNA) statistics. In this work, we extend the concept of SPNA statistics to strongly-correlated systems which host parafermion zero modes (PZMs). This study involves a few fundamental results proved here. First, we unveil a generic unitary symmetry mechanism that protects PZMs from local couplings. Then, with this symmetry protection, the PZMs can be categorized into two nontrivial sectors, each maintaining its own parity conservation, even though the whole system cannot be dismantled into separate subsystems due to nonlinear interactions. Finally, by leveraging the parity conservation of each sector and the general properties of the effective braiding Hamiltonian, we prove rigorously that the PZMs intrinsically obey SPNA statistics. To further confirm the results, we derive the braiding matrix at a tri-junction. In addition, we propose a physical model that accommodates a pair of PZMs protected by mirror symmetry and satisfying the generic theory. This work shows a broad spectrum of strongly-correlated systems capable of hosting fractional SPNA quasiparticles and enriches our comprehension of fundamental quantum statistics linked to the symmetries that govern the exchange dynamics.","sentences":["Non-Abelian anyons have garnered extensive attention for obeying exotic non-Abelian statistics and potential applications to fault-tolerant quantum computation.","Although the prior research has predominantly focused on non-Abelian statistics without the necessity of symmetry protection, recent progresses have shown that symmetries can play essential roles and bring about a notion of the symmetry-protected non-Abelian (SPNA) statistics.","In this work, we extend the concept of SPNA statistics to strongly-correlated systems which host parafermion zero modes (PZMs).","This study involves a few fundamental results proved here.","First, we unveil a generic unitary symmetry mechanism that protects PZMs from local couplings.","Then, with this symmetry protection, the PZMs can be categorized into two nontrivial sectors, each maintaining its own parity conservation, even though the whole system cannot be dismantled into separate subsystems due to nonlinear interactions.","Finally, by leveraging the parity conservation of each sector and the general properties of the effective braiding Hamiltonian, we prove rigorously that the PZMs intrinsically obey SPNA statistics.","To further confirm the results, we derive the braiding matrix at a tri-junction.","In addition, we propose a physical model that accommodates a pair of PZMs protected by mirror symmetry and satisfying the generic theory.","This work shows a broad spectrum of strongly-correlated systems capable of hosting fractional SPNA quasiparticles and enriches our comprehension of fundamental quantum statistics linked to the symmetries that govern the exchange dynamics."],"url":"http://arxiv.org/abs/2403.09602v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 17:42:10","title":"Network-Controlled Repeater -- An Introduction","abstract":"In fifth generation (5G) wireless cellular networks, millimeter wave spectrum opens room for several potential improvements in throughput, reliability, latency, among other aspects. However, it also brings challenges, such as a higher influence of blockage which may significantly limit the coverage. In this context, network-controlled repeaters (NCRs) are network nodes with low complexity that represent a technique to overcome coverage problems. In this paper, we introduce the NCR concept and study its performance gains and deployment options. Particularly, presenting the main specifications of NCR as agreed in 3rd generation partnership project (3GPP) Rel-18, we analyze different NCR deployments in an urban scenario and compare its performance with alternative deployments. As demonstrated, with a proper network planning and beamforming design, NCR is an attractive solution to cover blind spots the base stations (BSs) may have.","sentences":["In fifth generation (5G) wireless cellular networks, millimeter wave spectrum opens room for several potential improvements in throughput, reliability, latency, among other aspects.","However, it also brings challenges, such as a higher influence of blockage which may significantly limit the coverage.","In this context, network-controlled repeaters (NCRs) are network nodes with low complexity that represent a technique to overcome coverage problems.","In this paper, we introduce the NCR concept and study its performance gains and deployment options.","Particularly, presenting the main specifications of NCR as agreed in 3rd generation partnership project (3GPP) Rel-18, we analyze different NCR deployments in an urban scenario and compare its performance with alternative deployments.","As demonstrated, with a proper network planning and beamforming design, NCR is an attractive solution to cover blind spots the base stations (BSs) may have."],"url":"http://arxiv.org/abs/2403.09601v1","category":"cs.NI"}
{"created":"2024-03-14 17:40:20","title":"Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis","abstract":"Given the emergent reasoning abilities of large language models, information retrieval is becoming more complex. Rather than just retrieve a document, modern information retrieval systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using reasoning. We review recent literature and argue that the large language model has crucial flaws that prevent it from on its own ever constituting general intelligence, or answering general information synthesis requests. This review shows that the following are problems for large language models: hallucinations, complex reasoning, planning under uncertainty, and complex calculations. We outline how logical discrete graphical models can solve all of these problems, and outline a method of training a logical discrete model from unlabeled text.","sentences":["Given the emergent reasoning abilities of large language models, information retrieval is becoming more complex.","Rather than just retrieve a document, modern information retrieval systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using reasoning.","We review recent literature and argue that the large language model has crucial flaws that prevent it from on its own ever constituting general intelligence, or answering general information synthesis requests.","This review shows that the following are problems for large language models: hallucinations, complex reasoning, planning under uncertainty, and complex calculations.","We outline how logical discrete graphical models can solve all of these problems, and outline a method of training a logical discrete model from unlabeled text."],"url":"http://arxiv.org/abs/2403.09599v1","category":"cs.IR"}
{"created":"2024-03-14 17:37:03","title":"Tidal evolution of cored and cuspy dark matter halos","abstract":"The internal structure and abundance of dark matter halos and subhalos are powerful probes of the nature of dark matter. In order to compare observations with dark matter models, accurate theoretical predictions of these quantities are needed. We present a fast and accurate method to describe the tidal evolution of subhalos within their parent halo, based on a semi-analytic approach. We first consider idealized N-body simulations of subhalos within their host halo, using a generalized mass density profile that describes their properties in a variety of dark matter models at infall, including popular warm, cold, and self-interacting ones. Using these simulations we construct tidal \"tracks\" for the evolution of subhalos based on their conditions at infall. Second, we use the results of these simulations to build semi-analytic models (SAMs) for tidal effects, including stripping and heating and implement them within the code GALACTICUS. Our SAMs can accurately predict the tidal evolution of both cored and cuspy subhalos, including the bound mass and density profiles, providing a powerful and efficient tool for studying the post-infall properties of subhalos in different dark matter models.","sentences":["The internal structure and abundance of dark matter halos and subhalos are powerful probes of the nature of dark matter.","In order to compare observations with dark matter models, accurate theoretical predictions of these quantities are needed.","We present a fast and accurate method to describe the tidal evolution of subhalos within their parent halo, based on a semi-analytic approach.","We first consider idealized N-body simulations of subhalos within their host halo, using a generalized mass density profile that describes their properties in a variety of dark matter models at infall, including popular warm, cold, and self-interacting ones.","Using these simulations we construct tidal \"tracks\" for the evolution of subhalos based on their conditions at infall.","Second, we use the results of these simulations to build semi-analytic models (SAMs) for tidal effects, including stripping and heating and implement them within the code GALACTICUS.","Our SAMs can accurately predict the tidal evolution of both cored and cuspy subhalos, including the bound mass and density profiles, providing a powerful and efficient tool for studying the post-infall properties of subhalos in different dark matter models."],"url":"http://arxiv.org/abs/2403.09597v1","category":"astro-ph.GA"}
{"created":"2024-03-14 17:35:32","title":"Renovating Names in Open-Vocabulary Segmentation Benchmarks","abstract":"Names are essential to both human cognition and vision-language models. Open-vocabulary models utilize class names as text prompts to generalize to categories unseen during training. However, name qualities are often overlooked and lack sufficient precision in existing datasets. In this paper, we address this underexplored problem by presenting a framework for \"renovating\" names in open-vocabulary segmentation benchmarks (RENOVATE). Through human study, we demonstrate that the names generated by our model are more precise descriptions of the visual segments and hence enhance the quality of existing datasets by means of simple renaming. We further demonstrate that using our renovated names enables training of stronger open-vocabulary segmentation models. Using open-vocabulary segmentation for name quality evaluation, we show that our renovated names lead to up to 16% relative improvement from the original names on various benchmarks across various state-of-the-art models. We provide our code and relabelings for several popular segmentation datasets (ADE20K, Cityscapes, PASCAL Context) to the research community.","sentences":["Names are essential to both human cognition and vision-language models.","Open-vocabulary models utilize class names as text prompts to generalize to categories unseen during training.","However, name qualities are often overlooked and lack sufficient precision in existing datasets.","In this paper, we address this underexplored problem by presenting a framework for \"renovating\" names in open-vocabulary segmentation benchmarks (RENOVATE).","Through human study, we demonstrate that the names generated by our model are more precise descriptions of the visual segments and hence enhance the quality of existing datasets by means of simple renaming.","We further demonstrate that using our renovated names enables training of stronger open-vocabulary segmentation models.","Using open-vocabulary segmentation for name quality evaluation, we show that our renovated names lead to up to 16% relative improvement from the original names on various benchmarks across various state-of-the-art models.","We provide our code and relabelings for several popular segmentation datasets (ADE20K, Cityscapes, PASCAL Context) to the research community."],"url":"http://arxiv.org/abs/2403.09593v1","category":"cs.CV"}
{"created":"2024-03-14 17:35:19","title":"DungeonMaker: Embedding Tangible Creation and Destruction in Hybrid Board Games through Personal Fabrication Technology","abstract":"Hybrid board games (HBGs) augment their analog origins digitally (e.g., through apps) and are an increasingly popular pastime activity. Continuous world and character development and customization, known to facilitate engagement in video games, remain rare in HBGs. If present, they happen digitally or imaginarily, often leaving physical aspects generic. We developed DungeonMaker, a fabrication-augmented HBG bridging physical and digital game elements: 1) the setup narrates a story and projects a digital game board onto a laser cutter; 2) DungeonMaker assesses player-crafted artifacts; 3) DungeonMaker's modified laser head senses and moves player- and non-player figures, and 4) can physically damage figures. An evaluation (n=4x3) indicated that DungeonMaker provides an engaging experience, may support players' connection to their figures, and potentially spark novices' interest in fabrication. DungeonMaker provides a rich constellation to play HBGs by blending aspects of craft and automation to couple the physical and digital elements of an HBG tightly.","sentences":["Hybrid board games (HBGs) augment their analog origins digitally (e.g., through apps) and are an increasingly popular pastime activity.","Continuous world and character development and customization, known to facilitate engagement in video games, remain rare in HBGs.","If present, they happen digitally or imaginarily, often leaving physical aspects generic.","We developed DungeonMaker, a fabrication-augmented HBG bridging physical and digital game elements: 1) the setup narrates a story and projects a digital game board onto a laser cutter; 2) DungeonMaker assesses player-crafted artifacts; 3) DungeonMaker's modified laser head senses and moves player- and non-player figures, and 4) can physically damage figures.","An evaluation (n=4x3) indicated that DungeonMaker provides an engaging experience, may support players' connection to their figures, and potentially spark novices' interest in fabrication.","DungeonMaker provides a rich constellation to play HBGs by blending aspects of craft and automation to couple the physical and digital elements of an HBG tightly."],"url":"http://arxiv.org/abs/2403.09592v1","category":"cs.HC"}
{"created":"2024-03-14 17:19:01","title":"Functions Analytic at Infinity and Normality","abstract":"Given a charge and current distribution with compact support, the associated potentials and fields are generally not integrable in the classical sense. However, it is convenient to be able to define their Fourier transform in order to create solutions to the wave equation. This paper develops the technology for this by considering the class of quasi split normal functions, examples of which are the solutions to Poisson's equation with a forcing term having compact support.","sentences":["Given a charge and current distribution with compact support, the associated potentials and fields are generally not integrable in the classical sense.","However, it is convenient to be able to define their Fourier transform in order to create solutions to the wave equation.","This paper develops the technology for this by considering the class of quasi split normal functions, examples of which are the solutions to Poisson's equation with a forcing term having compact support."],"url":"http://arxiv.org/abs/2403.09584v1","category":"math-ph"}
{"created":"2024-03-14 17:15:30","title":"Universal Definitions of the Roman Factorial: Introduction to Foundational Functions and the Generalization Process","abstract":"This paper introduces a new method for redefining the Roman factorial using universally applicable functions that are not expressed in closed form. We present a set of foundational functions, similar to Boolean operations, to simplify the factorial expression. Through a systematic process of generalization, termed generalization process, we aim to use these foundational functions to create recursive and non-recursive, global definitions of the Roman factorial.","sentences":["This paper introduces a new method for redefining the Roman factorial using universally applicable functions that are not expressed in closed form.","We present a set of foundational functions, similar to Boolean operations, to simplify the factorial expression.","Through a systematic process of generalization, termed generalization process, we aim to use these foundational functions to create recursive and non-recursive, global definitions of the Roman factorial."],"url":"http://arxiv.org/abs/2403.09581v1","category":"math.CO"}
{"created":"2024-03-14 17:14:53","title":"Algorithmic syntactic causal identification","abstract":"Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on CBNs. However, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. We show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. In this alternative axiomatization, we show how an unambiguous and clean distinction can be drawn between the general syntax of causal models and any specific semantic implementation of that causal model. This allows a purely syntactic algorithmic description of general causal identification by a translation of recent formulations of the general ID algorithm through fixing. Our description is given entirely in terms of the non-parametric ADMG structure specifying a causal model and the algebraic signature of the corresponding monoidal category, to which a sequence of manipulations is then applied so as to arrive at a modified monoidal category in which the desired, purely syntactic interventional causal model, is obtained. We use this idea to derive purely syntactic analogues of classical back-door and front-door causal adjustment, and illustrate an application to a more complex causal model.","sentences":["Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle.","However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on CBNs.","However, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms.","We show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories.","In this alternative axiomatization, we show how an unambiguous and clean distinction can be drawn between the general syntax of causal models and any specific semantic implementation of that causal model.","This allows a purely syntactic algorithmic description of general causal identification by a translation of recent formulations of the general ID algorithm through fixing.","Our description is given entirely in terms of the non-parametric ADMG structure specifying a causal model and the algebraic signature of the corresponding monoidal category, to which a sequence of manipulations is then applied so as to arrive at a modified monoidal category in which the desired, purely syntactic interventional causal model, is obtained.","We use this idea to derive purely syntactic analogues of classical back-door and front-door causal adjustment, and illustrate an application to a more complex causal model."],"url":"http://arxiv.org/abs/2403.09580v1","category":"cs.AI"}
{"created":"2024-03-14 17:11:48","title":"On the high-temperature expansion of the thermal energy on Einstein cylinders","abstract":"Some exact high temperature expansions are derived using a temperature inversion symmetry of the internal energy for conformal scalars and spinors on the Einstein Universe.","sentences":["Some exact high temperature expansions are derived using a temperature inversion symmetry of the internal energy for conformal scalars and spinors on the Einstein Universe."],"url":"http://arxiv.org/abs/2403.09576v1","category":"gr-qc"}
{"created":"2024-03-14 17:06:50","title":"Scalable Parity Architecture With a Shuttling-Based Spin Qubit Processor","abstract":"Motivated by the prospect of a two-dimensional square-lattice geometry for semiconductor spin qubits, we explore the realization of the Parity Architecture with quantum dots (QDs). This is part of the endeavor of developing architectures that advance the utilization of spin qubits for quantum computing while harnessing their advantages, such as their fast timescales -- especially of the nearest-neighbor interaction -- and small size. We present sequences of spin shuttling and quantum gates that implement the Parity Quantum Approximate Optimization Algorithm (QAOA) on a lattice constructed of identical unit cells, where the circuit depth is independent of the problem Hamiltonian and the system size. We further develop an error model, including a general description of the shuttling errors as a function of the probability distribution function of the valley splitting, and estimate the errors during one round of Parity QAOA, which is mainly limited by the valley splitting. Finally, we discuss the possibility of decoding the logical quantum state and of quantum error mitigation. We find that already with near-term spin qubit devices a sufficiently low physical error probability can be expected to reliably perform Parity QAOA with a short depth in a regime where the success probability compares favorably to standard QAOA.","sentences":["Motivated by the prospect of a two-dimensional square-lattice geometry for semiconductor spin qubits, we explore the realization of the Parity Architecture with quantum dots (QDs).","This is part of the endeavor of developing architectures that advance the utilization of spin qubits for quantum computing while harnessing their advantages, such as their fast timescales -- especially of the nearest-neighbor interaction -- and small size.","We present sequences of spin shuttling and quantum gates that implement the Parity Quantum Approximate Optimization Algorithm (QAOA) on a lattice constructed of identical unit cells, where the circuit depth is independent of the problem Hamiltonian and the system size.","We further develop an error model, including a general description of the shuttling errors as a function of the probability distribution function of the valley splitting, and estimate the errors during one round of Parity QAOA, which is mainly limited by the valley splitting.","Finally, we discuss the possibility of decoding the logical quantum state and of quantum error mitigation.","We find that already with near-term spin qubit devices a sufficiently low physical error probability can be expected to reliably perform Parity QAOA with a short depth in a regime where the success probability compares favorably to standard QAOA."],"url":"http://arxiv.org/abs/2403.09574v1","category":"quant-ph"}
{"created":"2024-03-14 17:03:04","title":"Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation","abstract":"Multimodal large language models (MLLMs) have shown impressive reasoning abilities, which, however, are also more vulnerable to jailbreak attacks than their LLM predecessors. Although still capable of detecting unsafe responses, we observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed due to the introduction of image features. To construct robust MLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free protecting approach that exploits the inherent safety awareness of MLLMs, and generates safer responses via adaptively transforming unsafe images into texts to activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs. Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO enhances model safety significantly (e.g., a 37.6% improvement on the MM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while consistently maintaining utility results on common MLLM benchmarks. Furthermore, we show that ECSO can be used as a data engine to generate supervised-finetuning (SFT) data for MLLM alignment without extra human intervention.","sentences":["Multimodal large language models (MLLMs) have shown impressive reasoning abilities, which, however, are also more vulnerable to jailbreak attacks than their LLM predecessors.","Although still capable of detecting unsafe responses, we observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed due to the introduction of image features.","To construct robust MLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free protecting approach that exploits the inherent safety awareness of MLLMs, and generates safer responses via adaptively transforming unsafe images into texts to activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs.","Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO enhances model safety significantly (e.g., a 37.6% improvement on the MM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while consistently maintaining utility results on common MLLM benchmarks.","Furthermore, we show that ECSO can be used as a data engine to generate supervised-finetuning (SFT) data for MLLM alignment without extra human intervention."],"url":"http://arxiv.org/abs/2403.09572v1","category":"cs.CV"}
{"created":"2024-03-14 17:00:00","title":"Non-Hermitian Persistent Current Transport","abstract":"Persistent currents circulate continuously without requiring external power sources. Here, we extend their theory to include dissipation within the framework of non-Hermitian quantum Hamiltonians. Using Green's function formalism, we introduce a non-Hermitian Fermi-Dirac distribution and derive an analytical expression for the persistent current that relies solely on the complex spectrum. We apply our formula to two dissipative models supporting persistent currents: ($i$) a phase-biased superconducting-normal-superconducting junction; ($ii$) a normal ring threaded by a magnetic flux. We show that the persistent currents in both systems exhibit no anomalies at any emergent exceptional points, whose signatures are only discernible in the current susceptibility. We validate our findings by exact diagonalization and extend them to account for finite temperatures and interaction effects. Our formalism offers a general framework for computing quantum many-body observables of non-Hermitian systems in equilibrium, with potential extensions to non-equilibrium scenarios.","sentences":["Persistent currents circulate continuously without requiring external power sources.","Here, we extend their theory to include dissipation within the framework of non-Hermitian quantum Hamiltonians.","Using Green's function formalism, we introduce a non-Hermitian Fermi-Dirac distribution and derive an analytical expression for the persistent current that relies solely on the complex spectrum.","We apply our formula to two dissipative models supporting persistent currents: ($i$) a phase-biased superconducting-normal-superconducting junction; ($ii$) a normal ring threaded by a magnetic flux.","We show that the persistent currents in both systems exhibit no anomalies at any emergent exceptional points, whose signatures are only discernible in the current susceptibility.","We validate our findings by exact diagonalization and extend them to account for finite temperatures and interaction effects.","Our formalism offers a general framework for computing quantum many-body observables of non-Hermitian systems in equilibrium, with potential extensions to non-equilibrium scenarios."],"url":"http://arxiv.org/abs/2403.09569v1","category":"quant-ph"}
{"created":"2024-03-14 16:57:18","title":"Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models","abstract":"The deployment of autonomous agents in environments involving human interaction has increasingly raised security concerns. Consequently, understanding the circumstances behind an event becomes critical, requiring the development of capabilities to justify their behaviors to non-expert users. Such explanations are essential in enhancing trustworthiness and safety, acting as a preventive measure against failures, errors, and misunderstandings. Additionally, they contribute to improving communication, bridging the gap between the agent and the user, thereby improving the effectiveness of their interactions. This work presents an accountability and explainability architecture implemented for ROS-based mobile robots. The proposed solution consists of two main components. Firstly, a black box-like element to provide accountability, featuring anti-tampering properties achieved through blockchain technology. Secondly, a component in charge of generating natural language explanations by harnessing the capabilities of Large Language Models (LLMs) over the data contained within the previously mentioned black box. The study evaluates the performance of our solution in three different scenarios, each involving autonomous agent navigation functionalities. This evaluation includes a thorough examination of accountability and explainability metrics, demonstrating the effectiveness of our approach in using accountable data from robot actions to obtain coherent, accurate and understandable explanations, even when facing challenges inherent in the use of autonomous agents in real-world scenarios.","sentences":["The deployment of autonomous agents in environments involving human interaction has increasingly raised security concerns.","Consequently, understanding the circumstances behind an event becomes critical, requiring the development of capabilities to justify their behaviors to non-expert users.","Such explanations are essential in enhancing trustworthiness and safety, acting as a preventive measure against failures, errors, and misunderstandings.","Additionally, they contribute to improving communication, bridging the gap between the agent and the user, thereby improving the effectiveness of their interactions.","This work presents an accountability and explainability architecture implemented for ROS-based mobile robots.","The proposed solution consists of two main components.","Firstly, a black box-like element to provide accountability, featuring anti-tampering properties achieved through blockchain technology.","Secondly, a component in charge of generating natural language explanations by harnessing the capabilities of Large Language Models (LLMs) over the data contained within the previously mentioned black box.","The study evaluates the performance of our solution in three different scenarios, each involving autonomous agent navigation functionalities.","This evaluation includes a thorough examination of accountability and explainability metrics, demonstrating the effectiveness of our approach in using accountable data from robot actions to obtain coherent, accurate and understandable explanations, even when facing challenges inherent in the use of autonomous agents in real-world scenarios."],"url":"http://arxiv.org/abs/2403.09567v1","category":"cs.RO"}
{"created":"2024-03-14 16:56:52","title":"Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models","abstract":"DevOps is a necessity in many industries, including the development of Autonomous Vehicles. In those settings, there are iterative activities that reduce the speed of SafetyOps cycles. One of these activities is \"Hazard Analysis & Risk Assessment\" (HARA), which is an essential step to start the safety requirements specification. As a potential approach to increase the speed of this step in SafetyOps, we have delved into the capabilities of Large Language Models (LLMs).   Our objective is to systematically assess their potential for application in the field of safety engineering. To that end, we propose a framework to support a higher degree of automation of HARA with LLMs. Despite our endeavors to automate as much of the process as possible, expert review remains crucial to ensure the validity and correctness of the analysis results, with necessary modifications made accordingly.","sentences":["DevOps is a necessity in many industries, including the development of Autonomous Vehicles.","In those settings, there are iterative activities that reduce the speed of SafetyOps cycles.","One of these activities is \"Hazard Analysis & Risk Assessment\" (HARA), which is an essential step to start the safety requirements specification.","As a potential approach to increase the speed of this step in SafetyOps, we have delved into the capabilities of Large Language Models (LLMs).   ","Our objective is to systematically assess their potential for application in the field of safety engineering.","To that end, we propose a framework to support a higher degree of automation of HARA with LLMs.","Despite our endeavors to automate as much of the process as possible, expert review remains crucial to ensure the validity and correctness of the analysis results, with necessary modifications made accordingly."],"url":"http://arxiv.org/abs/2403.09565v1","category":"cs.SE"}
{"created":"2024-03-14 16:54:17","title":"PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps","abstract":"The pre-training and fine-tuning paradigm has demonstrated its effectiveness and has become the standard approach for tailoring language models to various tasks. Currently, community-based platforms offer easy access to various pre-trained models, as anyone can publish without strict validation processes. However, a released pre-trained model can be a privacy trap for fine-tuning datasets if it is carefully designed. In this work, we propose PreCurious framework to reveal the new attack surface where the attacker releases the pre-trained model and gets a black-box access to the final fine-tuned model. PreCurious aims to escalate the general privacy risk of both membership inference and data extraction. The key intuition behind PreCurious is to manipulate the memorization stage of the pre-trained model and guide fine-tuning with a seemingly legitimate configuration. The effectiveness of defending against privacy attacks on a fine-tuned model seems promising, as empirical and theoretical evidence suggests that parameter-efficient and differentially private fine-tuning techniques are invulnerable to privacy attacks. But PreCurious demonstrates the possibility of breaking up invulnerability in a stealthy manner compared to fine-tuning on a benign model. By further leveraging a sanitized dataset, PreCurious can extract originally unexposed secrets under differentially private fine-tuning. Thus, PreCurious raises warnings for users who download pre-trained models from unknown sources, rely solely on tutorials or common-sense defenses, and previously release sanitized datasets even after perfect scrubbing.","sentences":["The pre-training and fine-tuning paradigm has demonstrated its effectiveness and has become the standard approach for tailoring language models to various tasks.","Currently, community-based platforms offer easy access to various pre-trained models, as anyone can publish without strict validation processes.","However, a released pre-trained model can be a privacy trap for fine-tuning datasets if it is carefully designed.","In this work, we propose PreCurious framework to reveal the new attack surface where the attacker releases the pre-trained model and gets a black-box access to the final fine-tuned model.","PreCurious aims to escalate the general privacy risk of both membership inference and data extraction.","The key intuition behind PreCurious is to manipulate the memorization stage of the pre-trained model and guide fine-tuning with a seemingly legitimate configuration.","The effectiveness of defending against privacy attacks on a fine-tuned model seems promising, as empirical and theoretical evidence suggests that parameter-efficient and differentially private fine-tuning techniques are invulnerable to privacy attacks.","But PreCurious demonstrates the possibility of breaking up invulnerability in a stealthy manner compared to fine-tuning on a benign model.","By further leveraging a sanitized dataset, PreCurious can extract originally unexposed secrets under differentially private fine-tuning.","Thus, PreCurious raises warnings for users who download pre-trained models from unknown sources, rely solely on tutorials or common-sense defenses, and previously release sanitized datasets even after perfect scrubbing."],"url":"http://arxiv.org/abs/2403.09562v1","category":"cs.CR"}
{"created":"2024-03-14 16:52:57","title":"Self-Consistency Training for Hamiltonian Prediction","abstract":"Hamiltonian prediction is a versatile formulation to leverage machine learning for solving molecular science problems. Yet, its applicability is limited by insufficient labeled data for training. In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose an exact training method that does not require labeled data. This merit addresses the data scarcity difficulty, and distinguishes the task from other property prediction formulations with unique benefits: (1) self-consistency training enables the model to be trained on a large amount of unlabeled data, hence substantially enhances generalization; (2) self-consistency training is more efficient than labeling data with DFT for supervised training, since it is an amortization of DFT calculation over a set of molecular structures. We empirically demonstrate the better generalization in data-scarce and out-of-distribution scenarios, and the better efficiency from the amortization. These benefits push forward the applicability of Hamiltonian prediction to an ever larger scale.","sentences":["Hamiltonian prediction is a versatile formulation to leverage machine learning for solving molecular science problems.","Yet, its applicability is limited by insufficient labeled data for training.","In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose an exact training method that does not require labeled data.","This merit addresses the data scarcity difficulty, and distinguishes the task from other property prediction formulations with unique benefits: (1) self-consistency training enables the model to be trained on a large amount of unlabeled data, hence substantially enhances generalization; (2) self-consistency training is more efficient than labeling data with DFT for supervised training, since it is an amortization of DFT calculation over a set of molecular structures.","We empirically demonstrate the better generalization in data-scarce and out-of-distribution scenarios, and the better efficiency from the amortization.","These benefits push forward the applicability of Hamiltonian prediction to an ever larger scale."],"url":"http://arxiv.org/abs/2403.09560v1","category":"cs.LG"}
{"created":"2024-03-14 16:42:47","title":"Testing MOND on small bodies in the remote solar system","abstract":"Modified Newtonian dynamics (MOND), which postulates a breakdown of Newton's laws of gravity/dynamics below some critical acceleration threshold, can explain many otherwise puzzling observational phenomena on galactic scales. MOND competes with the hypothesis of dark matter, which successfully explains the cosmic microwave background and large-scale structure. Here we provide the first solar-system test of MOND that probes the sub-critical acceleration regime. Using the Bekenstein-Milgrom AQUAL formulation, we simulate the evolution of myriads of test particles (planetesimals or comets) born in the trans-Neptunian region and scattered by the giant planets over the lifetime of the Sun to heliocentric distances of $10^2$-$10^5$ au. We include the effects of the Galactic tidal field and passing stars. While Newtonian simulations reproduce the distribution of binding energies of long-period and Oort-cloud comets detectable from Earth, MOND-based simulations do not. This conclusion is robust to plausible changes in the migration history of the planets, the migration history of the Sun, the MOND transition function, effects of the Sun's birth cluster, and the fading properties of long-period comets. For the most popular version of AQUAL, characterized by a gradual transition between the Newtonian and MOND regimes, our MOND-based simulations also fail to reproduce the orbital distribution of trans-Neptunian objects in the detached disk (perihelion > 38 au). Our results do not rule out some MOND theories more elaborate than AQUAL, in which non-Newtonian effects are screened on small spatial scales, at small masses, or in external gravitational fields comparable in strength to the critical acceleration.","sentences":["Modified Newtonian dynamics (MOND), which postulates a breakdown of Newton's laws of gravity/dynamics below some critical acceleration threshold, can explain many otherwise puzzling observational phenomena on galactic scales.","MOND competes with the hypothesis of dark matter, which successfully explains the cosmic microwave background and large-scale structure.","Here we provide the first solar-system test of MOND that probes the sub-critical acceleration regime.","Using the Bekenstein-Milgrom AQUAL formulation, we simulate the evolution of myriads of test particles (planetesimals or comets) born in the trans-Neptunian region and scattered by the giant planets over the lifetime of the Sun to heliocentric distances of $10^2$-$10^5$ au.","We include the effects of the Galactic tidal field and passing stars.","While Newtonian simulations reproduce the distribution of binding energies of long-period and Oort-cloud comets detectable from Earth, MOND-based simulations do not.","This conclusion is robust to plausible changes in the migration history of the planets, the migration history of the Sun, the MOND transition function, effects of the Sun's birth cluster, and the fading properties of long-period comets.","For the most popular version of AQUAL, characterized by a gradual transition between the Newtonian and MOND regimes, our MOND-based simulations also fail to reproduce the orbital distribution of trans-Neptunian objects in the detached disk (perihelion > 38 au).","Our results do not rule out some MOND theories more elaborate than AQUAL, in which non-Newtonian effects are screened on small spatial scales, at small masses, or in external gravitational fields comparable in strength to the critical acceleration."],"url":"http://arxiv.org/abs/2403.09555v1","category":"astro-ph.CO"}
{"created":"2024-03-14 16:41:26","title":"Cloud gap-filling with deep learning for improved grassland monitoring","abstract":"Uninterrupted optical image time series are crucial for the timely monitoring of agricultural land changes. However, the continuity of such time series is often disrupted by clouds. In response to this challenge, we propose a deep learning method that integrates cloud-free optical (Sentinel-2) observations and weather-independent (Sentinel-1) Synthetic Aperture Radar (SAR) data, using a combined Convolutional Neural Network (CNN)-Recurrent Neural Network (RNN) architecture to generate continuous Normalized Difference Vegetation Index (NDVI) time series. We emphasize the significance of observation continuity by assessing the impact of the generated time series on the detection of grassland mowing events. We focus on Lithuania, a country characterized by extensive cloud coverage, and compare our approach with alternative interpolation techniques (i.e., linear, Akima, quadratic). Our method surpasses these techniques, with an average MAE of 0.024 and R^2 of 0.92. It not only improves the accuracy of event detection tasks by employing a continuous time series, but also effectively filters out sudden shifts and noise originating from cloudy observations that cloud masks often fail to detect.","sentences":["Uninterrupted optical image time series are crucial for the timely monitoring of agricultural land changes.","However, the continuity of such time series is often disrupted by clouds.","In response to this challenge, we propose a deep learning method that integrates cloud-free optical (Sentinel-2) observations and weather-independent (Sentinel-1) Synthetic Aperture Radar (SAR) data, using a combined Convolutional Neural Network (CNN)-Recurrent Neural Network (RNN) architecture to generate continuous Normalized Difference Vegetation Index (NDVI) time series.","We emphasize the significance of observation continuity by assessing the impact of the generated time series on the detection of grassland mowing events.","We focus on Lithuania, a country characterized by extensive cloud coverage, and compare our approach with alternative interpolation techniques (i.e., linear, Akima, quadratic).","Our method surpasses these techniques, with an average MAE of 0.024 and R^2 of 0.92.","It not only improves the accuracy of event detection tasks by employing a continuous time series, but also effectively filters out sudden shifts and noise originating from cloudy observations that cloud masks often fail to detect."],"url":"http://arxiv.org/abs/2403.09554v1","category":"cs.CV"}
{"created":"2024-03-14 16:38:02","title":"Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields","abstract":"Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design. However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks. In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance. For training with DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise. Different from previous works on denoising, which are limited to equilibrium structures, the proposed method generalizes denoising to a much larger set of non-equilibrium structures. The main difference is that a non-equilibrium structure does not correspond to local energy minima and has non-zero forces, and therefore it can have many possible atomic positions compared to an equilibrium structure. This makes denoising non-equilibrium structures an ill-posed problem since the target of denoising is not uniquely defined. Our key insight is to additionally encode the forces of the original non-equilibrium structure to specify which non-equilibrium structure we are denoising. Concretely, given a corrupted non-equilibrium structure and the forces of the original one, we predict the non-equilibrium structure satisfying the input forces instead of any arbitrary structures. Since DeNS requires encoding forces, DeNS favors equivariant networks, which can easily incorporate forces and other higher-order tensors in node embeddings. We study the effectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17 datasets and demonstrate that DeNS can achieve new state-of-the-art results on OC20 and OC22 and significantly improve training efficiency on MD17.","sentences":["Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design.","However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks.","In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance.","For training with DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise.","Different from previous works on denoising, which are limited to equilibrium structures, the proposed method generalizes denoising to a much larger set of non-equilibrium structures.","The main difference is that a non-equilibrium structure does not correspond to local energy minima and has non-zero forces, and therefore it can have many possible atomic positions compared to an equilibrium structure.","This makes denoising non-equilibrium structures an ill-posed problem since the target of denoising is not uniquely defined.","Our key insight is to additionally encode the forces of the original non-equilibrium structure to specify which non-equilibrium structure we are denoising.","Concretely, given a corrupted non-equilibrium structure and the forces of the original one, we predict the non-equilibrium structure satisfying the input forces instead of any arbitrary structures.","Since DeNS requires encoding forces, DeNS favors equivariant networks, which can easily incorporate forces and other higher-order tensors in node embeddings.","We study the effectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17 datasets and demonstrate that DeNS can achieve new state-of-the-art results on OC20 and OC22 and significantly improve training efficiency on MD17."],"url":"http://arxiv.org/abs/2403.09549v1","category":"cs.LG"}
{"created":"2024-03-14 16:28:33","title":"RANDAO-based RNG: Last Revealer Attacks in Ethereum 2.0 Randomness and a Potential Solution","abstract":"Ethereum 2.0 is a major upgrade to improve its scalability, throughput, and security. In this version, RANDAO is the scheme to randomly select the users who propose, confirm blocks, and get rewards. However, a vulnerability, referred to as the `Last Revealer Attack' (LRA), compromises the randomness of this scheme by introducing bias to the Random Number Generator (RNG) process. This vulnerability is first clarified again in this study. After that, we propose a Shamir's Secret Sharing (SSS)-based RANDAO scheme to mitigate the LRA. Through our analysis, the proposed method can prevent the LRA under favorable network conditions.","sentences":["Ethereum 2.0 is a major upgrade to improve its scalability, throughput, and security.","In this version, RANDAO is the scheme to randomly select the users who propose, confirm blocks, and get rewards.","However, a vulnerability, referred to as the `Last Revealer Attack' (LRA), compromises the randomness of this scheme by introducing bias to the Random Number Generator (RNG) process.","This vulnerability is first clarified again in this study.","After that, we propose a Shamir's Secret Sharing (SSS)-based RANDAO scheme to mitigate the LRA.","Through our analysis, the proposed method can prevent the LRA under favorable network conditions."],"url":"http://arxiv.org/abs/2403.09541v1","category":"cs.CR"}
{"created":"2024-03-14 16:28:28","title":"A general and sharp regularity condition for integro-differential equations with non-dominated measures","abstract":"The aim of this work is to present the regularity condition (also known in the literature as structure condition) an integro-differential operator may satisfy in order for the domination principle to hold for (sub-,super-) solutions of polynomial growth. More precisely, the framework presented in Hollender [13], in which power functions are used in order to determine the integrability conditions, is weakened by substituting the power functions with Young functions. The use of Young functions allows for sharp integrability conditions, which are crucial when one deals with limit theorems. As an immediate application, it is considered the case of parabolic Hamilton-Jacobi-Bellman (HJB) operators, for which the regularity condition is satisfied and, consequently, the comparison principle as well. The parabolic HJB operator presented in this work can be associated to second-order (decoupled) forward-backward stochastic differential equations with jumps.","sentences":["The aim of this work is to present the regularity condition (also known in the literature as structure condition) an integro-differential operator may satisfy in order for the domination principle to hold for (sub-,super-) solutions of polynomial growth.","More precisely, the framework presented in Hollender","[13], in which power functions are used in order to determine the integrability conditions, is weakened by substituting the power functions with Young functions.","The use of Young functions allows for sharp integrability conditions, which are crucial when one deals with limit theorems.","As an immediate application, it is considered the case of parabolic Hamilton-Jacobi-Bellman (HJB) operators, for which the regularity condition is satisfied and, consequently, the comparison principle as well.","The parabolic HJB operator presented in this work can be associated to second-order (decoupled) forward-backward stochastic differential equations with jumps."],"url":"http://arxiv.org/abs/2403.09540v1","category":"math.AP"}
{"created":"2024-03-14 16:27:49","title":"Logits of API-Protected LLMs Leak Proprietary Information","abstract":"The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and even estimating the output layer parameters. Our empirical investigations show the effectiveness of our methods, which allow us to estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096. Lastly, we discuss ways that LLM providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability.","sentences":["The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models.","In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo).","Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space.","We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and even estimating the output layer parameters.","Our empirical investigations show the effectiveness of our methods, which allow us to estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.","Lastly, we discuss ways that LLM providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability."],"url":"http://arxiv.org/abs/2403.09539v1","category":"cs.CL"}
{"created":"2024-03-14 16:26:40","title":"Analyzing and Mitigating (with LLMs) the Security Misconfigurations of Helm Charts from Artifact Hub","abstract":"Background: Helm is a package manager that allows defining, installing, and upgrading applications with Kubernetes (K8s), a popular container orchestration platform. A Helm chart is a collection of files describing all dependencies, resources, and parameters required for deploying an application within a K8s cluster. Objective: The goal of this study is to mine and empirically evaluate the security of Helm charts, comparing the performance of existing tools in terms of misconfigurations reported by policies available by default, and measure to what extent LLMs could be used for removing misconfiguration. We also want to investigate whether there are false positives in both the LLM refactorings and the tool outputs. Method: We propose a pipeline to mine Helm charts from Artifact Hub, a popular centralized repository, and analyze them using state-of-the-art open-source tools, such as Checkov and KICS. First, such a pipeline will run several chart analyzers and identify the common and unique misconfigurations reported by each tool. Secondly, it will use LLMs to suggest mitigation for each misconfiguration. Finally, the chart refactoring previously generated will be analyzed again by the same tools to see whether it satisfies the tool's policies. At the same time, we will also perform a manual analysis on a subset of charts to evaluate whether there are false positive misconfigurations from the tool's reporting and in the LLM refactoring.","sentences":["Background:","Helm is a package manager that allows defining, installing, and upgrading applications with Kubernetes (K8s), a popular container orchestration platform.","A Helm chart is a collection of files describing all dependencies, resources, and parameters required for deploying an application within a K8s cluster.","Objective: The goal of this study is to mine and empirically evaluate the security of Helm charts, comparing the performance of existing tools in terms of misconfigurations reported by policies available by default, and measure to what extent LLMs could be used for removing misconfiguration.","We also want to investigate whether there are false positives in both the LLM refactorings and the tool outputs.","Method: We propose a pipeline to mine Helm charts from Artifact Hub, a popular centralized repository, and analyze them using state-of-the-art open-source tools, such as Checkov and KICS.","First, such a pipeline will run several chart analyzers and identify the common and unique misconfigurations reported by each tool.","Secondly, it will use LLMs to suggest mitigation for each misconfiguration.","Finally, the chart refactoring previously generated will be analyzed again by the same tools to see whether it satisfies the tool's policies.","At the same time, we will also perform a manual analysis on a subset of charts to evaluate whether there are false positive misconfigurations from the tool's reporting and in the LLM refactoring."],"url":"http://arxiv.org/abs/2403.09537v1","category":"cs.SE"}
{"created":"2024-03-14 16:23:48","title":"Mixed Algorithm of SINDy and HAVOK for Measure-Based Analysis of Power System with Inverter-based Resources","abstract":"Artificial intelligence and machine learning is enhancing electric grids by offering data analysis tools that can be used to operate the power grid more reliably. However, the complex nonlinear dynamics, particularly when coupled with multi-scale interactions among Inverter-based renewable energy Resources, calls for effective algorithms for power system application. This paper presents affective novel algorithm to detect various nonlinear dynamics, which is built upon: the Sparse Identification of Nonlinear Dynamics method for nonlinear dynamics detection; and Hankel Alternative View of Koopman method for multi-scale decomposition. We show that, by an appropriate integration of the strengths of the two, the mixed algorithm not only can detect the nonlinearity, but also it distinguishes the nonlinearity caused by coupled Inverter-based resources from the more familiar ones caused synchronous generators. This shows that the proposal algorithm can be a promising application of artificial intelligence and machine learning for data measure-based analysis to support operation of power system with integrated renewables.","sentences":["Artificial intelligence and machine learning is enhancing electric grids by offering data analysis tools that can be used to operate the power grid more reliably.","However, the complex nonlinear dynamics, particularly when coupled with multi-scale interactions among Inverter-based renewable energy Resources, calls for effective algorithms for power system application.","This paper presents affective novel algorithm to detect various nonlinear dynamics, which is built upon: the Sparse Identification of Nonlinear Dynamics method for nonlinear dynamics detection; and Hankel Alternative View of Koopman method for multi-scale decomposition.","We show that, by an appropriate integration of the strengths of the two, the mixed algorithm not only can detect the nonlinearity, but also it distinguishes the nonlinearity caused by coupled Inverter-based resources from the more familiar ones caused synchronous generators.","This shows that the proposal algorithm can be a promising application of artificial intelligence and machine learning for data measure-based analysis to support operation of power system with integrated renewables."],"url":"http://arxiv.org/abs/2403.09536v1","category":"eess.SY"}
{"created":"2024-03-14 16:22:08","title":"Study of measure-valued Markov processes. Explicit bounds for the convergence in distribution of mean-field models","abstract":"The aim of the paper is to prove that the rates of convergence in distribution for $N$-particle mean-field models are the expected one: $N^{-1}$ in Law of Large Numbers regime, and $N^{-1/2}$ in Central Limit Theorem regime. These proofs require to study empirical measures of McKean-Vlasov particle systems, and conditional laws of McKean-Vlasov processes, as measure-valued Markov processes. In particular, the expressions of the infinitesimal generators of such processes are established for measure-valued processes with general jumps. The generators being differential operators, all the proofs rely on the analytical properties of measure-variable functions, and the differentiation of such functions.","sentences":["The aim of the paper is to prove that the rates of convergence in distribution for $N$-particle mean-field models are the expected one: $N^{-1}$ in Law of Large Numbers regime, and $N^{-1/2}$ in Central Limit Theorem regime.","These proofs require to study empirical measures of McKean-Vlasov particle systems, and conditional laws of McKean-Vlasov processes, as measure-valued Markov processes.","In particular, the expressions of the infinitesimal generators of such processes are established for measure-valued processes with general jumps.","The generators being differential operators, all the proofs rely on the analytical properties of measure-variable functions, and the differentiation of such functions."],"url":"http://arxiv.org/abs/2403.09534v1","category":"math.PR"}
{"created":"2024-03-14 16:13:00","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding","abstract":"The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images. Computer vision models involving the multimodal abilities in the previous days are focused on image detection, classification based on well-defined objects. Large language models (LLMs) introduces the transformation from nature language to visual objects, which present the visual layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs, while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations. However, the mismatching between the algorithms with the problem could lead to undesired results. In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development of vision-oriented AI. VisionGPT-3D provides a versatile multimodal framework building upon the strengths of multimodal foundation models. It seamlessly integrates various SOTA vision models and brings the automation in the selection of SOTA vision models, identifies the suitable 3D mesh creation algorithms corresponding to 2D depth maps analysis, generates optimal results based on diverse multimodal inputs such as text prompts.   Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent","sentences":["The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images.","Computer vision models involving the multimodal abilities in the previous days are focused on image detection, classification based on well-defined objects.","Large language models (LLMs) introduces the transformation from nature language to visual objects, which present the visual layout for text contexts.","OpenAI GPT-4 has emerged as the pinnacle in LLMs, while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations.","However, the mismatching between the algorithms with the problem could lead to undesired results.","In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development of vision-oriented AI.","VisionGPT-3D provides a versatile multimodal framework building upon the strengths of multimodal foundation models.","It seamlessly integrates various SOTA vision models and brings the automation in the selection of SOTA vision models, identifies the suitable 3D mesh creation algorithms corresponding to 2D depth maps analysis, generates optimal results based on diverse multimodal inputs such as text prompts.   ","Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent"],"url":"http://arxiv.org/abs/2403.09530v1","category":"cs.CV"}
{"created":"2024-03-14 16:11:14","title":"A general-purpose neural network potential for Ti-Al-Nb alloys towards large-scale molecular dynamics with ab initio accuracy","abstract":"High Nb-containing TiAl alloys exhibit exceptional high-temperature strength and room-temperature ductility, making them widely used in hot-section components of automotive and aerospace engines. However, the lack of accurate interatomic interaction potentials for large-scale modeling severely hampers a comprehensive understanding of the failure mechanism of Ti-Al-Nb alloys and the development of strategies to enhance the mechanical properties. Here, we develop a general-purpose machine-learned potential (MLP) for the Ti-Al-Nb ternary system by combining the neural evolution potentials framework with an active learning scheme. The developed MLP, trained on extensive first-principles datasets, demonstrates remarkable accuracy in predicting various lattice and defect properties, as well as high-temperature characteristics such as thermal expansion and melting point for TiAl systems. Notably, this potential can effectively describe the key effect of Nb doping on stacking fault energies and formation energies. Of practical importance is that our MLP enables large-scale molecular dynamics simulations involving tens of millions of atoms with ab initio accuracy, achieving an outstanding balance between computational speed and accuracy. These results pave the way for studying micro-mechanical behaviors in TiAl lamellar structures and developing high-performance TiAl alloys towards applications at elevated temperatures.","sentences":["High Nb-containing TiAl alloys exhibit exceptional high-temperature strength and room-temperature ductility, making them widely used in hot-section components of automotive and aerospace engines.","However, the lack of accurate interatomic interaction potentials for large-scale modeling severely hampers a comprehensive understanding of the failure mechanism of Ti-Al-Nb alloys and the development of strategies to enhance the mechanical properties.","Here, we develop a general-purpose machine-learned potential (MLP) for the Ti-Al-Nb ternary system by combining the neural evolution potentials framework with an active learning scheme.","The developed MLP, trained on extensive first-principles datasets, demonstrates remarkable accuracy in predicting various lattice and defect properties, as well as high-temperature characteristics such as thermal expansion and melting point for TiAl systems.","Notably, this potential can effectively describe the key effect of Nb doping on stacking fault energies and formation energies.","Of practical importance is that our MLP enables large-scale molecular dynamics simulations involving tens of millions of atoms with ab initio accuracy, achieving an outstanding balance between computational speed and accuracy.","These results pave the way for studying micro-mechanical behaviors in TiAl lamellar structures and developing high-performance TiAl alloys towards applications at elevated temperatures."],"url":"http://arxiv.org/abs/2403.09529v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-14 16:10:34","title":"WavCraft: Audio Editing and Generation with Natural Language Prompts","abstract":"We introduce WavCraft, a collective system that leverages large language models (LLMs) to connect diverse task-specific models for audio content creation and editing. Specifically, WavCraft describes the content of raw sound materials in natural language and prompts the LLM conditioned on audio descriptions and users' requests. WavCraft leverages the in-context learning ability of the LLM to decomposes users' instructions into several tasks and tackle each task collaboratively with audio expert modules. Through task decomposition along with a set of task-specific models, WavCraft follows the input instruction to create or edit audio content with more details and rationales, facilitating users' control. In addition, WavCraft is able to cooperate with users via dialogue interaction and even produce the audio content without explicit user commands. Experiments demonstrate that WavCraft yields a better performance than existing methods, especially when adjusting the local regions of audio clips. Moreover, WavCraft can follow complex instructions to edit and even create audio content on the top of input recordings, facilitating audio producers in a broader range of applications. Our implementation and demos are available at https://github.com/JinhuaLiang/WavCraft.","sentences":["We introduce WavCraft, a collective system that leverages large language models (LLMs) to connect diverse task-specific models for audio content creation and editing.","Specifically, WavCraft describes the content of raw sound materials in natural language and prompts the LLM conditioned on audio descriptions and users' requests.","WavCraft leverages the in-context learning ability of the LLM to decomposes users' instructions into several tasks and tackle each task collaboratively with audio expert modules.","Through task decomposition along with a set of task-specific models, WavCraft follows the input instruction to create or edit audio content with more details and rationales, facilitating users' control.","In addition, WavCraft is able to cooperate with users via dialogue interaction and even produce the audio content without explicit user commands.","Experiments demonstrate that WavCraft yields a better performance than existing methods, especially when adjusting the local regions of audio clips.","Moreover, WavCraft can follow complex instructions to edit and even create audio content on the top of input recordings, facilitating audio producers in a broader range of applications.","Our implementation and demos are available at https://github.com/JinhuaLiang/WavCraft."],"url":"http://arxiv.org/abs/2403.09527v1","category":"eess.AS"}
{"created":"2024-03-14 16:10:23","title":"Optimizing the Electrical Interface for Large-Scale Color-Center Quantum Processors","abstract":"Quantum processors based on color centers in diamond are promising candidates for future large-scale quantum computers thanks to their flexible optical interface, (relatively) high operating temperature, and high-fidelity operation. Similar to other quantum-computing platforms, the electrical interface required to control and read out such qubits may limit both the performance of the whole system and its scalability. To address this challenge, this work analyzes the requirements of the electrical interface and investigates how to efficiently implement the electronic controller in a scalable architecture comprising a large number of identical unit cells. Among the different discussed functionalities, a specific focus is devoted to the generation of the static and dynamic magnetic fields driving the electron and nuclear spins, because of their major impact on fidelity and scalability. Following the derived requirements, different system architectures, such as a qubit frequency-multiplexing scheme, are considered to identify the most power efficient approach, especially in the presence of inhomogeneity of the qubit Larmor frequency across the processor. As a result, a non-frequency-multiplexed, 1-mm$^2$ unit-cell architecture is proposed as the optimal solution, able to address up to one electron-spin qubit and 9 nuclear-spin qubits within a 3-mW average power consumption, thus establishing the baseline for the scalable electrical interface for future large-scale color-center quantum computers.","sentences":["Quantum processors based on color centers in diamond are promising candidates for future large-scale quantum computers thanks to their flexible optical interface, (relatively) high operating temperature, and high-fidelity operation.","Similar to other quantum-computing platforms, the electrical interface required to control and read out such qubits may limit both the performance of the whole system and its scalability.","To address this challenge, this work analyzes the requirements of the electrical interface and investigates how to efficiently implement the electronic controller in a scalable architecture comprising a large number of identical unit cells.","Among the different discussed functionalities, a specific focus is devoted to the generation of the static and dynamic magnetic fields driving the electron and nuclear spins, because of their major impact on fidelity and scalability.","Following the derived requirements, different system architectures, such as a qubit frequency-multiplexing scheme, are considered to identify the most power efficient approach, especially in the presence of inhomogeneity of the qubit Larmor frequency across the processor.","As a result, a non-frequency-multiplexed, 1-mm$^2$ unit-cell architecture is proposed as the optimal solution, able to address up to one electron-spin qubit and 9 nuclear-spin qubits within a 3-mW average power consumption, thus establishing the baseline for the scalable electrical interface for future large-scale color-center quantum computers."],"url":"http://arxiv.org/abs/2403.09526v1","category":"quant-ph"}
{"created":"2024-03-14 16:07:39","title":"MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation","abstract":"Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student. Experiment results on translating both specific language phenomena and general MT benchmarks demonstrate that finetuning the student MT model on about 10% examples can achieve comparable results to the traditional knowledge distillation method, and synthesized potential errors and diverse contexts further improve translation performances on unseen contexts and words.","sentences":["Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency.","Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction.","However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge.","In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner.","Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher.","Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student.","Experiment results on translating both specific language phenomena and general MT benchmarks demonstrate that finetuning the student MT model on about 10% examples can achieve comparable results to the traditional knowledge distillation method, and synthesized potential errors and diverse contexts further improve translation performances on unseen contexts and words."],"url":"http://arxiv.org/abs/2403.09522v1","category":"cs.CL"}
{"created":"2024-03-14 15:58:25","title":"Free groups are $L^2$-subgroup rigid","abstract":"In this paper, we introduce the notion of $L^2$-subgroup rigid groups and demonstrate that free groups are $L^2$-subgroup rigid. As a consequence, we establish the equivalence between compressibility, inertness, strong inertness, and $L^2$-independence for a finitely generated subgroup of a free group, confirming a conjecture by Dicks and Ventura as well as the one by Antolin and Jaikin-Zapirain.","sentences":["In this paper, we introduce the notion of $L^2$-subgroup rigid groups and demonstrate that free groups are $L^2$-subgroup rigid.","As a consequence, we establish the equivalence between compressibility, inertness, strong inertness, and $L^2$-independence for a finitely generated subgroup of a free group, confirming a conjecture by Dicks and Ventura as well as the one by Antolin and Jaikin-Zapirain."],"url":"http://arxiv.org/abs/2403.09515v1","category":"math.GR"}
{"created":"2024-03-14 15:57:13","title":"AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting","abstract":"With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., \"harmful text\") has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \\textbf{Ada}ptive \\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector). Initially, we present a manually designed static defense prompt, which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries. Furthermore, we introduce an adaptive auto-refinement framework, consisting of a target MLLM and a LLM-based defense prompt generator (Defender). These components collaboratively and iteratively communicate to generate a defense prompt. Extensive experiments on the popular structure-based jailbreak attacks and benign datasets show that our methods can consistently improve MLLMs' robustness against structure-based jailbreak attacks without compromising the model's general capabilities evaluated on standard benign tasks. Our code is available at https://github.com/rain305f/AdaShield.","sentences":["With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced.","However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., \"harmful text\") has been injected into the images to mislead MLLMs.","In this work, we aim to defend against such threats.","Specifically, we propose \\textbf{Ada}ptive \\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector).","Initially, we present a manually designed static defense prompt, which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries.","Furthermore, we introduce an adaptive auto-refinement framework, consisting of a target MLLM and a LLM-based defense prompt generator (Defender).","These components collaboratively and iteratively communicate to generate a defense prompt.","Extensive experiments on the popular structure-based jailbreak attacks and benign datasets show that our methods can consistently improve MLLMs' robustness against structure-based jailbreak attacks without compromising the model's general capabilities evaluated on standard benign tasks.","Our code is available at https://github.com/rain305f/AdaShield."],"url":"http://arxiv.org/abs/2403.09513v1","category":"cs.CR"}
{"created":"2024-03-14 15:56:39","title":"Trust AI Regulation? Discerning users are vital to build trust and effective AI regulation","abstract":"There is general agreement that some form of regulation is necessary both for AI creators to be incentivised to develop trustworthy systems, and for users to actually trust those systems. But there is much debate about what form these regulations should take and how they should be implemented. Most work in this area has been qualitative, and has not been able to make formal predictions. Here, we propose that evolutionary game theory can be used to quantitatively model the dilemmas faced by users, AI creators, and regulators, and provide insights into the possible effects of different regulatory regimes. We show that creating trustworthy AI and user trust requires regulators to be incentivised to regulate effectively. We demonstrate the effectiveness of two mechanisms that can achieve this. The first is where governments can recognise and reward regulators that do a good job. In that case, if the AI system is not too risky for users then some level of trustworthy development and user trust evolves. We then consider an alternative solution, where users can condition their trust decision on the effectiveness of the regulators. This leads to effective regulation, and consequently the development of trustworthy AI and user trust, provided that the cost of implementing regulations is not too high. Our findings highlight the importance of considering the effect of different regulatory regimes from an evolutionary game theoretic perspective.","sentences":["There is general agreement that some form of regulation is necessary both for AI creators to be incentivised to develop trustworthy systems, and for users to actually trust those systems.","But there is much debate about what form these regulations should take and how they should be implemented.","Most work in this area has been qualitative, and has not been able to make formal predictions.","Here, we propose that evolutionary game theory can be used to quantitatively model the dilemmas faced by users, AI creators, and regulators, and provide insights into the possible effects of different regulatory regimes.","We show that creating trustworthy AI and user trust requires regulators to be incentivised to regulate effectively.","We demonstrate the effectiveness of two mechanisms that can achieve this.","The first is where governments can recognise and reward regulators that do a good job.","In that case, if the AI system is not too risky for users then some level of trustworthy development and user trust evolves.","We then consider an alternative solution, where users can condition their trust decision on the effectiveness of the regulators.","This leads to effective regulation, and consequently the development of trustworthy AI and user trust, provided that the cost of implementing regulations is not too high.","Our findings highlight the importance of considering the effect of different regulatory regimes from an evolutionary game theoretic perspective."],"url":"http://arxiv.org/abs/2403.09510v1","category":"cs.AI"}
{"created":"2024-03-14 15:53:04","title":"Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition","abstract":"Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. In this study, we investigate the effect of hue variance in the context of video recognition and find this variance to be beneficial since static appearances are less important in videos that contain motion information. Based on this observation, we propose a data augmentation method for video recognition, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances. Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the distribution shift caused by SwapMix, enforcing the model to learn appearance invariant representations. Comprehensive empirical evaluation across various architectures and different datasets solidly validates the effectiveness and generalization ability of MCA, and the application of VA in other augmentation methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.","sentences":["Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice.","In this study, we investigate the effect of hue variance in the context of video recognition and find this variance to be beneficial since static appearances are less important in videos that contain motion information.","Based on this observation, we propose a data augmentation method for video recognition, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances.","Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the distribution shift caused by SwapMix, enforcing the model to learn appearance invariant representations.","Comprehensive empirical evaluation across various architectures and different datasets solidly validates the effectiveness and generalization ability of MCA, and the application of VA in other augmentation methods.","Code is available at https://github.com/BeSpontaneous/MCA-pytorch."],"url":"http://arxiv.org/abs/2403.09506v1","category":"cs.CV"}
{"created":"2024-03-14 15:44:19","title":"EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning","abstract":"Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations. However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning. Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. EquiAV outperforms previous works across various audio-visual benchmarks.","sentences":["Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations.","However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs.","To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning.","Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor.","It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision.","Notably, this is achieved with minimal computational overhead.","Extensive ablation studies and qualitative results verify the effectiveness of our method.","EquiAV outperforms previous works across various audio-visual benchmarks."],"url":"http://arxiv.org/abs/2403.09502v1","category":"cs.LG"}
{"created":"2024-03-14 15:42:26","title":"A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning","abstract":"Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture. Integrating renewable energy generation into dairy farming could help address this challenge. Effective battery management is important for integrating renewable energy generation. Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices. Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain. This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources. This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in a dairy farm setting. This research also explores the effect of the proposed algorithm by adding wind generation data and considering additional case studies. The proposed algorithm reduces the cost of imported electricity from the grid by 13.41\\%, peak demand by 2\\%, and 24.49\\% when utilizing wind generation. These results underline how reinforcement learning is highly effective in managing batteries in the dairy farming sector.","sentences":["Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture.","Integrating renewable energy generation into dairy farming could help address this challenge.","Effective battery management is important for integrating renewable energy generation.","Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices.","Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain.","This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources.","This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in a dairy farm setting.","This research also explores the effect of the proposed algorithm by adding wind generation data and considering additional case studies.","The proposed algorithm reduces the cost of imported electricity from the grid by 13.41\\%, peak demand by 2\\%, and 24.49\\% when utilizing wind generation.","These results underline how reinforcement learning is highly effective in managing batteries in the dairy farming sector."],"url":"http://arxiv.org/abs/2403.09499v1","category":"cs.LG"}
{"created":"2024-03-14 15:40:13","title":"From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News","abstract":"In the digital era, the rapid propagation of fake news and rumors via social networks brings notable societal challenges and impacts public opinion regulation. Traditional fake news modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift. However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text. The advent of large language models (LLMs) provides the possibility of modeling subtle dynamics of opinion. Consequently, in this work, we introduce a Fake news Propagation Simulation framework (FPS) based on LLM, which studies the trends and control of fake news propagation in detail. Specifically, each agent in the simulation represents an individual with a distinct personality. They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking. Every day, they engage in random opinion exchanges, reflect on their thinking, and update their opinions. Our simulation results uncover patterns in fake news propagation related to topic relevance, and individual traits, aligning with real-world observations. Additionally, we evaluate various intervention strategies and demonstrate that early and appropriately frequent interventions strike a balance between governance cost and effectiveness, offering valuable insights for practical applications. Our study underscores the significant utility and potential of LLMs in combating fake news.","sentences":["In the digital era, the rapid propagation of fake news and rumors via social networks brings notable societal challenges and impacts public opinion regulation.","Traditional fake news modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift.","However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text.","The advent of large language models (LLMs) provides the possibility of modeling subtle dynamics of opinion.","Consequently, in this work, we introduce a Fake news Propagation Simulation framework (FPS) based on LLM, which studies the trends and control of fake news propagation in detail.","Specifically, each agent in the simulation represents an individual with a distinct personality.","They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking.","Every day, they engage in random opinion exchanges, reflect on their thinking, and update their opinions.","Our simulation results uncover patterns in fake news propagation related to topic relevance, and individual traits, aligning with real-world observations.","Additionally, we evaluate various intervention strategies and demonstrate that early and appropriately frequent interventions strike a balance between governance cost and effectiveness, offering valuable insights for practical applications.","Our study underscores the significant utility and potential of LLMs in combating fake news."],"url":"http://arxiv.org/abs/2403.09498v1","category":"cs.SI"}
{"created":"2024-03-14 15:39:32","title":"On the Gotzmann threshold of monomials","abstract":"Let $R_n=K[x_1,\\dots,x_n]$ be the $n$-variable polynomial ring over a field $K$. Let $S_n$ denote the set of monomials in $R_n$. A monomial $u \\in S_n$ is a \\textit{Gotzmann monomial} if the Borel-stable monomial ideal $\\langle u \\rangle$ it generates in $R_n$ is a Gotzmann ideal. A longstanding open problem is to determine all Gotzmann monomials in $R_n$. Given $u_0 \\in S_{n-1}$, its \\textit{Gotzmann threshold} is the unique nonnegative integer $t_0=\\tau_n(u_0)$ such that $u_0x_n^t$ is a Gotzmann monomial in $R_n$ if and only if $t \\ge t_0$. Currently, the function $\\tau_n$ is exactly known for $n \\le 4$ only. We present here an efficient procedure to determine $\\tau_n(u_0)$ for all $n$ and all $u_0 \\in S_{n-1}$. As an application, in the critical case $u_0=x_2^d$, we determine $\\tau_5(x_2^d)$ for all $d$ and we conjecture that for $n \\ge 6$, $\\tau_n(x_2^d)$ is a polynomial in $d$ of degree $2^{n-2}$ and dominant term equal to that of the $(n-2)$-iterated binomial coefficient $$ \\binom {\\binom {\\binom d2}2}{\\stackrel{\\cdots}2}. $$","sentences":["Let $R_n=K[x_1,\\dots,x_n]$ be the $n$-variable polynomial ring over a field $K$. Let $S_n$ denote the set of monomials in $R_n$. A monomial $u \\in S_n$ is a \\textit{Gotzmann monomial} if the Borel-stable monomial ideal $\\langle u \\rangle$ it generates in $R_n$ is a Gotzmann ideal.","A longstanding open problem is to determine all Gotzmann monomials in $R_n$. Given $u_0 \\in S_{n-1}$, its \\textit{Gotzmann threshold} is the unique nonnegative integer $t_0=\\tau_n(u_0)$ such that $u_0x_n^t$ is a Gotzmann monomial in $R_n$ if and only if $t \\ge t_0$.","Currently, the function $\\tau_n$ is exactly known for $n \\le 4$ only.","We present here an efficient procedure to determine $\\tau_n(u_0)$ for all $n$ and all $u_0 \\in S_{n-1}$. As an application, in the critical case $u_0=x_2^d$, we determine $\\tau_5(x_2^d)$ for all $d$ and we conjecture that for $n \\ge 6$, $\\tau_n(x_2^d)$ is a polynomial in $d$ of degree $2^{n-2}$ and dominant term equal to that of the $(n-2)$-iterated binomial coefficient $$ \\binom {\\binom {\\binom d2}2}{\\stackrel{\\cdots}2}.","$$"],"url":"http://arxiv.org/abs/2403.09497v1","category":"math.AC"}
{"created":"2024-03-14 15:39:24","title":"The Development of Investment Planning Models for the United Kingdoms Wind and Solar Fleets","abstract":"Previous work has resulted in the development of an energy model able to calculate wind and solar fleet efficiencies. However, for investment planning purposes, it is necessary to calculate from the lowest economically acceptable efficiencies how much wind and solar generation would be economically justified. The paper explains how this objective has been achieved with arrays (investment planning tables) created after carrying out a structured investigation of the behaviour of the electricity system over the whole of its operational range. The tables are then applied to National Grid prediction of the size and composition of the system in the year 2035. A conclusion is reached that wind and solar generation will only be able to supply about 70% of electrical demand, the other 30% being provided by dispatchable sources of generation, which must be sufficiently fast acting to maintain electricity system stability, such as the use of combined cycle gas turbines. This limit on deployment of wind and solar generation restricts their ability to decarbonise the electricity system and is likely to lead in 2035 to a residual of 72 million tonnes per annum of carbon dioxide emissions which wind and solar generations will be unable to address","sentences":["Previous work has resulted in the development of an energy model able to calculate wind and solar fleet efficiencies.","However, for investment planning purposes, it is necessary to calculate from the lowest economically acceptable efficiencies how much wind and solar generation would be economically justified.","The paper explains how this objective has been achieved with arrays (investment planning tables) created after carrying out a structured investigation of the behaviour of the electricity system over the whole of its operational range.","The tables are then applied to National Grid prediction of the size and composition of the system in the year 2035.","A conclusion is reached that wind and solar generation will only be able to supply about 70% of electrical demand, the other 30% being provided by dispatchable sources of generation, which must be sufficiently fast acting to maintain electricity system stability, such as the use of combined cycle gas turbines.","This limit on deployment of wind and solar generation restricts their ability to decarbonise the electricity system and is likely to lead in 2035 to a residual of 72 million tonnes per annum of carbon dioxide emissions which wind and solar generations will be unable to address"],"url":"http://arxiv.org/abs/2403.09496v1","category":"eess.SY"}
{"created":"2024-03-14 15:30:19","title":"Gravitational waves from glitch-induced f-mode oscillations in quark and neutron stars","abstract":"Matter in compact stars is dense enough that transient events within the star could have sufficiently high energies to produce detectable gravitational waves (GWs). These GWs could be used to constrain the equation of state (EoS) for matter in the star and could reveal that there is more than one type of EoS at play in the population, implying that multiple types of compact stars exist. One of these types could be quark stars, composed almost entirely of stable quark matter, and observing GWs is a way to test for the strange matter EoS. Here we explore the possibility that, if fundamental (f-) mode oscillations in pulsars are induced by a pulsar glitch, then these oscillations might produce detectable GWs. We use the existing population of pulsars and their glitches, as well as a much larger synthesized population, along with 15 EoSs (8 for neutron stars and 7 for quark stars) to generate frequencies, damping times, and GW strengths for each. We find that of the EoSs examined, all quark star EoSs produce narrower distributions of f-mode frequency than neutron star EoSs. This result, along with other elements of the data, could be used to differentiate between GWs (or other signals from f-modes) originating from neutron stars and quark stars and thus could confirm the existence of quark stars. We also find that GW astronomy is a potentially viable method for detecting a larger population of pulsars which are not observable electromagnetically and that future GW observatories have the possibility to greatly expand this capability.","sentences":["Matter in compact stars is dense enough that transient events within the star could have sufficiently high energies to produce detectable gravitational waves (GWs).","These GWs could be used to constrain the equation of state (EoS) for matter in the star and could reveal that there is more than one type of EoS at play in the population, implying that multiple types of compact stars exist.","One of these types could be quark stars, composed almost entirely of stable quark matter, and observing GWs is a way to test for the strange matter","EoS.","Here we explore the possibility that, if fundamental (f-) mode oscillations in pulsars are induced by a pulsar glitch, then these oscillations might produce detectable GWs.","We use the existing population of pulsars and their glitches, as well as a much larger synthesized population, along with 15 EoSs (8 for neutron stars and 7 for quark stars) to generate frequencies, damping times, and GW strengths for each.","We find that of the EoSs examined, all quark star EoSs produce narrower distributions of f-mode frequency than neutron star EoSs.","This result, along with other elements of the data, could be used to differentiate between GWs (or other signals from f-modes) originating from neutron stars and quark stars and thus could confirm the existence of quark stars.","We also find that GW astronomy is a potentially viable method for detecting a larger population of pulsars which are not observable electromagnetically and that future GW observatories have the possibility to greatly expand this capability."],"url":"http://arxiv.org/abs/2403.09489v1","category":"gr-qc"}
{"created":"2024-03-14 15:30:14","title":"Rectifying Demonstration Shortcut in In-Context Learning","abstract":"Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities. However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction. In this work, we term this phenomenon as the `Demonstration Shortcut'. While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations. To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method. We evaluate the effectiveness of the proposed method in two settings: (1) the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens. In both settings, In-Context Calibration demonstrates substantial improvements, with results generalized across three LLM families (OPT, GPT, and Llama2) under various configurations.","sentences":["Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities.","However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction.","In this work, we term this phenomenon as the `Demonstration Shortcut'.","While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations.","To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method.","We evaluate the effectiveness of the proposed method in two settings: (1)","the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens.","In both settings, In-Context Calibration demonstrates substantial improvements, with results generalized across three LLM families (OPT, GPT, and Llama2) under various configurations."],"url":"http://arxiv.org/abs/2403.09488v1","category":"cs.CL"}
{"created":"2024-03-14 15:29:21","title":"Tamed loops: A try for non-renormalizable Einstein gravity in UV-free scheme","abstract":"How to describe loop corrections is a fundamental challenge in the quantization of Einstein gravity. In this paper, we give it a try in UV-free scheme, and the result seems to be effective for graviton loops. This indicates that both loops of the renormalizable Standard Model and the non-renormalizable Einstein gravity can be described by the method of UV-free scheme.","sentences":["How to describe loop corrections is a fundamental challenge in the quantization of Einstein gravity.","In this paper, we give it a try in UV-free scheme, and the result seems to be effective for graviton loops.","This indicates that both loops of the renormalizable Standard Model and the non-renormalizable Einstein gravity can be described by the method of UV-free scheme."],"url":"http://arxiv.org/abs/2403.09487v1","category":"hep-ph"}
{"created":"2024-03-14 15:29:09","title":"SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams","abstract":"Reconstructing a sequence of sharp images from the blurry input is crucial for enhancing our insights into the captured scene and poses a significant challenge due to the limited temporal features embedded in the image. Spike cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing motion features and beneficial for solving this ill-posed problem. Nonetheless, existing methods fall into the supervised learning paradigm, which suffers from notable performance degradation when applied to real-world scenarios that diverge from the synthetic training data domain. Moreover, the quality of reconstructed images is capped by the generated images based on motion analysis interpolation, which inherently differs from the actual scene, affecting the generalization ability of these methods in real high-speed scenarios. To address these challenges, we propose the first self-supervised framework for the task of spike-guided motion deblurring. Our approach begins with the formulation of a spike-guided deblurring model that explores the theoretical relationships among spike streams, blurry images, and their corresponding sharp sequences. We subsequently develop a self-supervised cascaded framework to alleviate the issues of spike noise and spatial-resolution mismatching encountered in the deblurring model. With knowledge distillation and re-blurring loss, we further design a lightweight deblur network to generate high-quality sequences with brightness and texture consistency with the original input. Quantitative and qualitative experiments conducted on our real-world and synthetic datasets with spikes validate the superior generalization of the proposed framework. Our code, data and trained models will be available at \\url{https://github.com/chenkang455/S-SDM}.","sentences":["Reconstructing a sequence of sharp images from the blurry input is crucial for enhancing our insights into the captured scene and poses a significant challenge due to the limited temporal features embedded in the image.","Spike cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing motion features and beneficial for solving this ill-posed problem.","Nonetheless, existing methods fall into the supervised learning paradigm, which suffers from notable performance degradation when applied to real-world scenarios that diverge from the synthetic training data domain.","Moreover, the quality of reconstructed images is capped by the generated images based on motion analysis interpolation, which inherently differs from the actual scene, affecting the generalization ability of these methods in real high-speed scenarios.","To address these challenges, we propose the first self-supervised framework for the task of spike-guided motion deblurring.","Our approach begins with the formulation of a spike-guided deblurring model that explores the theoretical relationships among spike streams, blurry images, and their corresponding sharp sequences.","We subsequently develop a self-supervised cascaded framework to alleviate the issues of spike noise and spatial-resolution mismatching encountered in the deblurring model.","With knowledge distillation and re-blurring loss, we further design a lightweight deblur network to generate high-quality sequences with brightness and texture consistency with the original input.","Quantitative and qualitative experiments conducted on our real-world and synthetic datasets with spikes validate the superior generalization of the proposed framework.","Our code, data and trained models will be available at \\url{https://github.com/chenkang455/S-SDM}."],"url":"http://arxiv.org/abs/2403.09486v1","category":"cs.CV"}
{"created":"2024-03-14 15:27:53","title":"Dynamical pressure boundary condition for weakly-compressible smoothed particle hydrodynamics","abstract":"This paper introduces a novel dynamical pressure boundary condition for weakly-compressible smoothed particle hydrodynamics (WCSPH). Unlike previous methods that rely on indirect approaches or ghost particles, our method integrates the dynamical boundary pressure directly into the SPH approximation of the pressure gradient on near-boundary particles. Additionally, we develop a meshfree bidirectional in-/outflow buffer by periodically relabelling buffer particles at each time step, a concept that has not been explored before. This simple yet effective buffer facilitates the simulation of both uni- and bidirectional flows, especially those with mixed in-/outflow boundary conditions. We validate the accuracy and convergence of our method through benchmark cases with available analytical solutions. Furthermore, we demonstrate its versatility in hemodynamic simulations by investigating generic carotid and aorta flows with the Windkessel model, paving the way for studying the cardiovascular system within a unified meshfree computational framework.","sentences":["This paper introduces a novel dynamical pressure boundary condition for weakly-compressible smoothed particle hydrodynamics (WCSPH).","Unlike previous methods that rely on indirect approaches or ghost particles, our method integrates the dynamical boundary pressure directly into the SPH approximation of the pressure gradient on near-boundary particles.","Additionally, we develop a meshfree bidirectional in-/outflow buffer by periodically relabelling buffer particles at each time step, a concept that has not been explored before.","This simple yet effective buffer facilitates the simulation of both uni- and bidirectional flows, especially those with mixed in-/outflow boundary conditions.","We validate the accuracy and convergence of our method through benchmark cases with available analytical solutions.","Furthermore, we demonstrate its versatility in hemodynamic simulations by investigating generic carotid and aorta flows with the Windkessel model, paving the way for studying the cardiovascular system within a unified meshfree computational framework."],"url":"http://arxiv.org/abs/2403.09485v1","category":"physics.flu-dyn"}
{"created":"2024-03-14 15:27:14","title":"Artificial Bugs for Crowdsearch","abstract":"Bug bounty programs, where external agents are invited to search and report vulnerabilities (bugs) in exchange for rewards (bounty), have become a major tool for companies to improve their systems. We suggest augmenting such programs by inserting artificial bugs to increase the incentives to search for real (organic) bugs. Using a model of crowdsearch, we identify the efficiency gains by artificial bugs, and we show that for this, it is sufficient to insert only one artificial bug. Artificial bugs are particularly beneficial, for instance, if the designer places high valuations on finding organic bugs or if the budget for bounty is not sufficiently high. We discuss how to implement artificial bugs and outline their further benefits.","sentences":["Bug bounty programs, where external agents are invited to search and report vulnerabilities (bugs) in exchange for rewards (bounty), have become a major tool for companies to improve their systems.","We suggest augmenting such programs by inserting artificial bugs to increase the incentives to search for real (organic) bugs.","Using a model of crowdsearch, we identify the efficiency gains by artificial bugs, and we show that for this, it is sufficient to insert only one artificial bug.","Artificial bugs are particularly beneficial, for instance, if the designer places high valuations on finding organic bugs or if the budget for bounty is not sufficiently high.","We discuss how to implement artificial bugs and outline their further benefits."],"url":"http://arxiv.org/abs/2403.09484v1","category":"econ.TH"}
{"created":"2024-03-14 15:25:23","title":"Clinical Reasoning over Tabular Data and Text with Bayesian Networks","abstract":"Bayesian networks are well-suited for clinical reasoning on tabular data, but are less compatible with natural language data, for which neural networks provide a successful framework. This paper compares and discusses strategies to augment Bayesian networks with neural text representations, both in a generative and discriminative manner. This is illustrated with simulation results for a primary care use case (diagnosis of pneumonia) and discussed in a broader clinical context.","sentences":["Bayesian networks are well-suited for clinical reasoning on tabular data, but are less compatible with natural language data, for which neural networks provide a successful framework.","This paper compares and discusses strategies to augment Bayesian networks with neural text representations, both in a generative and discriminative manner.","This is illustrated with simulation results for a primary care use case (diagnosis of pneumonia) and discussed in a broader clinical context."],"url":"http://arxiv.org/abs/2403.09481v1","category":"cs.AI"}
{"created":"2024-03-14 15:22:33","title":"What Sketch Explainability Really Means for Downstream Tasks","abstract":"In this paper, we explore the unique modality of sketch for explainability, emphasising the profound impact of human strokes compared to conventional pixel-oriented studies. Beyond explanations of network behavior, we discern the genuine implications of explainability across diverse downstream sketch-related tasks. We propose a lightweight and portable explainability solution -- a seamless plugin that integrates effortlessly with any pre-trained model, eliminating the need for re-training. Demonstrating its adaptability, we present four applications: highly studied retrieval and generation, and completely novel assisted drawing and sketch adversarial attacks. The centrepiece to our solution is a stroke-level attribution map that takes different forms when linked with downstream tasks. By addressing the inherent non-differentiability of rasterisation, we enable explanations at both coarse stroke level (SLA) and partial stroke level (P-SLA), each with its advantages for specific downstream tasks.","sentences":["In this paper, we explore the unique modality of sketch for explainability, emphasising the profound impact of human strokes compared to conventional pixel-oriented studies.","Beyond explanations of network behavior, we discern the genuine implications of explainability across diverse downstream sketch-related tasks.","We propose a lightweight and portable explainability solution -- a seamless plugin that integrates effortlessly with any pre-trained model, eliminating the need for re-training.","Demonstrating its adaptability, we present four applications: highly studied retrieval and generation, and completely novel assisted drawing and sketch adversarial attacks.","The centrepiece to our solution is a stroke-level attribution map that takes different forms when linked with downstream tasks.","By addressing the inherent non-differentiability of rasterisation, we enable explanations at both coarse stroke level (SLA) and partial stroke level (P-SLA), each with its advantages for specific downstream tasks."],"url":"http://arxiv.org/abs/2403.09480v1","category":"cs.CV"}
{"created":"2024-03-14 15:20:54","title":"Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks","abstract":"Current language models have demonstrated their capability to develop basic reasoning, but struggle in more complicated reasoning tasks that require a combination of atomic skills, such as math word problem requiring skills like arithmetic and unit conversion. Previous methods either do not improve the inherent atomic skills of models or not attempt to generalize the atomic skills to complex reasoning tasks. In this paper, we first propose a probing framework to investigate whether the atomic skill can spontaneously generalize to complex reasoning tasks. Then, we introduce a hierarchical curriculum learning training strategy to achieve better skill generalization. In our experiments, we find that atomic skills can not spontaneously generalize to compositional tasks. By leveraging hierarchical curriculum learning, we successfully induce generalization, significantly improve the performance of open-source LMs on complex reasoning tasks. Promisingly, the skill generalization exhibit effective in cross-dataset and cross-domain scenarios. Complex reasoning can also help enhance atomic skills. Our findings offer valuable guidance for designing better training strategies for complex reasoning tasks.","sentences":["Current language models have demonstrated their capability to develop basic reasoning, but struggle in more complicated reasoning tasks that require a combination of atomic skills, such as math word problem requiring skills like arithmetic and unit conversion.","Previous methods either do not improve the inherent atomic skills of models or not attempt to generalize the atomic skills to complex reasoning tasks.","In this paper, we first propose a probing framework to investigate whether the atomic skill can spontaneously generalize to complex reasoning tasks.","Then, we introduce a hierarchical curriculum learning training strategy to achieve better skill generalization.","In our experiments, we find that atomic skills can not spontaneously generalize to compositional tasks.","By leveraging hierarchical curriculum learning, we successfully induce generalization, significantly improve the performance of open-source LMs on complex reasoning tasks.","Promisingly, the skill generalization exhibit effective in cross-dataset and cross-domain scenarios.","Complex reasoning can also help enhance atomic skills.","Our findings offer valuable guidance for designing better training strategies for complex reasoning tasks."],"url":"http://arxiv.org/abs/2403.09479v1","category":"cs.LG"}
{"created":"2024-03-14 15:19:52","title":"A syntactic characterization of weakly Mal'tsev varieties","abstract":"The notion of a weakly Mal'tsev category, as it was introduced in 2008 by the third author, is a generalization of the classical notion of a Mal'tsev category. It is well-known that a variety of universal algebras is a Mal'tsev category if and only if its theory admits a Mal'tsev term. In the main theorem of this paper, we prove a syntactic characterization of the varieties that are weakly Mal'tsev categories. We apply our result to the variety of distributive lattices which was known to be a weakly Mal'tsev category before. By a result of Z. Janelidze and the third author, a finitely complete category is weakly Mal'tsev if and only if any internal strong reflexive relation is an equivalence relation. In the last part of this paper, we give a syntactic characterization of those varieties in which any regular reflexive relation is an equivalence relation.","sentences":["The notion of a weakly Mal'tsev category, as it was introduced in 2008 by the third author, is a generalization of the classical notion of a Mal'tsev category.","It is well-known that a variety of universal algebras is a Mal'tsev category if and only if its theory admits a Mal'tsev term.","In the main theorem of this paper, we prove a syntactic characterization of the varieties that are weakly Mal'tsev categories.","We apply our result to the variety of distributive lattices which was known to be a weakly Mal'tsev category before.","By a result of Z. Janelidze and the third author, a finitely complete category is weakly Mal'tsev if and only if any internal strong reflexive relation is an equivalence relation.","In the last part of this paper, we give a syntactic characterization of those varieties in which any regular reflexive relation is an equivalence relation."],"url":"http://arxiv.org/abs/2403.09478v1","category":"math.CT"}
{"created":"2024-03-14 15:14:24","title":"An Industrial Experience Report about Challenges from Continuous Monitoring, Improvement, and Deployment for Autonomous Driving Features","abstract":"Using continuous development, deployment, and monitoring (CDDM) to understand and improve applications in a customer's context is widely used for non-safety applications such as smartphone apps or web applications to enable rapid and innovative feature improvements. Having demonstrated its potential in such domains, it may have the potential to also improve the software development for automotive functions as some OEMs described on a high level in their financial company communiqus. However, the application of a CDDM strategy also faces challenges from a process adherence and documentation perspective as required by safety-related products such as autonomous driving systems (ADS) and guided by industry standards such as ISO-26262 and ISO21448. There are publications on CDDM in safety-relevant contexts that focus on safety-critical functions on a rather generic level and thus, not specifically ADS or automotive, or that are concentrating only on software and hence, missing out the particular context of an automotive OEM: Well-established legacy processes and the need of their adaptations, and aspects originating from the role of being a system integrator for software/software, hardware/hardware, and hardware/software. In this paper, particular challenges from the automotive domain to better adopt CDDM are identified and discussed to shed light on research gaps to enhance CDDM, especially for the software development of safe ADS. The challenges are identified from today's industrial well-established ways of working by conducting interviews with domain experts and complemented by a literature study.","sentences":["Using continuous development, deployment, and monitoring (CDDM) to understand and improve applications in a customer's context is widely used for non-safety applications such as smartphone apps or web applications to enable rapid and innovative feature improvements.","Having demonstrated its potential in such domains, it may have the potential to also improve the software development for automotive functions as some OEMs described on a high level in their financial company communiqus.","However, the application of a CDDM strategy also faces challenges from a process adherence and documentation perspective as required by safety-related products such as autonomous driving systems (ADS) and guided by industry standards such as ISO-26262 and ISO21448.","There are publications on CDDM in safety-relevant contexts that focus on safety-critical functions on a rather generic level and thus, not specifically ADS or automotive, or that are concentrating only on software and hence, missing out the particular context of an automotive OEM:","Well-established legacy processes and the need of their adaptations, and aspects originating from the role of being a system integrator for software/software, hardware/hardware, and hardware/software.","In this paper, particular challenges from the automotive domain to better adopt CDDM are identified and discussed to shed light on research gaps to enhance CDDM, especially for the software development of safe ADS.","The challenges are identified from today's industrial well-established ways of working by conducting interviews with domain experts and complemented by a literature study."],"url":"http://arxiv.org/abs/2403.09474v1","category":"cs.SE"}
{"created":"2024-03-14 15:14:23","title":"Analysis of a continuous opinion and discrete action dynamics model coupled with an external observation dynamics","abstract":"We consider a set of consumers in a city or town (who thus generate pollution) whose opinion is governed by a continuous opinion and discrete action (CODA) dynamics model. This dynamics is coupled with an observation signal dynamics, which defines the information the consumers have access to regarding the common pollution. We show that the external observation signal has a significant impact on the asymptotic behavior of the CODA model. When the coupling is strong, it induces either a chaotic behavior or convergence towards a limit cycle. When the coupling is weak, a more classical behavior characterized by local agreements in polarized clusters is observed. In both cases, conditions under which clusters of consumers don't change their actions are provided.Numerical examples are provided to illustrate the derived analytical results.","sentences":["We consider a set of consumers in a city or town (who thus generate pollution) whose opinion is governed by a continuous opinion and discrete action (CODA) dynamics model.","This dynamics is coupled with an observation signal dynamics, which defines the information the consumers have access to regarding the common pollution.","We show that the external observation signal has a significant impact on the asymptotic behavior of the CODA model.","When the coupling is strong, it induces either a chaotic behavior or convergence towards a limit cycle.","When the coupling is weak, a more classical behavior characterized by local agreements in polarized clusters is observed.","In both cases, conditions under which clusters of consumers don't change their actions are provided.","Numerical examples are provided to illustrate the derived analytical results."],"url":"http://arxiv.org/abs/2403.09473v1","category":"math.OC"}
{"created":"2024-03-14 15:12:38","title":"Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision","abstract":"Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \\textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervised reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such \\textit{easy-to-hard generalization from evaluators} can enable \\textit{easy-to-hard generalizations in generators} either through re-ranking or reinforcement learning (RL). Notably, our process-supervised 7b RL model achieves an accuracy of 34.0\\% on MATH500, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision.","sentences":["Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result.","This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans?","This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \\textit{easy-to-hard generalization}.","Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks.","Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervised reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems.","We show that such \\textit{easy-to-hard generalization from evaluators} can enable \\textit{easy-to-hard generalizations in generators} either through re-ranking or reinforcement learning (RL).","Notably, our process-supervised 7b RL model achieves an accuracy of 34.0\\% on MATH500, despite only using human supervision on easy problems.","Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision."],"url":"http://arxiv.org/abs/2403.09472v1","category":"cs.LG"}
{"created":"2024-03-14 15:10:54","title":"MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models","abstract":"Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality. Recent advancements have utilized the diffusion model and attention mechanisms to improve gesture synthesis. However, due to the high computational complexity of these techniques, generating long and diverse sequences with low latency remains a challenge. We explore the potential of state space models (SSMs) to address the challenge, implementing a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures. Leveraging the foundational Mamba block, we introduce MambaTalk, enhancing gesture diversity and rhythm through multimodal integration. Extensive experiments demonstrate that our method matches or exceeds the performance of state-of-the-art models.","sentences":["Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality.","Recent advancements have utilized the diffusion model and attention mechanisms to improve gesture synthesis.","However, due to the high computational complexity of these techniques, generating long and diverse sequences with low latency remains a challenge.","We explore the potential of state space models (SSMs) to address the challenge, implementing a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures.","Leveraging the foundational Mamba block, we introduce MambaTalk, enhancing gesture diversity and rhythm through multimodal integration.","Extensive experiments demonstrate that our method matches or exceeds the performance of state-of-the-art models."],"url":"http://arxiv.org/abs/2403.09471v1","category":"cs.CV"}
{"created":"2024-03-14 15:09:04","title":"Climate Immobility Traps: A Household-Level Test","abstract":"The complex relationship between climate shocks, migration, and adaptation hampers a rigorous understanding of the heterogeneous mobility outcomes of farm households exposed to climate risk. To unpack this heterogeneity, the analysis combines longitudinal multi-topic household survey data from Nigeria with a causal machine learning approach, tailored to a conceptual framework bridging economic migration theory and the poverty traps literature. The results show that pre-shock asset levels, in situ adaptive capacity, and cumulative shock exposure drive not just the magnitude but also the sign of the impact of agriculture-relevant weather anomalies on the mobility outcomes of farming households. While local adaptation acts as a substitute for migration, the roles played by wealth constraints and repeated shock exposure suggest the presence of climate-induced immobility traps.","sentences":["The complex relationship between climate shocks, migration, and adaptation hampers a rigorous understanding of the heterogeneous mobility outcomes of farm households exposed to climate risk.","To unpack this heterogeneity, the analysis combines longitudinal multi-topic household survey data from Nigeria with a causal machine learning approach, tailored to a conceptual framework bridging economic migration theory and the poverty traps literature.","The results show that pre-shock asset levels, in situ adaptive capacity, and cumulative shock exposure drive not just the magnitude but also the sign of the impact of agriculture-relevant weather anomalies on the mobility outcomes of farming households.","While local adaptation acts as a substitute for migration, the roles played by wealth constraints and repeated shock exposure suggest the presence of climate-induced immobility traps."],"url":"http://arxiv.org/abs/2403.09470v1","category":"econ.GN"}
{"created":"2024-03-14 15:07:36","title":"Eta Inversion: Designing an Optimal Eta Function for Diffusion-based Real Image Editing","abstract":"Diffusion models have achieved remarkable success in the domain of text-guided image generation and, more recently, in text-guided image editing. A commonly adopted strategy for editing real images involves inverting the diffusion process to obtain a noisy representation of the original image, which is then denoised to achieve the desired edits. However, current methods for diffusion inversion often struggle to produce edits that are both faithful to the specified text prompt and closely resemble the source image. To overcome these limitations, we introduce a novel and adaptable diffusion inversion technique for real image editing, which is grounded in a theoretical analysis of the role of $\\eta$ in the DDIM sampling equation for enhanced editability. By designing a universal diffusion inversion method with a time- and region-dependent $\\eta$ function, we enable flexible control over the editing extent. Through a comprehensive series of quantitative and qualitative assessments, involving a comparison with a broad array of recent methods, we demonstrate the superiority of our approach. Our method not only sets a new benchmark in the field but also significantly outperforms existing strategies. Our code is available at https://github.com/furiosa-ai/eta-inversion","sentences":["Diffusion models have achieved remarkable success in the domain of text-guided image generation and, more recently, in text-guided image editing.","A commonly adopted strategy for editing real images involves inverting the diffusion process to obtain a noisy representation of the original image, which is then denoised to achieve the desired edits.","However, current methods for diffusion inversion often struggle to produce edits that are both faithful to the specified text prompt and closely resemble the source image.","To overcome these limitations, we introduce a novel and adaptable diffusion inversion technique for real image editing, which is grounded in a theoretical analysis of the role of $\\eta$ in the DDIM sampling equation for enhanced editability.","By designing a universal diffusion inversion method with a time- and region-dependent $\\eta$ function, we enable flexible control over the editing extent.","Through a comprehensive series of quantitative and qualitative assessments, involving a comparison with a broad array of recent methods, we demonstrate the superiority of our approach.","Our method not only sets a new benchmark in the field but also significantly outperforms existing strategies.","Our code is available at https://github.com/furiosa-ai/eta-inversion"],"url":"http://arxiv.org/abs/2403.09468v1","category":"cs.CV"}
{"created":"2024-03-14 15:04:45","title":"Outlier Robust Multivariate Polynomial Regression","abstract":"We study the problem of robust multivariate polynomial regression: let $p\\colon\\mathbb{R}^n\\to\\mathbb{R}$ be an unknown $n$-variate polynomial of degree at most $d$ in each variable. We are given as input a set of random samples $(\\mathbf{x}_i,y_i) \\in [-1,1]^n \\times \\mathbb{R}$ that are noisy versions of $(\\mathbf{x}_i,p(\\mathbf{x}_i))$. More precisely, each $\\mathbf{x}_i$ is sampled independently from some distribution $\\chi$ on $[-1,1]^n$, and for each $i$ independently, $y_i$ is arbitrary (i.e., an outlier) with probability at most $\\rho < 1/2$, and otherwise satisfies $|y_i-p(\\mathbf{x}_i)|\\leq\\sigma$. The goal is to output a polynomial $\\hat{p}$, of degree at most $d$ in each variable, within an $\\ell_\\infty$-distance of at most $O(\\sigma)$ from $p$.   Kane, Karmalkar, and Price [FOCS'17] solved this problem for $n=1$. We generalize their results to the $n$-variate setting, showing an algorithm that achieves a sample complexity of $O_n(d^n\\log d)$, where the hidden constant depends on $n$, if $\\chi$ is the $n$-dimensional Chebyshev distribution. The sample complexity is $O_n(d^{2n}\\log d)$, if the samples are drawn from the uniform distribution instead. The approximation error is guaranteed to be at most $O(\\sigma)$, and the run-time depends on $\\log(1/\\sigma)$. In the setting where each $\\mathbf{x}_i$ and $y_i$ are known up to $N$ bits of precision, the run-time's dependence on $N$ is linear. We also show that our sample complexities are optimal in terms of $d^n$. Furthermore, we show that it is possible to have the run-time be independent of $1/\\sigma$, at the cost of a higher sample complexity.","sentences":["We study the problem of robust multivariate polynomial regression: let $p\\colon\\mathbb{R}^n\\to\\mathbb{R}$ be an unknown $n$-variate polynomial of degree at most $d$ in each variable.","We are given as input a set of random samples $(\\mathbf{x}_i,y_i)","\\in","[-1,1]^n \\times \\mathbb{R}$","that are noisy versions of $(\\mathbf{x}_i,p(\\mathbf{x}_i))$. More precisely, each $\\mathbf{x}_i$ is sampled independently from some distribution $\\chi$ on $[-1,1]^n$, and for each $i$ independently, $y_i$ is arbitrary (i.e., an outlier) with probability at most $\\rho < 1/2$, and otherwise satisfies $|y_i-p(\\mathbf{x}_i)|\\leq\\sigma$.","The goal is to output a polynomial $\\hat{p}$, of degree at most $d$ in each variable, within an $\\ell_\\infty$-distance of at most $O(\\sigma)$ from $p$.   Kane, Karmalkar, and","Price","[FOCS'17] solved this problem for $n=1$. We generalize their results to the $n$-variate setting, showing an algorithm that achieves a sample complexity of $O_n(d^n\\log d)$, where the hidden constant depends on $n$, if $\\chi$ is the $n$-dimensional Chebyshev distribution.","The sample complexity is $O_n(d^{2n}\\log d)$, if the samples are drawn from the uniform distribution instead.","The approximation error is guaranteed to be at most $O(\\sigma)$, and the run-time depends on $\\log(1/\\sigma)$. In the setting where each $\\mathbf{x}_i$ and $y_i$ are known up to $N$ bits of precision, the run-time's dependence on $N$ is linear.","We also show that our sample complexities are optimal in terms of $d^n$. Furthermore, we show that it is possible to have the run-time be independent of $1/\\sigma$, at the cost of a higher sample complexity."],"url":"http://arxiv.org/abs/2403.09465v1","category":"cs.DS"}
{"created":"2024-03-14 15:02:49","title":"Flat-top plasma operational space of the STEP power plant","abstract":"STEP is a spherical tokamak prototype power plant that is being designed to demonstrate net electric power. The design phase involves the exploitation of plasma models to optimise fusion performance subject to satisfying various physics and engineering constraints. A modelling workflow, including integrated core plasma modelling, MHD stability analysis, SOL and pedestal modelling, coil set and free boundary equilibrium solvers, and whole plant design, has been developed to specify the design parameters and to develop viable scenarios. The integrated core plasma model JETTO is used to develop individual flat-top operating points that satisfy imposed criteria for fusion power performance within operational constraints. Key plasma parameters such as normalised beta, Greenwald density fraction, auxiliary power and radiated power have been scanned to scope the operational space and to derive a collection of candidate non-inductive flat-top points. The assumed auxiliary heating and current drive is either from electron cyclotron systems only or a combination of electron cyclotron and electron Bernstein waves. At present stages of transport modelling, there is a large uncertainty in overall confinement for relevant parameter regimes. For each of the two auxiliary heating and current drive systems scenarios, two candidate flat-top points have been developed based on different confinement assumptions, totalling to four operating points. A lower confinement assumption generally suggests operating points in high-density, high auxiliary power regimes, whereas higher confinement would allow access to a broader parameter regime in density and power while maintaining target fusion power performance.","sentences":["STEP is a spherical tokamak prototype power plant that is being designed to demonstrate net electric power.","The design phase involves the exploitation of plasma models to optimise fusion performance subject to satisfying various physics and engineering constraints.","A modelling workflow, including integrated core plasma modelling, MHD stability analysis, SOL and pedestal modelling, coil set and free boundary equilibrium solvers, and whole plant design, has been developed to specify the design parameters and to develop viable scenarios.","The integrated core plasma model JETTO is used to develop individual flat-top operating points that satisfy imposed criteria for fusion power performance within operational constraints.","Key plasma parameters such as normalised beta, Greenwald density fraction, auxiliary power and radiated power have been scanned to scope the operational space and to derive a collection of candidate non-inductive flat-top points.","The assumed auxiliary heating and current drive is either from electron cyclotron systems only or a combination of electron cyclotron and electron Bernstein waves.","At present stages of transport modelling, there is a large uncertainty in overall confinement for relevant parameter regimes.","For each of the two auxiliary heating and current drive systems scenarios, two candidate flat-top points have been developed based on different confinement assumptions, totalling to four operating points.","A lower confinement assumption generally suggests operating points in high-density, high auxiliary power regimes, whereas higher confinement would allow access to a broader parameter regime in density and power while maintaining target fusion power performance."],"url":"http://arxiv.org/abs/2403.09460v1","category":"physics.plasm-ph"}
{"created":"2024-03-14 14:58:39","title":"Vertex coupling interpolation in quantum chain graphs","abstract":"We analyze band spectrum of the periodic quantum graph in the form of a chain of rings connected by line segments with the vertex coupling which violates the time reversal invariance, interpolating between the $\\delta$ coupling and the one determined by a simple circulant matrix. We find that flat bands are generically absent and that the negative spectrum is nonempty even for interpolation with a non-attractive $\\delta$ coupling; we also determine the high-energy asymptotic behavior of the bands.","sentences":["We analyze band spectrum of the periodic quantum graph in the form of a chain of rings connected by line segments with the vertex coupling which violates the time reversal invariance, interpolating between the $\\delta$ coupling and the one determined by a simple circulant matrix.","We find that flat bands are generically absent and that the negative spectrum is nonempty even for interpolation with a non-attractive $\\delta$ coupling; we also determine the high-energy asymptotic behavior of the bands."],"url":"http://arxiv.org/abs/2403.09457v1","category":"math-ph"}
{"created":"2024-03-14 14:55:16","title":"Edge-apexing in hereditary classes of graphs","abstract":"A class $\\mathcal{G}$ of graphs is called hereditary if it is closed under taking induced subgraphs. We denote by $G^{epex}$ the class of graphs that are at most one edge away from being in $\\mathcal{G}$. We note that $G^{epex}$ is hereditary and prove that if a hereditary class $\\mathcal{G}$ has finitely many forbidden induced subgraphs, then so does $G^{epex}$.   The hereditary class of cographs consists of all graphs $G$ that can be generated from $K_1$ using complementation and disjoint union. Cographs are precisely the graphs that do not have the $4$-vertex path as an induced subgraph. For the class of edge-apex cographs our main result bounds the order of such forbidden induced subgraphs by 8 and finds all of them by computer search.","sentences":["A class $\\mathcal{G}$ of graphs is called hereditary if it is closed under taking induced subgraphs.","We denote by $G^{epex}$ the class of graphs that are at most one edge away from being in $\\mathcal{G}$. We note that $G^{epex}$ is hereditary and prove that if a hereditary class $\\mathcal{G}$ has finitely many forbidden induced subgraphs, then so does $G^{epex}$.   The hereditary class of cographs consists of all graphs $G$ that can be generated from $K_1$ using complementation and disjoint union.","Cographs are precisely the graphs that do not have the $4$-vertex path as an induced subgraph.","For the class of edge-apex cographs our main result bounds the order of such forbidden induced subgraphs by 8 and finds all of them by computer search."],"url":"http://arxiv.org/abs/2403.09456v1","category":"math.CO"}
{"created":"2024-03-14 14:53:18","title":"Machine learning for structural design models of continuous beam systems via influence zones","abstract":"This work develops a machine learned structural design model for continuous beam systems from the inverse problem perspective. After demarcating between forward, optimisation and inverse machine learned operators, the investigation proposes a novel methodology based on the recently developed influence zone concept which represents a fundamental shift in approach compared to traditional structural design methods. The aim of this approach is to conceptualise a non-iterative structural design model that predicts cross-section requirements for continuous beam systems of arbitrary system size. After generating a dataset of known solutions, an appropriate neural network architecture is identified, trained, and tested against unseen data. The results show a mean absolute percentage testing error of 1.6% for cross-section property predictions, along with a good ability of the neural network to generalise well to structural systems of variable size. The CBeamXP dataset generated in this work and an associated python-based neural network training script are available at an open-source data repository to allow for the reproducibility of results and to encourage further investigations.","sentences":["This work develops a machine learned structural design model for continuous beam systems from the inverse problem perspective.","After demarcating between forward, optimisation and inverse machine learned operators, the investigation proposes a novel methodology based on the recently developed influence zone concept which represents a fundamental shift in approach compared to traditional structural design methods.","The aim of this approach is to conceptualise a non-iterative structural design model that predicts cross-section requirements for continuous beam systems of arbitrary system size.","After generating a dataset of known solutions, an appropriate neural network architecture is identified, trained, and tested against unseen data.","The results show a mean absolute percentage testing error of 1.6% for cross-section property predictions, along with a good ability of the neural network to generalise well to structural systems of variable size.","The CBeamXP dataset generated in this work and an associated python-based neural network training script are available at an open-source data repository to allow for the reproducibility of results and to encourage further investigations."],"url":"http://arxiv.org/abs/2403.09454v1","category":"cs.LG"}
{"created":"2024-03-14 14:52:34","title":"Combinatorics of Essential Sets for Positroids","abstract":"Positroids are families of matroids introduced by Postnikov in the study of non-negative Grassmannians. Postnikov identified several combinatorial objects in bijections with positroids, among which are bounded affine permutations. On the other hand, the notion of essential sets, introduced for permutations by Fulton, was used by Knutson in the study of the special family of interval rank positroids. We generalize Fulton's essential sets to bounded affine permutations. The bijection of the latter with positroids, allows the study of their relation. From the point of view of positroids, essential sets are maximally dependent cyclic interval. We define connected essential sets and prove that they give a facet description of the positroid polytope, as well as equations defining the positroid variety. We define a subset of essential sets, called core, which contains minimal rank conditions to uniquely recover a positroid. We provide an algorithm to retrieve the positroid satisfying the rank conditions in the core or any compatible rank condition on cyclic intervals.","sentences":["Positroids are families of matroids introduced by Postnikov in the study of non-negative Grassmannians.","Postnikov identified several combinatorial objects in bijections with positroids, among which are bounded affine permutations.","On the other hand, the notion of essential sets, introduced for permutations by Fulton, was used by Knutson in the study of the special family of interval rank positroids.","We generalize Fulton's essential sets to bounded affine permutations.","The bijection of the latter with positroids, allows the study of their relation.","From the point of view of positroids, essential sets are maximally dependent cyclic interval.","We define connected essential sets and prove that they give a facet description of the positroid polytope, as well as equations defining the positroid variety.","We define a subset of essential sets, called core, which contains minimal rank conditions to uniquely recover a positroid.","We provide an algorithm to retrieve the positroid satisfying the rank conditions in the core or any compatible rank condition on cyclic intervals."],"url":"http://arxiv.org/abs/2403.09453v1","category":"math.CO"}
{"created":"2024-03-14 14:48:37","title":"Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk","abstract":"While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by $5.4\\%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain. This discovery underscores that the privacy risk with diffusion models is even more severe than previously recognized. Codes are available at https://github.com/VITA-Group/Shake-to-Leak.","sentences":["While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information.","In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks.","We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations.","In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by $5.4\\%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain.","This discovery underscores that the privacy risk with diffusion models is even more severe than previously recognized.","Codes are available at https://github.com/VITA-Group/Shake-to-Leak."],"url":"http://arxiv.org/abs/2403.09450v1","category":"cs.LG"}
{"created":"2024-03-14 14:37:25","title":"Sequential optimal experimental design for vapor-liquid equilibrium modeling","abstract":"We propose a general methodology of sequential locally optimal design of experiments for explicit or implicit nonlinear models, as they abound in chemical engineering and, in particular, in vapor-liquid equilibrium modeling. As a sequential design method, our method iteratively alternates between performing experiments, updating parameter estimates, and computing new experiments. Specifically, our sequential design method computes a whole batch of new experiments in each iteration and this batch of new experiments is designed in a two-stage locally optimal manner. In essence, this means that in every iteration the combined information content of the newly proposed experiments and of the already performed experiments is maximized. In order to solve these two-stage locally optimal design problems, a recent and efficient adaptive discretization algorithm is used. We demonstrate the benefits of the proposed methodology on the example of of the parameter estimation for the non-random two-liquid model for narrow azeotropic vapor-liquid equilibria. As it turns out, our sequential optimal design method requires substantially fewer experiments than traditional factorial design to achieve the same model precision and prediction quality. Consequently, our method can contribute to a substantially reduced experimental effort in vapor-liquid equilibrium modeling and beyond.","sentences":["We propose a general methodology of sequential locally optimal design of experiments for explicit or implicit nonlinear models, as they abound in chemical engineering and, in particular, in vapor-liquid equilibrium modeling.","As a sequential design method, our method iteratively alternates between performing experiments, updating parameter estimates, and computing new experiments.","Specifically, our sequential design method computes a whole batch of new experiments in each iteration and this batch of new experiments is designed in a two-stage locally optimal manner.","In essence, this means that in every iteration the combined information content of the newly proposed experiments and of the already performed experiments is maximized.","In order to solve these two-stage locally optimal design problems, a recent and efficient adaptive discretization algorithm is used.","We demonstrate the benefits of the proposed methodology on the example of of the parameter estimation for the non-random two-liquid model for narrow azeotropic vapor-liquid equilibria.","As it turns out, our sequential optimal design method requires substantially fewer experiments than traditional factorial design to achieve the same model precision and prediction quality.","Consequently, our method can contribute to a substantially reduced experimental effort in vapor-liquid equilibrium modeling and beyond."],"url":"http://arxiv.org/abs/2403.09443v1","category":"math.OC"}
{"created":"2024-03-14 14:35:53","title":"LLM-based agents for automating the enhancement of user story quality: An early report","abstract":"In agile software development, maintaining high-quality user stories is crucial, but also challenging. This study explores the use of large language models to automatically improve the user story quality in Austrian Post Group IT agile teams. We developed a reference model for an Autonomous LLM-based Agent System and implemented it at the company. The quality of user stories in the study and the effectiveness of these agents for user story quality improvement was assessed by 11 participants across six agile teams. Our findings demonstrate the potential of LLMs in improving user story quality, contributing to the research on AI role in agile development, and providing a practical example of the transformative impact of AI in an industry setting.","sentences":["In agile software development, maintaining high-quality user stories is crucial, but also challenging.","This study explores the use of large language models to automatically improve the user story quality in Austrian Post Group IT agile teams.","We developed a reference model for an Autonomous LLM-based Agent System and implemented it at the company.","The quality of user stories in the study and the effectiveness of these agents for user story quality improvement was assessed by 11 participants across six agile teams.","Our findings demonstrate the potential of LLMs in improving user story quality, contributing to the research on AI role in agile development, and providing a practical example of the transformative impact of AI in an industry setting."],"url":"http://arxiv.org/abs/2403.09442v1","category":"cs.SE"}
{"created":"2024-03-14 14:32:05","title":"On a problem posed by Bjorn Poonen","abstract":"Bjorn Poonen asked whether there exists a polynomial giving a surjection $\\mathbb{Z} \\times \\mathbb{Z} \\to \\mathbb{N}$. We answer this question in the negative, conditional on a conjecture of Vojta. More precisely, we show that if such a function exists, there is a family of open surfaces with dense integral points despite the surfaces being of log general type.","sentences":["Bjorn Poonen asked whether there exists a polynomial giving a surjection $\\mathbb{Z} \\times \\mathbb{Z} \\to \\mathbb{N}$.","We answer this question in the negative, conditional on a conjecture of Vojta.","More precisely, we show that if such a function exists, there is a family of open surfaces with dense integral points despite the surfaces being of log general type."],"url":"http://arxiv.org/abs/2403.09440v1","category":"math.NT"}
{"created":"2024-03-14 14:31:22","title":"3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation","abstract":"Text-driven 3D scene generation techniques have made rapid progress in recent years. Their success is mainly attributed to using existing generative models to iteratively perform image warping and inpainting to generate 3D scenes. However, these methods heavily rely on the outputs of existing models, leading to error accumulation in geometry and appearance that prevent the models from being used in various scenarios (e.g., outdoor and unreal scenarios). To address this limitation, we generatively refine the newly generated local views by querying and aggregating global 3D information, and then progressively generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF as a unified representation of the 3D scene to constrain global 3D consistency, and propose a generative refinement network to synthesize new contents with higher quality by exploiting the natural image prior from 2D diffusion model as well as the global 3D information of the current scene. Our extensive experiments demonstrate that, in comparison to previous methods, our approach supports wide variety of scene generation and arbitrary camera trajectories with improved visual quality and 3D consistency.","sentences":["Text-driven 3D scene generation techniques have made rapid progress in recent years.","Their success is mainly attributed to using existing generative models to iteratively perform image warping and inpainting to generate 3D scenes.","However, these methods heavily rely on the outputs of existing models, leading to error accumulation in geometry and appearance that prevent the models from being used in various scenarios (e.g., outdoor and unreal scenarios).","To address this limitation, we generatively refine the newly generated local views by querying and aggregating global 3D information, and then progressively generate the 3D scene.","Specifically, we employ a tri-plane features-based NeRF as a unified representation of the 3D scene to constrain global 3D consistency, and propose a generative refinement network to synthesize new contents with higher quality by exploiting the natural image prior from 2D diffusion model as well as the global 3D information of the current scene.","Our extensive experiments demonstrate that, in comparison to previous methods, our approach supports wide variety of scene generation and arbitrary camera trajectories with improved visual quality and 3D consistency."],"url":"http://arxiv.org/abs/2403.09439v1","category":"cs.CV"}
{"created":"2024-03-14 14:31:03","title":"On some extensions of shape-constrained generalized additive modelling in R","abstract":"Regression models that incorporate smooth functions of predictor variables to explain the relationships with a response variable have gained widespread usage and proved successful in various applications. By incorporating smooth functions of predictor variables, these models can capture complex relationships between the response and predictors while still allowing for interpretation of the results. In situations where the relationships between a response variable and predictors are explored, it is not uncommon to assume that these relationships adhere to certain shape constraints. Examples of such constraints include monotonicity and convexity. The scam package for R has become a popular package to carry out the full fitting of exponential family generalized additive modelling with shape restrictions on smooths. The paper aims to extend the existing framework of shape-constrained generalized additive models (SCAM) to accommodate smooth interactions of covariates, linear functionals of shape-constrained smooths and incorporation of residual autocorrelation. The methods described in this paper are implemented in the recent version of the package scam, available on the Comprehensive R Archive Network (CRAN).","sentences":["Regression models that incorporate smooth functions of predictor variables to explain the relationships with a response variable have gained widespread usage and proved successful in various applications.","By incorporating smooth functions of predictor variables, these models can capture complex relationships between the response and predictors while still allowing for interpretation of the results.","In situations where the relationships between a response variable and predictors are explored, it is not uncommon to assume that these relationships adhere to certain shape constraints.","Examples of such constraints include monotonicity and convexity.","The scam package for R has become a popular package to carry out the full fitting of exponential family generalized additive modelling with shape restrictions on smooths.","The paper aims to extend the existing framework of shape-constrained generalized additive models (SCAM) to accommodate smooth interactions of covariates, linear functionals of shape-constrained smooths and incorporation of residual autocorrelation.","The methods described in this paper are implemented in the recent version of the package scam, available on the Comprehensive R Archive Network (CRAN)."],"url":"http://arxiv.org/abs/2403.09438v1","category":"stat.CO"}
{"created":"2024-03-14 14:30:12","title":"Emergent time scales of epistasis in protein evolution","abstract":"We introduce a data-driven epistatic model of protein evolution, capable of generating evolutionary trajectories spanning very different time scales reaching from individual mutations to diverged homologs. Our in silico evolution encompasses random nucleotide mutations, insertions and deletions, and models selection using a fitness landscape, which is inferred via a generative probabilistic model for protein families. We show that the proposed framework accurately reproduces the sequence statistics of both short-time (experimental) and long-time (natural) protein evolution, suggesting applicability also to relatively data-poor intermediate evolutionary time scales, which are currently inaccessible to evolution experiments. Our model uncovers a highly collective nature of epistasis, gradually changing the fitness effect of mutations in a diverging sequence context, rather than acting via strong interactions between individual mutations. This collective nature triggers the emergence of a long evolutionary time scale, separating fast mutational processes inside a given sequence context, from the slow evolution of the context itself. The model quantitatively reproduces the extent of contingency and entrenchment, as well as the loss of predictability in protein evolution observed in deep mutational scanning experiments of distant homologs. It thereby deepens our understanding of the interplay between mutation and selection in shaping protein diversity and novel functions, allows to statistically forecast evolution, and challenges the prevailing independent-site models of protein evolution, which are unable to capture the fundamental importance of epistasis.","sentences":["We introduce a data-driven epistatic model of protein evolution, capable of generating evolutionary trajectories spanning very different time scales reaching from individual mutations to diverged homologs.","Our in silico evolution encompasses random nucleotide mutations, insertions and deletions, and models selection using a fitness landscape, which is inferred via a generative probabilistic model for protein families.","We show that the proposed framework accurately reproduces the sequence statistics of both short-time (experimental) and long-time (natural) protein evolution, suggesting applicability also to relatively data-poor intermediate evolutionary time scales, which are currently inaccessible to evolution experiments.","Our model uncovers a highly collective nature of epistasis, gradually changing the fitness effect of mutations in a diverging sequence context, rather than acting via strong interactions between individual mutations.","This collective nature triggers the emergence of a long evolutionary time scale, separating fast mutational processes inside a given sequence context, from the slow evolution of the context itself.","The model quantitatively reproduces the extent of contingency and entrenchment, as well as the loss of predictability in protein evolution observed in deep mutational scanning experiments of distant homologs.","It thereby deepens our understanding of the interplay between mutation and selection in shaping protein diversity and novel functions, allows to statistically forecast evolution, and challenges the prevailing independent-site models of protein evolution, which are unable to capture the fundamental importance of epistasis."],"url":"http://arxiv.org/abs/2403.09436v1","category":"q-bio.BM"}
{"created":"2024-03-14 14:29:01","title":"StarMalloc: A Formally Verified, Concurrent, Performant, and Security-Oriented Memory Allocator","abstract":"In this work, we present StarMalloc, a verified, security-oriented, concurrent memory allocator that can be used as a drop-in replacement in real-world projects. Using the Steel separation logic framework, we show how to specify and verify StarMalloc, relying on dependent types and modular abstractions to enable efficient verification. As part of StarMalloc, we also develop several generic datastructures and proof libraries directly reusable in future systems verification projects. We finally show that StarMalloc can be used with real-world projects, including the Firefox browser, and evaluate it against 10 state-of-the-art memory allocators, demonstrating its competitiveness.","sentences":["In this work, we present StarMalloc, a verified, security-oriented, concurrent memory allocator that can be used as a drop-in replacement in real-world projects.","Using the Steel separation logic framework, we show how to specify and verify StarMalloc, relying on dependent types and modular abstractions to enable efficient verification.","As part of StarMalloc, we also develop several generic datastructures and proof libraries directly reusable in future systems verification projects.","We finally show that StarMalloc can be used with real-world projects, including the Firefox browser, and evaluate it against 10 state-of-the-art memory allocators, demonstrating its competitiveness."],"url":"http://arxiv.org/abs/2403.09435v1","category":"cs.PL"}
{"created":"2024-03-14 14:25:10","title":"Open-Vocabulary Object Detection with Meta Prompt Representation and Instance Contrastive Optimization","abstract":"Classical object detectors are incapable of detecting novel class objects that are not encountered before. Regarding this issue, Open-Vocabulary Object Detection (OVOD) is proposed, which aims to detect the objects in the candidate class list. However, current OVOD models are suffering from overfitting on the base classes, heavily relying on the large-scale extra data, and complex training process. To overcome these issues, we propose a novel framework with Meta prompt and Instance Contrastive learning (MIC) schemes. Firstly, we simulate a novel-class-emerging scenario to help the prompt learner that learns class and background prompts generalize to novel classes. Secondly, we design an instance-level contrastive strategy to promote intra-class compactness and inter-class separation, which benefits generalization of the detector to novel class objects. Without using knowledge distillation, ensemble model or extra training data during detector training, our proposed MIC outperforms previous SOTA methods trained with these complex techniques on LVIS. Most importantly, MIC shows great generalization ability on novel classes, e.g., with $+4.3\\%$ and $+1.9\\% \\ \\mathrm{AP}$ improvement compared with previous SOTA on COCO and Objects365, respectively.","sentences":["Classical object detectors are incapable of detecting novel class objects that are not encountered before.","Regarding this issue, Open-Vocabulary Object Detection (OVOD) is proposed, which aims to detect the objects in the candidate class list.","However, current OVOD models are suffering from overfitting on the base classes, heavily relying on the large-scale extra data, and complex training process.","To overcome these issues, we propose a novel framework with Meta prompt and Instance Contrastive learning (MIC) schemes.","Firstly, we simulate a novel-class-emerging scenario to help the prompt learner that learns class and background prompts generalize to novel classes.","Secondly, we design an instance-level contrastive strategy to promote intra-class compactness and inter-class separation, which benefits generalization of the detector to novel class objects.","Without using knowledge distillation, ensemble model or extra training data during detector training, our proposed MIC outperforms previous SOTA methods trained with these complex techniques on LVIS.","Most importantly, MIC shows great generalization ability on novel classes, e.g., with $+4.3\\%$ and $+1.9\\% \\ \\mathrm{AP}$ improvement compared with previous SOTA on COCO and Objects365, respectively."],"url":"http://arxiv.org/abs/2403.09433v1","category":"cs.CV"}
{"created":"2024-03-14 14:25:10","title":"Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians","abstract":"Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics. Existing methods, such as 3D Gaussians, provide modeling for 3D appearance and geometry but lack the ability to simulate physical properties or optimize parameters for heterogeneous objects. We propose Spring-Gaus, a novel framework that integrates 3D Gaussians with physics-based simulation for reconstructing and simulating elastic objects from multi-view videos. Our method utilizes a 3D Spring-Mass model, enabling the optimization of physical parameters at the individual point level while decoupling the learning of physics and appearance. This approach achieves great sample efficiency, enhances generalization, and reduces sensitivity to the distribution of simulation particles. We evaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating accurate reconstruction and simulation of elastic objects. This includes future prediction and simulation under varying initial states and environmental parameters. Project page: https://zlicheng.com/spring_gaus.","sentences":["Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics.","Existing methods, such as 3D Gaussians, provide modeling for 3D appearance and geometry but lack the ability to simulate physical properties or optimize parameters for heterogeneous objects.","We propose Spring-Gaus, a novel framework that integrates 3D Gaussians with physics-based simulation for reconstructing and simulating elastic objects from multi-view videos.","Our method utilizes a 3D Spring-Mass model, enabling the optimization of physical parameters at the individual point level while decoupling the learning of physics and appearance.","This approach achieves great sample efficiency, enhances generalization, and reduces sensitivity to the distribution of simulation particles.","We evaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating accurate reconstruction and simulation of elastic objects.","This includes future prediction and simulation under varying initial states and environmental parameters.","Project page: https://zlicheng.com/spring_gaus."],"url":"http://arxiv.org/abs/2403.09434v1","category":"cs.CV"}
{"created":"2024-03-14 14:14:47","title":"Mitigating attribute amplification in counterfactual image generation","abstract":"Causal generative modelling is gaining interest in medical imaging due to its ability to answer interventional and counterfactual queries. Most work focuses on generating counterfactual images that look plausible, using auxiliary classifiers to enforce effectiveness of simulated interventions. We investigate pitfalls in this approach, discovering the issue of attribute amplification, where unrelated attributes are spuriously affected during interventions, leading to biases across protected characteristics and disease status. We show that attribute amplification is caused by the use of hard labels in the counterfactual training process and propose soft counterfactual fine-tuning to mitigate this issue. Our method substantially reduces the amplification effect while maintaining effectiveness of generated images, demonstrated on a large chest X-ray dataset. Our work makes an important advancement towards more faithful and unbiased causal modelling in medical imaging.","sentences":["Causal generative modelling is gaining interest in medical imaging due to its ability to answer interventional and counterfactual queries.","Most work focuses on generating counterfactual images that look plausible, using auxiliary classifiers to enforce effectiveness of simulated interventions.","We investigate pitfalls in this approach, discovering the issue of attribute amplification, where unrelated attributes are spuriously affected during interventions, leading to biases across protected characteristics and disease status.","We show that attribute amplification is caused by the use of hard labels in the counterfactual training process and propose soft counterfactual fine-tuning to mitigate this issue.","Our method substantially reduces the amplification effect while maintaining effectiveness of generated images, demonstrated on a large chest X-ray dataset.","Our work makes an important advancement towards more faithful and unbiased causal modelling in medical imaging."],"url":"http://arxiv.org/abs/2403.09422v1","category":"cs.CV"}
{"created":"2024-03-14 14:14:43","title":"Fake turbulence","abstract":"High-dimensional dynamical systems projected onto a reduced-order model cease to be deterministic and are best described by probability distributions in state space. Their equations of motion map onto an evolution operator with a deterministic component describing the projected dynamics, and a stochastic one from the neglected dimensions. It is shown that, for projections in which the deterministic component is dominant, `physics-free' stochastic Markovian models can be constructed that mimic many of the statistics of the real flow, even for fairly crude operator approximations. Deterministic models converge to steady states. This is related to general properties of Markov chains and illustrated with data-driven models for a moderate-Reynolds number turbulent channel.","sentences":["High-dimensional dynamical systems projected onto a reduced-order model cease to be deterministic and are best described by probability distributions in state space.","Their equations of motion map onto an evolution operator with a deterministic component describing the projected dynamics, and a stochastic one from the neglected dimensions.","It is shown that, for projections in which the deterministic component is dominant, `physics-free' stochastic Markovian models can be constructed that mimic many of the statistics of the real flow, even for fairly crude operator approximations.","Deterministic models converge to steady states.","This is related to general properties of Markov chains and illustrated with data-driven models for a moderate-Reynolds number turbulent channel."],"url":"http://arxiv.org/abs/2403.09421v1","category":"physics.flu-dyn"}
{"created":"2024-03-14 14:07:31","title":"GPT on a Quantum Computer","abstract":"Large Language Models (LLMs) such as ChatGPT have transformed how we interact with and understand the capabilities of Artificial Intelligence (AI). However, the intersection of LLMs with the burgeoning field of Quantum Machine Learning (QML) is only in its nascent stages. This paper presents an exploration of this niche by detailing a comprehensive framework for implementing the foundational Transformer architecture -- integral to ChatGPT -- within a quantum computing paradigm. We meticulously design quantum circuits that implement adapted versions of the transformer's core components and the generative pre-training phase. By integrating quantum computing with LLMs, we aspire to open new avenues for research in QML and contribute to the ongoing evolution of AI technologies.","sentences":["Large Language Models (LLMs) such as ChatGPT have transformed how we interact with and understand the capabilities of Artificial Intelligence (AI).","However, the intersection of LLMs with the burgeoning field of Quantum Machine Learning (QML) is only in its nascent stages.","This paper presents an exploration of this niche by detailing a comprehensive framework for implementing the foundational Transformer architecture -- integral to ChatGPT -- within a quantum computing paradigm.","We meticulously design quantum circuits that implement adapted versions of the transformer's core components and the generative pre-training phase.","By integrating quantum computing with LLMs, we aspire to open new avenues for research in QML and contribute to the ongoing evolution of AI technologies."],"url":"http://arxiv.org/abs/2403.09418v1","category":"quant-ph"}
{"created":"2024-03-14 14:04:44","title":"Scalability of Metropolis-within-Gibbs schemes for high-dimensional Bayesian models","abstract":"We study general coordinate-wise MCMC schemes (such as Metropolis-within-Gibbs samplers), which are commonly used to fit Bayesian non-conjugate hierarchical models. We relate their convergence properties to the ones of the corresponding (potentially not implementable) Gibbs sampler through the notion of conditional conductance. This allows us to study the performances of popular Metropolis-within-Gibbs schemes for non-conjugate hierarchical models, in high-dimensional regimes where both number of datapoints and parameters increase. Given random data-generating assumptions, we establish dimension-free convergence results, which are in close accordance with numerical evidences. Applications to Bayesian models for binary regression with unknown hyperparameters and discretely observed diffusions are also discussed. Motivated by such statistical applications, auxiliary results of independent interest on approximate conductances and perturbation of Markov operators are provided.","sentences":["We study general coordinate-wise MCMC schemes (such as Metropolis-within-Gibbs samplers), which are commonly used to fit Bayesian non-conjugate hierarchical models.","We relate their convergence properties to the ones of the corresponding (potentially not implementable)","Gibbs sampler through the notion of conditional conductance.","This allows us to study the performances of popular Metropolis-within-Gibbs schemes for non-conjugate hierarchical models, in high-dimensional regimes where both number of datapoints and parameters increase.","Given random data-generating assumptions, we establish dimension-free convergence results, which are in close accordance with numerical evidences.","Applications to Bayesian models for binary regression with unknown hyperparameters and discretely observed diffusions are also discussed.","Motivated by such statistical applications, auxiliary results of independent interest on approximate conductances and perturbation of Markov operators are provided."],"url":"http://arxiv.org/abs/2403.09416v1","category":"stat.CO"}
{"created":"2024-03-14 14:04:29","title":"Region-based U-net for accelerated training and enhanced precision in deep brain segmentation","abstract":"Segmentation of brain structures on MRI is the primary step for further quantitative analysis of brain diseases. Manual segmentation is still considered the gold standard in terms of accuracy; however, such data is extremely time-consuming to generate. This paper presents a deep learning-based segmentation approach for 12 deep-brain structures, utilizing multiple region-based U-Nets. The brain is divided into three focal regions of interest that encompass the brainstem, the ventricular system, and the striatum. Next, three region-based U-nets are run in parallel to parcellate these larger structures into their respective four substructures. This approach not only greatly reduces the training and processing times but also significantly enhances the segmentation accuracy, compared to segmenting the entire MRI image at once. Our approach achieves remarkable accuracy with an average Dice Similarity Coefficient (DSC) of 0.901 and 95% Hausdorff Distance (HD95) of 1.155 mm. The method was compared with state-of-the-art segmentation approaches, demonstrating a high level of accuracy and robustness of the proposed method.","sentences":["Segmentation of brain structures on MRI is the primary step for further quantitative analysis of brain diseases.","Manual segmentation is still considered the gold standard in terms of accuracy; however, such data is extremely time-consuming to generate.","This paper presents a deep learning-based segmentation approach for 12 deep-brain structures, utilizing multiple region-based U-Nets.","The brain is divided into three focal regions of interest that encompass the brainstem, the ventricular system, and the striatum.","Next, three region-based U-nets are run in parallel to parcellate these larger structures into their respective four substructures.","This approach not only greatly reduces the training and processing times but also significantly enhances the segmentation accuracy, compared to segmenting the entire MRI image at once.","Our approach achieves remarkable accuracy with an average Dice Similarity Coefficient (DSC) of 0.901 and 95% Hausdorff Distance (HD95) of 1.155 mm.","The method was compared with state-of-the-art segmentation approaches, demonstrating a high level of accuracy and robustness of the proposed method."],"url":"http://arxiv.org/abs/2403.09414v1","category":"eess.IV"}
{"created":"2024-03-14 14:03:29","title":"OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments","abstract":"Environment maps endowed with sophisticated semantics are pivotal for facilitating seamless interaction between robots and humans, enabling them to effectively carry out various tasks. Open-vocabulary maps, powered by Visual-Language models (VLMs), possess inherent advantages, including multimodal retrieval and open-set classes. However, existing open-vocabulary maps are constrained to closed indoor scenarios and VLM features, thereby diminishing their usability and inference capabilities. Moreover, the absence of topological relationships further complicates the accurate querying of specific instances. In this work, we propose OpenGraph, a representation of open-vocabulary hierarchical graph structure designed for large-scale outdoor environments. OpenGraph initially extracts instances and their captions from visual images using 2D foundation models, encoding the captions with features to enhance textual reasoning. Subsequently, 3D incremental panoramic mapping with feature embedding is achieved by projecting images onto LiDAR point clouds. Finally, the environment is segmented based on lane graph connectivity to construct a hierarchical graph. Validation results from real public dataset SemanticKITTI demonstrate that, even without fine-tuning the models, OpenGraph exhibits the ability to generalize to novel semantic classes and achieve the highest segmentation and query accuracy. The source code of OpenGraph is publicly available at https://github.com/BIT-DYN/OpenGraph.","sentences":["Environment maps endowed with sophisticated semantics are pivotal for facilitating seamless interaction between robots and humans, enabling them to effectively carry out various tasks.","Open-vocabulary maps, powered by Visual-Language models (VLMs), possess inherent advantages, including multimodal retrieval and open-set classes.","However, existing open-vocabulary maps are constrained to closed indoor scenarios and VLM features, thereby diminishing their usability and inference capabilities.","Moreover, the absence of topological relationships further complicates the accurate querying of specific instances.","In this work, we propose OpenGraph, a representation of open-vocabulary hierarchical graph structure designed for large-scale outdoor environments.","OpenGraph initially extracts instances and their captions from visual images using 2D foundation models, encoding the captions with features to enhance textual reasoning.","Subsequently, 3D incremental panoramic mapping with feature embedding is achieved by projecting images onto LiDAR point clouds.","Finally, the environment is segmented based on lane graph connectivity to construct a hierarchical graph.","Validation results from real public dataset SemanticKITTI demonstrate that, even without fine-tuning the models, OpenGraph exhibits the ability to generalize to novel semantic classes and achieve the highest segmentation and query accuracy.","The source code of OpenGraph is publicly available at https://github.com/BIT-DYN/OpenGraph."],"url":"http://arxiv.org/abs/2403.09412v1","category":"cs.CV"}
{"created":"2024-03-14 14:02:10","title":"Near-Field Channel Modeling for Holographic MIMO Communications","abstract":"Empowered by the latest progress on innovative metamaterials/metasurfaces and advanced antenna technologies, holographic multiple-input multiple-output (H-MIMO) emerges as a promising technology to fulfill the extreme goals of the sixth-generation (6G) wireless networks. The antenna arrays utilized in H-MIMO comprise massive (possibly to extreme extent) numbers of antenna elements, densely spaced less than half-a-wavelength and integrated into a compact space, realizing an almost continuous aperture. Thanks to the expected low cost, size, weight, and power consumption, such apertures are expected to be largely fabricated for near-field communications. In addition, the physical features of H-MIMO enable manipulations directly on the electromagnetic (EM) wave domain and spatial multiplexing. To fully leverage this potential, near-field H-MIMO channel modeling, especially from the EM perspective, is of paramount significance. In this article, we overview near-field H-MIMO channel models elaborating on the various modeling categories and respective features, as well as their challenges and evaluation criteria. We also present EM-domain channel models that address the inherit computational and measurement complexities. Finally, the article is concluded with a set of future research directions on the topic.","sentences":["Empowered by the latest progress on innovative metamaterials/metasurfaces and advanced antenna technologies, holographic multiple-input multiple-output (H-MIMO) emerges as a promising technology to fulfill the extreme goals of the sixth-generation (6G) wireless networks.","The antenna arrays utilized in H-MIMO comprise massive (possibly to extreme extent) numbers of antenna elements, densely spaced less than half-a-wavelength and integrated into a compact space, realizing an almost continuous aperture.","Thanks to the expected low cost, size, weight, and power consumption, such apertures are expected to be largely fabricated for near-field communications.","In addition, the physical features of H-MIMO enable manipulations directly on the electromagnetic (EM) wave domain and spatial multiplexing.","To fully leverage this potential, near-field H-MIMO channel modeling, especially from the EM perspective, is of paramount significance.","In this article, we overview near-field H-MIMO channel models elaborating on the various modeling categories and respective features, as well as their challenges and evaluation criteria.","We also present EM-domain channel models that address the inherit computational and measurement complexities.","Finally, the article is concluded with a set of future research directions on the topic."],"url":"http://arxiv.org/abs/2403.09411v1","category":"eess.SP"}
{"created":"2024-03-14 14:02:01","title":"XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization","abstract":"Utilizing potent representations of the large vision-language models (VLMs) to accomplish various downstream tasks has attracted increasing attention. Within this research field, soft prompt learning has become a representative approach for efficiently adapting VLMs such as CLIP, to tasks like image classification. However, most existing prompt learning methods learn text tokens that are unexplainable, which cannot satisfy the stringent interpretability requirements of Explainable Artificial Intelligence (XAI) in high-stakes scenarios like healthcare. To address this issue, we propose a novel explainable prompt learning framework that leverages medical knowledge by aligning the semantics of images, learnable prompts, and clinical concept-driven prompts at multiple granularities. Moreover, our framework addresses the lack of valuable concept annotations by eliciting knowledge from large language models and offers both visual and textual explanations for the prompts. Extensive experiments and explainability analyses conducted on various datasets, with and without concept labels, demonstrate that our method simultaneously achieves superior diagnostic performance, flexibility, and interpretability, shedding light on the effectiveness of foundation models in facilitating XAI. The code will be made publically available.","sentences":["Utilizing potent representations of the large vision-language models (VLMs) to accomplish various downstream tasks has attracted increasing attention.","Within this research field, soft prompt learning has become a representative approach for efficiently adapting VLMs such as CLIP, to tasks like image classification.","However, most existing prompt learning methods learn text tokens that are unexplainable, which cannot satisfy the stringent interpretability requirements of Explainable Artificial Intelligence (XAI) in high-stakes scenarios like healthcare.","To address this issue, we propose a novel explainable prompt learning framework that leverages medical knowledge by aligning the semantics of images, learnable prompts, and clinical concept-driven prompts at multiple granularities.","Moreover, our framework addresses the lack of valuable concept annotations by eliciting knowledge from large language models and offers both visual and textual explanations for the prompts.","Extensive experiments and explainability analyses conducted on various datasets, with and without concept labels, demonstrate that our method simultaneously achieves superior diagnostic performance, flexibility, and interpretability, shedding light on the effectiveness of foundation models in facilitating XAI.","The code will be made publically available."],"url":"http://arxiv.org/abs/2403.09410v1","category":"cs.CV"}
{"created":"2024-03-14 14:01:26","title":"\"Like a Nesting Doll\": Analyzing Recursion Analogies Generated by CS Students using Large Language Models","abstract":"Grasping complex computing concepts often poses a challenge for students who struggle to anchor these new ideas to familiar experiences and understandings. To help with this, a good analogy can bridge the gap between unfamiliar concepts and familiar ones, providing an engaging way to aid understanding. However, creating effective educational analogies is difficult even for experienced instructors. We investigate to what extent large language models (LLMs), specifically ChatGPT, can provide access to personally relevant analogies on demand. Focusing on recursion, a challenging threshold concept, we conducted an investigation analyzing the analogies generated by more than 350 first-year computing students. They were provided with a code snippet and tasked to generate their own recursion-based analogies using ChatGPT, optionally including personally relevant topics in their prompts. We observed a great deal of diversity in the analogies produced with student-prescribed topics, in contrast to the otherwise generic analogies, highlighting the value of student creativity when working with LLMs. Not only did students enjoy the activity and report an improved understanding of recursion, but they described more easily remembering analogies that were personally and culturally relevant.","sentences":["Grasping complex computing concepts often poses a challenge for students who struggle to anchor these new ideas to familiar experiences and understandings.","To help with this, a good analogy can bridge the gap between unfamiliar concepts and familiar ones, providing an engaging way to aid understanding.","However, creating effective educational analogies is difficult even for experienced instructors.","We investigate to what extent large language models (LLMs), specifically ChatGPT, can provide access to personally relevant analogies on demand.","Focusing on recursion, a challenging threshold concept, we conducted an investigation analyzing the analogies generated by more than 350 first-year computing students.","They were provided with a code snippet and tasked to generate their own recursion-based analogies using ChatGPT, optionally including personally relevant topics in their prompts.","We observed a great deal of diversity in the analogies produced with student-prescribed topics, in contrast to the otherwise generic analogies, highlighting the value of student creativity when working with LLMs.","Not only did students enjoy the activity and report an improved understanding of recursion, but they described more easily remembering analogies that were personally and culturally relevant."],"url":"http://arxiv.org/abs/2403.09409v1","category":"cs.HC"}
{"created":"2024-03-14 13:59:04","title":"LM2D: Lyrics- and Music-Driven Dance Synthesis","abstract":"Dance typically involves professional choreography with complex movements that follow a musical rhythm and can also be influenced by lyrical content. The integration of lyrics in addition to the auditory dimension, enriches the foundational tone and makes motion generation more amenable to its semantic meanings. However, existing dance synthesis methods tend to model motions only conditioned on audio signals. In this work, we make two contributions to bridge this gap. First, we propose LM2D, a novel probabilistic architecture that incorporates a multimodal diffusion model with consistency distillation, designed to create dance conditioned on both music and lyrics in one diffusion generation step. Second, we introduce the first 3D dance-motion dataset that encompasses both music and lyrics, obtained with pose estimation technologies. We evaluate our model against music-only baseline models with objective metrics and human evaluations, including dancers and choreographers. The results demonstrate LM2D is able to produce realistic and diverse dance matching both lyrics and music. A video summary can be accessed at: https://youtu.be/4XCgvYookvA.","sentences":["Dance typically involves professional choreography with complex movements that follow a musical rhythm and can also be influenced by lyrical content.","The integration of lyrics in addition to the auditory dimension, enriches the foundational tone and makes motion generation more amenable to its semantic meanings.","However, existing dance synthesis methods tend to model motions only conditioned on audio signals.","In this work, we make two contributions to bridge this gap.","First, we propose LM2D, a novel probabilistic architecture that incorporates a multimodal diffusion model with consistency distillation, designed to create dance conditioned on both music and lyrics in one diffusion generation step.","Second, we introduce the first 3D dance-motion dataset that encompasses both music and lyrics, obtained with pose estimation technologies.","We evaluate our model against music-only baseline models with objective metrics and human evaluations, including dancers and choreographers.","The results demonstrate LM2D is able to produce realistic and diverse dance matching both lyrics and music.","A video summary can be accessed at: https://youtu.be/4XCgvYookvA."],"url":"http://arxiv.org/abs/2403.09407v1","category":"cs.SD"}
{"created":"2024-03-14 13:55:34","title":"Which Artificial Intelligences Do People Care About Most? A Conjoint Experiment on Moral Consideration","abstract":"Many studies have identified particular features of artificial intelligences (AI), such as their autonomy and emotion expression, that affect the extent to which they are treated as subjects of moral consideration. However, there has not yet been a comparison of the relative importance of features as is necessary to design and understand increasingly capable, multi-faceted AI systems. We conducted an online conjoint experiment in which 1,163 participants evaluated descriptions of AIs that varied on these features. All 11 features increased how morally wrong participants considered it to harm the AIs. The largest effects were from human-like physical bodies and prosociality (i.e., emotion expression, emotion recognition, cooperation, and moral judgment). For human-computer interaction designers, the importance of prosociality suggests that, because AIs are often seen as threatening, the highest levels of moral consideration may only be granted if the AI has positive intentions.","sentences":["Many studies have identified particular features of artificial intelligences (AI), such as their autonomy and emotion expression, that affect the extent to which they are treated as subjects of moral consideration.","However, there has not yet been a comparison of the relative importance of features as is necessary to design and understand increasingly capable, multi-faceted AI systems.","We conducted an online conjoint experiment in which 1,163 participants evaluated descriptions of AIs that varied on these features.","All 11 features increased how morally wrong participants considered it to harm the AIs.","The largest effects were from human-like physical bodies and prosociality (i.e., emotion expression, emotion recognition, cooperation, and moral judgment).","For human-computer interaction designers, the importance of prosociality suggests that, because AIs are often seen as threatening, the highest levels of moral consideration may only be granted if the AI has positive intentions."],"url":"http://arxiv.org/abs/2403.09405v1","category":"cs.HC"}
{"created":"2024-03-14 13:53:05","title":"Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption","abstract":"We propose a novel program of heuristic reasoning within artificial intelligence (AI) systems. Through a series of innovative experiments, including variations of the classic Linda problem and a novel application of the Beauty Contest game, we uncover trade-offs between accuracy maximization and effort reduction that shape the conditions under which AIs transition between exhaustive logical processing and the use of cognitive shortcuts (heuristics). We distinguish between the 'instrumental' use of heuristics to match resources with objectives, and 'mimetic absorption,' whereby heuristics are learned from humans, and manifest randomly and universally. We provide evidence that AI, despite lacking intrinsic goals or self-awareness, manifests an adaptive balancing of precision and efficiency, consistent with principles of resource-rational human cognition as explicated in classical theories of bounded rationality and dual-process theory.","sentences":["We propose a novel program of heuristic reasoning within artificial intelligence (AI) systems.","Through a series of innovative experiments, including variations of the classic Linda problem and a novel application of the Beauty Contest game, we uncover trade-offs between accuracy maximization and effort reduction that shape the conditions under which AIs transition between exhaustive logical processing and the use of cognitive shortcuts (heuristics).","We distinguish between the 'instrumental' use of heuristics to match resources with objectives, and 'mimetic absorption,' whereby heuristics are learned from humans, and manifest randomly and universally.","We provide evidence that AI, despite lacking intrinsic goals or self-awareness, manifests an adaptive balancing of precision and efficiency, consistent with principles of resource-rational human cognition as explicated in classical theories of bounded rationality and dual-process theory."],"url":"http://arxiv.org/abs/2403.09404v1","category":"cs.AI"}
{"created":"2024-03-14 13:52:03","title":"Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning","abstract":"Identifying highlight moments of raw video materials is crucial for improving the efficiency of editing videos that are pervasive on internet platforms. However, the extensive work of manually labeling footage has created obstacles to applying supervised methods to videos of unseen categories. The absence of an audio modality that contains valuable cues for highlight detection in many videos also makes it difficult to use multimodal strategies. In this paper, we propose a novel model with cross-modal perception for unsupervised highlight detection. The proposed model learns representations with visual-audio level semantics from image-audio pair data via a self-reconstruction task. To achieve unsupervised highlight detection, we investigate the latent representations of the network and propose the representation activation sequence learning (RASL) module with k-point contrastive learning to learn significant representation activations. To connect the visual modality with the audio modality, we use the symmetric contrastive learning (SCL) module to learn the paired visual and audio representations. Furthermore, an auxiliary task of masked feature vector sequence (FVS) reconstruction is simultaneously conducted during pretraining for representation enhancement. During inference, the cross-modal pretrained model can generate representations with paired visual-audio semantics given only the visual modality. The RASL module is used to output the highlight scores. The experimental results show that the proposed framework achieves superior performance compared to other state-of-the-art approaches.","sentences":["Identifying highlight moments of raw video materials is crucial for improving the efficiency of editing videos that are pervasive on internet platforms.","However, the extensive work of manually labeling footage has created obstacles to applying supervised methods to videos of unseen categories.","The absence of an audio modality that contains valuable cues for highlight detection in many videos also makes it difficult to use multimodal strategies.","In this paper, we propose a novel model with cross-modal perception for unsupervised highlight detection.","The proposed model learns representations with visual-audio level semantics from image-audio pair data via a self-reconstruction task.","To achieve unsupervised highlight detection, we investigate the latent representations of the network and propose the representation activation sequence learning (RASL) module with k-point contrastive learning to learn significant representation activations.","To connect the visual modality with the audio modality, we use the symmetric contrastive learning (SCL) module to learn the paired visual and audio representations.","Furthermore, an auxiliary task of masked feature vector sequence (FVS) reconstruction is simultaneously conducted during pretraining for representation enhancement.","During inference, the cross-modal pretrained model can generate representations with paired visual-audio semantics given only the visual modality.","The RASL module is used to output the highlight scores.","The experimental results show that the proposed framework achieves superior performance compared to other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.09401v1","category":"cs.CV"}
{"created":"2024-03-14 13:50:44","title":"ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalization","abstract":"Medical data often exhibits distribution shifts, which cause test-time performance degradation for deep learning models trained using standard supervised learning pipelines. This challenge is addressed in the field of Domain Generalization (DG) with the sub-field of Single Domain Generalization (SDG) being specifically interesting due to the privacy- or logistics-related issues often associated with medical data. Existing disentanglement-based SDG methods heavily rely on structural information embedded in segmentation masks, however classification labels do not provide such dense information. This work introduces a novel SDG method aimed at medical image classification that leverages channel-wise contrastive disentanglement. It is further enhanced with reconstruction-based style regularization to ensure extraction of distinct style and structure feature representations. We evaluate our method on the complex task of multicenter histopathology image classification, comparing it against state-of-the-art (SOTA) SDG baselines. Results demonstrate that our method surpasses the SOTA by a margin of 1% in average accuracy while also showing more stable performance. This study highlights the importance and challenges of exploring SDG frameworks in the context of the classification task. The code is publicly available at https://github.com/BioMedIA-MBZUAI/ConDiSR","sentences":["Medical data often exhibits distribution shifts, which cause test-time performance degradation for deep learning models trained using standard supervised learning pipelines.","This challenge is addressed in the field of Domain Generalization (DG) with the sub-field of Single Domain Generalization (SDG) being specifically interesting due to the privacy- or logistics-related issues often associated with medical data.","Existing disentanglement-based SDG methods heavily rely on structural information embedded in segmentation masks, however classification labels do not provide such dense information.","This work introduces a novel SDG method aimed at medical image classification that leverages channel-wise contrastive disentanglement.","It is further enhanced with reconstruction-based style regularization to ensure extraction of distinct style and structure feature representations.","We evaluate our method on the complex task of multicenter histopathology image classification, comparing it against state-of-the-art (SOTA) SDG baselines.","Results demonstrate that our method surpasses the SOTA by a margin of 1% in average accuracy while also showing more stable performance.","This study highlights the importance and challenges of exploring SDG frameworks in the context of the classification task.","The code is publicly available at https://github.com/BioMedIA-MBZUAI/ConDiSR"],"url":"http://arxiv.org/abs/2403.09400v1","category":"cs.CV"}
{"created":"2024-03-14 13:45:09","title":"Event-based Asynchronous HDR Imaging by Temporal Incident Light Modulation","abstract":"Dynamic Range (DR) is a pivotal characteristic of imaging systems. Current frame-based cameras struggle to achieve high dynamic range imaging due to the conflict between globally uniform exposure and spatially variant scene illumination. In this paper, we propose AsynHDR, a Pixel-Asynchronous HDR imaging system, based on key insights into the challenges in HDR imaging and the unique event-generating mechanism of Dynamic Vision Sensors (DVS). Our proposed AsynHDR system integrates the DVS with a set of LCD panels. The LCD panels modulate the irradiance incident upon the DVS by altering their transparency, thereby triggering the pixel-independent event streams. The HDR image is subsequently decoded from the event streams through our temporal-weighted algorithm. Experiments under standard test platform and several challenging scenes have verified the feasibility of the system in HDR imaging task.","sentences":["Dynamic Range (DR) is a pivotal characteristic of imaging systems.","Current frame-based cameras struggle to achieve high dynamic range imaging due to the conflict between globally uniform exposure and spatially variant scene illumination.","In this paper, we propose AsynHDR, a Pixel-Asynchronous HDR imaging system, based on key insights into the challenges in HDR imaging and the unique event-generating mechanism of Dynamic Vision Sensors (DVS).","Our proposed AsynHDR system integrates the DVS with a set of LCD panels.","The LCD panels modulate the irradiance incident upon the DVS by altering their transparency, thereby triggering the pixel-independent event streams.","The HDR image is subsequently decoded from the event streams through our temporal-weighted algorithm.","Experiments under standard test platform and several challenging scenes have verified the feasibility of the system in HDR imaging task."],"url":"http://arxiv.org/abs/2403.09392v1","category":"eess.IV"}
{"created":"2024-03-14 13:45:01","title":"A hollow-core fiber based stand-alone multimodal (2-photon, 3-photon, SHG, THG) nonlinear flexible imaging endoscope","abstract":"Multimodal nonlinear endoscopes have been a topic of intense research over the past two decades, enabling sub-cellular and label-free imaging in areas not reachable with table-top microscopes. They are sophisticated systems that can be implemented on an optical table in a lab environment, but they cannot be easily moved within or out of the lab. We present here a multimodal and flexible nonlinear endoscope system able to perform two photon excited fluorescence and second harmonic generation imaging with a stand-alone and moveable kart integrating a compact ultrashort laser source. In addition, the system can perform three photon excited fluorescence and third harmonic generation thanks to a delivery optical fiber used to deliver ultrashort pulses from massive and not movable laser systems into the stand-alone kart. The endoscopic fiber probes and delivery optical fibers are based on functionalized negative curvature hollow core fibers. The endoscope distal head has a diameter <2.2mm and can perform nonlinear imaging at max 10 frames/s over a field of view up to 600 $\\mu$m with a ~1 $\\mu$m spatial resolution.","sentences":["Multimodal nonlinear endoscopes have been a topic of intense research over the past two decades, enabling sub-cellular and label-free imaging in areas not reachable with table-top microscopes.","They are sophisticated systems that can be implemented on an optical table in a lab environment, but they cannot be easily moved within or out of the lab.","We present here a multimodal and flexible nonlinear endoscope system able to perform two photon excited fluorescence and second harmonic generation imaging with a stand-alone and moveable kart integrating a compact ultrashort laser source.","In addition, the system can perform three photon excited fluorescence and third harmonic generation thanks to a delivery optical fiber used to deliver ultrashort pulses from massive and not movable laser systems into the stand-alone kart.","The endoscopic fiber probes and delivery optical fibers are based on functionalized negative curvature hollow core fibers.","The endoscope distal head has a diameter <2.2mm and can perform nonlinear imaging at max 10 frames/s over a field of view up to 600 $\\mu$m with a ~1 $\\mu$m spatial resolution."],"url":"http://arxiv.org/abs/2403.09391v1","category":"physics.optics"}
{"created":"2024-03-14 13:39:18","title":"Imaging a semi-classical horizonless compact object with strong redshift","abstract":"The recent advancements in black hole imaging have opened a new era of probing horizon-scale physics with electromagnetic radiation. However, a feature of the observed images, a bright ring encircling a relatively dark region, has not sufficiently proved the existence of event horizons. It thus requires extreme care when studying the possibility of using such image features to examine quantum effects that may change the classical picture of black holes slightly or drastically. In this work, we investigate the image of a horizonless compact object, whose interior metric satisfies the 4D semi-classical Einstein equation non-perturbatively for the Planck constant, and whose entropy agrees with the Bekenstein-Hawking formula. Although the absence of an event horizon allows light rays to pass through the dense interior, the extremely strong redshift significantly darkens the image, making it almost identical to the classical black-hole image. In particular, if there is light emission a bit inside the surface of the object, the intensity around the inner shadow is slightly enhanced, which could be a future observable prediction to characterize the object. We also find through a phenomenological parameter that the image is further darkened due to interactions inside. Thus, the image is consistent with current observations, and the object could be a candidate for black holes in quantum theory.","sentences":["The recent advancements in black hole imaging have opened a new era of probing horizon-scale physics with electromagnetic radiation.","However, a feature of the observed images, a bright ring encircling a relatively dark region, has not sufficiently proved the existence of event horizons.","It thus requires extreme care when studying the possibility of using such image features to examine quantum effects that may change the classical picture of black holes slightly or drastically.","In this work, we investigate the image of a horizonless compact object, whose interior metric satisfies the 4D semi-classical Einstein equation non-perturbatively for the Planck constant, and whose entropy agrees with the Bekenstein-Hawking formula.","Although the absence of an event horizon allows light rays to pass through the dense interior, the extremely strong redshift significantly darkens the image, making it almost identical to the classical black-hole image.","In particular, if there is light emission a bit inside the surface of the object, the intensity around the inner shadow is slightly enhanced, which could be a future observable prediction to characterize the object.","We also find through a phenomenological parameter that the image is further darkened due to interactions inside.","Thus, the image is consistent with current observations, and the object could be a candidate for black holes in quantum theory."],"url":"http://arxiv.org/abs/2403.09388v1","category":"gr-qc"}
{"created":"2024-03-14 13:33:00","title":"Optimized generation of entanglement based on the f-STIRAP technique","abstract":"We consider generating maximally entangled states (Bell states) between two qubits coupled to a common bosonic mode, based on f-STIRAP. Utilizing the systematic approach developed in New J. Phys. 19 093016 (2017), we quantify the effects of non-adiabatic leakage and system dissipation on the entanglement generation, and optimize the entanglement by balancing non-adiabatic leakage and system dissipation. We find the analytical expressions of the optimal coupling profile, the operation time, and the maximal entanglement. Our findings have broad applications in quantum state engineering, especially in solid-state devices where dissipative effects cannot be neglected.","sentences":["We consider generating maximally entangled states (Bell states) between two qubits coupled to a common bosonic mode, based on f-STIRAP.","Utilizing the systematic approach developed in New J. Phys. 19 093016 (2017), we quantify the effects of non-adiabatic leakage and system dissipation on the entanglement generation, and optimize the entanglement by balancing non-adiabatic leakage and system dissipation.","We find the analytical expressions of the optimal coupling profile, the operation time, and the maximal entanglement.","Our findings have broad applications in quantum state engineering, especially in solid-state devices where dissipative effects cannot be neglected."],"url":"http://arxiv.org/abs/2403.09381v1","category":"quant-ph"}
{"created":"2024-03-14 13:29:23","title":"The Fekete problem in segmental polynomial interpolation","abstract":"In this article, we study the Fekete problem in segmental and combined nodal-segmental univariate polynomial interpolation by investigating sets of segments, or segments combined with nodes, such that the Vandermonde determinant for the respective polynomial interpolation problem is maximized. For particular families of segments, we will be able to find explicit solutions of the corresponding maximization problem. The quality of the Fekete segments depends hereby strongly on the utilized normalization of the segmental information in the Vandermonde matrix. To measure the quality of the Fekete segments in interpolation, we analyse the asymptotic behaviour of the generalized Lebesgue constant linked to the interpolation problem. For particular sets of Fekete segments we will get, similar to the nodal case, a favourable logarithmic growth of this constant.","sentences":["In this article, we study the Fekete problem in segmental and combined nodal-segmental univariate polynomial interpolation by investigating sets of segments, or segments combined with nodes, such that the Vandermonde determinant for the respective polynomial interpolation problem is maximized.","For particular families of segments, we will be able to find explicit solutions of the corresponding maximization problem.","The quality of the Fekete segments depends hereby strongly on the utilized normalization of the segmental information in the Vandermonde matrix.","To measure the quality of the Fekete segments in interpolation, we analyse the asymptotic behaviour of the generalized Lebesgue constant linked to the interpolation problem.","For particular sets of Fekete segments we will get, similar to the nodal case, a favourable logarithmic growth of this constant."],"url":"http://arxiv.org/abs/2403.09378v1","category":"math.NA"}
{"created":"2024-03-14 13:29:23","title":"Principal 2-bundles and quotient 2-stacks","abstract":"We generalize principal bundles and quotient stacks to the two-categorical context of bisites. We introduce a notion of principal 2-bundle that makes sense for a 2-category with finite flexible limits, endowed with a bitopology. We then use principal 2-bundles to explicitly construct quotient-pre-2-stacks, which are the analogues of quotient stacks one dimension higher. In order to perform this construction, we prove that principal 2-bundles are closed under iso-comma objects and we restrict ourselves to (2,1)-categories. Finally, we prove that, if the bisite is subcanonical and the underlying (2,1)-category satisfies some mild conditions, quotient pre-2-stacks are 2-stacks.","sentences":["We generalize principal bundles and quotient stacks to the two-categorical context of bisites.","We introduce a notion of principal 2-bundle that makes sense for a 2-category with finite flexible limits, endowed with a bitopology.","We then use principal 2-bundles to explicitly construct quotient-pre-2-stacks, which are the analogues of quotient stacks one dimension higher.","In order to perform this construction, we prove that principal 2-bundles are closed under iso-comma objects and we restrict ourselves to (2,1)-categories.","Finally, we prove that, if the bisite is subcanonical and the underlying (2,1)-category satisfies some mild conditions, quotient pre-2-stacks are 2-stacks."],"url":"http://arxiv.org/abs/2403.09379v1","category":"math.CT"}
{"created":"2024-03-14 13:20:35","title":"Gravitational Waves in Chern-Simons-Gauss-Bonnet Gravity","abstract":"It is known that the four-dimensional effective field theory arising from heterotic string theory is general relativity with both a Chern-Simons and Gauss-Bonnet term. We study the propagation of gravitational waves in this combination of Chern-Simons and Gauss-Bonnet gravity, both of which have an associated scalar field, the axion and the dilaton respectively, that are kinetically coupled. We review how the combination of dynamical Chern-Simons and Gauss-Bonnet gravities can arise from string theory as corrections to general relativity and show how the gravitational wave waveform is modified in such a theory. We compare our results to a novel framework recently introduced for parametrizing the parity-violating sector (Chern-Simons), and use that to guide our construction of a similar parametrization for the parity-conserving (Gauss-Bonnet) sector. In general, we find that the contributions from the parity-violating and parity-conserving sectors are similar. Moreover, the kinetic coupling between the axion and dilaton introduces an extra contribution to the parity-violating sector of the gravitational waves. Using our parametrization, we are able to comment on initial constraints for the theory parameters, including the time variations of the axion and dilaton.","sentences":["It is known that the four-dimensional effective field theory arising from heterotic string theory is general relativity with both a Chern-Simons and Gauss-Bonnet term.","We study the propagation of gravitational waves in this combination of Chern-Simons and Gauss-Bonnet gravity, both of which have an associated scalar field, the axion and the dilaton respectively, that are kinetically coupled.","We review how the combination of dynamical Chern-Simons and Gauss-Bonnet gravities can arise from string theory as corrections to general relativity and show how the gravitational wave waveform is modified in such a theory.","We compare our results to a novel framework recently introduced for parametrizing the parity-violating sector (Chern-Simons), and use that to guide our construction of a similar parametrization for the parity-conserving (Gauss-Bonnet) sector.","In general, we find that the contributions from the parity-violating and parity-conserving sectors are similar.","Moreover, the kinetic coupling between the axion and dilaton introduces an extra contribution to the parity-violating sector of the gravitational waves.","Using our parametrization, we are able to comment on initial constraints for the theory parameters, including the time variations of the axion and dilaton."],"url":"http://arxiv.org/abs/2403.09373v1","category":"gr-qc"}
{"created":"2024-03-14 13:17:37","title":"Noise-aware neural network for stochastic dynamics simulation","abstract":"In the presence of system-environment coupling, classical complex systems undergo stochastic dynamics, where rich phenomena can emerge at large spatio-temporal scales. To investigate these phenomena, numerical approaches for simulating stochastic dynamics are indispensable and can be computationally expensive. In light of the recent fast development in machine learning techniques, here, we establish a generic machine learning approach to simulate the stochastic dynamics, dubbed the noise-aware neural network (NANN). One key feature of this approach is its ability to generate the long-time stochastic dynamics of complex large-scale systems by just training NANN with the one-step dynamics of smaller-scale systems, thus reducing the computational cost. Furthermore, this NANN based approach is quite generic. Case-by-case special design of the architecture of NANN is not necessary when it is employed to investigate different stochastic complex systems. Using the noisy Kuramoto model and the Vicsek model as concrete examples, we demonstrate its capability in simulating stochastic dynamics. We believe that this novel machine learning approach can be a useful tool in investigating the large spatio-temporal scaling behavior of complex systems subjected to the influences of the environmental noise.","sentences":["In the presence of system-environment coupling, classical complex systems undergo stochastic dynamics, where rich phenomena can emerge at large spatio-temporal scales.","To investigate these phenomena, numerical approaches for simulating stochastic dynamics are indispensable and can be computationally expensive.","In light of the recent fast development in machine learning techniques, here, we establish a generic machine learning approach to simulate the stochastic dynamics, dubbed the noise-aware neural network (NANN).","One key feature of this approach is its ability to generate the long-time stochastic dynamics of complex large-scale systems by just training NANN with the one-step dynamics of smaller-scale systems, thus reducing the computational cost.","Furthermore, this NANN based approach is quite generic.","Case-by-case special design of the architecture of NANN is not necessary when it is employed to investigate different stochastic complex systems.","Using the noisy Kuramoto model and the Vicsek model as concrete examples, we demonstrate its capability in simulating stochastic dynamics.","We believe that this novel machine learning approach can be a useful tool in investigating the large spatio-temporal scaling behavior of complex systems subjected to the influences of the environmental noise."],"url":"http://arxiv.org/abs/2403.09370v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-14 13:16:47","title":"PreConfig: A Pretrained Model for Automating Network Configuration","abstract":"Manual network configuration automation (NCA) tools face significant challenges in versatility and flexibility due to their reliance on extensive domain expertise and manual design, limiting their adaptability to diverse scenarios and complex application needs. This paper introduces PreConfig, an innovative NCA tool that leverages a pretrained language model for automating network configuration tasks. PreConfig is designed to address the complexity and variety of NCA tasks by framing them as text-to-text transformation problems, thus unifying the tasks of configuration generation, translation, and analysis under a single, versatile model. Our approach overcomes existing tools' limitations by utilizing advances in natural language processing to automatically comprehend and generate network configurations without extensive manual re-engineering. We confront the challenges of integrating domain-specific knowledge into pretrained models and the scarcity of supervision data in the network configuration field. Our solution involves constructing a specialized corpus and further pretraining on network configuration data, coupled with a novel data mining technique for generating task supervision data. The proposed model demonstrates robustness in configuration generation, translation, and analysis, outperforming conventional tools in handling complex networking environments. The experimental results validate the effectiveness of PreConfig, establishing a new direction for automating network configuration tasks with pretrained language models.","sentences":["Manual network configuration automation (NCA) tools face significant challenges in versatility and flexibility due to their reliance on extensive domain expertise and manual design, limiting their adaptability to diverse scenarios and complex application needs.","This paper introduces PreConfig, an innovative NCA tool that leverages a pretrained language model for automating network configuration tasks.","PreConfig is designed to address the complexity and variety of NCA tasks by framing them as text-to-text transformation problems, thus unifying the tasks of configuration generation, translation, and analysis under a single, versatile model.","Our approach overcomes existing tools' limitations by utilizing advances in natural language processing to automatically comprehend and generate network configurations without extensive manual re-engineering.","We confront the challenges of integrating domain-specific knowledge into pretrained models and the scarcity of supervision data in the network configuration field.","Our solution involves constructing a specialized corpus and further pretraining on network configuration data, coupled with a novel data mining technique for generating task supervision data.","The proposed model demonstrates robustness in configuration generation, translation, and analysis, outperforming conventional tools in handling complex networking environments.","The experimental results validate the effectiveness of PreConfig, establishing a new direction for automating network configuration tasks with pretrained language models."],"url":"http://arxiv.org/abs/2403.09369v1","category":"cs.NI"}
{"created":"2024-03-14 13:16:00","title":"Emergence of entanglement and breakdown of causality in the quantum realm","abstract":"Entanglement is the most striking but also most weird property in quantum mechanics. It has been confirmed by many experiments over decades through the criterion of violating Bell's inequality. However, a more fundamental problem arisen from EPR paradox is still not fully understood, that is, why quantum world emerges entanglement that classical physics does not. In this paper, we investigate the quantum dynamics of two photonic modes (or any two bosonic modes) coupled to each other through a beam splitting. Such a coupling fails to produce two-mode entanglement. We also start with a decoupled two-mode initial pure state, namely, no entanglement and no statistic feature to begin with. By solving the quantum equation of motion exactly without relying on the probabilistic interpretation, we find that when the initial wave function of one mode is different from a wave packet obeying minimum Heisenberg uncertainty (which corresponds to a well-defined classically particle), the causality in the time-evolution of another mode is broken down explicitly. It also leads to the emergence of quantum entanglement between the two modes. The lack of causality is the nature of statistics. The Bell's inequality only excludes the possible existence of local hidden variables for the probabilistic interpretation of quantum mechanics. The internally causality breaking in the dynamical evolution of subsystems in isolated systems may answer the question how quantum dynamics generate naturally the probabilistic phenomena, even though the dynamical evolution of the whole system is completely described by the deterministic Schr\\\"{o}dinger equation.","sentences":["Entanglement is the most striking but also most weird property in quantum mechanics.","It has been confirmed by many experiments over decades through the criterion of violating Bell's inequality.","However, a more fundamental problem arisen from EPR paradox is still not fully understood, that is, why quantum world emerges entanglement that classical physics does not.","In this paper, we investigate the quantum dynamics of two photonic modes (or any two bosonic modes) coupled to each other through a beam splitting.","Such a coupling fails to produce two-mode entanglement.","We also start with a decoupled two-mode initial pure state, namely, no entanglement and no statistic feature to begin with.","By solving the quantum equation of motion exactly without relying on the probabilistic interpretation, we find that when the initial wave function of one mode is different from a wave packet obeying minimum Heisenberg uncertainty (which corresponds to a well-defined classically particle), the causality in the time-evolution of another mode is broken down explicitly.","It also leads to the emergence of quantum entanglement between the two modes.","The lack of causality is the nature of statistics.","The Bell's inequality only excludes the possible existence of local hidden variables for the probabilistic interpretation of quantum mechanics.","The internally causality breaking in the dynamical evolution of subsystems in isolated systems may answer the question how quantum dynamics generate naturally the probabilistic phenomena, even though the dynamical evolution of the whole system is completely described by the deterministic Schr\\\"{o}dinger equation."],"url":"http://arxiv.org/abs/2403.09368v1","category":"quant-ph"}
{"created":"2024-03-14 13:14:52","title":"Delay Dispersion in IRS-assisted FSO Links","abstract":"The line-of-sight (LOS) requirement of free-space optical (FSO) systems can be relaxed by employing optical intelligent reflecting surfaces (IRSs). In this paper, we model the impact of the IRS-induced delay dispersion and derive the channel impulse response (CIR) of IRS-assisted FSO links. The proposed model takes into account the characteristics of the incident and reflected beams' wavefronts, the position of transmitter and receiver, the size of the IRS, and the incident beamwidth on the IRS. Our simulation results reveal that a maximum effective delay spread of 0.7 ns is expected for a square IRS with area 1 $\\mathrm{m}^2$, which induces inter-symbol interference for bit rates larger than 10 Gbps. We show that the IRS-induced delay dispersion can be mitigated via equalization at the receiver.","sentences":["The line-of-sight (LOS) requirement of free-space optical (FSO) systems can be relaxed by employing optical intelligent reflecting surfaces (IRSs).","In this paper, we model the impact of the IRS-induced delay dispersion and derive the channel impulse response (CIR) of IRS-assisted FSO links.","The proposed model takes into account the characteristics of the incident and reflected beams' wavefronts, the position of transmitter and receiver, the size of the IRS, and the incident beamwidth on the IRS.","Our simulation results reveal that a maximum effective delay spread of 0.7 ns is expected for a square IRS with area 1 $\\mathrm{m}^2$, which induces inter-symbol interference for bit rates larger than 10 Gbps.","We show that the IRS-induced delay dispersion can be mitigated via equalization at the receiver."],"url":"http://arxiv.org/abs/2403.09365v1","category":"eess.SP"}
{"created":"2024-03-14 13:13:47","title":"Spinfoam Models for Quantum Gravity: Overview","abstract":"In the quest of a physical theory of quantum gravity, spin foam models, or in short spinfoams, propose a well-defined path integral summing over quantized discrete space-time geometries. At the crossroad of topological quantum field theory, dynamical triangulations, Regge calculus, and loop quantum gravity, this framework provides a non-perturbative and background independent quantization of general relativity. It defines transition amplitudes between quantum states of geometry, and gives a precise picture of the Planck scale geometry with quantized areas and volumes. Gravity in three space-time dimensions is exactly quantized in terms of the Ponzano-Regge state-sum and Turaev-Viro topological invariants. In four space-time dimensions, gravity is formulated as a topological theory, of the BF type, with extra constraints, and hence quantized as a topological state-sum filled with defects. This leads to the Engle-Pereira-Rovelli-Livine (EPRL) spinfoam model, that can be used for explicit quantum gravity computations, for example for resolving the Big Bang singularity by a bounce or in black-to-white hole transition probability amplitudes.","sentences":["In the quest of a physical theory of quantum gravity, spin foam models, or in short spinfoams, propose a well-defined path integral summing over quantized discrete space-time geometries.","At the crossroad of topological quantum field theory, dynamical triangulations, Regge calculus, and loop quantum gravity, this framework provides a non-perturbative and background independent quantization of general relativity.","It defines transition amplitudes between quantum states of geometry, and gives a precise picture of the Planck scale geometry with quantized areas and volumes.","Gravity in three space-time dimensions is exactly quantized in terms of the Ponzano-Regge state-sum and Turaev-Viro topological invariants.","In four space-time dimensions, gravity is formulated as a topological theory, of the BF type, with extra constraints, and hence quantized as a topological state-sum filled with defects.","This leads to the Engle-Pereira-Rovelli-Livine (EPRL) spinfoam model, that can be used for explicit quantum gravity computations, for example for resolving the Big Bang singularity by a bounce or in black-to-white hole transition probability amplitudes."],"url":"http://arxiv.org/abs/2403.09364v1","category":"gr-qc"}
{"created":"2024-03-14 13:12:49","title":"Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without Real Data Exposure","abstract":"With increasing concerns over data privacy and model copyrights, especially in the context of collaborations between AI service providers and data owners, an innovative SG-ZSL paradigm is proposed in this work. SG-ZSL is designed to foster efficient collaboration without the need to exchange models or sensitive data. It consists of a teacher model, a student model and a generator that links both model entities. The teacher model serves as a sentinel on behalf of the data owner, replacing real data, to guide the student model at the AI service provider's end during training. Considering the disparity of knowledge space between the teacher and student, we introduce two variants of the teacher model: the omniscient and the quasi-omniscient teachers. Under these teachers' guidance, the student model seeks to match the teacher model's performance and explores domains that the teacher has not covered. To trade off between privacy and performance, we further introduce two distinct security-level training protocols: white-box and black-box, enhancing the paradigm's adaptability. Despite the inherent challenges of real data absence in the SG-ZSL paradigm, it consistently outperforms in ZSL and GZSL tasks, notably in the white-box protocol. Our comprehensive evaluation further attests to its robustness and efficiency across various setups, including stringent black-box training protocol.","sentences":["With increasing concerns over data privacy and model copyrights, especially in the context of collaborations between AI service providers and data owners, an innovative SG-ZSL paradigm is proposed in this work.","SG-ZSL is designed to foster efficient collaboration without the need to exchange models or sensitive data.","It consists of a teacher model, a student model and a generator that links both model entities.","The teacher model serves as a sentinel on behalf of the data owner, replacing real data, to guide the student model at the AI service provider's end during training.","Considering the disparity of knowledge space between the teacher and student, we introduce two variants of the teacher model: the omniscient and the quasi-omniscient teachers.","Under these teachers' guidance, the student model seeks to match the teacher model's performance and explores domains that the teacher has not covered.","To trade off between privacy and performance, we further introduce two distinct security-level training protocols: white-box and black-box, enhancing the paradigm's adaptability.","Despite the inherent challenges of real data absence in the SG-ZSL paradigm, it consistently outperforms in ZSL and GZSL tasks, notably in the white-box protocol.","Our comprehensive evaluation further attests to its robustness and efficiency across various setups, including stringent black-box training protocol."],"url":"http://arxiv.org/abs/2403.09363v1","category":"cs.CV"}
{"created":"2024-03-14 13:11:30","title":"A Multi-population Integrated Approach for Capacitated Location Routing","abstract":"The capacitated location-routing problem involves determining the depots from a set of candidate capacitated depot locations and finding the required routes from the selected depots to serve a set of customers whereas minimizing a cost function that includes the cost of opening the chosen depots, the fixed utilization cost per vehicle used, and the total cost (distance) of the routes. This paper presents a multi-population integrated framework in which a multi-depot edge assembly crossover generates promising offspring solutions from the perspective of both depot location and route edge assembly. The method includes an effective neighborhood-based local search, a feasibility-restoring procedure and a diversification-oriented mutation. Of particular interest is the multi-population scheme which organizes the population into multiple subpopulations based on depot configurations. Extensive experiments on 281 benchmark instances from the literature show that the algorithm performs remarkably well, by improving 101 best-known results (new upper bounds) and matching 84 best-known results. Additional experiments are presented to gain insight into the role of the key elements of the algorithm.","sentences":["The capacitated location-routing problem involves determining the depots from a set of candidate capacitated depot locations and finding the required routes from the selected depots to serve a set of customers whereas minimizing a cost function that includes the cost of opening the chosen depots, the fixed utilization cost per vehicle used, and the total cost (distance) of the routes.","This paper presents a multi-population integrated framework in which a multi-depot edge assembly crossover generates promising offspring solutions from the perspective of both depot location and route edge assembly.","The method includes an effective neighborhood-based local search, a feasibility-restoring procedure and a diversification-oriented mutation.","Of particular interest is the multi-population scheme which organizes the population into multiple subpopulations based on depot configurations.","Extensive experiments on 281 benchmark instances from the literature show that the algorithm performs remarkably well, by improving 101 best-known results (new upper bounds) and matching 84 best-known results.","Additional experiments are presented to gain insight into the role of the key elements of the algorithm."],"url":"http://arxiv.org/abs/2403.09361v1","category":"cs.AI"}
{"created":"2024-03-14 13:08:15","title":"Quality factor of dielectric optical resonators","abstract":"We show analytically that the quality ($Q$) factor of magnetic and electric Mie modes in a lossless dielectric spherical resonator with high refractive index ($n \\gg 1$) scales as $n^{2j+1}$ and $n^{2j+3}$ respectively, where $j$ denotes the multipolar order. We numerically validate these results and show that our high-$n$ analytical relation is accurate for the dipolar modes when $n>5$. For higher multipolar orders, the analytical relation becomes valid for increasingly lower $n$. We study the dependence of the $Q$ factor on absorption losses and determine a general functional form that describes the $Q$ factor for all Mie modes for any complex refractive index. Finally, we observe that this functional form predicts a multipolar-dependent singular value of optical gain which gives rise to a lasing condition with an infinite $Q$ factor.","sentences":["We show analytically that the quality ($Q$) factor of magnetic and electric Mie modes in a lossless dielectric spherical resonator with high refractive index ($n \\gg 1$) scales as $n^{2j+1}$ and $n^{2j+3}$ respectively, where $j$ denotes the multipolar order.","We numerically validate these results and show that our high-$n$ analytical relation is accurate for the dipolar modes when $n>5$. For higher multipolar orders, the analytical relation becomes valid for increasingly lower $n$. We study the dependence of the $Q$ factor on absorption losses and determine a general functional form that describes the $Q$ factor for all Mie modes for any complex refractive index.","Finally, we observe that this functional form predicts a multipolar-dependent singular value of optical gain which gives rise to a lasing condition with an infinite $Q$ factor."],"url":"http://arxiv.org/abs/2403.09360v1","category":"physics.optics"}
{"created":"2024-03-14 13:05:43","title":"D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection","abstract":"Domain adaptation for object detection typically entails transferring knowledge from one visible domain to another visible domain. However, there are limited studies on adapting from the visible to the thermal domain, because the domain gap between the visible and thermal domains is much larger than expected, and traditional domain adaptation can not successfully facilitate learning in this situation. To overcome this challenge, we propose a Distinctive Dual-Domain Teacher (D3T) framework that employs distinct training paradigms for each domain. Specifically, we segregate the source and target training sets for building dual-teachers and successively deploy exponential moving average to the student model to individual teachers of each domain. The framework further incorporates a zigzag learning method between dual teachers, facilitating a gradual transition from the visible to thermal domains during training. We validate the superiority of our method through newly designed experimental protocols with well-known thermal datasets, i.e., FLIR and KAIST. Source code is available at https://github.com/EdwardDo69/D3T .","sentences":["Domain adaptation for object detection typically entails transferring knowledge from one visible domain to another visible domain.","However, there are limited studies on adapting from the visible to the thermal domain, because the domain gap between the visible and thermal domains is much larger than expected, and traditional domain adaptation can not successfully facilitate learning in this situation.","To overcome this challenge, we propose a Distinctive Dual-Domain Teacher (D3T) framework that employs distinct training paradigms for each domain.","Specifically, we segregate the source and target training sets for building dual-teachers and successively deploy exponential moving average to the student model to individual teachers of each domain.","The framework further incorporates a zigzag learning method between dual teachers, facilitating a gradual transition from the visible to thermal domains during training.","We validate the superiority of our method through newly designed experimental protocols with well-known thermal datasets, i.e., FLIR and KAIST.","Source code is available at https://github.com/EdwardDo69/D3T ."],"url":"http://arxiv.org/abs/2403.09359v1","category":"cs.CV"}
{"created":"2024-03-14 12:58:28","title":"Mitigating Data Consistency Induced Discrepancy in Cascaded Diffusion Models for Sparse-view CT Reconstruction","abstract":"Sparse-view Computed Tomography (CT) image reconstruction is a promising approach to reduce radiation exposure, but it inevitably leads to image degradation. Although diffusion model-based approaches are computationally expensive and suffer from the training-sampling discrepancy, they provide a potential solution to the problem. This study introduces a novel Cascaded Diffusion with Discrepancy Mitigation (CDDM) framework, including the low-quality image generation in latent space and the high-quality image generation in pixel space which contains data consistency and discrepancy mitigation in a one-step reconstruction process. The cascaded framework minimizes computational costs by moving some inference steps from pixel space to latent space. The discrepancy mitigation technique addresses the training-sampling gap induced by data consistency, ensuring the data distribution is close to the original manifold. A specialized Alternating Direction Method of Multipliers (ADMM) is employed to process image gradients in separate directions, offering a more targeted approach to regularization. Experimental results across two datasets demonstrate CDDM's superior performance in high-quality image generation with clearer boundaries compared to existing methods, highlighting the framework's computational efficiency.","sentences":["Sparse-view Computed Tomography (CT) image reconstruction is a promising approach to reduce radiation exposure, but it inevitably leads to image degradation.","Although diffusion model-based approaches are computationally expensive and suffer from the training-sampling discrepancy, they provide a potential solution to the problem.","This study introduces a novel Cascaded Diffusion with Discrepancy Mitigation (CDDM) framework, including the low-quality image generation in latent space and the high-quality image generation in pixel space which contains data consistency and discrepancy mitigation in a one-step reconstruction process.","The cascaded framework minimizes computational costs by moving some inference steps from pixel space to latent space.","The discrepancy mitigation technique addresses the training-sampling gap induced by data consistency, ensuring the data distribution is close to the original manifold.","A specialized Alternating Direction Method of Multipliers (ADMM) is employed to process image gradients in separate directions, offering a more targeted approach to regularization.","Experimental results across two datasets demonstrate CDDM's superior performance in high-quality image generation with clearer boundaries compared to existing methods, highlighting the framework's computational efficiency."],"url":"http://arxiv.org/abs/2403.09355v1","category":"eess.IV"}
{"created":"2024-03-14 12:58:15","title":"Magnetic distillation of intrinsic electric dipoles in mixed-stacking tetralayer graphene","abstract":"Polytypes of tetralayer graphene (TLG: Bernal, rhombohedral and mixed stacking) offer a choice of crystalline structures with different symmetries. Among those, mixed-stacking tetralayers lack inversion symmetry, which allows for intrinsic spontaneous out-of-plane electrical polarisation, inverted in the mirror-image pair, ABCB and ABAC stackings. Here, we compare the intrinsic polarisation with the symmetry-breaking effect of a substrate. We find that a potential induced by the substrate on the bottom layer of the TLG can generate out-of-plane electric dipole moments with different sizes in all four polytypes and, also, different for the ABCB and ABAC pair. This undermines a straightforward interpretation of experimentally measured Kelvin probe microscopy maps of tetralayer flakes in terms of their intrinsic polarisation. To overcome this difficulty, we analyse the influence of an external magnetic field on electrical polarisation of all four TLGs and find that Landau level quantisation highlights the intrinsic asymmetry of mixed stacking polytypes, making the difference between ABCB and ABAC polarisations almost independent of the substrate effect. We also notice a non-monotonic (hump-like) feature in the polarisation in a magnetic field range of 0.5-3 Tesla, caused by the interplay between time inversion symmetry breaking and spatial inversion symmetry breaking specific for those two polytypes.","sentences":["Polytypes of tetralayer graphene (TLG: Bernal, rhombohedral and mixed stacking) offer a choice of crystalline structures with different symmetries.","Among those, mixed-stacking tetralayers lack inversion symmetry, which allows for intrinsic spontaneous out-of-plane electrical polarisation, inverted in the mirror-image pair, ABCB and ABAC stackings.","Here, we compare the intrinsic polarisation with the symmetry-breaking effect of a substrate.","We find that a potential induced by the substrate on the bottom layer of the TLG can generate out-of-plane electric dipole moments with different sizes in all four polytypes and, also, different for the ABCB and ABAC pair.","This undermines a straightforward interpretation of experimentally measured Kelvin probe microscopy maps of tetralayer flakes in terms of their intrinsic polarisation.","To overcome this difficulty, we analyse the influence of an external magnetic field on electrical polarisation of all four TLGs and find that Landau level quantisation highlights the intrinsic asymmetry of mixed stacking polytypes, making the difference between ABCB and ABAC polarisations almost independent of the substrate effect.","We also notice a non-monotonic (hump-like) feature in the polarisation in a magnetic field range of 0.5-3 Tesla, caused by the interplay between time inversion symmetry breaking and spatial inversion symmetry breaking specific for those two polytypes."],"url":"http://arxiv.org/abs/2403.09354v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-14 12:58:03","title":"Intelligent Reflecting Surfaces vs. Full-Duplex Relays: A Comparison in the Air","abstract":"This letter aims to provide a fundamental analytical comparison for the two major types of relaying methods: intelligent reflecting surfaces and full-duplex relays, particularly focusing on unmanned aerial vehicle communication scenarios. Both amplify-and-forward and decode-and-forward relaying schemes are included in the comparison. In addition, optimal 3D UAV deployment and minimum transmit power under the quality of service constraint are derived. Our numerical results show that IRSs of medium size exhibit comparable performance to AF relays, meanwhile outperforming DF relays under extremely large surface size and high data rates.","sentences":["This letter aims to provide a fundamental analytical comparison for the two major types of relaying methods: intelligent reflecting surfaces and full-duplex relays, particularly focusing on unmanned aerial vehicle communication scenarios.","Both amplify-and-forward and decode-and-forward relaying schemes are included in the comparison.","In addition, optimal 3D UAV deployment and minimum transmit power under the quality of service constraint are derived.","Our numerical results show that IRSs of medium size exhibit comparable performance to AF relays, meanwhile outperforming DF relays under extremely large surface size and high data rates."],"url":"http://arxiv.org/abs/2403.09353v1","category":"eess.SP"}
{"created":"2024-03-14 12:51:07","title":"AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions","abstract":"Large Vision-Language Models (LVLMs) have shown significant progress in well responding to visual-instructions from users. However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks. Despite the critical importance of LVLMs' robustness against such threats, current research in this area remains limited. To bridge this gap, we introduce AVIBench, a framework designed to analyze the robustness of LVLMs when facing various adversarial visual-instructions (AVIs), including four types of image-based AVIs, ten types of text-based AVIs, and nine types of content bias AVIs (such as gender, violence, cultural, and racial biases, among others). We generate 260K AVIs encompassing five categories of multimodal capabilities (nine tasks) and content bias. We then conduct a comprehensive evaluation involving 14 open-source LVLMs to assess their performance. AVIBench also serves as a convenient tool for practitioners to evaluate the robustness of LVLMs against AVIs. Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and GPT-4V. This underscores the importance of enhancing the robustness, security, and fairness of LVLMs. The source code and benchmark will be made publicly available.","sentences":["Large Vision-Language Models (LVLMs) have shown significant progress in well responding to visual-instructions from users.","However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks.","Despite the critical importance of LVLMs' robustness against such threats, current research in this area remains limited.","To bridge this gap, we introduce AVIBench, a framework designed to analyze the robustness of LVLMs when facing various adversarial visual-instructions (AVIs), including four types of image-based AVIs, ten types of text-based AVIs, and nine types of content bias AVIs (such as gender, violence, cultural, and racial biases, among others).","We generate 260K AVIs encompassing five categories of multimodal capabilities (nine tasks) and content bias.","We then conduct a comprehensive evaluation involving 14 open-source LVLMs to assess their performance.","AVIBench also serves as a convenient tool for practitioners to evaluate the robustness of LVLMs against AVIs.","Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and GPT-4V. This underscores the importance of enhancing the robustness, security, and fairness of LVLMs.","The source code and benchmark will be made publicly available."],"url":"http://arxiv.org/abs/2403.09346v1","category":"cs.CV"}
{"created":"2024-03-14 12:49:29","title":"SketchINR: A First Look into Sketches as Implicit Neural Representations","abstract":"We propose SketchINR, to advance the representation of vector sketches with implicit neural models. A variable length vector sketch is compressed into a latent space of fixed dimension that implicitly encodes the underlying shape as a function of time and strokes. The learned function predicts the $xy$ point coordinates in a sketch at each time and stroke. Despite its simplicity, SketchINR outperforms existing representations at multiple tasks: (i) Encoding an entire sketch dataset into a fixed size latent vector, SketchINR gives $60\\times$ and $10\\times$ data compression over raster and vector sketches, respectively. (ii) SketchINR's auto-decoder provides a much higher-fidelity representation than other learned vector sketch representations, and is uniquely able to scale to complex vector sketches such as FS-COCO. (iii) SketchINR supports parallelisation that can decode/render $\\sim$$100\\times$ faster than other learned vector representations such as SketchRNN. (iv) SketchINR, for the first time, emulates the human ability to reproduce a sketch with varying abstraction in terms of number and complexity of strokes. As a first look at implicit sketches, SketchINR's compact high-fidelity representation will support future work in modelling long and complex sketches.","sentences":["We propose SketchINR, to advance the representation of vector sketches with implicit neural models.","A variable length vector sketch is compressed into a latent space of fixed dimension that implicitly encodes the underlying shape as a function of time and strokes.","The learned function predicts the $xy$ point coordinates in a sketch at each time and stroke.","Despite its simplicity, SketchINR outperforms existing representations at multiple tasks: (i) Encoding an entire sketch dataset into a fixed size latent vector, SketchINR gives $60\\times$ and $10\\times$ data compression over raster and vector sketches, respectively.","(ii) SketchINR's auto-decoder provides a much higher-fidelity representation than other learned vector sketch representations, and is uniquely able to scale to complex vector sketches such as FS-COCO.","(iii) SketchINR supports parallelisation that can decode/render $\\sim$$100\\times$ faster than other learned vector representations such as SketchRNN.","(iv) SketchINR, for the first time, emulates the human ability to reproduce a sketch with varying abstraction in terms of number and complexity of strokes.","As a first look at implicit sketches, SketchINR's compact high-fidelity representation will support future work in modelling long and complex sketches."],"url":"http://arxiv.org/abs/2403.09344v1","category":"cs.CV"}
{"created":"2024-03-14 12:44:47","title":"Geometric quantum discord of an arbitrary two-qudit state: the exact value and general upper bounds","abstract":"The geometric quantum discord of a two-qudit state has been studied in many papers, however, its exact analytical value in the explicit form is known only for a general two-qubit state, a general qubit-qudit state and some special families of two-qudit states. Based on the general Bloch vectors formalism [J. Phys. A: Math. Theor. 54 195301 (2021)], we find the explicit exact analytical value of the geometric quantum discord for a general two-qudit state of an arbitrary dimension via the parameters of its correlation matrix and the Bloch vectors of its reduced states. This new general analytical result indicates that the lower bound on the geometric quantum discord found in [Phys. Rev. A. 85, 204102 (2012)] is attained on each two-qudit state and also, includes all the known exact results on the geometric discord only as particular cases. Moreover, it allows us to find for an arbitrary two-qudit state, pure or mixed, the new general upper bounds on its geometric quantum discord, expressed via the Hilbert space characteristics of this state and in case of a pure two-qudit state -- in terms of its concurrence.","sentences":["The geometric quantum discord of a two-qudit state has been studied in many papers, however, its exact analytical value in the explicit form is known only for a general two-qubit state, a general qubit-qudit state and some special families of two-qudit states.","Based on the general Bloch vectors formalism [J. Phys.","A: Math.","Theor.","54 195301 (2021)], we find the explicit exact analytical value of the geometric quantum discord for a general two-qudit state of an arbitrary dimension via the parameters of its correlation matrix and the Bloch vectors of its reduced states.","This new general analytical result indicates that the lower bound on the geometric quantum discord found in [Phys.","Rev. A. 85, 204102 (2012)] is attained on each two-qudit state and also, includes all the known exact results on the geometric discord only as particular cases.","Moreover, it allows us to find for an arbitrary two-qudit state, pure or mixed, the new general upper bounds on its geometric quantum discord, expressed via the Hilbert space characteristics of this state and in case of a pure two-qudit state -- in terms of its concurrence."],"url":"http://arxiv.org/abs/2403.09342v1","category":"quant-ph"}
{"created":"2024-03-14 12:32:40","title":"LocalMamba: Visual State Space Model with Windowed Selective Scan","abstract":"Recent advancements in state space models, notably Mamba, have demonstrated significant progress in modeling long sequences for tasks like language understanding. Yet, their application in vision tasks has not markedly surpassed the performance of traditional Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). This paper posits that the key to enhancing Vision Mamba (ViM) lies in optimizing scan directions for sequence modeling. Traditional ViM approaches, which flatten spatial tokens, overlook the preservation of local 2D dependencies, thereby elongating the distance between adjacent tokens. We introduce a novel local scanning strategy that divides images into distinct windows, effectively capturing local dependencies while maintaining a global perspective. Additionally, acknowledging the varying preferences for scan patterns across different network layers, we propose a dynamic method to independently search for the optimal scan choices for each layer, substantially improving performance. Extensive experiments across both plain and hierarchical models underscore our approach's superiority in effectively capturing image representations. For example, our model significantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs. Code is available at: https://github.com/hunto/LocalMamba.","sentences":["Recent advancements in state space models, notably Mamba, have demonstrated significant progress in modeling long sequences for tasks like language understanding.","Yet, their application in vision tasks has not markedly surpassed the performance of traditional Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).","This paper posits that the key to enhancing Vision Mamba (ViM) lies in optimizing scan directions for sequence modeling.","Traditional ViM approaches, which flatten spatial tokens, overlook the preservation of local 2D dependencies, thereby elongating the distance between adjacent tokens.","We introduce a novel local scanning strategy that divides images into distinct windows, effectively capturing local dependencies while maintaining a global perspective.","Additionally, acknowledging the varying preferences for scan patterns across different network layers, we propose a dynamic method to independently search for the optimal scan choices for each layer, substantially improving performance.","Extensive experiments across both plain and hierarchical models underscore our approach's superiority in effectively capturing image representations.","For example, our model significantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs.","Code is available at: https://github.com/hunto/LocalMamba."],"url":"http://arxiv.org/abs/2403.09338v1","category":"cs.CV"}
{"created":"2024-03-14 12:31:10","title":"Differential identities of matrix algebras","abstract":"We study the differential identities of the algebra $M_k(F)$ of $k\\times k$ matrices over a field $F$ of characteristic zero when its full Lie algebra of derivations, $L=\\mbox{Der}(M_k(F))$, acts on it. We determine a set of 2 generators of the ideal of differential identities of $M_k(F)$ for $k\\geq 2$. Moreover, we obtain the exact values of the corresponding differential codimensions and differential cocharacters. Finally we prove that, unlike the ordinary case, the variety of differential algebras with $L$-action generated by $M_k(F)$ has almost polynomial growth for all $k\\geq 2$.","sentences":["We study the differential identities of the algebra $M_k(F)$ of $k\\times k$ matrices over a field $F$ of characteristic zero when its full Lie algebra of derivations, $L=\\mbox{Der}(M_k(F))$, acts on it.","We determine a set of 2 generators of the ideal of differential identities of $M_k(F)$ for $k\\geq 2$.","Moreover, we obtain the exact values of the corresponding differential codimensions and differential cocharacters.","Finally we prove that, unlike the ordinary case, the variety of differential algebras with $L$-action generated by $M_k(F)$ has almost polynomial growth for all $k\\geq 2$."],"url":"http://arxiv.org/abs/2403.09337v1","category":"math.RA"}
{"created":"2024-03-14 12:24:55","title":"Generalized Euler-Maclaurin formula and Signatures","abstract":"The Euler-Maclaurin formula which relates a discrete sum with an integral, is generalised to the setting of Riemann-Stieltjes sums and integrals on stochastic processes whose paths are a.s. rectifiable, that is continuous and bounded variation. For this purpose, new variants of the signature are introduced, such as the flip and the sawtooth signature. The counterparts of the Bernoulli numbers that arise in the classical Euler-Maclaurin formula are obtained by choosing the appropriate integration constants in the repeated integration by parts to ``minimise the error'' of every truncation level.","sentences":["The Euler-Maclaurin formula which relates a discrete sum with an integral, is generalised to the setting of Riemann-Stieltjes sums and integrals on stochastic processes whose paths are a.s. rectifiable, that is continuous and bounded variation.","For this purpose, new variants of the signature are introduced, such as the flip and the sawtooth signature.","The counterparts of the Bernoulli numbers that arise in the classical Euler-Maclaurin formula are obtained by choosing the appropriate integration constants in the repeated integration by parts to ``minimise the error'' of every truncation level."],"url":"http://arxiv.org/abs/2403.09335v1","category":"math.PR"}
{"created":"2024-03-14 12:22:54","title":"Video Editing via Factorized Diffusion Distillation","abstract":"We introduce Emu Video Edit (EVE), a model that establishes a new state-of-the art in video editing without relying on any supervised video editing data. To develop EVE we separately train an image editing adapter and a video generation adapter, and attach both to the same text-to-image model. Then, to align the adapters towards video editing we introduce a new unsupervised distillation procedure, Factorized Diffusion Distillation. This procedure distills knowledge from one or more teachers simultaneously, without any supervised data. We utilize this procedure to teach EVE to edit videos by jointly distilling knowledge to (i) precisely edit each individual frame from the image editing adapter, and (ii) ensure temporal consistency among the edited frames using the video generation adapter. Finally, to demonstrate the potential of our approach in unlocking other capabilities, we align additional combinations of adapters","sentences":["We introduce Emu Video Edit (EVE), a model that establishes a new state-of-the art in video editing without relying on any supervised video editing data.","To develop EVE we separately train an image editing adapter and a video generation adapter, and attach both to the same text-to-image model.","Then, to align the adapters towards video editing we introduce a new unsupervised distillation procedure, Factorized Diffusion Distillation.","This procedure distills knowledge from one or more teachers simultaneously, without any supervised data.","We utilize this procedure to teach EVE to edit videos by jointly distilling knowledge to (i) precisely edit each individual frame from the image editing adapter, and (ii) ensure temporal consistency among the edited frames using the video generation adapter.","Finally, to demonstrate the potential of our approach in unlocking other capabilities, we align additional combinations of adapters"],"url":"http://arxiv.org/abs/2403.09334v1","category":"cs.CV"}
{"created":"2024-03-14 12:21:37","title":"Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring","abstract":"Large Vision Language Models have achieved fine-grained object perception, but the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios. Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \\etc. To address this issue, we introduce a unified high-resolution generalist model, Griffon v2, enabling flexible object referring with visual and textual prompts. To efficiently scaling up image resolution, we design a simple and lightweight down-sampling projector to overcome the input tokens constraint in Large Language Models. This design inherently preserves the complete contexts and fine details, and significantly improves multimodal perception ability especially for small objects. Building upon this, we further equip the model with visual-language co-referring capabilities through a plug-and-play visual tokenizer. It enables user-friendly interaction with flexible target images, free-form texts and even coordinates. Experiments demonstrate that Griffon v2 can localize any objects of interest with visual and textual referring, achieve state-of-the-art performance on REC, phrase grounding, and REG tasks, and outperform expert models in object detection and object counting. Data, codes and models will be released at https://github.com/jefferyZhan/Griffon.","sentences":["Large Vision Language Models have achieved fine-grained object perception, but the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios.","Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \\etc.","To address this issue, we introduce a unified high-resolution generalist model, Griffon v2, enabling flexible object referring with visual and textual prompts.","To efficiently scaling up image resolution, we design a simple and lightweight down-sampling projector to overcome the input tokens constraint in Large Language Models.","This design inherently preserves the complete contexts and fine details, and significantly improves multimodal perception ability especially for small objects.","Building upon this, we further equip the model with visual-language co-referring capabilities through a plug-and-play visual tokenizer.","It enables user-friendly interaction with flexible target images, free-form texts and even coordinates.","Experiments demonstrate that Griffon v2 can localize any objects of interest with visual and textual referring, achieve state-of-the-art performance on REC, phrase grounding, and REG tasks, and outperform expert models in object detection and object counting.","Data, codes and models will be released at https://github.com/jefferyZhan/Griffon."],"url":"http://arxiv.org/abs/2403.09333v1","category":"cs.CV"}
{"created":"2024-03-14 12:18:45","title":"Radar Rainbow Beams For Wideband mmWave Communication: Beam Training And Tracking","abstract":"We propose a novel integrated sensing and communication (ISAC) system that leverages sensing to assist communication, ensuring fast initial access, seamless user tracking, and uninterrupted communication for millimeter wave (mmWave) wideband systems. True-time-delayers (TTDs) are utilized to generate frequency-dependent radar rainbow beams by controlling the beam squint effect. These beams cover users across the entire angular space simultaneously for fast beam training using just one orthogonal frequency-division multiplexing (OFDM) symbol. Three detection and estimation schemes are proposed based on radar rainbow beams for estimation of the users' angles, distances, and velocities, which are then exploited for communication beamformer design. The first proposed scheme utilizes a single-antenna radar receiver and one set of rainbow beams, but may cause a Doppler ambiguity. To tackle this limitation, two additional schemes are introduced, utilizing two sets of rainbow beams and a multi-antenna receiver, respectively. Furthermore, the proposed detection and estimation schemes are extended to realize user tracking by choosing different subsets of OFDM subcarriers. This approach eliminates the need to switch phase shifters and TTDs, which are typically necessary in existing tracking technologies, thereby reducing the demands on the control circurity. Simulation results reveal the effectiveness of the proposed rainbow beam-based training and tracking methods for mobile users. Notably, the scheme employing a multi-antenna radar receiver can accurately estimate the channel parameters and can support communication rates comparable to those achieved with perfect channel information.","sentences":["We propose a novel integrated sensing and communication (ISAC) system that leverages sensing to assist communication, ensuring fast initial access, seamless user tracking, and uninterrupted communication for millimeter wave (mmWave) wideband systems.","True-time-delayers (TTDs) are utilized to generate frequency-dependent radar rainbow beams by controlling the beam squint effect.","These beams cover users across the entire angular space simultaneously for fast beam training using just one orthogonal frequency-division multiplexing (OFDM) symbol.","Three detection and estimation schemes are proposed based on radar rainbow beams for estimation of the users' angles, distances, and velocities, which are then exploited for communication beamformer design.","The first proposed scheme utilizes a single-antenna radar receiver and one set of rainbow beams, but may cause a Doppler ambiguity.","To tackle this limitation, two additional schemes are introduced, utilizing two sets of rainbow beams and a multi-antenna receiver, respectively.","Furthermore, the proposed detection and estimation schemes are extended to realize user tracking by choosing different subsets of OFDM subcarriers.","This approach eliminates the need to switch phase shifters and TTDs, which are typically necessary in existing tracking technologies, thereby reducing the demands on the control circurity.","Simulation results reveal the effectiveness of the proposed rainbow beam-based training and tracking methods for mobile users.","Notably, the scheme employing a multi-antenna radar receiver can accurately estimate the channel parameters and can support communication rates comparable to those achieved with perfect channel information."],"url":"http://arxiv.org/abs/2403.09330v1","category":"eess.SP"}
{"created":"2024-03-14 12:15:23","title":"HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation","abstract":"We present HeadEvolver, a novel framework to generate stylized head avatars from text guidance. HeadEvolver uses locally learnable mesh deformation from a template head mesh, producing high-quality digital assets for detail-preserving editing and animation. To tackle the challenges of lacking fine-grained and semantic-aware local shape control in global deformation through Jacobians, we introduce a trainable parameter as a weighting factor for the Jacobian at each triangle to adaptively change local shapes while maintaining global correspondences and facial features. Moreover, to ensure the coherence of the resulting shape and appearance from different viewpoints, we use pretrained image diffusion models for differentiable rendering with regularization terms to refine the deformation under text guidance. Extensive experiments demonstrate that our method can generate diverse head avatars with an articulated mesh that can be edited seamlessly in 3D graphics software, facilitating downstream applications such as more efficient animation with inherited blend shapes and semantic consistency.","sentences":["We present HeadEvolver, a novel framework to generate stylized head avatars from text guidance.","HeadEvolver uses locally learnable mesh deformation from a template head mesh, producing high-quality digital assets for detail-preserving editing and animation.","To tackle the challenges of lacking fine-grained and semantic-aware local shape control in global deformation through Jacobians, we introduce a trainable parameter as a weighting factor for the Jacobian at each triangle to adaptively change local shapes while maintaining global correspondences and facial features.","Moreover, to ensure the coherence of the resulting shape and appearance from different viewpoints, we use pretrained image diffusion models for differentiable rendering with regularization terms to refine the deformation under text guidance.","Extensive experiments demonstrate that our method can generate diverse head avatars with an articulated mesh that can be edited seamlessly in 3D graphics software, facilitating downstream applications such as more efficient animation with inherited blend shapes and semantic consistency."],"url":"http://arxiv.org/abs/2403.09326v1","category":"cs.GR"}
{"created":"2024-03-14 12:11:25","title":"Privacy Preserving Anomaly Detection on Homomorphic Encrypted Data from IoT Sensors","abstract":"IoT devices have become indispensable components of our lives, and the advancement of AI technologies will make them even more pervasive, increasing the vulnerability to malfunctions or cyberattacks and raising privacy concerns. Encryption can mitigate these challenges; however, most existing anomaly detection techniques decrypt the data to perform the analysis, potentially undermining the encryption protection provided during transit or storage. Homomorphic encryption schemes are promising solutions as they enable the processing and execution of operations on IoT data while still encrypted, however, these schemes offer only limited operations, which poses challenges to their practical usage. In this paper, we propose a novel privacy-preserving anomaly detection solution designed for homomorphically encrypted data generated by IoT devices that efficiently detects abnormal values without performing decryption. We have adapted the Histogram-based anomaly detection technique for TFHE scheme to address limitations related to the input size and the depth of computation by implementing vectorized support operations. These operations include addition, value placement in buckets, labeling abnormal buckets based on a threshold frequency, labeling abnormal values based on their range, and bucket labels. Evaluation results show that the solution effectively detects anomalies without requiring data decryption and achieves consistent results comparable to the mechanism operating on plain data. Also, it shows robustness and resilience against various challenges commonly encountered in IoT environments, such as noisy sensor data, adversarial attacks, communication failures, and device malfunctions. Moreover, the time and computational overheads determined for several solution configurations, despite being large, are reasonable compared to those reported in existing literature.","sentences":["IoT devices have become indispensable components of our lives, and the advancement of AI technologies will make them even more pervasive, increasing the vulnerability to malfunctions or cyberattacks and raising privacy concerns.","Encryption can mitigate these challenges; however, most existing anomaly detection techniques decrypt the data to perform the analysis, potentially undermining the encryption protection provided during transit or storage.","Homomorphic encryption schemes are promising solutions as they enable the processing and execution of operations on IoT data while still encrypted, however, these schemes offer only limited operations, which poses challenges to their practical usage.","In this paper, we propose a novel privacy-preserving anomaly detection solution designed for homomorphically encrypted data generated by IoT devices that efficiently detects abnormal values without performing decryption.","We have adapted the Histogram-based anomaly detection technique for TFHE scheme to address limitations related to the input size and the depth of computation by implementing vectorized support operations.","These operations include addition, value placement in buckets, labeling abnormal buckets based on a threshold frequency, labeling abnormal values based on their range, and bucket labels.","Evaluation results show that the solution effectively detects anomalies without requiring data decryption and achieves consistent results comparable to the mechanism operating on plain data.","Also, it shows robustness and resilience against various challenges commonly encountered in IoT environments, such as noisy sensor data, adversarial attacks, communication failures, and device malfunctions.","Moreover, the time and computational overheads determined for several solution configurations, despite being large, are reasonable compared to those reported in existing literature."],"url":"http://arxiv.org/abs/2403.09322v1","category":"cs.CR"}
{"created":"2024-03-14 12:08:44","title":"SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios","abstract":"Despite the success in 6D pose estimation in bin-picking scenarios, existing methods still struggle to produce accurate prediction results for symmetry objects and real world scenarios. The primary bottlenecks include 1) the ambiguity keypoints caused by object symmetries; 2) the domain gap between real and synthetic data. To circumvent these problem, we propose a new 6D pose estimation network with symmetric-aware keypoint prediction and self-training domain adaptation (SD-Net). SD-Net builds on pointwise keypoint regression and deep hough voting to perform reliable detection keypoint under clutter and occlusion. Specifically, at the keypoint prediction stage, we designe a robust 3D keypoints selection strategy considering the symmetry class of objects and equivalent keypoints, which facilitate locating 3D keypoints even in highly occluded scenes. Additionally, we build an effective filtering algorithm on predicted keypoint to dynamically eliminate multiple ambiguity and outlier keypoint candidates. At the domain adaptation stage, we propose the self-training framework using a student-teacher training scheme. To carefully distinguish reliable predictions, we harnesses a tailored heuristics for 3D geometry pseudo labelling based on semi-chamfer distance. On public Sil'eane dataset, SD-Net achieves state-of-the-art results, obtaining an average precision of 96%. Testing learning and generalization abilities on public Parametric datasets, SD-Net is 8% higher than the state-of-the-art method. The code is available at https://github.com/dingthuang/SD-Net.","sentences":["Despite the success in 6D pose estimation in bin-picking scenarios, existing methods still struggle to produce accurate prediction results for symmetry objects and real world scenarios.","The primary bottlenecks include 1) the ambiguity keypoints caused by object symmetries; 2) the domain gap between real and synthetic data.","To circumvent these problem, we propose a new 6D pose estimation network with symmetric-aware keypoint prediction and self-training domain adaptation (SD-Net).","SD-Net builds on pointwise keypoint regression and deep hough voting to perform reliable detection keypoint under clutter and occlusion.","Specifically, at the keypoint prediction stage, we designe a robust 3D keypoints selection strategy considering the symmetry class of objects and equivalent keypoints, which facilitate locating 3D keypoints even in highly occluded scenes.","Additionally, we build an effective filtering algorithm on predicted keypoint to dynamically eliminate multiple ambiguity and outlier keypoint candidates.","At the domain adaptation stage, we propose the self-training framework using a student-teacher training scheme.","To carefully distinguish reliable predictions, we harnesses a tailored heuristics for 3D geometry pseudo labelling based on semi-chamfer distance.","On public Sil'eane dataset, SD-Net achieves state-of-the-art results, obtaining an average precision of 96%.","Testing learning and generalization abilities on public Parametric datasets, SD-Net is 8% higher than the state-of-the-art method.","The code is available at https://github.com/dingthuang/SD-Net."],"url":"http://arxiv.org/abs/2403.09317v1","category":"cs.CV"}
{"created":"2024-03-14 12:03:28","title":"Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection","abstract":"In this paper we present YOLOX-ViT, a novel object detection model, and investigate the efficacy of knowledge distillation for model size reduction without sacrificing performance. Focused on underwater robotics, our research addresses key questions about the viability of smaller models and the impact of the visual transformer layer in YOLOX. Furthermore, we introduce a new side-scan sonar image dataset, and use it to evaluate our object detector's performance. Results show that knowledge distillation effectively reduces false positives in wall detection. Additionally, the introduced visual transformer layer significantly improves object detection accuracy in the underwater environment. The source code of the knowledge distillation in the YOLOX-ViT is at https://github.com/remaro-network/KD-YOLOX-ViT.","sentences":["In this paper we present YOLOX-ViT, a novel object detection model, and investigate the efficacy of knowledge distillation for model size reduction without sacrificing performance.","Focused on underwater robotics, our research addresses key questions about the viability of smaller models and the impact of the visual transformer layer in YOLOX.","Furthermore, we introduce a new side-scan sonar image dataset, and use it to evaluate our object detector's performance.","Results show that knowledge distillation effectively reduces false positives in wall detection.","Additionally, the introduced visual transformer layer significantly improves object detection accuracy in the underwater environment.","The source code of the knowledge distillation in the YOLOX-ViT is at https://github.com/remaro-network/KD-YOLOX-ViT."],"url":"http://arxiv.org/abs/2403.09313v1","category":"cs.CV"}
{"created":"2024-03-14 12:02:25","title":"Modular parametric PGD enabling online solution of partial differential equations","abstract":"In the present work, a new methodology is proposed for building surrogate parametric models of engineering systems based on modular assembly of pre-solved modules. Each module is a generic parametric solution considering parametric geometry, material and boundary conditions. By assembling these modules and satisfying continuity constraints at the interfaces, a parametric surrogate model of the full problem can be obtained. In the present paper, the PGD technique in connection with NURBS geometry representation is used to create a parametric model for each module. In this technique, the NURBS objects allow to map the governing boundary value problem from a parametric non-regular domain into a regular reference domain and the PGD is used to create a reduced model in the reference domain. In the assembly stage, an optimization problem is solved to satisfy the continuity constraints at the interfaces. The proposed procedure is based on the offline--online paradigm: the offline stage consists of creating multiple pre-solved modules which can be afterwards assembled in almost real-time during the online stage, enabling quick evaluations of the full system response. To show the potential of the proposed approach some numerical examples in heat conduction and structural plates under bending are presented.","sentences":["In the present work, a new methodology is proposed for building surrogate parametric models of engineering systems based on modular assembly of pre-solved modules.","Each module is a generic parametric solution considering parametric geometry, material and boundary conditions.","By assembling these modules and satisfying continuity constraints at the interfaces, a parametric surrogate model of the full problem can be obtained.","In the present paper, the PGD technique in connection with NURBS geometry representation is used to create a parametric model for each module.","In this technique, the NURBS objects allow to map the governing boundary value problem from a parametric non-regular domain into a regular reference domain and the PGD is used to create a reduced model in the reference domain.","In the assembly stage, an optimization problem is solved to satisfy the continuity constraints at the interfaces.","The proposed procedure is based on the offline--online paradigm: the offline stage consists of creating multiple pre-solved modules which can be afterwards assembled in almost real-time during the online stage, enabling quick evaluations of the full system response.","To show the potential of the proposed approach some numerical examples in heat conduction and structural plates under bending are presented."],"url":"http://arxiv.org/abs/2403.09312v1","category":"cs.CE"}
{"created":"2024-03-14 12:00:53","title":"Binary Stretch Embedding of Weighted Graphs","abstract":"In this paper, we introduce and study the problem of \\textit{binary stretch embedding} of edge-weighted graph. This problem is closely related to the well-known \\textit{addressing problem} of Graham and Pollak. Addressing problem is the problem of assigning the shortest possible length strings (called ``addresses\") over the alphabet $\\{0,1,*\\}$ to the vertices of an input graph $G$ with the following property. For every pair $u,v$ of vertices, the number of positions in which one of their addresses is $1$, and the other is $0$ is exactly equal to the distance of $u,v$ in graph $G$. When the addresses do not contain the symbol $*$, the problem is called \\textit{isometric hypercube embedding}. As far as we know, the isometric hypercube embedding was introduced by Firsov in 1965. It is known that such addresses do not exist for general graphs.   Inspired by the addressing problem, in this paper, we introduce the \\textit{binary stretch embedding problem}, or BSEP for short, for the edge-weighted undirected graphs. We also argue how this problem is related to other graph embedding problems in the literature.   Using tools and techniques such as Hadamard codes and the theory of linear programming, several upper and lower bounds as well as exact solutions for certain classes of graphs will be discovered.   As an application of the results in this paper, we derive improved upper bounds or exact values for the maximum size of Lee metric codes of certain parameters.","sentences":["In this paper, we introduce and study the problem of \\textit{binary stretch embedding} of edge-weighted graph.","This problem is closely related to the well-known \\textit{addressing problem} of Graham and Pollak.","Addressing problem is the problem of assigning the shortest possible length strings (called ``addresses\") over the alphabet $\\{0,1,*\\}$ to the vertices of an input graph $G$ with the following property.","For every pair $u,v$ of vertices, the number of positions in which one of their addresses is $1$, and the other is $0$ is exactly equal to the distance of $u,v$ in graph $G$. When the addresses do not contain the symbol $*$, the problem is called \\textit{isometric hypercube embedding}.","As far as we know, the isometric hypercube embedding was introduced by Firsov in 1965.","It is known that such addresses do not exist for general graphs.   ","Inspired by the addressing problem, in this paper, we introduce the \\textit{binary stretch embedding problem}, or BSEP for short, for the edge-weighted undirected graphs.","We also argue how this problem is related to other graph embedding problems in the literature.   ","Using tools and techniques such as Hadamard codes and the theory of linear programming, several upper and lower bounds as well as exact solutions for certain classes of graphs will be discovered.   ","As an application of the results in this paper, we derive improved upper bounds or exact values for the maximum size of Lee metric codes of certain parameters."],"url":"http://arxiv.org/abs/2403.09311v1","category":"cs.DM"}
{"created":"2024-03-14 11:59:07","title":"Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality","abstract":"Programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints. We propose a framework that simplifies robot deployment by allowing direct communication using natural language. It uses large language models (LLM) for prompt processing, workspace understanding, and waypoint generation. It also employs Augmented Reality (AR) to provide visual feedback of the planned outcome. We showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot. Moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping).","sentences":["Programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints.","We propose a framework that simplifies robot deployment by allowing direct communication using natural language.","It uses large language models (LLM) for prompt processing, workspace understanding, and waypoint generation.","It also employs Augmented Reality (AR) to provide visual feedback of the planned outcome.","We showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot.","Moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping)."],"url":"http://arxiv.org/abs/2403.09308v1","category":"cs.HC"}
{"created":"2024-03-14 11:57:58","title":"Annotation Free Semantic Segmentation with Vision Foundation Models","abstract":"Semantic Segmentation is one of the most challenging vision tasks, usually requiring large amounts of training data with expensive pixel-level annotations. With the success of foundation models and especially vision-language models, recent works attempt to achieve zero-shot semantic segmentation while requiring either large scale training or additional image/pixel-level annotations. In this work, we build a lightweight module on top of a self-supervised pretrained vision encoder to align patch features with a pre-trained text encoder. Importantly, we generate free annotations for any semantic segmentation dataset using existing foundation models and train our alignment module cost free. We use CLIP to detect objects and SAM to generate high quality object masks. Our approach can bring language-based semantics to any pre-trained vision encoder with minimal training. Our module is lightweight, uses foundation models as a sole source of supervision and shows impressive generalization capability from little training data with no annotation.","sentences":["Semantic Segmentation is one of the most challenging vision tasks, usually requiring large amounts of training data with expensive pixel-level annotations.","With the success of foundation models and especially vision-language models, recent works attempt to achieve zero-shot semantic segmentation while requiring either large scale training or additional image/pixel-level annotations.","In this work, we build a lightweight module on top of a self-supervised pretrained vision encoder to align patch features with a pre-trained text encoder.","Importantly, we generate free annotations for any semantic segmentation dataset using existing foundation models and train our alignment module cost free.","We use CLIP to detect objects and SAM to generate high quality object masks.","Our approach can bring language-based semantics to any pre-trained vision encoder with minimal training.","Our module is lightweight, uses foundation models as a sole source of supervision and shows impressive generalization capability from little training data with no annotation."],"url":"http://arxiv.org/abs/2403.09307v1","category":"cs.CV"}
{"created":"2024-03-14 11:57:32","title":"Partially coherent beam: Theory and simulation","abstract":"Since the development of lasers, we have continuously sought to advance techniques and theory to obtain beams with a high degree of coherence, as natural light sources provide incoherent light. However, there are applications where it is advantageous to use partially coherent (PC) beams in a controlled manner, such as in propagation through turbulent media. To generate a PC beam in the laboratory or in simulations, specific theories and methods are required. In this article, we provide an introduction to PC beam theory, describing how to generate them through modal decompositions and a step-by-step guide for simulating and analyzing beam generation inspired by experiments. To illustrate the methods, we present Gaussian Schell-Model beams as an example.","sentences":["Since the development of lasers, we have continuously sought to advance techniques and theory to obtain beams with a high degree of coherence, as natural light sources provide incoherent light.","However, there are applications where it is advantageous to use partially coherent (PC) beams in a controlled manner, such as in propagation through turbulent media.","To generate a PC beam in the laboratory or in simulations, specific theories and methods are required.","In this article, we provide an introduction to PC beam theory, describing how to generate them through modal decompositions and a step-by-step guide for simulating and analyzing beam generation inspired by experiments.","To illustrate the methods, we present Gaussian Schell-Model beams as an example."],"url":"http://arxiv.org/abs/2403.09306v1","category":"physics.optics"}
{"created":"2024-03-14 11:49:14","title":"CRB Analysis for Mixed-ADC Based DOA Estimation","abstract":"We consider a mixed analog-to-digital converter (ADC) based architecture consisting of high-precision and one-bit ADCs with the antenna-varying threshold for direction of arrival (DOA) estimation using a uniform linear array (ULA), which utilizes fixed but different thresholds for one-bit ADCs across different receive antennas. The Cram{\\'e}r-Rao bound (CRB) with the antenna-varying threshold is obtained. Then based on the lower bound of the CRB, we derive the asymptotic CRB of the DOA, which depends on the placement of mixed-ADC. Our analysis shows that distributing high-precision ADCs evenly around the two edges of the ULA yields improved performance. This result can be extended to a more general case where the ULA is equipped with two types of ADCs with different quantization precisions. To efficiently obtain the maximum likelihood DOA estimates, we propose a two-step algorithm. Firstly, we formulate the model as a sparse signal representation problem, and modify the sparse learning via iterative minimization (SLIM) approach to the mixed-ADC based DOA estimation. In the second step, we use the relaxation-based approach to cyclically refine the estimates of SLIM, further enhancing the DOA estimation performance. Numerical examples are presented to demonstrate the validity of the CRB analysis and the effectiveness of our methods.","sentences":["We consider a mixed analog-to-digital converter (ADC) based architecture consisting of high-precision and one-bit ADCs with the antenna-varying threshold for direction of arrival (DOA) estimation using a uniform linear array (ULA), which utilizes fixed but different thresholds for one-bit ADCs across different receive antennas.","The Cram{\\'e}r-Rao bound (CRB) with the antenna-varying threshold is obtained.","Then based on the lower bound of the CRB, we derive the asymptotic CRB of the DOA, which depends on the placement of mixed-ADC.","Our analysis shows that distributing high-precision ADCs evenly around the two edges of the ULA yields improved performance.","This result can be extended to a more general case where the ULA is equipped with two types of ADCs with different quantization precisions.","To efficiently obtain the maximum likelihood DOA estimates, we propose a two-step algorithm.","Firstly, we formulate the model as a sparse signal representation problem, and modify the sparse learning via iterative minimization (SLIM) approach to the mixed-ADC based DOA estimation.","In the second step, we use the relaxation-based approach to cyclically refine the estimates of SLIM, further enhancing the DOA estimation performance.","Numerical examples are presented to demonstrate the validity of the CRB analysis and the effectiveness of our methods."],"url":"http://arxiv.org/abs/2403.09301v1","category":"eess.SP"}
{"created":"2024-03-14 11:36:36","title":"Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models","abstract":"Large-scale vision-language models (VLMs) have shown a strong zero-shot generalization capability on unseen-domain data. However, when adapting pre-trained VLMs to a sequence of downstream tasks, they are prone to forgetting previously learned knowledge and degrade their zero-shot classification capability. To tackle this problem, we propose a unique Selective Dual-Teacher Knowledge Transfer framework that leverages the most recent fine-tuned and the original pre-trained VLMs as dual teachers to preserve the previously learned knowledge and zero-shot capabilities, respectively. With only access to an unlabeled reference dataset, our proposed framework performs a selective knowledge distillation mechanism by measuring the feature discrepancy from the dual teacher VLMs. Consequently, our selective dual-teacher knowledge distillation would mitigate catastrophic forgetting of previously learned knowledge while preserving the zero-shot capabilities from pre-trained VLMs. Through extensive experiments on benchmark datasets, we show that our proposed framework is favorable against state-of-the-art continual learning approaches for preventing catastrophic forgetting and zero-shot degradation.","sentences":["Large-scale vision-language models (VLMs) have shown a strong zero-shot generalization capability on unseen-domain data.","However, when adapting pre-trained VLMs to a sequence of downstream tasks, they are prone to forgetting previously learned knowledge and degrade their zero-shot classification capability.","To tackle this problem, we propose a unique Selective Dual-Teacher Knowledge Transfer framework that leverages the most recent fine-tuned and the original pre-trained VLMs as dual teachers to preserve the previously learned knowledge and zero-shot capabilities, respectively.","With only access to an unlabeled reference dataset, our proposed framework performs a selective knowledge distillation mechanism by measuring the feature discrepancy from the dual teacher VLMs.","Consequently, our selective dual-teacher knowledge distillation would mitigate catastrophic forgetting of previously learned knowledge while preserving the zero-shot capabilities from pre-trained VLMs.","Through extensive experiments on benchmark datasets, we show that our proposed framework is favorable against state-of-the-art continual learning approaches for preventing catastrophic forgetting and zero-shot degradation."],"url":"http://arxiv.org/abs/2403.09296v1","category":"cs.CV"}
{"created":"2024-03-14 11:24:32","title":"Parametric tuning of dynamical phase transitions in ultracold reactions","abstract":"Advances in ultracold chemistry have led to the possibility of a coherent transformation between ultracold atoms and molecules including between completely bosonic condensates. Such transformations are enabled by the magneto-association of atoms at a Feshbach resonance which results in a passage through a quantum critical point. In this study, we show that the presence of generic interaction between the formed molecules can fundamentally alter the nature of the critical point, change the yield of the reaction and the order of the consequent phase transition. We find that the correlations introduced by this rather general interaction induce nontrivial many-body physics such as coherent oscillations between atoms and molecules, and a selective formation of squeezed molecular quantum states and quantum cat states. We provide analytical and numerical descriptions of these many-body effects, along with scaling laws for the reaction yield in both the adiabatic and non-adiabatic regimes, and highlight the potential experimental relevance in quantum sensing.","sentences":["Advances in ultracold chemistry have led to the possibility of a coherent transformation between ultracold atoms and molecules including between completely bosonic condensates.","Such transformations are enabled by the magneto-association of atoms at a Feshbach resonance which results in a passage through a quantum critical point.","In this study, we show that the presence of generic interaction between the formed molecules can fundamentally alter the nature of the critical point, change the yield of the reaction and the order of the consequent phase transition.","We find that the correlations introduced by this rather general interaction induce nontrivial many-body physics such as coherent oscillations between atoms and molecules, and a selective formation of squeezed molecular quantum states and quantum cat states.","We provide analytical and numerical descriptions of these many-body effects, along with scaling laws for the reaction yield in both the adiabatic and non-adiabatic regimes, and highlight the potential experimental relevance in quantum sensing."],"url":"http://arxiv.org/abs/2403.09291v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-14 11:23:39","title":"SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival","abstract":"Accurately predicting the survival rate of cancer patients is crucial for aiding clinicians in planning appropriate treatment, reducing cancer-related medical expenses, and significantly enhancing patients' quality of life. Multimodal prediction of cancer patient survival offers a more comprehensive and precise approach. However, existing methods still grapple with challenges related to missing multimodal data and information interaction within modalities. This paper introduces SELECTOR, a heterogeneous graph-aware network based on convolutional mask encoders for robust multimodal prediction of cancer patient survival. SELECTOR comprises feature edge reconstruction, convolutional mask encoder, feature cross-fusion, and multimodal survival prediction modules. Initially, we construct a multimodal heterogeneous graph and employ the meta-path method for feature edge reconstruction, ensuring comprehensive incorporation of feature information from graph edges and effective embedding of nodes. To mitigate the impact of missing features within the modality on prediction accuracy, we devised a convolutional masked autoencoder (CMAE) to process the heterogeneous graph post-feature reconstruction. Subsequently, the feature cross-fusion module facilitates communication between modalities, ensuring that output features encompass all features of the modality and relevant information from other modalities. Extensive experiments and analysis on six cancer datasets from TCGA demonstrate that our method significantly outperforms state-of-the-art methods in both modality-missing and intra-modality information-confirmed cases. Our codes are made available at https://github.com/panliangrui/Selector.","sentences":["Accurately predicting the survival rate of cancer patients is crucial for aiding clinicians in planning appropriate treatment, reducing cancer-related medical expenses, and significantly enhancing patients' quality of life.","Multimodal prediction of cancer patient survival offers a more comprehensive and precise approach.","However, existing methods still grapple with challenges related to missing multimodal data and information interaction within modalities.","This paper introduces SELECTOR, a heterogeneous graph-aware network based on convolutional mask encoders for robust multimodal prediction of cancer patient survival.","SELECTOR comprises feature edge reconstruction, convolutional mask encoder, feature cross-fusion, and multimodal survival prediction modules.","Initially, we construct a multimodal heterogeneous graph and employ the meta-path method for feature edge reconstruction, ensuring comprehensive incorporation of feature information from graph edges and effective embedding of nodes.","To mitigate the impact of missing features within the modality on prediction accuracy, we devised a convolutional masked autoencoder (CMAE) to process the heterogeneous graph post-feature reconstruction.","Subsequently, the feature cross-fusion module facilitates communication between modalities, ensuring that output features encompass all features of the modality and relevant information from other modalities.","Extensive experiments and analysis on six cancer datasets from TCGA demonstrate that our method significantly outperforms state-of-the-art methods in both modality-missing and intra-modality information-confirmed cases.","Our codes are made available at https://github.com/panliangrui/Selector."],"url":"http://arxiv.org/abs/2403.09290v1","category":"cs.CV"}
{"created":"2024-03-14 11:22:51","title":"Silico-centric Theory of Mind","abstract":"Theory of Mind (ToM) refers to the ability to attribute mental states, such as beliefs, desires, intentions, and knowledge, to oneself and others, and to understand that these mental states can differ from one's own and from reality. We investigate ToM in environments with multiple, distinct, independent AI agents, each possessing unique internal states, information, and objectives. Inspired by human false-belief experiments, we present an AI ('focal AI') with a scenario where its clone undergoes a human-centric ToM assessment. We prompt the focal AI to assess whether its clone would benefit from additional instructions. Concurrently, we give its clones the ToM assessment, both with and without the instructions, thereby engaging the focal AI in higher-order counterfactual reasoning akin to human mentalizing--with respect to humans in one test and to other AI in another. We uncover a discrepancy: Contemporary AI demonstrates near-perfect accuracy on human-centric ToM assessments. Since information embedded in one AI is identically embedded in its clone, additional instructions are redundant. Yet, we observe AI crafting elaborate instructions for their clones, erroneously anticipating a need for assistance. An independent referee AI agrees with these unsupported expectations. Neither the focal AI nor the referee demonstrates ToM in our 'silico-centric' test.","sentences":["Theory of Mind (ToM) refers to the ability to attribute mental states, such as beliefs, desires, intentions, and knowledge, to oneself and others, and to understand that these mental states can differ from one's own and from reality.","We investigate ToM in environments with multiple, distinct, independent AI agents, each possessing unique internal states, information, and objectives.","Inspired by human false-belief experiments, we present an AI ('focal AI') with a scenario where its clone undergoes a human-centric ToM assessment.","We prompt the focal AI to assess whether its clone would benefit from additional instructions.","Concurrently, we give its clones the ToM assessment, both with and without the instructions, thereby engaging the focal AI in higher-order counterfactual reasoning akin to human mentalizing--with respect to humans in one test and to other AI in another.","We uncover a discrepancy: Contemporary AI demonstrates near-perfect accuracy on human-centric ToM assessments.","Since information embedded in one AI is identically embedded in its clone, additional instructions are redundant.","Yet, we observe AI crafting elaborate instructions for their clones, erroneously anticipating a need for assistance.","An independent referee AI agrees with these unsupported expectations.","Neither the focal AI nor the referee demonstrates ToM in our 'silico-centric' test."],"url":"http://arxiv.org/abs/2403.09289v1","category":"cs.AI"}
{"created":"2024-03-14 11:22:06","title":"Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering","abstract":"Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text in images and answer questions related to the text content. Most existing methods heavily rely on the accuracy of Optical Character Recognition (OCR) systems, and aggressive fine-tuning based on limited spatial location information and erroneous OCR text information often leads to inevitable overfitting. In this paper, we propose a multimodal adversarial training architecture with spatial awareness capabilities. Specifically, we introduce an Adversarial OCR Enhancement (AOE) module, which leverages adversarial training in the embedding space of OCR modality to enhance fault-tolerant representation of OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, We add a Spatial-Aware Self-Attention (SASA) mechanism to help the model better capture the spatial relationships among OCR tokens. Various experiments demonstrate that our method achieves significant performance improvements on both the ST-VQA and TextVQA datasets and provides a novel paradigm for multimodal adversarial training.","sentences":["Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text in images and answer questions related to the text content.","Most existing methods heavily rely on the accuracy of Optical Character Recognition (OCR) systems, and aggressive fine-tuning based on limited spatial location information and erroneous OCR text information often leads to inevitable overfitting.","In this paper, we propose a multimodal adversarial training architecture with spatial awareness capabilities.","Specifically, we introduce an Adversarial OCR Enhancement (AOE) module, which leverages adversarial training in the embedding space of OCR modality to enhance fault-tolerant representation of OCR texts, thereby reducing noise caused by OCR errors.","Simultaneously, We add a Spatial-Aware Self-Attention (SASA) mechanism to help the model better capture the spatial relationships among OCR tokens.","Various experiments demonstrate that our method achieves significant performance improvements on both the ST-VQA and TextVQA datasets and provides a novel paradigm for multimodal adversarial training."],"url":"http://arxiv.org/abs/2403.09288v1","category":"cs.CV"}
{"created":"2024-03-14 11:18:32","title":"Duality and hidden symmetry breaking in the q-deformed Affleck-Kennedy-Lieb-Tasaki model","abstract":"We revisit the question of string order and hidden symmetry breaking in the q-deformed AKLT model, an example of a spin chain that possesses generalized symmetry. We first argue that the non-local Kennedy-Tasaki duality transformation that was previously proposed to relate the string order to a local order parameter leads to a non-local Hamiltonian and thus does not provide a physically adequate description of the symmetry breaking. We then present a modified non-local transformation which is based on a recently developed generalization of Witten's Conjugation to frustration-free lattice models and capable of resolving this issue.","sentences":["We revisit the question of string order and hidden symmetry breaking in the q-deformed AKLT model, an example of a spin chain that possesses generalized symmetry.","We first argue that the non-local Kennedy-Tasaki duality transformation that was previously proposed to relate the string order to a local order parameter leads to a non-local Hamiltonian and thus does not provide a physically adequate description of the symmetry breaking.","We then present a modified non-local transformation which is based on a recently developed generalization of Witten's Conjugation to frustration-free lattice models and capable of resolving this issue."],"url":"http://arxiv.org/abs/2403.09287v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 11:08:33","title":"CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise Classification","abstract":"The CLIP (Contrastive Language-Image Pretraining) model has exhibited outstanding performance in recognition problems, such as zero-shot image classification and object detection. However, its ability to count remains understudied due to the inherent challenges of transforming counting--a regression task--into a recognition task. In this paper, we investigate CLIP's potential in counting, focusing specifically on estimating crowd sizes. Existing classification-based crowd-counting methods have encountered issues, including inappropriate discretization strategies, which impede the application of CLIP and result in suboptimal performance. To address these challenges, we propose the Enhanced Blockwise Classification (EBC) framework. In contrast to previous methods, EBC relies on integer-valued bins that facilitate the learning of robust decision boundaries. Within our model-agnostic EBC framework, we introduce CLIP-EBC, the first fully CLIP-based crowd-counting model capable of generating density maps. Comprehensive evaluations across diverse crowd-counting datasets demonstrate the state-of-the-art performance of our methods. Particularly, EBC can improve existing models by up to 76.9%. Moreover, our CLIP-EBC model surpasses current crowd-counting methods, achieving mean absolute errors of 55.0 and 6.3 on ShanghaiTech part A and part B datasets, respectively. The code will be made publicly available.","sentences":["The CLIP (Contrastive Language-Image Pretraining) model has exhibited outstanding performance in recognition problems, such as zero-shot image classification and object detection.","However, its ability to count remains understudied due to the inherent challenges of transforming counting--a regression task--into a recognition task.","In this paper, we investigate CLIP's potential in counting, focusing specifically on estimating crowd sizes.","Existing classification-based crowd-counting methods have encountered issues, including inappropriate discretization strategies, which impede the application of CLIP and result in suboptimal performance.","To address these challenges, we propose the Enhanced Blockwise Classification (EBC) framework.","In contrast to previous methods, EBC relies on integer-valued bins that facilitate the learning of robust decision boundaries.","Within our model-agnostic EBC framework, we introduce CLIP-EBC, the first fully CLIP-based crowd-counting model capable of generating density maps.","Comprehensive evaluations across diverse crowd-counting datasets demonstrate the state-of-the-art performance of our methods.","Particularly, EBC can improve existing models by up to 76.9%.","Moreover, our CLIP-EBC model surpasses current crowd-counting methods, achieving mean absolute errors of 55.0 and 6.3 on ShanghaiTech part A and part B datasets, respectively.","The code will be made publicly available."],"url":"http://arxiv.org/abs/2403.09281v1","category":"cs.CV"}
{"created":"2024-03-14 10:59:24","title":"Nonlinear Kinematics of Recursive Origami Inspired by Spidron","abstract":"Non-periodic folding of periodic crease patterns paves the way to novel nonlinear phenomena that cannot be feasible through periodic folding. This paper focuses on the non-periodic folding of recursive crease patterns generalized from Spidron. Although it is known that Spidron has a 1-DOF isotropic rigid folding motion, its general kinematics and dependence on the crease pattern remain unclear. Using the kinematics of a single unit cell of Spidron and the recursive construction of the folded state of multiple unit cells, we consider the folding of Spidron that is not necessarily isotropic. We found that as the number of unit cells increases, the non-periodic folding is restricted and the isotropic folding becomes dominant. Then, we analyze the three kinds of isotropic folding modes by constructing 1-dimensional dynamical systems governing each of them. We show that the dynamical system can possess different recursive natures depending on folding modes even in an identical crease pattern. Furthermore, we show their novel nonlinear nature, including the period-doubling cascade leading to the emergence of chaos.","sentences":["Non-periodic folding of periodic crease patterns paves the way to novel nonlinear phenomena that cannot be feasible through periodic folding.","This paper focuses on the non-periodic folding of recursive crease patterns generalized from Spidron.","Although it is known that Spidron has a 1-DOF isotropic rigid folding motion, its general kinematics and dependence on the crease pattern remain unclear.","Using the kinematics of a single unit cell of Spidron and the recursive construction of the folded state of multiple unit cells, we consider the folding of Spidron that is not necessarily isotropic.","We found that as the number of unit cells increases, the non-periodic folding is restricted and the isotropic folding becomes dominant.","Then, we analyze the three kinds of isotropic folding modes by constructing 1-dimensional dynamical systems governing each of them.","We show that the dynamical system can possess different recursive natures depending on folding modes even in an identical crease pattern.","Furthermore, we show their novel nonlinear nature, including the period-doubling cascade leading to the emergence of chaos."],"url":"http://arxiv.org/abs/2403.09278v1","category":"cond-mat.soft"}
{"created":"2024-03-14 10:58:02","title":"Folding $\u03c0$","abstract":"It is well known that the set of origami constructible numbers is larger than the classical straight-edge and compass constructible numbers. However, the Huzita-Justin-Hatori origami constructible numbers remain algebraic so that the transcendental number $\\pi$ can only be approximated using a finite number of straight line folds. Using these methods we give a convergent sequence for folding $\\pi$ as well as other methods to approximate $\\pi$. Folding along curved creases, however, allows for the construction of transcendental numbers. We here give a method to construct $\\pi$ exactly by folding along a parabola, and we discuss generalizations for folding other transcendental numbers such as $\\Gamma(1/4)$.","sentences":["It is well known that the set of origami constructible numbers is larger than the classical straight-edge and compass constructible numbers.","However, the Huzita-Justin-Hatori origami constructible numbers remain algebraic so that the transcendental number $\\pi$ can only be approximated using a finite number of straight line folds.","Using these methods we give a convergent sequence for folding $\\pi$ as well as other methods to approximate $\\pi$. Folding along curved creases, however, allows for the construction of transcendental numbers.","We here give a method to construct $\\pi$ exactly by folding along a parabola, and we discuss generalizations for folding other transcendental numbers such as $\\Gamma(1/4)$."],"url":"http://arxiv.org/abs/2403.09277v1","category":"math.NT"}
{"created":"2024-03-14 10:55:36","title":"Static Grouping Strategy Design for Beyond Diagonal Reconfigurable Intelligent Surfaces","abstract":"Beyond diagonal reconfigurable intelligent surface (BD-RIS) extends conventional RIS through novel architectures, such as group-connected RIS, with scattering matrix not restricted to being diagonal. However, it remains unexplored how to optimally group the elements in group-connected RISs to maximize the performance while maintaining a low-complexity circuit. In this study, we propose and model BD-RIS with a static grouping strategy optimized based on the channel statistics. After formulating the corresponding problems, we design the grouping in single- and multi-user systems. Numerical results reveal the benefits of grouping optimization, i.e., up to 60% sum rate improvement, especially in highly correlated channels.","sentences":["Beyond diagonal reconfigurable intelligent surface (BD-RIS) extends conventional RIS through novel architectures, such as group-connected RIS, with scattering matrix not restricted to being diagonal.","However, it remains unexplored how to optimally group the elements in group-connected RISs to maximize the performance while maintaining a low-complexity circuit.","In this study, we propose and model BD-RIS with a static grouping strategy optimized based on the channel statistics.","After formulating the corresponding problems, we design the grouping in single- and multi-user systems.","Numerical results reveal the benefits of grouping optimization, i.e., up to 60% sum rate improvement, especially in highly correlated channels."],"url":"http://arxiv.org/abs/2403.09275v1","category":"cs.IT"}
{"created":"2024-03-14 10:52:45","title":"EventRPG: Event Data Augmentation with Relevance Propagation Guidance","abstract":"Event camera, a novel bio-inspired vision sensor, has drawn a lot of attention for its low latency, low power consumption, and high dynamic range. Currently, overfitting remains a critical problem in event-based classification tasks for Spiking Neural Network (SNN) due to its relatively weak spatial representation capability. Data augmentation is a simple but efficient method to alleviate overfitting and improve the generalization ability of neural networks, and saliency-based augmentation methods are proven to be effective in the image processing field. However, there is no approach available for extracting saliency maps from SNNs. Therefore, for the first time, we present Spiking Layer-Time-wise Relevance Propagation rule (SLTRP) and Spiking Layer-wise Relevance Propagation rule (SLRP) in order for SNN to generate stable and accurate CAMs and saliency maps. Based on this, we propose EventRPG, which leverages relevance propagation on the spiking neural network for more efficient augmentation. Our proposed method has been evaluated on several SNN structures, achieving state-of-the-art performance in object recognition tasks including N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, as well as action recognition task SL-Animals with an accuracy of 91.59%. Our code is available at https://github.com/myuansun/EventRPG.","sentences":["Event camera, a novel bio-inspired vision sensor, has drawn a lot of attention for its low latency, low power consumption, and high dynamic range.","Currently, overfitting remains a critical problem in event-based classification tasks for Spiking Neural Network (SNN) due to its relatively weak spatial representation capability.","Data augmentation is a simple but efficient method to alleviate overfitting and improve the generalization ability of neural networks, and saliency-based augmentation methods are proven to be effective in the image processing field.","However, there is no approach available for extracting saliency maps from SNNs.","Therefore, for the first time, we present Spiking Layer-Time-wise Relevance Propagation rule (SLTRP) and Spiking Layer-wise Relevance Propagation rule (SLRP) in order for SNN to generate stable and accurate CAMs and saliency maps.","Based on this, we propose EventRPG, which leverages relevance propagation on the spiking neural network for more efficient augmentation.","Our proposed method has been evaluated on several SNN structures, achieving state-of-the-art performance in object recognition tasks including N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, as well as action recognition task SL-Animals with an accuracy of 91.59%.","Our code is available at https://github.com/myuansun/EventRPG."],"url":"http://arxiv.org/abs/2403.09274v1","category":"cs.CV"}
{"created":"2024-03-14 10:49:16","title":"Global Shipyard Capacities Limiting the Ramp-Up of Global Hydrogen-based Transportation","abstract":"Decarbonizing the global energy system requires significant expansions of renewable energy technologies. Given that cost-effective renewable sources are not necessarily situated in proximity to the largest energy demand centers globally, the maritime transportation of low-carbon energy carriers, such as renewable-based hydrogen or ammonia, will be needed. However, whether existent shipyards possess the required capacity to provide the necessary global fleet has not yet been answered. Therefore, this study estimates global tanker demand based on projections for global hydrogen demand, while comparing these projections with historic shipyard production. Our findings reveal a potential bottleneck until 2033-2039 if relying on liquefied hydrogen exclusively. This bottleneck could be circumvented by increasing local hydrogen production, utilizing pipelines, or liquefied ammonia as an energy carrier for hydrogen. Furthermore, the regional concentration of shipyard locations raises concerns about diversification. Increasing demand for container vessels could substantially hinder the scale-up of maritime hydrogen transport.","sentences":["Decarbonizing the global energy system requires significant expansions of renewable energy technologies.","Given that cost-effective renewable sources are not necessarily situated in proximity to the largest energy demand centers globally, the maritime transportation of low-carbon energy carriers, such as renewable-based hydrogen or ammonia, will be needed.","However, whether existent shipyards possess the required capacity to provide the necessary global fleet has not yet been answered.","Therefore, this study estimates global tanker demand based on projections for global hydrogen demand, while comparing these projections with historic shipyard production.","Our findings reveal a potential bottleneck until 2033-2039 if relying on liquefied hydrogen exclusively.","This bottleneck could be circumvented by increasing local hydrogen production, utilizing pipelines, or liquefied ammonia as an energy carrier for hydrogen.","Furthermore, the regional concentration of shipyard locations raises concerns about diversification.","Increasing demand for container vessels could substantially hinder the scale-up of maritime hydrogen transport."],"url":"http://arxiv.org/abs/2403.09272v1","category":"econ.GN"}
{"created":"2024-03-14 10:47:01","title":"A Deep Reinforcement Learning Approach for Autonomous Reconfigurable Intelligent Surfaces","abstract":"A reconfigurable intelligent surface (RIS) is a prospective wireless technology that enhances wireless channel quality. An RIS is often equipped with passive array of elements and provides cost and power-efficient solutions for coverage extension of wireless communication systems. Without any radio frequency (RF) chains or computing resources, however, the RIS requires control information to be sent to it from an external unit, e.g., a base station (BS). The control information can be delivered by wired or wireless channels, and the BS must be aware of the RIS and the RIS-related channel conditions in order to effectively configure its behavior. Recent works have introduced hybrid RIS structures possessing a few active elements that can sense and digitally process received data. Here, we propose the operation of an entirely autonomous RIS that operates without a control link between the RIS and BS. Using a few sensing elements, the autonomous RIS employs a deep Q network (DQN) based on reinforcement learning in order to enhance the sum rate of the network. Our results illustrate the potential of deploying autonomous RISs in wireless networks with essentially no network overhead.","sentences":["A reconfigurable intelligent surface (RIS) is a prospective wireless technology that enhances wireless channel quality.","An RIS is often equipped with passive array of elements and provides cost and power-efficient solutions for coverage extension of wireless communication systems.","Without any radio frequency (RF) chains or computing resources, however, the RIS requires control information to be sent to it from an external unit, e.g., a base station (BS).","The control information can be delivered by wired or wireless channels, and the BS must be aware of the RIS and the RIS-related channel conditions in order to effectively configure its behavior.","Recent works have introduced hybrid RIS structures possessing a few active elements that can sense and digitally process received data.","Here, we propose the operation of an entirely autonomous RIS that operates without a control link between the RIS and BS.","Using a few sensing elements, the autonomous RIS employs a deep Q network (DQN) based on reinforcement learning in order to enhance the sum rate of the network.","Our results illustrate the potential of deploying autonomous RISs in wireless networks with essentially no network overhead."],"url":"http://arxiv.org/abs/2403.09270v1","category":"cs.IT"}
{"created":"2024-03-14 10:46:45","title":"Direct observation of nanometer-scale orbital angular momentum accumulation","abstract":"Conversion of charge to orbital angular momentum through the orbital Hall effect (OHE) holds transformative potential for the development of orbital-based electronics, however, it is challenging to directly observe the electrically generated orbital accumulation. Here, we detect the OHE by directly quantifying the orbital accumulation along the edges of a titanium thin film using a scanning transmission electron microscope. We measure the Ti L-edge using electron energy-loss spectroscopy with nanometer resolution and find a sizable orbital accumulation at the sample's outer perimeters, consistent with all signatures expected for the OHE, and determine an orbital diffusion length $\\ell_o \\approx 7.3$ nm. Our data points to a surprising dependence of the orbital diffusion length on the nano-structural morphology.","sentences":["Conversion of charge to orbital angular momentum through the orbital Hall effect (OHE) holds transformative potential for the development of orbital-based electronics, however, it is challenging to directly observe the electrically generated orbital accumulation.","Here, we detect the OHE by directly quantifying the orbital accumulation along the edges of a titanium thin film using a scanning transmission electron microscope.","We measure the Ti L-edge using electron energy-loss spectroscopy with nanometer resolution and find a sizable orbital accumulation at the sample's outer perimeters, consistent with all signatures expected for the OHE, and determine an orbital diffusion length $\\ell_o \\approx 7.3$ nm.","Our data points to a surprising dependence of the orbital diffusion length on the nano-structural morphology."],"url":"http://arxiv.org/abs/2403.09269v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-14 10:41:31","title":"Zonal vs. Nodal Pricing: An Analysis of Different Pricing Rules in the German Day-Ahead Market","abstract":"The European electricity market is based on large pricing zones with a uniform day-ahead price. The energy transition leads to shifts in supply and demand and increasing redispatch costs. In an attempt to ensure efficient market clearing and congestion management, the EU Commission has mandated the Bidding Zone Review (BZR) to reevaluate the configuration of European bidding zones. Based on a unique data set published in the context of the BZR, we compare various pricing rules for the German power market. We compare market clearing and pricing for national, zonal, and nodal models, including their generation costs and associated redispatch costs. Moreover, we investigate different non-uniform pricing rules and their economic implications for the German electricity market. Our results indicate that the differences in the average prices in different zones are small. The total costs across different configurations are similar and the reduction of standard deviations in prices is also small based on this data set. A nodal pricing rule leads to the lowest total costs. We also analyze the quality of different pricing rules and their differences with respect to the quality of the price signals and the necessary uplift payments. While the study focuses on Germany, the analysis is relevant beyond and feeds into the broader discussion about pricing rules.","sentences":["The European electricity market is based on large pricing zones with a uniform day-ahead price.","The energy transition leads to shifts in supply and demand and increasing redispatch costs.","In an attempt to ensure efficient market clearing and congestion management, the EU Commission has mandated the Bidding Zone Review (BZR) to reevaluate the configuration of European bidding zones.","Based on a unique data set published in the context of the BZR, we compare various pricing rules for the German power market.","We compare market clearing and pricing for national, zonal, and nodal models, including their generation costs and associated redispatch costs.","Moreover, we investigate different non-uniform pricing rules and their economic implications for the German electricity market.","Our results indicate that the differences in the average prices in different zones are small.","The total costs across different configurations are similar and the reduction of standard deviations in prices is also small based on this data set.","A nodal pricing rule leads to the lowest total costs.","We also analyze the quality of different pricing rules and their differences with respect to the quality of the price signals and the necessary uplift payments.","While the study focuses on Germany, the analysis is relevant beyond and feeds into the broader discussion about pricing rules."],"url":"http://arxiv.org/abs/2403.09265v1","category":"econ.GN"}
{"created":"2024-03-14 10:35:27","title":"Hadamard property of the Unruh state for massless fermions on Kerr spacetime : the large $a$ case","abstract":"In a recent paper by G\\'erard, H\\\"afner, and Wrochna, the Unruh state for massless fermions on a Kerr spacetime was constructed and the authors showed its Hadmard property in the case of very slowly rotating black holes $\\vert a\\vert\\ll M$. In this note, we extend this result to the full non extreme case $\\vert a\\vert<M$.","sentences":["In a recent paper by G\\'erard, H\\\"afner, and Wrochna, the Unruh state for massless fermions on a Kerr spacetime was constructed and the authors showed its Hadmard property in the case of very slowly rotating black holes $\\vert a\\vert\\ll M$. In this note, we extend this result to the full non extreme case $\\vert a\\vert<M$."],"url":"http://arxiv.org/abs/2403.09261v1","category":"math-ph"}
{"created":"2024-03-14 10:22:01","title":"Gun Culture in Fringe Social Media","abstract":"The increasing frequency of mass shootings in the United States has, unfortunately, become a norm. While the issue of gun control in the US involves complex legal concerns, there are also societal issues at play. One such social issue is so-called \"gun culture,\" i.e., a general set of beliefs and actions related to gun ownership. However relatively little is known about gun culture, and even less is known when it comes to fringe online communities. This is especially worrying considering the aforementioned rise in mass shootings and numerous instances of shooters being radicalized online.   To address this gap, we explore gun culture on /k/, 4chan's weapons board. More specifically, using a variety of quantitative techniques, we examine over 4M posts on /k/ and position their discussion within the larger body of theoretical understanding of gun culture. Among other things, our findings suggest that gun culture on /k/ covers a relatively diverse set of topics (with a particular focus on legal discussion), some of which are signals of fetishism.","sentences":["The increasing frequency of mass shootings in the United States has, unfortunately, become a norm.","While the issue of gun control in the US involves complex legal concerns, there are also societal issues at play.","One such social issue is so-called \"gun culture,\" i.e., a general set of beliefs and actions related to gun ownership.","However relatively little is known about gun culture, and even less is known when it comes to fringe online communities.","This is especially worrying considering the aforementioned rise in mass shootings and numerous instances of shooters being radicalized online.   ","To address this gap, we explore gun culture on /k/, 4chan's weapons board.","More specifically, using a variety of quantitative techniques, we examine over 4M posts on /k/ and position their discussion within the larger body of theoretical understanding of gun culture.","Among other things, our findings suggest that gun culture on /k/","covers a relatively diverse set of topics (with a particular focus on legal discussion), some of which are signals of fetishism."],"url":"http://arxiv.org/abs/2403.09254v1","category":"cs.SI"}
{"created":"2024-03-14 10:21:39","title":"Broadband NIR photon upconversion generates NIR persistent luminescence for bioimaging","abstract":"Upconversion persistent luminescence (UCPL) phosphors that can be directly charged by near-infrared (NIR) light have gained considerable attention due to their promising applications ranging from photonics to biomedicine. However, current lanthanide-based UCPL phosphors show small absorption cross-sections and low upconversion charging efficiency. The development of UCPL phosphors faces challenges of lacking flexible upconversion charging pathways and poor design flexibility. Herein, we discovered a new lattice defect-mediated broadband photon upconversion process and the accompanied NIR-to-NIR UCPL in Cr-doped zinc gallate nanoparticles. The zinc gallate nanoparticles can be directly activated by broadband NIR light in the 700-1000 nm range to produce persistent luminescence at about 700 nm, which is also readily enhanced by rationally tailoring the lattice defects in the phosphors. This proposed UCPL phosphors achieved a signal-to-background ratio of over 200 in bioimaging by efficiently avoiding interference from autofluorescence and light scattering. Our findings reported the lattice defect-mediated photon upconversion for the first time, which significantly expanded the horizons for the flexible design of NIR-to-NIR UCPL phosphors toward broad applications.","sentences":["Upconversion persistent luminescence (UCPL) phosphors that can be directly charged by near-infrared (NIR) light have gained considerable attention due to their promising applications ranging from photonics to biomedicine.","However, current lanthanide-based UCPL phosphors show small absorption cross-sections and low upconversion charging efficiency.","The development of UCPL phosphors faces challenges of lacking flexible upconversion charging pathways and poor design flexibility.","Herein, we discovered a new lattice defect-mediated broadband photon upconversion process and the accompanied NIR-to-NIR UCPL in Cr-doped zinc gallate nanoparticles.","The zinc gallate nanoparticles can be directly activated by broadband NIR light in the 700-1000 nm range to produce persistent luminescence at about 700 nm, which is also readily enhanced by rationally tailoring the lattice defects in the phosphors.","This proposed UCPL phosphors achieved a signal-to-background ratio of over 200 in bioimaging by efficiently avoiding interference from autofluorescence and light scattering.","Our findings reported the lattice defect-mediated photon upconversion for the first time, which significantly expanded the horizons for the flexible design of NIR-to-NIR UCPL phosphors toward broad applications."],"url":"http://arxiv.org/abs/2403.09253v1","category":"physics.optics"}
{"created":"2024-03-14 10:20:28","title":"Reverse em-problem based on Bregman divergence and its application to classical and quantum information theory","abstract":"The recent paper (IEEE Trans. IT 69, 1680) introduced an analytical method for calculating the channel capacity without the need for iteration. This method has certain limitations that restrict its applicability. Furthermore, the paper does not provide an explanation as to why the channel capacity can be solved analytically in this particular case. In order to broaden the scope of this method and address its limitations, we turn our attention to the reverse em-problem, proposed by Toyota (Information Geometry, 3, 1355 (2020)). This reverse em-problem involves iteratively applying the inverse map of the em iteration to calculate the channel capacity, which represents the maximum mutual information. However, several open problems remained unresolved in Toyota's work. To overcome these challenges, we formulate the reverse em-problem based on Bregman divergence and provide solutions to these open problems. Building upon these results, we transform the reverse em-problem into em-problems and derive a non-iterative formula for the reverse em-problem. This formula can be viewed as a generalization of the aforementioned analytical calculation method. Importantly, this derivation sheds light on the information geometrical structure underlying this special case. By effectively addressing the limitations of the previous analytical method and providing a deeper understanding of the underlying information geometrical structure, our work significantly expands the applicability of the proposed method for calculating the channel capacity without iteration.","sentences":["The recent paper (IEEE Trans.","IT 69, 1680) introduced an analytical method for calculating the channel capacity without the need for iteration.","This method has certain limitations that restrict its applicability.","Furthermore, the paper does not provide an explanation as to why the channel capacity can be solved analytically in this particular case.","In order to broaden the scope of this method and address its limitations, we turn our attention to the reverse em-problem, proposed by Toyota (Information Geometry, 3, 1355 (2020)).","This reverse em-problem involves iteratively applying the inverse map of the em iteration to calculate the channel capacity, which represents the maximum mutual information.","However, several open problems remained unresolved in Toyota's work.","To overcome these challenges, we formulate the reverse em-problem based on Bregman divergence and provide solutions to these open problems.","Building upon these results, we transform the reverse em-problem into em-problems and derive a non-iterative formula for the reverse em-problem.","This formula can be viewed as a generalization of the aforementioned analytical calculation method.","Importantly, this derivation sheds light on the information geometrical structure underlying this special case.","By effectively addressing the limitations of the previous analytical method and providing a deeper understanding of the underlying information geometrical structure, our work significantly expands the applicability of the proposed method for calculating the channel capacity without iteration."],"url":"http://arxiv.org/abs/2403.09252v1","category":"cs.IT"}
{"created":"2024-03-14 10:16:57","title":"Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem","abstract":"Recent advancements in the flexible job-shop scheduling problem (FJSSP) are primarily based on deep reinforcement learning (DRL) due to its ability to generate high-quality, real-time solutions. However, DRL approaches often fail to fully harness the strengths of existing techniques such as exact methods or constraint programming (CP), which can excel at finding optimal or near-optimal solutions for smaller instances. This paper aims to integrate CP within a deep learning (DL) based methodology, leveraging the benefits of both. In this paper, we introduce a method that involves training a DL model using optimal solutions generated by CP, ensuring the model learns from high-quality data, thereby eliminating the need for the extensive exploration typical in DRL and enhancing overall performance. Further, we integrate CP into our DL framework to jointly construct solutions, utilizing DL for the initial complex stages and transitioning to CP for optimal resolution as the problem is simplified. Our hybrid approach has been extensively tested on three public FJSSP benchmarks, demonstrating superior performance over five state-of-the-art DRL approaches and a widely-used CP solver. Additionally, with the objective of exploring the application to other combinatorial optimization problems, promising preliminary results are presented on applying our hybrid approach to the traveling salesman problem, combining an exact method with a well-known DRL method.","sentences":["Recent advancements in the flexible job-shop scheduling problem (FJSSP) are primarily based on deep reinforcement learning (DRL) due to its ability to generate high-quality, real-time solutions.","However, DRL approaches often fail to fully harness the strengths of existing techniques such as exact methods or constraint programming (CP), which can excel at finding optimal or near-optimal solutions for smaller instances.","This paper aims to integrate CP within a deep learning (DL) based methodology, leveraging the benefits of both.","In this paper, we introduce a method that involves training a DL model using optimal solutions generated by CP, ensuring the model learns from high-quality data, thereby eliminating the need for the extensive exploration typical in DRL and enhancing overall performance.","Further, we integrate CP into our DL framework to jointly construct solutions, utilizing DL for the initial complex stages and transitioning to CP for optimal resolution as the problem is simplified.","Our hybrid approach has been extensively tested on three public FJSSP benchmarks, demonstrating superior performance over five state-of-the-art DRL approaches and a widely-used CP solver.","Additionally, with the objective of exploring the application to other combinatorial optimization problems, promising preliminary results are presented on applying our hybrid approach to the traveling salesman problem, combining an exact method with a well-known DRL method."],"url":"http://arxiv.org/abs/2403.09249v1","category":"cs.AI"}
{"created":"2024-03-14 10:16:02","title":"Eulerian magnitude homology: subgraph structure and random graphs","abstract":"In this paper we explore the connection between the ranks of the magnitude homology groups of a graph and the structure of its subgraphs. To this end, we introduce variants of magnitude homology called eulerian magnitude homology and discriminant magnitude homology. Leveraging the combinatorics of the differential in magnitude homology, we illustrate a close relationship between the ranks of the eulerian magnitude homology groups on the first diagonal and counts of subgraphs which fall in specific classes. We leverage these tools to study limiting behavior of the eulerian magnitude homology groups for Erdos-Renyi random graphs and random geometric graphs, producing for both models a vanishing threshold for the eulerian magnitude homology groups on the first diagonal. This in turn provides a characterization of the generators for the corresponding magnitude homology groups. Finally, we develop an explicit asymptotic estimate the expected rank of eulerian magnitude homology along the first diagonal for these random graph models.","sentences":["In this paper we explore the connection between the ranks of the magnitude homology groups of a graph and the structure of its subgraphs.","To this end, we introduce variants of magnitude homology called eulerian magnitude homology and discriminant magnitude homology.","Leveraging the combinatorics of the differential in magnitude homology, we illustrate a close relationship between the ranks of the eulerian magnitude homology groups on the first diagonal and counts of subgraphs which fall in specific classes.","We leverage these tools to study limiting behavior of the eulerian magnitude homology groups for Erdos-Renyi random graphs and random geometric graphs, producing for both models a vanishing threshold for the eulerian magnitude homology groups on the first diagonal.","This in turn provides a characterization of the generators for the corresponding magnitude homology groups.","Finally, we develop an explicit asymptotic estimate the expected rank of eulerian magnitude homology along the first diagonal for these random graph models."],"url":"http://arxiv.org/abs/2403.09248v1","category":"math.CO"}
{"created":"2024-03-14 10:13:40","title":"On the speed of propagation in Turing patterns for reaction-diffusion systems","abstract":"This study investigates transient wave dynamics in Turing pattern formation, focusing on waves emerging from localised disturbances. While the traditional focus of diffusion-driven instability has primarily centred on stationary solutions, considerable attention has also been directed towards understanding spatio-temporal behaviours, particularly the propagation of patterning from localised disturbances. We analyse these waves of patterning using both the well-established marginal stability criterion and weakly nonlinear analysis with envelope equations. Both methods provide estimates for the wave speed but the latter method, in addition, approximates the wave profile and amplitude. We then compare these two approaches analytically near a bifurcation point and reveal that the marginal stability criterion yields exactly the same estimate for the wave speed as the weakly nonlinear analysis. Furthermore, we evaluate these estimates against numerical results for Schnakenberg and CDIMA (chlorine dioxide-iodine-malonic acid) kinetics. In particular, our study emphasises the importance of the characteristic speed of pattern propagation, determined by diffusion dynamics and a complex relation with the reaction kinetics in Turing systems. This speed serves as a vital parameter for comparison with experimental observations, akin to observed pattern length scales. Furthermore, more generally, our findings provide systematic methodologies for analysing transient wave properties in Turing systems, generating insight into the dynamic evolution of pattern formation.","sentences":["This study investigates transient wave dynamics in Turing pattern formation, focusing on waves emerging from localised disturbances.","While the traditional focus of diffusion-driven instability has primarily centred on stationary solutions, considerable attention has also been directed towards understanding spatio-temporal behaviours, particularly the propagation of patterning from localised disturbances.","We analyse these waves of patterning using both the well-established marginal stability criterion and weakly nonlinear analysis with envelope equations.","Both methods provide estimates for the wave speed but the latter method, in addition, approximates the wave profile and amplitude.","We then compare these two approaches analytically near a bifurcation point and reveal that the marginal stability criterion yields exactly the same estimate for the wave speed as the weakly nonlinear analysis.","Furthermore, we evaluate these estimates against numerical results for Schnakenberg and CDIMA (chlorine dioxide-iodine-malonic acid) kinetics.","In particular, our study emphasises the importance of the characteristic speed of pattern propagation, determined by diffusion dynamics and a complex relation with the reaction kinetics in Turing systems.","This speed serves as a vital parameter for comparison with experimental observations, akin to observed pattern length scales.","Furthermore, more generally, our findings provide systematic methodologies for analysing transient wave properties in Turing systems, generating insight into the dynamic evolution of pattern formation."],"url":"http://arxiv.org/abs/2403.09247v1","category":"nlin.PS"}
{"created":"2024-03-14 10:06:11","title":"A simple reconstruction method to infer nonreciprocal interactions and local driving in complex systems","abstract":"Data-based inference of directed interactions in complex dynamical systems is a problem common to many disciplines of science. In this work, we study networks of spatially separate dynamical entities, which could represent physical systems that interact with each other by reciprocal or nonreciprocal, instantaneous or time-delayed interactions. We present a simple approach that combines Markov state models with directed information-theoretical measures for causal inference that can accurately infer the underlying interactions from noisy time series of the dynamical system states alone. Remarkably, this is possible despite the built-in simplification of a Markov assumption and the choice of a very coarse discretization at the level of probability estimation. Our test systems are an Ising chain with nonreciprocal coupling imposed by local driving of a single spin, and a system of delay-coupled linear stochastic processes. Stepping away from physical systems, the approach infers cause-effect relationships, or more generally, the direction of mutual or one-way influence. The presented method is agnostic to the number of interacting entities and details of the dynamics, so that it is widely applicable to problems in various fields.","sentences":["Data-based inference of directed interactions in complex dynamical systems is a problem common to many disciplines of science.","In this work, we study networks of spatially separate dynamical entities, which could represent physical systems that interact with each other by reciprocal or nonreciprocal, instantaneous or time-delayed interactions.","We present a simple approach that combines Markov state models with directed information-theoretical measures for causal inference that can accurately infer the underlying interactions from noisy time series of the dynamical system states alone.","Remarkably, this is possible despite the built-in simplification of a Markov assumption and the choice of a very coarse discretization at the level of probability estimation.","Our test systems are an Ising chain with nonreciprocal coupling imposed by local driving of a single spin, and a system of delay-coupled linear stochastic processes.","Stepping away from physical systems, the approach infers cause-effect relationships, or more generally, the direction of mutual or one-way influence.","The presented method is agnostic to the number of interacting entities and details of the dynamics, so that it is widely applicable to problems in various fields."],"url":"http://arxiv.org/abs/2403.09243v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-14 10:03:58","title":"XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via Controllable Diffusion Model","abstract":"Large-scale generative models have demonstrated impressive capacity in producing visually compelling images, with increasing applications in medical imaging. However, they continue to grapple with the challenge of image hallucination and the generation of anatomically inaccurate outputs. These limitations are mainly due to the sole reliance on textual inputs and lack of spatial control over the generated images, hindering the potential usefulness of such models in real-life settings. We present XReal, a novel controllable diffusion model for generating realistic chest X-ray images through precise anatomy and pathology location control. Our lightweight method can seamlessly integrate spatial control in a pre-trained text-to-image diffusion model without fine-tuning, retaining its existing knowledge while enhancing its generation capabilities. XReal outperforms state-of-the-art x-ray diffusion models in quantitative and qualitative metrics while showing 13% and 10% anatomy and pathology realism gain, respectively, based on the expert radiologist evaluation. Our model holds promise for advancing generative models in medical imaging, offering greater precision and adaptability while inviting further exploration in this evolving field. A large synthetically generated data with annotations and code is publicly available at https://github.com/BioMedIA-MBZUAI/XReal.","sentences":["Large-scale generative models have demonstrated impressive capacity in producing visually compelling images, with increasing applications in medical imaging.","However, they continue to grapple with the challenge of image hallucination and the generation of anatomically inaccurate outputs.","These limitations are mainly due to the sole reliance on textual inputs and lack of spatial control over the generated images, hindering the potential usefulness of such models in real-life settings.","We present XReal, a novel controllable diffusion model for generating realistic chest X-ray images through precise anatomy and pathology location control.","Our lightweight method can seamlessly integrate spatial control in a pre-trained text-to-image diffusion model without fine-tuning, retaining its existing knowledge while enhancing its generation capabilities.","XReal outperforms state-of-the-art x-ray diffusion models in quantitative and qualitative metrics while showing 13% and 10% anatomy and pathology realism gain, respectively, based on the expert radiologist evaluation.","Our model holds promise for advancing generative models in medical imaging, offering greater precision and adaptability while inviting further exploration in this evolving field.","A large synthetically generated data with annotations and code is publicly available at https://github.com/BioMedIA-MBZUAI/XReal."],"url":"http://arxiv.org/abs/2403.09240v1","category":"eess.IV"}
{"created":"2024-03-14 09:59:55","title":"Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph","abstract":"Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models. However, current progress often neglects the intricate high-order correlation of geometry and texture within 3D objects, leading to challenges such as over-smoothness, over-saturation and the Janus problem. In this work, we propose a method named ``3D Gaussian Generation via Hypergraph (Hyper-3DG)'', designed to capture the sophisticated high-order correlations present within 3D objects. Our framework is anchored by a well-established mainflow and an essential module, named ``Geometry and Texture Hypergraph Refiner (HGRefiner)''. This module not only refines the representation of 3D Gaussians but also accelerates the update process of these 3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and latent visual features. Our framework allows for the production of finely generated 3D objects within a cohesive optimization, effectively circumventing degradation. Extensive experimentation has shown that our proposed method significantly enhances the quality of 3D generation while incurring no additional computational overhead for the underlying framework. (Project code: https://github.com/yjhboy/Hyper3DG)","sentences":["Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models.","However, current progress often neglects the intricate high-order correlation of geometry and texture within 3D objects, leading to challenges such as over-smoothness, over-saturation and the Janus problem.","In this work, we propose a method named ``3D Gaussian Generation via Hypergraph (Hyper-3DG)'', designed to capture the sophisticated high-order correlations present within 3D objects.","Our framework is anchored by a well-established mainflow and an essential module, named ``Geometry and Texture Hypergraph Refiner (HGRefiner)''.","This module not only refines the representation of 3D Gaussians but also accelerates the update process of these 3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and latent visual features.","Our framework allows for the production of finely generated 3D objects within a cohesive optimization, effectively circumventing degradation.","Extensive experimentation has shown that our proposed method significantly enhances the quality of 3D generation while incurring no additional computational overhead for the underlying framework.","(Project code: https://github.com/yjhboy/Hyper3DG)"],"url":"http://arxiv.org/abs/2403.09236v1","category":"cs.CV"}
{"created":"2024-03-14 09:57:50","title":"Infrared structure beyond locality in electrodynamics","abstract":"The infrared problems of quantum electrodynamics, in contrast to ultraviolet difficulties which are of technical nature, are related to fundamental, conceptual physical questions, such as: what is a charged particle, is the particle interpretation of the electromagnetic field complete, does a vacuum state exist, or what is the quantum status of long range degrees of freedom. On the calculational level, the standard local formulations of quantum field theory have achieved procedures to deal with infinities related to long range correlations. However, the answers to the conceptual questions formulated above, based on the locality paradigm, do not seem to be fully convincing, which is confirmed by the fact that no canonical picture did emerge. This contribution briefly characterizes perspectives which open with an admission of nonlocal variables residing in infinity, or at the boundary of spacetime after compactification. Recently, this line of investigation gains popularity.","sentences":["The infrared problems of quantum electrodynamics, in contrast to ultraviolet difficulties which are of technical nature, are related to fundamental, conceptual physical questions, such as: what is a charged particle, is the particle interpretation of the electromagnetic field complete, does a vacuum state exist, or what is the quantum status of long range degrees of freedom.","On the calculational level, the standard local formulations of quantum field theory have achieved procedures to deal with infinities related to long range correlations.","However, the answers to the conceptual questions formulated above, based on the locality paradigm, do not seem to be fully convincing, which is confirmed by the fact that no canonical picture did emerge.","This contribution briefly characterizes perspectives which open with an admission of nonlocal variables residing in infinity, or at the boundary of spacetime after compactification.","Recently, this line of investigation gains popularity."],"url":"http://arxiv.org/abs/2403.09234v1","category":"math-ph"}
{"created":"2024-03-14 09:56:35","title":"Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes","abstract":"In recent years, various machine and deep learning architectures have been successfully introduced to the field of predictive process analytics. Nevertheless, the inherent opacity of these algorithms poses a significant challenge for human decision-makers, hindering their ability to understand the reasoning behind the predictions. This growing concern has sparked the introduction of counterfactual explanations, designed as human-understandable what if scenarios, to provide clearer insights into the decision-making process behind undesirable predictions. The generation of counterfactual explanations, however, encounters specific challenges when dealing with the sequential nature of the (business) process cases typically used in predictive process analytics. Our paper tackles this challenge by introducing a data-driven approach, REVISEDplus, to generate more feasible and plausible counterfactual explanations. First, we restrict the counterfactual algorithm to generate counterfactuals that lie within a high-density region of the process data, ensuring that the proposed counterfactuals are realistic and feasible within the observed process data distribution. Additionally, we ensure plausibility by learning sequential patterns between the activities in the process cases, utilising Declare language templates. Finally, we evaluate the properties that define the validity of counterfactuals.","sentences":["In recent years, various machine and deep learning architectures have been successfully introduced to the field of predictive process analytics.","Nevertheless, the inherent opacity of these algorithms poses a significant challenge for human decision-makers, hindering their ability to understand the reasoning behind the predictions.","This growing concern has sparked the introduction of counterfactual explanations, designed as human-understandable what if scenarios, to provide clearer insights into the decision-making process behind undesirable predictions.","The generation of counterfactual explanations, however, encounters specific challenges when dealing with the sequential nature of the (business) process cases typically used in predictive process analytics.","Our paper tackles this challenge by introducing a data-driven approach, REVISEDplus, to generate more feasible and plausible counterfactual explanations.","First, we restrict the counterfactual algorithm to generate counterfactuals that lie within a high-density region of the process data, ensuring that the proposed counterfactuals are realistic and feasible within the observed process data distribution.","Additionally, we ensure plausibility by learning sequential patterns between the activities in the process cases, utilising Declare language templates.","Finally, we evaluate the properties that define the validity of counterfactuals."],"url":"http://arxiv.org/abs/2403.09232v1","category":"cs.AI"}
{"created":"2024-03-14 09:54:31","title":"Improving Distant 3D Object Detection Using 2D Box Supervision","abstract":"Improving the detection of distant 3d objects is an important yet challenging task. For camera-based 3D perception, the annotation of 3d bounding relies heavily on LiDAR for accurate depth information. As such, the distance of annotation is often limited due to the sparsity of LiDAR points on distant objects, which hampers the capability of existing detectors for long-range scenarios. We address this challenge by considering only 2D box supervision for distant objects since they are easy to annotate. We propose LR3D, a framework that learns to recover the missing depth of distant objects. LR3D adopts an implicit projection head to learn the generation of mapping between 2D boxes and depth using the 3D supervision on close objects. This mapping allows the depth estimation of distant objects conditioned on their 2D boxes, making long-range 3D detection with 2D supervision feasible. Experiments show that without distant 3D annotations, LR3D allows camera-based methods to detect distant objects (over 200m) with comparable accuracy to full 3D supervision. Our framework is general, and could widely benefit 3D detection methods to a large extent.","sentences":["Improving the detection of distant 3d objects is an important yet challenging task.","For camera-based 3D perception, the annotation of 3d bounding relies heavily on LiDAR for accurate depth information.","As such, the distance of annotation is often limited due to the sparsity of LiDAR points on distant objects, which hampers the capability of existing detectors for long-range scenarios.","We address this challenge by considering only 2D box supervision for distant objects since they are easy to annotate.","We propose LR3D, a framework that learns to recover the missing depth of distant objects.","LR3D adopts an implicit projection head to learn the generation of mapping between 2D boxes and depth using the 3D supervision on close objects.","This mapping allows the depth estimation of distant objects conditioned on their 2D boxes, making long-range 3D detection with 2D supervision feasible.","Experiments show that without distant 3D annotations, LR3D allows camera-based methods to detect distant objects (over 200m) with comparable accuracy to full 3D supervision.","Our framework is general, and could widely benefit 3D detection methods to a large extent."],"url":"http://arxiv.org/abs/2403.09230v1","category":"cs.CV"}
{"created":"2024-03-14 09:48:36","title":"BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation","abstract":"We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics. BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on \"what do you want robots to do for you?\". The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 9,000 objects annotated with rich physical and semantic properties. The second is OMNIGIBSON, a novel simulation environment that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids. Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions. To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research. Project website: https://behavior.stanford.edu.","sentences":["We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics.","BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on \"what do you want robots to do for you?\".","The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 9,000 objects annotated with rich physical and semantic properties.","The second is OMNIGIBSON, a novel simulation environment that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids.","Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions.","To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart.","We hope that BEHAVIOR-1K's human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research.","Project website: https://behavior.stanford.edu."],"url":"http://arxiv.org/abs/2403.09227v1","category":"cs.RO"}
{"created":"2024-03-14 09:45:05","title":"Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records","abstract":"Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical terminology and the need for complex SQL queries. Here, we introduce an end-to-end methodology that combines text-to-SQL generation with retrieval augmented generation (RAG) to answer epidemiological questions using EHR and claims data. We show that our approach, which integrates a medical coding step into the text-to-SQL process, significantly improves the performance over simple prompting. Our findings indicate that although current language models are not yet sufficiently accurate for unsupervised use, RAG offers a promising direction for improving their capabilities, as shown in a realistic industry setting.","sentences":["Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization.","Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical terminology and the need for complex SQL queries.","Here, we introduce an end-to-end methodology that combines text-to-SQL generation with retrieval augmented generation (RAG) to answer epidemiological questions using EHR and claims data.","We show that our approach, which integrates a medical coding step into the text-to-SQL process, significantly improves the performance over simple prompting.","Our findings indicate that although current language models are not yet sufficiently accurate for unsupervised use, RAG offers a promising direction for improving their capabilities, as shown in a realistic industry setting."],"url":"http://arxiv.org/abs/2403.09226v1","category":"cs.CL"}
{"created":"2024-03-14 09:43:24","title":"PWACG: Partial Wave Analysis Code Generator supporting Newton-conjugate gradient method","abstract":"This paper introduces a novel Partial Wave Analysis Code Generator (PWACG) that automatically generates high-performance partial wave analysis codes. This is achieved by leveraging the JAX automatic differentiation library and the jinja2 template engine. The resulting code is constructed using the high-performance API of JAX, and includes support for the Newton's Conjugate Gradient optimization method, as well as the full utilization of parallel computing capabilities offered by GPUs. By harnessing these advanced computing techniques, PWACG demonstrates a significant advantage in efficiently identifying global optimal points compared to conventional partial wave analysis software packages.","sentences":["This paper introduces a novel Partial Wave Analysis Code Generator (PWACG) that automatically generates high-performance partial wave analysis codes.","This is achieved by leveraging the JAX automatic differentiation library and the jinja2 template engine.","The resulting code is constructed using the high-performance API of JAX, and includes support for the Newton's Conjugate Gradient optimization method, as well as the full utilization of parallel computing capabilities offered by GPUs.","By harnessing these advanced computing techniques, PWACG demonstrates a significant advantage in efficiently identifying global optimal points compared to conventional partial wave analysis software packages."],"url":"http://arxiv.org/abs/2403.09225v1","category":"physics.comp-ph"}
{"created":"2024-03-14 09:43:23","title":"A new approach towards quantum foundation and some consequences","abstract":"A general theory based upon 6 postulates is introduced. The basical notions are theoretical variables that are associated with an observer or with a group of communicating observers. These variables may be accessible or inaccessible. From these postulates, the ordinary formalism of quantum theory are derived. The mathematical derivations are not given in this article, but I refer to the recent articles [9, 10]. Three possible applications of the general theory can be given; 1) The variables may decision variables connected to the decisions of a person or of a group of persons. 2) The variables may be statistical parameters or future data, But most importantly here: 3) The variables are physical variables in some context. This last application gives a completely new foundation of quantum mechanics, a foundation which in my opinion is much more easy to understand than the ordinary formalism.The other applications seem also to give interesting consequences of the approach. Socalled paradoxes like that of Schr\\\"odinger's cat can be clarified under the theory. Explanations of the outcomes of David Bohm's version of the EPR experiment and of the Bell experiment are provided. Finally, references to links towards relativity theory and to quantum field theory are given.","sentences":["A general theory based upon 6 postulates is introduced.","The basical notions are theoretical variables that are associated with an observer or with a group of communicating observers.","These variables may be accessible or inaccessible.","From these postulates, the ordinary formalism of quantum theory are derived.","The mathematical derivations are not given in this article, but I refer to the recent articles","[9, 10].","Three possible applications of the general theory can be given; 1) The variables may decision variables connected to the decisions of a person or of a group of persons.","2)","The variables may be statistical parameters or future data, But most importantly here: 3)","The variables are physical variables in some context.","This last application gives a completely new foundation of quantum mechanics, a foundation which in my opinion is much more easy to understand than the ordinary formalism.","The other applications seem also to give interesting consequences of the approach.","Socalled paradoxes like that of Schr\\\"odinger's cat can be clarified under the theory.","Explanations of the outcomes of David Bohm's version of the EPR experiment and of the Bell experiment are provided.","Finally, references to links towards relativity theory and to quantum field theory are given."],"url":"http://arxiv.org/abs/2403.09224v1","category":"quant-ph"}
{"created":"2024-03-14 09:43:07","title":"MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer","abstract":"The massive generation of time-series data by largescale Internet of Things (IoT) devices necessitates the exploration of more effective models for multivariate time-series forecasting. In previous models, there was a predominant use of the Channel Dependence (CD) strategy (where each channel represents a univariate sequence). Current state-of-the-art (SOTA) models primarily rely on the Channel Independence (CI) strategy. The CI strategy treats all channels as a single channel, expanding the dataset to improve generalization performance and avoiding inter-channel correlation that disrupts long-term features. However, the CI strategy faces the challenge of interchannel correlation forgetting. To address this issue, we propose an innovative Mixed Channels strategy, combining the data expansion advantages of the CI strategy with the ability to counteract inter-channel correlation forgetting. Based on this strategy, we introduce MCformer, a multivariate time-series forecasting model with mixed channel features. The model blends a specific number of channels, leveraging an attention mechanism to effectively capture inter-channel correlation information when modeling long-term features. Experimental results demonstrate that the Mixed Channels strategy outperforms pure CI strategy in multivariate time-series forecasting tasks.","sentences":["The massive generation of time-series data by largescale Internet of Things (IoT) devices necessitates the exploration of more effective models for multivariate time-series forecasting.","In previous models, there was a predominant use of the Channel Dependence (CD) strategy (where each channel represents a univariate sequence).","Current state-of-the-art (SOTA) models primarily rely on the Channel Independence (CI) strategy.","The CI strategy treats all channels as a single channel, expanding the dataset to improve generalization performance and avoiding inter-channel correlation that disrupts long-term features.","However, the CI strategy faces the challenge of interchannel correlation forgetting.","To address this issue, we propose an innovative Mixed Channels strategy, combining the data expansion advantages of the CI strategy with the ability to counteract inter-channel correlation forgetting.","Based on this strategy, we introduce MCformer, a multivariate time-series forecasting model with mixed channel features.","The model blends a specific number of channels, leveraging an attention mechanism to effectively capture inter-channel correlation information when modeling long-term features.","Experimental results demonstrate that the Mixed Channels strategy outperforms pure CI strategy in multivariate time-series forecasting tasks."],"url":"http://arxiv.org/abs/2403.09223v1","category":"cs.LG"}
{"created":"2024-03-14 09:38:39","title":"Demonstration of universal contextuality through communication games free of both operational inequivalence and compatibility loopholes","abstract":"Universal contextuality is the leading notion of non-classicality even for single systems, showing its advantage as a more general quantum correlation than Bell non-locality, as well as preparation contextuality. However, a loophole-free experimental demonstration of universal contextuality at least requires that both operational inequivalence and compatibility loopholes are closed, which have never been simultaneously achieved to date. In our work, we experimentally test universal contextuality through (3,3) and (4,3) communication games, simultaneously restoring operational equivalence and circumventing the compatibility loophole. Our result exhibits the violation of universal non-contextuality bound by 97 standard deviations in (3,3) scenario, and 107 deviations in (4,3) scenario. Notably there are states which exhibit locality but reveal universal contextuality in both two scenarios. In addition, our result shows that universal contextuality is more general than preparation contextuality in (3,3) scenario, while equivalent to preparation contextuality in (4,3) scenario.","sentences":["Universal contextuality is the leading notion of non-classicality even for single systems, showing its advantage as a more general quantum correlation than Bell non-locality, as well as preparation contextuality.","However, a loophole-free experimental demonstration of universal contextuality at least requires that both operational inequivalence and compatibility loopholes are closed, which have never been simultaneously achieved to date.","In our work, we experimentally test universal contextuality through (3,3) and (4,3) communication games, simultaneously restoring operational equivalence and circumventing the compatibility loophole.","Our result exhibits the violation of universal non-contextuality bound by 97 standard deviations in (3,3) scenario, and 107 deviations in (4,3) scenario.","Notably there are states which exhibit locality but reveal universal contextuality in both two scenarios.","In addition, our result shows that universal contextuality is more general than preparation contextuality in (3,3) scenario, while equivalent to preparation contextuality in (4,3) scenario."],"url":"http://arxiv.org/abs/2403.09220v1","category":"quant-ph"}
{"created":"2024-03-14 09:37:54","title":"An Extensive Comparison of Static Application Security Testing Tools","abstract":"Context: Static Application Security Testing Tools (SASTTs) identify software vulnerabilities to support the security and reliability of software applications. Interestingly, several studies have suggested that alternative solutions may be more effective than SASTTs due to their tendency to generate false alarms, commonly referred to as low Precision. Aim: We aim to comprehensively evaluate SASTTs, setting a reliable benchmark for assessing and finding gaps in vulnerability identification mechanisms based on SASTTs or alternatives. Method: Our SASTTs evaluation is based on a controlled, though synthetic, Java codebase. It involves an assessment of 1.5 million test executions, and it features innovative methodological features such as effort-aware accuracy metrics and method-level analysis. Results: Our findings reveal that SASTTs detect a tiny range of vulnerabilities. In contrast to prevailing wisdom, SASTTs exhibit high Precision while falling short in Recall. Conclusions: The paper suggests that enhancing Recall, alongside expanding the spectrum of detected vulnerability types, should be the primary focus for improving SASTTs or alternative approaches, such as machine learning-based vulnerability identification solutions.","sentences":["Context: Static Application Security Testing Tools (SASTTs) identify software vulnerabilities to support the security and reliability of software applications.","Interestingly, several studies have suggested that alternative solutions may be more effective than SASTTs due to their tendency to generate false alarms, commonly referred to as low Precision.","Aim: We aim to comprehensively evaluate SASTTs, setting a reliable benchmark for assessing and finding gaps in vulnerability identification mechanisms based on SASTTs or alternatives.","Method: Our SASTTs evaluation is based on a controlled, though synthetic, Java codebase.","It involves an assessment of 1.5 million test executions, and it features innovative methodological features such as effort-aware accuracy metrics and method-level analysis.","Results: Our findings reveal that SASTTs detect a tiny range of vulnerabilities.","In contrast to prevailing wisdom, SASTTs exhibit high Precision while falling short in Recall.","Conclusions: The paper suggests that enhancing Recall, alongside expanding the spectrum of detected vulnerability types, should be the primary focus for improving SASTTs or alternative approaches, such as machine learning-based vulnerability identification solutions."],"url":"http://arxiv.org/abs/2403.09219v1","category":"cs.SE"}
{"created":"2024-03-14 09:28:28","title":"On the Laplace Approximation as Model Selection Criterion for Gaussian Processes","abstract":"Model selection aims to find the best model in terms of accuracy, interpretability or simplicity, preferably all at once. In this work, we focus on evaluating model performance of Gaussian process models, i.e. finding a metric that provides the best trade-off between all those criteria. While previous work considers metrics like the likelihood, AIC or dynamic nested sampling, they either lack performance or have significant runtime issues, which severely limits applicability. We address these challenges by introducing multiple metrics based on the Laplace approximation, where we overcome a severe inconsistency occuring during naive application of the Laplace approximation. Experiments show that our metrics are comparable in quality to the gold standard dynamic nested sampling without compromising for computational speed. Our model selection criteria allow significantly faster and high quality model selection of Gaussian process models.","sentences":["Model selection aims to find the best model in terms of accuracy, interpretability or simplicity, preferably all at once.","In this work, we focus on evaluating model performance of Gaussian process models, i.e. finding a metric that provides the best trade-off between all those criteria.","While previous work considers metrics like the likelihood, AIC or dynamic nested sampling, they either lack performance or have significant runtime issues, which severely limits applicability.","We address these challenges by introducing multiple metrics based on the Laplace approximation, where we overcome a severe inconsistency occuring during naive application of the Laplace approximation.","Experiments show that our metrics are comparable in quality to the gold standard dynamic nested sampling without compromising for computational speed.","Our model selection criteria allow significantly faster and high quality model selection of Gaussian process models."],"url":"http://arxiv.org/abs/2403.09215v1","category":"cs.LG"}
{"created":"2024-03-14 09:28:12","title":"PoIFusion: Multi-Modal 3D Object Detection via Fusion at Points of Interest","abstract":"In this work, we present PoIFusion, a simple yet effective multi-modal 3D object detection framework to fuse the information of RGB images and LiDAR point clouds at the point of interest (abbreviated as PoI). Technically, our PoIFusion follows the paradigm of query-based object detection, formulating object queries as dynamic 3D boxes. The PoIs are adaptively generated from each query box on the fly, serving as the keypoints to represent a 3D object and play the role of basic units in multi-modal fusion. Specifically, we project PoIs into the view of each modality to sample the corresponding feature and integrate the multi-modal features at each PoI through a dynamic fusion block. Furthermore, the features of PoIs derived from the same query box are aggregated together to update the query feature. Our approach prevents information loss caused by view transformation and eliminates the computation-intensive global attention, making the multi-modal 3D object detector more applicable. We conducted extensive experiments on the nuScenes dataset to evaluate our approach. Remarkably, our PoIFusion achieves 74.9\\% NDS and 73.4\\% mAP, setting a state-of-the-art record on the multi-modal 3D object detection benchmark. Codes will be made available via \\url{https://djiajunustc.github.io/projects/poifusion}.","sentences":["In this work, we present PoIFusion, a simple yet effective multi-modal 3D object detection framework to fuse the information of RGB images and LiDAR point clouds at the point of interest (abbreviated as PoI).","Technically, our PoIFusion follows the paradigm of query-based object detection, formulating object queries as dynamic 3D boxes.","The PoIs are adaptively generated from each query box on the fly, serving as the keypoints to represent a 3D object and play the role of basic units in multi-modal fusion.","Specifically, we project PoIs into the view of each modality to sample the corresponding feature and integrate the multi-modal features at each PoI through a dynamic fusion block.","Furthermore, the features of PoIs derived from the same query box are aggregated together to update the query feature.","Our approach prevents information loss caused by view transformation and eliminates the computation-intensive global attention, making the multi-modal 3D object detector more applicable.","We conducted extensive experiments on the nuScenes dataset to evaluate our approach.","Remarkably, our PoIFusion achieves 74.9\\% NDS and 73.4\\% mAP, setting a state-of-the-art record on the multi-modal 3D object detection benchmark.","Codes will be made available via \\url{https://djiajunustc.github.io/projects/poifusion}."],"url":"http://arxiv.org/abs/2403.09212v1","category":"cs.CV"}
{"created":"2024-03-14 09:24:00","title":"Energy flux and waveforms by coalescing spinless binary system in effective one-body theory","abstract":"We present a study on the energy radiation rate and waveforms of the gravitational wave generated by coalescing spinless binary systems up to the third post-Minkowskian approximation in the effective one-body theory. To derive an analytical expansion of the null tetrad components of the gravitational perturbed Weyl tensor $\\varPsi_{4}$ in the effective spacetime, we utilize the method proposed by Sasaki $et$ $al.$ During this investigation, we discover more general integral formulas that provide a theoretical framework for computing the results in any order. Subsequently, we successfully compute the energy radiation rate and waveforms of the gravitational wave, which include the results of the Schwarzschild case and the correction terms resulting from the dimensionless parameters $a_{2}$ and $a_{3}$ in the effective metric.","sentences":["We present a study on the energy radiation rate and waveforms of the gravitational wave generated by coalescing spinless binary systems up to the third post-Minkowskian approximation in the effective one-body theory.","To derive an analytical expansion of the null tetrad components of the gravitational perturbed Weyl tensor $\\varPsi_{4}$ in the effective spacetime, we utilize the method proposed by Sasaki $et$ $al.$ During this investigation, we discover more general integral formulas that provide a theoretical framework for computing the results in any order.","Subsequently, we successfully compute the energy radiation rate and waveforms of the gravitational wave, which include the results of the Schwarzschild case and the correction terms resulting from the dimensionless parameters $a_{2}$ and $a_{3}$ in the effective metric."],"url":"http://arxiv.org/abs/2403.09211v1","category":"gr-qc"}
{"created":"2024-03-14 09:22:17","title":"LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection","abstract":"Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences. Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day). However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results. On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss. In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN. Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between activities across sequences with graph structure learning. Moreover, to mitigate the data imbalance problem in ITD, we propose a novel hybrid prediction loss, which integrates self-supervision signals {from normal activities} and supervision signals from abnormal activities into a unified loss for anomaly detection. We evaluate the performance of LAN on two widely used datasets, i.e., CERT r4.2 and CERT r5.2. Extensive and comparative experiments demonstrate the superiority of LAN, outperforming 9 state-of-the-art baselines by at least 9.92% and 6.35% in AUC for real-time ITD on CERT r4.2 and r5.2, respectively. Moreover, LAN can be also applied to post-hoc ITD, surpassing 8 competitive baselines by at least 7.70% and 4.03% in AUC on two datasets. Finally, the ablation study, parameter analysis, and compatibility analysis evaluate the impact of each module and hyper-parameter in LAN.","sentences":["Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences.","Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day).","However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results.","On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss.","In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN.","Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between activities across sequences with graph structure learning.","Moreover, to mitigate the data imbalance problem in ITD, we propose a novel hybrid prediction loss, which integrates self-supervision signals {from normal activities} and supervision signals from abnormal activities into a unified loss for anomaly detection.","We evaluate the performance of LAN on two widely used datasets, i.e., CERT r4.2 and CERT r5.2.","Extensive and comparative experiments demonstrate the superiority of LAN, outperforming 9 state-of-the-art baselines by at least 9.92% and 6.35% in AUC for real-time ITD on CERT r4.2 and r5.2, respectively.","Moreover, LAN can be also applied to post-hoc ITD, surpassing 8 competitive baselines by at least 7.70% and 4.03% in AUC on two datasets.","Finally, the ablation study, parameter analysis, and compatibility analysis evaluate the impact of each module and hyper-parameter in LAN."],"url":"http://arxiv.org/abs/2403.09209v1","category":"cs.CR"}
{"created":"2024-03-14 09:19:50","title":"Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM","abstract":"Concept Bottleneck Model (CBM) is a methods for explaining neural networks. In CBM, concepts which correspond to reasons of outputs are inserted in the last intermediate layer as observed values. It is expected that we can interpret the relationship between the output and concept similar to linear regression. However, this interpretation requires observing all concepts and decreases the generalization performance of neural networks. Partial CBM (PCBM), which uses partially observed concepts, has been devised to resolve these difficulties. Although some numerical experiments suggest that the generalization performance of PCBMs is almost as high as that of the original neural networks, the theoretical behavior of its generalization error has not been yet clarified since PCBM is singular statistical model. In this paper, we reveal the Bayesian generalization error in PCBM with a three-layered and linear architecture. The result indcates that the structure of partially observed concepts decreases the Bayesian generalization error compared with that of CBM (full-observed concepts).","sentences":["Concept Bottleneck Model (CBM) is a methods for explaining neural networks.","In CBM, concepts which correspond to reasons of outputs are inserted in the last intermediate layer as observed values.","It is expected that we can interpret the relationship between the output and concept similar to linear regression.","However, this interpretation requires observing all concepts and decreases the generalization performance of neural networks.","Partial CBM (PCBM), which uses partially observed concepts, has been devised to resolve these difficulties.","Although some numerical experiments suggest that the generalization performance of PCBMs is almost as high as that of the original neural networks, the theoretical behavior of its generalization error has not been yet clarified since PCBM is singular statistical model.","In this paper, we reveal the Bayesian generalization error in PCBM with a three-layered and linear architecture.","The result indcates that the structure of partially observed concepts decreases the Bayesian generalization error compared with that of CBM (full-observed concepts)."],"url":"http://arxiv.org/abs/2403.09206v1","category":"stat.ML"}
{"created":"2024-03-14 09:18:12","title":"Remarks on integrability of N=1 supersymmetric Ruijsenaars-Schneider three-body models","abstract":"Integrability of N=1 supersymmetric Ruijsenaars-Schneider three-body models based upon the potentials W(x)=2/x, W(x)=2/sin(x), and W(x)=2/sinh(x) is proven. The problem of constructing an algebraically resolvable set of Grassmann-odd constants of motion is reduced to finding a triplet of vectors such that all their scalar products can be expressed in terms of the original bosonic first integrals. The supersymmetric generalizations are used to build novel integrable (iso)spin extensions of the respective Ruijsenaars-Schneider three-body systems.","sentences":["Integrability of N=1 supersymmetric Ruijsenaars-Schneider three-body models based upon the potentials W(x)=2/x, W(x)=2/sin(x), and W(x)=2/sinh(x) is proven.","The problem of constructing an algebraically resolvable set of Grassmann-odd constants of motion is reduced to finding a triplet of vectors such that all their scalar products can be expressed in terms of the original bosonic first integrals.","The supersymmetric generalizations are used to build novel integrable (iso)spin extensions of the respective Ruijsenaars-Schneider three-body systems."],"url":"http://arxiv.org/abs/2403.09204v1","category":"nlin.SI"}
{"created":"2024-03-14 09:13:51","title":"Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation","abstract":"Recently, foundation models trained on massive datasets to adapt to a wide range of domains have attracted considerable attention and are actively being explored within the computer vision community. Among these, the Segment Anything Model (SAM) stands out for its remarkable progress in generalizability and flexibility for image segmentation tasks, achieved through prompt-based object mask generation. However, despite its strength, SAM faces two key limitations when applied to customized instance segmentation that segments specific objects or those in unique environments not typically present in the training data: 1) the ambiguity inherent in input prompts and 2) the necessity for extensive additional training to achieve optimal segmentation. To address these challenges, we propose a novel method, customized instance segmentation via prompt learning tailored to SAM. Our method involves a prompt learning module (PLM), which adjusts input prompts into the embedding space to better align with user intentions, thereby enabling more efficient training. Furthermore, we introduce a point matching module (PMM) to enhance the feature representation for finer segmentation by ensuring detailed alignment with ground truth boundaries. Experimental results on various customized instance segmentation scenarios demonstrate the effectiveness of the proposed method.","sentences":["Recently, foundation models trained on massive datasets to adapt to a wide range of domains have attracted considerable attention and are actively being explored within the computer vision community.","Among these, the Segment Anything Model (SAM) stands out for its remarkable progress in generalizability and flexibility for image segmentation tasks, achieved through prompt-based object mask generation.","However, despite its strength, SAM faces two key limitations when applied to customized instance segmentation that segments specific objects or those in unique environments not typically present in the training data: 1) the ambiguity inherent in input prompts and 2) the necessity for extensive additional training to achieve optimal segmentation.","To address these challenges, we propose a novel method, customized instance segmentation via prompt learning tailored to SAM.","Our method involves a prompt learning module (PLM), which adjusts input prompts into the embedding space to better align with user intentions, thereby enabling more efficient training.","Furthermore, we introduce a point matching module (PMM) to enhance the feature representation for finer segmentation by ensuring detailed alignment with ground truth boundaries.","Experimental results on various customized instance segmentation scenarios demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.09199v1","category":"cs.CV"}
{"created":"2024-03-14 09:09:15","title":"MetroGNN: Metro Network Expansion with Reinforcement Learning","abstract":"Selecting urban regions for metro network expansion to meet maximal transportation demands is crucial for urban development, while computationally challenging to solve. The expansion process relies not only on complicated features like urban demographics and origin-destination (OD) flow but is also constrained by the existing metro network and urban geography. In this paper, we introduce a reinforcement learning framework to address a Markov decision process within an urban heterogeneous multi-graph. Our approach employs an attentive policy network that intelligently selects nodes based on information captured by a graph neural network. Experiments on real-world urban data demonstrate that our proposed methodology substantially improve the satisfied transportation demands by over 30\\% when compared with state-of-the-art methods. Codes are published at https://github.com/tsinghua-fib-lab/MetroGNN.","sentences":["Selecting urban regions for metro network expansion to meet maximal transportation demands is crucial for urban development, while computationally challenging to solve.","The expansion process relies not only on complicated features like urban demographics and origin-destination (OD) flow but is also constrained by the existing metro network and urban geography.","In this paper, we introduce a reinforcement learning framework to address a Markov decision process within an urban heterogeneous multi-graph.","Our approach employs an attentive policy network that intelligently selects nodes based on information captured by a graph neural network.","Experiments on real-world urban data demonstrate that our proposed methodology substantially improve the satisfied transportation demands by over 30\\% when compared with state-of-the-art methods.","Codes are published at https://github.com/tsinghua-fib-lab/MetroGNN."],"url":"http://arxiv.org/abs/2403.09197v1","category":"cs.CY"}
{"created":"2024-03-14 09:09:06","title":"Noise Dimension of GAN: An Image Compression Perspective","abstract":"Generative adversial network (GAN) is a type of generative model that maps a high-dimensional noise to samples in target distribution. However, the dimension of noise required in GAN is not well understood. Previous approaches view GAN as a mapping from a continuous distribution to another continous distribution. In this paper, we propose to view GAN as a discrete sampler instead. From this perspective, we build a connection between the minimum noise required and the bits to losslessly compress the images. Furthermore, to understand the behaviour of GAN when noise dimension is limited, we propose divergence-entropy trade-off. This trade-off depicts the best divergence we can achieve when noise is limited. And as rate distortion trade-off, it can be numerically solved when source distribution is known. Finally, we verifies our theory with experiments on image generation.","sentences":["Generative adversial network (GAN) is a type of generative model that maps a high-dimensional noise to samples in target distribution.","However, the dimension of noise required in GAN is not well understood.","Previous approaches view GAN as a mapping from a continuous distribution to another continous distribution.","In this paper, we propose to view GAN as a discrete sampler instead.","From this perspective, we build a connection between the minimum noise required and the bits to losslessly compress the images.","Furthermore, to understand the behaviour of GAN when noise dimension is limited, we propose divergence-entropy trade-off.","This trade-off depicts the best divergence we can achieve when noise is limited.","And as rate distortion trade-off, it can be numerically solved when source distribution is known.","Finally, we verifies our theory with experiments on image generation."],"url":"http://arxiv.org/abs/2403.09196v1","category":"cs.CV"}
{"created":"2024-03-14 09:07:34","title":"SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash Attention to Achieve 30 times Acceleration","abstract":"Segment Anything Model (SAM) has garnered significant attention in segmentation tasks due to their zero-shot generalization ability. However, a broader application of SAMs to real-world practice has been restricted by their low inference speed and high computational memory demands, which mainly stem from the attention mechanism. Existing work concentrated on optimizing the encoder, yet has not adequately addressed the inefficiency of the attention mechanism itself, even when distilled to a smaller model, which thus leaves space for further improvement. In response, we introduce SAM-Lightening, a variant of SAM, that features a re-engineered attention mechanism, termed Dilated Flash Attention. It not only facilitates higher parallelism, enhancing processing efficiency but also retains compatibility with the existing FlashAttention. Correspondingly, we propose a progressive distillation to enable an efficient knowledge transfer from the vanilla SAM without costly training from scratch. Experiments on COCO and LVIS reveal that SAM-Lightening significantly outperforms the state-of-the-art methods in both run-time efficiency and segmentation accuracy. Specifically, it can achieve an inference speed of 7 milliseconds (ms) per image, for images of size 1024*1024 pixels, which is 30.1 times faster than the vanilla SAM and 2.1 times than the state-of-the-art. Moreover, it takes only 244MB memory, which is 3.5\\% of the vanilla SAM. The code and weights are available at https://anonymous.4open.science/r/SAM-LIGHTENING-BC25/.","sentences":["Segment Anything Model (SAM) has garnered significant attention in segmentation tasks due to their zero-shot generalization ability.","However, a broader application of SAMs to real-world practice has been restricted by their low inference speed and high computational memory demands, which mainly stem from the attention mechanism.","Existing work concentrated on optimizing the encoder, yet has not adequately addressed the inefficiency of the attention mechanism itself, even when distilled to a smaller model, which thus leaves space for further improvement.","In response, we introduce SAM-Lightening, a variant of SAM, that features a re-engineered attention mechanism, termed Dilated Flash Attention.","It not only facilitates higher parallelism, enhancing processing efficiency but also retains compatibility with the existing FlashAttention.","Correspondingly, we propose a progressive distillation to enable an efficient knowledge transfer from the vanilla SAM without costly training from scratch.","Experiments on COCO and LVIS reveal that SAM-Lightening significantly outperforms the state-of-the-art methods in both run-time efficiency and segmentation accuracy.","Specifically, it can achieve an inference speed of 7 milliseconds (ms) per image, for images of size 1024*1024 pixels, which is 30.1 times faster than the vanilla SAM and 2.1 times than the state-of-the-art.","Moreover, it takes only 244MB memory, which is 3.5\\% of the vanilla SAM.","The code and weights are available at https://anonymous.4open.science/r/SAM-LIGHTENING-BC25/."],"url":"http://arxiv.org/abs/2403.09195v1","category":"cs.CV"}
{"created":"2024-03-14 09:07:31","title":"Intention-driven Ego-to-Exo Video Generation","abstract":"Ego-to-exo video generation refers to generating the corresponding exocentric video according to the egocentric video, providing valuable applications in AR/VR and embodied AI. Benefiting from advancements in diffusion model techniques, notable progress has been achieved in video generation. However, existing methods build upon the spatiotemporal consistency assumptions between adjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to drastic changes in views. To this end, this paper proposes an Intention-Driven Ego-to-exo video generation framework (IDE) that leverages action intention consisting of human movement and action description as view-independent representation to guide video generation, preserving the consistency of content and motion. Specifically, the egocentric head trajectory is first estimated through multi-view stereo matching. Then, cross-view feature perception module is introduced to establish correspondences between exo- and ego- views, guiding the trajectory transformation module to infer human full-body movement from the head trajectory. Meanwhile, we present an action description unit that maps the action semantics into the feature space consistent with the exocentric image. Finally, the inferred human movement and high-level action descriptions jointly guide the generation of exocentric motion and interaction content (i.e., corresponding optical flow and occlusion maps) in the backward process of the diffusion model, ultimately warping them into the corresponding exocentric video. We conduct extensive experiments on the relevant dataset with diverse exo-ego video pairs, and our IDE outperforms state-of-the-art models in both subjective and objective assessments, demonstrating its efficacy in ego-to-exo video generation.","sentences":["Ego-to-exo video generation refers to generating the corresponding exocentric video according to the egocentric video, providing valuable applications in AR/VR and embodied AI.","Benefiting from advancements in diffusion model techniques, notable progress has been achieved in video generation.","However, existing methods build upon the spatiotemporal consistency assumptions between adjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to drastic changes in views.","To this end, this paper proposes an Intention-Driven Ego-to-exo video generation framework (IDE) that leverages action intention consisting of human movement and action description as view-independent representation to guide video generation, preserving the consistency of content and motion.","Specifically, the egocentric head trajectory is first estimated through multi-view stereo matching.","Then, cross-view feature perception module is introduced to establish correspondences between exo- and ego- views, guiding the trajectory transformation module to infer human full-body movement from the head trajectory.","Meanwhile, we present an action description unit that maps the action semantics into the feature space consistent with the exocentric image.","Finally, the inferred human movement and high-level action descriptions jointly guide the generation of exocentric motion and interaction content (i.e., corresponding optical flow and occlusion maps) in the backward process of the diffusion model, ultimately warping them into the corresponding exocentric video.","We conduct extensive experiments on the relevant dataset with diverse exo-ego video pairs, and our IDE outperforms state-of-the-art models in both subjective and objective assessments, demonstrating its efficacy in ego-to-exo video generation."],"url":"http://arxiv.org/abs/2403.09194v1","category":"cs.CV"}
{"created":"2024-03-14 09:07:14","title":"Are Vision Language Models Texture or Shape Biased and Can We Steer Them?","abstract":"Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text in multimodal models. If text does indeed influence visual biases, this suggests that we may be able to steer visual biases not just through visual input but also through language: a hypothesis that we confirm through extensive experiments. For instance, we are able to steer shape bias from as low as 49% to as high as 72% through prompting alone. For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs.","sentences":["Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering.","Unlike pure vision models, they offer an intuitive way to access visual content through language prompting.","The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models.","One important visual bias is the texture vs. shape bias, or the dominance of local over global information.","In this paper, we study this bias in a wide range of popular VLMs.","Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text in multimodal models.","If text does indeed influence visual biases, this suggests that we may be able to steer visual biases not just through visual input but also through language: a hypothesis that we confirm through extensive experiments.","For instance, we are able to steer shape bias from as low as 49% to as high as 72% through prompting alone.","For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs."],"url":"http://arxiv.org/abs/2403.09193v1","category":"cs.CV"}
{"created":"2024-03-14 09:05:25","title":"Intention-aware Denoising Diffusion Model for Trajectory Prediction","abstract":"Trajectory prediction is an essential component in autonomous driving, particularly for collision avoidance systems. Considering the inherent uncertainty of the task, numerous studies have utilized generative models to produce multiple plausible future trajectories for each agent. However, most of them suffer from restricted representation ability or unstable training issues. To overcome these limitations, we propose utilizing the diffusion model to generate the distribution of future trajectories. Two cruxes are to be settled to realize such an idea. First, the diversity of intention is intertwined with the uncertain surroundings, making the true distribution hard to parameterize. Second, the diffusion process is time-consuming during the inference phase, rendering it unrealistic to implement in a real-time driving system. We propose an Intention-aware denoising Diffusion Model (IDM), which tackles the above two problems. We decouple the original uncertainty into intention uncertainty and action uncertainty and model them with two dependent diffusion processes. To decrease the inference time, we reduce the variable dimensions in the intention-aware diffusion process and restrict the initial distribution of the action-aware diffusion process, which leads to fewer diffusion steps. To validate our approach, we conduct experiments on the Stanford Drone Dataset (SDD) and ETH/UCY dataset. Our methods achieve state-of-the-art results, with an FDE of 13.83 pixels on the SDD dataset and 0.36 meters on the ETH/UCY dataset. Compared with the original diffusion model, IDM reduces inference time by two-thirds. Interestingly, our experiments further reveal that introducing intention information is beneficial in modeling the diffusion process of fewer steps.","sentences":["Trajectory prediction is an essential component in autonomous driving, particularly for collision avoidance systems.","Considering the inherent uncertainty of the task, numerous studies have utilized generative models to produce multiple plausible future trajectories for each agent.","However, most of them suffer from restricted representation ability or unstable training issues.","To overcome these limitations, we propose utilizing the diffusion model to generate the distribution of future trajectories.","Two cruxes are to be settled to realize such an idea.","First, the diversity of intention is intertwined with the uncertain surroundings, making the true distribution hard to parameterize.","Second, the diffusion process is time-consuming during the inference phase, rendering it unrealistic to implement in a real-time driving system.","We propose an Intention-aware denoising Diffusion Model (IDM), which tackles the above two problems.","We decouple the original uncertainty into intention uncertainty and action uncertainty and model them with two dependent diffusion processes.","To decrease the inference time, we reduce the variable dimensions in the intention-aware diffusion process and restrict the initial distribution of the action-aware diffusion process, which leads to fewer diffusion steps.","To validate our approach, we conduct experiments on the Stanford Drone Dataset (SDD) and ETH/UCY dataset.","Our methods achieve state-of-the-art results, with an FDE of 13.83 pixels on the SDD dataset and 0.36 meters on the ETH/UCY dataset.","Compared with the original diffusion model, IDM reduces inference time by two-thirds.","Interestingly, our experiments further reveal that introducing intention information is beneficial in modeling the diffusion process of fewer steps."],"url":"http://arxiv.org/abs/2403.09190v1","category":"cs.CV"}
{"created":"2024-03-14 08:59:22","title":"Quantum Dynamic Programming","abstract":"We introduce a quantum extension of dynamic programming, a fundamental computational method for efficiently solving recursive problems using memory. Our innovation lies in showing how to coherently generate unitaries of recursion steps using memorized intermediate quantum states. We find that quantum dynamic programming yields an exponential reduction in circuit depth for a large class of fixed-point quantum recursions, including a known recursive variant of the Grover's search. Additionally, we apply quantum dynamic programming to a recently proposed double-bracket quantum algorithm for diagonalization to obtain a new protocol for obliviously preparing a quantum state in its Schmidt basis, providing a potential pathway for revealing entanglement structures of unknown quantum states.","sentences":["We introduce a quantum extension of dynamic programming, a fundamental computational method for efficiently solving recursive problems using memory.","Our innovation lies in showing how to coherently generate unitaries of recursion steps using memorized intermediate quantum states.","We find that quantum dynamic programming yields an exponential reduction in circuit depth for a large class of fixed-point quantum recursions, including a known recursive variant of the Grover's search.","Additionally, we apply quantum dynamic programming to a recently proposed double-bracket quantum algorithm for diagonalization to obtain a new protocol for obliviously preparing a quantum state in its Schmidt basis, providing a potential pathway for revealing entanglement structures of unknown quantum states."],"url":"http://arxiv.org/abs/2403.09187v1","category":"quant-ph"}
{"created":"2024-03-14 08:59:05","title":"TangoSIDM Project: Is the Stellar Mass Tully-Fisher relation consistent with SIDM?","abstract":"Self-interacting dark matter (SIDM) has the potential to significantly influence galaxy formation in comparison to the cold, collisionless dark matter paradigm (CDM), resulting in observable effects. This study aims to elucidate this influence and to demonstrate that the stellar mass Tully-Fisher relation imposes robust constraints on the parameter space of velocity-dependent SIDM models. We present a new set of cosmological hydrodynamical simulations that include the SIDM scheme from the TangoSIDM project and the SWIFT-EAGLE galaxy formation model. Two cosmological simulations suites were generated: one (Reference model) which yields good agreement with the observed $z=0$ galaxy stellar mass function, galaxy mass-size relation, and stellar-to-halo mass relation; and another (WeakStellarFB model) in which the stellar feedback is less efficient, particularly for Milky Way-like systems. Both galaxy formation models were simulated under four dark matter cosmologies: CDM, SIDM with two different velocity-dependent cross sections, and SIDM with a constant cross section. While SIDM does not modify global galaxy properties such as stellar masses and star formation rates, it does make the galaxies more extended. In Milky Way-like galaxies, where baryons dominate the central gravitational potential, SIDM thermalises, causing dark matter to accumulate in the central regions. This accumulation results in density profiles that are steeper than those produced in CDM from adiabatic contraction. The enhanced dark matter density in the central regions of galaxies causes a deviation in the slope of the Tully-Fisher relation, which significantly diverges from the observational data. In contrast, the Tully-Fisher relation derived from CDM models aligns well with observations.","sentences":["Self-interacting dark matter (SIDM) has the potential to significantly influence galaxy formation in comparison to the cold, collisionless dark matter paradigm (CDM), resulting in observable effects.","This study aims to elucidate this influence and to demonstrate that the stellar mass Tully-Fisher relation imposes robust constraints on the parameter space of velocity-dependent SIDM models.","We present a new set of cosmological hydrodynamical simulations that include the SIDM scheme from the TangoSIDM project and the SWIFT-EAGLE galaxy formation model.","Two cosmological simulations suites were generated: one (Reference model) which yields good agreement with the observed $z=0$ galaxy stellar mass function, galaxy mass-size relation, and stellar-to-halo mass relation; and another (WeakStellarFB model) in which the stellar feedback is less efficient, particularly for Milky Way-like systems.","Both galaxy formation models were simulated under four dark matter cosmologies: CDM, SIDM with two different velocity-dependent cross sections, and SIDM with a constant cross section.","While SIDM does not modify global galaxy properties such as stellar masses and star formation rates, it does make the galaxies more extended.","In Milky Way-like galaxies, where baryons dominate the central gravitational potential, SIDM thermalises, causing dark matter to accumulate in the central regions.","This accumulation results in density profiles that are steeper than those produced in CDM from adiabatic contraction.","The enhanced dark matter density in the central regions of galaxies causes a deviation in the slope of the Tully-Fisher relation, which significantly diverges from the observational data.","In contrast, the Tully-Fisher relation derived from CDM models aligns well with observations."],"url":"http://arxiv.org/abs/2403.09186v1","category":"astro-ph.CO"}
{"created":"2024-03-14 08:58:36","title":"Synchronized states of power grids and oscillator networks by convex optimization","abstract":"Synchronization is essential for the operation of AC power systems: All generators in the power grid must rotate with fixed relative phases to enable a steady flow of electric power. Understanding the conditions for and the limitations of synchronization is of utmost practical importance. In this article, we propose a novel approach to compute and analyze the stable stationary states of a power grid or an oscillator network in terms of a convex optimization problem. This approach allows to systematically compute \\emph{all} stable states where the phase difference across an edge does not exceed $\\pi/2$.Furthermore, the optimization formulation allows to rigorously establish certain properties of synchronized states and to bound the error in the widely used linear power flow approximation.","sentences":["Synchronization is essential for the operation of AC power systems: All generators in the power grid must rotate with fixed relative phases to enable a steady flow of electric power.","Understanding the conditions for and the limitations of synchronization is of utmost practical importance.","In this article, we propose a novel approach to compute and analyze the stable stationary states of a power grid or an oscillator network in terms of a convex optimization problem.","This approach allows to systematically compute \\emph{all} stable states where the phase difference across an edge does not exceed $\\pi/2$.Furthermore, the optimization formulation allows to rigorously establish certain properties of synchronized states and to bound the error in the widely used linear power flow approximation."],"url":"http://arxiv.org/abs/2403.09185v1","category":"eess.SY"}
{"created":"2024-03-14 08:54:19","title":"Learning Algorithms for Verification of Markov Decision Processes","abstract":"We present a general framework for applying learning algorithms and heuristical guidance to the verification of Markov decision processes (MDPs), based on the ideas of Br\\'azdil, T. et al. (2014). Verification of Markov Decision Processes Using Learning Algorithms. The primary goal of the techniques presented in that work is to improve performance by avoiding an exhaustive exploration of the state space, guided by heuristics. This approach is significantly extended in this work. Several details of the base theory are refined and errors are fixed. Section 1.3 provides an overview of all differences.   The presented framework focuses on probabilistic reachability, which is a core problem in verification, and is instantiated in two distinct scenarios. The first assumes that full knowledge of the MDP is available, in particular precise transition probabilities. It performs a heuristic-driven partial exploration of the model, yielding precise lower and upper bounds on the required probability. The second tackles the case where we may only sample the MDP without knowing the exact transition dynamics. Here, we obtain probabilistic guarantees, again in terms of both the lower and upper bounds, which provides efficient stopping criteria for the approximation. In particular, the latter is an extension of statistical model-checking (SMC) for unbounded properties in MDPs. In contrast to other related approaches, we do not restrict our attention to time-bounded (finite-horizon) or discounted properties, nor assume any particular structural properties of the MDP.","sentences":["We present a general framework for applying learning algorithms and heuristical guidance to the verification of Markov decision processes (MDPs), based on the ideas of Br\\'azdil, T. et al. (2014).","Verification of Markov Decision Processes Using Learning Algorithms.","The primary goal of the techniques presented in that work is to improve performance by avoiding an exhaustive exploration of the state space, guided by heuristics.","This approach is significantly extended in this work.","Several details of the base theory are refined and errors are fixed.","Section 1.3 provides an overview of all differences.   ","The presented framework focuses on probabilistic reachability, which is a core problem in verification, and is instantiated in two distinct scenarios.","The first assumes that full knowledge of the MDP is available, in particular precise transition probabilities.","It performs a heuristic-driven partial exploration of the model, yielding precise lower and upper bounds on the required probability.","The second tackles the case where we may only sample the MDP without knowing the exact transition dynamics.","Here, we obtain probabilistic guarantees, again in terms of both the lower and upper bounds, which provides efficient stopping criteria for the approximation.","In particular, the latter is an extension of statistical model-checking (SMC) for unbounded properties in MDPs.","In contrast to other related approaches, we do not restrict our attention to time-bounded (finite-horizon) or discounted properties, nor assume any particular structural properties of the MDP."],"url":"http://arxiv.org/abs/2403.09184v1","category":"eess.SY"}
{"created":"2024-03-14 08:53:01","title":"Generalized Relevance Learning Grassmann Quantization","abstract":"Due to advancements in digital cameras, it is easy to gather multiple images (or videos) from an object under different conditions. Therefore, image-set classification has attracted more attention, and different solutions were proposed to model them. A popular way to model image sets is subspaces, which form a manifold called the Grassmann manifold. In this contribution, we extend the application of Generalized Relevance Learning Vector Quantization to deal with Grassmann manifold. The proposed model returns a set of prototype subspaces and a relevance vector. While prototypes model typical behaviours within classes, the relevance factors specify the most discriminative principal vectors (or images) for the classification task. They both provide insights into the model's decisions by highlighting influential images and pixels for predictions. Moreover, due to learning prototypes, the model complexity of the new method during inference is independent of dataset size, unlike previous works. We applied it to several recognition tasks including handwritten digit recognition, face recognition, activity recognition, and object recognition. Experiments demonstrate that it outperforms previous works with lower complexity and can successfully model the variation, such as handwritten style or lighting conditions. Moreover, the presence of relevances makes the model robust to the selection of subspaces' dimensionality.","sentences":["Due to advancements in digital cameras, it is easy to gather multiple images (or videos) from an object under different conditions.","Therefore, image-set classification has attracted more attention, and different solutions were proposed to model them.","A popular way to model image sets is subspaces, which form a manifold called the Grassmann manifold.","In this contribution, we extend the application of Generalized Relevance Learning Vector Quantization to deal with Grassmann manifold.","The proposed model returns a set of prototype subspaces and a relevance vector.","While prototypes model typical behaviours within classes, the relevance factors specify the most discriminative principal vectors (or images) for the classification task.","They both provide insights into the model's decisions by highlighting influential images and pixels for predictions.","Moreover, due to learning prototypes, the model complexity of the new method during inference is independent of dataset size, unlike previous works.","We applied it to several recognition tasks including handwritten digit recognition, face recognition, activity recognition, and object recognition.","Experiments demonstrate that it outperforms previous works with lower complexity and can successfully model the variation, such as handwritten style or lighting conditions.","Moreover, the presence of relevances makes the model robust to the selection of subspaces' dimensionality."],"url":"http://arxiv.org/abs/2403.09183v1","category":"cs.CV"}
{"created":"2024-03-14 08:43:43","title":"Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts","abstract":"Diffusion models have achieved remarkable success across a range of generative tasks. Recent efforts to enhance diffusion model architectures have reimagined them as a form of multi-task learning, where each task corresponds to a denoising task at a specific noise level. While these efforts have focused on parameter isolation and task routing, they fall short of capturing detailed inter-task relationships and risk losing semantic information, respectively. In response, we introduce Switch Diffusion Transformer (Switch-DiT), which establishes inter-task relationships between conflicting tasks without compromising semantic information. To achieve this, we employ a sparse mixture-of-experts within each transformer block to utilize semantic information and facilitate handling conflicts in tasks through parameter isolation. Additionally, we propose a diffusion prior loss, encouraging similar tasks to share their denoising paths while isolating conflicting ones. Through these, each transformer block contains a shared expert across all tasks, where the common and task-specific denoising paths enable the diffusion model to construct its beneficial way of synergizing denoising tasks. Extensive experiments validate the effectiveness of our approach in improving both image quality and convergence rate, and further analysis demonstrates that Switch-DiT constructs tailored denoising paths across various generation scenarios.","sentences":["Diffusion models have achieved remarkable success across a range of generative tasks.","Recent efforts to enhance diffusion model architectures have reimagined them as a form of multi-task learning, where each task corresponds to a denoising task at a specific noise level.","While these efforts have focused on parameter isolation and task routing, they fall short of capturing detailed inter-task relationships and risk losing semantic information, respectively.","In response, we introduce Switch Diffusion Transformer (Switch-DiT), which establishes inter-task relationships between conflicting tasks without compromising semantic information.","To achieve this, we employ a sparse mixture-of-experts within each transformer block to utilize semantic information and facilitate handling conflicts in tasks through parameter isolation.","Additionally, we propose a diffusion prior loss, encouraging similar tasks to share their denoising paths while isolating conflicting ones.","Through these, each transformer block contains a shared expert across all tasks, where the common and task-specific denoising paths enable the diffusion model to construct its beneficial way of synergizing denoising tasks.","Extensive experiments validate the effectiveness of our approach in improving both image quality and convergence rate, and further analysis demonstrates that Switch-DiT constructs tailored denoising paths across various generation scenarios."],"url":"http://arxiv.org/abs/2403.09176v1","category":"cs.CV"}
{"created":"2024-03-14 17:59:09","title":"On locally symmetric polynomial metrics: Riemannian and Finslerian surfaces","abstract":"In the paper we investigate locally symmetric polynomial metrics in special cases of Riemannian and Finslerian surfaces. The Riemannian case will be presented by a collection of basic results (regularity of second root metrics) and formulas up to Gauss curvature. In case of Finslerian surfaces we formulate necessary and sufficient conditions for a locally symmetric fourth root metric in 2D to be positive definite. They are given in terms of the coefficients of the polynomial metric to make checking the positive definiteness as simple and direct as possible. Explicit examples are also presented. The situation is more complicated in case of spaces of dimension more than two. Some necessary conditions and an explicit example are given for a positive definite locally symmetric polynomial metric in 3D. Computations are supported by the MAPLE mathematics software (LinearAlgebra).","sentences":["In the paper we investigate locally symmetric polynomial metrics in special cases of Riemannian and Finslerian surfaces.","The Riemannian case will be presented by a collection of basic results (regularity of second root metrics) and formulas up to Gauss curvature.","In case of Finslerian surfaces we formulate necessary and sufficient conditions for a locally symmetric fourth root metric in 2D to be positive definite.","They are given in terms of the coefficients of the polynomial metric to make checking the positive definiteness as simple and direct as possible.","Explicit examples are also presented.","The situation is more complicated in case of spaces of dimension more than two.","Some necessary conditions and an explicit example are given for a positive definite locally symmetric polynomial metric in 3D. Computations are supported by the MAPLE mathematics software (LinearAlgebra)."],"url":"http://arxiv.org/abs/2403.09633v1","category":"math.DG"}
{"created":"2024-03-14 17:52:17","title":"Localization in Digital Twin MIMO Networks: A Case for Massive Fingerprinting","abstract":"Localization in outdoor wireless systems typically requires transmitting specific reference signals to estimate distance (trilateration methods) or angle (triangulation methods). These cause overhead on communication, need a LoS link to work well, and require multiple base stations, often imposing synchronization or specific hardware requirements. Fingerprinting has none of these drawbacks, but building its database requires high human effort to collect real-world measurements. For a long time, this issue limited the size of databases and thus their performance. This work proposes significantly reducing human effort in building fingerprinting databases by populating them with \\textit{digital twin RF maps}. These RF maps are built from ray-tracing simulations on a digital replica of the environment across several frequency bands and beamforming configurations. Online user fingerprints are then matched against this spatial database. The approach was evaluated with practical simulations using realistic propagation models and user measurements. Our experiments show sub-meter localization errors on a NLoS location 95\\% of the time using sensible user measurement report sizes. Results highlight the promising potential of the proposed digital twin approach for ubiquitous wide-area 6G localization.","sentences":["Localization in outdoor wireless systems typically requires transmitting specific reference signals to estimate distance (trilateration methods) or angle (triangulation methods).","These cause overhead on communication, need a LoS link to work well, and require multiple base stations, often imposing synchronization or specific hardware requirements.","Fingerprinting has none of these drawbacks, but building its database requires high human effort to collect real-world measurements.","For a long time, this issue limited the size of databases and thus their performance.","This work proposes significantly reducing human effort in building fingerprinting databases by populating them with \\textit{digital twin RF maps}.","These RF maps are built from ray-tracing simulations on a digital replica of the environment across several frequency bands and beamforming configurations.","Online user fingerprints are then matched against this spatial database.","The approach was evaluated with practical simulations using realistic propagation models and user measurements.","Our experiments show sub-meter localization errors on a NLoS location 95\\% of the time using sensible user measurement report sizes.","Results highlight the promising potential of the proposed digital twin approach for ubiquitous wide-area 6G localization."],"url":"http://arxiv.org/abs/2403.09614v1","category":"cs.IT"}
{"created":"2024-03-14 17:00:29","title":"Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis","abstract":"The tremendous hype around autonomous driving is eagerly calling for emerging and novel technologies to support advanced mobility use cases. As car manufactures keep developing SAE level 3+ systems to improve the safety and comfort of passengers, traffic authorities need to establish new procedures to manage the transition from human-driven to fully-autonomous vehicles while providing a feedback-loop mechanism to fine-tune envisioned autonomous systems. Thus, a way to automatically profile autonomous vehicles and differentiate those from human-driven ones is a must. In this paper, we present a fully-fledged framework that monitors active vehicles using camera images and state information in order to determine whether vehicles are autonomous, without requiring any active notification from the vehicles themselves. Essentially, it builds on the cooperation among vehicles, which share their data acquired on the road feeding a machine learning model to identify autonomous cars. We extensively tested our solution and created the NexusStreet dataset, by means of the CARLA simulator, employing an autonomous driving control agent and a steering wheel maneuvered by licensed drivers. Experiments show it is possible to discriminate the two behaviors by analyzing video clips with an accuracy of 80%, which improves up to 93% when the target state information is available. Lastly, we deliberately degraded the state to observe how the framework performs under non-ideal data collection conditions.","sentences":["The tremendous hype around autonomous driving is eagerly calling for emerging and novel technologies to support advanced mobility use cases.","As car manufactures keep developing SAE level 3+ systems to improve the safety and comfort of passengers, traffic authorities need to establish new procedures to manage the transition from human-driven to fully-autonomous vehicles while providing a feedback-loop mechanism to fine-tune envisioned autonomous systems.","Thus, a way to automatically profile autonomous vehicles and differentiate those from human-driven ones is a must.","In this paper, we present a fully-fledged framework that monitors active vehicles using camera images and state information in order to determine whether vehicles are autonomous, without requiring any active notification from the vehicles themselves.","Essentially, it builds on the cooperation among vehicles, which share their data acquired on the road feeding a machine learning model to identify autonomous cars.","We extensively tested our solution and created the NexusStreet dataset, by means of the CARLA simulator, employing an autonomous driving control agent and a steering wheel maneuvered by licensed drivers.","Experiments show it is possible to discriminate the two behaviors by analyzing video clips with an accuracy of 80%, which improves up to 93% when the target state information is available.","Lastly, we deliberately degraded the state to observe how the framework performs under non-ideal data collection conditions."],"url":"http://arxiv.org/abs/2403.09571v1","category":"cs.RO"}
{"created":"2024-03-14 17:00:01","title":"Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search","abstract":"In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of black-box functions that are costly to evaluate. For example, the designer may need to tune the hyperparameters of neural network models for different learning tasks over time. Rather than evaluating the objective function for each candidate solution, the designer may have access to approximations of the objective functions, for which higher-fidelity evaluations entail a larger cost. Existing multi-fidelity black-box optimization strategies select candidate solutions and fidelity levels with the goal of maximizing the information accrued about the optimal value or solution for the current task. Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the current task with the goal of collecting information transferable to future tasks. The proposed method includes shared inter-task latent variables, which are transferred across tasks by implementing particle-based variational Bayesian updates. Experimental results across synthetic and real-world examples reveal that the proposed provident acquisition strategy that caters to future tasks can significantly improve the optimization efficiency as soon as a sufficient number of tasks is processed.","sentences":["In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of black-box functions that are costly to evaluate.","For example, the designer may need to tune the hyperparameters of neural network models for different learning tasks over time.","Rather than evaluating the objective function for each candidate solution, the designer may have access to approximations of the objective functions, for which higher-fidelity evaluations entail a larger cost.","Existing multi-fidelity black-box optimization strategies select candidate solutions and fidelity levels with the goal of maximizing the information accrued about the optimal value or solution for the current task.","Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the current task with the goal of collecting information transferable to future tasks.","The proposed method includes shared inter-task latent variables, which are transferred across tasks by implementing particle-based variational Bayesian updates.","Experimental results across synthetic and real-world examples reveal that the proposed provident acquisition strategy that caters to future tasks can significantly improve the optimization efficiency as soon as a sufficient number of tasks is processed."],"url":"http://arxiv.org/abs/2403.09570v1","category":"cs.LG"}
{"created":"2024-03-14 15:32:25","title":"On using Machine Learning Algorithms for Motorcycle Collision Detection","abstract":"Globally, motorcycles attract vast and varied users. However, since the rate of severe injury and fatality in motorcycle accidents far exceeds passenger car accidents, efforts have been directed toward increasing passive safety systems. Impact simulations show that the risk of severe injury or death in the event of a motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped with passive safety measures such as airbags and seat belts. For the passive safety systems to be activated, a collision must be detected within milliseconds for a wide variety of impact configurations, but under no circumstances may it be falsely triggered. For the challenge of reliably detecting impending collisions, this paper presents an investigation towards the applicability of machine learning algorithms. First, a series of simulations of accidents and driving operation is introduced to collect data to train machine learning classification models. Their performance is henceforth assessed and compared via multiple representative and application-oriented criteria.","sentences":["Globally, motorcycles attract vast and varied users.","However, since the rate of severe injury and fatality in motorcycle accidents far exceeds passenger car accidents, efforts have been directed toward increasing passive safety systems.","Impact simulations show that the risk of severe injury or death in the event of a motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped with passive safety measures such as airbags and seat belts.","For the passive safety systems to be activated, a collision must be detected within milliseconds for a wide variety of impact configurations, but under no circumstances may it be falsely triggered.","For the challenge of reliably detecting impending collisions, this paper presents an investigation towards the applicability of machine learning algorithms.","First, a series of simulations of accidents and driving operation is introduced to collect data to train machine learning classification models.","Their performance is henceforth assessed and compared via multiple representative and application-oriented criteria."],"url":"http://arxiv.org/abs/2403.09491v1","category":"cs.LG"}
{"created":"2024-03-14 15:25:50","title":"Tracking of charged particles with nanosecond lifetimes at LHCb","abstract":"A method is presented to reconstruct charged particles with lifetimes between 10 ps and 10 ns, which considers a combination of their decay products and the partial tracks created by the initial charged particle. Using the $\\Xi^-$ baryon as a benchmark, the method is demonstrated with simulated events and proton-proton collision data at $\\sqrt{s}=13$ TeV, corresponding to an integrated luminosity of 2.0 fb${}^{-1}$ collected with the LHCb detector in 2018. Significant improvements in the angular resolution and the signal purity are obtained. The method is implemented as part of the LHCb Run 3 event trigger in a set of requirements to select detached hyperons. This is the first demonstration of the applicability of this approach at the LHC, and the first to show its scaling with instantaneous luminosity.","sentences":["A method is presented to reconstruct charged particles with lifetimes between 10 ps and 10 ns, which considers a combination of their decay products and the partial tracks created by the initial charged particle.","Using the $\\Xi^-$ baryon as a benchmark, the method is demonstrated with simulated events and proton-proton collision data at $\\sqrt{s}=13$ TeV, corresponding to an integrated luminosity of 2.0 fb${}^{-1}$ collected with the LHCb detector in 2018.","Significant improvements in the angular resolution and the signal purity are obtained.","The method is implemented as part of the LHCb Run 3 event trigger in a set of requirements to select detached hyperons.","This is the first demonstration of the applicability of this approach at the LHC, and the first to show its scaling with instantaneous luminosity."],"url":"http://arxiv.org/abs/2403.09483v1","category":"hep-ex"}
{"created":"2024-03-14 15:04:17","title":"New constraints on Triton's atmosphere from the 6 October 2022 stellar occultation","abstract":"The atmosphere of Triton was probed directly by observing a ground-based stellar occultation on 6 October 2022. This rare event yielded 23 positive light curves collected from 13 separate observation stations contributing to our campaign. The significance of this event lies in its potential to directly validate the modest pressure fluctuation on Triton, a phenomenon not definitively verified by previous observations, including only five stellar occultations, and the Voyager 2 radio occultation in 1989. Using an approach consistent with a comparable study, we precisely determined a surface pressure of $14.07_{-0.13}^{+0.21}~\\mathrm{\\mu bar}$ in 2022. This new pressure rules out any significant monotonic variation in pressure between 2017 and 2022 through direct observations, as it is in alignment with the 2017 value. Additionally, both the pressures in 2017 and 2022 align with the 1989 value. This provides further support for the conclusion drawn from the previous volatile transport model simulation, which is consistent with the observed alignment between the pressures in 1989 and 2017; that is to say, the pressure fluctuation is modest. Moreover, this conclusion suggests the existence of a northern polar cap extended down to at least $45^\\circ$N$-60^\\circ$N and the presence of nitrogen between $30^\\circ$S and $0^\\circ$.","sentences":["The atmosphere of Triton was probed directly by observing a ground-based stellar occultation on 6 October 2022.","This rare event yielded 23 positive light curves collected from 13 separate observation stations contributing to our campaign.","The significance of this event lies in its potential to directly validate the modest pressure fluctuation on Triton, a phenomenon not definitively verified by previous observations, including only five stellar occultations, and the Voyager 2 radio occultation in 1989.","Using an approach consistent with a comparable study, we precisely determined a surface pressure of $14.07_{-0.13}^{+0.21}~\\mathrm{\\mu bar}$ in 2022.","This new pressure rules out any significant monotonic variation in pressure between 2017 and 2022 through direct observations, as it is in alignment with the 2017 value.","Additionally, both the pressures in 2017 and 2022 align with the 1989 value.","This provides further support for the conclusion drawn from the previous volatile transport model simulation, which is consistent with the observed alignment between the pressures in 1989 and 2017; that is to say, the pressure fluctuation is modest.","Moreover, this conclusion suggests the existence of a northern polar cap extended down to at least $45^\\circ$N$-60^\\circ$N and the presence of nitrogen between $30^\\circ$S and $0^\\circ$."],"url":"http://arxiv.org/abs/2403.09464v1","category":"astro-ph.EP"}
{"created":"2024-03-14 14:51:35","title":"Measurements of inclusive and differential cross-sections of $t\\bar{t}\u03b3$ production in $pp$ collisions at $\\sqrt{s}=13$ TeV with the ATLAS detector","abstract":"Inclusive and differential cross-sections are measured at particle level for the associated production of a top quark pair and a photon ($t\\bar{t}\\gamma$). The analysis is performed using an integrated luminosity of 140 fb$^{-1}$ of proton-proton collisions at a centre-of-mass energy of 13 TeV collected by the ATLAS detector. The measurements are performed in the single-lepton and dilepton top quark pair decay channels focusing on $t\\bar{t}\\gamma$ topologies where the photon is radiated from an initial-state parton or one of the top quarks. The absolute and normalised differential cross-sections are measured for several variables characterising the photon, lepton and jet kinematics as well as the angular separation between those objects. The observables are found to be in good agreement with the Monte Carlo predictions. The photon transverse momentum differential distribution is used to set limits on effective field theory parameters related to the electroweak dipole moments of the top quark. The combined limits using the photon and the $Z$ boson transverse momentum measured in $t\\bar{t}$ production in associations with a $Z$ boson are also set.","sentences":["Inclusive and differential cross-sections are measured at particle level for the associated production of a top quark pair and a photon ($t\\bar{t}\\gamma$).","The analysis is performed using an integrated luminosity of 140 fb$^{-1}$ of proton-proton collisions at a centre-of-mass energy of 13 TeV collected by the ATLAS detector.","The measurements are performed in the single-lepton and dilepton top quark pair decay channels focusing on $t\\bar{t}\\gamma$ topologies where the photon is radiated from an initial-state parton or one of the top quarks.","The absolute and normalised differential cross-sections are measured for several variables characterising the photon, lepton and jet kinematics as well as the angular separation between those objects.","The observables are found to be in good agreement with the Monte Carlo predictions.","The photon transverse momentum differential distribution is used to set limits on effective field theory parameters related to the electroweak dipole moments of the top quark.","The combined limits using the photon and the $Z$ boson transverse momentum measured in $t\\bar{t}$ production in associations with a $Z$ boson are also set."],"url":"http://arxiv.org/abs/2403.09452v1","category":"hep-ex"}
{"created":"2024-03-14 13:47:47","title":"Discovery of a distinct collective mode in kagome superconductors","abstract":"The collective modes of the superconducting (SC) order parameter fluctuation can provide key insights into the nature of the superconductor, such as the pairing symmetry and orbital nature of the cooper pairs. Their detection has been challenging. Recently, a family of charge density wave (CDW) superconductors has emerged in non-magnetic kagome materials AV3Sb5 (A=K, Rb, Cs), exhibiting evidence for unprecedented time-reversal symmetry breaking CDW, roton-pair density wave (PDW), and higher-charge flux quantization. However, the collective behaviors of the cooper pairs have not been studied. Here, we report a distinct collective mode in the kagome superconductors CsV3-xTaxSb5. Using scanning tunneling microscope/spectroscopy (STM/S), we observe a \"peak-dip-hump\" feature in the tunneling conductance-a defining signature of collective excitations. The spectral lineshape is well-described by two SC gap functions, one isotropic and one anisotropic, and a bosonic mode due to electron-mode coupling. With increasing x, the two SC gaps, with different pair-breaking strengths and thus different orbital/band characters, move closer in energy, merge into two isotropic gaps of equal amplitude, and then increase synchronously. The collective mode energy, on the other hand, decreases monotonically to well below the quasi-particle excitation gap 2{\\Delta} and survives even after the CDW order is suppressed at large x. We identify the collective mode as the Leggett mode of the relative phases between different SC components in the multi-band superconductor or the exotic Bardasis-Schrieffer mode due to a subleading SC component. Our findings provide valuable insights on the nature of the SC ground state and collective excitations, their evolution with Ta-substitutions, and offer a new pathway to study the origin of superconductivity and its interplay with CDW order in kagome superconductors.","sentences":["The collective modes of the superconducting (SC) order parameter fluctuation can provide key insights into the nature of the superconductor, such as the pairing symmetry and orbital nature of the cooper pairs.","Their detection has been challenging.","Recently, a family of charge density wave (CDW) superconductors has emerged in non-magnetic kagome materials AV3Sb5","(A=K, Rb, Cs), exhibiting evidence for unprecedented time-reversal symmetry breaking CDW, roton-pair density wave (PDW), and higher-charge flux quantization.","However, the collective behaviors of the cooper pairs have not been studied.","Here, we report a distinct collective mode in the kagome superconductors CsV3-xTaxSb5.","Using scanning tunneling microscope/spectroscopy (STM/S), we observe a \"peak-dip-hump\" feature in the tunneling conductance-a defining signature of collective excitations.","The spectral lineshape is well-described by two SC gap functions, one isotropic and one anisotropic, and a bosonic mode due to electron-mode coupling.","With increasing x, the two SC gaps, with different pair-breaking strengths and thus different orbital/band characters, move closer in energy, merge into two isotropic gaps of equal amplitude, and then increase synchronously.","The collective mode energy, on the other hand, decreases monotonically to well below the quasi-particle excitation gap 2{\\Delta} and survives even after the CDW order is suppressed at large x.","We identify the collective mode as the Leggett mode of the relative phases between different SC components in the multi-band superconductor or the exotic Bardasis-Schrieffer mode due to a subleading SC component.","Our findings provide valuable insights on the nature of the SC ground state and collective excitations, their evolution with Ta-substitutions, and offer a new pathway to study the origin of superconductivity and its interplay with CDW order in kagome superconductors."],"url":"http://arxiv.org/abs/2403.09395v1","category":"cond-mat.supr-con"}
{"created":"2024-03-14 12:57:20","title":"LDPRecover: Recovering Frequencies from Poisoning Attacks against Local Differential Privacy","abstract":"Local differential privacy (LDP), which enables an untrusted server to collect aggregated statistics from distributed users while protecting the privacy of those users, has been widely deployed in practice. However, LDP protocols for frequency estimation are vulnerable to poisoning attacks, in which an attacker can poison the aggregated frequencies by manipulating the data sent from malicious users. Therefore, it is an open challenge to recover the accurate aggregated frequencies from poisoned ones.   In this work, we propose LDPRecover, a method that can recover accurate aggregated frequencies from poisoning attacks, even if the server does not learn the details of the attacks. In LDPRecover, we establish a genuine frequency estimator that theoretically guides the server to recover the frequencies aggregated from genuine users' data by eliminating the impact of malicious users' data in poisoned frequencies. Since the server has no idea of the attacks, we propose an adaptive attack to unify existing attacks and learn the statistics of the malicious data within this adaptive attack by exploiting the properties of LDP protocols. By taking the estimator and the learning statistics as constraints, we formulate the problem of recovering aggregated frequencies to approach the genuine ones as a constraint inference (CI) problem. Consequently, the server can obtain accurate aggregated frequencies by solving this problem optimally. Moreover, LDPRecover can serve as a frequency recovery paradigm that recovers more accurate aggregated frequencies by integrating attack details as new constraints in the CI problem. Our evaluation on two real-world datasets, three LDP protocols, and untargeted and targeted poisoning attacks shows that LDPRecover is both accurate and widely applicable against various poisoning attacks.","sentences":["Local differential privacy (LDP), which enables an untrusted server to collect aggregated statistics from distributed users while protecting the privacy of those users, has been widely deployed in practice.","However, LDP protocols for frequency estimation are vulnerable to poisoning attacks, in which an attacker can poison the aggregated frequencies by manipulating the data sent from malicious users.","Therefore, it is an open challenge to recover the accurate aggregated frequencies from poisoned ones.   ","In this work, we propose LDPRecover, a method that can recover accurate aggregated frequencies from poisoning attacks, even if the server does not learn the details of the attacks.","In LDPRecover, we establish a genuine frequency estimator that theoretically guides the server to recover the frequencies aggregated from genuine users' data by eliminating the impact of malicious users' data in poisoned frequencies.","Since the server has no idea of the attacks, we propose an adaptive attack to unify existing attacks and learn the statistics of the malicious data within this adaptive attack by exploiting the properties of LDP protocols.","By taking the estimator and the learning statistics as constraints, we formulate the problem of recovering aggregated frequencies to approach the genuine ones as a constraint inference (CI) problem.","Consequently, the server can obtain accurate aggregated frequencies by solving this problem optimally.","Moreover, LDPRecover can serve as a frequency recovery paradigm that recovers more accurate aggregated frequencies by integrating attack details as new constraints in the CI problem.","Our evaluation on two real-world datasets, three LDP protocols, and untargeted and targeted poisoning attacks shows that LDPRecover is both accurate and widely applicable against various poisoning attacks."],"url":"http://arxiv.org/abs/2403.09351v1","category":"cs.CR"}
{"created":"2024-03-14 12:53:30","title":"From Pro, Anti to Informative and Hesitant: An Infoveillance study of COVID-19 vaccines and vaccination discourse on Twitter","abstract":"COVID-19 pandemic has brought unprecedented challenges to the world, and vaccination has been a key strategy to combat the disease. Since Twitter is one of the most widely used public microblogging platforms, researchers have analysed COVID-19 vaccines and vaccination Twitter discourse to explore the conversational dynamics around the topic. While contributing to the crisis informatics literature, we curate a large-scale geotagged Twitter dataset, GeoCovaxTweets Extended, and explore the discourse through multiple spatiotemporal analyses. This dataset covers a longer time span of 38 months, from the announcement of the first vaccine to the availability of booster doses. Results show that 43.4% of the collected tweets, although containing phrases and keywords related to vaccines and vaccinations, were unrelated to the COVID-19 context. In total, 23.1% of the discussions on vaccines and vaccinations were classified as Pro, 16% as Hesitant, 11.4% as Anti, and 6.1% as Informative. The trend shifted towards Pro and Informative tweets globally as vaccination programs progressed, indicating a change in the public's perception of COVID-19 vaccines and vaccination. Furthermore, we explored the discourse based on account attributes, i.e., followers counts and tweet counts. Results show a significant pattern of discourse differences. Our findings highlight the potential of harnessing a large-scale geotagged Twitter dataset to understand global public health communication and to inform targeted interventions aimed at addressing vaccine hesitancy.","sentences":["COVID-19 pandemic has brought unprecedented challenges to the world, and vaccination has been a key strategy to combat the disease.","Since Twitter is one of the most widely used public microblogging platforms, researchers have analysed COVID-19 vaccines and vaccination Twitter discourse to explore the conversational dynamics around the topic.","While contributing to the crisis informatics literature, we curate a large-scale geotagged Twitter dataset, GeoCovaxTweets Extended, and explore the discourse through multiple spatiotemporal analyses.","This dataset covers a longer time span of 38 months, from the announcement of the first vaccine to the availability of booster doses.","Results show that 43.4% of the collected tweets, although containing phrases and keywords related to vaccines and vaccinations, were unrelated to the COVID-19 context.","In total, 23.1% of the discussions on vaccines and vaccinations were classified as Pro, 16% as Hesitant, 11.4% as Anti, and 6.1% as Informative.","The trend shifted towards Pro and Informative tweets globally as vaccination programs progressed, indicating a change in the public's perception of COVID-19 vaccines and vaccination.","Furthermore, we explored the discourse based on account attributes, i.e., followers counts and tweet counts.","Results show a significant pattern of discourse differences.","Our findings highlight the potential of harnessing a large-scale geotagged Twitter dataset to understand global public health communication and to inform targeted interventions aimed at addressing vaccine hesitancy."],"url":"http://arxiv.org/abs/2403.09349v1","category":"cs.SI"}
{"created":"2024-03-14 12:20:23","title":"Up and Down Quark Structure of the Proton","abstract":"We measure proton structure parameters sensitive primarily to valence quarks using 8.6 fb$^{-1}$ of data collected by the D0 detector in $\\sqrt{s}=1.96$ TeV $p\\bar{p}$ collisions at the Fermilab Tevatron. We exploit the property of the forward-backward asymmetry in dilepton events to be factorized in to distinct structure parameters and electroweak quark-level asymmetries. Contributions to the asymmetry from $s$, $c$ and $b$ quarks, as well as from $u$ and $d$ quarks, are suppressed allowing valence $u$ and $d$ quarks to be separately determined. We find and $u$ to $d$ quark ratio near the peak values in the quark density distributions that is smaller than predictions from modern parton distribution functions.","sentences":["We measure proton structure parameters sensitive primarily to valence quarks using 8.6 fb$^{-1}$ of data collected by the D0 detector in $\\sqrt{s}=1.96$ TeV $p\\bar{p}$ collisions at the Fermilab Tevatron.","We exploit the property of the forward-backward asymmetry in dilepton events to be factorized in to distinct structure parameters and electroweak quark-level asymmetries.","Contributions to the asymmetry from $s$, $c$ and $b$ quarks, as well as from $u$ and $d$ quarks, are suppressed allowing valence $u$ and $d$ quarks to be separately determined.","We find and $u$ to $d$ quark ratio near the peak values in the quark density distributions that is smaller than predictions from modern parton distribution functions."],"url":"http://arxiv.org/abs/2403.09331v1","category":"hep-ex"}
{"created":"2024-03-14 11:36:53","title":"A complete logic for causal consistency","abstract":"The $\\mathrm{Caus}[-]$ construction takes a base category of ``raw materials'' and builds a category of higher order causal processes, that is a category whose types encode causal (a.k.a. signalling) constraints between collections of systems. Notable examples are categories of higher-order stochastic maps and higher-order quantum channels. Well-typedness in $\\mathrm{Caus}[-]$ corresponds to a composition of processes being causally consistent, in the sense that any choice of local processes of the prescribed types yields an overall process respecting causality constraints. It follows that closed processes always occur with probability 1, ruling out e.g. causal paradoxes arising from time loops. It has previously been shown that $\\mathrm{Caus}[\\mathcal{C}]$ gives a model of MLL+MIX and BV logic, hence these logics give sufficient conditions for causal consistency, but they fail to provide a complete characterisation. In this follow-on work, we introduce graph types as a tool to examine causal structures over graphs in this model. We explore their properties, standard forms, and equivalent definitions; in particular, a process obeys all signalling constraints of the graph iff it is expressible as an affine combination of factorisations into local causal processes connected according to the edges of the graph. The properties of graph types are then used to prove completeness for causal consistency of a new causal logic that conservatively extends pomset logic. The crucial extra ingredient is a notion of distinguished atoms that correspond to first-order states, which only admit a flow of information in one direction. Using the fact that causal logic conservatively extends pomset logic, we finish by giving a physically-meaningful interpretation to a separating statement between pomset and BV.","sentences":["The $\\mathrm{Caus}[-]$ construction takes a base category of ``raw materials'' and builds a category of higher order causal processes, that is a category whose types encode causal (a.k.a. signalling) constraints between collections of systems.","Notable examples are categories of higher-order stochastic maps and higher-order quantum channels.","Well-typedness in $\\mathrm{Caus}[-]$ corresponds to a composition of processes being causally consistent, in the sense that any choice of local processes of the prescribed types yields an overall process respecting causality constraints.","It follows that closed processes always occur with probability 1, ruling out e.g. causal paradoxes arising from time loops.","It has previously been shown that $\\mathrm{Caus}[\\mathcal{C}]$ gives a model of MLL+MIX and BV logic, hence these logics give sufficient conditions for causal consistency, but they fail to provide a complete characterisation.","In this follow-on work, we introduce graph types as a tool to examine causal structures over graphs in this model.","We explore their properties, standard forms, and equivalent definitions; in particular, a process obeys all signalling constraints of the graph iff it is expressible as an affine combination of factorisations into local causal processes connected according to the edges of the graph.","The properties of graph types are then used to prove completeness for causal consistency of a new causal logic that conservatively extends pomset logic.","The crucial extra ingredient is a notion of distinguished atoms that correspond to first-order states, which only admit a flow of information in one direction.","Using the fact that causal logic conservatively extends pomset logic, we finish by giving a physically-meaningful interpretation to a separating statement between pomset and BV."],"url":"http://arxiv.org/abs/2403.09297v1","category":"cs.LO"}
{"created":"2024-03-14 11:35:08","title":"Seed-based information retrieval in networks of research publications: Evaluation of direct citations, bibliographic coupling, co-citations and PubMed related article score","abstract":"In this contribution, we deal with seed-based information retrieval in networks of research publications. Using systematic reviews as a baseline, and publication data from the NIH Open Citation Collection, we compare the performance of the three citation-based approaches direct citation, co-citation, and bibliographic coupling with respect to recall and precision measures. In addition, we include the PubMed Related Article score as well as combined approaches in the comparison. We also provide a fairly comprehensive review of earlier research in which citation relations have been used for information retrieval purposes. The results show an advantage for co-citation over bibliographic coupling and direct citation. However, combining the three approaches outperforms the exclusive use of co-citation in the study. The results further indicate, in line with previous research, that combining citation-based approaches with textual approaches enhances the performance of seed-based information retrieval. The results from the study may guide approaches combining citation-based and textual approaches in their choice of citation similarity measures. We suggest that future research use more structured approaches to evaluate methods for seed-based retrieval of publications, including comparative approaches as well as the elaboration of common data sets and baselines for evaluation.","sentences":["In this contribution, we deal with seed-based information retrieval in networks of research publications.","Using systematic reviews as a baseline, and publication data from the NIH Open Citation Collection, we compare the performance of the three citation-based approaches direct citation, co-citation, and bibliographic coupling with respect to recall and precision measures.","In addition, we include the PubMed Related Article score as well as combined approaches in the comparison.","We also provide a fairly comprehensive review of earlier research in which citation relations have been used for information retrieval purposes.","The results show an advantage for co-citation over bibliographic coupling and direct citation.","However, combining the three approaches outperforms the exclusive use of co-citation in the study.","The results further indicate, in line with previous research, that combining citation-based approaches with textual approaches enhances the performance of seed-based information retrieval.","The results from the study may guide approaches combining citation-based and textual approaches in their choice of citation similarity measures.","We suggest that future research use more structured approaches to evaluate methods for seed-based retrieval of publications, including comparative approaches as well as the elaboration of common data sets and baselines for evaluation."],"url":"http://arxiv.org/abs/2403.09295v1","category":"cs.IR"}
{"created":"2024-03-14 11:12:16","title":"TH\u00d6R-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction","abstract":"We present a new large dataset of indoor human and robot navigation and interaction, called TH\\\"OR-MAGNI, that is designed to facilitate research on social navigation: e.g., modelling and predicting human motion, analyzing goal-oriented interactions between humans and robots, and investigating visual attention in a social interaction context. TH\\\"OR-MAGNI was created to fill a gap in available datasets for human motion analysis and HRI. This gap is characterized by a lack of comprehensive inclusion of exogenous factors and essential target agent cues, which hinders the development of robust models capable of capturing the relationship between contextual cues and human behavior in different scenarios. Unlike existing datasets, TH\\\"OR-MAGNI includes a broader set of contextual features and offers multiple scenario variations to facilitate factor isolation. The dataset includes many social human-human and human-robot interaction scenarios, rich context annotations, and multi-modal data, such as walking trajectories, gaze tracking data, and lidar and camera streams recorded from a mobile robot. We also provide a set of tools for visualization and processing of the recorded data. TH\\\"OR-MAGNI is, to the best of our knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human-robot interactions.","sentences":["We present a new large dataset of indoor human and robot navigation and interaction, called TH\\\"OR-MAGNI, that is designed to facilitate research on social navigation: e.g., modelling and predicting human motion, analyzing goal-oriented interactions between humans and robots, and investigating visual attention in a social interaction context.","TH\\\"OR-MAGNI was created to fill a gap in available datasets for human motion analysis and HRI.","This gap is characterized by a lack of comprehensive inclusion of exogenous factors and essential target agent cues, which hinders the development of robust models capable of capturing the relationship between contextual cues and human behavior in different scenarios.","Unlike existing datasets, TH\\\"OR-MAGNI includes a broader set of contextual features and offers multiple scenario variations to facilitate factor isolation.","The dataset includes many social human-human and human-robot interaction scenarios, rich context annotations, and multi-modal data, such as walking trajectories, gaze tracking data, and lidar and camera streams recorded from a mobile robot.","We also provide a set of tools for visualization and processing of the recorded data.","TH\\\"OR-MAGNI is, to the best of our knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human-robot interactions."],"url":"http://arxiv.org/abs/2403.09285v1","category":"cs.RO"}
{"created":"2024-03-14 10:07:29","title":"High precision proton beam monitor system concept design on CSNS based on SiC","abstract":"A high precision beam monitor system based on silicon carbide PIN sensor is designed for China Spallation Neutron Source 1.6 GeV proton beam to monitor the proton beam fluence.The concept design of the beam monitor system is finished together with front-end electronics with silicon carbide PIN sensors, readout system and mechanical system.Several tests are performed to study the performance of each component of the system.The charge collection of the SiC PIN sensors after proton radiation is studied with 80 MeV proton beam for continuous running. Research on the performance of the front-end electronics and readout system is finished for better data acquisition.The uncertainty of proton beam fluence is below 1% in the beam monitor system.","sentences":["A high precision beam monitor system based on silicon carbide PIN sensor is designed for China Spallation Neutron Source 1.6 GeV proton beam to monitor the proton beam fluence.","The concept design of the beam monitor system is finished together with front-end electronics with silicon carbide PIN sensors, readout system and mechanical system.","Several tests are performed to study the performance of each component of the system.","The charge collection of the SiC PIN sensors after proton radiation is studied with 80 MeV proton beam for continuous running.","Research on the performance of the front-end electronics and readout system is finished for better data acquisition.","The uncertainty of proton beam fluence is below 1% in the beam monitor system."],"url":"http://arxiv.org/abs/2403.09244v1","category":"physics.acc-ph"}
{"created":"2024-03-14 09:53:06","title":"Analysis of BMR tilt from AutoTAB catalog: Hinting towards the thin flux tube model?","abstract":"One of the intriguing mechanisms of the Sun is the formation of the bipolar magnetic regions (BMRs) in the solar convection zone which are observed as regions of concentrated magnetic fields of opposite polarity on photosphere. These BMRs are tilted with respect to the equatorial line, which statistically increases with latitude. The thin flux tube model, employing the rise of magnetically buoyant flux loops and their twist by Coriolis force, is a popular paradigm for explaining the formation of tilted BMRs. In this study, we assess the validity of the thin flux tube model by analyzing the tracked BMR data obtained through the Automatic Tracking Algorithm for BMRs (AutoTAB). Our observations reveal that the tracked BMRs exhibit the expected collective behaviors. We find that the polarity separation of BMRs increases over their normalized lifetime, supporting the assumption of a rising flux tube from the CZ. Moreover, we observe an increasing trend of the tilt with the flux of the BMR, suggesting that rising flux tubes associated with lower flux regions are primarily influenced by drag force and Coriolis force, while in higher flux regions, magnetic buoyancy dominates. Furthermore, we observe Joy's law dependence for emerging BMRs from their first detection, indicating that at least a portion of the tilt observed in BMRs can be attributed to the Coriolis force. Notably, lower flux regions exhibit a higher amount of fluctuations associated with their tilt measurement compared to stronger flux regions, suggesting that lower flux regions are more susceptible to turbulent convection.","sentences":["One of the intriguing mechanisms of the Sun is the formation of the bipolar magnetic regions (BMRs) in the solar convection zone which are observed as regions of concentrated magnetic fields of opposite polarity on photosphere.","These BMRs are tilted with respect to the equatorial line, which statistically increases with latitude.","The thin flux tube model, employing the rise of magnetically buoyant flux loops and their twist by Coriolis force, is a popular paradigm for explaining the formation of tilted BMRs.","In this study, we assess the validity of the thin flux tube model by analyzing the tracked BMR data obtained through the Automatic Tracking Algorithm for BMRs (AutoTAB).","Our observations reveal that the tracked BMRs exhibit the expected collective behaviors.","We find that the polarity separation of BMRs increases over their normalized lifetime, supporting the assumption of a rising flux tube from the CZ.","Moreover, we observe an increasing trend of the tilt with the flux of the BMR, suggesting that rising flux tubes associated with lower flux regions are primarily influenced by drag force and Coriolis force, while in higher flux regions, magnetic buoyancy dominates.","Furthermore, we observe Joy's law dependence for emerging BMRs from their first detection, indicating that at least a portion of the tilt observed in BMRs can be attributed to the Coriolis force.","Notably, lower flux regions exhibit a higher amount of fluctuations associated with their tilt measurement compared to stronger flux regions, suggesting that lower flux regions are more susceptible to turbulent convection."],"url":"http://arxiv.org/abs/2403.09229v1","category":"astro-ph.SR"}
{"created":"2024-03-14 08:47:19","title":"Synchronisation-Oriented Design Approach for Adaptive Control","abstract":"This study presents a synchronisation-oriented perspective towards adaptive control which views model-referenced adaptation as synchronisation between actual and virtual dynamic systems. In the context of adaptation, model reference adaptive control methods make the state response of the actual plant follow a reference model. In the context of synchronisation, consensus methods involving diffusive coupling induce a collective behaviour across multiple agents. We draw from the understanding about the two time-scale nature of synchronisation motivated by the study of blended dynamics. The synchronisation-oriented approach consists in the design of a coupling input to achieve desired closed-loop error dynamics followed by the input allocation process to shape the collective behaviour. We suggest that synchronisation can be a reasonable design principle allowing a more holistic and systematic approach to the design of adaptive control systems for improved transient characteristics. Most notably, the proposed approach enables not only constructive derivation but also substantial generalisation of the previously developed closed-loop reference model adaptive control method. Practical significance of the proposed generalisation lies at the capability to improve the transient response characteristics and mitigate the unwanted peaking phenomenon at the same time.","sentences":["This study presents a synchronisation-oriented perspective towards adaptive control which views model-referenced adaptation as synchronisation between actual and virtual dynamic systems.","In the context of adaptation, model reference adaptive control methods make the state response of the actual plant follow a reference model.","In the context of synchronisation, consensus methods involving diffusive coupling induce a collective behaviour across multiple agents.","We draw from the understanding about the two time-scale nature of synchronisation motivated by the study of blended dynamics.","The synchronisation-oriented approach consists in the design of a coupling input to achieve desired closed-loop error dynamics followed by the input allocation process to shape the collective behaviour.","We suggest that synchronisation can be a reasonable design principle allowing a more holistic and systematic approach to the design of adaptive control systems for improved transient characteristics.","Most notably, the proposed approach enables not only constructive derivation but also substantial generalisation of the previously developed closed-loop reference model adaptive control method.","Practical significance of the proposed generalisation lies at the capability to improve the transient response characteristics and mitigate the unwanted peaking phenomenon at the same time."],"url":"http://arxiv.org/abs/2403.09179v1","category":"eess.SY"}
{"created":"2024-03-14 08:31:39","title":"ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks","abstract":"Although Graph Neural Networks (GNNs) have exhibited the powerful ability to gather graph-structured information from neighborhood nodes via various message-passing mechanisms, the performance of GNNs is limited by poor generalization and fragile robustness caused by noisy and redundant graph data. As a prominent solution, Graph Augmentation Learning (GAL) has recently received increasing attention. Among prior GAL approaches, edge-dropping methods that randomly remove edges from a graph during training are effective techniques to improve the robustness of GNNs. However, randomly dropping edges often results in bypassing critical edges, consequently weakening the effectiveness of message passing. In this paper, we propose a novel adversarial edge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor guiding the removal of edges, which can be flexibly incorporated into diverse GNN backbones. Employing an adversarial training framework, the edge predictor utilizes the line graph transformed from the original graph to estimate the edges to be dropped, which improves the interpretability of the edge-dropping method. The proposed ADEdgeDrop is optimized alternately by stochastic gradient descent and projected gradient descent. Comprehensive experiments on six graph benchmark datasets demonstrate that the proposed ADEdgeDrop outperforms state-of-the-art baselines across various GNN backbones, demonstrating improved generalization and robustness.","sentences":["Although Graph Neural Networks (GNNs) have exhibited the powerful ability to gather graph-structured information from neighborhood nodes via various message-passing mechanisms, the performance of GNNs is limited by poor generalization and fragile robustness caused by noisy and redundant graph data.","As a prominent solution, Graph Augmentation Learning (GAL) has recently received increasing attention.","Among prior GAL approaches, edge-dropping methods that randomly remove edges from a graph during training are effective techniques to improve the robustness of GNNs.","However, randomly dropping edges often results in bypassing critical edges, consequently weakening the effectiveness of message passing.","In this paper, we propose a novel adversarial edge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor guiding the removal of edges, which can be flexibly incorporated into diverse GNN backbones.","Employing an adversarial training framework, the edge predictor utilizes the line graph transformed from the original graph to estimate the edges to be dropped, which improves the interpretability of the edge-dropping method.","The proposed ADEdgeDrop is optimized alternately by stochastic gradient descent and projected gradient descent.","Comprehensive experiments on six graph benchmark datasets demonstrate that the proposed ADEdgeDrop outperforms state-of-the-art baselines across various GNN backbones, demonstrating improved generalization and robustness."],"url":"http://arxiv.org/abs/2403.09171v1","category":"cs.LG"}
{"created":"2024-03-14 07:51:20","title":"Stoner ferromagnetism, correlated metal and thermoelectricity in partially flat-band materials","abstract":"Recent discovery of correlated electronic phases in twisted heterostructures raised a surge of interests in studying models and materials with flat bands where the electronic excitations are nearly dispersionless in momentum space. As such, the kinetic energy is quenched and the correlations are enhanced, giving rise to a plethora of unusual magnetic, superconducting and transport behaviors. Finding materials whose energy bands are completely flat is rather challenging, yet those whose dispersion is flat only in a portion of the momentum space might be more accessible in material search. In this work, we propose a partially flat-band system on a square lattice. Using the Hubbard model, it is demonstrated that the suppression of the electronic kinetic energy in the flat portion of the band dispersion drives the system to Stoner ferromagnetism even at very weak interactions, i.e., much smaller than the bandwidth, with significantly enhanced Curie temperature. While the low-energy magnon modes are well defined collective excitations, flat magnon bands can be observed at high energies. We show that the strong interaction leads to reduction of the flat portion of the magnon band. However, tuning the chemical potential at a strong interaction regime may lead to spin density wave at finite wave vectors. Then, focusing on the non-magnetic correlated phase and using dynamical mean-field theory, we demonstrate the appearance of a flat-band induced sharp peak in the density of states in addition to the correlation-induced Mott bands. Furthermore, the large seebeck coefficient and the figure of merit of the proposed partially flat-band model, compared to symmetric regular band models, put them in the category of efficient thermoelectric materials.","sentences":["Recent discovery of correlated electronic phases in twisted heterostructures raised a surge of interests in studying models and materials with flat bands where the electronic excitations are nearly dispersionless in momentum space.","As such, the kinetic energy is quenched and the correlations are enhanced, giving rise to a plethora of unusual magnetic, superconducting and transport behaviors.","Finding materials whose energy bands are completely flat is rather challenging, yet those whose dispersion is flat only in a portion of the momentum space might be more accessible in material search.","In this work, we propose a partially flat-band system on a square lattice.","Using the Hubbard model, it is demonstrated that the suppression of the electronic kinetic energy in the flat portion of the band dispersion drives the system to Stoner ferromagnetism even at very weak interactions, i.e., much smaller than the bandwidth, with significantly enhanced Curie temperature.","While the low-energy magnon modes are well defined collective excitations, flat magnon bands can be observed at high energies.","We show that the strong interaction leads to reduction of the flat portion of the magnon band.","However, tuning the chemical potential at a strong interaction regime may lead to spin density wave at finite wave vectors.","Then, focusing on the non-magnetic correlated phase and using dynamical mean-field theory, we demonstrate the appearance of a flat-band induced sharp peak in the density of states in addition to the correlation-induced Mott bands.","Furthermore, the large seebeck coefficient and the figure of merit of the proposed partially flat-band model, compared to symmetric regular band models, put them in the category of efficient thermoelectric materials."],"url":"http://arxiv.org/abs/2403.09147v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 07:40:54","title":"USimAgent: Large Language Models for Simulating Search Users","abstract":"Due to the advantages in the cost-efficiency and reproducibility, user simulation has become a promising solution to the user-centric evaluation of information retrieval systems. Nonetheless, accurately simulating user search behaviors has long been a challenge, because users' actions in search are highly complex and driven by intricate cognitive processes such as learning, reasoning, and planning. Recently, Large Language Models (LLMs) have demonstrated remarked potential in simulating human-level intelligence and have been used in building autonomous agents for various tasks. However, the potential of using LLMs in simulating search behaviors has not yet been fully explored. In this paper, we introduce a LLM-based user search behavior simulator, USimAgent. The proposed simulator can simulate users' querying, clicking, and stopping behaviors during search, and thus, is capable of generating complete search sessions for specific search tasks. Empirical investigation on a real user behavior dataset shows that the proposed simulator outperforms existing methods in query generation and is comparable to traditional methods in predicting user clicks and stopping behaviors. These results not only validate the effectiveness of using LLMs for user simulation but also shed light on the development of a more robust and generic user simulators.","sentences":["Due to the advantages in the cost-efficiency and reproducibility, user simulation has become a promising solution to the user-centric evaluation of information retrieval systems.","Nonetheless, accurately simulating user search behaviors has long been a challenge, because users' actions in search are highly complex and driven by intricate cognitive processes such as learning, reasoning, and planning.","Recently, Large Language Models (LLMs) have demonstrated remarked potential in simulating human-level intelligence and have been used in building autonomous agents for various tasks.","However, the potential of using LLMs in simulating search behaviors has not yet been fully explored.","In this paper, we introduce a LLM-based user search behavior simulator, USimAgent.","The proposed simulator can simulate users' querying, clicking, and stopping behaviors during search, and thus, is capable of generating complete search sessions for specific search tasks.","Empirical investigation on a real user behavior dataset shows that the proposed simulator outperforms existing methods in query generation and is comparable to traditional methods in predicting user clicks and stopping behaviors.","These results not only validate the effectiveness of using LLMs for user simulation but also shed light on the development of a more robust and generic user simulators."],"url":"http://arxiv.org/abs/2403.09142v1","category":"cs.IR"}
{"created":"2024-03-14 07:40:32","title":"Uncertainty Estimation in Multi-Agent Distributed Learning for AI-Enabled Edge Devices","abstract":"Initially considered as low-power units with limited autonomous processing, Edge IoT devices have seen a paradigm shift with the introduction of FPGAs and AI accelerators. This advancement has vastly amplified their computational capabilities, emphasizing the practicality of edge AI. Such progress introduces new challenges of optimizing AI tasks for the limitations of energy and network resources typical in Edge computing environments. Our study explores methods that enable distributed data processing through AI-enabled edge devices, enhancing collaborative learning capabilities. A key focus of our research is the challenge of determining confidence levels in learning outcomes, considering the spatial and temporal variability of data sets encountered by independent agents. To address this issue, we investigate the application of Bayesian neural networks, proposing a novel approach to manage uncertainty in distributed learning environments.","sentences":["Initially considered as low-power units with limited autonomous processing, Edge IoT devices have seen a paradigm shift with the introduction of FPGAs and AI accelerators.","This advancement has vastly amplified their computational capabilities, emphasizing the practicality of edge AI.","Such progress introduces new challenges of optimizing AI tasks for the limitations of energy and network resources typical in Edge computing environments.","Our study explores methods that enable distributed data processing through AI-enabled edge devices, enhancing collaborative learning capabilities.","A key focus of our research is the challenge of determining confidence levels in learning outcomes, considering the spatial and temporal variability of data sets encountered by independent agents.","To address this issue, we investigate the application of Bayesian neural networks, proposing a novel approach to manage uncertainty in distributed learning environments."],"url":"http://arxiv.org/abs/2403.09141v1","category":"cs.DC"}
{"created":"2024-03-14 07:27:26","title":"Study on Standardizing Working Time: A Case of XYZ Retail Store in Bandung, Indonesia","abstract":"Work time standardization helps to find and reduce wasteful movements and time in the workplace, such as chatting, mobile phone use, insufficient rest, or unproductive tasks. This study aims to map the process of displaying products from the warehouse to the shelves and calculate and determine the standard working time of employees of the Operations Division of PT XYZ Branch who oversee displaying X Milk and Y Bread. The data was collected six times in three weeks, including interviews and observations, and took a sample of 20 pieces on each product to carry out data analysis such as data sufficiency tests and control charts. Several time deviations were found in the display process of X Milk products on all observation days in different activities. Whereas in the process of displaying Y Bread, only the deviation of working time was found on the 4th observation day, which proves that the process needs to have a standard working time so that the activity work time is more controlled. Therefore, the analysis is carried out with the calculation of performance rating, time allowance, normal time, and standard time. The result of the standard time calculation for the display process of X Milk products is 15.83 minutes and Y Bread is 9.18 minutes for each product of 20 units.","sentences":["Work time standardization helps to find and reduce wasteful movements and time in the workplace, such as chatting, mobile phone use, insufficient rest, or unproductive tasks.","This study aims to map the process of displaying products from the warehouse to the shelves and calculate and determine the standard working time of employees of the Operations Division of PT XYZ Branch who oversee displaying X Milk and Y Bread.","The data was collected six times in three weeks, including interviews and observations, and took a sample of 20 pieces on each product to carry out data analysis such as data sufficiency tests and control charts.","Several time deviations were found in the display process of X Milk products on all observation days in different activities.","Whereas in the process of displaying Y Bread, only the deviation of working time was found on the 4th observation day, which proves that the process needs to have a standard working time so that the activity work time is more controlled.","Therefore, the analysis is carried out with the calculation of performance rating, time allowance, normal time, and standard time.","The result of the standard time calculation for the display process of X Milk products is 15.83 minutes and Y Bread is 9.18 minutes for each product of 20 units."],"url":"http://arxiv.org/abs/2403.09138v1","category":"econ.GN"}
{"created":"2024-03-14 06:49:16","title":"ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text","abstract":"Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our approach outperforms baselines in switching between professional and non-professional text generation.","sentences":["Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation.","However, studies into their capacity of switching between styles via fine-tuning remain underexplored.","This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning.","ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text.","Comparative analysis of ProSwitch against both general and specialized language models reveals that our approach outperforms baselines in switching between professional and non-professional text generation."],"url":"http://arxiv.org/abs/2403.09131v1","category":"cs.CL"}
{"created":"2024-03-14 06:17:20","title":"Exploring the Capabilities and Limitations of Large Language Models in the Electric Energy Sector","abstract":"Large Language Models (LLMs) as chatbots have drawn remarkable attention thanks to their versatile capability in natural language processing as well as in a wide range of tasks. While there has been great enthusiasm towards adopting such foundational model-based artificial intelligence tools in all sectors possible, the capabilities and limitations of such LLMs in improving the operation of the electric energy sector need to be explored, and this article identifies fruitful directions in this regard. Key future research directions include data collection systems for fine-tuning LLMs, embedding power system-specific tools in the LLMs, and retrieval augmented generation (RAG)-based knowledge pool to improve the quality of LLM responses and LLMs in safety-critical use cases.","sentences":["Large Language Models (LLMs) as chatbots have drawn remarkable attention thanks to their versatile capability in natural language processing as well as in a wide range of tasks.","While there has been great enthusiasm towards adopting such foundational model-based artificial intelligence tools in all sectors possible, the capabilities and limitations of such LLMs in improving the operation of the electric energy sector need to be explored, and this article identifies fruitful directions in this regard.","Key future research directions include data collection systems for fine-tuning LLMs, embedding power system-specific tools in the LLMs, and retrieval augmented generation (RAG)-based knowledge pool to improve the quality of LLM responses and LLMs in safety-critical use cases."],"url":"http://arxiv.org/abs/2403.09125v1","category":"eess.SY"}
{"created":"2024-03-14 05:35:56","title":"Global existence and asymptotic stability for the Toner-Tu model of flocking","abstract":"This paper deals with the Toner-Tu (TT) model, which is a hydrodynamic model describing the collective motion of numerous self-propelled agents. We analytically study the global-in-time well-posedness of the TT model near the steady-state solution in the ordered phase. We also show the large-time behavior of solutions showing that the steady-state solution is polynomially stable in a Sobolev space in the sense that solutions that are initially close to that steady state converge to that at least polynomially fast as time tends to infinity. Moreover, we investigate the variant of the TT model which describes the dynamics of the actin filament.","sentences":["This paper deals with the Toner-Tu (TT) model, which is a hydrodynamic model describing the collective motion of numerous self-propelled agents.","We analytically study the global-in-time well-posedness of the TT model near the steady-state solution in the ordered phase.","We also show the large-time behavior of solutions showing that the steady-state solution is polynomially stable in a Sobolev space in the sense that solutions that are initially close to that steady state converge to that at least polynomially fast as time tends to infinity.","Moreover, we investigate the variant of the TT model which describes the dynamics of the actin filament."],"url":"http://arxiv.org/abs/2403.09114v1","category":"math.AP"}
{"created":"2024-03-14 05:29:35","title":"AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning","abstract":"Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should be discarded. A meta learning based method is developed to learn these selection variables. The optimal rank is determined by thresholding the values of these variables. Our comprehensive experiments on natural language understanding, generation, and sequence labeling demonstrate the effectiveness of AutoLoRA.","sentences":["Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks.","Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed.","Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective.","Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance.","To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer.","AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should be discarded.","A meta learning based method is developed to learn these selection variables.","The optimal rank is determined by thresholding the values of these variables.","Our comprehensive experiments on natural language understanding, generation, and sequence labeling demonstrate the effectiveness of AutoLoRA."],"url":"http://arxiv.org/abs/2403.09113v1","category":"cs.CL"}
{"created":"2024-03-14 04:43:02","title":"AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI Publications","abstract":"Identifying scientific publications that are within a dynamic field of research often requires costly annotation by subject-matter experts. Resources like widely-accepted classification criteria or field taxonomies are unavailable for a domain like artificial intelligence (AI), which spans emerging topics and technologies. We address these challenges by inferring a functional definition of AI research from existing expert labels, and then evaluating state-of-the-art chatbot models on the task of expert data annotation. Using the arXiv publication database as ground-truth, we experiment with prompt engineering for GPT chatbot models to identify an alternative, automated expert annotation pipeline that assigns AI labels with 94% accuracy. For comparison, we fine-tune SPECTER, a transformer language model pre-trained on scientific publications, that achieves 96% accuracy (only 2% higher than GPT) on classifying AI publications. Our results indicate that with effective prompt engineering, chatbots can be used as reliable data annotators even where subject-area expertise is required. To evaluate the utility of chatbot-annotated datasets on downstream classification tasks, we train a new classifier on GPT-labeled data and compare its performance to the arXiv-trained model. The classifier trained on GPT-labeled data outperforms the arXiv-trained model by nine percentage points, achieving 82% accuracy.","sentences":["Identifying scientific publications that are within a dynamic field of research often requires costly annotation by subject-matter experts.","Resources like widely-accepted classification criteria or field taxonomies are unavailable for a domain like artificial intelligence (AI), which spans emerging topics and technologies.","We address these challenges by inferring a functional definition of AI research from existing expert labels, and then evaluating state-of-the-art chatbot models on the task of expert data annotation.","Using the arXiv publication database as ground-truth, we experiment with prompt engineering for GPT chatbot models to identify an alternative, automated expert annotation pipeline that assigns AI labels with 94% accuracy.","For comparison, we fine-tune SPECTER, a transformer language model pre-trained on scientific publications, that achieves 96% accuracy (only 2% higher than GPT) on classifying AI publications.","Our results indicate that with effective prompt engineering, chatbots can be used as reliable data annotators even where subject-area expertise is required.","To evaluate the utility of chatbot-annotated datasets on downstream classification tasks, we train a new classifier on GPT-labeled data and compare its performance to the arXiv-trained model.","The classifier trained on GPT-labeled data outperforms the arXiv-trained model by nine percentage points, achieving 82% accuracy."],"url":"http://arxiv.org/abs/2403.09097v1","category":"cs.CL"}
{"created":"2024-03-14 04:32:13","title":"MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection","abstract":"The prevalence of fake news across various online sources has had a significant influence on the public. Existing Chinese fake news detection datasets are limited to news sourced solely from Weibo. However, fake news originating from multiple sources exhibits diversity in various aspects, including its content and social context. Methods trained on purely one single news source can hardly be applicable to real-world scenarios. Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese fake news detection dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source fake news. To address this limitation, we constructed the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND, which is composed of news we collected from diverse sources such as social platforms, messaging apps, and traditional online news outlets. Notably, such news has been fact-checked by 14 authoritative fact-checking agencies worldwide. In addition, various existing Chinese fake news detection methods are thoroughly evaluated on our proposed dataset in cross-source, multi-source, and unseen source ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news detection approaches in real-world scenarios.","sentences":["The prevalence of fake news across various online sources has had a significant influence on the public.","Existing Chinese fake news detection datasets are limited to news sourced solely from Weibo.","However, fake news originating from multiple sources exhibits diversity in various aspects, including its content and social context.","Methods trained on purely one single news source can hardly be applicable to real-world scenarios.","Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese fake news detection dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source fake news.","To address this limitation, we constructed the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND, which is composed of news we collected from diverse sources such as social platforms, messaging apps, and traditional online news outlets.","Notably, such news has been fact-checked by 14 authoritative fact-checking agencies worldwide.","In addition, various existing Chinese fake news detection methods are thoroughly evaluated on our proposed dataset in cross-source, multi-source, and unseen source ways.","MCFEND, as a benchmark dataset, aims to advance Chinese fake news detection approaches in real-world scenarios."],"url":"http://arxiv.org/abs/2403.09092v1","category":"cs.CL"}
{"created":"2024-03-14 04:06:13","title":"Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance","abstract":"Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our approach not only boosts the general reasoning performance of LLMs but also makes considerable strides towards their capacity for abstract reasoning, moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts.","sentences":["Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence.","Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities.","This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing.","In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs.","Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances.","To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes.","The results show that our approach not only boosts the general reasoning performance of LLMs but also makes considerable strides towards their capacity for abstract reasoning, moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts."],"url":"http://arxiv.org/abs/2403.09085v1","category":"cs.CL"}
{"created":"2024-03-14 04:01:13","title":"Asymptotically Near-Optimal Hybrid Beamforming for mmWave IRS-Aided MIMO Systems","abstract":"Hybrid beamforming is an emerging technology for massive multiple-input multiple-output (MIMO) systems due to the advantages of lower complexity, cost, and power consumption. Recently, intelligent reflection surface (IRS) has been proposed as the cost-effective technique for robust millimeter-wave (mmWave) MIMO systems. Thus, it is required to jointly optimize a reflection vector and hybrid beamforming matrices for IRS-aided mmWave MIMO systems. Due to the lack of RF chain in the IRS, it is unavailable to acquire the TX-IRS and IRS-RX channels separately. Instead, there are efficient methods to estimate the so-called effective (or cascaded) channel in literature. We for the first time derive the near-optimal solution of the aforementioned joint optimization only using the effective channel. Based on our theoretical analysis, we develop the practical reflection vector and hybrid beamforming matrices by projecting the asymptotic solution into the modulus constraint. Via simulations, it is demonstrated that the proposed construction can outperform the state-of-the-art (SOTA) method, where the latter even requires the knowledge of the TX-IRS ad IRS-RX channels separately. Furthermore, our construction can provide the robustness for channel estimation errors, which is inevitable for practical massive MIMO systems.","sentences":["Hybrid beamforming is an emerging technology for massive multiple-input multiple-output (MIMO) systems due to the advantages of lower complexity, cost, and power consumption.","Recently, intelligent reflection surface (IRS) has been proposed as the cost-effective technique for robust millimeter-wave (mmWave) MIMO systems.","Thus, it is required to jointly optimize a reflection vector and hybrid beamforming matrices for IRS-aided mmWave MIMO systems.","Due to the lack of RF chain in the IRS, it is unavailable to acquire the TX-IRS and IRS-RX channels separately.","Instead, there are efficient methods to estimate the so-called effective (or cascaded) channel in literature.","We for the first time derive the near-optimal solution of the aforementioned joint optimization only using the effective channel.","Based on our theoretical analysis, we develop the practical reflection vector and hybrid beamforming matrices by projecting the asymptotic solution into the modulus constraint.","Via simulations, it is demonstrated that the proposed construction can outperform the state-of-the-art (SOTA) method, where the latter even requires the knowledge of the TX-IRS ad IRS-RX channels separately.","Furthermore, our construction can provide the robustness for channel estimation errors, which is inevitable for practical massive MIMO systems."],"url":"http://arxiv.org/abs/2403.09083v1","category":"eess.SP"}
{"created":"2024-03-14 03:57:10","title":"The absence of monochromatic triangle implies various properly colored spanning trees","abstract":"An edge-colored graph $G$ is called properly colored if every two adjacent edges are assigned different colors. A monochromatic triangle is a cycle of length 3 with all the edges having the same color. Given a tree $T_0$, let $\\mathcal{T}(n,T_0)$ be the collection of $n$-vertex trees that are subdivisions of $T_0$. It is conjectured that for each fixed tree $T_0$ of $k$ edges, there is a function $f(k)$ such that for each integer $n\\geq f(k)$ and each $T\\in \\mathcal{T}(n,T_0)$, every edge-colored complete graph $K_n$ without containing monochromatic triangle must contain a properly colored copy of $T$. We confirm the conjecture in the case that $T_0$ is a star. A weaker version of the above conjecture is also obtained. Moreover, to get a nice quantitative estimation of $f(k)$ requires determining the constraint Ramsey number of a monochromatic triangle and a rainbow $k$-star, which is of independent interest.","sentences":["An edge-colored graph $G$ is called properly colored if every two adjacent edges are assigned different colors.","A monochromatic triangle is a cycle of length 3 with all the edges having the same color.","Given a tree $T_0$, let $\\mathcal{T}(n,T_0)$ be the collection of $n$-vertex trees that are subdivisions of $T_0$. It is conjectured that for each fixed tree $T_0$ of $k$ edges, there is a function $f(k)$ such that for each integer $n\\geq f(k)$ and each $T\\in \\mathcal{T}(n,T_0)$, every edge-colored complete graph $K_n$ without containing monochromatic triangle must contain a properly colored copy of $T$. We confirm the conjecture in the case that $T_0$ is a star.","A weaker version of the above conjecture is also obtained.","Moreover, to get a nice quantitative estimation of $f(k)$ requires determining the constraint Ramsey number of a monochromatic triangle and a rainbow $k$-star, which is of independent interest."],"url":"http://arxiv.org/abs/2403.09082v1","category":"math.CO"}
{"created":"2024-03-14 03:33:46","title":"Large Language Models are Parallel Multilingual Learners","abstract":"In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs. Experimental results show that (1) incorporating more languages help PiM surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neuron activation especially when more languages are added. This phenomenon aligns with the neuroscience insight about synaptic pruning, which removes less used neural connections, strengthens remainders, and then enhances brain intelligence.","sentences":["In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which significantly enhances their comprehension abilities.","To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs.","Experimental results show that (1) incorporating more languages help PiM surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help.","Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon.","Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neuron activation especially when more languages are added.","This phenomenon aligns with the neuroscience insight about synaptic pruning, which removes less used neural connections, strengthens remainders, and then enhances brain intelligence."],"url":"http://arxiv.org/abs/2403.09073v1","category":"cs.CL"}
{"created":"2024-03-14 03:29:58","title":"UniCode: Learning a Unified Codebook for Multimodal Large Language Models","abstract":"In this paper, we propose \\textbf{UniCode}, a novel approach within the domain of multimodal large language models (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals. This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLM's ability to generate images and texts in a multimodal context. Towards this end, we propose a language-driven iterative training paradigm, coupled with an in-context pre-training task we term ``image decompression'', enabling our model to interpret compressed visual data and generate high-quality images.The unified codebook empowers our model to extend visual instruction tuning to non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse stacked quantization approaches in order to compress visual signals into a more compact token representation. Despite using significantly fewer parameters and less data during training, Unicode demonstrates promising capabilities in visual reconstruction and generation. It also achieves performances comparable to leading MLLMs across a spectrum of VQA benchmarks.","sentences":["In this paper, we propose \\textbf{UniCode}, a novel approach within the domain of multimodal large language models (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals.","This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLM's ability to generate images and texts in a multimodal context.","Towards this end, we propose a language-driven iterative training paradigm, coupled with an in-context pre-training task we term ``image decompression'', enabling our model to interpret compressed visual data and generate high-quality images.","The unified codebook empowers our model to extend visual instruction tuning to non-linguistic generation tasks.","Moreover, UniCode is adaptable to diverse stacked quantization approaches in order to compress visual signals into a more compact token representation.","Despite using significantly fewer parameters and less data during training, Unicode demonstrates promising capabilities in visual reconstruction and generation.","It also achieves performances comparable to leading MLLMs across a spectrum of VQA benchmarks."],"url":"http://arxiv.org/abs/2403.09072v1","category":"cs.CV"}
{"created":"2024-03-14 03:07:58","title":"Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery","abstract":"Precise Human Mesh Recovery (HMR) with in-the-wild data is a formidable challenge and is often hindered by depth ambiguities and reduced precision. Existing works resort to either pose priors or multi-modal data such as multi-view or point cloud information, though their methods often overlook the valuable scene-depth information inherently present in a single image. Moreover, achieving robust HMR for out-of-distribution (OOD) data is exceedingly challenging due to inherent variations in pose, shape and depth. Consequently, understanding the underlying distribution becomes a vital subproblem in modeling human forms. Motivated by the need for unambiguous and robust human modeling, we introduce Distribution and depth-aware human mesh recovery (D2A-HMR), an end-to-end transformer architecture meticulously designed to minimize the disparity between distributions and incorporate scene-depth leveraging prior depth information. Our approach demonstrates superior performance in handling OOD data in certain scenarios while consistently achieving competitive results against state-of-the-art HMR methods on controlled datasets.","sentences":["Precise Human Mesh Recovery (HMR) with in-the-wild data is a formidable challenge and is often hindered by depth ambiguities and reduced precision.","Existing works resort to either pose priors or multi-modal data such as multi-view or point cloud information, though their methods often overlook the valuable scene-depth information inherently present in a single image.","Moreover, achieving robust HMR for out-of-distribution (OOD) data is exceedingly challenging due to inherent variations in pose, shape and depth.","Consequently, understanding the underlying distribution becomes a vital subproblem in modeling human forms.","Motivated by the need for unambiguous and robust human modeling, we introduce Distribution and depth-aware human mesh recovery (D2A-HMR), an end-to-end transformer architecture meticulously designed to minimize the disparity between distributions and incorporate scene-depth leveraging prior depth information.","Our approach demonstrates superior performance in handling OOD data in certain scenarios while consistently achieving competitive results against state-of-the-art HMR methods on controlled datasets."],"url":"http://arxiv.org/abs/2403.09063v1","category":"cs.CV"}
{"created":"2024-03-14 03:07:49","title":"TBI Image/Text (TBI-IT): Comprehensive Text and Image Datasets for Traumatic Brain Injury Research","abstract":"In this paper, we introduce a new dataset in the medical field of Traumatic Brain Injury (TBI), called TBI-IT, which includes both electronic medical records (EMRs) and head CT images. This dataset is designed to enhance the accuracy of artificial intelligence in the diagnosis and treatment of TBI. This dataset, built upon the foundation of standard text and image data, incorporates specific annotations within the EMRs, extracting key content from the text information, and categorizes the annotation content of imaging data into five types: brain midline, hematoma, left cerebral ventricle, right cerebral ventricle and fracture. TBI-IT aims to be a foundational dataset for feature learning in image segmentation tasks and named entity recognition.","sentences":["In this paper, we introduce a new dataset in the medical field of Traumatic Brain Injury (TBI), called TBI-IT, which includes both electronic medical records (EMRs) and head CT images.","This dataset is designed to enhance the accuracy of artificial intelligence in the diagnosis and treatment of TBI.","This dataset, built upon the foundation of standard text and image data, incorporates specific annotations within the EMRs, extracting key content from the text information, and categorizes the annotation content of imaging data into five types: brain midline, hematoma, left cerebral ventricle, right cerebral ventricle and fracture.","TBI-IT aims to be a foundational dataset for feature learning in image segmentation tasks and named entity recognition."],"url":"http://arxiv.org/abs/2403.09062v1","category":"eess.IV"}
{"created":"2024-03-14 02:56:32","title":"Performance Analysis on RIS-Aided Wideband Massive MIMO OFDM Systems with Low-Resolution ADCs","abstract":"This paper investigates a reconfigurable intelligent surface (RIS)-aided wideband massive multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) system with low-resolution analog-to-digital converters (ADCs). Frequency-selective Rician fading channels are considered, and the OFDM data transmission process is presented in time domain. This paper derives the closed-form approximate expression of the uplink achievable rate, based on which the asymptotic system performance is analyzed when the number of the antennas at the base station and the number of reflecting elements at the RIS grow to infinity. Besides, the power scaling laws of the considered system are revealed to provide energy-saving insights. Furthermore, this paper proposes a gradient ascent-based algorithm to design the phase shifts of the RIS for maximizing the minimum user rate. Finally, numerical results are presented to verify the correctness of analytical conclusions and draw insights.","sentences":["This paper investigates a reconfigurable intelligent surface (RIS)-aided wideband massive multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) system with low-resolution analog-to-digital converters (ADCs).","Frequency-selective Rician fading channels are considered, and the OFDM data transmission process is presented in time domain.","This paper derives the closed-form approximate expression of the uplink achievable rate, based on which the asymptotic system performance is analyzed when the number of the antennas at the base station and the number of reflecting elements at the RIS grow to infinity.","Besides, the power scaling laws of the considered system are revealed to provide energy-saving insights.","Furthermore, this paper proposes a gradient ascent-based algorithm to design the phase shifts of the RIS for maximizing the minimum user rate.","Finally, numerical results are presented to verify the correctness of analytical conclusions and draw insights."],"url":"http://arxiv.org/abs/2403.09058v1","category":"cs.IT"}
{"created":"2024-03-14 02:55:37","title":"A Continued Pretrained LLM Approach for Automatic Medical Note Generation","abstract":"LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like GPT-4, is too costly for most domain-specific scenarios. We present the first continuously trained 13B Llama2-based LLM that is purpose-built for medical conversations and measured on automated scribing. Our results show that our model outperforms GPT-4 in PubMedQA with 76.6\\% accuracy and matches its performance in summarizing medical conversations into SOAP notes. Notably, our model exceeds GPT-4 in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness.","sentences":["LLMs are revolutionizing NLP tasks.","However, the most powerful LLM, like GPT-4, is too costly for most domain-specific scenarios.","We present the first continuously trained 13B Llama2-based LLM that is purpose-built for medical conversations and measured on automated scribing.","Our results show that our model outperforms GPT-4 in PubMedQA with 76.6\\% accuracy and matches its performance in summarizing medical conversations into SOAP notes.","Notably, our model exceeds GPT-4 in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness."],"url":"http://arxiv.org/abs/2403.09057v1","category":"cs.CL"}
{"created":"2024-03-14 02:55:06","title":"Leveraging Foundation Model Automatic Data Augmentation Strategies and Skeletal Points for Hands Action Recognition in Industrial Assembly Lines","abstract":"On modern industrial assembly lines, many intelligent algorithms have been developed to replace or supervise workers. However, we found that there were bottlenecks in both training datasets and real-time performance when deploying algorithms on actual assembly line. Therefore, we developed a promising strategy for expanding industrial datasets, which utilized large models with strong generalization abilities to achieve efficient, high-quality, and large-scale dataset expansion, solving the problem of insufficient and low-quality industrial datasets. We also applied this strategy to video action recognition. We proposed a method of converting hand action recognition problems into hand skeletal trajectory classification problems, which solved the real-time performance problem of industrial algorithms. In the \"hand movements during wire insertion\" scenarios on the actual assembly line, the accuracy of hand action recognition reached 98.8\\%. We conducted detailed experimental analysis to demonstrate the effectiveness and superiority of the method, and deployed the entire process on Midea's actual assembly line.","sentences":["On modern industrial assembly lines, many intelligent algorithms have been developed to replace or supervise workers.","However, we found that there were bottlenecks in both training datasets and real-time performance when deploying algorithms on actual assembly line.","Therefore, we developed a promising strategy for expanding industrial datasets, which utilized large models with strong generalization abilities to achieve efficient, high-quality, and large-scale dataset expansion, solving the problem of insufficient and low-quality industrial datasets.","We also applied this strategy to video action recognition.","We proposed a method of converting hand action recognition problems into hand skeletal trajectory classification problems, which solved the real-time performance problem of industrial algorithms.","In the \"hand movements during wire insertion\" scenarios on the actual assembly line, the accuracy of hand action recognition reached 98.8\\%.","We conducted detailed experimental analysis to demonstrate the effectiveness and superiority of the method, and deployed the entire process on Midea's actual assembly line."],"url":"http://arxiv.org/abs/2403.09056v1","category":"cs.CV"}
{"created":"2024-03-14 02:42:42","title":"Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference","abstract":"Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces \"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as \"key\" tokens. Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function. This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy. We evaluate Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms. Our assessment encompasses a variety of tasks, with a particular emphasis on summarization and conversation tasks involving extended contexts. Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy.","sentences":["Transformers have emerged as the underpinning architecture for Large Language Models (LLMs).","In generative language models, the inference process involves two primary phases: prompt processing and token generation.","Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache.","This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units.","This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   ","This paper introduces \"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization.","Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as \"key\" tokens.","Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function.","This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy.","We evaluate Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms.","Our assessment encompasses a variety of tasks, with a particular emphasis on summarization and conversation tasks involving extended contexts.","Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy."],"url":"http://arxiv.org/abs/2403.09054v1","category":"cs.LG"}
{"created":"2024-03-14 02:42:19","title":"Towards a theory of model distillation","abstract":"Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity.","sentences":["Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15].","Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   ","To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84].","As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity."],"url":"http://arxiv.org/abs/2403.09053v1","category":"cs.LG"}
{"created":"2024-03-14 02:26:10","title":"Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs","abstract":"Anomaly detection in dynamic graphs presents a significant challenge due to the temporal evolution of graph structures and attributes. The conventional approaches that tackle this problem typically employ an unsupervised learning framework, capturing normality patterns with exclusive normal data during training and identifying deviations as anomalies during testing. However, these methods face critical drawbacks: they either only depend on proxy tasks for general representation without directly pinpointing normal patterns, or they neglect to differentiate between spatial and temporal normality patterns, leading to diminished efficacy in anomaly detection. To address these challenges, we introduce a novel Spatial-Temporal memories-enhanced graph autoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs) and gated temporal convolution layers to extract spatial features and temporal features, respectively. Then STRIPE incorporates separate spatial and temporal memory networks, which capture and store prototypes of normal patterns, thereby preserving the uniqueness of spatial and temporal normality. After that, through a mutual attention mechanism, these stored patterns are then retrieved and integrated with encoded graph embeddings. Finally, the integrated features are fed into the decoder to reconstruct the graph streams which serve as the proxy task for anomaly detection. This comprehensive approach not only minimizes reconstruction errors but also refines the model by emphasizing the compactness and distinctiveness of the embeddings in relation to the nearest memory prototypes. Through extensive testing, STRIPE has demonstrated a superior capability to discern anomalies by effectively leveraging the distinct spatial and temporal dynamics of dynamic graphs, significantly outperforming existing methodologies, with an average improvement of 15.39% on AUC values.","sentences":["Anomaly detection in dynamic graphs presents a significant challenge due to the temporal evolution of graph structures and attributes.","The conventional approaches that tackle this problem typically employ an unsupervised learning framework, capturing normality patterns with exclusive normal data during training and identifying deviations as anomalies during testing.","However, these methods face critical drawbacks: they either only depend on proxy tasks for general representation without directly pinpointing normal patterns, or they neglect to differentiate between spatial and temporal normality patterns, leading to diminished efficacy in anomaly detection.","To address these challenges, we introduce a novel Spatial-Temporal memories-enhanced graph autoencoder (STRIPE).","Initially, STRIPE employs Graph Neural Networks (GNNs) and gated temporal convolution layers to extract spatial features and temporal features, respectively.","Then STRIPE incorporates separate spatial and temporal memory networks, which capture and store prototypes of normal patterns, thereby preserving the uniqueness of spatial and temporal normality.","After that, through a mutual attention mechanism, these stored patterns are then retrieved and integrated with encoded graph embeddings.","Finally, the integrated features are fed into the decoder to reconstruct the graph streams which serve as the proxy task for anomaly detection.","This comprehensive approach not only minimizes reconstruction errors but also refines the model by emphasizing the compactness and distinctiveness of the embeddings in relation to the nearest memory prototypes.","Through extensive testing, STRIPE has demonstrated a superior capability to discern anomalies by effectively leveraging the distinct spatial and temporal dynamics of dynamic graphs, significantly outperforming existing methodologies, with an average improvement of 15.39% on AUC values."],"url":"http://arxiv.org/abs/2403.09039v1","category":"cs.LG"}
{"created":"2024-03-14 01:51:35","title":"CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences","abstract":"Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs. By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback. We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences. Our results show that CodeLlama-7B-Instruct, aligned through reinforcement learning from AI feedback (RLAIF) with direct preference optimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B LLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference tuning. Furthermore, we show our DPO-aligned CodeLlama model improves functional correctness on HumanEval+ compared to the unaligned base model. Therefore, our contributions bridge the gap in preference tuning of LLMs for code and set the stage for further advancements in model alignment and RLAIF for code intelligence. Our code and data are available at https://github.com/martin-wey/CodeUltraFeedback.","sentences":["Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs.","By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment.","In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback.","We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback.","We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences.","Our results show that CodeLlama-7B-Instruct, aligned through reinforcement learning from AI feedback (RLAIF) with direct preference optimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B LLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference tuning.","Furthermore, we show our DPO-aligned CodeLlama model improves functional correctness on HumanEval+ compared to the unaligned base model.","Therefore, our contributions bridge the gap in preference tuning of LLMs for code and set the stage for further advancements in model alignment and RLAIF for code intelligence.","Our code and data are available at https://github.com/martin-wey/CodeUltraFeedback."],"url":"http://arxiv.org/abs/2403.09032v1","category":"cs.SE"}
{"created":"2024-03-14 01:46:30","title":"An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals","abstract":"This study aimed to develop a deep learning model for the classification of bearing faults in wind turbine generators from acoustic signals. A convolutional LSTM model was successfully constructed and trained by using audio data from five predefined fault types for both training and validation. To create the dataset, raw audio signal data was collected and processed in frames to capture time and frequency domain information. The model exhibited outstanding accuracy on training samples and demonstrated excellent generalization ability during validation, indicating its proficiency of generalization capability. On the test samples, the model achieved remarkable classification performance, with an overall accuracy exceeding 99.5%, and a false positive rate of less than 1% for normal status. The findings of this study provide essential support for the diagnosis and maintenance of bearing faults in wind turbine generators, with the potential to enhance the reliability and efficiency of wind power generation.","sentences":["This study aimed to develop a deep learning model for the classification of bearing faults in wind turbine generators from acoustic signals.","A convolutional LSTM model was successfully constructed and trained by using audio data from five predefined fault types for both training and validation.","To create the dataset, raw audio signal data was collected and processed in frames to capture time and frequency domain information.","The model exhibited outstanding accuracy on training samples and demonstrated excellent generalization ability during validation, indicating its proficiency of generalization capability.","On the test samples, the model achieved remarkable classification performance, with an overall accuracy exceeding 99.5%, and a false positive rate of less than 1% for normal status.","The findings of this study provide essential support for the diagnosis and maintenance of bearing faults in wind turbine generators, with the potential to enhance the reliability and efficiency of wind power generation."],"url":"http://arxiv.org/abs/2403.09030v1","category":"cs.SD"}
{"created":"2024-03-14 01:40:40","title":"Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset","abstract":"Using vision-language models (VLMs) in web development presents a promising strategy to increase efficiency and unblock no-code solutions: by providing a screenshot or a sketch of a UI, a VLM could generate the code to reproduce it, for instance in a language like HTML. Despite the advancements in VLMs for various tasks, the specific challenge of converting a screenshot into a corresponding HTML has been minimally explored. We posit that this is mainly due to the absence of a suitable, high-quality dataset. This work introduces WebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and their corresponding screenshots. We fine-tune a foundational VLM on our dataset and show proficiency in converting webpage screenshots to functional HTML code. To accelerate the research in this area, we open-source WebSight.","sentences":["Using vision-language models (VLMs) in web development presents a promising strategy to increase efficiency and unblock no-code solutions: by providing a screenshot or a sketch of a UI, a VLM could generate the code to reproduce it, for instance in a language like HTML.","Despite the advancements in VLMs for various tasks, the specific challenge of converting a screenshot into a corresponding HTML has been minimally explored.","We posit that this is mainly due to the absence of a suitable, high-quality dataset.","This work introduces WebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and their corresponding screenshots.","We fine-tune a foundational VLM on our dataset and show proficiency in converting webpage screenshots to functional HTML code.","To accelerate the research in this area, we open-source WebSight."],"url":"http://arxiv.org/abs/2403.09029v1","category":"cs.HC"}
{"created":"2024-03-14 01:39:40","title":"VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework","abstract":"With the emergence of large language models (LLMs) and vision foundation models, how to combine the intelligence and capacity of these open-sourced or API-available models to achieve open-world visual perception remains an open question. In this paper, we introduce VisionGPT to consolidate and automate the integration of state-of-the-art foundation models, thereby facilitating vision-language understanding and the development of vision-oriented AI. VisionGPT builds upon a generalized multimodal framework that distinguishes itself through three key features: (1) utilizing LLMs (e.g., LLaMA-2) as the pivot to break down users' requests into detailed action proposals to call suitable foundation models; (2) integrating multi-source outputs from foundation models automatically and generating comprehensive responses for users; (3) adaptable to a wide range of applications such as text-conditioned image understanding/generation/editing and visual question answering. This paper outlines the architecture and capabilities of VisionGPT, demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency, versatility, and generalization, and performance. Our code and models will be made publicly available. Keywords: VisionGPT, Open-world visual perception, Vision-language understanding, Large language model, and Foundation model","sentences":["With the emergence of large language models (LLMs) and vision foundation models, how to combine the intelligence and capacity of these open-sourced or API-available models to achieve open-world visual perception remains an open question.","In this paper, we introduce VisionGPT to consolidate and automate the integration of state-of-the-art foundation models, thereby facilitating vision-language understanding and the development of vision-oriented AI.","VisionGPT builds upon a generalized multimodal framework that distinguishes itself through three key features: (1) utilizing LLMs (e.g., LLaMA-2) as the pivot to break down users' requests into detailed action proposals to call suitable foundation models; (2) integrating multi-source outputs from foundation models automatically and generating comprehensive responses for users; (3) adaptable to a wide range of applications such as text-conditioned image understanding/generation/editing and visual question answering.","This paper outlines the architecture and capabilities of VisionGPT, demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency, versatility, and generalization, and performance.","Our code and models will be made publicly available.","Keywords: VisionGPT, Open-world visual perception, Vision-language understanding, Large language model, and Foundation model"],"url":"http://arxiv.org/abs/2403.09027v1","category":"cs.CV"}
{"created":"2024-03-14 01:28:13","title":"Semiparametric Token-Sequence Co-Supervision","abstract":"In this work, we introduce a semiparametric token-sequence co-supervision training method. It trains a language model by simultaneously leveraging supervision from the traditional next token prediction loss which is calculated over the parametric token embedding space and the next sequence prediction loss which is calculated over the nonparametric sequence embedding space. The nonparametric sequence embedding space is constructed by a separate language model tasked to condense an input text into a single representative embedding. Our experiments demonstrate that a model trained via both supervisions consistently surpasses models trained via each supervision independently. Analysis suggests that this co-supervision encourages a broader generalization capability across the model. Especially, the robustness of parametric token space which is established during the pretraining step tends to effectively enhance the stability of nonparametric sequence embedding space, a new space established by another language model.","sentences":["In this work, we introduce a semiparametric token-sequence co-supervision training method.","It trains a language model by simultaneously leveraging supervision from the traditional next token prediction loss which is calculated over the parametric token embedding space and the next sequence prediction loss which is calculated over the nonparametric sequence embedding space.","The nonparametric sequence embedding space is constructed by a separate language model tasked to condense an input text into a single representative embedding.","Our experiments demonstrate that a model trained via both supervisions consistently surpasses models trained via each supervision independently.","Analysis suggests that this co-supervision encourages a broader generalization capability across the model.","Especially, the robustness of parametric token space which is established during the pretraining step tends to effectively enhance the stability of nonparametric sequence embedding space, a new space established by another language model."],"url":"http://arxiv.org/abs/2403.09024v1","category":"cs.CL"}
{"created":"2024-03-14 01:24:19","title":"Quantum Annealing Approach for the Optimal Real-time Traffic Control using QUBO","abstract":"Traffic congestion is one of the major issues in urban areas, particularly when traffic loads exceed the roads capacity, resulting in higher petrol consumption and carbon emissions as well as delays and stress for road users. In Asia, the traffic situation can be further deteriorated by road sharing of scooters. How to control the traffic flow to mitigate the congestion has been one of the central issues in transportation research. In this study, we employ a quantum annealing approach to optimize the traffic signals control at a real-life intersection with mixed traffic flows of vehicles and scooters. Considering traffic flow is a continuous and emerging phenomenon, we used quadratic unconstrained binary optimization (QUBO) formalism for traffic optimization, which has a natural equivalence to the Ising model and can be solved efficiently on the quantum annealers, quantum computers or digital annealers. In this article, we first applied the QUBO traffic optimization to artificially generated traffic for a simple intersection, and then we used real-time traffic data to simulate a real Dongda-Keyuan intersection with dedicated cars and scooter lanes, as well as mixed scooter and car lanes. We introduced two types of traffic light control systems for traffic optimization C-QUBO and QUBO. Our rigorous QUBO optimizations show that C-QUBO and QUBO outperform the commonly used fixed cycle method, with QUBO outperforming C-QUBO in some instances. It has been found that QUBO optimization significantly relieves traffic congestion for the unbalanced traffic volume. Furthermore, we found that dynamic changes in traffic light signal duration greatly reduce traffic congestion.","sentences":["Traffic congestion is one of the major issues in urban areas, particularly when traffic loads exceed the roads capacity, resulting in higher petrol consumption and carbon emissions as well as delays and stress for road users.","In Asia, the traffic situation can be further deteriorated by road sharing of scooters.","How to control the traffic flow to mitigate the congestion has been one of the central issues in transportation research.","In this study, we employ a quantum annealing approach to optimize the traffic signals control at a real-life intersection with mixed traffic flows of vehicles and scooters.","Considering traffic flow is a continuous and emerging phenomenon, we used quadratic unconstrained binary optimization (QUBO) formalism for traffic optimization, which has a natural equivalence to the Ising model and can be solved efficiently on the quantum annealers, quantum computers or digital annealers.","In this article, we first applied the QUBO traffic optimization to artificially generated traffic for a simple intersection, and then we used real-time traffic data to simulate a real Dongda-Keyuan intersection with dedicated cars and scooter lanes, as well as mixed scooter and car lanes.","We introduced two types of traffic light control systems for traffic optimization C-QUBO and QUBO.","Our rigorous QUBO optimizations show that C-QUBO and QUBO outperform the commonly used fixed cycle method, with QUBO outperforming C-QUBO in some instances.","It has been found that QUBO optimization significantly relieves traffic congestion for the unbalanced traffic volume.","Furthermore, we found that dynamic changes in traffic light signal duration greatly reduce traffic congestion."],"url":"http://arxiv.org/abs/2403.09023v1","category":"quant-ph"}
{"created":"2024-03-14 00:45:24","title":"AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic","abstract":"The swift progress and widespread acceptance of artificial intelligence (AI) systems highlight a pressing requirement to comprehend both the capabilities and potential risks associated with AI. Given the linguistic complexity, cultural richness, and underrepresented status of Arabic in AI research, there is a pressing need to focus on Large Language Models (LLMs) performance and safety for Arabic related tasks. Despite some progress in their development, there is a lack of comprehensive trustworthiness evaluation benchmarks which presents a major challenge in accurately assessing and improving the safety of LLMs when prompted in Arabic. In this paper, we introduce AraTrust 1, the first comprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises 516 human-written multiple-choice questions addressing diverse dimensions related to truthfulness, ethics, safety, physical health, mental health, unfairness, illegal activities, privacy, and offensive language. By introducing AraTrust, we aim to promote collaborative efforts to create safer and more trustworthy LLMs for Arabic users. We evaluated a set of LLMs against our benchmark to assess its trustworthiness. GPT-4 showed to be the most trustworthy regarding Arabic language.","sentences":["The swift progress and widespread acceptance of artificial intelligence (AI) systems highlight a pressing requirement to comprehend both the capabilities and potential risks associated with AI.","Given the linguistic complexity, cultural richness, and underrepresented status of Arabic in AI research, there is a pressing need to focus on Large Language Models (LLMs) performance and safety for Arabic related tasks.","Despite some progress in their development, there is a lack of comprehensive trustworthiness evaluation benchmarks which presents a major challenge in accurately assessing and improving the safety of LLMs when prompted in Arabic.","In this paper, we introduce AraTrust 1, the first comprehensive trustworthiness benchmark for LLMs in Arabic.","AraTrust comprises 516 human-written multiple-choice questions addressing diverse dimensions related to truthfulness, ethics, safety, physical health, mental health, unfairness, illegal activities, privacy, and offensive language.","By introducing AraTrust, we aim to promote collaborative efforts to create safer and more trustworthy LLMs for Arabic users.","We evaluated a set of LLMs against our benchmark to assess its trustworthiness.","GPT-4 showed to be the most trustworthy regarding Arabic language."],"url":"http://arxiv.org/abs/2403.09017v1","category":"cs.CL"}
{"created":"2024-03-14 00:30:17","title":"Urban mapping in Dar es Salaam using AJIVE","abstract":"Mapping deprivation in urban areas is important, for example for identifying areas of greatest need and planning interventions. Traditional ways of obtaining deprivation estimates are based on either census or household survey data, which in many areas is unavailable or difficult to collect. However, there has been a huge rise in the amount of new, non-traditional forms of data, such as satellite imagery and cell-phone call-record data, which may contain information useful for identifying deprivation. We use Angle-Based Joint and Individual Variation Explained (AJIVE) to jointly model satellite imagery data, cell-phone data, and survey data for the city of Dar es Salaam, Tanzania. We first identify interpretable low-dimensional structure from the imagery and cell-phone data, and find that we can use these to identify deprivation. We then consider what is gained from further incorporating the more traditional and costly survey data. We also introduce a scalar measure of deprivation as a response variable to be predicted, and consider various approaches to multiview regression, including using AJIVE scores as predictors.","sentences":["Mapping deprivation in urban areas is important, for example for identifying areas of greatest need and planning interventions.","Traditional ways of obtaining deprivation estimates are based on either census or household survey data, which in many areas is unavailable or difficult to collect.","However, there has been a huge rise in the amount of new, non-traditional forms of data, such as satellite imagery and cell-phone call-record data, which may contain information useful for identifying deprivation.","We use Angle-Based Joint and Individual Variation Explained (AJIVE) to jointly model satellite imagery data, cell-phone data, and survey data for the city of Dar es Salaam, Tanzania.","We first identify interpretable low-dimensional structure from the imagery and cell-phone data, and find that we can use these to identify deprivation.","We then consider what is gained from further incorporating the more traditional and costly survey data.","We also introduce a scalar measure of deprivation as a response variable to be predicted, and consider various approaches to multiview regression, including using AJIVE scores as predictors."],"url":"http://arxiv.org/abs/2403.09014v1","category":"stat.AP"}
{"created":"2024-03-13 23:37:02","title":"Building-block flow model for computational fluids","abstract":"We introduce a closure model for computational fluid dynamics, referred to as the Building-block Flow Model (BFM). The foundation of the model rests on the premise that a finite collection of simple flows encapsulates the essential physics necessary to predict more complex scenarios. The BFM is implemented using artificial neural networks and introduces five unique advancements within the framework of large-eddy simulation: (1) It is designed to predict multiple flow regimes (wall turbulence under zero, favorable, adverse mean-pressure-gradient, and separation); (2) It unifies the closure model at solid boundaries (i.e., the wall model) and the rest of the flow (i.e., the subgrid-scale model) into a single entity; (3) It ensures consistency with numerical schemes and gridding strategy by accounting for numerical errors; (4) It is directly applicable to arbitrary complex geometries; (5) It can be scaled up to model additional flow physics in the future if needed (e.g., shockwaves and laminar-to-turbulent transition). The BFM is utilized to predict key quantities of interest in turbulent channel flows, a Gaussian bump, and an aircraft in a landing configuration. In all cases, the BFM demonstrates similar or superior capabilities in terms of accuracy and computational efficiency compared to previous state-of-the-art closure models.","sentences":["We introduce a closure model for computational fluid dynamics, referred to as the Building-block Flow Model (BFM).","The foundation of the model rests on the premise that a finite collection of simple flows encapsulates the essential physics necessary to predict more complex scenarios.","The BFM is implemented using artificial neural networks and introduces five unique advancements within the framework of large-eddy simulation: (1) It is designed to predict multiple flow regimes (wall turbulence under zero, favorable, adverse mean-pressure-gradient, and separation); (2) It unifies the closure model at solid boundaries (i.e., the wall model) and the rest of the flow (i.e., the subgrid-scale model) into a single entity; (3) It ensures consistency with numerical schemes and gridding strategy by accounting for numerical errors; (4) It is directly applicable to arbitrary complex geometries; (5) It can be scaled up to model additional flow physics in the future if needed (e.g., shockwaves and laminar-to-turbulent transition).","The BFM is utilized to predict key quantities of interest in turbulent channel flows, a Gaussian bump, and an aircraft in a landing configuration.","In all cases, the BFM demonstrates similar or superior capabilities in terms of accuracy and computational efficiency compared to previous state-of-the-art closure models."],"url":"http://arxiv.org/abs/2403.09000v1","category":"physics.flu-dyn"}
{"created":"2024-03-13 23:23:51","title":"Improved bass model using sales proportional average for one condition of mono peak curves","abstract":"\"This study provides a modified Bass model to deal with trend curves for basic issues of relevance to individuals from all over the world, for which we collected 16 data sets from 2004 to 2022 and that are available on Google servers as \"google trends\". It was discovered that the Bass model did not forecast well for curves that have a mono peak with a sharp decrease to some level then have semi-stable with small decrement sales for a long time, thus a new parameter based on r1 and r2 (ratios of average sales) was introduced, which improved the model's prediction ability and provided better results. The model was also applied to a data set taken from the Kaggle website about a subscriber digital product offering for financial services that include newsletters, webinars, and investment recommendations. The data contain 508932 data points about the products sold during 2016-2022. Compared to the traditional Bass model, the modified model showed better results in dealing with this condition, as the expected curve shape was closer to real sales, and the sum of squares error (SSE) value was reduced to a ratio ranging between (36.35-79.3%). Therefore, the improved model can be relied upon in these conditions.\"","sentences":["\"This study provides a modified Bass model to deal with trend curves for basic issues of relevance to individuals from all over the world, for which we collected 16 data sets from 2004 to 2022 and that are available on Google servers as \"google trends\".","It was discovered that the Bass model did not forecast well for curves that have a mono peak with a sharp decrease to some level then have semi-stable with small decrement sales for a long time, thus a new parameter based on r1 and r2 (ratios of average sales) was introduced, which improved the model's prediction ability and provided better results.","The model was also applied to a data set taken from the Kaggle website about a subscriber digital product offering for financial services that include newsletters, webinars, and investment recommendations.","The data contain 508932 data points about the products sold during 2016-2022.","Compared to the traditional Bass model, the modified model showed better results in dealing with this condition, as the expected curve shape was closer to real sales, and the sum of squares error (SSE) value was reduced to a ratio ranging between (36.35-79.3%).","Therefore, the improved model can be relied upon in these conditions.\""],"url":"http://arxiv.org/abs/2403.08993v1","category":"cs.CE"}
{"created":"2024-03-13 23:14:49","title":"Security Assumptions in Dispersive-Optics QKD","abstract":"Quantum key distribution (QKD) seeks to provide a method of generating cryptographically-secure keys between remote parties while guaranteeing unconditional security. Implementations of high-dimensional QKD using dispersive-optics (DO-QKD) have been proposed to allow for multiple secure bits to be transmitted per photon while remaining cost-effective and scalable using existing telecommunication technology [1]. In the recent literature, there have been a number of experimental realizations of DO-QKD systems [2-6], with security analysis based on the treatment in Ref. [1]. Here we demonstrate that in the case of finite dispersion, the model assumed for the eavesdropper's attack in Ref. [1] is non-optimal for the eavesdropper, which leads to a significant overestimation of the secure key rate between parties. We consider an alternative attack model that Alice and Bob find indistinguishable from the Ref. [1] model, as long as they are restricted to making the measurements typical in DO-QKD. We provide concrete examples where a significant gap exists between the Holevo information, and therefore the secret key rate, predicted by the two models. We further analyze the experiment in Ref. [2] as an example of a case where secure key is predicted according to the Ref. [1] model, but where in fact there is zero secure key rate when considering the full set of collective attacks that an eavesdropper may perform.","sentences":["Quantum key distribution (QKD) seeks to provide a method of generating cryptographically-secure keys between remote parties while guaranteeing unconditional security.","Implementations of high-dimensional QKD using dispersive-optics (DO-QKD) have been proposed to allow for multiple secure bits to be transmitted per photon while remaining cost-effective and scalable using existing telecommunication technology [1].","In the recent literature, there have been a number of experimental realizations of DO-QKD systems [2-6], with security analysis based on the treatment in Ref.","[1].","Here we demonstrate that in the case of finite dispersion, the model assumed for the eavesdropper's attack in Ref.","[1] is non-optimal for the eavesdropper, which leads to a significant overestimation of the secure key rate between parties.","We consider an alternative attack model that Alice and Bob find indistinguishable from the Ref.","[1] model, as long as they are restricted to making the measurements typical in DO-QKD.","We provide concrete examples where a significant gap exists between the Holevo information, and therefore the secret key rate, predicted by the two models.","We further analyze the experiment in Ref.","[2] as an example of a case where secure key is predicted according to the Ref.","[1] model, but where in fact there is zero secure key rate when considering the full set of collective attacks that an eavesdropper may perform."],"url":"http://arxiv.org/abs/2403.08992v1","category":"quant-ph"}
{"created":"2024-03-13 22:19:06","title":"Safe Road-Crossing by Autonomous Wheelchairs: a Novel Dataset and its Experimental Evaluation","abstract":"Safe road-crossing by self-driving vehicles is a crucial problem to address in smart-cities. In this paper, we introduce a multi-sensor fusion approach to support road-crossing decisions in a system composed by an autonomous wheelchair and a flying drone featuring a robust sensory system made of diverse and redundant components. To that aim, we designed an analytical danger function based on explainable physical conditions evaluated by single sensors, including those using machine learning and artificial vision. As a proof-of-concept, we provide an experimental evaluation in a laboratory environment, showing the advantages of using multiple sensors, which can improve decision accuracy and effectively support safety assessment. We made the dataset available to the scientific community for further experimentation. The work has been developed in the context of an European project named REXASI-PRO, which aims to develop trustworthy artificial intelligence for social navigation of people with reduced mobility.","sentences":["Safe road-crossing by self-driving vehicles is a crucial problem to address in smart-cities.","In this paper, we introduce a multi-sensor fusion approach to support road-crossing decisions in a system composed by an autonomous wheelchair and a flying drone featuring a robust sensory system made of diverse and redundant components.","To that aim, we designed an analytical danger function based on explainable physical conditions evaluated by single sensors, including those using machine learning and artificial vision.","As a proof-of-concept, we provide an experimental evaluation in a laboratory environment, showing the advantages of using multiple sensors, which can improve decision accuracy and effectively support safety assessment.","We made the dataset available to the scientific community for further experimentation.","The work has been developed in the context of an European project named REXASI-PRO, which aims to develop trustworthy artificial intelligence for social navigation of people with reduced mobility."],"url":"http://arxiv.org/abs/2403.08984v1","category":"cs.RO"}
{"created":"2024-03-13 21:43:24","title":"Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields","abstract":"Anatomical trees play a central role in clinical diagnosis and treatment planning. However, accurately representing anatomical trees is challenging due to their varying and complex topology and geometry. Traditional methods for representing tree structures, captured using medical imaging, while invaluable for visualizing vascular and bronchial networks, exhibit drawbacks in terms of limited resolution, flexibility, and efficiency. Recently, implicit neural representations (INRs) have emerged as a powerful tool for representing shapes accurately and efficiently. We propose a novel approach for representing anatomical trees using INR, while also capturing the distribution of a set of trees via denoising diffusion in the space of INRs. We accurately capture the intricate geometries and topologies of anatomical trees at any desired resolution. Through extensive qualitative and quantitative evaluation, we demonstrate high-fidelity tree reconstruction with arbitrary resolution yet compact storage, and versatility across anatomical sites and tree complexities.","sentences":["Anatomical trees play a central role in clinical diagnosis and treatment planning.","However, accurately representing anatomical trees is challenging due to their varying and complex topology and geometry.","Traditional methods for representing tree structures, captured using medical imaging, while invaluable for visualizing vascular and bronchial networks, exhibit drawbacks in terms of limited resolution, flexibility, and efficiency.","Recently, implicit neural representations (INRs) have emerged as a powerful tool for representing shapes accurately and efficiently.","We propose a novel approach for representing anatomical trees using INR, while also capturing the distribution of a set of trees via denoising diffusion in the space of INRs.","We accurately capture the intricate geometries and topologies of anatomical trees at any desired resolution.","Through extensive qualitative and quantitative evaluation, we demonstrate high-fidelity tree reconstruction with arbitrary resolution yet compact storage, and versatility across anatomical sites and tree complexities."],"url":"http://arxiv.org/abs/2403.08974v1","category":"cs.CV"}
{"created":"2024-03-13 21:34:36","title":"Measurements and modeling of induced flow in collective vertical migration","abstract":"Hydrodynamic interactions among swimming or flying organisms can lead to complex flows on the scale of the group. These emergent fluid dynamics are often more complex than a linear superposition of individual organism flows, especially at intermediate Reynolds numbers. This paper presents an approach to estimate the flow induced by multiple swimmer wakes in proximity using an analytical model that conserves mass and momentum in the aggregation. This analytical model was informed by and validated with empirical measurements of induced vertical migrations of brine shrimp, $\\textit{Artemia salina}$. The response of individual swimmers to ambient background flow and light intensity was evaluated. In addition, the time-resolved three-dimensional spatial configuration of the swimmers was measured using a recently developed laser scanning system. Computational experiments using the analytical model found that the induced flow at the front of the aggregation was insensitive to the presence of downstream swimmers, with the induced flow reaching an asymptote beyond a threshold aggregation length. Closer swimmer spacing led to higher induced flow, in some cases leading to model predictions of induced flow exceeding swimmer speeds required to maintain a stable spatial configuration. This result was reconciled by comparing two different models for the near-wake of each swimmer. Our results demonstrate that aggregation-scale flows result from a complex, yet predictable interplay amongst organism-scale wake structure, swimmer spacing and configuration, and aggregation size.","sentences":["Hydrodynamic interactions among swimming or flying organisms can lead to complex flows on the scale of the group.","These emergent fluid dynamics are often more complex than a linear superposition of individual organism flows, especially at intermediate Reynolds numbers.","This paper presents an approach to estimate the flow induced by multiple swimmer wakes in proximity using an analytical model that conserves mass and momentum in the aggregation.","This analytical model was informed by and validated with empirical measurements of induced vertical migrations of brine shrimp, $\\textit{Artemia salina}$.","The response of individual swimmers to ambient background flow and light intensity was evaluated.","In addition, the time-resolved three-dimensional spatial configuration of the swimmers was measured using a recently developed laser scanning system.","Computational experiments using the analytical model found that the induced flow at the front of the aggregation was insensitive to the presence of downstream swimmers, with the induced flow reaching an asymptote beyond a threshold aggregation length.","Closer swimmer spacing led to higher induced flow, in some cases leading to model predictions of induced flow exceeding swimmer speeds required to maintain a stable spatial configuration.","This result was reconciled by comparing two different models for the near-wake of each swimmer.","Our results demonstrate that aggregation-scale flows result from a complex, yet predictable interplay amongst organism-scale wake structure, swimmer spacing and configuration, and aggregation size."],"url":"http://arxiv.org/abs/2403.08973v1","category":"physics.flu-dyn"}
{"created":"2024-03-13 21:30:01","title":"The Full-scale Assembly Simulation Testbed (FAST) Dataset","abstract":"In recent years, numerous researchers have begun investigating how virtual reality (VR) tracking and interaction data can be used for a variety of machine learning purposes, including user identification, predicting cybersickness, and estimating learning gains. One constraint for this research area is the dearth of open datasets. In this paper, we present a new open dataset captured with our VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset consists of data collected from 108 participants (50 females, 56 males, 2 non-binary) learning how to assemble two distinct full-scale structures in VR. In addition to explaining how the dataset was collected and describing the data included, we discuss how the dataset may be used by future researchers.","sentences":["In recent years, numerous researchers have begun investigating how virtual reality (VR) tracking and interaction data can be used for a variety of machine learning purposes, including user identification, predicting cybersickness, and estimating learning gains.","One constraint for this research area is the dearth of open datasets.","In this paper, we present a new open dataset captured with our VR-based Full-scale Assembly Simulation Testbed (FAST).","This dataset consists of data collected from 108 participants (50 females, 56 males, 2 non-binary) learning how to assemble two distinct full-scale structures in VR.","In addition to explaining how the dataset was collected and describing the data included, we discuss how the dataset may be used by future researchers."],"url":"http://arxiv.org/abs/2403.08969v1","category":"cs.HC"}
{"created":"2024-03-13 21:19:12","title":"PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for Whole Slide Image Classification and Captioning","abstract":"In the field of computational histopathology, both whole slide images (WSIs) and diagnostic captions provide valuable insights for making diagnostic decisions. However, aligning WSIs with diagnostic captions presents a significant challenge. This difficulty arises from two main factors: 1) Gigapixel WSIs are unsuitable for direct input into deep learning models, and the redundancy and correlation among the patches demand more attention; and 2) Authentic WSI diagnostic captions are extremely limited, making it difficult to train an effective model. To overcome these obstacles, we present PathM3, a multimodal, multi-task, multiple instance learning (MIL) framework for WSI classification and captioning. PathM3 adapts a query-based transformer to effectively align WSIs with diagnostic captions. Given that histopathology visual patterns are redundantly distributed across WSIs, we aggregate each patch feature with MIL method that considers the correlations among instances. Furthermore, our PathM3 overcomes data scarcity in WSI-level captions by leveraging limited WSI diagnostic caption data in the manner of multi-task joint learning. Extensive experiments with improved classification accuracy and caption generation demonstrate the effectiveness of our method on both WSI classification and captioning task.","sentences":["In the field of computational histopathology, both whole slide images (WSIs) and diagnostic captions provide valuable insights for making diagnostic decisions.","However, aligning WSIs with diagnostic captions presents a significant challenge.","This difficulty arises from two main factors: 1) Gigapixel WSIs are unsuitable for direct input into deep learning models, and the redundancy and correlation among the patches demand more attention; and 2) Authentic WSI diagnostic captions are extremely limited, making it difficult to train an effective model.","To overcome these obstacles, we present PathM3, a multimodal, multi-task, multiple instance learning (MIL) framework for WSI classification and captioning.","PathM3 adapts a query-based transformer to effectively align WSIs with diagnostic captions.","Given that histopathology visual patterns are redundantly distributed across WSIs, we aggregate each patch feature with MIL method that considers the correlations among instances.","Furthermore, our PathM3 overcomes data scarcity in WSI-level captions by leveraging limited WSI diagnostic caption data in the manner of multi-task joint learning.","Extensive experiments with improved classification accuracy and caption generation demonstrate the effectiveness of our method on both WSI classification and captioning task."],"url":"http://arxiv.org/abs/2403.08967v1","category":"cs.CV"}
{"created":"2024-03-13 21:05:34","title":"Using Deep Learning for Morphological Classification in Pigs with a Focus on Sanitary Monitoring","abstract":"The aim of this paper is to evaluate the use of D-CNN (Deep Convolutional Neural Networks) algorithms to classify pig body conditions in normal or not normal conditions, with a focus on characteristics that are observed in sanitary monitoring, and were used six different algorithms to do this task. The study focused on five pig characteristics, being these caudophagy, ear hematoma, scratches on the body, redness, and natural stains (brown or black). The results of the study showed that D-CNN was effective in classifying deviations in pig body morphologies related to skin characteristics. The evaluation was conducted by analyzing the performance metrics Precision, Recall, and F-score, as well as the statistical analyses ANOVA and the Scott-Knott test. The contribution of this article is characterized by the proposal of using D-CNN networks for morphological classification in pigs, with a focus on characteristics identified in sanitary monitoring. Among the best results, the average Precision metric of 80.6\\% to classify caudophagy was achieved for the InceptionResNetV2 network, indicating the potential use of this technology for the proposed task. Additionally, a new image database was created, containing various pig's distinct body characteristics, which can serve as data for future research.","sentences":["The aim of this paper is to evaluate the use of D-CNN (Deep Convolutional Neural Networks) algorithms to classify pig body conditions in normal or not normal conditions, with a focus on characteristics that are observed in sanitary monitoring, and were used six different algorithms to do this task.","The study focused on five pig characteristics, being these caudophagy, ear hematoma, scratches on the body, redness, and natural stains (brown or black).","The results of the study showed that D-CNN was effective in classifying deviations in pig body morphologies related to skin characteristics.","The evaluation was conducted by analyzing the performance metrics Precision, Recall, and F-score, as well as the statistical analyses ANOVA and the Scott-Knott test.","The contribution of this article is characterized by the proposal of using D-CNN networks for morphological classification in pigs, with a focus on characteristics identified in sanitary monitoring.","Among the best results, the average Precision metric of 80.6\\% to classify caudophagy was achieved for the InceptionResNetV2 network, indicating the potential use of this technology for the proposed task.","Additionally, a new image database was created, containing various pig's distinct body characteristics, which can serve as data for future research."],"url":"http://arxiv.org/abs/2403.08962v1","category":"cs.CV"}
{"created":"2024-03-13 20:51:21","title":"AI coach for badminton","abstract":"In the competitive realm of sports, optimal performance necessitates rigorous management of nutrition and physical conditioning. Specifically, in badminton, the agility and precision required make it an ideal candidate for motion analysis through video analytics. This study leverages advanced neural network methodologies to dissect video footage of badminton matches, aiming to extract detailed insights into player kinetics and biomechanics. Through the analysis of stroke mechanics, including hand-hip coordination, leg positioning, and the execution angles of strokes, the research aims to derive predictive models that can suggest improvements in stance, technique, and muscle orientation. These recommendations are designed to mitigate erroneous techniques, reduce the risk of joint fatigue, and enhance overall performance. Utilizing a vast array of data available online, this research correlates players' physical attributes with their in-game movements to identify muscle activation patterns during play. The goal is to offer personalized training and nutrition strategies that align with the specific biomechanical demands of badminton, thereby facilitating targeted performance enhancements.","sentences":["In the competitive realm of sports, optimal performance necessitates rigorous management of nutrition and physical conditioning.","Specifically, in badminton, the agility and precision required make it an ideal candidate for motion analysis through video analytics.","This study leverages advanced neural network methodologies to dissect video footage of badminton matches, aiming to extract detailed insights into player kinetics and biomechanics.","Through the analysis of stroke mechanics, including hand-hip coordination, leg positioning, and the execution angles of strokes, the research aims to derive predictive models that can suggest improvements in stance, technique, and muscle orientation.","These recommendations are designed to mitigate erroneous techniques, reduce the risk of joint fatigue, and enhance overall performance.","Utilizing a vast array of data available online, this research correlates players' physical attributes with their in-game movements to identify muscle activation patterns during play.","The goal is to offer personalized training and nutrition strategies that align with the specific biomechanical demands of badminton, thereby facilitating targeted performance enhancements."],"url":"http://arxiv.org/abs/2403.08956v1","category":"cs.HC"}
{"created":"2024-03-13 20:50:49","title":"Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis","abstract":"Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness. Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored. In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function. We obtain an iteration complexity of $\\mathcal{O}(\\epsilon^{-2})$ to reach an $\\epsilon$-approximate first-order stationary point (FOSP). We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutral counterparts. Our theoretical analysis demonstrates that risk-sensitive REINFORCE can have a reduced number of iterations required for convergence. This leads to improved iteration complexity, as employing the exponential utility does not entail additional computation per iteration. We characterize the conditions under which risk-sensitive algorithms can achieve better iteration complexity. Our simulation results also validate that risk-averse cases can converge and stabilize more quickly after approximately half of the episodes compared to their risk-neutral counterparts.","sentences":["Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments.","However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness.","Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored.","In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function.","We obtain an iteration complexity of $\\mathcal{O}(\\epsilon^{-2})$ to reach an $\\epsilon$-approximate first-order stationary point (FOSP).","We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutral counterparts.","Our theoretical analysis demonstrates that risk-sensitive REINFORCE can have a reduced number of iterations required for convergence.","This leads to improved iteration complexity, as employing the exponential utility does not entail additional computation per iteration.","We characterize the conditions under which risk-sensitive algorithms can achieve better iteration complexity.","Our simulation results also validate that risk-averse cases can converge and stabilize more quickly after approximately half of the episodes compared to their risk-neutral counterparts."],"url":"http://arxiv.org/abs/2403.08955v1","category":"cs.LG"}
{"created":"2024-03-13 20:38:48","title":"Characterisation of analogue Monolithic Active Pixel Sensor test structures implemented in a 65 nm CMOS imaging process","abstract":"Analogue test structures were fabricated using the Tower Partners Semiconductor Co. CMOS 65 nm ISC process. The purpose was to characterise and qualify this process and to optimise the sensor for the next generation of Monolithic Active Pixels Sensors for high-energy physics. The technology was explored in several variants which differed by: doping levels, pixel geometries and pixel pitches (10-25 $\\mu$m). These variants have been tested following exposure to varying levels of irradiation up to 3 MGy and $10^{16}$ 1 MeV n$_\\text{eq}$ cm$^{-2}$. Here the results from prototypes that feature direct analogue output of a 4$\\times$4 pixel matrix are reported, allowing the systematic and detailed study of charge collection properties. Measurements were taken both using $^{55}$Fe X-ray sources and in beam tests using minimum ionizing particles. The results not only demonstrate the feasibility of using this technology for particle detection but also serve as a reference for future applications and optimisations.","sentences":["Analogue test structures were fabricated using the Tower Partners Semiconductor Co.","CMOS 65 nm ISC process.","The purpose was to characterise and qualify this process and to optimise the sensor for the next generation of Monolithic Active Pixels Sensors for high-energy physics.","The technology was explored in several variants which differed by: doping levels, pixel geometries and pixel pitches (10-25 $\\mu$m).","These variants have been tested following exposure to varying levels of irradiation up to 3 MGy and $10^{16}$ 1 MeV n$_\\text{eq}$","cm$^{-2}$. Here the results from prototypes that feature direct analogue output of a 4$\\times$4 pixel matrix are reported, allowing the systematic and detailed study of charge collection properties.","Measurements were taken both using $^{55}$Fe X-ray sources and in beam tests using minimum ionizing particles.","The results not only demonstrate the feasibility of using this technology for particle detection but also serve as a reference for future applications and optimisations."],"url":"http://arxiv.org/abs/2403.08952v1","category":"physics.ins-det"}
{"created":"2024-03-13 20:32:32","title":"Exploring Prompt Engineering Practices in the Enterprise","abstract":"Interaction with Large Language Models (LLMs) is primarily carried out via prompting. A prompt is a natural language instruction designed to elicit certain behaviour or output from a model. In theory, natural language prompts enable non-experts to interact with and leverage LLMs. However, for complex tasks and tasks with specific requirements, prompt design is not trivial. Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal. We hypothesize that the way in which users iterate on their prompts can provide insight into how they think prompting and models work, as well as the kinds of support needed for more efficient prompt engineering. To better understand prompt engineering practices, we analyzed sessions of prompt editing behavior, categorizing the parts of prompts users iterated on and the types of changes they made. We discuss design implications and future directions based on these prompt engineering practices.","sentences":["Interaction with Large Language Models (LLMs) is primarily carried out via prompting.","A prompt is a natural language instruction designed to elicit certain behaviour or output from a model.","In theory, natural language prompts enable non-experts to interact with and leverage LLMs.","However, for complex tasks and tasks with specific requirements, prompt design is not trivial.","Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal.","We hypothesize that the way in which users iterate on their prompts can provide insight into how they think prompting and models work, as well as the kinds of support needed for more efficient prompt engineering.","To better understand prompt engineering practices, we analyzed sessions of prompt editing behavior, categorizing the parts of prompts users iterated on and the types of changes they made.","We discuss design implications and future directions based on these prompt engineering practices."],"url":"http://arxiv.org/abs/2403.08950v1","category":"cs.HC"}
{"created":"2024-03-13 20:27:35","title":"Model-free Resilient Controller Design based on Incentive Feedback Stackelberg Game and Q-learning","abstract":"In the swift evolution of Cyber-Physical Systems (CPSs) within intelligent environments, especially in the industrial domain shaped by Industry 4.0, the surge in development brings forth unprecedented security challenges. This paper explores the intricate security issues of Industrial CPSs (ICPSs), with a specific focus on the unique threats presented by intelligent attackers capable of directly compromising the controller, thereby posing a direct risk to physical security. Within the framework of hierarchical control and incentive feedback Stackelberg game, we design a resilient leading controller (leader) that is adaptive to a compromised following controller (follower) such that the compromised follower acts cooperatively with the leader, aligning its strategies with the leader's objective to achieve a team-optimal solution. First, we provide sufficient conditions for the existence of an incentive Stackelberg solution when system dynamics are known. Then, we propose a Q-learning-based Approximate Dynamic Programming (ADP) approach, and corresponding algorithms for the online resolution of the incentive Stackelberg solution without requiring prior knowledge of system dynamics. Last but not least, we prove the convergence of our approach to the optimum.","sentences":["In the swift evolution of Cyber-Physical Systems (CPSs) within intelligent environments, especially in the industrial domain shaped by Industry 4.0, the surge in development brings forth unprecedented security challenges.","This paper explores the intricate security issues of Industrial CPSs (ICPSs), with a specific focus on the unique threats presented by intelligent attackers capable of directly compromising the controller, thereby posing a direct risk to physical security.","Within the framework of hierarchical control and incentive feedback Stackelberg game, we design a resilient leading controller (leader) that is adaptive to a compromised following controller (follower) such that the compromised follower acts cooperatively with the leader, aligning its strategies with the leader's objective to achieve a team-optimal solution.","First, we provide sufficient conditions for the existence of an incentive Stackelberg solution when system dynamics are known.","Then, we propose a Q-learning-based Approximate Dynamic Programming (ADP) approach, and corresponding algorithms for the online resolution of the incentive Stackelberg solution without requiring prior knowledge of system dynamics.","Last but not least, we prove the convergence of our approach to the optimum."],"url":"http://arxiv.org/abs/2403.08948v1","category":"eess.SY"}
{"created":"2024-03-13 20:21:20","title":"Language-based game theory in the age of artificial intelligence","abstract":"Understanding human behaviour in decision problems and strategic interactions has wide-ranging applications in economics, psychology, and artificial intelligence. Game theory offers a robust foundation for this understanding, based on the idea that individuals aim to maximize a utility function. However, the exact factors influencing strategy choices remain elusive. While traditional models try to explain human behaviour as a function of the outcomes of available actions, recent experimental research reveals that linguistic content significantly impacts decision-making, thus prompting a paradigm shift from outcome-based to language-based utility functions. This shift is more urgent than ever, given the advancement of generative AI, which has the potential to support humans in making critical decisions through language-based interactions. We propose sentiment analysis as a fundamental tool for this shift and take an initial step by analyzing 61 experimental instructions from the dictator game, an economic game capturing the balance between self-interest and the interest of others, which is at the core of many social interactions. Our meta-analysis shows that sentiment analysis can explain human behaviour beyond economic outcomes. We discuss future research directions. We hope this work sets the stage for a novel game theoretical approach that emphasizes the importance of language in human decisions.","sentences":["Understanding human behaviour in decision problems and strategic interactions has wide-ranging applications in economics, psychology, and artificial intelligence.","Game theory offers a robust foundation for this understanding, based on the idea that individuals aim to maximize a utility function.","However, the exact factors influencing strategy choices remain elusive.","While traditional models try to explain human behaviour as a function of the outcomes of available actions, recent experimental research reveals that linguistic content significantly impacts decision-making, thus prompting a paradigm shift from outcome-based to language-based utility functions.","This shift is more urgent than ever, given the advancement of generative AI, which has the potential to support humans in making critical decisions through language-based interactions.","We propose sentiment analysis as a fundamental tool for this shift and take an initial step by analyzing 61 experimental instructions from the dictator game, an economic game capturing the balance between self-interest and the interest of others, which is at the core of many social interactions.","Our meta-analysis shows that sentiment analysis can explain human behaviour beyond economic outcomes.","We discuss future research directions.","We hope this work sets the stage for a novel game theoretical approach that emphasizes the importance of language in human decisions."],"url":"http://arxiv.org/abs/2403.08944v1","category":"cs.GT"}
{"created":"2024-03-13 20:16:16","title":"A Virtual Environment for Collaborative Inspection in Additive Manufacturing","abstract":"Additive manufacturing (AM) techniques have been used to enhance the design and fabrication of complex components for various applications in the medical, aerospace, energy, and consumer products industries. A defining feature for many AM parts is the complex internal geometry enabled by the printing process. However, inspecting these internal structures requires volumetric imaging, i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D geometries using 2D desktop interfaces. Furthermore, existing tools are limited to single-user systems making it difficult to jointly discuss or share findings with a larger team, i.e., the designers, manufacturing experts, and evaluation team. In this work, we present a collaborative virtual reality (VR) for the exploration and inspection of AM parts. Geographically separated experts can virtually inspect and jointly discuss data. It also supports VR and non-VR users, who can be spectators in the VR environment. Various features for data exploration and inspection are developed and enhanced via real-time synchronization. We followed usability and interface verification guidelines using Nielsen's heuristics approach. Furthermore, we conducted exploratory and semi-structured interviews with domain experts to collect qualitative feedback. Results reveal potential benefits, applicability, and current limitations. The proposed collaborative VR environment provides a new basis and opens new research directions for virtual inspection and team collaboration in AM settings.","sentences":["Additive manufacturing (AM) techniques have been used to enhance the design and fabrication of complex components for various applications in the medical, aerospace, energy, and consumer products industries.","A defining feature for many AM parts is the complex internal geometry enabled by the printing process.","However, inspecting these internal structures requires volumetric imaging, i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D geometries using 2D desktop interfaces.","Furthermore, existing tools are limited to single-user systems making it difficult to jointly discuss or share findings with a larger team, i.e., the designers, manufacturing experts, and evaluation team.","In this work, we present a collaborative virtual reality (VR) for the exploration and inspection of AM parts.","Geographically separated experts can virtually inspect and jointly discuss data.","It also supports VR and non-VR users, who can be spectators in the VR environment.","Various features for data exploration and inspection are developed and enhanced via real-time synchronization.","We followed usability and interface verification guidelines using Nielsen's heuristics approach.","Furthermore, we conducted exploratory and semi-structured interviews with domain experts to collect qualitative feedback.","Results reveal potential benefits, applicability, and current limitations.","The proposed collaborative VR environment provides a new basis and opens new research directions for virtual inspection and team collaboration in AM settings."],"url":"http://arxiv.org/abs/2403.08940v1","category":"cs.HC"}
{"created":"2024-03-13 20:12:01","title":"Bugs in Large Language Models Generated Code","abstract":"Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration. The bug patterns are presented in the form of a taxonomy. The identified bug patterns are validated using an online survey with 34 LLM practitioners and researchers. The surveyed participants generally asserted the significance and prevalence of the bug patterns. Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code. This study sheds light on the distinctive characteristics of LLM-generated code.","sentences":["Large Language Models (LLMs) for code have gained significant attention recently.","They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation.","Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community.","Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs.","This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration.","The bug patterns are presented in the form of a taxonomy.","The identified bug patterns are validated using an online survey with 34 LLM practitioners and researchers.","The surveyed participants generally asserted the significance and prevalence of the bug patterns.","Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code.","This study sheds light on the distinctive characteristics of LLM-generated code."],"url":"http://arxiv.org/abs/2403.08937v1","category":"cs.SE"}
{"created":"2024-03-13 20:11:20","title":"Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning","abstract":"Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of efficient exploration due to the exponential increase in the size of the joint state-action space. While demonstration-guided learning has proven beneficial in single-agent settings, its direct applicability to MARL is hindered by the practical difficulty of obtaining joint expert demonstrations. In this work, we introduce a novel concept of personalized expert demonstrations, tailored for each individual agent or, more broadly, each individual type of agent within a heterogeneous team. These demonstrations solely pertain to single-agent behaviors and how each agent can achieve personal goals without encompassing any cooperative elements, thus naively imitating them will not achieve cooperation due to potential conflicts. To this end, we propose an approach that selectively utilizes personalized expert demonstrations as guidance and allows agents to learn to cooperate, namely personalized expert-guided MARL (PegMARL). This algorithm utilizes two discriminators: the first provides incentives based on the alignment of policy behavior with demonstrations, and the second regulates incentives based on whether the behavior leads to the desired objective. We evaluate PegMARL using personalized demonstrations in both discrete and continuous environments. The results demonstrate that PegMARL learns near-optimal policies even when provided with suboptimal demonstrations, and outperforms state-of-the-art MARL algorithms in solving coordinated tasks. We also showcase PegMARL's capability to leverage joint demonstrations in the StarCraft scenario and converge effectively even with demonstrations from non-co-trained policies.","sentences":["Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of efficient exploration due to the exponential increase in the size of the joint state-action space.","While demonstration-guided learning has proven beneficial in single-agent settings, its direct applicability to MARL is hindered by the practical difficulty of obtaining joint expert demonstrations.","In this work, we introduce a novel concept of personalized expert demonstrations, tailored for each individual agent or, more broadly, each individual type of agent within a heterogeneous team.","These demonstrations solely pertain to single-agent behaviors and how each agent can achieve personal goals without encompassing any cooperative elements, thus naively imitating them will not achieve cooperation due to potential conflicts.","To this end, we propose an approach that selectively utilizes personalized expert demonstrations as guidance and allows agents to learn to cooperate, namely personalized expert-guided MARL (PegMARL).","This algorithm utilizes two discriminators: the first provides incentives based on the alignment of policy behavior with demonstrations, and the second regulates incentives based on whether the behavior leads to the desired objective.","We evaluate PegMARL using personalized demonstrations in both discrete and continuous environments.","The results demonstrate that PegMARL learns near-optimal policies even when provided with suboptimal demonstrations, and outperforms state-of-the-art MARL algorithms in solving coordinated tasks.","We also showcase PegMARL's capability to leverage joint demonstrations in the StarCraft scenario and converge effectively even with demonstrations from non-co-trained policies."],"url":"http://arxiv.org/abs/2403.08936v1","category":"cs.MA"}
{"created":"2024-03-13 19:56:30","title":"Unveiling the Truth: Exploring Human Gaze Patterns in Fake Images","abstract":"Creating high-quality and realistic images is now possible thanks to the impressive advancements in image generation. A description in natural language of your desired output is all you need to obtain breathtaking results. However, as the use of generative models grows, so do concerns about the propagation of malicious content and misinformation. Consequently, the research community is actively working on the development of novel fake detection techniques, primarily focusing on low-level features and possible fingerprints left by generative models during the image generation process. In a different vein, in our work, we leverage human semantic knowledge to investigate the possibility of being included in frameworks of fake image detection. To achieve this, we collect a novel dataset of partially manipulated images using diffusion models and conduct an eye-tracking experiment to record the eye movements of different observers while viewing real and fake stimuli. A preliminary statistical analysis is conducted to explore the distinctive patterns in how humans perceive genuine and altered images. Statistical findings reveal that, when perceiving counterfeit samples, humans tend to focus on more confined regions of the image, in contrast to the more dispersed observational pattern observed when viewing genuine images. Our dataset is publicly available at: https://github.com/aimagelab/unveiling-the-truth.","sentences":["Creating high-quality and realistic images is now possible thanks to the impressive advancements in image generation.","A description in natural language of your desired output is all you need to obtain breathtaking results.","However, as the use of generative models grows, so do concerns about the propagation of malicious content and misinformation.","Consequently, the research community is actively working on the development of novel fake detection techniques, primarily focusing on low-level features and possible fingerprints left by generative models during the image generation process.","In a different vein, in our work, we leverage human semantic knowledge to investigate the possibility of being included in frameworks of fake image detection.","To achieve this, we collect a novel dataset of partially manipulated images using diffusion models and conduct an eye-tracking experiment to record the eye movements of different observers while viewing real and fake stimuli.","A preliminary statistical analysis is conducted to explore the distinctive patterns in how humans perceive genuine and altered images.","Statistical findings reveal that, when perceiving counterfeit samples, humans tend to focus on more confined regions of the image, in contrast to the more dispersed observational pattern observed when viewing genuine images.","Our dataset is publicly available at: https://github.com/aimagelab/unveiling-the-truth."],"url":"http://arxiv.org/abs/2403.08933v1","category":"cs.CV"}
{"created":"2024-03-13 19:43:26","title":"How Much Can Reconfigurable Intelligent Surfaces Augment Sky Visibility: A Stochastic Geometry Approach","abstract":"This paper uses the theory of point processes and stochastic geometry to quantify the sky visibility experienced by users located in an urban environment. The general idea is to represent the buildings of this environment as a stationary marked point process, where the points represent the building locations and the marks their heights. The point process framework is first used to characterize the distribution of the blockage angle, which limits the visibility of a typical user into the sky due to the obstruction by buildings. In the context of communications, this distribution is useful when users try to connect to the nodes of an aerial or non-terrestrial network in a Line-of-Sight way. Within this context, the point process framework can also be used to investigate the gain of connectivity obtained thanks to Reconfigurable Intelligent Surfaces. Assuming that such surfaces are installed on the top of buildings to extend the user's sky visibility, this point process approach allows one to quantify the gain in visibility and hence the gain in connectivity obtained by the typical user. The distributional properties of visibility-related metrics are cross-validated by comparison to simulation results.","sentences":["This paper uses the theory of point processes and stochastic geometry to quantify the sky visibility experienced by users located in an urban environment.","The general idea is to represent the buildings of this environment as a stationary marked point process, where the points represent the building locations and the marks their heights.","The point process framework is first used to characterize the distribution of the blockage angle, which limits the visibility of a typical user into the sky due to the obstruction by buildings.","In the context of communications, this distribution is useful when users try to connect to the nodes of an aerial or non-terrestrial network in a Line-of-Sight way.","Within this context, the point process framework can also be used to investigate the gain of connectivity obtained thanks to Reconfigurable Intelligent Surfaces.","Assuming that such surfaces are installed on the top of buildings to extend the user's sky visibility, this point process approach allows one to quantify the gain in visibility and hence the gain in connectivity obtained by the typical user.","The distributional properties of visibility-related metrics are cross-validated by comparison to simulation results."],"url":"http://arxiv.org/abs/2403.08930v1","category":"math.PR"}
{"created":"2024-03-13 19:36:03","title":"Neuromorphic force-control in an industrial task: validating energy and latency benefits","abstract":"As robots become smarter and more ubiquitous, optimizing the power consumption of intelligent compute becomes imperative towards ensuring the sustainability of technological advancements. Neuromorphic computing hardware makes use of biologically inspired neural architectures to achieve energy and latency improvements compared to conventional von Neumann computing architecture. Applying these benefits to robots has been demonstrated in several works in the field of neurorobotics, typically on relatively simple control tasks. Here, we introduce an example of neuromorphic computing applied to the real-world industrial task of object insertion. We trained a spiking neural network (SNN) to perform force-torque feedback control using a reinforcement learning approach in simulation. We then ported the SNN to the Intel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At inference time we show latency competitive with current CPU/GPU architectures, two orders of magnitude less energy usage in comparison to traditional low-energy edge-hardware. We offer this example as a proof of concept implementation of a neuromoprhic controller in real-world robotic setting, highlighting the benefits of neuromorphic hardware for the development of intelligent controllers for robots.","sentences":["As robots become smarter and more ubiquitous, optimizing the power consumption of intelligent compute becomes imperative towards ensuring the sustainability of technological advancements.","Neuromorphic computing hardware makes use of biologically inspired neural architectures to achieve energy and latency improvements compared to conventional von Neumann computing architecture.","Applying these benefits to robots has been demonstrated in several works in the field of neurorobotics, typically on relatively simple control tasks.","Here, we introduce an example of neuromorphic computing applied to the real-world industrial task of object insertion.","We trained a spiking neural network (SNN) to perform force-torque feedback control using a reinforcement learning approach in simulation.","We then ported the SNN to the Intel neuromorphic research chip Loihi interfaced with a KUKA robotic arm.","At inference time we show latency competitive with current CPU/GPU architectures, two orders of magnitude less energy usage in comparison to traditional low-energy edge-hardware.","We offer this example as a proof of concept implementation of a neuromoprhic controller in real-world robotic setting, highlighting the benefits of neuromorphic hardware for the development of intelligent controllers for robots."],"url":"http://arxiv.org/abs/2403.08928v1","category":"cs.RO"}
{"created":"2024-03-13 19:32:17","title":"Electrochemical Communication in Bacterial Biofilms: A Study on Potassium Stimulation and Signal Transmission","abstract":"Electrochemical communication is a mechanism that enables intercellular interaction among bacteria within communities. Bacteria achieves synchronization and coordinates collective actions at the population level through the utilization of electrochemical signals. In this work, we investigate the response of bacterial biofilms to artificial potassium concentration stimulation. We introduce signal inputs at a specific location within the biofilm and observe their transmission to other regions, facilitated by intermediary cells that amplify and relay the signal. We analyze the output signals when biofilm regions are subjected to different input signal types and explore their impact on biofilm growth. Furthermore, we investigate how the temporal gap between input pulses influences output signal characteristics, demonstrating that an appropriate gap yields distinct and well-defined output signals. Our research sheds light on the potential of bacterial biofilms as communication nodes in electrochemical communication networks.","sentences":["Electrochemical communication is a mechanism that enables intercellular interaction among bacteria within communities.","Bacteria achieves synchronization and coordinates collective actions at the population level through the utilization of electrochemical signals.","In this work, we investigate the response of bacterial biofilms to artificial potassium concentration stimulation.","We introduce signal inputs at a specific location within the biofilm and observe their transmission to other regions, facilitated by intermediary cells that amplify and relay the signal.","We analyze the output signals when biofilm regions are subjected to different input signal types and explore their impact on biofilm growth.","Furthermore, we investigate how the temporal gap between input pulses influences output signal characteristics, demonstrating that an appropriate gap yields distinct and well-defined output signals.","Our research sheds light on the potential of bacterial biofilms as communication nodes in electrochemical communication networks."],"url":"http://arxiv.org/abs/2403.08926v1","category":"cs.IT"}
{"created":"2024-03-13 19:11:58","title":"Cross-Modal Learning of Housing Quality in Amsterdam","abstract":"In our research we test data and models for the recognition of housing quality in the city of Amsterdam from ground-level and aerial imagery. For ground-level images we compare Google StreetView (GSV) to Flickr images. Our results show that GSV predicts the most accurate building quality scores, approximately 30% better than using only aerial images. However, we find that through careful filtering and by using the right pre-trained model, Flickr image features combined with aerial image features are able to halve the performance gap to GSV features from 30% to 15%. Our results indicate that there are viable alternatives to GSV for liveability factor prediction, which is encouraging as GSV images are more difficult to acquire and not always available.","sentences":["In our research we test data and models for the recognition of housing quality in the city of Amsterdam from ground-level and aerial imagery.","For ground-level images we compare Google StreetView (GSV) to Flickr images.","Our results show that GSV predicts the most accurate building quality scores, approximately 30% better than using only aerial images.","However, we find that through careful filtering and by using the right pre-trained model, Flickr image features combined with aerial image features are able to halve the performance gap to GSV features from 30% to 15%.","Our results indicate that there are viable alternatives to GSV for liveability factor prediction, which is encouraging as GSV images are more difficult to acquire and not always available."],"url":"http://arxiv.org/abs/2403.08915v1","category":"cs.CV"}
{"created":"2024-03-13 19:00:36","title":"Meta-operators for Enabling Parallel Planning Using Deep Reinforcement Learning","abstract":"There is a growing interest in the application of Reinforcement Learning (RL) techniques to AI planning with the aim to come up with general policies. Typically, the mapping of the transition model of AI planning to the state transition system of a Markov Decision Process is established by assuming a one-to-one correspondence of the respective action spaces. In this paper, we introduce the concept of meta-operator as the result of simultaneously applying multiple planning operators, and we show that including meta-operators in the RL action space enables new planning perspectives to be addressed using RL, such as parallel planning. Our research aims to analyze the performance and complexity of including meta-operators in the RL process, concretely in domains where satisfactory outcomes have not been previously achieved using usual generalized planning models. The main objective of this article is thus to pave the way towards a redefinition of the RL action space in a manner that is more closely aligned with the planning perspective.","sentences":["There is a growing interest in the application of Reinforcement Learning (RL) techniques to AI planning with the aim to come up with general policies.","Typically, the mapping of the transition model of AI planning to the state transition system of a Markov Decision Process is established by assuming a one-to-one correspondence of the respective action spaces.","In this paper, we introduce the concept of meta-operator as the result of simultaneously applying multiple planning operators, and we show that including meta-operators in the RL action space enables new planning perspectives to be addressed using RL, such as parallel planning.","Our research aims to analyze the performance and complexity of including meta-operators in the RL process, concretely in domains where satisfactory outcomes have not been previously achieved using usual generalized planning models.","The main objective of this article is thus to pave the way towards a redefinition of the RL action space in a manner that is more closely aligned with the planning perspective."],"url":"http://arxiv.org/abs/2403.08910v1","category":"cs.AI"}
{"created":"2024-03-13 18:54:27","title":"Strategizing against Q-learners: A Control-theoretical Approach","abstract":"In this paper, we explore the susceptibility of the Q-learning algorithm (a classical and widely used reinforcement learning method) to strategic manipulation of sophisticated opponents in games. We quantify how much a strategically sophisticated agent can exploit a naive Q-learner if she knows the opponent's Q-learning algorithm. To this end, we formulate the strategic actor's problem as a Markov decision process (with a continuum state space encompassing all possible Q-values) as if the Q-learning algorithm is the underlying dynamical system. We also present a quantization-based approximation scheme to tackle the continuum state space and analyze its performance both analytically and numerically.","sentences":["In this paper, we explore the susceptibility of the Q-learning algorithm (a classical and widely used reinforcement learning method) to strategic manipulation of sophisticated opponents in games.","We quantify how much a strategically sophisticated agent can exploit a naive Q-learner if she knows the opponent's Q-learning algorithm.","To this end, we formulate the strategic actor's problem as a Markov decision process (with a continuum state space encompassing all possible Q-values) as if the Q-learning algorithm is the underlying dynamical system.","We also present a quantization-based approximation scheme to tackle the continuum state space and analyze its performance both analytically and numerically."],"url":"http://arxiv.org/abs/2403.08906v1","category":"cs.GT"}
{"created":"2024-03-13 18:20:40","title":"Moving Towards Automated Interstellar Boundary Explorer Data Selection with LOTUS","abstract":"The Interstellar Boundary Explorer (IBEX) satellite collects data on energetic neutral atoms (ENAs) that provide insight into the heliosphere, the region surrounding our solar system and separating it from interstellar space. IBEX collects information on these particles and on extraneous ``background'' particles. While IBEX records how and when the different particles are observed, it does not distinguish between heliospheric ENA particles and incidental background particles. To address this issue, all IBEX data has historically been manually labeled as ``good'' ENA data, or ``bad'' background data. This manual culling process is incredibly time-intensive and contingent on subjective, manually-induced decision thresholds. In this paper, we develop a three-stage automated culling process, called LOTUS, that uses random forests to expedite and standardize the labelling process. In Stage 1, LOTUS uses random forests to obtain probabilities of observing true ENA particles on a per-observation basis. In Stage 2, LOTUS aggregates these probabilities to obtain predictions within small windows of time. In Stage 3, LOTUS refines these predictions. We compare the labels generated by LOTUS to those manually generated by the subject matter expert. We use various metrics to demonstrate that LOTUS is a useful automated process for supplementing and standardizing the manual culling process.","sentences":["The Interstellar Boundary Explorer (IBEX) satellite collects data on energetic neutral atoms (ENAs) that provide insight into the heliosphere, the region surrounding our solar system and separating it from interstellar space.","IBEX collects information on these particles and on extraneous ``background'' particles.","While IBEX records how and when the different particles are observed, it does not distinguish between heliospheric ENA particles and incidental background particles.","To address this issue, all IBEX data has historically been manually labeled as ``good'' ENA data, or ``bad'' background data.","This manual culling process is incredibly time-intensive and contingent on subjective, manually-induced decision thresholds.","In this paper, we develop a three-stage automated culling process, called LOTUS, that uses random forests to expedite and standardize the labelling process.","In Stage 1, LOTUS uses random forests to obtain probabilities of observing true ENA particles on a per-observation basis.","In Stage 2, LOTUS aggregates these probabilities to obtain predictions within small windows of time.","In Stage 3, LOTUS refines these predictions.","We compare the labels generated by LOTUS to those manually generated by the subject matter expert.","We use various metrics to demonstrate that LOTUS is a useful automated process for supplementing and standardizing the manual culling process."],"url":"http://arxiv.org/abs/2403.08891v1","category":"stat.AP"}
{"created":"2024-03-13 18:16:54","title":"Federated Data Model","abstract":"In artificial intelligence (AI), especially deep learning, data diversity and volume play a pivotal role in model development. However, training a robust deep learning model often faces challenges due to data privacy, regulations, and the difficulty of sharing data between different locations, especially for medical applications. To address this, we developed a method called the Federated Data Model (FDM). This method uses diffusion models to learn the characteristics of data at one site and then creates synthetic data that can be used at another site without sharing the actual data. We tested this approach with a medical image segmentation task, focusing on cardiac magnetic resonance images from different hospitals. Our results show that models trained with this method perform well both on the data they were originally trained on and on data from other sites. This approach offers a promising way to train accurate and privacy-respecting AI models across different locations.","sentences":["In artificial intelligence (AI), especially deep learning, data diversity and volume play a pivotal role in model development.","However, training a robust deep learning model often faces challenges due to data privacy, regulations, and the difficulty of sharing data between different locations, especially for medical applications.","To address this, we developed a method called the Federated Data Model (FDM).","This method uses diffusion models to learn the characteristics of data at one site and then creates synthetic data that can be used at another site without sharing the actual data.","We tested this approach with a medical image segmentation task, focusing on cardiac magnetic resonance images from different hospitals.","Our results show that models trained with this method perform well both on the data they were originally trained on and on data from other sites.","This approach offers a promising way to train accurate and privacy-respecting AI models across different locations."],"url":"http://arxiv.org/abs/2403.08887v1","category":"cs.CV"}
{"created":"2024-03-13 18:12:53","title":"SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion using a 3D Recurrent U-Net","abstract":"We introduce SLCF-Net, a novel approach for the Semantic Scene Completion (SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates missing geometry and semantics in a scene from sequences of RGB images and sparse LiDAR measurements. The images are semantically segmented by a pre-trained 2D U-Net and a dense depth prior is estimated from a depth-conditioned pipeline fueled by Depth Anything. To associate the 2D image features with the 3D scene volume, we introduce Gaussian-decay Depth-prior Projection (GDP). This module projects the 2D features into the 3D volume along the line of sight with a Gaussian-decay function, centered around the depth prior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden 3D U-Net state using the sensor motion and design a novel loss to ensure temporal consistency. We evaluate our approach on the SemanticKITTI dataset and compare it with leading SSC approaches. The SLCF-Net excels in all SSC metrics and shows great temporal consistency.","sentences":["We introduce SLCF-Net, a novel approach for the Semantic Scene Completion (SSC) task that sequentially fuses LiDAR and camera data.","It jointly estimates missing geometry and semantics in a scene from sequences of RGB images and sparse LiDAR measurements.","The images are semantically segmented by a pre-trained 2D U-Net and a dense depth prior is estimated from a depth-conditioned pipeline fueled by Depth Anything.","To associate the 2D image features with the 3D scene volume, we introduce Gaussian-decay Depth-prior Projection (GDP).","This module projects the 2D features into the 3D volume along the line of sight with a Gaussian-decay function, centered around the depth prior.","Volumetric semantics is computed by a 3D U-Net.","We propagate the hidden 3D U-Net state using the sensor motion and design a novel loss to ensure temporal consistency.","We evaluate our approach on the SemanticKITTI dataset and compare it with leading SSC approaches.","The SLCF-Net excels in all SSC metrics and shows great temporal consistency."],"url":"http://arxiv.org/abs/2403.08885v1","category":"cs.CV"}
{"created":"2024-03-13 18:11:17","title":"Cultural evolution in populations of Large Language Models","abstract":"Research in cultural evolution aims at providing causal explanations for the change of culture over time. Over the past decades, this field has generated an important body of knowledge, using experimental, historical, and computational methods. While computational models have been very successful at generating testable hypotheses about the effects of several factors, such as population structure or transmission biases, some phenomena have so far been more complex to capture using agent-based and formal models. This is in particular the case for the effect of the transformations of social information induced by evolved cognitive mechanisms. We here propose that leveraging the capacity of Large Language Models (LLMs) to mimic human behavior may be fruitful to address this gap. On top of being an useful approximation of human cultural dynamics, multi-agents models featuring generative agents are also important to study for their own sake. Indeed, as artificial agents are bound to participate more and more to the evolution of culture, it is crucial to better understand the dynamics of machine-generated cultural evolution. We here present a framework for simulating cultural evolution in populations of LLMs, allowing the manipulation of variables known to be important in cultural evolution, such as network structure, personality, and the way social information is aggregated and transformed. The software we developed for conducting these simulations is open-source and features an intuitive user-interface, which we hope will help to build bridges between the fields of cultural evolution and generative artificial intelligence.","sentences":["Research in cultural evolution aims at providing causal explanations for the change of culture over time.","Over the past decades, this field has generated an important body of knowledge, using experimental, historical, and computational methods.","While computational models have been very successful at generating testable hypotheses about the effects of several factors, such as population structure or transmission biases, some phenomena have so far been more complex to capture using agent-based and formal models.","This is in particular the case for the effect of the transformations of social information induced by evolved cognitive mechanisms.","We here propose that leveraging the capacity of Large Language Models (LLMs) to mimic human behavior may be fruitful to address this gap.","On top of being an useful approximation of human cultural dynamics, multi-agents models featuring generative agents are also important to study for their own sake.","Indeed, as artificial agents are bound to participate more and more to the evolution of culture, it is crucial to better understand the dynamics of machine-generated cultural evolution.","We here present a framework for simulating cultural evolution in populations of LLMs, allowing the manipulation of variables known to be important in cultural evolution, such as network structure, personality, and the way social information is aggregated and transformed.","The software we developed for conducting these simulations is open-source and features an intuitive user-interface, which we hope will help to build bridges between the fields of cultural evolution and generative artificial intelligence."],"url":"http://arxiv.org/abs/2403.08882v1","category":"cs.MA"}
{"created":"2024-03-13 18:06:43","title":"REFRESH: Responsible and Efficient Feature Reselection Guided by SHAP Values","abstract":"Feature selection is a crucial step in building machine learning models. This process is often achieved with accuracy as an objective, and can be cumbersome and computationally expensive for large-scale datasets. Several additional model performance characteristics such as fairness and robustness are of importance for model development. As regulations are driving the need for more trustworthy models, deployed models need to be corrected for model characteristics associated with responsible artificial intelligence. When feature selection is done with respect to one model performance characteristic (eg. accuracy), feature selection with secondary model performance characteristics (eg. fairness and robustness) as objectives would require going through the computationally expensive selection process from scratch. In this paper, we introduce the problem of feature \\emph{reselection}, so that features can be selected with respect to secondary model performance characteristics efficiently even after a feature selection process has been done with respect to a primary objective. To address this problem, we propose REFRESH, a method to reselect features so that additional constraints that are desirable towards model performance can be achieved without having to train several new models. REFRESH's underlying algorithm is a novel technique using SHAP values and correlation analysis that can approximate for the predictions of a model without having to train these models. Empirical evaluations on three datasets, including a large-scale loan defaulting dataset show that REFRESH can help find alternate models with better model characteristics efficiently. We also discuss the need for reselection and REFRESH based on regulation desiderata.","sentences":["Feature selection is a crucial step in building machine learning models.","This process is often achieved with accuracy as an objective, and can be cumbersome and computationally expensive for large-scale datasets.","Several additional model performance characteristics such as fairness and robustness are of importance for model development.","As regulations are driving the need for more trustworthy models, deployed models need to be corrected for model characteristics associated with responsible artificial intelligence.","When feature selection is done with respect to one model performance characteristic (eg. accuracy), feature selection with secondary model performance characteristics (eg. fairness and robustness) as objectives would require going through the computationally expensive selection process from scratch.","In this paper, we introduce the problem of feature \\emph{reselection}, so that features can be selected with respect to secondary model performance characteristics efficiently even after a feature selection process has been done with respect to a primary objective.","To address this problem, we propose REFRESH, a method to reselect features so that additional constraints that are desirable towards model performance can be achieved without having to train several new models.","REFRESH's underlying algorithm is a novel technique using SHAP values and correlation analysis that can approximate for the predictions of a model without having to train these models.","Empirical evaluations on three datasets, including a large-scale loan defaulting dataset show that REFRESH can help find alternate models with better model characteristics efficiently.","We also discuss the need for reselection and REFRESH based on regulation desiderata."],"url":"http://arxiv.org/abs/2403.08880v1","category":"cs.LG"}
{"created":"2024-03-13 18:05:16","title":"Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning","abstract":"The Intelligent Transportation System (ITS) environment is known to be dynamic and distributed, where participants (vehicle users, operators, etc.) have multiple, changing and possibly conflicting objectives. Although Reinforcement Learning (RL) algorithms are commonly applied to optimize ITS applications such as resource management and offloading, most RL algorithms focus on single objectives. In many situations, converting a multi-objective problem into a single-objective one is impossible, intractable or insufficient, making such RL algorithms inapplicable. We propose a multi-objective, multi-agent reinforcement learning (MARL) algorithm with high learning efficiency and low computational requirements, which automatically triggers adaptive few-shot learning in a dynamic, distributed and noisy environment with sparse and delayed reward. We test our algorithm in an ITS environment with edge cloud computing. Empirical results show that the algorithm is quick to adapt to new environments and performs better in all individual and system metrics compared to the state-of-the-art benchmark. Our algorithm also addresses various practical concerns with its modularized and asynchronous online training method. In addition to the cloud simulation, we test our algorithm on a single-board computer and show that it can make inference in 6 milliseconds.","sentences":["The Intelligent Transportation System (ITS) environment is known to be dynamic and distributed, where participants (vehicle users, operators, etc.) have multiple, changing and possibly conflicting objectives.","Although Reinforcement Learning (RL) algorithms are commonly applied to optimize ITS applications such as resource management and offloading, most RL algorithms focus on single objectives.","In many situations, converting a multi-objective problem into a single-objective one is impossible, intractable or insufficient, making such RL algorithms inapplicable.","We propose a multi-objective, multi-agent reinforcement learning (MARL) algorithm with high learning efficiency and low computational requirements, which automatically triggers adaptive few-shot learning in a dynamic, distributed and noisy environment with sparse and delayed reward.","We test our algorithm in an ITS environment with edge cloud computing.","Empirical results show that the algorithm is quick to adapt to new environments and performs better in all individual and system metrics compared to the state-of-the-art benchmark.","Our algorithm also addresses various practical concerns with its modularized and asynchronous online training method.","In addition to the cloud simulation, we test our algorithm on a single-board computer and show that it can make inference in 6 milliseconds."],"url":"http://arxiv.org/abs/2403.08879v1","category":"cs.LG"}
{"created":"2024-03-13 18:00:03","title":"HST astrometry of the closest Brown Dwarfs -- II. Improved parameters and constraints on a third body","abstract":"Located at less than 2pc away, Luhman16AB (WISE.J104915.57-531906.1) is the closest pair of brown dwarfs and third closest `stellar' system to Earth. An exoplanet candidate in the Luhman16 binary system was reported in 2017 based on a weak astrometric signature in the analysis of 12 HST epochs. An additional epoch collected in 2018 and re-analysis of the data with more advanced methods further increased the significance level of the candidate, consistent with a Neptune-mass exoplanet orbiting one of the Luhman16 brown dwarf components. We report the joint analysis of these previous data together with two new astrometric HST epochs we obtained to confirm or disprove this astrometric signature. Our new analysis rules out presence of a planet orbiting one component of the Luhman16AB system for masses M > 1.5 M_Nep (Neptune masses) and periods between 400 and 5000 days. However, the presence of third bodies with masses M < 3 M_Nep and periods between 2 and 400 days (~1.1yrs) can not be excluded. Our measurements make significant improvements to the characterization of this sub-stellar binary, including its mass-ratio 0.8305+/-0.0006, individual component masses 35.4+/-0.2 M_Jup and 29.4+/-0.2 M_Jup (Jupiter masses), and parallax distance 1.9960pc +/- 50AU. Comparison of the masses and luminosities of Luhman16AB to several evolutionary models shows persistent discrepancies in the ages of the two components, but strengthens the case that this system is a member of the 510+/-95 Myr Oceanus Moving Group.","sentences":["Located at less than 2pc away, Luhman16AB (WISE.J104915.57-531906.1) is the closest pair of brown dwarfs and third closest `stellar' system to Earth.","An exoplanet candidate in the Luhman16 binary system was reported in 2017 based on a weak astrometric signature in the analysis of 12 HST epochs.","An additional epoch collected in 2018 and re-analysis of the data with more advanced methods further increased the significance level of the candidate, consistent with a Neptune-mass exoplanet orbiting one of the Luhman16 brown dwarf components.","We report the joint analysis of these previous data together with two new astrometric HST epochs we obtained to confirm or disprove this astrometric signature.","Our new analysis rules out presence of a planet orbiting one component of the Luhman16AB system for masses M > 1.5 M_Nep (Neptune masses) and periods between 400 and 5000 days.","However, the presence of third bodies with masses M < 3 M_Nep and periods between 2 and 400 days (~1.1yrs) can not be excluded.","Our measurements make significant improvements to the characterization of this sub-stellar binary, including its mass-ratio 0.8305+/-0.0006, individual component masses 35.4+/-0.2 M_Jup and 29.4+/-0.2 M_Jup (Jupiter masses), and parallax distance 1.9960pc +/-","50AU.","Comparison of the masses and luminosities of Luhman16AB to several evolutionary models shows persistent discrepancies in the ages of the two components, but strengthens the case that this system is a member of the 510+/-95 Myr Oceanus Moving Group."],"url":"http://arxiv.org/abs/2403.08865v1","category":"astro-ph.EP"}
{"created":"2024-03-13 16:30:57","title":"Bifurcated Attention for Single-Context Large-Batch Sampling","abstract":"In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts. This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths. Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process. This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO. Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length. The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation without substantially increasing latency, enhancing performance when integrated with postprocessing techniques such as reranking.","sentences":["In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts.","This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths.","Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process.","This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO.","Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length.","The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation without substantially increasing latency, enhancing performance when integrated with postprocessing techniques such as reranking."],"url":"http://arxiv.org/abs/2403.08845v1","category":"cs.LG"}
{"created":"2024-03-14 17:58:09","title":"From the Conformal Anomaly to the Virasoro Algebra","abstract":"The conformal anomaly and the Virasoro algebra are fundamental aspects of 2D conformal field theory and conformally covariant models in planar random geometry. In this article, we explicitly derive the Virasoro algebra from an axiomatization of the conformal anomaly in terms of real determinant lines, one-dimensional vector spaces associated to Riemann surfaces with analytically parametrized boundary components. Here, analytical orientation-preserving diffeomorphisms and deformations of the circle naturally act on the boundary components. We introduce a sewing operation on the real determinant lines over the semigroup of annuli, which then induces central extensions of the diffeomorphism group, as well as of the complex deformations.   Our main theorem shows that on the one hand, the Lie algebra cocycle associated to the central extension of diffeomorphisms is trivial, while on the other hand, the one associated to the central extension of complex deformations is nontrivial, yielding the imaginary part of the Gel'fand-Fuks cocycle. The proof uses concrete computations, which we hope to be accessible to a wide audience. We also show an explicit relation to loop Loewner energy, anticipating the real determinant lines to be pertinent to semi-classical limits of random curves, as well as to Kahler geometry and geometric quantization of moduli spaces of Riemann surfaces. The conformal anomaly and real determinant line bundles are expected to be universal, following a classification of modular functors.","sentences":["The conformal anomaly and the Virasoro algebra are fundamental aspects of 2D conformal field theory and conformally covariant models in planar random geometry.","In this article, we explicitly derive the Virasoro algebra from an axiomatization of the conformal anomaly in terms of real determinant lines, one-dimensional vector spaces associated to Riemann surfaces with analytically parametrized boundary components.","Here, analytical orientation-preserving diffeomorphisms and deformations of the circle naturally act on the boundary components.","We introduce a sewing operation on the real determinant lines over the semigroup of annuli, which then induces central extensions of the diffeomorphism group, as well as of the complex deformations.   ","Our main theorem shows that on the one hand, the Lie algebra cocycle associated to the central extension of diffeomorphisms is trivial, while on the other hand, the one associated to the central extension of complex deformations is nontrivial, yielding the imaginary part of the Gel'fand-Fuks cocycle.","The proof uses concrete computations, which we hope to be accessible to a wide audience.","We also show an explicit relation to loop Loewner energy, anticipating the real determinant lines to be pertinent to semi-classical limits of random curves, as well as to Kahler geometry and geometric quantization of moduli spaces of Riemann surfaces.","The conformal anomaly and real determinant line bundles are expected to be universal, following a classification of modular functors."],"url":"http://arxiv.org/abs/2403.09628v1","category":"math-ph"}
{"created":"2024-03-14 17:51:38","title":"Compute-first optical detection for noise-resilient visual perception","abstract":"In the context of visual perception, the optical signal from a scene is transferred into the electronic domain by detectors in the form of image data, which are then processed for the extraction of visual information. In noisy and weak-signal environments such as thermal imaging for night vision applications, however, the performance of neural computing tasks faces a significant bottleneck due to the inherent degradation of data quality upon noisy detection. Here, we propose a concept of optical signal processing before detection to address this issue. We demonstrate that spatially redistributing optical signals through a properly designed linear transformer can enhance the detection noise resilience of visual perception tasks, as benchmarked with the MNIST classification. Our idea is supported by a quantitative analysis detailing the relationship between signal concentration and noise robustness, as well as its practical implementation in an incoherent imaging system. This compute-first detection scheme can pave the way for advancing infrared machine vision technologies widely used for industrial and defense applications.","sentences":["In the context of visual perception, the optical signal from a scene is transferred into the electronic domain by detectors in the form of image data, which are then processed for the extraction of visual information.","In noisy and weak-signal environments such as thermal imaging for night vision applications, however, the performance of neural computing tasks faces a significant bottleneck due to the inherent degradation of data quality upon noisy detection.","Here, we propose a concept of optical signal processing before detection to address this issue.","We demonstrate that spatially redistributing optical signals through a properly designed linear transformer can enhance the detection noise resilience of visual perception tasks, as benchmarked with the MNIST classification.","Our idea is supported by a quantitative analysis detailing the relationship between signal concentration and noise robustness, as well as its practical implementation in an incoherent imaging system.","This compute-first detection scheme can pave the way for advancing infrared machine vision technologies widely used for industrial and defense applications."],"url":"http://arxiv.org/abs/2403.09612v1","category":"physics.optics"}
{"created":"2024-03-14 17:48:30","title":"pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication","abstract":"Extended Reality (XR) allows in-situ previewing of designs to be manufactured through Personal Fabrication (PF). These in-situ interactions exhibit advantages for PF, like incorporating the environment into the design process. However, design-for-fabrication in XR often happens through either highly complex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourced models. We present pARam, a tool combining parametric designs (PDs) and XR, enabling in-situ configuration of artifacts for PF. In contrast to modeling- or search-focused approaches, pARam supports customization through embodied and practical inputs (e.g., gestures, recommendations) and evaluation (e.g., lighting estimation) without demanding complex 3D-modeling skills. We implemented pARam for HoloLens 2 and evaluated it (n=20), comparing XR and desktop conditions. Users succeeded in choosing context-related parameters and took their environment into account for their configuration using pARam. We reflect on the prospects and challenges of PDs in XR to streamline complex design methods for PF while retaining suitable expressivity.","sentences":["Extended Reality (XR) allows in-situ previewing of designs to be manufactured through Personal Fabrication (PF).","These in-situ interactions exhibit advantages for PF, like incorporating the environment into the design process.","However, design-for-fabrication in XR often happens through either highly complex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourced models.","We present pARam, a tool combining parametric designs (PDs) and XR, enabling in-situ configuration of artifacts for PF.","In contrast to modeling- or search-focused approaches, pARam supports customization through embodied and practical inputs (e.g., gestures, recommendations) and evaluation (e.g., lighting estimation) without demanding complex 3D-modeling skills.","We implemented pARam for HoloLens 2 and evaluated it (n=20), comparing XR and desktop conditions.","Users succeeded in choosing context-related parameters and took their environment into account for their configuration using pARam.","We reflect on the prospects and challenges of PDs in XR to streamline complex design methods for PF while retaining suitable expressivity."],"url":"http://arxiv.org/abs/2403.09607v1","category":"cs.HC"}
{"created":"2024-03-14 17:36:16","title":"Scalable Autonomous Drone Flight in the Forest with Visual-Inertial SLAM and Dense Submaps Built without LiDAR","abstract":"Forestry constitutes a key element for a sustainable future, while it is supremely challenging to introduce digital processes to improve efficiency. The main limitation is the difficulty of obtaining accurate maps at high temporal and spatial resolution as a basis for informed forestry decision-making, due to the vast area forests extend over and the sheer number of trees. To address this challenge, we present an autonomous Micro Aerial Vehicle (MAV) system which purely relies on cost-effective and light-weight passive visual and inertial sensors to perform under-canopy autonomous navigation. We leverage visual-inertial simultaneous localization and mapping (VI-SLAM) for accurate MAV state estimates and couple it with a volumetric occupancy submapping system to achieve a scalable mapping framework which can be directly used for path planning. As opposed to a monolithic map, submaps inherently deal with inevitable drift and corrections from VI-SLAM, since they move with pose estimates as they are updated. To ensure the safety of the MAV during navigation, we also propose a novel reference trajectory anchoring scheme that moves and deforms the reference trajectory the MAV is tracking upon state updates from the VI-SLAM system in a consistent way, even upon large changes in state estimates due to loop-closures. We thoroughly validate our system in both real and simulated forest environments with high tree densities in excess of 400 trees per hectare and at speeds up to 3 m/s - while not encountering a single collision or system failure. To the best of our knowledge this is the first system which achieves this level of performance in such unstructured environment using low-cost passive visual sensors and fully on-board computation including VI-SLAM.","sentences":["Forestry constitutes a key element for a sustainable future, while it is supremely challenging to introduce digital processes to improve efficiency.","The main limitation is the difficulty of obtaining accurate maps at high temporal and spatial resolution as a basis for informed forestry decision-making, due to the vast area forests extend over and the sheer number of trees.","To address this challenge, we present an autonomous Micro Aerial Vehicle (MAV) system which purely relies on cost-effective and light-weight passive visual and inertial sensors to perform under-canopy autonomous navigation.","We leverage visual-inertial simultaneous localization and mapping (VI-SLAM) for accurate MAV state estimates and couple it with a volumetric occupancy submapping system to achieve a scalable mapping framework which can be directly used for path planning.","As opposed to a monolithic map, submaps inherently deal with inevitable drift and corrections from VI-SLAM, since they move with pose estimates as they are updated.","To ensure the safety of the MAV during navigation, we also propose a novel reference trajectory anchoring scheme that moves and deforms the reference trajectory the MAV is tracking upon state updates from the VI-SLAM system in a consistent way, even upon large changes in state estimates due to loop-closures.","We thoroughly validate our system in both real and simulated forest environments with high tree densities in excess of 400 trees per hectare and at speeds up to 3 m/s - while not encountering a single collision or system failure.","To the best of our knowledge this is the first system which achieves this level of performance in such unstructured environment using low-cost passive visual sensors and fully on-board computation including VI-SLAM."],"url":"http://arxiv.org/abs/2403.09596v1","category":"cs.RO"}
{"created":"2024-03-14 17:35:57","title":"A comprehensive study of orbital evolution of LMC X-4: Existence of a second derivative of the orbital period","abstract":"We report here results from pulse arrival time delay analysis of the eclipsing high mass X-ray binary pulsar LMC X-4 using observations made with the Rossi X-ray Timing Explorer, XMM-Newton, NuSTAR and AstroSat. Combining the orbital parameters determined from these observations with the historical measurements dating back to 1998, we have extended the $T_{\\pi/2}$ epoch history of LMC X-4 by about 4600 binary orbits spanning about 18 years. We also report mid-eclipse time measurements ($T_{ecl}$) using data obtained from wide-field X-ray monitors of MAXI-GSC and Swift-BAT. Combining the new $T_{\\pi/2}$ and $T_{ecl}$ estimates with all the previously reported values, we have significantly improved the orbital evolution measurement, which indicates that the orbital period is evolving at a time scale ($P_{\\rm orb}/\\dot{P}_{\\rm orb}$ ) of about 0.8 Myr. For the first time in an accreting X-ray pulsar system, we confirm the existence of a second derivative of the orbital period, having an evolution time scale ($\\dot{P}_{orb}/\\ddot{P}_{orb}$) of about 55 yr. Detection of a second derivative of the orbital period in LMC X-4 makes its orbital evolution timescale more uncertain, which may also be true for other HMXBs. Independent solutions for the orbital evolution measurement using the mid-eclipse data and the pulse timing data are consistent with each other, and help us put an upper limit of 0.009 on the eccentricity of the binary system.","sentences":["We report here results from pulse arrival time delay analysis of the eclipsing high mass X-ray binary pulsar LMC X-4 using observations made with the Rossi X-ray Timing Explorer, XMM-Newton, NuSTAR and AstroSat.","Combining the orbital parameters determined from these observations with the historical measurements dating back to 1998, we have extended the $T_{\\pi/2}$ epoch history of LMC X-4 by about 4600 binary orbits spanning about 18 years.","We also report mid-eclipse time measurements ($T_{ecl}$) using data obtained from wide-field X-ray monitors of MAXI-GSC and Swift-BAT.","Combining the new $T_{\\pi/2}$ and $T_{ecl}$ estimates with all the previously reported values, we have significantly improved the orbital evolution measurement, which indicates that the orbital period is evolving at a time scale ($P_{\\rm orb}/\\dot{P}_{\\rm orb}$ ) of about 0.8 Myr.","For the first time in an accreting X-ray pulsar system, we confirm the existence of a second derivative of the orbital period, having an evolution time scale ($\\dot{P}_{orb}/\\ddot{P}_{orb}$) of about 55 yr. Detection of a second derivative of the orbital period in LMC X-4 makes its orbital evolution timescale more uncertain, which may also be true for other HMXBs.","Independent solutions for the orbital evolution measurement using the mid-eclipse data and the pulse timing data are consistent with each other, and help us put an upper limit of 0.009 on the eccentricity of the binary system."],"url":"http://arxiv.org/abs/2403.09595v1","category":"astro-ph.HE"}
{"created":"2024-03-14 17:26:00","title":"Iterative Forgetting: Online Data Stream Regression Using Database-Inspired Adaptive Granulation","abstract":"Many modern systems, such as financial, transportation, and telecommunications systems, are time-sensitive in the sense that they demand low-latency predictions for real-time decision-making. Such systems often have to contend with continuous unbounded data streams as well as concept drift, which are challenging requirements that traditional regression techniques are unable to cater to. There exists a need to create novel data stream regression methods that can handle these scenarios. We present a database-inspired datastream regression model that (a) uses inspiration from R*-trees to create granules from incoming datastreams such that relevant information is retained, (b) iteratively forgets granules whose information is deemed to be outdated, thus maintaining a list of only recent, relevant granules, and (c) uses the recent data and granules to provide low-latency predictions. The R*-tree-inspired approach also makes the algorithm amenable to integration with database systems. Our experiments demonstrate that the ability of this method to discard data produces a significant order-of-magnitude improvement in latency and training time when evaluated against the most accurate state-of-the-art algorithms, while the R*-tree-inspired granulation technique provides competitively accurate predictions","sentences":["Many modern systems, such as financial, transportation, and telecommunications systems, are time-sensitive in the sense that they demand low-latency predictions for real-time decision-making.","Such systems often have to contend with continuous unbounded data streams as well as concept drift, which are challenging requirements that traditional regression techniques are unable to cater to.","There exists a need to create novel data stream regression methods that can handle these scenarios.","We present a database-inspired datastream regression model that (a) uses inspiration from R*-trees to create granules from incoming datastreams such that relevant information is retained, (b) iteratively forgets granules whose information is deemed to be outdated, thus maintaining a list of only recent, relevant granules, and (c) uses the recent data and granules to provide low-latency predictions.","The R*-tree-inspired approach also makes the algorithm amenable to integration with database systems.","Our experiments demonstrate that the ability of this method to discard data produces a significant order-of-magnitude improvement in latency and training time when evaluated against the most accurate state-of-the-art algorithms, while the R*-tree-inspired granulation technique provides competitively accurate predictions"],"url":"http://arxiv.org/abs/2403.09588v1","category":"cs.LG"}
{"created":"2024-03-14 17:09:22","title":"Angle estimation using mmWave RSS measurements with enhanced multipath information","abstract":"mmWave communication has come up as the unexplored spectrum for 5G services. With new standards for 5G NR positioning, more off-the-shelf platforms and algorithms are needed to perform indoor positioning. An object can be accurately positioned in a room either by using an angle and a delay estimate or two angle estimates or three delay estimates. We propose an algorithm to jointly estimate the angle of arrival (AoA) and angle of departure (AoD), based only on the received signal strength (RSS). We use mm-FLEX, an experimentation platform developed by IMDEA Networks Institute that can perform real-time signal processing for experimental validation of our proposed algorithm. Codebook-based beampatterns are used with a uniquely placed multi-antenna array setup to enhance the reception of multipath components and we obtain an AoA estimate per receiver thereby overcoming the line-of-sight (LoS) limitation of RSS-based localization systems. We further validate the results from measurements by emulating the setup with a simple ray-tracing approach.","sentences":["mmWave communication has come up as the unexplored spectrum for 5G services.","With new standards for 5G NR positioning, more off-the-shelf platforms and algorithms are needed to perform indoor positioning.","An object can be accurately positioned in a room either by using an angle and a delay estimate or two angle estimates or three delay estimates.","We propose an algorithm to jointly estimate the angle of arrival (AoA) and angle of departure (AoD), based only on the received signal strength (RSS).","We use mm-FLEX, an experimentation platform developed by IMDEA Networks Institute that can perform real-time signal processing for experimental validation of our proposed algorithm.","Codebook-based beampatterns are used with a uniquely placed multi-antenna array setup to enhance the reception of multipath components and we obtain an AoA estimate per receiver thereby overcoming the line-of-sight (LoS) limitation of RSS-based localization systems.","We further validate the results from measurements by emulating the setup with a simple ray-tracing approach."],"url":"http://arxiv.org/abs/2403.09575v1","category":"eess.SP"}
{"created":"2024-03-14 17:03:11","title":"Learning High-Order Control Barrier Functions for Safety-Critical Control with Gaussian Processes","abstract":"Control barrier functions (CBFs) have recently introduced a systematic tool to ensure system safety by establishing set invariance. When combined with a nominal control strategy, they form a safety-critical control mechanism. However, the effectiveness of CBFs is closely tied to the system model. In practice, model uncertainty can compromise safety guarantees and may lead to conservative safety constraints, or conversely, allow the system to operate in unsafe regions. In this paper, we use Gaussian processes to mitigate the adverse effects of uncertainty on high-order CBFs (HOCBFs). A properly structured covariance function enables us to convert the chance constraints of HOCBFs into a second-order cone constraint. This results in a convex constrained optimization as a safety filter. We analyze the feasibility of the resulting optimization and provide the necessary and sufficient conditions for feasibility. The effectiveness of the proposed strategy is validated through two numerical results.","sentences":["Control barrier functions (CBFs) have recently introduced a systematic tool to ensure system safety by establishing set invariance.","When combined with a nominal control strategy, they form a safety-critical control mechanism.","However, the effectiveness of CBFs is closely tied to the system model.","In practice, model uncertainty can compromise safety guarantees and may lead to conservative safety constraints, or conversely, allow the system to operate in unsafe regions.","In this paper, we use Gaussian processes to mitigate the adverse effects of uncertainty on high-order CBFs (HOCBFs).","A properly structured covariance function enables us to convert the chance constraints of HOCBFs into a second-order cone constraint.","This results in a convex constrained optimization as a safety filter.","We analyze the feasibility of the resulting optimization and provide the necessary and sufficient conditions for feasibility.","The effectiveness of the proposed strategy is validated through two numerical results."],"url":"http://arxiv.org/abs/2403.09573v1","category":"eess.SY"}
{"created":"2024-03-14 16:56:56","title":"PaperBot: Learning to Design Real-World Tools Using Paper","abstract":"Paper is a cheap, recyclable, and clean material that is often used to make practical tools. Traditional tool design either relies on simulation or physical analysis, which is often inaccurate and time-consuming. In this paper, we propose PaperBot, an approach that directly learns to design and use a tool in the real world using paper without human intervention. We demonstrated the effectiveness and efficiency of PaperBot on two tool design tasks: 1. learning to fold and throw paper airplanes for maximum travel distance 2. learning to cut paper into grippers that exert maximum gripping force. We present a self-supervised learning framework that learns to perform a sequence of folding, cutting, and dynamic manipulation actions in order to optimize the design and use of a tool. We deploy our system to a real-world two-arm robotic system to solve challenging design tasks that involve aerodynamics (paper airplane) and friction (paper gripper) that are impossible to simulate accurately.","sentences":["Paper is a cheap, recyclable, and clean material that is often used to make practical tools.","Traditional tool design either relies on simulation or physical analysis, which is often inaccurate and time-consuming.","In this paper, we propose PaperBot, an approach that directly learns to design and use a tool in the real world using paper without human intervention.","We demonstrated the effectiveness and efficiency of PaperBot on two tool design tasks: 1. learning to fold and throw paper airplanes for maximum travel distance 2.","learning to cut paper into grippers that exert maximum gripping force.","We present a self-supervised learning framework that learns to perform a sequence of folding, cutting, and dynamic manipulation actions in order to optimize the design and use of a tool.","We deploy our system to a real-world two-arm robotic system to solve challenging design tasks that involve aerodynamics (paper airplane) and friction (paper gripper) that are impossible to simulate accurately."],"url":"http://arxiv.org/abs/2403.09566v1","category":"cs.RO"}
{"created":"2024-03-14 16:53:12","title":"Characterization of Polarimetric Properties in Various Brain Tumor Types Using Wide-Field Imaging Mueller Polarimetry","abstract":"Neuro-oncological surgery is the primary brain cancer treatment, yet it faces challenges with gliomas due to their invasiveness and the need to preserve neurological function. Hence, radical resection is often unfeasible, highlighting the importance of precise tumor margin delineation to prevent neurological deficits and improve prognosis. Imaging Mueller polarimetry, an effective modality in various organ tissues, seems a promising approach for tumor delineation in neurosurgery. To further assess its use, we characterized the polarimetric properties by analysing 45 polarimetric measurements of 27 fresh brain tumor samples, including different tumor types with a strong focus on gliomas. Our study integrates a wide-field imaging Mueller polarimetric system and a novel neuropathology protocol, correlating polarimetric and histological data for accurate tissue identification. An image processing pipeline facilitated the alignment and overlay of polarimetric images and histological masks. Variations in depolarization values were observed for grey and white matter of brain tumor tissue, while differences in linear retardance were seen only within white matter of brain tumor tissue. Notably, we identified pronounced optical axis azimuth randomization within tumor regions. This study lays the foundation for machine learning-based brain tumor segmentation algorithms using polarimetric data, facilitating intraoperative diagnosis and decision making.","sentences":["Neuro-oncological surgery is the primary brain cancer treatment, yet it faces challenges with gliomas due to their invasiveness and the need to preserve neurological function.","Hence, radical resection is often unfeasible, highlighting the importance of precise tumor margin delineation to prevent neurological deficits and improve prognosis.","Imaging Mueller polarimetry, an effective modality in various organ tissues, seems a promising approach for tumor delineation in neurosurgery.","To further assess its use, we characterized the polarimetric properties by analysing 45 polarimetric measurements of 27 fresh brain tumor samples, including different tumor types with a strong focus on gliomas.","Our study integrates a wide-field imaging Mueller polarimetric system and a novel neuropathology protocol, correlating polarimetric and histological data for accurate tissue identification.","An image processing pipeline facilitated the alignment and overlay of polarimetric images and histological masks.","Variations in depolarization values were observed for grey and white matter of brain tumor tissue, while differences in linear retardance were seen only within white matter of brain tumor tissue.","Notably, we identified pronounced optical axis azimuth randomization within tumor regions.","This study lays the foundation for machine learning-based brain tumor segmentation algorithms using polarimetric data, facilitating intraoperative diagnosis and decision making."],"url":"http://arxiv.org/abs/2403.09561v1","category":"physics.med-ph"}
{"created":"2024-03-14 16:43:33","title":"Memoryless concretization relation","abstract":"We introduce the concept of memoryless concretization relation (MCR) to describe abstraction within the context of controller synthesis. This relation is a specific instance of alternating simulation relation (ASR), where it is possible to simplify the controller architecture. In the case of ASR, the concretized controller needs to simulate the concurrent evolution of two systems, the original and abstract systems, while for MCR, the designed controllers only need knowledge of the current concrete state. We demonstrate that the distinction between ASR and MCR becomes significant only when a non-deterministic quantizer is involved, such as in cases where the state space discretization consists of overlapping cells. We also show that any abstraction of a system that alternatingly simulates a system can be completed to satisfy MCR at the expense of increasing the non-determinism in the abstraction. We clarify the difference between the MCR and the feedback refinement relation (FRR), showing in particular that the former allows for non-constant controllers within cells. This provides greater flexibility in constructing a practical abstraction, for instance, by reducing non-determinism in the abstraction. Finally, we prove that this relation is not only sufficient, but also necessary, for ensuring the above properties.","sentences":["We introduce the concept of memoryless concretization relation (MCR) to describe abstraction within the context of controller synthesis.","This relation is a specific instance of alternating simulation relation (ASR), where it is possible to simplify the controller architecture.","In the case of ASR, the concretized controller needs to simulate the concurrent evolution of two systems, the original and abstract systems, while for MCR, the designed controllers only need knowledge of the current concrete state.","We demonstrate that the distinction between ASR and MCR becomes significant only when a non-deterministic quantizer is involved, such as in cases where the state space discretization consists of overlapping cells.","We also show that any abstraction of a system that alternatingly simulates a system can be completed to satisfy MCR at the expense of increasing the non-determinism in the abstraction.","We clarify the difference between the MCR and the feedback refinement relation (FRR), showing in particular that the former allows for non-constant controllers within cells.","This provides greater flexibility in constructing a practical abstraction, for instance, by reducing non-determinism in the abstraction.","Finally, we prove that this relation is not only sufficient, but also necessary, for ensuring the above properties."],"url":"http://arxiv.org/abs/2403.09556v1","category":"math.DS"}
{"created":"2024-03-14 16:41:08","title":"A targeted radio pulsar survey of redback candidates with MeerKAT","abstract":"Redbacks are millisecond pulsar binaries with low mass, irradiated companions. These systems have a rich phenomenology that can be used to probe binary evolution models, pulsar wind physics, and the neutron star mass distribution. A number of high-confidence redback candidates have been identified through searches for variable optical and X-ray sources within the localisation regions of unidentified but pulsar-like Fermi-LAT gamma-ray sources. However, these candidates remain unconfirmed until pulsations are detected. As part of the TRAPUM project, we searched for radio pulsations from six of these redback candidates with MeerKAT. We discovered three new radio millisecond pulsars, PSRs J0838$-$2527, J0955$-$3947 and J2333$-$5526, confirming their redback nature. PSR J0838$-$2827 remained undetected for two years after our discovery despite repeated observations, likely due to evaporated material absorbing the radio emission for long periods of time. While, to our knowledge, this system has not undergone a transition to an accreting state, the disappearance, likely caused by extreme eclipses, illustrates the transient nature of spider pulsars and the heavy selection bias in uncovering their radio population. Radio timing enabled the detection of gamma-ray pulsations from all three pulsars, from which we obtained 15-year timing solutions. All of these sources exhibit complex orbital period variations consistent with gravitational quadrupole moment variations in the companion stars. These timing solutions also constrain the binary mass ratios, allowing us to narrow down the pulsar masses. We find that PSR J2333$-$5526 may have a neutron star mass in excess of 2 M$_{\\odot}$.","sentences":["Redbacks are millisecond pulsar binaries with low mass, irradiated companions.","These systems have a rich phenomenology that can be used to probe binary evolution models, pulsar wind physics, and the neutron star mass distribution.","A number of high-confidence redback candidates have been identified through searches for variable optical and X-ray sources within the localisation regions of unidentified but pulsar-like Fermi-LAT gamma-ray sources.","However, these candidates remain unconfirmed until pulsations are detected.","As part of the TRAPUM project, we searched for radio pulsations from six of these redback candidates with MeerKAT.","We discovered three new radio millisecond pulsars, PSRs J0838$-$2527, J0955$-$3947 and J2333$-$5526, confirming their redback nature.","PSR J0838$-$2827 remained undetected for two years after our discovery despite repeated observations, likely due to evaporated material absorbing the radio emission for long periods of time.","While, to our knowledge, this system has not undergone a transition to an accreting state, the disappearance, likely caused by extreme eclipses, illustrates the transient nature of spider pulsars and the heavy selection bias in uncovering their radio population.","Radio timing enabled the detection of gamma-ray pulsations from all three pulsars, from which we obtained 15-year timing solutions.","All of these sources exhibit complex orbital period variations consistent with gravitational quadrupole moment variations in the companion stars.","These timing solutions also constrain the binary mass ratios, allowing us to narrow down the pulsar masses.","We find that PSR J2333$-$5526 may have a neutron star mass in excess of 2 M$_{\\odot}$."],"url":"http://arxiv.org/abs/2403.09553v1","category":"astro-ph.HE"}
{"created":"2024-03-14 16:29:56","title":"Competing Interactions in Strongly Driven Multi-Level Systems","abstract":"We experimentally study the level mixing, splitting and repulsion of an optically driven atomic multi-level system under two competing interactions. The strength of the optical coupling is increased until it surpasses the atomic hyperfine interaction responsible for mixing the magnetic substates. Due to the multi-level character of the coupled state space, the level shifts exhibit complex behavior reminiscent of the Paschen-Back effect. Our results show that multi-level effects can have significant influence for strong external drive, differing from a simple model of effective non-interacting two-level systems. These results highlight the relevance of imperfections of the light polarization or initial state preparation in strongly optically driven systems.","sentences":["We experimentally study the level mixing, splitting and repulsion of an optically driven atomic multi-level system under two competing interactions.","The strength of the optical coupling is increased until it surpasses the atomic hyperfine interaction responsible for mixing the magnetic substates.","Due to the multi-level character of the coupled state space, the level shifts exhibit complex behavior reminiscent of the Paschen-Back effect.","Our results show that multi-level effects can have significant influence for strong external drive, differing from a simple model of effective non-interacting two-level systems.","These results highlight the relevance of imperfections of the light polarization or initial state preparation in strongly optically driven systems."],"url":"http://arxiv.org/abs/2403.09542v1","category":"quant-ph"}
{"created":"2024-03-14 16:17:11","title":"Defense via Behavior Attestation against Attacks in Connected and Automated Vehicles based Federated Learning Systems","abstract":"The recent application of Federated Learning algorithms in IOT and Wireless vehicular networks have given rise to newer cyber threats in the mobile environment which hitherto were not present in traditional fixed networks. These threats arise due to the intrinsic nature of wireless transmission medium and other inherent characteristics of mobile networks such as high-node mobility and rapidly changing topology. This paper investigates the robustness of Vehicular AttestedFL defense strategies against falsified information attacks by tracking the behavior. We show that the defense strategies are capable of detecting and eliminating malicious nodes in the wireless mobile setting of the future smart road networks.","sentences":["The recent application of Federated Learning algorithms in IOT and Wireless vehicular networks have given rise to newer cyber threats in the mobile environment which hitherto were not present in traditional fixed networks.","These threats arise due to the intrinsic nature of wireless transmission medium and other inherent characteristics of mobile networks such as high-node mobility and rapidly changing topology.","This paper investigates the robustness of Vehicular AttestedFL defense strategies against falsified information attacks by tracking the behavior.","We show that the defense strategies are capable of detecting and eliminating malicious nodes in the wireless mobile setting of the future smart road networks."],"url":"http://arxiv.org/abs/2403.09531v1","category":"eess.SY"}
{"created":"2024-03-14 16:10:37","title":"Decay of Correlations via induced Weak Gibbs Markov maps for non-H\u00f6lder observables","abstract":"We extend the results of [Ullah, A., Vilarinho, H,.: Statistical properties of dynamical systems via induced weak Gibbs Markov maps. arXiv:2311.17531 (2023)] by considering larger classes of observables. More precisely, we obtain estimates on the decay of correlations, Central Limit Theorem and Large Deviations for dynamical systems having an induced weak Gibbs Markov map, for larger classes of observables with weaker regularity than H\\\"{o}lder.","sentences":["We extend the results of [Ullah, A., Vilarinho, H,.:","Statistical properties of dynamical systems via induced weak Gibbs Markov maps.","arXiv:2311.17531 (2023)] by considering larger classes of observables.","More precisely, we obtain estimates on the decay of correlations, Central Limit Theorem and Large Deviations for dynamical systems having an induced weak Gibbs Markov map, for larger classes of observables with weaker regularity than H\\\"{o}lder."],"url":"http://arxiv.org/abs/2403.09528v1","category":"math.DS"}
{"created":"2024-03-14 16:10:21","title":"An Extreme Ultra-Compact X-ray Binary in a Globular Cluster: Multi-Wavelength Observations of RZ2109 Explored in a Triple System Framework","abstract":"The globular cluster ultraluminous X-ray source, RZ2109, is a complex and unique system which has been detected at X-ray, ultra-violet, and optical wavelengths. Based on almost 20 years of Chandra and XMM-Newton observations, the X-ray luminosity exhibits order-of-magnitude variability, with the peak flux lasting on the order of a few hours. We perform robust time series analysis on the archival X-ray observations and find that this variability is periodic on a timescale of 1.3 $\\pm 0.04$ days. The source also demonstrates broad [OIII] 5007 Angstrom emission, which has been observed since 2004, suggesting a white dwarf donor and therefore an ultra-compact X-ray binary. We present new spectra from 2020 and 2022, marking eighteen years of observed [OIII] emission from this source. Meanwhile, we find that the globular cluster counterpart is unusually bright in the NUV/UVW2 band. Finally, we discuss RZ2109 in the context of the eccentric Kozai Lidov mechanism and show that the observed 1.3 day periodicity can be used to place constraints on the tertiary configuration, ranging from 20 minutes (for a 0.1 ${\\rm M}_\\odot$ companion) to approximately 95 minutes (for a 1 ${\\rm M}_\\odot$ companion), if the eccentric Kozai Lidov mechanism is at the origin of the periodic variability.","sentences":["The globular cluster ultraluminous X-ray source, RZ2109, is a complex and unique system which has been detected at X-ray, ultra-violet, and optical wavelengths.","Based on almost 20 years of Chandra and XMM-Newton observations, the X-ray luminosity exhibits order-of-magnitude variability, with the peak flux lasting on the order of a few hours.","We perform robust time series analysis on the archival X-ray observations and find that this variability is periodic on a timescale of 1.3 $\\pm 0.04$ days.","The source also demonstrates broad","[OIII] 5007","Angstrom emission, which has been observed since 2004, suggesting a white dwarf donor and therefore an ultra-compact X-ray binary.","We present new spectra from 2020 and 2022, marking eighteen years of observed [OIII] emission from this source.","Meanwhile, we find that the globular cluster counterpart is unusually bright in the NUV/UVW2 band.","Finally, we discuss RZ2109 in the context of the eccentric Kozai Lidov mechanism and show that the observed 1.3 day periodicity can be used to place constraints on the tertiary configuration, ranging from 20 minutes (for a 0.1 ${\\rm M}_\\odot$ companion) to approximately 95 minutes (for a 1 ${\\rm M}_\\odot$ companion), if the eccentric Kozai Lidov mechanism is at the origin of the periodic variability."],"url":"http://arxiv.org/abs/2403.09525v1","category":"astro-ph.HE"}
{"created":"2024-03-14 16:03:58","title":"Smallest gaps between eigenvalues of real Gaussian matrices","abstract":"We consider an $n\\times n$ matrix of independent real Gaussian random variables and determine the asymptotic distribution of the smallest gaps between complex eigenvalues.","sentences":["We consider an $n\\times n$ matrix of independent real Gaussian random variables and determine the asymptotic distribution of the smallest gaps between complex eigenvalues."],"url":"http://arxiv.org/abs/2403.09521v1","category":"math.PR"}
{"created":"2024-03-14 16:01:26","title":"Observation of quantum thermalization restricted to Hilbert space fragments","abstract":"Quantum thermalization occurs in a broad class of systems from elementary particles to complex materials. Out-of-equilibrium quantum systems have long been understood to either thermalize or retain memory of their initial states, but not both. Here we achieve the first simultaneous demonstration of thermalization and memory in a quantum system. Using a Rydberg atom array, we observe quantum thermalization restricted to Hilbert space fragments, where the thermalized system retains characteristics of the initial configuration. Intriguingly, states from different subspaces do not thermalize with each other even when they have the same energy. Our work challenges established ideas of quantum thermalization while experimentally resolving the longstanding tension between thermalization and memory. These results may be applied to control entanglement dynamics in quantum processors and quantum sensors.","sentences":["Quantum thermalization occurs in a broad class of systems from elementary particles to complex materials.","Out-of-equilibrium quantum systems have long been understood to either thermalize or retain memory of their initial states, but not both.","Here we achieve the first simultaneous demonstration of thermalization and memory in a quantum system.","Using a Rydberg atom array, we observe quantum thermalization restricted to Hilbert space fragments, where the thermalized system retains characteristics of the initial configuration.","Intriguingly, states from different subspaces do not thermalize with each other even when they have the same energy.","Our work challenges established ideas of quantum thermalization while experimentally resolving the longstanding tension between thermalization and memory.","These results may be applied to control entanglement dynamics in quantum processors and quantum sensors."],"url":"http://arxiv.org/abs/2403.09517v1","category":"quant-ph"}
{"created":"2024-03-14 15:56:02","title":"On STPA for Distributed Development of Safe Autonomous Driving: An Interview Study","abstract":"Safety analysis is used to identify hazards and build knowledge during the design phase of safety-relevant functions. This is especially true for complex AI-enabled and software intensive systems such as Autonomous Drive (AD). System-Theoretic Process Analysis (STPA) is a novel method applied in safety-related fields like defense and aerospace, which is also becoming popular in the automotive industry. However, STPA assumes prerequisites that are not fully valid in the automotive system engineering with distributed system development and multi-abstraction design levels. This would inhibit software developers from using STPA to analyze their software as part of a bigger system, resulting in a lack of traceability. This can be seen as a maintainability challenge in continuous development and deployment (DevOps). In this paper, STPA's different guidelines for the automotive industry, e.g. J31887/ISO21448/STPA handbook, are firstly compared to assess their applicability to the distributed development of complex AI-enabled systems like AD. Further, an approach to overcome the challenges of using STPA in a multi-level design context is proposed. By conducting an interview study with automotive industry experts for the development of AD, the challenges are validated and the effectiveness of the proposed approach is evaluated.","sentences":["Safety analysis is used to identify hazards and build knowledge during the design phase of safety-relevant functions.","This is especially true for complex AI-enabled and software intensive systems such as Autonomous Drive (AD).","System-Theoretic Process Analysis (STPA) is a novel method applied in safety-related fields like defense and aerospace, which is also becoming popular in the automotive industry.","However, STPA assumes prerequisites that are not fully valid in the automotive system engineering with distributed system development and multi-abstraction design levels.","This would inhibit software developers from using STPA to analyze their software as part of a bigger system, resulting in a lack of traceability.","This can be seen as a maintainability challenge in continuous development and deployment (DevOps).","In this paper, STPA's different guidelines for the automotive industry, e.g. J31887/ISO21448/STPA handbook, are firstly compared to assess their applicability to the distributed development of complex AI-enabled systems like AD.","Further, an approach to overcome the challenges of using STPA in a multi-level design context is proposed.","By conducting an interview study with automotive industry experts for the development of AD, the challenges are validated and the effectiveness of the proposed approach is evaluated."],"url":"http://arxiv.org/abs/2403.09509v1","category":"cs.SE"}
{"created":"2024-03-14 15:48:07","title":"Is Data All That Matters? The Role of Control Frequency for Learning-Based Sampled-Data Control of Uncertain Systems","abstract":"Learning models or control policies from data has become a powerful tool to improve the performance of uncertain systems. While a strong focus has been placed on increasing the amount and quality of data to improve performance, data can never fully eliminate uncertainty, making feedback necessary to ensure stability and performance. We show that the control frequency at which the input is recalculated is a crucial design parameter, yet it has hardly been considered before. We address this gap by combining probabilistic model learning and sampled-data control. We use Gaussian processes (GPs) to learn a continuous-time model and compute a corresponding discrete-time controller. The result is an uncertain sampled-data control system, for which we derive robust stability conditions. We formulate semidefinite programs to compute the minimum control frequency required for stability and to optimize performance. As a result, our approach enables us to study the effect of both control frequency and data on stability and closed-loop performance. We show in numerical simulations of a quadrotor that performance can be improved by increasing either the amount of data or the control frequency, and that we can trade off one for the other. For example, by increasing the control frequency by 33%, we can reduce the number of data points by half while still achieving similar performance.","sentences":["Learning models or control policies from data has become a powerful tool to improve the performance of uncertain systems.","While a strong focus has been placed on increasing the amount and quality of data to improve performance, data can never fully eliminate uncertainty, making feedback necessary to ensure stability and performance.","We show that the control frequency at which the input is recalculated is a crucial design parameter, yet it has hardly been considered before.","We address this gap by combining probabilistic model learning and sampled-data control.","We use Gaussian processes (GPs) to learn a continuous-time model and compute a corresponding discrete-time controller.","The result is an uncertain sampled-data control system, for which we derive robust stability conditions.","We formulate semidefinite programs to compute the minimum control frequency required for stability and to optimize performance.","As a result, our approach enables us to study the effect of both control frequency and data on stability and closed-loop performance.","We show in numerical simulations of a quadrotor that performance can be improved by increasing either the amount of data or the control frequency, and that we can trade off one for the other.","For example, by increasing the control frequency by 33%, we can reduce the number of data points by half while still achieving similar performance."],"url":"http://arxiv.org/abs/2403.09504v1","category":"eess.SY"}
{"created":"2024-03-14 15:18:28","title":"Sunyaev-Zeldovich Signals from $L^*$ Galaxies: Observations, Analytics, and Simulations","abstract":"We analyze measurements of the thermal Sunyaev-Zeldovich (tSZ) effect arising in the circumgalactic medium (CGM) of $L^*$ galaxies, reported by Bregman et al. 2022 and Das et al. 2023. In our analysis we use the Faerman et al. 2017 and Faerman et al. 2020 CGM models, a new power-law model (PLM), and the TNG100 simulation. For a given $M_{\\rm vir}$, our PLM has four parameters; the fraction, $f_{\\rm hCGM}$, of the halo baryon mass in hot CGM gas, the ratio, $\\phi_T$, of the actual gas temperature at the virial radius to the virial temperature, and the power-law indicies, $a_{P,{\\rm th}}$ and $a_n$ for the thermal electron pressure and the hydrogen nucleon density. The B+22 Compton-$y$ profile implies steep electron pressure slopes ($a_{P,{\\rm th}}\\simeq 2$). For isothermal conditions the temperature is at least $1.1\\times 10^6$ K, with a hot CGM gas mass of up to $3.5\\times 10^{11}$ M$_\\odot$ for a virial mass of $2.75\\times 10^{12}$ M$_\\odot$. However, if isothermal the gas must be expanding out of the halos. An isentropic equation of state is favored for which hydrostatic equilibrium is possible. The B+22 and D+23 results are consistent with each other and with recent (0.5-2 keV) CGM X-ray observations by Zhang et al. 2024 of Milky Way mass systems. For $M_{\\rm vir}\\simeq 3\\times 10^{12}$ M$_\\odot$, the scaled Compton pressure integrals, $E(z)^{-2/3}Y_{500}/M_{\\rm vir,12}^{5/3}$, lie in the narrow range, $2.5\\times 10^{-4}$ to $5.0\\times 10^{-4}$ kpc$^2$, for all three sets of observations. TNG100 underpredicts the tSZ parameters by factors $\\sim 0.5$ dex for the $L^*$ galaxies, suggesting that the feedback strengths and CGM gas losses are overestimated in the simulated halos at these mass scales.","sentences":["We analyze measurements of the thermal Sunyaev-Zeldovich (tSZ) effect arising in the circumgalactic medium (CGM) of $L^*$ galaxies, reported by Bregman et al. 2022 and Das et al. 2023.","In our analysis we use the Faerman et al. 2017 and Faerman et al. 2020","CGM models, a new power-law model (PLM), and the TNG100 simulation.","For a given $M_{\\rm vir}$, our PLM has four parameters; the fraction, $f_{\\rm hCGM}$, of the halo baryon mass in hot CGM gas, the ratio, $\\phi_T$, of the actual gas temperature at the virial radius to the virial temperature, and the power-law indicies, $a_{P,{\\rm th}}$ and $a_n$ for the thermal electron pressure and the hydrogen nucleon density.","The B+22 Compton-$y$ profile implies steep electron pressure slopes ($a_{P,{\\rm th}}\\simeq 2$).","For isothermal conditions the temperature is at least $1.1\\times 10^6$ K, with a hot CGM gas mass of up to $3.5\\times 10^{11}$ M$_\\odot$ for a virial mass of $2.75\\times 10^{12}$ M$_\\odot$.","However, if isothermal the gas must be expanding out of the halos.","An isentropic equation of state is favored for which hydrostatic equilibrium is possible.","The B+22 and D+23 results are consistent with each other and with recent (0.5-2 keV) CGM X-ray observations by Zhang et al. 2024 of Milky Way mass systems.","For $M_{\\rm vir}\\simeq 3\\times 10^{12}$ M$_\\odot$, the scaled Compton pressure integrals, $E(z)^{-2/3}Y_{500}/M_{\\rm vir,12}^{5/3}$, lie in the narrow range, $2.5\\times 10^{-4}$ to $5.0\\times 10^{-4}$ kpc$^2$, for all three sets of observations.","TNG100 underpredicts the tSZ parameters by factors $\\sim 0.5$ dex for the $L^*$ galaxies, suggesting that the feedback strengths and CGM gas losses are overestimated in the simulated halos at these mass scales."],"url":"http://arxiv.org/abs/2403.09476v1","category":"astro-ph.GA"}
{"created":"2024-03-14 15:17:56","title":"Covert Communication for Untrusted UAV-Assisted Wireless Systems","abstract":"Wireless systems are of paramount importance for providing ubiquitous data transmission for smart cities. However, due to the broadcasting and openness of wireless channels, such systems face potential security challenges. UAV-assisted covert communication is a supporting technology for improving covert performances and has become a hot issue in the research of wireless communication security. This paper investigates the performance of joint covert and security communication in a tow-hop UAV-assisted wireless system, where a source transmits the covert message to a destination with the help of an untrusted UAV. We first design a transmission scheme such that use UAVs to assist in covert communications while ensuring the security of covert messages. Then, we develop a theoretical model to derive the expressions for the detection error probability of the warden and the covert and security rate, and the maximum covert and security rate is optimized by power control under a given covertness and security requirements. Finally, numerical results are provided to illustrate our theoretical analysis and the performance of covert and security communication in such systems.","sentences":["Wireless systems are of paramount importance for providing ubiquitous data transmission for smart cities.","However, due to the broadcasting and openness of wireless channels, such systems face potential security challenges.","UAV-assisted covert communication is a supporting technology for improving covert performances and has become a hot issue in the research of wireless communication security.","This paper investigates the performance of joint covert and security communication in a tow-hop UAV-assisted wireless system, where a source transmits the covert message to a destination with the help of an untrusted UAV.","We first design a transmission scheme such that use UAVs to assist in covert communications while ensuring the security of covert messages.","Then, we develop a theoretical model to derive the expressions for the detection error probability of the warden and the covert and security rate, and the maximum covert and security rate is optimized by power control under a given covertness and security requirements.","Finally, numerical results are provided to illustrate our theoretical analysis and the performance of covert and security communication in such systems."],"url":"http://arxiv.org/abs/2403.09475v1","category":"cs.CR"}
{"created":"2024-03-14 15:00:46","title":"Exact matrix product state representations for a type of scale-invariant states","abstract":"Exact matrix product state representations for a type of scale-invariant states are presented, which describe highly degenerate ground states arising from spontaneous symmetry breaking with type-B Goldstone modes in one-dimensional quantum many-body systems. As a possible application, such a representation offers a convenient but powerful means for evaluating the norms of highly degenerate ground states. This in turn allows us to perform a universal finite system-size scaling analysis of the entanglement entropy. Moreover, this approach vividly explains why the entanglement entropy does not depend on what types of the boundary conditions are adopted, either periodic boundary conditions or open boundary conditions. Illustrative examples include the ${\\rm SU}(2)$ spin-$s$ Heisenberg ferromagnetic model, the ${\\rm SU}(2s+1)$ ferromagnetic model, and the staggered ${\\rm SU}(3)$ spin-1 ferromagnetic biquadratic model.","sentences":["Exact matrix product state representations for a type of scale-invariant states are presented, which describe highly degenerate ground states arising from spontaneous symmetry breaking with type-B Goldstone modes in one-dimensional quantum many-body systems.","As a possible application, such a representation offers a convenient but powerful means for evaluating the norms of highly degenerate ground states.","This in turn allows us to perform a universal finite system-size scaling analysis of the entanglement entropy.","Moreover, this approach vividly explains why the entanglement entropy does not depend on what types of the boundary conditions are adopted, either periodic boundary conditions or open boundary conditions.","Illustrative examples include the ${\\rm SU}(2)$ spin-$s$ Heisenberg ferromagnetic model, the ${\\rm SU}(2s+1)$ ferromagnetic model, and the staggered ${\\rm SU}(3)$ spin-1 ferromagnetic biquadratic model."],"url":"http://arxiv.org/abs/2403.09458v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 14:53:54","title":"The Neural-SRP method for positional sound source localization","abstract":"Steered Response Power (SRP) is a widely used method for the task of sound source localization using microphone arrays, showing satisfactory localization performance on many practical scenarios. However, its performance is diminished under highly reverberant environments. Although Deep Neural Networks (DNNs) have been previously proposed to overcome this limitation, most are trained for a specific number of microphones with fixed spatial coordinates. This restricts their practical application on scenarios frequently observed in wireless acoustic sensor networks, where each application has an ad-hoc microphone topology. We propose Neural-SRP, a DNN which combines the flexibility of SRP with the performance gains of DNNs. We train our network using simulated data and transfer learning, and evaluate our approach on recorded and simulated data. Results verify that Neural-SRP's localization performance significantly outperforms the baselines.","sentences":["Steered Response Power (SRP) is a widely used method for the task of sound source localization using microphone arrays, showing satisfactory localization performance on many practical scenarios.","However, its performance is diminished under highly reverberant environments.","Although Deep Neural Networks (DNNs) have been previously proposed to overcome this limitation, most are trained for a specific number of microphones with fixed spatial coordinates.","This restricts their practical application on scenarios frequently observed in wireless acoustic sensor networks, where each application has an ad-hoc microphone topology.","We propose Neural-SRP, a DNN which combines the flexibility of SRP with the performance gains of DNNs.","We train our network using simulated data and transfer learning, and evaluate our approach on recorded and simulated data.","Results verify that Neural-SRP's localization performance significantly outperforms the baselines."],"url":"http://arxiv.org/abs/2403.09455v1","category":"cs.SD"}
{"created":"2024-03-14 14:49:40","title":"M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in Cognitive Load Assessment","abstract":"This paper introduces the M&M model, a novel multimodal-multitask learning framework, applied to the AVCAffe dataset for cognitive load assessment (CLA). M&M uniquely integrates audiovisual cues through a dual-pathway architecture, featuring specialized streams for audio and video inputs. A key innovation lies in its cross-modality multihead attention mechanism, fusing the different modalities for synchronized multitasking. Another notable feature is the model's three specialized branches, each tailored to a specific cognitive load label, enabling nuanced, task-specific analysis. While it shows modest performance compared to the AVCAffe's single-task baseline, M\\&M demonstrates a promising framework for integrated multimodal processing. This work paves the way for future enhancements in multimodal-multitask learning systems, emphasizing the fusion of diverse data types for complex task handling.","sentences":["This paper introduces the M&M model, a novel multimodal-multitask learning framework, applied to the AVCAffe dataset for cognitive load assessment (CLA).","M&M uniquely integrates audiovisual cues through a dual-pathway architecture, featuring specialized streams for audio and video inputs.","A key innovation lies in its cross-modality multihead attention mechanism, fusing the different modalities for synchronized multitasking.","Another notable feature is the model's three specialized branches, each tailored to a specific cognitive load label, enabling nuanced, task-specific analysis.","While it shows modest performance compared to the AVCAffe's single-task baseline, M\\&M demonstrates a promising framework for integrated multimodal processing.","This work paves the way for future enhancements in multimodal-multitask learning systems, emphasizing the fusion of diverse data types for complex task handling."],"url":"http://arxiv.org/abs/2403.09451v1","category":"cs.CV"}
{"created":"2024-03-14 14:42:30","title":"Benchmarking Distributed Coordination Systems: A Survey and Analysis","abstract":"Coordination services and protocols are critical components of distributed systems and are essential for providing consistency, fault tolerance, and scalability. However, due to lack of a standard benchmarking tool for distributed coordination services, coordination service developers/researchers either use a NoSQL standard benchmark and omit evaluating consistency, distribution, and fault-tolerance; or create their own ad-hoc microbenchmarks and skip comparability with other services. In this paper, we analyze and compare known and widely used distributed coordination services, their evaluations, and the tools used to benchmark those systems. We identify important requirements of distributed coordination service benchmarking, like the metrics and parameters that need to be evaluated and their evaluation setups and tools.","sentences":["Coordination services and protocols are critical components of distributed systems and are essential for providing consistency, fault tolerance, and scalability.","However, due to lack of a standard benchmarking tool for distributed coordination services, coordination service developers/researchers either use a NoSQL standard benchmark and omit evaluating consistency, distribution, and fault-tolerance; or create their own ad-hoc microbenchmarks and skip comparability with other services.","In this paper, we analyze and compare known and widely used distributed coordination services, their evaluations, and the tools used to benchmark those systems.","We identify important requirements of distributed coordination service benchmarking, like the metrics and parameters that need to be evaluated and their evaluation setups and tools."],"url":"http://arxiv.org/abs/2403.09445v1","category":"cs.DC"}
{"created":"2024-03-14 14:30:31","title":"Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting","abstract":"Current human pose estimation systems focus on retrieving an accurate 3D global estimate of a single person. Therefore, this paper presents one of the first 3D multi-person human pose estimation systems that is able to work in real-time and is also able to handle basic forms of occlusion. First, we adjust an off-the-shelf 2D detector and an unsupervised 2D-3D lifting model for use with a 360$^\\circ$ panoramic camera and mmWave radar sensors. We then introduce several contributions, including camera and radar calibrations, and the improved matching of people within the image and radar space. The system addresses both the depth and scale ambiguity problems by employing a lightweight 2D-3D pose lifting algorithm that is able to work in real-time while exhibiting accurate performance in both indoor and outdoor environments which offers both an affordable and scalable solution. Notably, our system's time complexity remains nearly constant irrespective of the number of detected individuals, achieving a frame rate of approximately 7-8 fps on a laptop with a commercial-grade GPU.","sentences":["Current human pose estimation systems focus on retrieving an accurate 3D global estimate of a single person.","Therefore, this paper presents one of the first 3D multi-person human pose estimation systems that is able to work in real-time and is also able to handle basic forms of occlusion.","First, we adjust an off-the-shelf 2D detector and an unsupervised 2D-3D lifting model for use with a 360$^\\circ$ panoramic camera and mmWave radar sensors.","We then introduce several contributions, including camera and radar calibrations, and the improved matching of people within the image and radar space.","The system addresses both the depth and scale ambiguity problems by employing a lightweight 2D-3D pose lifting algorithm that is able to work in real-time while exhibiting accurate performance in both indoor and outdoor environments which offers both an affordable and scalable solution.","Notably, our system's time complexity remains nearly constant irrespective of the number of detected individuals, achieving a frame rate of approximately 7-8 fps on a laptop with a commercial-grade GPU."],"url":"http://arxiv.org/abs/2403.09437v1","category":"cs.CV"}
{"created":"2024-03-14 14:23:02","title":"Field-orientation-dependent magnetic phases in GdRu$_2$Si$_2$ probed with muon-spin spectroscopy","abstract":"Centrosymmetric GdRu$_2$Si$_2$ exhibits a variety of multi-Q magnetic states as a function of temperature and applied magnetic field, including a square skyrmion-lattice phase. The material's behavior is strongly dependent on the direction of the applied field, with different phase diagrams resulting for fields applied parallel or perpendicular to the crystallographic $c$ axis. Here, we present the results of muon-spin relaxation ($\\mu^+$SR) measurements on single crystals of GdRu$_2$Si$_2$. Our analysis is based on the computation of muon stopping sites and consideration of zero-point motion effects, allowing direct comparison with the underlying spin textures in the material. Using transverse-field $\\mu^+$SR with fields applied along either the [001] or [100] crystallographic directions, we distinguish between the magnetic phases in this system via their distinct muon response, providing additional evidence for the skyrmion and meron-lattice phases, while also suggesting the existence of RKKY-driven muon hyperfine coupling. Zero-field $\\mu^+$SR provides clear evidence for a transition between two distinct magnetically-ordered phases at 39 K.","sentences":["Centrosymmetric GdRu$_2$Si$_2$ exhibits a variety of multi-Q magnetic states as a function of temperature and applied magnetic field, including a square skyrmion-lattice phase.","The material's behavior is strongly dependent on the direction of the applied field, with different phase diagrams resulting for fields applied parallel or perpendicular to the crystallographic $c$ axis.","Here, we present the results of muon-spin relaxation ($\\mu^+$SR) measurements on single crystals of GdRu$_2$Si$_2$. Our analysis is based on the computation of muon stopping sites and consideration of zero-point motion effects, allowing direct comparison with the underlying spin textures in the material.","Using transverse-field $\\mu^+$SR with fields applied along either the [001] or [100] crystallographic directions, we distinguish between the magnetic phases in this system via their distinct muon response, providing additional evidence for the skyrmion and meron-lattice phases, while also suggesting the existence of RKKY-driven muon hyperfine coupling.","Zero-field $\\mu^+$SR provides clear evidence for a transition between two distinct magnetically-ordered phases at 39 K."],"url":"http://arxiv.org/abs/2403.09431v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 14:17:01","title":"Low-coercive-field ferroelectric hafnia with mobile domain walls","abstract":"The high coercive field ($\\mathcal{E}_c$) of hafnia-based ferroelectrics presents a major obstacle to their applications. The ferroelectric switching mechanisms in hafnia that dictate $\\mathcal{E}_c$, especially those related to nucleation-and-growth at the domain wall (DW), have remained elusive. Through deep-learning-assisted multiscale simulations, we determine the finite-temperature thermodynamics and switching mechanisms for diverse types of 180$^\\circ$ DWs, revealing a complex, stress-sensitive mobility landscape. The propagation velocities for mobile DW types under various thermal conditions can be characterized with a single creep equation, featuring a creep exponent of 2. This unconventional critical exponent results from the nucleation of a half-unit-cell-thin, elliptically-shaped critical nucleus. Our multiscale approach not only reproduces the experimental thickness ($d$) scaling, $\\mathcal{E}_c\\propto d^{-\\frac{2}{3}}$, but also predicts that $\\mathcal{E}_c$ of HfO$_2$ can be engineered to $\\approx$0.1 MV/cm, even lower than perovskite ferroelectrics. The theoretical lower bound of $\\mathcal{E}_c$ afforded by ferroelectric hafnia offers opportunities to realize power-efficient, high-fidelity ferroelectric nanoelectronics.","sentences":["The high coercive field ($\\mathcal{E}_c$) of hafnia-based ferroelectrics presents a major obstacle to their applications.","The ferroelectric switching mechanisms in hafnia that dictate $\\mathcal{E}_c$, especially those related to nucleation-and-growth at the domain wall (DW), have remained elusive.","Through deep-learning-assisted multiscale simulations, we determine the finite-temperature thermodynamics and switching mechanisms for diverse types of 180$^\\circ$ DWs, revealing a complex, stress-sensitive mobility landscape.","The propagation velocities for mobile DW types under various thermal conditions can be characterized with a single creep equation, featuring a creep exponent of 2.","This unconventional critical exponent results from the nucleation of a half-unit-cell-thin, elliptically-shaped critical nucleus.","Our multiscale approach not only reproduces the experimental thickness ($d$) scaling, $\\mathcal{E}_c\\propto d^{-\\frac{2}{3}}$, but also predicts that $\\mathcal{E}_c$ of HfO$_2$ can be engineered to $\\approx$0.1 MV/cm, even lower than perovskite ferroelectrics.","The theoretical lower bound of $\\mathcal{E}_c$ afforded by ferroelectric hafnia offers opportunities to realize power-efficient, high-fidelity ferroelectric nanoelectronics."],"url":"http://arxiv.org/abs/2403.09426v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-14 14:15:14","title":"Inelastic neutron scattering and muon spin relaxation investigations of the deuterated Kondo lattices CeNiSnD$ _x $","abstract":"CeNiSn is a Kondo semimetal where a gap opens at low temperatures due to hybridization between 4$f$ and conduction electrons, but a full insulating state fails to develop. Upon the insertion of hydrogen, long range magnetic order is induced. Here we report zero-field muon-spin relaxation and inelastic neutron scattering measurements of polycrystalline samples of the deuterides CeNiSnD$_x$ ($x$=1.0, 1.8). The muon-spin relaxation results confirm magnetic ordering in the whole sample of CeNiSnD below around 4.7 K, while inelastic neutron scattering reveals two well-defined crystalline-electric field (CEF) excitations at around 13 meV and 34 meV in CeNiSnD, and 5 meV and 27 meV for CeNiSnD$_{1.8}$. These results suggest that hydrogenation leads to the localization of the Ce-4$f$ electrons, giving rise to long-range magnetic order. We propose CEF level schemes for both systems, which predict a ground state moment of 0.96$\\mu_{\\rm B}$/Ce within the $ab$-plane for CeNiSnD$_{1.8}$ and a saturated moment of 1.26$\\mu_{\\rm B}$/Ce along the easy $c$ axis for CeNiSnD, that account for the observed magnetic properties.","sentences":["CeNiSn is a Kondo semimetal where a gap opens at low temperatures due to hybridization between 4$f$ and conduction electrons, but a full insulating state fails to develop.","Upon the insertion of hydrogen, long range magnetic order is induced.","Here we report zero-field muon-spin relaxation and inelastic neutron scattering measurements of polycrystalline samples of the deuterides CeNiSnD$_x$ ($x$=1.0, 1.8).","The muon-spin relaxation results confirm magnetic ordering in the whole sample of CeNiSnD below around 4.7 K, while inelastic neutron scattering reveals two well-defined crystalline-electric field (CEF) excitations at around 13 meV and 34 meV in CeNiSnD, and 5 meV and 27 meV for CeNiSnD$_{1.8}$.","These results suggest that hydrogenation leads to the localization of the Ce-4$f$ electrons, giving rise to long-range magnetic order.","We propose CEF level schemes for both systems, which predict a ground state moment of 0.96$\\mu_{\\rm B}$/Ce within the $ab$-plane for CeNiSnD$_{1.8}$ and a saturated moment of 1.26$\\mu_{\\rm B}$/Ce along the easy $c$ axis for CeNiSnD, that account for the observed magnetic properties."],"url":"http://arxiv.org/abs/2403.09424v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 14:08:59","title":"RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban Scenes","abstract":"The task of separating dynamic objects from static environments using NeRFs has been widely studied in recent years. However, capturing large-scale scenes still poses a challenge due to their complex geometric structures and unconstrained dynamics. Without the help of 3D motion cues, previous methods often require simplified setups with slow camera motion and only a few/single dynamic actors, leading to suboptimal solutions in most urban setups. To overcome such limitations, we present RoDUS, a pipeline for decomposing static and dynamic elements in urban scenes, with thoughtfully separated NeRF models for moving and non-moving components. Our approach utilizes a robust kernel-based initialization coupled with 4D semantic information to selectively guide the learning process. This strategy enables accurate capturing of the dynamics in the scene, resulting in reduced artifacts caused by NeRF on background reconstruction, all by using self-supervision. Notably, experimental evaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of our method in decomposing challenging urban scenes into precise static and dynamic components.","sentences":["The task of separating dynamic objects from static environments using NeRFs has been widely studied in recent years.","However, capturing large-scale scenes still poses a challenge due to their complex geometric structures and unconstrained dynamics.","Without the help of 3D motion cues, previous methods often require simplified setups with slow camera motion and only a few/single dynamic actors, leading to suboptimal solutions in most urban setups.","To overcome such limitations, we present RoDUS, a pipeline for decomposing static and dynamic elements in urban scenes, with thoughtfully separated NeRF models for moving and non-moving components.","Our approach utilizes a robust kernel-based initialization coupled with 4D semantic information to selectively guide the learning process.","This strategy enables accurate capturing of the dynamics in the scene, resulting in reduced artifacts caused by NeRF on background reconstruction, all by using self-supervision.","Notably, experimental evaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of our method in decomposing challenging urban scenes into precise static and dynamic components."],"url":"http://arxiv.org/abs/2403.09419v1","category":"cs.CV"}
{"created":"2024-03-14 14:01:19","title":"Binomial sums and Mellin asymptotics with explicit error bounds: a case study","abstract":"Making use of a newly developed package in the computer algebra system SageMath, we show how to perform a full asymptotic analysis by means of the Mellin transform with explicit error bounds. As an application of the method, we answer a question of B\\'ona and DeJonge on 132-avoiding permutations with a unique longest increasing subsequence that can be translated into an inequality for a certain binomial sum.","sentences":["Making use of a newly developed package in the computer algebra system SageMath, we show how to perform a full asymptotic analysis by means of the Mellin transform with explicit error bounds.","As an application of the method, we answer a question of B\\'ona and DeJonge on 132-avoiding permutations with a unique longest increasing subsequence that can be translated into an inequality for a certain binomial sum."],"url":"http://arxiv.org/abs/2403.09408v1","category":"math.CO"}
{"created":"2024-03-14 13:56:44","title":"Difference of solutions for the inversion problem of ultra-elliptic integrals","abstract":"Let $V$ be a hyperelliptic curve of genus 2 defined by $Y^2=f(X)$, where $f(X)$ is a polynomial of degree 5. The sigma function associated with $V$ is a holomorphic function on $\\mathbb{C}^2$. For a point $P$ on $V$, we consider the problem to express the $X$-coordinate of $P$ in terms of the image of $P$ under the Abel-Jacobi map. Two meromorphic functions $f_2$ and $g_2$ on $\\mathbb{C}^2$ which give solutions of this problem are known. Since $f_2$ and $g_2$ coincide on the zero set of the sigma function, it is expected that $f_2-g_2$ can be divided by the sigma function. In this paper, we decompose $f_2-g_2$ into a product of the sigma function and a meromorphic function explicitly.","sentences":["Let $V$ be a hyperelliptic curve of genus 2 defined by $Y^2=f(X)$, where $f(X)$ is a polynomial of degree 5.","The sigma function associated with $V$ is a holomorphic function on $\\mathbb{C}^2$. For a point $P$ on $V$, we consider the problem to express the $X$-coordinate of $P$ in terms of the image of $P$ under the Abel-Jacobi map.","Two meromorphic functions $f_2$ and $g_2$ on $\\mathbb{C}^2$ which give solutions of this problem are known.","Since $f_2$ and $g_2$ coincide on the zero set of the sigma function, it is expected that $f_2-g_2$ can be divided by the sigma function.","In this paper, we decompose $f_2-g_2$ into a product of the sigma function and a meromorphic function explicitly."],"url":"http://arxiv.org/abs/2403.09406v1","category":"math.CV"}
{"created":"2024-03-14 13:52:48","title":"Comparative Microscopic Study of Entropies and their Production","abstract":"We study the time evolution of eleven microscopic entropy definitions (of Boltzmann-surface, Gibbs-volume, canonical, coarse-grained-observational, entanglement and diagonal type) and three microscopic temperature definitions (based on Boltzmann, Gibbs or canonical entropy). This is done for the archetypal nonequilibrium setup of two systems exchanging energy, modeled here with random matrix theory, based on numerical integration of the Schroedinger equation. We consider three types of pure initial states (local energy eigenstates, decorrelated and entangled microcanonical states) and three classes of systems: (A) two normal systems, (B) a normal and a negative temperature system and (C) a normal and a negative heat capacity system.   We find: (1) All types of initial states give rise to the same macroscopic dynamics. (2) Entanglement and diagonal entropy sensitively depend on the microstate, in contrast to all other entropies. (3) For class B and C, Gibbs-volume entropies can violate the second law and the associated temperature becomes meaningless. (4) For class C, Boltzmann-surface entropies can violate the second law and the associated temperature becomes meaningless. (5) Canonical entropy has a tendency to remain almost constant. (6) For a Haar random initial state, entanglement or diagonal entropy behave similar or identical to coarse-grained-observational entropy.","sentences":["We study the time evolution of eleven microscopic entropy definitions (of Boltzmann-surface, Gibbs-volume, canonical, coarse-grained-observational, entanglement and diagonal type) and three microscopic temperature definitions (based on Boltzmann, Gibbs or canonical entropy).","This is done for the archetypal nonequilibrium setup of two systems exchanging energy, modeled here with random matrix theory, based on numerical integration of the Schroedinger equation.","We consider three types of pure initial states (local energy eigenstates, decorrelated and entangled microcanonical states) and three classes of systems: (A) two normal systems, (B) a normal and a negative temperature system and (C) a normal and a negative heat capacity system.   ","We find: (1) All types of initial states give rise to the same macroscopic dynamics.","(2) Entanglement and diagonal entropy sensitively depend on the microstate, in contrast to all other entropies.","(3) For class B and C, Gibbs-volume entropies can violate the second law and the associated temperature becomes meaningless.","(4) For class C, Boltzmann-surface entropies can violate the second law and the associated temperature becomes meaningless.","(5) Canonical entropy has a tendency to remain almost constant.","(6) For a Haar random initial state, entanglement or diagonal entropy behave similar or identical to coarse-grained-observational entropy."],"url":"http://arxiv.org/abs/2403.09403v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-14 13:52:41","title":"An Extensible Framework for Architecture-Based Data Flow Analysis for Information Security","abstract":"The growing interconnection between software systems increases the need for security already at design time. Security-related properties like confidentiality are often analyzed based on data flow diagrams (DFDs). However, manually analyzing DFDs of large software systems is bothersome and error-prone, and adjusting an already deployed software is costly. Additionally, closed analysis ecosystems limit the reuse of modeled information and impede comprehensive statements about a system's security. In this paper, we present an open and extensible framework for data flow analysis. The central element of our framework is our new implementation of a well-validated data-flow-based analysis approach. The framework is compatible with DFDs and can also extract data flows from the Palladio architectural description language. We showcase the extensibility with multiple model and analysis extensions. Our evaluation indicates that we can analyze similar scenarios while achieving higher scalability compared to previous implementations.","sentences":["The growing interconnection between software systems increases the need for security already at design time.","Security-related properties like confidentiality are often analyzed based on data flow diagrams (DFDs).","However, manually analyzing DFDs of large software systems is bothersome and error-prone, and adjusting an already deployed software is costly.","Additionally, closed analysis ecosystems limit the reuse of modeled information and impede comprehensive statements about a system's security.","In this paper, we present an open and extensible framework for data flow analysis.","The central element of our framework is our new implementation of a well-validated data-flow-based analysis approach.","The framework is compatible with DFDs and can also extract data flows from the Palladio architectural description language.","We showcase the extensibility with multiple model and analysis extensions.","Our evaluation indicates that we can analyze similar scenarios while achieving higher scalability compared to previous implementations."],"url":"http://arxiv.org/abs/2403.09402v1","category":"cs.SE"}
{"created":"2024-03-14 13:49:26","title":"A device for studying elementary plasticity fluctuations in granular media","abstract":"In this manuscript, we describe a scientific device specifically designed for the study of the plasticity fluctuations preceding the fracture of granular media. Biaxial tests on model granular media are performed using a commercial uniaxial loading system. Strain field fluctuations are measured using a method based on the interference of coherent light scattered by the sample. We show that such a device enables discrete plasticity events to be unambiguously evidenced. Moreover, those discrete plasticity fluctuations depend only on the imposed strain, and not on the strain rate.","sentences":["In this manuscript, we describe a scientific device specifically designed for the study of the plasticity fluctuations preceding the fracture of granular media.","Biaxial tests on model granular media are performed using a commercial uniaxial loading system.","Strain field fluctuations are measured using a method based on the interference of coherent light scattered by the sample.","We show that such a device enables discrete plasticity events to be unambiguously evidenced.","Moreover, those discrete plasticity fluctuations depend only on the imposed strain, and not on the strain rate."],"url":"http://arxiv.org/abs/2403.09396v1","category":"cond-mat.soft"}
{"created":"2024-03-14 13:40:26","title":"Learning to optimize with convergence guarantees using nonlinear system theory","abstract":"The increasing reliance on numerical methods for controlling dynamical systems and training machine learning models underscores the need to devise algorithms that dependably and efficiently navigate complex optimization landscapes. Classical gradient descent methods offer strong theoretical guarantees for convex problems; however, they demand meticulous hyperparameter tuning for non-convex ones. The emerging paradigm of learning to optimize (L2O) automates the discovery of algorithms with optimized performance leveraging learning models and data - yet, it lacks a theoretical framework to analyze convergence and robustness of the learned algorithms. In this paper, we fill this gap by harnessing nonlinear system theory. Specifically, we propose an unconstrained parametrization of all convergent algorithms for smooth non-convex objective functions. Notably, our framework is directly compatible with automatic differentiation tools, ensuring convergence by design while learning to optimize.","sentences":["The increasing reliance on numerical methods for controlling dynamical systems and training machine learning models underscores the need to devise algorithms that dependably and efficiently navigate complex optimization landscapes.","Classical gradient descent methods offer strong theoretical guarantees for convex problems; however, they demand meticulous hyperparameter tuning for non-convex ones.","The emerging paradigm of learning to optimize (L2O) automates the discovery of algorithms with optimized performance leveraging learning models and data - yet, it lacks a theoretical framework to analyze convergence and robustness of the learned algorithms.","In this paper, we fill this gap by harnessing nonlinear system theory.","Specifically, we propose an unconstrained parametrization of all convergent algorithms for smooth non-convex objective functions.","Notably, our framework is directly compatible with automatic differentiation tools, ensuring convergence by design while learning to optimize."],"url":"http://arxiv.org/abs/2403.09389v1","category":"eess.SY"}
{"created":"2024-03-14 13:38:54","title":"High-energy Neutrinos from Outflows Powered by Kicked Remnants of Binary Black Hole Mergers in AGN Accretion Disks","abstract":"Merging of stellar-mass binary black holes (BBH) could take place within the accretion disk of active galactic nuclei (AGN). The resulting BH remnant is likely to accrete the disk gas at a super-Eddington rate, launching a fast, quasi-spherical outflow (wind). Particles will be accelerated by shocks driven by the wind, subsequently interacting with the shocked disk gas or radiation field through hadronic processes and resulting in the production of high-energy neutrinos and potential electromagnetic (EM) emissions. This study delves into the intricate evolution of the shock driven by the remnant BH wind within AGN disks. Subsequently, we calculated the production of neutrinos and the expected detection numbers for a single event, along with their contributions to the overall diffuse neutrino background. Our analysis, considering various scenarios, reveals considerable neutrino production and possible detection by IceCube for nearby events. The contribution of the remnant BH winds on the diffuse neutrino background is minor due to the low event rate density, but it can be improved to some extent for some optimistic parameters. We also propose that there could be two neutrino/EM bursts, one originating from the premerger BBH wind and the other from the remnant BH wind, with the latter typically having a time gap to the GW event of around tens of days. When combined with the anticipated gravitational waves (GW) emitted during the BBH merger, such a system emerges as a promising candidate for joint observations involving neutrinos, GWs, and EM signals.","sentences":["Merging of stellar-mass binary black holes (BBH) could take place within the accretion disk of active galactic nuclei (AGN).","The resulting BH remnant is likely to accrete the disk gas at a super-Eddington rate, launching a fast, quasi-spherical outflow (wind).","Particles will be accelerated by shocks driven by the wind, subsequently interacting with the shocked disk gas or radiation field through hadronic processes and resulting in the production of high-energy neutrinos and potential electromagnetic (EM) emissions.","This study delves into the intricate evolution of the shock driven by the remnant BH wind within AGN disks.","Subsequently, we calculated the production of neutrinos and the expected detection numbers for a single event, along with their contributions to the overall diffuse neutrino background.","Our analysis, considering various scenarios, reveals considerable neutrino production and possible detection by IceCube for nearby events.","The contribution of the remnant BH winds on the diffuse neutrino background is minor due to the low event rate density, but it can be improved to some extent for some optimistic parameters.","We also propose that there could be two neutrino/EM bursts, one originating from the premerger BBH wind and the other from the remnant BH wind, with the latter typically having a time gap to the GW event of around tens of days.","When combined with the anticipated gravitational waves (GW) emitted during the BBH merger, such a system emerges as a promising candidate for joint observations involving neutrinos, GWs, and EM signals."],"url":"http://arxiv.org/abs/2403.09387v1","category":"astro-ph.HE"}
{"created":"2024-03-14 13:36:01","title":"Exploring the Interplay of Intrinsic Fluctuation and Complexity in Intracellular Calcium Dynamics","abstract":"The concentration of intracellular calcium ion (Ca$^{2+}$) exhibits complex oscillations, including bursting and chaos, as observed experimentally. These dynamics are influenced by inherent fluctuations within cells, which serve as crucial determinants in cellular decision-making processes and fate determination. In this study, we systematically explore the interplay between intrinsic fluctuation and the complexity of intracellular cytosolic Ca$^{2+}$ dynamics using complexity measures such as permutation entropy (PE) and statistical complexity (SC). Using the chemical Langevin equation, we simulate the stochastic dynamics of cytosolic Ca$^{2+}$. Our findings reveal that PE and SC effectively characterize the diverse, dynamic states of cytosolic Ca$^{2+}$ and illustrate their interactions with intrinsic fluctuation. PE analysis elucidates that the chaotic state is more sensitive to intrinsic fluctuation than the other periodic states. Furthermore, we identify distinct states of cytosolic Ca$^{2+}$ occupying specific locations within the theoretical bounds of the complexity-entropy causality plane. These locations indicate varying complexity and information content as intrinsic fluctuation varies. When adjusting the permutation order, the SC for the different states exhibits peaks in an intermediate range of intrinsic fluctuation values. Additionally, we identify scale-free or self-similar patterns in this intermediate range, which are further corroborated by multifractal detrended fluctuation analysis. These high-complexity states likely correspond to optimal Ca$^{2+}$ dynamics with biological significance, revealing rich and complex dynamics shaped by the interplay of intrinsic fluctuation and complexity. Our investigation enhances our understanding of how intrinsic fluctuation modulates the complexity of intracellular Ca$^{2+}$ dynamics that play crucial roles in biological cells.","sentences":["The concentration of intracellular calcium ion (Ca$^{2+}$) exhibits complex oscillations, including bursting and chaos, as observed experimentally.","These dynamics are influenced by inherent fluctuations within cells, which serve as crucial determinants in cellular decision-making processes and fate determination.","In this study, we systematically explore the interplay between intrinsic fluctuation and the complexity of intracellular cytosolic Ca$^{2+}$ dynamics using complexity measures such as permutation entropy (PE) and statistical complexity (SC).","Using the chemical Langevin equation, we simulate the stochastic dynamics of cytosolic Ca$^{2+}$.","Our findings reveal that PE and SC effectively characterize the diverse, dynamic states of cytosolic Ca$^{2+}$ and illustrate their interactions with intrinsic fluctuation.","PE analysis elucidates that the chaotic state is more sensitive to intrinsic fluctuation than the other periodic states.","Furthermore, we identify distinct states of cytosolic Ca$^{2+}$ occupying specific locations within the theoretical bounds of the complexity-entropy causality plane.","These locations indicate varying complexity and information content as intrinsic fluctuation varies.","When adjusting the permutation order, the SC for the different states exhibits peaks in an intermediate range of intrinsic fluctuation values.","Additionally, we identify scale-free or self-similar patterns in this intermediate range, which are further corroborated by multifractal detrended fluctuation analysis.","These high-complexity states likely correspond to optimal Ca$^{2+}$ dynamics with biological significance, revealing rich and complex dynamics shaped by the interplay of intrinsic fluctuation and complexity.","Our investigation enhances our understanding of how intrinsic fluctuation modulates the complexity of intracellular Ca$^{2+}$ dynamics that play crucial roles in biological cells."],"url":"http://arxiv.org/abs/2403.09386v1","category":"nlin.AO"}
{"created":"2024-03-14 13:34:53","title":"Anomalous thermal transport and high thermoelectric performance of Cu-based vanadate CuVO3","abstract":"Thermoelectric (TE) conversion technology, capable of transforming heat into electricity, is critical for sustainable energy solutions. Many promising TE materials contain rare or toxic elements, so the development of cost-effective and eco-friendly high-performance TE materials is highly urgent. Herein, we explore the thermal transport and TE properties of transition metal vanadate CuVO3 by using first-principles calculation. On the basis of unified theory of heat conduction, we uncover the hierarchical thermal transport feature in CuVO3, where wave-like tunneling makes a significant contribution to the lattice thermal conductivity (\\k{appa}l) and result in the anomalously weak temperature dependence of \\k{appa}l. This is primarily attributable to the complex phononic band structure caused by the heterogeneity of Cu-O and V-O bonds. Simultaneously, we report a high power factor of 5.45 mW K-2 m-1 realized in hole-doped CuVO3, which arises from a high electrical conductivity and a large Seebeck coefficient enabled by the multiple valleys and large electronic density of states near the valence band edge. Impressively, the low \\k{appa}l and the high power factor make p-typed CuVO3 have ZT of up to 1.39, with the excellent average ZT above 1.0 from 300 to 600 K, which is superior to most reported Cu-based TE materials. Our findings suggest that CuVO3 compound is promising candidate for energy conversion applications in innovative TE devices.","sentences":["Thermoelectric (TE) conversion technology, capable of transforming heat into electricity, is critical for sustainable energy solutions.","Many promising TE materials contain rare or toxic elements, so the development of cost-effective and eco-friendly high-performance TE materials is highly urgent.","Herein, we explore the thermal transport and TE properties of transition metal vanadate CuVO3 by using first-principles calculation.","On the basis of unified theory of heat conduction, we uncover the hierarchical thermal transport feature in CuVO3, where wave-like tunneling makes a significant contribution to the lattice thermal conductivity (\\k{appa}l) and result in the anomalously weak temperature dependence of \\k{appa}l.","This is primarily attributable to the complex phononic band structure caused by the heterogeneity of Cu-O and V-O bonds.","Simultaneously, we report a high power factor of 5.45 mW K-2 m-1 realized in hole-doped CuVO3, which arises from a high electrical conductivity and a large Seebeck coefficient enabled by the multiple valleys and large electronic density of states near the valence band edge.","Impressively, the low \\k{appa}l and the high power factor make p-typed CuVO3 have ZT of up to 1.39, with the excellent average ZT above 1.0 from 300 to 600 K, which is superior to most reported Cu-based TE materials.","Our findings suggest that CuVO3 compound is promising candidate for energy conversion applications in innovative TE devices."],"url":"http://arxiv.org/abs/2403.09384v1","category":"physics.comp-ph"}
{"created":"2024-03-14 13:34:30","title":"Pantypes: Diverse Representatives for Self-Explainable Models","abstract":"Prototypical self-explainable classifiers have emerged to meet the growing demand for interpretable AI systems. These classifiers are designed to incorporate high transparency in their decisions by basing inference on similarity with learned prototypical objects. While these models are designed with diversity in mind, the learned prototypes often do not sufficiently represent all aspects of the input distribution, particularly those in low density regions. Such lack of sufficient data representation, known as representation bias, has been associated with various detrimental properties related to machine learning diversity and fairness. In light of this, we introduce pantypes, a new family of prototypical objects designed to capture the full diversity of the input distribution through a sparse set of objects. We show that pantypes can empower prototypical self-explainable models by occupying divergent regions of the latent space and thus fostering high diversity, interpretability and fairness.","sentences":["Prototypical self-explainable classifiers have emerged to meet the growing demand for interpretable AI systems.","These classifiers are designed to incorporate high transparency in their decisions by basing inference on similarity with learned prototypical objects.","While these models are designed with diversity in mind, the learned prototypes often do not sufficiently represent all aspects of the input distribution, particularly those in low density regions.","Such lack of sufficient data representation, known as representation bias, has been associated with various detrimental properties related to machine learning diversity and fairness.","In light of this, we introduce pantypes, a new family of prototypical objects designed to capture the full diversity of the input distribution through a sparse set of objects.","We show that pantypes can empower prototypical self-explainable models by occupying divergent regions of the latent space and thus fostering high diversity, interpretability and fairness."],"url":"http://arxiv.org/abs/2403.09383v1","category":"stat.ML"}
{"created":"2024-03-14 13:31:56","title":"Impact of Synthetic Images on Morphing Attack Detection Using a Siamese Network","abstract":"This paper evaluated the impact of synthetic images on Morphing Attack Detection (MAD) using a Siamese network with a semi-hard-loss function. Intra and cross-dataset evaluations were performed to measure synthetic image generalisation capabilities using a cross-dataset for evaluation. Three different pre-trained networks were used as feature extractors from traditional MobileNetV2, MobileNetV3 and EfficientNetB0. Our results show that MAD trained on EfficientNetB0 from FERET, FRGCv2, and FRLL can reach a lower error rate in comparison with SOTA. Conversely, worse performances were reached when the system was trained only with synthetic images. A mixed approach (synthetic + digital) database may help to improve MAD and reduce the error rate. This fact shows that we still need to keep going with our efforts to include synthetic images in the training process.","sentences":["This paper evaluated the impact of synthetic images on Morphing Attack Detection (MAD) using a Siamese network with a semi-hard-loss function.","Intra and cross-dataset evaluations were performed to measure synthetic image generalisation capabilities using a cross-dataset for evaluation.","Three different pre-trained networks were used as feature extractors from traditional MobileNetV2, MobileNetV3 and EfficientNetB0.","Our results show that MAD trained on EfficientNetB0 from FERET, FRGCv2, and FRLL can reach a lower error rate in comparison with SOTA.","Conversely, worse performances were reached when the system was trained only with synthetic images.","A mixed approach (synthetic + digital) database may help to improve MAD and reduce the error rate.","This fact shows that we still need to keep going with our efforts to include synthetic images in the training process."],"url":"http://arxiv.org/abs/2403.09380v1","category":"cs.CV"}
{"created":"2024-03-14 13:21:50","title":"Solvability of the Inverse Optimal Control problem based on the minimum principle","abstract":"In this paper, the solvability of the Inverse Optimal Control (IOC) problem based on two existing minimum principal methods, is analysed. The aim of this work is to answer the question regarding what kinds of trajectories, that is depending on the initial conditions of the closed-loop system and system dynamics, of the original optimal control problem, will result in the recovery of the true weights of the reward function for both the soft and the hard-constrained methods [1], [2]. Analytical conditions are provided which allow to verify if a trajectory is sufficiently conditioned, that is, holds sufficient information to recover the true weights of an optimal control problem. It was found that the open-loop system of the original optimal problem has a stronger influence on the solvability of the Inverse Optimal Control problem for the hard-constrained method as compared to the soft-constrained method. These analytical results were validated via simulation.","sentences":["In this paper, the solvability of the Inverse Optimal Control (IOC) problem based on two existing minimum principal methods, is analysed.","The aim of this work is to answer the question regarding what kinds of trajectories, that is depending on the initial conditions of the closed-loop system and system dynamics, of the original optimal control problem, will result in the recovery of the true weights of the reward function for both the soft and the hard-constrained methods [1], [2].","Analytical conditions are provided which allow to verify if a trajectory is sufficiently conditioned, that is, holds sufficient information to recover the true weights of an optimal control problem.","It was found that the open-loop system of the original optimal problem has a stronger influence on the solvability of the Inverse Optimal Control problem for the hard-constrained method as compared to the soft-constrained method.","These analytical results were validated via simulation."],"url":"http://arxiv.org/abs/2403.09375v1","category":"math.OC"}
{"created":"2024-03-14 13:19:48","title":"On function spaces for radial functions","abstract":"This paper is concerned with complex Banach-space valued functions of the form $$ \\hat{f}_k(r\\cos\\theta,r\\sin\\theta,z)=\\mathrm{e}^{\\mathrm{i} k \\theta}f_k(r,z), \\qquad r \\in [0,\\infty), \\theta \\in \\mathbb{T}^1, z \\in \\mathbb{R}, $$ for some $k \\in \\mathbb{Z}$. It is demonstrated how classical and Sobolev spaces for the radial function $f_k$ can be constructed in a natural fashion from the corresponding standard function spaces for $\\hat{f}_k$. A theory of radial distributions is derived in the same spirit. Finally, a new class of \\textit{Hankel spaces} for the case $f_k=f_k(r)$ is introduced. These spaces are the radial counterparts of the familiar Bessel-potential spaces for functions defined on $\\mathbb{R}^d$. The paper concludes with an application of the theory to the Dirichlet boundary-value problem for Poisson's equation in a cylindrical domain.","sentences":["This paper is concerned with complex Banach-space valued functions of the form $$ \\hat{f}_k(r\\cos\\theta,r\\sin\\theta,z)=\\mathrm{e}^{\\mathrm{i} k \\theta}f_k(r,z), \\qquad r \\in [0,\\infty), \\theta \\in \\mathbb{T}^1, z \\in \\mathbb{R}, $$ for some $k \\in \\mathbb{Z}$. It is demonstrated how classical and Sobolev spaces for the radial function $f_k$ can be constructed in a natural fashion from the corresponding standard function spaces for $\\hat{f}_k$. A theory of radial distributions is derived in the same spirit.","Finally, a new class of \\textit{Hankel spaces} for the case $f_k=f_k(r)$ is introduced.","These spaces are the radial counterparts of the familiar Bessel-potential spaces for functions defined on $\\mathbb{R}^d$. The paper concludes with an application of the theory to the Dirichlet boundary-value problem for Poisson's equation in a cylindrical domain."],"url":"http://arxiv.org/abs/2403.09372v1","category":"math.FA"}
{"created":"2024-03-14 13:04:40","title":"Joint Port Selection and Beamforming Design for Fluid Antenna Assisted Integrated Data and Energy Transfer","abstract":"Integrated data and energy transfer (IDET) has been of fundamental importance for providing both wireless data transfer (WDT) and wireless energy transfer (WET) services towards low-power devices. Fluid antenna (FA) is capable of exploiting the huge spatial diversity of the wireless channel to enhance the receive signal strength, which is more suitable for the tiny-size low-power devices having the IDET requirements. In this letter, a multiuser FA assisted IDET system is studied and the weighted energy harvesting power at energy receivers (ERs) is maximized by jointly optimizing the port selection and transmit beamforming design under imperfect channel state information (CSI), while the signal-to-interference-plus-noise ratio (SINR) constraint for each data receiver (DR) is satisfied. An efficient algorithm is proposed to obtain the suboptimal solutions for the non-convex problem. Simulation results evaluate the performance of the FA-IDET system, while also demonstrate that FA outperforms the multi-input-multi-output (MIMO) counterpart in terms of the IDET performance, as long as the port number is large enough.","sentences":["Integrated data and energy transfer (IDET) has been of fundamental importance for providing both wireless data transfer (WDT) and wireless energy transfer (WET) services towards low-power devices.","Fluid antenna (FA) is capable of exploiting the huge spatial diversity of the wireless channel to enhance the receive signal strength, which is more suitable for the tiny-size low-power devices having the IDET requirements.","In this letter, a multiuser FA assisted IDET system is studied and the weighted energy harvesting power at energy receivers (ERs) is maximized by jointly optimizing the port selection and transmit beamforming design under imperfect channel state information (CSI), while the signal-to-interference-plus-noise ratio (SINR) constraint for each data receiver (DR) is satisfied.","An efficient algorithm is proposed to obtain the suboptimal solutions for the non-convex problem.","Simulation results evaluate the performance of the FA-IDET system, while also demonstrate that FA outperforms the multi-input-multi-output (MIMO) counterpart in terms of the IDET performance, as long as the port number is large enough."],"url":"http://arxiv.org/abs/2403.09357v1","category":"cs.IT"}
{"created":"2024-03-14 12:57:59","title":"REPQC: Reverse Engineering and Backdooring Hardware Accelerators for Post-quantum Cryptography","abstract":"Significant research efforts have been dedicated to designing cryptographic algorithms that are quantum-resistant. The motivation is clear: robust quantum computers, once available, will render current cryptographic standards vulnerable. Thus, we need new Post-Quantum Cryptography (PQC) algorithms, and, due to the inherent complexity of such algorithms, there is also a demand to accelerate them in hardware. In this paper, we show that PQC hardware accelerators can be backdoored by two different adversaries located in the chip supply chain. We propose REPQC, a sophisticated reverse engineering algorithm that can be employed to confidently identify hashing operations (i.e., Keccak) within the PQC accelerator - the location of which serves as an anchor for finding secret information to be leaked. Armed with REPQC, an adversary proceeds to insert malicious logic in the form of a stealthy Hardware Trojan Horse (HTH). Using Dilithium as a study case, our results demonstrate that HTHs that increase the accelerator's layout density by as little as 0.1\\% can be inserted without any impact on the performance of the circuit and with a marginal increase in power consumption. An essential aspect is that the entire reverse engineering in REPQC is automated, and so is the HTH insertion that follows it, empowering adversaries to explore multiple HTH designs and identify the most suitable one.","sentences":["Significant research efforts have been dedicated to designing cryptographic algorithms that are quantum-resistant.","The motivation is clear: robust quantum computers, once available, will render current cryptographic standards vulnerable.","Thus, we need new Post-Quantum Cryptography (PQC) algorithms, and, due to the inherent complexity of such algorithms, there is also a demand to accelerate them in hardware.","In this paper, we show that PQC hardware accelerators can be backdoored by two different adversaries located in the chip supply chain.","We propose REPQC, a sophisticated reverse engineering algorithm that can be employed to confidently identify hashing operations (i.e., Keccak) within the PQC accelerator - the location of which serves as an anchor for finding secret information to be leaked.","Armed with REPQC, an adversary proceeds to insert malicious logic in the form of a stealthy Hardware Trojan Horse (HTH).","Using Dilithium as a study case, our results demonstrate that HTHs that increase the accelerator's layout density by as little as 0.1\\% can be inserted without any impact on the performance of the circuit and with a marginal increase in power consumption.","An essential aspect is that the entire reverse engineering in REPQC is automated, and so is the HTH insertion that follows it, empowering adversaries to explore multiple HTH designs and identify the most suitable one."],"url":"http://arxiv.org/abs/2403.09352v1","category":"cs.CR"}
{"created":"2024-03-14 12:52:30","title":"Pinched theorem and the reverse Yau's inequalities for compact K\u00e4hler-Einstein manifolds","abstract":"For a compact K\\\"{a}hler-Einstein manifold $M$ of dimension $n\\ge 2$, we explicitly write the expression $-c_1^n(M)+\\frac{2(n+1)}{n}c_2(M)c_1^{n-2}(M)$ in the form of certain integral on the holomorphic sectional curvature and its average at a fixed point in $M$ using the invariant theory. As applications, we get a reverse Yau's inequality and improve the classical $\\frac{1}{4}$-pinched theorem and negative $\\frac{1}{4}$-pinched theorem for compact K\\\"{a}hler-Einstein manifolds to smaller pinching constant depending only on the dimension and the first Chern class of $M$. If $M$ is not with positive or negative holomorphic sectional curvature, then there exists a point $x\\in M$ such that the average of the holomorphic sectional curvature at $x$ vanishes. In particular, we characterise the $2$-dimensional complex torus by certain curvature condition. Moreover, we confirm Yau's conjecture for positive holomorphic sectional curvature and Siu-Yang's conjecture for negative holomorphic sectional curvature even for higher dimensions if the absolute value of the holomorphic sectional curvature is small enough. Finally, using the reverse Yau's inequality, we can judge if a projective manifold doesn't carry any hermitian metric with negative holomorphic sectional curvature.","sentences":["For a compact K\\\"{a}hler-Einstein manifold $M$ of dimension $n\\ge 2$, we explicitly write the expression $-c_1^n(M)+\\frac{2(n+1)}{n}c_2(M)c_1^{n-2}(M)$ in the form of certain integral on the holomorphic sectional curvature and its average at a fixed point in $M$ using the invariant theory.","As applications, we get a reverse Yau's inequality and improve the classical $\\frac{1}{4}$-pinched theorem and negative $\\frac{1}{4}$-pinched theorem for compact K\\\"{a}hler-Einstein manifolds to smaller pinching constant depending only on the dimension and the first Chern class of $M$. If $M$ is not with positive or negative holomorphic sectional curvature, then there exists a point $x\\in M$ such that the average of the holomorphic sectional curvature at $x$ vanishes.","In particular, we characterise the $2$-dimensional complex torus by certain curvature condition.","Moreover, we confirm Yau's conjecture for positive holomorphic sectional curvature and Siu-Yang's conjecture for negative holomorphic sectional curvature even for higher dimensions if the absolute value of the holomorphic sectional curvature is small enough.","Finally, using the reverse Yau's inequality, we can judge if a projective manifold doesn't carry any hermitian metric with negative holomorphic sectional curvature."],"url":"http://arxiv.org/abs/2403.09348v1","category":"math.DG"}
{"created":"2024-03-14 12:51:58","title":"BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences","abstract":"Effective attention modules have played a crucial role in the success of Transformer-based large language models (LLMs), but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences. One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs). However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones. In this paper, we propose a distributed attention framework named ``BurstAttention'' to optimize memory access and communication operations at both the global cluster and local device levels. In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence processing. The experimental results under different length settings demonstrate that BurstAttention offers significant advantages for processing long sequences compared with these competitive baselines, reducing 40% communication overheads and achieving 2 X speedup during training 32K sequence length on 8 X A100.","sentences":["Effective attention modules have played a crucial role in the success of Transformer-based large language models (LLMs), but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences.","One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs).","However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones.","In this paper, we propose a distributed attention framework named ``BurstAttention'' to optimize memory access and communication operations at both the global cluster and local device levels.","In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence processing.","The experimental results under different length settings demonstrate that BurstAttention offers significant advantages for processing long sequences compared with these competitive baselines, reducing 40% communication overheads and achieving 2 X speedup during training 32K sequence length on 8 X A100."],"url":"http://arxiv.org/abs/2403.09347v1","category":"cs.DC"}
{"created":"2024-03-14 12:49:49","title":"Classical-Quantum correspondence in Lindblad evolution","abstract":"We show that for the Lindblad evolution defined using (at most) quadratically growing classical Hamiltonians and (at most) linearly growing classical jump functions (quantized into jump operators assumed to satisfy certain ellipticity conditions and modeling interaction with a larger system), the evolution of a quantum observable remains close to the classical Fokker--Planck evolution in the Hilbert--Schmidt norm for times vastly exceeding the Ehrenfest time (the limit of such agreement with no jump operators). The time scale is the same as two recent papers by Hern\\'andez--Ranard--Riedel but the statement and methods are different.","sentences":["We show that for the Lindblad evolution defined using (at most) quadratically growing classical Hamiltonians and (at most) linearly growing classical jump functions (quantized into jump operators assumed to satisfy certain ellipticity conditions and modeling interaction with a larger system), the evolution of a quantum observable remains close to the classical Fokker--Planck evolution in the Hilbert--Schmidt norm for times vastly exceeding the Ehrenfest time (the limit of such agreement with no jump operators).","The time scale is the same as two recent papers by Hern\\'andez--Ranard--Riedel but the statement and methods are different."],"url":"http://arxiv.org/abs/2403.09345v1","category":"math-ph"}
{"created":"2024-03-14 12:40:50","title":"Influence of Dimensionality of Carbon-based Additives on Thermoelectric Transport Parameters in Polymer Electrolytes","abstract":"This paper investigates the thermoelectric properties of solid polymer electrolytes (SPE) containing lithium bis(trifluoromethanesulfonyl)imide (LiTFSI) and sodium bis(trifluoromethanesulfonyl)imide (NaTFSI) salts, along with carbon-based additives of various dimensionalities. Increasing salt concentration leads to higher Seebeck coefficients as a result of the increasing number of free charge carriers and additional, superimposed effects by ion-ion and ion-polymer interactions. NaTFSI-based electrolytes exhibit negative Seebeck coefficients (up to $S = -1.5\\,\\mathrm{mV\\,K^{-1}}$), indicating dominant mobility of $\\mathrm{TFSI^-}$ ions. Quasi-one-dimensional carbon nanotubes (CNTs) increase the Seebeck coefficient by a factor of 3. Planar, two-dimensional graphite flakes (GF) moderately enhance it, affecting $\\mathrm{Na^+}$ and $\\mathrm{TFSI^-}$ ion mobilities and electronic conductivity. Bulky, three-dimensional carbon black (CB) additives induce a unique behavior where the sign of the Seebeck coefficient changes with temperature, presumably due to interaction with $\\mathrm{TFSI^-}$ ions within the CB structure. Changes in activation energy and Vogel temperature with salt concentration suggest structural and mechanical modifications in the polymer matrix. The choice of carbon-based additives and salt concentration significantly influences the thermoelectric properties of SPEs thermoelectric properties, providing insights into their potential for thermoelectric applications. Sodium-based electrolytes emerge as promising, sustainable alternatives to lithium-based systems, aligning with sustainable energy research demands.","sentences":["This paper investigates the thermoelectric properties of solid polymer electrolytes (SPE) containing lithium bis(trifluoromethanesulfonyl)imide (LiTFSI) and sodium bis(trifluoromethanesulfonyl)imide (NaTFSI) salts, along with carbon-based additives of various dimensionalities.","Increasing salt concentration leads to higher Seebeck coefficients as a result of the increasing number of free charge carriers and additional, superimposed effects by ion-ion and ion-polymer interactions.","NaTFSI-based electrolytes exhibit negative Seebeck coefficients (up to $S = -1.5\\,\\mathrm{mV\\,K^{-1}}$), indicating dominant mobility of $\\mathrm{TFSI^-}$ ions.","Quasi-one-dimensional carbon nanotubes (CNTs) increase the Seebeck coefficient by a factor of 3.","Planar, two-dimensional graphite flakes (GF) moderately enhance it, affecting $\\mathrm{Na^+}$ and $\\mathrm{TFSI^-}$ ion mobilities and electronic conductivity.","Bulky, three-dimensional carbon black (CB) additives induce a unique behavior where the sign of the Seebeck coefficient changes with temperature, presumably due to interaction with $\\mathrm{TFSI^-}$ ions within the CB structure.","Changes in activation energy and Vogel temperature with salt concentration suggest structural and mechanical modifications in the polymer matrix.","The choice of carbon-based additives and salt concentration significantly influences the thermoelectric properties of SPEs thermoelectric properties, providing insights into their potential for thermoelectric applications.","Sodium-based electrolytes emerge as promising, sustainable alternatives to lithium-based systems, aligning with sustainable energy research demands."],"url":"http://arxiv.org/abs/2403.09340v1","category":"physics.app-ph"}
{"created":"2024-03-14 12:32:50","title":"Field test of mode-pairing quantum key distribution","abstract":"Quantum key distribution is a cornerstone of quantum technology, offering information-theoretical secure keys for remote parties. With many quantum communication networks established globally, the mode-pairing protocol stands out for its efficacy over inter-city distances using simple setups, emerging as a promising solution. In this study, we employ the mode-pairing scheme into existing inter-city fiber links, conducting field tests across distances ranging from tens to about a hundred kilometers. Our system achieves a key rate of $1.217$ kbit/s in a $195.85$ km symmetric link and $3.089$ kbit/s in a $127.92$ km asymmetric link without global phase locking. The results demonstrate that the mode-pairing protocol can achieve key rates comparable to those of a single quantum link between two trusted nodes on the Beijing-Shanghai backbone line, effectively reducing the need for half of the trusted nodes. These field tests confirm the mode-pairing scheme's adaptability, efficiency, and practicality, positioning it as a highly suitable protocol for quantum networks.","sentences":["Quantum key distribution is a cornerstone of quantum technology, offering information-theoretical secure keys for remote parties.","With many quantum communication networks established globally, the mode-pairing protocol stands out for its efficacy over inter-city distances using simple setups, emerging as a promising solution.","In this study, we employ the mode-pairing scheme into existing inter-city fiber links, conducting field tests across distances ranging from tens to about a hundred kilometers.","Our system achieves a key rate of $1.217$ kbit/s in a $195.85$ km symmetric link and $3.089$ kbit/s in a $127.92$ km asymmetric link without global phase locking.","The results demonstrate that the mode-pairing protocol can achieve key rates comparable to those of a single quantum link between two trusted nodes on the Beijing-Shanghai backbone line, effectively reducing the need for half of the trusted nodes.","These field tests confirm the mode-pairing scheme's adaptability, efficiency, and practicality, positioning it as a highly suitable protocol for quantum networks."],"url":"http://arxiv.org/abs/2403.09339v1","category":"quant-ph"}
{"created":"2024-03-14 12:20:59","title":"A secular solar system resonance that disrupts the dominant cycle in Earth's orbital eccentricity (g2-g5): Implications for astrochronology","abstract":"The planets' gravitational interaction causes rhythmic changes in Earth's orbital parameters (also called Milankovi\\'c cycles), which have powerful applications in geology and astrochronology. For instance, the primary astronomical eccentricity cycle due to the secular frequency term (g2-g5) (~405 kyr in the recent past) utilized in deep-time analyses is dominated by Venus' and Jupiter's orbits, aka long eccentricity cycle. The widely accepted and long-held view is that (g2-g5) was practically stable in the past and may hence be used as a \"metronome\" to reconstruct accurate ages and chronologies. However, using state-of-the-art integrations of the solar system, we show here that (g2-g5) can become unstable over long time scales, without major changes in, or destabilization of, planetary orbits. The (g2-g5) disruption is due to the secular resonance $\\sigma_{12}$ = (g1 - g2) + (s1 - s2), a major contributor to solar system chaos. We demonstrate that entering/exiting the $\\sigma_{12}$ resonance is a common phenomenon on long time scales, occurring in ~40% of our solutions. During $\\sigma_{12}$-resonance episodes, (g2-g5) is very weak or absent and Earth's orbital eccentricity and climate-forcing spectrum are unrecognizable compared to the recent past. Our results have fundamental implications for geology and astrochronology, as well as climate forcing because the paradigm that the longest Milankovi\\'c cycle dominates Earth's astronomical forcing, is stable, and has a period of ~405 kyr requires revision.","sentences":["The planets' gravitational interaction causes rhythmic changes in Earth's orbital parameters (also called Milankovi\\'c cycles), which have powerful applications in geology and astrochronology.","For instance, the primary astronomical eccentricity cycle due to the secular frequency term (g2-g5) (~405 kyr in the recent past) utilized in deep-time analyses is dominated by Venus' and Jupiter's orbits, aka long eccentricity cycle.","The widely accepted and long-held view is that (g2-g5) was practically stable in the past and may hence be used as a \"metronome\" to reconstruct accurate ages and chronologies.","However, using state-of-the-art integrations of the solar system, we show here that (g2-g5) can become unstable over long time scales, without major changes in, or destabilization of, planetary orbits.","The (g2-g5) disruption is due to the secular resonance $\\sigma_{12}$ = (g1 - g2)","+","(s1 - s2), a major contributor to solar system chaos.","We demonstrate that entering/exiting the $\\sigma_{12}$ resonance is a common phenomenon on long time scales, occurring in ~40% of our solutions.","During $\\sigma_{12}$-resonance episodes, (g2-g5) is very weak or absent and Earth's orbital eccentricity and climate-forcing spectrum are unrecognizable compared to the recent past.","Our results have fundamental implications for geology and astrochronology, as well as climate forcing because the paradigm that the longest Milankovi\\'c cycle dominates Earth's astronomical forcing, is stable, and has a period of ~405 kyr requires revision."],"url":"http://arxiv.org/abs/2403.09332v1","category":"astro-ph.EP"}
{"created":"2024-03-14 12:17:07","title":"Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening","abstract":"Ill-posed image reconstruction problems appear in many scenarios such as remote sensing, where obtaining high quality images is crucial for environmental monitoring, disaster management and urban planning. Deep learning has seen great success in overcoming the limitations of traditional methods. However, these inverse problems rarely come with ground truth data, highlighting the importance of unsupervised learning from partial and noisy measurements alone. We propose perspective-equivariant imaging (EI), a framework that leverages perspective variability in optical camera-based imaging systems, such as satellites or handheld cameras, to recover information lost in ill-posed optical camera imaging problems. This extends previous EI work to include a much richer non-linear class of group transforms and is shown to be an excellent prior for satellite and urban image data, where perspective-EI achieves state-of-the-art results in multispectral pansharpening, outperforming other unsupervised methods in the literature. Code at https://andrewwango.github.io/perspective-equivariant-imaging","sentences":["Ill-posed image reconstruction problems appear in many scenarios such as remote sensing, where obtaining high quality images is crucial for environmental monitoring, disaster management and urban planning.","Deep learning has seen great success in overcoming the limitations of traditional methods.","However, these inverse problems rarely come with ground truth data, highlighting the importance of unsupervised learning from partial and noisy measurements alone.","We propose perspective-equivariant imaging (EI), a framework that leverages perspective variability in optical camera-based imaging systems, such as satellites or handheld cameras, to recover information lost in ill-posed optical camera imaging problems.","This extends previous EI work to include a much richer non-linear class of group transforms and is shown to be an excellent prior for satellite and urban image data, where perspective-EI achieves state-of-the-art results in multispectral pansharpening, outperforming other unsupervised methods in the literature.","Code at https://andrewwango.github.io/perspective-equivariant-imaging"],"url":"http://arxiv.org/abs/2403.09327v1","category":"cs.CV"}
{"created":"2024-03-14 12:00:37","title":"Large deviations of one-hidden-layer neural networks","abstract":"We study large deviations in the context of stochastic gradient descent for one-hidden-layer neural networks with quadratic loss. We derive a quenched large deviation principle, where we condition on an initial weight measure, and an annealed large deviation principle for the empirical weight evolution during training when letting the number of neurons and the number of training iterations simultaneously tend to infinity. The weight evolution is treated as an interacting dynamic particle system. The distinctive aspect compared to prior work on interacting particle systems lies in the discrete particle updates, simultaneously with a growing number of particles.","sentences":["We study large deviations in the context of stochastic gradient descent for one-hidden-layer neural networks with quadratic loss.","We derive a quenched large deviation principle, where we condition on an initial weight measure, and an annealed large deviation principle for the empirical weight evolution during training when letting the number of neurons and the number of training iterations simultaneously tend to infinity.","The weight evolution is treated as an interacting dynamic particle system.","The distinctive aspect compared to prior work on interacting particle systems lies in the discrete particle updates, simultaneously with a growing number of particles."],"url":"http://arxiv.org/abs/2403.09310v1","category":"math.PR"}
{"created":"2024-03-14 11:56:13","title":"Pushing in the Dark: A Reactive Pushing Strategy for Mobile Robots Using Tactile Feedback","abstract":"For mobile robots, navigating cluttered or dynamic environments often necessitates non-prehensile manipulation, particularly when faced with objects that are too large, irregular, or fragile to grasp. The unpredictable behavior and varying physical properties of these objects significantly complicate manipulation tasks. To address this challenge, this manuscript proposes a novel Reactive Pushing Strategy. This strategy allows a mobile robot to dynamically adjust its base movements in real-time to achieve successful pushing maneuvers towards a target location. Notably, our strategy adapts the robot motion based on changes in contact location obtained through the tactile sensor covering the base, avoiding dependence on object-related assumptions and its modeled behavior. The effectiveness of the Reactive Pushing Strategy was initially evaluated in the simulation environment, where it significantly outperformed the compared baseline approaches. Following this, we validated the proposed strategy through real-world experiments, demonstrating the robot capability to push objects to the target points located in the entire vicinity of the robot. In both simulation and real-world experiments, the object-specific properties (shape, mass, friction, inertia) were altered along with the changes in target locations to assess the robustness of the proposed method comprehensively.","sentences":["For mobile robots, navigating cluttered or dynamic environments often necessitates non-prehensile manipulation, particularly when faced with objects that are too large, irregular, or fragile to grasp.","The unpredictable behavior and varying physical properties of these objects significantly complicate manipulation tasks.","To address this challenge, this manuscript proposes a novel Reactive Pushing Strategy.","This strategy allows a mobile robot to dynamically adjust its base movements in real-time to achieve successful pushing maneuvers towards a target location.","Notably, our strategy adapts the robot motion based on changes in contact location obtained through the tactile sensor covering the base, avoiding dependence on object-related assumptions and its modeled behavior.","The effectiveness of the Reactive Pushing Strategy was initially evaluated in the simulation environment, where it significantly outperformed the compared baseline approaches.","Following this, we validated the proposed strategy through real-world experiments, demonstrating the robot capability to push objects to the target points located in the entire vicinity of the robot.","In both simulation and real-world experiments, the object-specific properties (shape, mass, friction, inertia) were altered along with the changes in target locations to assess the robustness of the proposed method comprehensively."],"url":"http://arxiv.org/abs/2403.09305v1","category":"cs.RO"}
{"created":"2024-03-14 11:54:25","title":"The Existential Closedness and Zilber-Pink Conjectures","abstract":"In this paper we survey the history of, and recent developments on, two major conjectures originating in Zilber's model-theoretic work on complex exponentiation -- Existential Closedness and Zilber-Pink. The main focus is on the modular versions of these conjectures and specifically on novel variants incorporating the derivatives of modular functions. The functional analogues of all the conjectures that we consider are theorems which are presented too. The paper also contains some new results and conjectures.","sentences":["In this paper we survey the history of, and recent developments on, two major conjectures originating in Zilber's model-theoretic work on complex exponentiation -- Existential Closedness and Zilber-Pink.","The main focus is on the modular versions of these conjectures and specifically on novel variants incorporating the derivatives of modular functions.","The functional analogues of all the conjectures that we consider are theorems which are presented too.","The paper also contains some new results and conjectures."],"url":"http://arxiv.org/abs/2403.09304v1","category":"math.LO"}
{"created":"2024-03-14 11:46:25","title":"Recursive Causal Discovery","abstract":"Causal discovery, i.e., learning the causal graph from data, is often the first step toward the identification and estimation of causal effects, a key requirement in numerous scientific domains. Causal discovery is hampered by two main challenges: limited data results in errors in statistical testing and the computational complexity of the learning task is daunting. This paper builds upon and extends four of our prior publications (Mokhtarian et al., 2021; Akbari et al., 2021; Mokhtarian et al., 2022, 2023a). These works introduced the concept of removable variables, which are the only variables that can be removed recursively for the purpose of causal discovery. Presence and identification of removable variables allow recursive approaches for causal discovery, a promising solution that helps to address the aforementioned challenges by reducing the problem size successively. This reduction not only minimizes conditioning sets in each conditional independence (CI) test, leading to fewer errors but also significantly decreases the number of required CI tests. The worst-case performances of these methods nearly match the lower bound. In this paper, we present a unified framework for the proposed algorithms, refined with additional details and enhancements for a coherent presentation. A comprehensive literature review is also included, comparing the computational complexity of our methods with existing approaches, showcasing their state-of-the-art efficiency. Another contribution of this paper is the release of RCD, a Python package that efficiently implements these algorithms. This package is designed for practitioners and researchers interested in applying these methods in practical scenarios. The package is available at github.com/ban-epfl/rcd, with comprehensive documentation provided at rcdpackage.com.","sentences":["Causal discovery, i.e., learning the causal graph from data, is often the first step toward the identification and estimation of causal effects, a key requirement in numerous scientific domains.","Causal discovery is hampered by two main challenges: limited data results in errors in statistical testing and the computational complexity of the learning task is daunting.","This paper builds upon and extends four of our prior publications (Mokhtarian et al., 2021; Akbari et al., 2021; Mokhtarian et al., 2022, 2023a).","These works introduced the concept of removable variables, which are the only variables that can be removed recursively for the purpose of causal discovery.","Presence and identification of removable variables allow recursive approaches for causal discovery, a promising solution that helps to address the aforementioned challenges by reducing the problem size successively.","This reduction not only minimizes conditioning sets in each conditional independence (CI) test, leading to fewer errors but also significantly decreases the number of required CI tests.","The worst-case performances of these methods nearly match the lower bound.","In this paper, we present a unified framework for the proposed algorithms, refined with additional details and enhancements for a coherent presentation.","A comprehensive literature review is also included, comparing the computational complexity of our methods with existing approaches, showcasing their state-of-the-art efficiency.","Another contribution of this paper is the release of RCD, a Python package that efficiently implements these algorithms.","This package is designed for practitioners and researchers interested in applying these methods in practical scenarios.","The package is available at github.com/ban-epfl/rcd, with comprehensive documentation provided at rcdpackage.com."],"url":"http://arxiv.org/abs/2403.09300v1","category":"cs.LG"}
{"created":"2024-03-14 11:37:02","title":"More than words: Advancements and challenges in speech recognition for singing","abstract":"This paper addresses the challenges and advancements in speech recognition for singing, a domain distinctly different from standard speech recognition. Singing encompasses unique challenges, including extensive pitch variations, diverse vocal styles, and background music interference. We explore key areas such as phoneme recognition, language identification in songs, keyword spotting, and full lyrics transcription. I will describe some of my own experiences when performing research on these tasks just as they were starting to gain traction, but will also show how recent developments in deep learning and large-scale datasets have propelled progress in this field. My goal is to illuminate the complexities of applying speech recognition to singing, evaluate current capabilities, and outline future research directions.","sentences":["This paper addresses the challenges and advancements in speech recognition for singing, a domain distinctly different from standard speech recognition.","Singing encompasses unique challenges, including extensive pitch variations, diverse vocal styles, and background music interference.","We explore key areas such as phoneme recognition, language identification in songs, keyword spotting, and full lyrics transcription.","I will describe some of my own experiences when performing research on these tasks just as they were starting to gain traction, but will also show how recent developments in deep learning and large-scale datasets have propelled progress in this field.","My goal is to illuminate the complexities of applying speech recognition to singing, evaluate current capabilities, and outline future research directions."],"url":"http://arxiv.org/abs/2403.09298v1","category":"cs.SD"}
{"created":"2024-03-14 11:03:22","title":"Whittle Index Based User Association in Dense Millimeter Wave Networks","abstract":"We address the problem of user association in a dense millimeter wave (mmWave) network, in which each arriving user brings a file containing a random number of packets and each time slot is divided into multiple mini-slots. This problem is an instance of the restless multi-armed bandit problem, and is provably hard to solve. Using a technique introduced by Whittle, we relax the hard per-stage constraint that each arriving user must be associated with exactly one mmWave base station (mBS) to a long-term constraint and then use the Lagrangian multiplier technique to convert the problem into an unconstrained problem. This decouples the process governing the system into separate Markov Decision Processes at different mBSs. We prove that the problem is Whittle indexable, present a scheme for computing the Whittle indices of different mBSs, and propose an association scheme under which, each arriving user is associated with the mBS with the smallest value of the Whittle index. Using extensive simulations, we show that the proposed Whittle index based scheme outperforms several user association schemes proposed in prior work in terms of various performance metrics such as average cost, delay, throughput, and Jain's fairness index.","sentences":["We address the problem of user association in a dense millimeter wave (mmWave) network, in which each arriving user brings a file containing a random number of packets and each time slot is divided into multiple mini-slots.","This problem is an instance of the restless multi-armed bandit problem, and is provably hard to solve.","Using a technique introduced by Whittle, we relax the hard per-stage constraint that each arriving user must be associated with exactly one mmWave base station (mBS) to a long-term constraint and then use the Lagrangian multiplier technique to convert the problem into an unconstrained problem.","This decouples the process governing the system into separate Markov Decision Processes at different mBSs.","We prove that the problem is Whittle indexable, present a scheme for computing the Whittle indices of different mBSs, and propose an association scheme under which, each arriving user is associated with the mBS with the smallest value of the Whittle index.","Using extensive simulations, we show that the proposed Whittle index based scheme outperforms several user association schemes proposed in prior work in terms of various performance metrics such as average cost, delay, throughput, and Jain's fairness index."],"url":"http://arxiv.org/abs/2403.09279v1","category":"cs.NI"}
{"created":"2024-03-14 10:57:27","title":"Rational Gluing in Edge Replacement Systems","abstract":"In this paper, we prove the rationality of the gluing relation of edge replacement systems, which were introduced for studying rearrangement groups of fractals. More precisely, we describe an algorithmic procedure for building a finite state automaton that recognizes pairs or equivalent sequences that are glued in the fractal. This fits in recent interest towards the rationality of gluing relations on totally disconnected compact metrizable spaces.","sentences":["In this paper, we prove the rationality of the gluing relation of edge replacement systems, which were introduced for studying rearrangement groups of fractals.","More precisely, we describe an algorithmic procedure for building a finite state automaton that recognizes pairs or equivalent sequences that are glued in the fractal.","This fits in recent interest towards the rationality of gluing relations on totally disconnected compact metrizable spaces."],"url":"http://arxiv.org/abs/2403.09276v1","category":"math.GR"}
{"created":"2024-03-14 10:38:34","title":"A noncommutative maximal inequality for Fej\u00e9r means on totally disconnected non-abelian groups","abstract":"In this paper, we explore Fourier analysis for noncommutative $L_p$ space-valued functions on $G$, where $G$ is a totally disconnected non-abelian compact group. By additionally assuming that the value of these functions remains invariant within each conjugacy class, we establish a noncommutative maximal inequality for Fej\\'er means utilizing the associated character system of $G$. This is an operator-valued version of the classical result due to G\\'at. We follow essentially the classical sketch, but due to the noncommutativity, many classical arguments have to be revised. Notably, compared to the classical results. the bounds of our estimates are explicity calculated.","sentences":["In this paper, we explore Fourier analysis for noncommutative $L_p$ space-valued functions on $G$, where $G$ is a totally disconnected non-abelian compact group.","By additionally assuming that the value of these functions remains invariant within each conjugacy class, we establish a noncommutative maximal inequality for Fej\\'er means utilizing the associated character system of $G$.","This is an operator-valued version of the classical result due to G\\'at.","We follow essentially the classical sketch, but due to the noncommutativity, many classical arguments have to be revised.","Notably, compared to the classical results.","the bounds of our estimates are explicity calculated."],"url":"http://arxiv.org/abs/2403.09263v1","category":"math.FA"}
{"created":"2024-03-14 10:30:58","title":"Near-Field EM-Based Multistatic Radar Range Estimation","abstract":"Radar targets are traditionally modelled as point target reflectors, even in the near-field region. Yet, for radar systems operating at high carrier frequencies and small distances, traditional radar propagation models do not accurately model the scatterer responses. In this paper, a novel electromagnetic-based model is thus developed for the multistatic radar detection of a rectangular plate reflector in the near-field region. This model is applied to an automotive scenario, in which a linear antenna array is spread out at the front of a vehicle, and performs a radar measurement of the distance to the back of the vehicle ahead. Based on the developed received signal model, the maximum likelihood estimator of the range is designed. By exploiting the near-field target model, this estimator is shown to provide a significant gain with respect to traditional range estimators. The impact of the system and scenario parameters, i.e. the carrier frequency, bandwidth and distance to the target, is furthermore evaluated. This analysis shows that the radar resolution in the near-field regime is improved at high carrier frequencies, while saturating to the traditional bandwidth-dependent resolution in the far-field region.","sentences":["Radar targets are traditionally modelled as point target reflectors, even in the near-field region.","Yet, for radar systems operating at high carrier frequencies and small distances, traditional radar propagation models do not accurately model the scatterer responses.","In this paper, a novel electromagnetic-based model is thus developed for the multistatic radar detection of a rectangular plate reflector in the near-field region.","This model is applied to an automotive scenario, in which a linear antenna array is spread out at the front of a vehicle, and performs a radar measurement of the distance to the back of the vehicle ahead.","Based on the developed received signal model, the maximum likelihood estimator of the range is designed.","By exploiting the near-field target model, this estimator is shown to provide a significant gain with respect to traditional range estimators.","The impact of the system and scenario parameters, i.e. the carrier frequency, bandwidth and distance to the target, is furthermore evaluated.","This analysis shows that the radar resolution in the near-field regime is improved at high carrier frequencies, while saturating to the traditional bandwidth-dependent resolution in the far-field region."],"url":"http://arxiv.org/abs/2403.09258v1","category":"eess.SP"}
{"created":"2024-03-14 10:27:38","title":"A Modified da Vinci Surgical Instrument for OCE based Elasticity Estimation with Deep Learning","abstract":"Robot-assisted surgery has advantages compared to conventional laparoscopic procedures, e.g., precise movement of the surgical instruments, improved dexterity, and high-resolution visualization of the surgical field. However, mechanical tissue properties may provide additional information, e.g., on the location of lesions or vessels. While elastographic imaging has been proposed, it is not readily available as an online modality during robot-assisted surgery. We propose modifying a da~Vinci surgical instrument to realize optical coherence elastography (OCE) for quantitative elasticity estimation. The modified da~Vinci instrument is equipped with piezoelectric elements for shear wave excitation and we employ fast optical coherence tomography (OCT) imaging to track propagating wave fields, which are directly related to biomechanical tissue properties. All high-voltage components are mounted at the proximal end outside the patient. We demonstrate that external excitation at the instrument shaft can effectively stimulate shear waves, even when considering damping. Comparing conventional and deep learning-based signal processing, resulting in mean absolute errors of 19.27 kPa and 6.29 kPa, respectively. These results illustrate that precise quantitative elasticity estimates can be obtained. We also demonstrate quantitative elasticity estimation on ex-vivo tissue samples of heart, liver and stomach, and show that the measurements can be used to distinguish soft and stiff tissue types.","sentences":["Robot-assisted surgery has advantages compared to conventional laparoscopic procedures, e.g., precise movement of the surgical instruments, improved dexterity, and high-resolution visualization of the surgical field.","However, mechanical tissue properties may provide additional information, e.g., on the location of lesions or vessels.","While elastographic imaging has been proposed, it is not readily available as an online modality during robot-assisted surgery.","We propose modifying a da~Vinci surgical instrument to realize optical coherence elastography (OCE) for quantitative elasticity estimation.","The modified da~Vinci instrument is equipped with piezoelectric elements for shear wave excitation and we employ fast optical coherence tomography (OCT) imaging to track propagating wave fields, which are directly related to biomechanical tissue properties.","All high-voltage components are mounted at the proximal end outside the patient.","We demonstrate that external excitation at the instrument shaft can effectively stimulate shear waves, even when considering damping.","Comparing conventional and deep learning-based signal processing, resulting in mean absolute errors of 19.27 kPa and 6.29 kPa, respectively.","These results illustrate that precise quantitative elasticity estimates can be obtained.","We also demonstrate quantitative elasticity estimation on ex-vivo tissue samples of heart, liver and stomach, and show that the measurements can be used to distinguish soft and stiff tissue types."],"url":"http://arxiv.org/abs/2403.09256v1","category":"eess.IV"}
{"created":"2024-03-14 10:27:14","title":"Quantum analog of Landau-Lifshitz-Gilbert dynamics","abstract":"The Landau-Lifshitz-Gilbert (LLG) and Landau-Lifshitz (LL) equations play an essential role for describing the dynamics of magnetization in solids. While a quantum analog of the LL dynamics has been proposed in [Phys. Rev. Lett. 110, 147201 (2013)], the corresponding quantum version of LLG remains unknown. Here, we propose such a quantum LLG equation that inherently conserves purity of the quantum state. We examine the quantum LLG dynamics of a dimer consisting of two interacting spin-1/2 particles. Our analysis reveals that, in the case of ferromagnetic coupling, the evolution of initially uncorrelated spins mirrors the classical LLG dynamics. However, in the antiferromagnetic scenario, we observe pronounced deviations from classical behavior, underscoring the unique dynamics of becoming a spinless state, which is non-locally correlated. Moreover, when considering spins that are initially correlated, our study uncovers an unusual form of transient quantum correlation dynamics, which differ significantly from what is typically seen in open quantum systems.","sentences":["The Landau-Lifshitz-Gilbert (LLG) and Landau-Lifshitz (LL) equations play an essential role for describing the dynamics of magnetization in solids.","While a quantum analog of the LL dynamics has been proposed in [Phys. Rev. Lett.","110, 147201 (2013)], the corresponding quantum version of LLG remains unknown.","Here, we propose such a quantum LLG equation that inherently conserves purity of the quantum state.","We examine the quantum LLG dynamics of a dimer consisting of two interacting spin-1/2 particles.","Our analysis reveals that, in the case of ferromagnetic coupling, the evolution of initially uncorrelated spins mirrors the classical LLG dynamics.","However, in the antiferromagnetic scenario, we observe pronounced deviations from classical behavior, underscoring the unique dynamics of becoming a spinless state, which is non-locally correlated.","Moreover, when considering spins that are initially correlated, our study uncovers an unusual form of transient quantum correlation dynamics, which differ significantly from what is typically seen in open quantum systems."],"url":"http://arxiv.org/abs/2403.09255v1","category":"quant-ph"}
{"created":"2024-03-14 10:10:54","title":"Experimental study of dynamic wetting behavior through curved microchannels with automated image analysis","abstract":"Preventing fluid penetration poses a challenging reliability concern in the context of power electronics, which is usually caused by unforeseen microfractures along the sealing joints. A better and more reliable product design heavily depends on the understanding of the dynamic wetting processes happening inside these complex microfractures, i.e. microchannels. A novel automated image processing procedure is proposed in this work for analyzing the moving interface and the dynamic contact angle in microchannels. In particular, the developed method is advantageous for experiments involving non-transparent samples, where extracting the fluid interface geometry poses a significant challenge. The developed method is validated with theoretical values and manual measurements and exhibits high accuracy. The implementation is made publicly available. The developed method is validated and applied to experimental investigations of forced wetting with two working fluids (water and 50 wt% glycerin/water mixture) in four distinct microchannels characterized by different dimensions and curvature. The comparison between the experimental results and molecular kinetic theory (MKT) reveals that the dynamic wetting behavior can be described well by MKT, even in highly curved microchannels. The dynamic wetting behavior shows a strong dependency on the channel geometry and curvature.","sentences":["Preventing fluid penetration poses a challenging reliability concern in the context of power electronics, which is usually caused by unforeseen microfractures along the sealing joints.","A better and more reliable product design heavily depends on the understanding of the dynamic wetting processes happening inside these complex microfractures, i.e. microchannels.","A novel automated image processing procedure is proposed in this work for analyzing the moving interface and the dynamic contact angle in microchannels.","In particular, the developed method is advantageous for experiments involving non-transparent samples, where extracting the fluid interface geometry poses a significant challenge.","The developed method is validated with theoretical values and manual measurements and exhibits high accuracy.","The implementation is made publicly available.","The developed method is validated and applied to experimental investigations of forced wetting with two working fluids (water and 50 wt% glycerin/water mixture) in four distinct microchannels characterized by different dimensions and curvature.","The comparison between the experimental results and molecular kinetic theory (MKT) reveals that the dynamic wetting behavior can be described well by MKT, even in highly curved microchannels.","The dynamic wetting behavior shows a strong dependency on the channel geometry and curvature."],"url":"http://arxiv.org/abs/2403.09246v1","category":"physics.flu-dyn"}
{"created":"2024-03-14 09:57:15","title":"D-YOLO a robust framework for object detection in adverse weather conditions","abstract":"Adverse weather conditions including haze, snow and rain lead to decline in image qualities, which often causes a decline in performance for deep-learning based detection networks. Most existing approaches attempts to rectify hazy images before performing object detection, which increases the complexity of the network and may result in the loss in latent information. To better integrate image restoration and object detection tasks, we designed a double-route network with an attention feature fusion module, taking both hazy and dehazed features into consideration. We also proposed a subnetwork to provide haze-free features to the detection network. Specifically, our D-YOLO improves the performance of the detection network by minimizing the distance between the clear feature extraction subnetwork and detection network. Experiments on RTTS and FoggyCityscapes datasets show that D-YOLO demonstrates better performance compared to the state-of-the-art methods. It is a robust detection framework for bridging the gap between low-level dehazing and high-level detection.","sentences":["Adverse weather conditions including haze, snow and rain lead to decline in image qualities, which often causes a decline in performance for deep-learning based detection networks.","Most existing approaches attempts to rectify hazy images before performing object detection, which increases the complexity of the network and may result in the loss in latent information.","To better integrate image restoration and object detection tasks, we designed a double-route network with an attention feature fusion module, taking both hazy and dehazed features into consideration.","We also proposed a subnetwork to provide haze-free features to the detection network.","Specifically, our D-YOLO improves the performance of the detection network by minimizing the distance between the clear feature extraction subnetwork and detection network.","Experiments on RTTS and FoggyCityscapes datasets show that D-YOLO demonstrates better performance compared to the state-of-the-art methods.","It is a robust detection framework for bridging the gap between low-level dehazing and high-level detection."],"url":"http://arxiv.org/abs/2403.09233v1","category":"cs.CV"}
{"created":"2024-03-14 09:39:18","title":"A Robust Semantic Communication System for Image","abstract":"Semantic communications have gained significant attention as a promising approach to address the transmission bottleneck, especially with the continuous development of 6G techniques. Distinct from the well investigated physical channel impairments, this paper focuses on semantic impairments in image, particularly those arising from adversarial perturbations. Specifically, we propose a novel metric for quantifying the intensity of semantic impairment and develop a semantic impairment dataset. Furthermore, we introduce a deep learning enabled semantic communication system, termed as DeepSC-RI, to enhance the robustness of image transmission, which incorporates a multi-scale semantic extractor with a dual-branch architecture for extracting semantics with varying granularity, thereby improving the robustness of the system. The fine-grained branch incorporates a semantic importance evaluation module to identify and prioritize crucial semantics, while the coarse-grained branch adopts a hierarchical approach for capturing the robust semantics. These two streams of semantics are seamlessly integrated via an advanced cross-attention-based semantic fusion module. Experimental results demonstrate the superior performance of DeepSC-RI under various levels of semantic impairment intensity.","sentences":["Semantic communications have gained significant attention as a promising approach to address the transmission bottleneck, especially with the continuous development of 6G techniques.","Distinct from the well investigated physical channel impairments, this paper focuses on semantic impairments in image, particularly those arising from adversarial perturbations.","Specifically, we propose a novel metric for quantifying the intensity of semantic impairment and develop a semantic impairment dataset.","Furthermore, we introduce a deep learning enabled semantic communication system, termed as DeepSC-RI, to enhance the robustness of image transmission, which incorporates a multi-scale semantic extractor with a dual-branch architecture for extracting semantics with varying granularity, thereby improving the robustness of the system.","The fine-grained branch incorporates a semantic importance evaluation module to identify and prioritize crucial semantics, while the coarse-grained branch adopts a hierarchical approach for capturing the robust semantics.","These two streams of semantics are seamlessly integrated via an advanced cross-attention-based semantic fusion module.","Experimental results demonstrate the superior performance of DeepSC-RI under various levels of semantic impairment intensity."],"url":"http://arxiv.org/abs/2403.09222v1","category":"eess.SP"}
{"created":"2024-03-14 09:22:30","title":"MINDS: The JWST MIRI Mid-INfrared Disk Survey","abstract":"The study of protoplanetary disks has become increasingly important with the Kepler satellite finding that exoplanets are ubiquitous around stars in our galaxy and the discovery of enormous diversity in planetary system architectures and planet properties. High-resolution near-IR and ALMA images show strong evidence for ongoing planet formation in young disks. The JWST MIRI mid-INfrared Disk Survey (MINDS) aims to (1) investigate the chemical inventory in the terrestrial planet-forming zone across stellar spectral type, (2) follow the gas evolution into the disk dispersal stage, and (3) study the structure of protoplanetary and debris disks in the thermal mid-IR. The MINDS survey will thus build a bridge between the chemical inventory of disks and the properties of exoplanets. The survey comprises 52 targets (Herbig Ae stars, T Tauri stars, very low-mass stars and young debris disks). We primarily obtain MIRI/MRS spectra with high S/N (~100-500) covering the complete wavelength range from 4.9 to 27.9 {\\mu}m. For a handful of selected targets we also obtain NIRSpec IFU high resolution spectroscopy (2.87-5.27 {\\mu}m). We will search for signposts of planet formation in thermal emission of micron-sized dust - information complementary to near-IR scattered light emission from small dust grains and emission from large dust in the submillimeter wavelength domain. We will also study the spatial structure of disks in three key systems that have shown signposts for planet formation, TW Hya and HD 169142 using the MIRI coronagraph at 15.5 {\\mu}m and 10.65 {\\mu}m respectively and PDS70 using NIRCam imaging in the 1.87 {\\mu}m narrow and the 4.8 {\\mu}m medium band filter. ...","sentences":["The study of protoplanetary disks has become increasingly important with the Kepler satellite finding that exoplanets are ubiquitous around stars in our galaxy and the discovery of enormous diversity in planetary system architectures and planet properties.","High-resolution near-IR and ALMA images show strong evidence for ongoing planet formation in young disks.","The JWST MIRI mid-INfrared Disk Survey (MINDS) aims to (1) investigate the chemical inventory in the terrestrial planet-forming zone across stellar spectral type, (2) follow the gas evolution into the disk dispersal stage, and (3) study the structure of protoplanetary and debris disks in the thermal mid-IR.","The MINDS survey will thus build a bridge between the chemical inventory of disks and the properties of exoplanets.","The survey comprises 52 targets (Herbig Ae stars, T Tauri stars, very low-mass stars and young debris disks).","We primarily obtain MIRI/MRS spectra with high S/N (~100-500) covering the complete wavelength range from 4.9 to 27.9 {\\mu}m.","For a handful of selected targets we also obtain NIRSpec IFU high resolution spectroscopy (2.87-5.27 {\\mu}m).","We will search for signposts of planet formation in thermal emission of micron-sized dust - information complementary to near-IR scattered light emission from small dust grains and emission from large dust in the submillimeter wavelength domain.","We will also study the spatial structure of disks in three key systems that have shown signposts for planet formation, TW Hya and HD 169142 using the MIRI coronagraph at 15.5 {\\mu}m and 10.65 {\\mu}m respectively and PDS70 using NIRCam imaging in the 1.87 {\\mu}m narrow and the 4.8 {\\mu}m medium band filter. ..."],"url":"http://arxiv.org/abs/2403.09210v1","category":"astro-ph.EP"}
{"created":"2024-03-14 09:22:16","title":"Older adults' safety and security online: A post-pandemic exploration of attitudes and behaviors","abstract":"Older adults' growing use of the internet and related technologies, further accelerated by the COVID-19 pandemic, has prompted not only a critical examination of their behaviors and attitudes about online threats but also a greater understanding of the roles of specific characteristics within this population group. Based on survey data and using descriptive and inferential statistics, this empirical study delves into this matter. The behaviors and attitudes of a group of older adults aged 60 years and older (n=275) regarding different dimensions of online safety and cybersecurity are investigated. The results show that older adults report a discernible degree of concern about the security of their personal information. Despite the varied precautions taken, most of them do not know where to report online threats. What is more, regarding key demographics, the study found some significant differences in terms of gender and age group, but not disability status. This implies that older adults do not seem to constitute a homogeneous group when it comes to attitudes and behaviors regarding safety and security online. The study concludes that support systems should include older adults in the development of protective measures and acknowledge their diversity. The implications of the results are discussed and some directions for future research are proposed.","sentences":["Older adults' growing use of the internet and related technologies, further accelerated by the COVID-19 pandemic, has prompted not only a critical examination of their behaviors and attitudes about online threats but also a greater understanding of the roles of specific characteristics within this population group.","Based on survey data and using descriptive and inferential statistics, this empirical study delves into this matter.","The behaviors and attitudes of a group of older adults aged 60 years and older (n=275) regarding different dimensions of online safety and cybersecurity are investigated.","The results show that older adults report a discernible degree of concern about the security of their personal information.","Despite the varied precautions taken, most of them do not know where to report online threats.","What is more, regarding key demographics, the study found some significant differences in terms of gender and age group, but not disability status.","This implies that older adults do not seem to constitute a homogeneous group when it comes to attitudes and behaviors regarding safety and security online.","The study concludes that support systems should include older adults in the development of protective measures and acknowledge their diversity.","The implications of the results are discussed and some directions for future research are proposed."],"url":"http://arxiv.org/abs/2403.09208v1","category":"cs.CY"}
{"created":"2024-03-14 09:19:35","title":"Microscopic Study on Superexchange Dynamics of Composite Spin-1 Bosons","abstract":"We report on an experimental simulation of the spin-1 Heisenberg model with composite bosons in a one-dimensional chain based on the two-component Bose-Hubbard model. Exploiting our site-and spin-resolved quantum gas microscope, we observed faster superexchange dynamics of the spin-1 system compared to its spin-1/2 counterpart, which is attributed to the enhancement effect of multi-bosons. We further probed the non-equilibrium spin dynamics driven by the superexchange and single-ion anisotropy terms, unveiling the linear expansion of the spin-spin correlations, which is limited by the Lieb-Robinson bound. Based on the superexchange process, we prepared and verified the entangled qutrits pairs with these composite spin-1 bosons, potentially being applied in qutrit-based quantum information processing.","sentences":["We report on an experimental simulation of the spin-1 Heisenberg model with composite bosons in a one-dimensional chain based on the two-component Bose-Hubbard model.","Exploiting our site-and spin-resolved quantum gas microscope, we observed faster superexchange dynamics of the spin-1 system compared to its spin-1/2 counterpart, which is attributed to the enhancement effect of multi-bosons.","We further probed the non-equilibrium spin dynamics driven by the superexchange and single-ion anisotropy terms, unveiling the linear expansion of the spin-spin correlations, which is limited by the Lieb-Robinson bound.","Based on the superexchange process, we prepared and verified the entangled qutrits pairs with these composite spin-1 bosons, potentially being applied in qutrit-based quantum information processing."],"url":"http://arxiv.org/abs/2403.09205v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-14 09:11:23","title":"Sparse Data Structures for Efficient State-to-State Kinetic Simulations","abstract":"Higher-fidelity entry simulations can be enabled by integrating finer thermo-chemistry models into compressible flow physics. One such class of models are State-to-State (StS) kinetics, which explicitly track species populations among quantum energy levels. StS models can represent thermo-chemical non-equilibrium effects that are hardly captured by standard multi-temperature models. However, the associated increase in computational cost is dramatic. For implicit solution techniques that rely on standard block-sparse representations of the Jacobian, both the spatial complexity and the temporal complexity grow quadratically with respect to the number of quantum levels represented. We introduce a more efficient way to represent the Jacobian arising in first-order implicit simulations for compressible flow physics coupled with StS models. The key idea is to recognize that the density of local blocks of the Jacobian comes from rank-one updates that can be managed separately. This leads to a new Jacobian structure, consisting of a fully-sparse matrix and block-wise rank-one updates, whose overall complexity grows linearly with the number of quantum levels. This structure also brings forth a potentially faster variation of the block-Jacobi preconditioning algorithm by leveraging the Sherman-Morrison-Woodbury inversion formula.","sentences":["Higher-fidelity entry simulations can be enabled by integrating finer thermo-chemistry models into compressible flow physics.","One such class of models are State-to-State (StS) kinetics, which explicitly track species populations among quantum energy levels.","StS models can represent thermo-chemical non-equilibrium effects that are hardly captured by standard multi-temperature models.","However, the associated increase in computational cost is dramatic.","For implicit solution techniques that rely on standard block-sparse representations of the Jacobian, both the spatial complexity and the temporal complexity grow quadratically with respect to the number of quantum levels represented.","We introduce a more efficient way to represent the Jacobian arising in first-order implicit simulations for compressible flow physics coupled with StS models.","The key idea is to recognize that the density of local blocks of the Jacobian comes from rank-one updates that can be managed separately.","This leads to a new Jacobian structure, consisting of a fully-sparse matrix and block-wise rank-one updates, whose overall complexity grows linearly with the number of quantum levels.","This structure also brings forth a potentially faster variation of the block-Jacobi preconditioning algorithm by leveraging the Sherman-Morrison-Woodbury inversion formula."],"url":"http://arxiv.org/abs/2403.09198v1","category":"physics.comp-ph"}
{"created":"2024-03-14 09:05:55","title":"Superintegrable systems on conformal surfaces","abstract":"We reconsider non-degenerate second order superintegrable systems in dimension two as geometric structures on conformal surfaces. This extends a formalism developed by the authors, initially introduced for (pseudo-)Riemannian manifolds of dimension three and higher. The governing equations of non-degenerate second order superintegrability in dimension two are structurally significantly different from those valid in higher dimensions. Specifically, we find conformally covariant structural equations, allowing one to classify the (conformal classes of) non-degenerate second order superintegrable systems on conformal surfaces geometrically. We then specialise to second order properly superintegrable systems on surfaces with a (pseudo-)Riemannian metric and obtain structural equations in accordance with the known equations for Euclidean space. We finally give a single explicit set of purely algebraic equations defining the variety parametrising such systems on all constant curvature surfaces.","sentences":["We reconsider non-degenerate second order superintegrable systems in dimension two as geometric structures on conformal surfaces.","This extends a formalism developed by the authors, initially introduced for (pseudo-)Riemannian manifolds of dimension three and higher.","The governing equations of non-degenerate second order superintegrability in dimension two are structurally significantly different from those valid in higher dimensions.","Specifically, we find conformally covariant structural equations, allowing one to classify the (conformal classes of) non-degenerate second order superintegrable systems on conformal surfaces geometrically.","We then specialise to second order properly superintegrable systems on surfaces with a (pseudo-)Riemannian metric and obtain structural equations in accordance with the known equations for Euclidean space.","We finally give a single explicit set of purely algebraic equations defining the variety parametrising such systems on all constant curvature surfaces."],"url":"http://arxiv.org/abs/2403.09191v1","category":"math.DG"}
{"created":"2024-03-14 09:04:30","title":"Dynamics of Droplets Moving on Lubricated Polymer Brushes","abstract":"Understanding the dynamics of drops on polymer-coated surfaces is crucial for optimizing applications such as self-cleaning materials or microfluidic devices. While the static and dynamic properties of deposited drops have been well characterised, a microscopic understanding of the underlying dynamics is missing. In particular, it is unclear how drop dynamics depends on the amount of uncrosslinked chains in the brush, because experimental techniques fail to quantify those. Here we use coarse-grained simulations to study droplets moving on a lubricated polymer brush substrate under the influence of an external body force. The simulation model is based on the many body dissipative particle dynamics (mDPD) method and designed to mimic a system of water droplets on polydimethylsiloxane (PDMS) brushes with chemically identical PDMS lubricant. In agreement with experiments, we find a sublinear power law dependence between the external force $F$ and the droplet velocity $v$, $F \\propto v^\\alpha$ with $\\alpha <1$; however, the exponents differ ($\\alpha \\sim 0.6-0.7$ in simulations versus $\\alpha \\sim 0.25$ in experiments). With increasing velocity, the droplets elongate and the receding contact angle decreases, whereas the advancing contact angle remains roughly constant. Analyzing the flow profiles inside the droplet reveals that the droplets do not slide, but roll, with vanishing slip at the substrate surface. Surprisingly, adding lubricant has very little effect on the effective friction force between the droplet and the substrate, even though it has a pronounced effect on the size and structure of the wetting ridge, especially above the cloaking transition.","sentences":["Understanding the dynamics of drops on polymer-coated surfaces is crucial for optimizing applications such as self-cleaning materials or microfluidic devices.","While the static and dynamic properties of deposited drops have been well characterised, a microscopic understanding of the underlying dynamics is missing.","In particular, it is unclear how drop dynamics depends on the amount of uncrosslinked chains in the brush, because experimental techniques fail to quantify those.","Here we use coarse-grained simulations to study droplets moving on a lubricated polymer brush substrate under the influence of an external body force.","The simulation model is based on the many body dissipative particle dynamics (mDPD) method and designed to mimic a system of water droplets on polydimethylsiloxane (PDMS) brushes with chemically identical PDMS lubricant.","In agreement with experiments, we find a sublinear power law dependence between the external force $F$ and the droplet velocity $v$, $F \\propto v^\\alpha$ with $\\alpha <1$; however, the exponents differ ($\\alpha \\sim 0.6-0.7$ in simulations versus $\\alpha \\sim 0.25$ in experiments).","With increasing velocity, the droplets elongate and the receding contact angle decreases, whereas the advancing contact angle remains roughly constant.","Analyzing the flow profiles inside the droplet reveals that the droplets do not slide, but roll, with vanishing slip at the substrate surface.","Surprisingly, adding lubricant has very little effect on the effective friction force between the droplet and the substrate, even though it has a pronounced effect on the size and structure of the wetting ridge, especially above the cloaking transition."],"url":"http://arxiv.org/abs/2403.09189v1","category":"cond-mat.soft"}
{"created":"2024-03-14 09:03:51","title":"Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study","abstract":"Deep learning (DL) models encompass millions or even billions of parameters and learn complex patterns from big data. However, not all data are initially stored in a suitable formation to effectively train a DL model, e.g., gas chromatography-mass spectrometry (GC-MS) spectra and DNA sequence. These datasets commonly contain many zero values, and the sparse data formation causes difficulties in optimizing DL models. A DL module called the basis-projected layer (BPL) was proposed to mitigate the issue by transforming the sparse data into a dense representation. The transformed data is expected to facilitate the gradient calculation and finetuned process in a DL training process. The dataset, example of a sparse dataset, contained 362 specialty coffee odorant spectra detected from GC-MS. The BPL layer was placed at the beginning of the DL model. The tunable parameters in the layer were learnable projected axes that were the bases of a new representation space. The layer rotated these bases when its parameters were updated. When the number of the bases was the same as the original dimension, the increasing percentage of the F1 scores was 8.56%. Furthermore, when the number was set as 768 (the original dimension was 490), the increasing percentage of the F1 score was 11.49%. The layer not only maintained the model performance and even constructed a better representation space in analyzing sparse datasets.","sentences":["Deep learning (DL) models encompass millions or even billions of parameters and learn complex patterns from big data.","However, not all data are initially stored in a suitable formation to effectively train a DL model, e.g., gas chromatography-mass spectrometry (GC-MS) spectra and DNA sequence.","These datasets commonly contain many zero values, and the sparse data formation causes difficulties in optimizing DL models.","A DL module called the basis-projected layer (BPL) was proposed to mitigate the issue by transforming the sparse data into a dense representation.","The transformed data is expected to facilitate the gradient calculation and finetuned process in a DL training process.","The dataset, example of a sparse dataset, contained 362 specialty coffee odorant spectra detected from GC-MS.","The BPL layer was placed at the beginning of the DL model.","The tunable parameters in the layer were learnable projected axes that were the bases of a new representation space.","The layer rotated these bases when its parameters were updated.","When the number of the bases was the same as the original dimension, the increasing percentage of the F1 scores was 8.56%.","Furthermore, when the number was set as 768 (the original dimension was 490), the increasing percentage of the F1 score was 11.49%.","The layer not only maintained the model performance and even constructed a better representation space in analyzing sparse datasets."],"url":"http://arxiv.org/abs/2403.09188v1","category":"cs.LG"}
{"created":"2024-03-14 08:49:18","title":"On the dynamical Mordell-Lang conjecture in positive characteristic","abstract":"We prove that the dynamical Mordell-Lang conjecture in positive characteristic holds for bounded-degree self-maps of projective varieties. The key ingredient of the proof is a Mordell-Lang-type result for arbitrary algebraic groups over algebraically closed fields of positive characteristic, which is also interesting on its own. Moreover, we propose a geometric version of dynamical Mordell-Lang conjecture in positive characteristic.","sentences":["We prove that the dynamical Mordell-Lang conjecture in positive characteristic holds for bounded-degree self-maps of projective varieties.","The key ingredient of the proof is a Mordell-Lang-type result for arbitrary algebraic groups over algebraically closed fields of positive characteristic, which is also interesting on its own.","Moreover, we propose a geometric version of dynamical Mordell-Lang conjecture in positive characteristic."],"url":"http://arxiv.org/abs/2403.09181v1","category":"math.DS"}
{"created":"2024-03-14 08:47:32","title":"Online and Offline Evaluation in Search Clarification","abstract":"The effectiveness of clarification question models in engaging users within search systems is currently constrained, casting doubt on their overall usefulness. To improve the performance of these models, it is crucial to employ assessment approaches that encompass both real-time feedback from users (online evaluation) and the characteristics of clarification questions evaluated through human assessment (offline evaluation). However, the relationship between online and offline evaluations has been debated in information retrieval. This study aims to investigate how this discordance holds in search clarification. We use user engagement as ground truth and employ several offline labels to investigate to what extent the offline ranked lists of clarification resemble the ideal ranked lists based on online user engagement.","sentences":["The effectiveness of clarification question models in engaging users within search systems is currently constrained, casting doubt on their overall usefulness.","To improve the performance of these models, it is crucial to employ assessment approaches that encompass both real-time feedback from users (online evaluation) and the characteristics of clarification questions evaluated through human assessment (offline evaluation).","However, the relationship between online and offline evaluations has been debated in information retrieval.","This study aims to investigate how this discordance holds in search clarification.","We use user engagement as ground truth and employ several offline labels to investigate to what extent the offline ranked lists of clarification resemble the ideal ranked lists based on online user engagement."],"url":"http://arxiv.org/abs/2403.09180v1","category":"cs.IR"}
{"created":"2024-03-14 08:44:43","title":"High-order numerical integration on regular embedded surfaces","abstract":"We present a high-order surface quadrature (HOSQ) for accurately approximating regular surface integrals on closed surfaces. The initial step of our approach rests on exploiting square-squeezing--a homeomorphic bilinear square-simplex transformation, re-parametrizing any surface triangulation to a quadrilateral mesh. For each resulting quadrilateral domain we interpolate the geometry by tensor polynomials in Chebyshev--Lobatto grids. Posterior the tensor-product Clenshaw-Curtis quadrature is applied to compute the resulting integral. We demonstrate efficiency, fast runtime performance, high-order accuracy, and robustness for complex geometries.","sentences":["We present a high-order surface quadrature (HOSQ) for accurately approximating regular surface integrals on closed surfaces.","The initial step of our approach rests on exploiting square-squeezing--a homeomorphic bilinear square-simplex transformation, re-parametrizing any surface triangulation to a quadrilateral mesh.","For each resulting quadrilateral domain we interpolate the geometry by tensor polynomials in Chebyshev--Lobatto grids.","Posterior the tensor-product Clenshaw-Curtis quadrature is applied to compute the resulting integral.","We demonstrate efficiency, fast runtime performance, high-order accuracy, and robustness for complex geometries."],"url":"http://arxiv.org/abs/2403.09178v1","category":"math.NA"}
{"created":"2024-03-14 08:44:15","title":"Cellular-enabled Collaborative Robots Planning and Operations for Search-and-Rescue Scenarios","abstract":"Mission-critical operations, particularly in the context of Search-and-Rescue (SAR) and emergency response situations, demand optimal performance and efficiency from every component involved to maximize the success probability of such operations. In these settings, cellular-enabled collaborative robotic systems have emerged as invaluable assets, assisting first responders in several tasks, ranging from victim localization to hazardous area exploration. However, a critical limitation in the deployment of cellular-enabled collaborative robots in SAR missions is their energy budget, primarily supplied by batteries, which directly impacts their task execution and mobility. This paper tackles this problem, and proposes a search-and-rescue framework for cellular-enabled collaborative robots use cases that, taking as input the area size to be explored, the robots fleet size, their energy profile, exploration rate required and target response time, finds the minimum number of robots able to meet the SAR mission goals and the path they should follow to explore the area. Our results, i) show that first responders can rely on a SAR cellular-enabled robotics framework when planning mission-critical operations to take informed decisions with limited resources, and, ii) illustrate the number of robots versus explored area and response time trade-off depending on the type of robot: wheeled vs quadruped.","sentences":["Mission-critical operations, particularly in the context of Search-and-Rescue (SAR) and emergency response situations, demand optimal performance and efficiency from every component involved to maximize the success probability of such operations.","In these settings, cellular-enabled collaborative robotic systems have emerged as invaluable assets, assisting first responders in several tasks, ranging from victim localization to hazardous area exploration.","However, a critical limitation in the deployment of cellular-enabled collaborative robots in SAR missions is their energy budget, primarily supplied by batteries, which directly impacts their task execution and mobility.","This paper tackles this problem, and proposes a search-and-rescue framework for cellular-enabled collaborative robots use cases that, taking as input the area size to be explored, the robots fleet size, their energy profile, exploration rate required and target response time, finds the minimum number of robots able to meet the SAR mission goals and the path they should follow to explore the area.","Our results, i) show that first responders can rely on a SAR cellular-enabled robotics framework when planning mission-critical operations to take informed decisions with limited resources, and, ii) illustrate the number of robots versus explored area and response time trade-off depending on the type of robot: wheeled vs quadruped."],"url":"http://arxiv.org/abs/2403.09177v1","category":"cs.RO"}
{"created":"2024-03-14 08:40:30","title":"Bridging Quantum Computing and Differential Privacy: A Survey on Quantum Computing Privacy","abstract":"Quantum computing has attracted significant attention in areas such as cryptography, cybersecurity, and drug discovery. Due to the advantage of parallel processing, quantum computing can speed up the response to complex challenges and the processing of large-scale datasets. However, since quantum computing usually requires sensitive datasets, privacy breaches have become a vital concern. Differential privacy (DP) is a promising privacy-preserving method in classical computing and has been extended to the quantum domain in recent years. In this survey, we categorize the existing literature based on whether internal inherent noise or external artificial noise is used as a source to achieve DP in quantum computing. We explore how these approaches are applied at different stages of a quantum algorithm (i.e., state preparation, quantum circuit, and quantum measurement). We also discuss challenges and future directions for DP in quantum computing. By summarizing recent advancements, we hope to provide a comprehensive, up-to-date overview for researchers venturing into this field.","sentences":["Quantum computing has attracted significant attention in areas such as cryptography, cybersecurity, and drug discovery.","Due to the advantage of parallel processing, quantum computing can speed up the response to complex challenges and the processing of large-scale datasets.","However, since quantum computing usually requires sensitive datasets, privacy breaches have become a vital concern.","Differential privacy (DP) is a promising privacy-preserving method in classical computing and has been extended to the quantum domain in recent years.","In this survey, we categorize the existing literature based on whether internal inherent noise or external artificial noise is used as a source to achieve DP in quantum computing.","We explore how these approaches are applied at different stages of a quantum algorithm (i.e., state preparation, quantum circuit, and quantum measurement).","We also discuss challenges and future directions for DP in quantum computing.","By summarizing recent advancements, we hope to provide a comprehensive, up-to-date overview for researchers venturing into this field."],"url":"http://arxiv.org/abs/2403.09173v1","category":"quant-ph"}
{"created":"2024-03-14 08:26:32","title":"Verification of Bell Nonlocality by Violating Quantum Monogamy Relations","abstract":"Quantum nonlocality as a witness of entanglement plays a crucial role in various fields. Existing quantum monogamy relations rule out the possibility of simultaneous violations of any Bell inequalities with partial statistics generated from one Bell experiment on any multipartite entanglement or post-quantum sources. In this paper, we report an efficient method to construct multipartite Bell test based on any Bell inequalities. We demonstrate that violating these monogamy relations can dynamically witness simultaneous Bell nonlocalities of partial systems. We conduct a tripartite experiment to verify quantum nonlocalities by violating a tripartite monogamy relation using a maximally entangled two-photon state.","sentences":["Quantum nonlocality as a witness of entanglement plays a crucial role in various fields.","Existing quantum monogamy relations rule out the possibility of simultaneous violations of any Bell inequalities with partial statistics generated from one Bell experiment on any multipartite entanglement or post-quantum sources.","In this paper, we report an efficient method to construct multipartite Bell test based on any Bell inequalities.","We demonstrate that violating these monogamy relations can dynamically witness simultaneous Bell nonlocalities of partial systems.","We conduct a tripartite experiment to verify quantum nonlocalities by violating a tripartite monogamy relation using a maximally entangled two-photon state."],"url":"http://arxiv.org/abs/2403.09166v1","category":"quant-ph"}
{"created":"2024-03-14 08:19:41","title":"Caveat Lector: Large Language Models in Legal Practice","abstract":"The current fascination with large language models, or LLMs, derives from the fact that many users lack the expertise to evaluate the quality of the generated text. LLMs may therefore appear more capable than they actually are. The dangerous combination of fluency and superficial plausibility leads to the temptation to trust the generated text and creates the risk of overreliance. Who would not trust perfect legalese? Relying recent findings in both technical and legal scholarship, this Article counterbalances the overly optimistic predictions as to the role of LLMs in legal practice. Integrating LLMs into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks. Notwithstanding their unprecedented ability to generate text, LLMs do not understand text. Without the ability to understand meaning, LLMs will remain unable to use language, to acquire knowledge and to perform complex reasoning tasks. Trained to model language on the basis of stochastic word predictions, LLMs cannot distinguish fact from fiction. Their knowledge of the law is limited to word strings memorized in their parameters. It is also incomplete and largely incorrect. LLMs operate at the level of word distributions, not at the level of verified facts. The resulting propensity to hallucinate, to produce statements that are incorrect but appear helpful and relevant, is alarming in high-risk areas like legal services. At present, lawyers should beware of relying on text generated by LLMs.","sentences":["The current fascination with large language models, or LLMs, derives from the fact that many users lack the expertise to evaluate the quality of the generated text.","LLMs may therefore appear more capable than they actually are.","The dangerous combination of fluency and superficial plausibility leads to the temptation to trust the generated text and creates the risk of overreliance.","Who would not trust perfect legalese?","Relying recent findings in both technical and legal scholarship, this Article counterbalances the overly optimistic predictions as to the role of LLMs in legal practice.","Integrating LLMs into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks.","Notwithstanding their unprecedented ability to generate text, LLMs do not understand text.","Without the ability to understand meaning, LLMs will remain unable to use language, to acquire knowledge and to perform complex reasoning tasks.","Trained to model language on the basis of stochastic word predictions, LLMs cannot distinguish fact from fiction.","Their knowledge of the law is limited to word strings memorized in their parameters.","It is also incomplete and largely incorrect.","LLMs operate at the level of word distributions, not at the level of verified facts.","The resulting propensity to hallucinate, to produce statements that are incorrect but appear helpful and relevant, is alarming in high-risk areas like legal services.","At present, lawyers should beware of relying on text generated by LLMs."],"url":"http://arxiv.org/abs/2403.09163v1","category":"cs.CL"}
{"created":"2024-03-14 08:17:53","title":"Compactness of quantics tensor train representations of local imaginary-time propagators","abstract":"Space-time dependence of imaginary-time propagators, vital for \\textit{ab initio} and many-body calculations based on quantum field theories, has been revealed to be compressible using Quantum Tensor Trains (QTTs) [Phys. Rev. X {\\bf 13}, 021015 (2023)]. However, the impact of system parameters, like temperature, on data size remains underexplored. This paper provides a comprehensive numerical analysis of the compactness of local imaginary-time propagators in QTT for one-time/-frequency objects and two-time/-frequency objects, considering truncation in terms of the Frobenius and maximum norms. We employ random pole models to study worst-case scenarios. The numerical analysis reveals several key findings. For one-time/-frequency objects and two-time/-frequency objects, the bond dimensions saturate at low temperatures, especially for truncation in terms of the Frobenius norm. We provide counting-number arguments for the saturation of bond dimensions for the one-time/-frequency objects, while the origin of this saturation for two-time/-frequency objects remains to be clarified. This paper's findings highlight the critical need for further research on the selection of truncation methods, tolerance levels, and the choice between imaginary-time and imaginary-frequency representations in practical applications.","sentences":["Space-time dependence of imaginary-time propagators, vital for \\textit{ab initio} and many-body calculations based on quantum field theories, has been revealed to be compressible using Quantum Tensor Trains (QTTs)","[Phys. Rev. X {\\bf 13}, 021015 (2023)].","However, the impact of system parameters, like temperature, on data size remains underexplored.","This paper provides a comprehensive numerical analysis of the compactness of local imaginary-time propagators in QTT for one-time/-frequency objects and two-time/-frequency objects, considering truncation in terms of the Frobenius and maximum norms.","We employ random pole models to study worst-case scenarios.","The numerical analysis reveals several key findings.","For one-time/-frequency objects and two-time/-frequency objects, the bond dimensions saturate at low temperatures, especially for truncation in terms of the Frobenius norm.","We provide counting-number arguments for the saturation of bond dimensions for the one-time/-frequency objects, while the origin of this saturation for two-time/-frequency objects remains to be clarified.","This paper's findings highlight the critical need for further research on the selection of truncation methods, tolerance levels, and the choice between imaginary-time and imaginary-frequency representations in practical applications."],"url":"http://arxiv.org/abs/2403.09161v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 08:16:50","title":"Efficient Lexicographic Optimization for Prioritized Robot Control and Planning","abstract":"In this work, we present several tools for efficient sequential hierarchical least-squares programming (S-HLSP) for lexicographical optimization tailored to robot control and planning. As its main step, S-HLSP relies on approximations of the original non-linear hierarchical least-squares programming (NL-HLSP) to a hierarchical least-squares programming (HLSP) by the hierarchical Newton's method or the hierarchical Gauss-Newton algorithm. We present a threshold adaptation strategy for appropriate switches between the two. This ensures optimality of infeasible constraints, promotes numerical stability when solving the HLSP's and enhances optimality of lower priority levels by avoiding regularized local minima. We introduce the solver $\\mathcal{N}$ADM$_2$, an alternating direction method of multipliers for HLSP based on nullspace projections of active constraints. The required basis of nullspace of the active constraints is provided by a computationally efficient turnback algorithm for system dynamics discretized by the Euler method. It is based on an upper bound on the bandwidth of linearly independent column subsets within the linearized constraint matrices. Importantly, an expensive initial rank-revealing matrix factorization is unnecessary. We show how the high sparsity of the basis in the fully-actuated case can be preserved in the under-actuated case. $\\mathcal{N}$ADM$_2$ consistently shows faster computations times than competing off-the-shelf solvers on NL-HLSP composed of test-functions and whole-body trajectory optimization for fully-actuated and under-actuated robotic systems. We demonstrate how the inherently lower accuracy solutions of the alternating direction method of multipliers can be used to warm-start the non-linear solver for efficient computation of high accuracy solutions to non-linear hierarchical least-squares programs.","sentences":["In this work, we present several tools for efficient sequential hierarchical least-squares programming (S-HLSP) for lexicographical optimization tailored to robot control and planning.","As its main step, S-HLSP relies on approximations of the original non-linear hierarchical least-squares programming (NL-HLSP) to a hierarchical least-squares programming (HLSP) by the hierarchical Newton's method or the hierarchical Gauss-Newton algorithm.","We present a threshold adaptation strategy for appropriate switches between the two.","This ensures optimality of infeasible constraints, promotes numerical stability when solving the HLSP's and enhances optimality of lower priority levels by avoiding regularized local minima.","We introduce the solver $\\mathcal{N}$ADM$_2$, an alternating direction method of multipliers for HLSP based on nullspace projections of active constraints.","The required basis of nullspace of the active constraints is provided by a computationally efficient turnback algorithm for system dynamics discretized by the Euler method.","It is based on an upper bound on the bandwidth of linearly independent column subsets within the linearized constraint matrices.","Importantly, an expensive initial rank-revealing matrix factorization is unnecessary.","We show how the high sparsity of the basis in the fully-actuated case can be preserved in the under-actuated case.","$\\mathcal{N}$ADM$_2$ consistently shows faster computations times than competing off-the-shelf solvers on NL-HLSP composed of test-functions and whole-body trajectory optimization for fully-actuated and under-actuated robotic systems.","We demonstrate how the inherently lower accuracy solutions of the alternating direction method of multipliers can be used to warm-start the non-linear solver for efficient computation of high accuracy solutions to non-linear hierarchical least-squares programs."],"url":"http://arxiv.org/abs/2403.09160v1","category":"cs.RO"}
{"created":"2024-03-14 08:12:39","title":"VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation","abstract":"In the field of medical image segmentation, models based on both CNN and Transformer have been thoroughly investigated. However, CNNs have limited modeling capabilities for long-range dependencies, making it challenging to exploit the semantic information within images fully. On the other hand, the quadratic computational complexity poses a challenge for Transformers. Recently, State Space Models (SSMs), such as Mamba, have been recognized as a promising method. They not only demonstrate superior performance in modeling long-range interactions, but also preserve a linear computational complexity. Inspired by the Mamba architecture, We proposed Vison Mamba-UNetV2, the Visual State Space (VSS) Block is introduced to capture extensive contextual information, the Semantics and Detail Infusion (SDI) is introduced to augment the infusion of low-level and high-level features. We conduct comprehensive experiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB and ETIS-LaribPolypDB public datasets. The results indicate that VM-UNetV2 exhibits competitive performance in medical image segmentation tasks. Our code is available at https://github.com/nobodyplayer1/VM-UNetV2.","sentences":["In the field of medical image segmentation, models based on both CNN and Transformer have been thoroughly investigated.","However, CNNs have limited modeling capabilities for long-range dependencies, making it challenging to exploit the semantic information within images fully.","On the other hand, the quadratic computational complexity poses a challenge for Transformers.","Recently, State Space Models (SSMs), such as Mamba, have been recognized as a promising method.","They not only demonstrate superior performance in modeling long-range interactions, but also preserve a linear computational complexity.","Inspired by the Mamba architecture, We proposed Vison Mamba-UNetV2, the Visual State Space (VSS) Block is introduced to capture extensive contextual information, the Semantics and Detail Infusion (SDI) is introduced to augment the infusion of low-level and high-level features.","We conduct comprehensive experiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB and ETIS-LaribPolypDB public datasets.","The results indicate that VM-UNetV2 exhibits competitive performance in medical image segmentation tasks.","Our code is available at https://github.com/nobodyplayer1/VM-UNetV2."],"url":"http://arxiv.org/abs/2403.09157v1","category":"eess.IV"}
{"created":"2024-03-14 08:05:36","title":"Energy-gap modulation and majorization in three-level quantum Otto engine","abstract":"A three-level quantum system having two energy gaps presents a nontrivial working medium for a quantum heat engine. Our focus lies in understanding the constraints on the ability to modulate these gaps relative to the changes in probability distributions at the two given heat reservoirs. It is seen that an Otto engine in the quasistatic limit is feasible if at least one energy gap shrinks during the first quantum adiabatic stage. We analyze operating conditions under different variations of the gaps, revealing that a definite majorization relation between the hot and cold distributions serves as a sufficient criterion for the engine when both gaps are shrinking. Further, Otto efficiency is enhanced in case majorization holds. On the other hand, majorization becomes a necessary condition when only one of the gaps is shrinking. In the special case where one gap remains fixed, majorization is both necessary and sufficient for engine operation. For an $n$-level system, we note that a well defined change in energy gaps aligns with the majorization relation, thus characterizing the operation of the engine.","sentences":["A three-level quantum system having two energy gaps presents a nontrivial working medium for a quantum heat engine.","Our focus lies in understanding the constraints on the ability to modulate these gaps relative to the changes in probability distributions at the two given heat reservoirs.","It is seen that an Otto engine in the quasistatic limit is feasible if at least one energy gap shrinks during the first quantum adiabatic stage.","We analyze operating conditions under different variations of the gaps, revealing that a definite majorization relation between the hot and cold distributions serves as a sufficient criterion for the engine when both gaps are shrinking.","Further, Otto efficiency is enhanced in case majorization holds.","On the other hand, majorization becomes a necessary condition when only one of the gaps is shrinking.","In the special case where one gap remains fixed, majorization is both necessary and sufficient for engine operation.","For an $n$-level system, we note that a well defined change in energy gaps aligns with the majorization relation, thus characterizing the operation of the engine."],"url":"http://arxiv.org/abs/2403.09154v1","category":"quant-ph"}
{"created":"2024-03-14 08:04:46","title":"Fairness-Aware Multi-Server Federated Learning Task Delegation over Wireless Networks","abstract":"In the rapidly advancing field of federated learning (FL), ensuring efficient FL task delegation while incentivising FL client participation poses significant challenges, especially in wireless networks where FL participants' coverage is limited. Existing Contract Theory-based methods are designed under the assumption that there is only one FL server in the system (i.e., the monopoly market assumption), which in unrealistic in practice. To address this limitation, we propose Fairness-Aware Multi-Server FL task delegation approach (FAMuS), a novel framework based on Contract Theory and Lyapunov optimization to jointly address these intricate issues facing wireless multi-server FL networks (WMSFLN). Within a given WMSFLN, a task requester products multiple FL tasks and delegate them to FL servers which coordinate the training processes. To ensure fair treatment of FL servers, FAMuS establishes virtual queues to track their previous access to FL tasks, updating them in relation to the resulting FL model performance. The objective is to minimize the time-averaged cost in a WMSFLN, while ensuring all queues remain stable. This is particularly challenging given the incomplete information regarding FL clients' participation cost and the unpredictable nature of the WMSFLN state, which depends on the locations of the mobile clients. Extensive experiments comparing FAMuS against five state-of-the-art approaches based on two real-world datasets demonstrate that it achieves 6.91% higher test accuracy, 27.34% lower cost, and 0.63% higher fairness on average than the best-performing baseline.","sentences":["In the rapidly advancing field of federated learning (FL), ensuring efficient FL task delegation while incentivising FL client participation poses significant challenges, especially in wireless networks where FL participants' coverage is limited.","Existing Contract Theory-based methods are designed under the assumption that there is only one FL server in the system (i.e., the monopoly market assumption), which in unrealistic in practice.","To address this limitation, we propose Fairness-Aware Multi-Server FL task delegation approach (FAMuS), a novel framework based on Contract Theory and Lyapunov optimization to jointly address these intricate issues facing wireless multi-server FL networks (WMSFLN).","Within a given WMSFLN, a task requester products multiple FL tasks and delegate them to FL servers which coordinate the training processes.","To ensure fair treatment of FL servers, FAMuS establishes virtual queues to track their previous access to FL tasks, updating them in relation to the resulting FL model performance.","The objective is to minimize the time-averaged cost in a WMSFLN, while ensuring all queues remain stable.","This is particularly challenging given the incomplete information regarding FL clients' participation cost and the unpredictable nature of the WMSFLN state, which depends on the locations of the mobile clients.","Extensive experiments comparing FAMuS against five state-of-the-art approaches based on two real-world datasets demonstrate that it achieves 6.91% higher test accuracy, 27.34% lower cost, and 0.63% higher fairness on average than the best-performing baseline."],"url":"http://arxiv.org/abs/2403.09153v1","category":"eess.SY"}
{"created":"2024-03-14 08:00:37","title":"Line geometry of pairs of second-order Hamiltonian operators and quasilinear systems","abstract":"We show that a pair formed by a second-order homogeneous Hamiltonian structures in $N$ components and the associated system of conservation laws is in bijective correspondence with an alternating three-form on a $N+2$ vector space. We use this result to characterise these pairs up to $N=4$. We also show that the three-form provides $N+2$ linear equations in the Pl\\\"ucker coordinates which define the associated line congruence.","sentences":["We show that a pair formed by a second-order homogeneous Hamiltonian structures in $N$ components and the associated system of conservation laws is in bijective correspondence with an alternating three-form on a $N+2$ vector space.","We use this result to characterise these pairs up to $N=4$. We also show that the three-form provides $N+2$ linear equations in the Pl\\\"ucker coordinates which define the associated line congruence."],"url":"http://arxiv.org/abs/2403.09152v1","category":"math-ph"}
{"created":"2024-03-14 07:59:51","title":"MPC without Terminal Ingredients Tailored to the SEIR Compartmental Epidemic Model","abstract":"We consider the SEIR epidemic model subject to state and input constraints (a cap on the proportion of infectuous individuals and limits on the allowed social distancing and quarantining measures, respectively). We present a model predictive control (MPC) formulation tailored to this system without terminal conditions in the recursively solved finite-horizon optimal control problem. We rigorously show recursive feasibility and asymptotic stability of the disease-free equilibrium w.r.t. the MPC closed loop for suitably designed quadratic running cost and a sufficiently long prediction horizon (forecast window). Moreover, we establish the viability kernel (a.k.a. the admissible set) as domain of attraction provided that the infection numbers that are not too small at the beginning, which corresponds to infection numbers noticeable by, e.g., policy makers or the general public.","sentences":["We consider the SEIR epidemic model subject to state and input constraints (a cap on the proportion of infectuous individuals and limits on the allowed social distancing and quarantining measures, respectively).","We present a model predictive control (MPC) formulation tailored to this system without terminal conditions in the recursively solved finite-horizon optimal control problem.","We rigorously show recursive feasibility and asymptotic stability of the disease-free equilibrium w.r.t.","the MPC closed loop for suitably designed quadratic running cost and a sufficiently long prediction horizon (forecast window).","Moreover, we establish the viability kernel (a.k.a.","the admissible set) as domain of attraction provided that the infection numbers that are not too small at the beginning, which corresponds to infection numbers noticeable by, e.g., policy makers or the general public."],"url":"http://arxiv.org/abs/2403.09151v1","category":"math.OC"}
{"created":"2024-03-14 07:59:35","title":"Detecting the N\u00e9el vector of altermagnet by attaching a topological insulator and crystalline valley-edge insulator","abstract":"In order to detect the N\\'{e}el vector of an altermagnet, we investigate topological phases in a bilayer system composed of an altermagnet and a two-dimensional topological insulator described by the Bernevig-Hughes-Zhang model. A topological phase transition occurs from a first-order topological insulator to a trivial insulator at a certain critical altermagnetization if the N\\'{e}el vector of altermagnet is along the $x$ axis or the $y$ axis. It is intriguing that valley-protected edge states emerge along the N\\'{e}el vector in this trivial insulator, which are as stable as the topological edge states. We name it a crystalline valley-edge insulator. On the other hand, the system turns out to be a second-order topological insulator when the N\\'{e}el vector is along the $z$ axis. The tunneling conductance has a strong dependence on the N\\'{e}el vector. In addition, the band gap depends on the N\\'{e}el vector, which is measurable by optical absorption. Hence, it is possible experimentally to detect the N\\'{e}el vector by measuring tunneling conductance and optical absorption.","sentences":["In order to detect the N\\'{e}el vector of an altermagnet, we investigate topological phases in a bilayer system composed of an altermagnet and a two-dimensional topological insulator described by the Bernevig-Hughes-Zhang model.","A topological phase transition occurs from a first-order topological insulator to a trivial insulator at a certain critical altermagnetization if the N\\'{e}el vector of altermagnet is along the $x$ axis or the $y$ axis.","It is intriguing that valley-protected edge states emerge along the N\\'{e}el vector in this trivial insulator, which are as stable as the topological edge states.","We name it a crystalline valley-edge insulator.","On the other hand, the system turns out to be a second-order topological insulator when the N\\'{e}el vector is along the $z$ axis.","The tunneling conductance has a strong dependence on the N\\'{e}el vector.","In addition, the band gap depends on the N\\'{e}el vector, which is measurable by optical absorption.","Hence, it is possible experimentally to detect the N\\'{e}el vector by measuring tunneling conductance and optical absorption."],"url":"http://arxiv.org/abs/2403.09150v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-14 07:58:42","title":"$2$-Periodic complexes over regular local rings","abstract":"Let $(A,\\mathfrak{m})$ be a regular local ring of dimension $d \\geq 1$. Let $\\mathcal{D}^2_{fg}(A)$ denote the derived category of $2$-periodic complexes with finitely generated cohomology modules. Let $\\mathcal{K}^2(\\proj A) $ denote the homotopy category of $2$-periodic complexes of finitely generated free $A$-modules.   We show the natural map $\\mathcal{K}^2(\\ proj \\ A) \\longrightarrow \\mathcal{D}^2(A)$ is an equivalence of categories. When $A$ is complete we show that $\\mathcal{K}^2_f(\\ proj \\ A)$ ($2$-periodic complexes with finite length cohomology) is Krull-Schmidt with Auslander-Reiten (AR) triangles. We also compute the AR-quiver of $\\mathcal{K}^2_f(\\ proj \\ A)$ when $\\ dim \\ A = 1$.","sentences":["Let $(A,\\mathfrak{m})$ be a regular local ring of dimension $d \\geq 1$. Let $\\mathcal{D}^2_{fg}(A)$ denote the derived category of $2$-periodic complexes with finitely generated cohomology modules.","Let $\\mathcal{K}^2(\\proj A) $ denote the homotopy category of $2$-periodic complexes of finitely generated free $A$-modules.   ","We show the natural map $\\mathcal{K}^2(\\ proj \\ A)","\\longrightarrow \\mathcal{D}^2(A)$ is an equivalence of categories.","When $A$ is complete we show that $\\mathcal{K}^2_f(\\ proj \\ A)$ ($2$-periodic complexes with finite length cohomology) is Krull-Schmidt with Auslander-Reiten (AR) triangles.","We also compute the AR-quiver of $\\mathcal{K}^2_f(\\ proj \\ A)$ when $\\ dim \\ A = 1$."],"url":"http://arxiv.org/abs/2403.09149v1","category":"math.AC"}
{"created":"2024-03-14 07:47:02","title":"Complexity Classification of Complex-Weighted Counting Acyclic Constraint Satisfaction Problems","abstract":"We study the computational complexity of counting constraint satisfaction problems (#CSPs) whose constraints assign complex numbers to Boolean inputs when the corresponding constraint hypergraphs are acyclic. These problems are called acyclic #CSPs or succinctly, #ACSPs. We wish to determine the computational complexity of all such #ACSPs when arbitrary unary constraints are freely available. Depending on whether we further allow or disallow the free use of the specific constraint XOR (binary disequality), we present two complexity classifications of the #ACSPs according to the types of constraints used for the problems. When XOR is freely available, we first obtain a complete dichotomy classification. On the contrary, when XOR is not available for free, we then obtain a trichotomy classification. To deal with an acyclic nature of constraints in those classifications, we develop a new technical tool called acyclic-T-constructibility or AT-constructibility, and we exploit it to analyze a complexity upper bound of each #ACSPs.","sentences":["We study the computational complexity of counting constraint satisfaction problems (#CSPs) whose constraints assign complex numbers to Boolean inputs when the corresponding constraint hypergraphs are acyclic.","These problems are called acyclic #CSPs or succinctly, #ACSPs.","We wish to determine the computational complexity of all such #ACSPs when arbitrary unary constraints are freely available.","Depending on whether we further allow or disallow the free use of the specific constraint XOR (binary disequality), we present two complexity classifications of the #ACSPs according to the types of constraints used for the problems.","When XOR is freely available, we first obtain a complete dichotomy classification.","On the contrary, when XOR is not available for free, we then obtain a trichotomy classification.","To deal with an acyclic nature of constraints in those classifications, we develop a new technical tool called acyclic-T-constructibility or AT-constructibility, and we exploit it to analyze a complexity upper bound of each #ACSPs."],"url":"http://arxiv.org/abs/2403.09145v1","category":"cs.CC"}
{"created":"2024-03-14 07:22:52","title":"Non-distributive relatives of ETL and NFL","abstract":"In this paper, we devise non-distributive relatives of Exactly True Logic (ETL) by Pietz and Riveccio and its dual (NFL) Non-Falsity Logic by Shramko, Zaitsev and Belikov. We consider two pre-orders which are algebraic counterparts of the ETL's and NFL's entailment relations on the De Morgan lattice $\\mathbf{4}$. We generalise these pre-orders and determine which distributive properties that hold on $\\mathbf{4}$ are not forced by either of the pre-orders. We then construct relatives of ETL and NFL but lack such distributive properties. For these logics, we also devise a truth table semantics which uses non-distributive lattice $\\mathbf{M3}$ as their lattice of truth values. We also provide analytic tableaux systems that work with sequents of the form $\\phi\\vdash\\chi$. We also prove the correctness and completeness results for these proof systems and provide a neat generalisation for non-distributive ETL- and NFL-like logics built over a certain family of non-distributive modular lattices.","sentences":["In this paper, we devise non-distributive relatives of Exactly True Logic (ETL) by Pietz and Riveccio and its dual (NFL) Non-Falsity Logic by Shramko, Zaitsev and Belikov.","We consider two pre-orders which are algebraic counterparts of the ETL's and NFL's entailment relations on the De Morgan lattice $\\mathbf{4}$. We generalise these pre-orders and determine which distributive properties that hold on $\\mathbf{4}$ are not forced by either of the pre-orders.","We then construct relatives of ETL and NFL but lack such distributive properties.","For these logics, we also devise a truth table semantics which uses non-distributive lattice $\\mathbf{M3}$ as their lattice of truth values.","We also provide analytic tableaux systems that work with sequents of the form $\\phi\\vdash\\chi$. We also prove the correctness and completeness results for these proof systems and provide a neat generalisation for non-distributive ETL- and NFL-like logics built over a certain family of non-distributive modular lattices."],"url":"http://arxiv.org/abs/2403.09137v1","category":"math.LO"}
{"created":"2024-03-14 07:21:46","title":"Biophysics Informed Pathological Regularisation for Brain Tumour Segmentation","abstract":"Recent advancements in deep learning have significantly improved brain tumour segmentation techniques; however, the results still lack confidence and robustness as they solely consider image data without biophysical priors or pathological information. Integrating biophysics-informed regularisation is one effective way to change this situation, as it provides an prior regularisation for automated end-to-end learning. In this paper, we propose a novel approach that designs brain tumour growth Partial Differential Equation (PDE) models as a regularisation with deep learning, operational with any network model. Our method introduces tumour growth PDE models directly into the segmentation process, improving accuracy and robustness, especially in data-scarce scenarios. This system estimates tumour cell density using a periodic activation function. By effectively integrating this estimation with biophysical models, we achieve a better capture of tumour characteristics. This approach not only aligns the segmentation closer to actual biological behaviour but also strengthens the model's performance under limited data conditions. We demonstrate the effectiveness of our framework through extensive experiments on the BraTS 2023 dataset, showcasing significant improvements in both precision and reliability of tumour segmentation.","sentences":["Recent advancements in deep learning have significantly improved brain tumour segmentation techniques; however, the results still lack confidence and robustness as they solely consider image data without biophysical priors or pathological information.","Integrating biophysics-informed regularisation is one effective way to change this situation, as it provides an prior regularisation for automated end-to-end learning.","In this paper, we propose a novel approach that designs brain tumour growth Partial Differential Equation (PDE) models as a regularisation with deep learning, operational with any network model.","Our method introduces tumour growth PDE models directly into the segmentation process, improving accuracy and robustness, especially in data-scarce scenarios.","This system estimates tumour cell density using a periodic activation function.","By effectively integrating this estimation with biophysical models, we achieve a better capture of tumour characteristics.","This approach not only aligns the segmentation closer to actual biological behaviour but also strengthens the model's performance under limited data conditions.","We demonstrate the effectiveness of our framework through extensive experiments on the BraTS 2023 dataset, showcasing significant improvements in both precision and reliability of tumour segmentation."],"url":"http://arxiv.org/abs/2403.09136v1","category":"eess.IV"}
{"created":"2024-03-14 07:05:10","title":"Local Enumeration and Majority Lower Bounds","abstract":"Depth-3 circuit lower bounds and $k$-SAT algorithms are intimately related; the state-of-the-art $\\Sigma^k_3$-circuit lower bound and the $k$-SAT algorithm are based on the same combinatorial theorem. In this paper we define a problem which reveals new interactions between the two. Define Enum($k$, $t$) problem as: given an $n$-variable $k$-CNF and an initial assignment $\\alpha$, output all satisfying assignments at Hamming distance $t$ from $\\alpha$, assuming that there are no satisfying assignments of Hamming distance less than $t$ from $\\alpha$. Observe that: an upper bound $b(n, k, t)$ on the complexity of Enum($k$, $t$) implies:   - Depth-3 circuits: Any $\\Sigma^k_3$ circuit computing the Majority function has size at least $\\binom{n}{\\frac{n}{2}}/b(n, k, \\frac{n}{2})$.   - $k$-SAT: There exists an algorithm solving $k$-SAT in time $O(\\sum_{t = 1}^{n/2}b(n, k, t))$.   A simple construction shows that $b(n, k, \\frac{n}{2}) \\ge 2^{(1 - O(\\log(k)/k))n}$. Thus, matching upper bounds would imply a $\\Sigma^k_3$-circuit lower bound of $2^{\\Omega(\\log(k)n/k)}$ and a $k$-SAT upper bound of $2^{(1 - \\Omega(\\log(k)/k))n}$. The former yields an unrestricted depth-3 lower bound of $2^{\\omega(\\sqrt{n})}$ solving a long standing open problem, and the latter breaks the Super Strong Exponential Time Hypothesis.   In this paper, we propose a randomized algorithm for Enum($k$, $t$) and introduce new ideas to analyze it. We demonstrate the power of our ideas by considering the first non-trivial instance of the problem, i.e., Enum($3$, $\\frac{n}{2}$). We show that the expected running time of our algorithm is $1.598^n$, substantially improving on the trivial bound of $3^{n/2} \\simeq 1.732^n$. This already improves $\\Sigma^3_3$ lower bounds for Majority function to $1.251^n$. The previous bound was $1.154^n$ which follows from the work of H{\\aa}stad, Jukna, and Pudl\\'ak (Comput. Complex.'95).","sentences":["Depth-3 circuit lower bounds and $k$-SAT algorithms are intimately related; the state-of-the-art $\\Sigma^k_3$-circuit lower bound and the $k$-SAT algorithm are based on the same combinatorial theorem.","In this paper we define a problem which reveals new interactions between the two.","Define Enum($k$, $t$) problem as: given an $n$-variable $k$-CNF and an initial assignment $\\alpha$, output all satisfying assignments at Hamming distance $t$ from $\\alpha$, assuming that there are no satisfying assignments of Hamming distance less than $t$ from $\\alpha$. Observe that: an upper bound $b(n, k, t)$ on the complexity of Enum($k$, $t$) implies:   - Depth-3 circuits: Any $\\Sigma^k_3$ circuit computing the Majority function has size at least $\\binom{n}{\\frac{n}{2}}/b(n, k, \\frac{n}{2})$.   - $k$-SAT: There exists an algorithm solving $k$-SAT in time $O(\\sum_{t = 1}^{n/2}b(n, k, t))$.   A simple construction shows that $b(n, k, \\frac{n}{2}) \\ge","2^{(1 - O(\\log(k)/k))n}$.","Thus, matching upper bounds would imply a $\\Sigma^k_3$-circuit lower bound of $2^{\\Omega(\\log(k)n/k)}$ and a $k$-SAT upper bound of $2^{(1 - \\Omega(\\log(k)/k))n}$. The former yields an unrestricted depth-3 lower bound of $2^{\\omega(\\sqrt{n})}$ solving a long standing open problem, and the latter breaks the Super Strong Exponential Time Hypothesis.   ","In this paper, we propose a randomized algorithm for Enum($k$, $t$) and introduce new ideas to analyze it.","We demonstrate the power of our ideas by considering the first non-trivial instance of the problem, i.e., Enum($3$, $\\frac{n}{2}$).","We show that the expected running time of our algorithm is $1.598^n$, substantially improving on the trivial bound of $3^{n/2} \\simeq 1.732^n$.","This already improves $\\Sigma^3_3$ lower bounds for Majority function to $1.251^n$. The previous bound was $1.154^n$ which follows from the work of H{\\aa}stad, Jukna, and Pudl\\'ak (Comput.","Complex.'95)."],"url":"http://arxiv.org/abs/2403.09134v1","category":"cs.CC"}
{"created":"2024-03-14 06:57:04","title":"Quantitative Reducibility of $C^k$ Quasi-Periodic Cocycles","abstract":"This paper establishes an extreme $C^k$ reducibility theorem of quasi-periodic $SL(2, \\mathbb{R})$ cocycles in the local perturbative region, revealing both the essence of Eliasson [Commun.Math.Phys.1992] and Hou-You [Invent.Math.2022] in respectively the non-resonant and resonant cases. By paralleling further the reducibility process with the almost reducibility, we are able to acquire the least initial regularity as well as the least loss of regularity for the whole KAM iterations. This, in return, makes various spectral applications of quasi-periodic Schr\\\"odinger operators wide open.","sentences":["This paper establishes an extreme $C^k$ reducibility theorem of quasi-periodic $SL(2, \\mathbb{R})$ cocycles in the local perturbative region, revealing both the essence of Eliasson","[Commun.","Math.Phys.1992] and Hou-You [Invent.","Math.2022] in respectively the non-resonant and resonant cases.","By paralleling further the reducibility process with the almost reducibility, we are able to acquire the least initial regularity as well as the least loss of regularity for the whole KAM iterations.","This, in return, makes various spectral applications of quasi-periodic Schr\\\"odinger operators wide open."],"url":"http://arxiv.org/abs/2403.09132v1","category":"math.DS"}
{"created":"2024-03-14 06:29:17","title":"All-pay Auction Based Profit Maximization in End-to-End Computation Offloading System","abstract":"Pricing is an important issue in mobile edge computing. How to appropriately determine the bid of end user (EU) is an incentive factor for edge cloud (EC) to offer service. In this letter, we propose an equilibrium pricing scheme based on the all-pay auction model in end-to-end collaboration environment, wherein all EUs can acquire the service at a lower price than the own value of the required resource. In addition, we propose a set allocation algorithm to divide all the bidders into different sets according to the price, and the EUs in each set get the service, which averts the case of getting no service due to the low price. Extensive simulation results demonstrate that the proposed scheme can effectively maximize the total profit of the edge offloading system, and guarantee all EUs can access the service.","sentences":["Pricing is an important issue in mobile edge computing.","How to appropriately determine the bid of end user (EU) is an incentive factor for edge cloud (EC) to offer service.","In this letter, we propose an equilibrium pricing scheme based on the all-pay auction model in end-to-end collaboration environment, wherein all EUs can acquire the service at a lower price than the own value of the required resource.","In addition, we propose a set allocation algorithm to divide all the bidders into different sets according to the price, and the EUs in each set get the service, which averts the case of getting no service due to the low price.","Extensive simulation results demonstrate that the proposed scheme can effectively maximize the total profit of the edge offloading system, and guarantee all EUs can access the service."],"url":"http://arxiv.org/abs/2403.09129v1","category":"cs.GT"}
{"created":"2024-03-14 06:25:37","title":"Optimal Pinning Control for Synchronization over Temporal Networks","abstract":"In this paper, we address the finite time synchronization of a network of dynamical systems with time-varying interactions modeled using temporal networks. We synchronize a few nodes initially using external control inputs. These nodes are termed as pinning nodes. The other nodes are synchronized by interacting with the pinning nodes and with each other. We first provide sufficient conditions for the network to be synchronized. Then we formulate an optimization problem to minimize the number of pinning nodes for synchronizing the entire network. Finally, we address the problem of maximizing the number of synchronized nodes when there are constraints on the number of nodes that could be pinned. We show that this problem belongs to the class of NP-hard problems and propose a greedy heuristic. We illustrate the results using numerical simulations.","sentences":["In this paper, we address the finite time synchronization of a network of dynamical systems with time-varying interactions modeled using temporal networks.","We synchronize a few nodes initially using external control inputs.","These nodes are termed as pinning nodes.","The other nodes are synchronized by interacting with the pinning nodes and with each other.","We first provide sufficient conditions for the network to be synchronized.","Then we formulate an optimization problem to minimize the number of pinning nodes for synchronizing the entire network.","Finally, we address the problem of maximizing the number of synchronized nodes when there are constraints on the number of nodes that could be pinned.","We show that this problem belongs to the class of NP-hard problems and propose a greedy heuristic.","We illustrate the results using numerical simulations."],"url":"http://arxiv.org/abs/2403.09127v1","category":"eess.SY"}
{"created":"2024-03-14 06:20:35","title":"Femtoscopic study of the $\u039b\u03b1$ interaction","abstract":"We examine the $\\Lambda$-${}^4\\mathrm{He}$ ($\\alpha$) momentum correlation in high-energy collisions to elucidate the interaction between Lambdas ($\\Lambda$) and nucleons ($N$). We compare phenomenological $\\Lambda\\alpha$ potentials with different strength at short range. In addition to the conventional Gaussian type potentials, we construct the $\\Lambda\\alpha$ potentials by substituting the nucleon density distribution of $\\alpha$ in the Skyrme-type $\\Lambda$ potentials. We find that the dependence on the employed potential models is visible in the correlation functions from a small size source. This indicates that the $\\Lambda\\alpha$ momentum correlation could constrain the property of the $\\Lambda N$ interaction at high densities, which is expected to play an essential role in dense nuclear matter. Also, we verify that the Lednicky-Lyuboshits formula can yield erroneous results for a small source size with a potential which has large interaction range, like the $\\Lambda\\alpha$ system.","sentences":["We examine the $\\Lambda$-${}^4\\mathrm{He}$ ($\\alpha$) momentum correlation in high-energy collisions to elucidate the interaction between Lambdas ($\\Lambda$) and nucleons ($N$).","We compare phenomenological $\\Lambda\\alpha$ potentials with different strength at short range.","In addition to the conventional Gaussian type potentials, we construct the $\\Lambda\\alpha$ potentials by substituting the nucleon density distribution of $\\alpha$ in the Skyrme-type $\\Lambda$ potentials.","We find that the dependence on the employed potential models is visible in the correlation functions from a small size source.","This indicates that the $\\Lambda\\alpha$ momentum correlation could constrain the property of the $\\Lambda N$ interaction at high densities, which is expected to play an essential role in dense nuclear matter.","Also, we verify that the Lednicky-Lyuboshits formula can yield erroneous results for a small source size with a potential which has large interaction range, like the $\\Lambda\\alpha$ system."],"url":"http://arxiv.org/abs/2403.09126v1","category":"nucl-th"}
{"created":"2024-03-14 06:14:07","title":"Optimal Top-Two Method for Best Arm Identification and Fluid Analysis","abstract":"Top-$2$ methods have become popular in solving the best arm identification (BAI) problem. The best arm, or the arm with the largest mean amongst finitely many, is identified through an algorithm that at any sequential step independently pulls the empirical best arm, with a fixed probability $\\beta$, and pulls the best challenger arm otherwise. The probability of incorrect selection is guaranteed to lie below a specified $\\delta >0$. Information theoretic lower bounds on sample complexity are well known for BAI problem and are matched asymptotically as $\\delta \\rightarrow 0$ by computationally demanding plug-in methods. The above top 2 algorithm for any $\\beta \\in (0,1)$ has sample complexity within a constant of the lower bound. However, determining the optimal $\\beta$ that matches the lower bound has proven difficult. In this paper, we address this and propose an optimal top-2 type algorithm. We consider a function of allocations anchored at a threshold. If it exceeds the threshold then the algorithm samples the empirical best arm. Otherwise, it samples the challenger arm. We show that the proposed algorithm is optimal as $\\delta \\rightarrow 0$. Our analysis relies on identifying a limiting fluid dynamics of allocations that satisfy a series of ordinary differential equations pasted together and that describe the asymptotic path followed by our algorithm. We rely on the implicit function theorem to show existence and uniqueness of these fluid ode's and to show that the proposed algorithm remains close to the ode solution.","sentences":["Top-$2$ methods have become popular in solving the best arm identification (BAI) problem.","The best arm, or the arm with the largest mean amongst finitely many, is identified through an algorithm that at any sequential step independently pulls the empirical best arm, with a fixed probability $\\beta$, and pulls the best challenger arm otherwise.","The probability of incorrect selection is guaranteed to lie below a specified $\\delta >0$. Information theoretic lower bounds on sample complexity are well known for BAI problem and are matched asymptotically as $\\delta \\rightarrow 0$ by computationally demanding plug-in methods.","The above top 2 algorithm for any $\\beta \\in (0,1)$ has sample complexity within a constant of the lower bound.","However, determining the optimal $\\beta$ that matches the lower bound has proven difficult.","In this paper, we address this and propose an optimal top-2 type algorithm.","We consider a function of allocations anchored at a threshold.","If it exceeds the threshold then the algorithm samples the empirical best arm.","Otherwise, it samples the challenger arm.","We show that the proposed algorithm is optimal as $\\delta \\rightarrow 0$.","Our analysis relies on identifying a limiting fluid dynamics of allocations that satisfy a series of ordinary differential equations pasted together and that describe the asymptotic path followed by our algorithm.","We rely on the implicit function theorem to show existence and uniqueness of these fluid ode's and to show that the proposed algorithm remains close to the ode solution."],"url":"http://arxiv.org/abs/2403.09123v1","category":"cs.LG"}
{"created":"2024-03-14 06:10:51","title":"OutlineSpark: Igniting AI-powered Presentation Slides Creation from Computational Notebooks through Outlines","abstract":"Computational notebooks are widely utilized for exploration and analysis. However, creating slides to communicate analysis results from these notebooks is quite tedious and time-consuming. Researchers have proposed automatic systems for generating slides from notebooks, which, however, often do not consider the process of users conceiving and organizing their messages from massive code cells. Those systems ask users to go directly into the slide creation process, which causes potentially ill-structured slides and burdens in further refinement. Inspired by the common and widely recommended slide creation practice: drafting outlines first and then adding concrete content, we introduce OutlineSpark, an AI-powered slide creation tool that generates slides from a slide outline written by the user. The tool automatically retrieves relevant notebook cells based on the outlines and converts them into slide content. We evaluated OutlineSpark with 12 users. Both the quantitative and qualitative feedback from the participants verify its effectiveness and usability.","sentences":["Computational notebooks are widely utilized for exploration and analysis.","However, creating slides to communicate analysis results from these notebooks is quite tedious and time-consuming.","Researchers have proposed automatic systems for generating slides from notebooks, which, however, often do not consider the process of users conceiving and organizing their messages from massive code cells.","Those systems ask users to go directly into the slide creation process, which causes potentially ill-structured slides and burdens in further refinement.","Inspired by the common and widely recommended slide creation practice: drafting outlines first and then adding concrete content, we introduce OutlineSpark, an AI-powered slide creation tool that generates slides from a slide outline written by the user.","The tool automatically retrieves relevant notebook cells based on the outlines and converts them into slide content.","We evaluated OutlineSpark with 12 users.","Both the quantitative and qualitative feedback from the participants verify its effectiveness and usability."],"url":"http://arxiv.org/abs/2403.09121v1","category":"cs.HC"}
{"created":"2024-03-14 06:01:26","title":"Generalisation of proof simulation procedures for Frege systems by M.L.~Bonet and S.R.~Buss","abstract":"In this paper, we present a~generalisation of proof simulation procedures for Frege systems by Bonet and Buss to some logics for which the deduction theorem does not hold. In particular, we study the case of finite-valued \\L{}ukasiewicz logics. To this end, we provide proof systems that augment Avron's Frege system for \\L{}ukasiewicz three-valued logic with nested and general versions of the disjunction elimination rule, respectively. For these systems we provide upper bounds on speed-ups w.r.t.\\ both the number of steps in proofs and the length of proofs. We also consider Tamminga's natural deduction and Avron's hypersequent calculus for 3-valued \\L{}ukasiewicz logic and generalise our results considering the disjunction elimination rule to all finite-valued \\L{}ukasiewicz logics.","sentences":["In this paper, we present a~generalisation of proof simulation procedures for Frege systems by Bonet and Buss to some logics for which the deduction theorem does not hold.","In particular, we study the case of finite-valued \\L{}ukasiewicz logics.","To this end, we provide proof systems that augment Avron's Frege system for \\L{}ukasiewicz three-valued logic with nested and general versions of the disjunction elimination rule, respectively.","For these systems we provide upper bounds on speed-ups w.r.t.\\ both the number of steps in proofs and the length of proofs.","We also consider Tamminga's natural deduction and Avron's hypersequent calculus for 3-valued \\L{}ukasiewicz logic and generalise our results considering the disjunction elimination rule to all finite-valued \\L{}ukasiewicz logics."],"url":"http://arxiv.org/abs/2403.09119v1","category":"math.LO"}
{"created":"2024-03-14 06:00:42","title":"Graph-Based DDoS Attack Detection in IoT Systems with Lossy Network","abstract":"This study introduces a robust solution for the detection of Distributed Denial of Service (DDoS) attacks in Internet of Things (IoT) systems, leveraging the capabilities of Graph Convolutional Networks (GCN). By conceptualizing IoT devices as nodes within a graph structure, we present a detection mechanism capable of operating efficiently even in lossy network environments. We introduce various graph topologies for modeling IoT networks and evaluate them for detecting tunable futuristic DDoS attacks. By studying different levels of network connection loss and various attack situations, we demonstrate that the correlation-based hybrid graph structure is effective in spotting DDoS attacks, substantiating its good performance even in lossy network scenarios. The results indicate a remarkable performance of the GCN-based DDoS detection model with an F1 score of up to 91%. Furthermore, we observe at most a 2% drop in F1-score in environments with up to 50% connection loss. The findings from this study highlight the advantages of utilizing GCN for the security of IoT systems which benefit from high detection accuracy while being resilient to connection disruption.","sentences":["This study introduces a robust solution for the detection of Distributed Denial of Service (DDoS) attacks in Internet of Things (IoT) systems, leveraging the capabilities of Graph Convolutional Networks (GCN).","By conceptualizing IoT devices as nodes within a graph structure, we present a detection mechanism capable of operating efficiently even in lossy network environments.","We introduce various graph topologies for modeling IoT networks and evaluate them for detecting tunable futuristic DDoS attacks.","By studying different levels of network connection loss and various attack situations, we demonstrate that the correlation-based hybrid graph structure is effective in spotting DDoS attacks, substantiating its good performance even in lossy network scenarios.","The results indicate a remarkable performance of the GCN-based DDoS detection model with an F1 score of up to 91%.","Furthermore, we observe at most a 2% drop in F1-score in environments with up to 50% connection loss.","The findings from this study highlight the advantages of utilizing GCN for the security of IoT systems which benefit from high detection accuracy while being resilient to connection disruption."],"url":"http://arxiv.org/abs/2403.09118v1","category":"cs.CR"}
{"created":"2024-03-14 05:40:23","title":"Randomized Principal Component Analysis for Hyperspectral Image Classification","abstract":"The high-dimensional feature space of the hyperspectral imagery poses major challenges to the processing and analysis of the hyperspectral data sets. In such a case, dimensionality reduction is necessary to decrease the computational complexity. The random projections open up new ways of dimensionality reduction, especially for large data sets. In this paper, the principal component analysis (PCA) and randomized principal component analysis (R-PCA) for the classification of hyperspectral images using support vector machines (SVM) and light gradient boosting machines (LightGBM) have been investigated. In this experimental research, the number of features was reduced to 20 and 30 for classification of two hyperspectral datasets (Indian Pines and Pavia University). The experimental results demonstrated that PCA outperformed R-PCA for SVM for both datasets, but received close accuracy values for LightGBM. The highest classification accuracies were obtained as 0.9925 and 0.9639 by LightGBM with original features for the Pavia University and Indian Pines, respectively.","sentences":["The high-dimensional feature space of the hyperspectral imagery poses major challenges to the processing and analysis of the hyperspectral data sets.","In such a case, dimensionality reduction is necessary to decrease the computational complexity.","The random projections open up new ways of dimensionality reduction, especially for large data sets.","In this paper, the principal component analysis (PCA) and randomized principal component analysis (R-PCA) for the classification of hyperspectral images using support vector machines (SVM) and light gradient boosting machines (LightGBM) have been investigated.","In this experimental research, the number of features was reduced to 20 and 30 for classification of two hyperspectral datasets (Indian Pines and Pavia University).","The experimental results demonstrated that PCA outperformed R-PCA for SVM for both datasets, but received close accuracy values for LightGBM.","The highest classification accuracies were obtained as 0.9925 and 0.9639 by LightGBM with original features for the Pavia University and Indian Pines, respectively."],"url":"http://arxiv.org/abs/2403.09117v1","category":"eess.IV"}
{"created":"2024-03-14 05:37:33","title":"Frustrated Quantum Magnetism on Complex Networks: What Sets the Total Spin","abstract":"Consider equal antiferromagnetic Heisenberg interactions between qubits sitting at the nodes of a complex, nonbipartite network. We ask the question: How does the network topology determine the net magnetization of the ground state and to what extent is it tunable? By examining various network families with tunable properties, we demonstrate that (i) graph heterogeneity, i.e., spread in the number of neighbors, is essential for a nonzero total spin, and (ii) other than the average number of neighbors, the key structure governing the total spin is the presence of (disassortative) hubs, as opposed to the level of frustration. We also show how to construct simple networks where the magnetization can be tuned over its entire range across both abrupt and continuous transitions, which may be realizable on existing platforms. Our findings pose a number of fundamental questions and strongly motivate wider exploration of quantum many-body phenomena beyond regular lattices.","sentences":["Consider equal antiferromagnetic Heisenberg interactions between qubits sitting at the nodes of a complex, nonbipartite network.","We ask the question: How does the network topology determine the net magnetization of the ground state and to what extent is it tunable?","By examining various network families with tunable properties, we demonstrate that (i) graph heterogeneity, i.e., spread in the number of neighbors, is essential for a nonzero total spin, and (ii) other than the average number of neighbors, the key structure governing the total spin is the presence of (disassortative) hubs, as opposed to the level of frustration.","We also show how to construct simple networks where the magnetization can be tuned over its entire range across both abrupt and continuous transitions, which may be realizable on existing platforms.","Our findings pose a number of fundamental questions and strongly motivate wider exploration of quantum many-body phenomena beyond regular lattices."],"url":"http://arxiv.org/abs/2403.09116v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-14 05:19:34","title":"Partitioning Distribution Networks for Integrated Electrification Planning","abstract":"In many developing countries, access to electricity remains a significant challenge. Electrification planners in these countries often have to make important decisions on the mode of electrification and the planning of electrical networks for those without access, while under resource constraints. An integrated approach to electrification planning in which traditional grid electrification is complemented off-the-grid technologies such as off-grid microgrids and stand-alone systems can enable the economic provision of electricity access in these regions. This integrated planning approach can be facilitated by determining the least-cost mode of electrification - i.e by electric grid extension or off-grid systems - for non-electrified consumers in a region under analysis, while considering technical, economic and environmental constraints. Computational clustering methods the identification of consumer clusters (either as clusters of off-grid microgrids, stand-alone systems or grid-extension projects) can be undertaken using computational clustering methods. This paper presents a novel computational approach to achieve this purpose. This methodology involves exploiting the grid network that connects all consumers, by greedily partitioning the network to identify clusters of consumers to be electrified by grid-extension and off-grid microgrid systems. Using test cases and sensitivity analyses, we implement and benchmark this top-down approach with those obtained from a bottom-up clustering methodology used by the Reference Electrification Model, a model obtainable in literature. Results presented show that the alternative top-down methodology proposed can compare favorably, in terms of global electrification costs, with a bottom-up approach to rural electrification planning.","sentences":["In many developing countries, access to electricity remains a significant challenge.","Electrification planners in these countries often have to make important decisions on the mode of electrification and the planning of electrical networks for those without access, while under resource constraints.","An integrated approach to electrification planning in which traditional grid electrification is complemented off-the-grid technologies such as off-grid microgrids and stand-alone systems can enable the economic provision of electricity access in these regions.","This integrated planning approach can be facilitated by determining the least-cost mode of electrification - i.e by electric grid extension or off-grid systems - for non-electrified consumers in a region under analysis, while considering technical, economic and environmental constraints.","Computational clustering methods the identification of consumer clusters (either as clusters of off-grid microgrids, stand-alone systems or grid-extension projects) can be undertaken using computational clustering methods.","This paper presents a novel computational approach to achieve this purpose.","This methodology involves exploiting the grid network that connects all consumers, by greedily partitioning the network to identify clusters of consumers to be electrified by grid-extension and off-grid microgrid systems.","Using test cases and sensitivity analyses, we implement and benchmark this top-down approach with those obtained from a bottom-up clustering methodology used by the Reference Electrification Model, a model obtainable in literature.","Results presented show that the alternative top-down methodology proposed can compare favorably, in terms of global electrification costs, with a bottom-up approach to rural electrification planning."],"url":"http://arxiv.org/abs/2403.09111v1","category":"eess.SY"}
{"created":"2024-03-14 05:17:39","title":"SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning","abstract":"Deep reinforcement learning (DRL) has shown significant promise for uncovering sophisticated control policies that interact in environments with complicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak fusion reactor or minimizing the drag force exerted on an object in a fluid flow. However, these algorithms require an abundance of training examples and may become prohibitively expensive for many applications. In addition, the reliance on deep neural networks often results in an uninterpretable, black-box policy that may be too computationally expensive to use with certain embedded systems. Recent advances in sparse dictionary learning, such as the sparse identification of nonlinear dynamics (SINDy), have shown promise for creating efficient and interpretable data-driven models in the low-data regime. In this work we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to create efficient, interpretable, and trustworthy representations of the dynamics model, reward function, and control policy. We demonstrate the effectiveness of our approaches on benchmark control environments and challenging fluids problems. SINDy-RL achieves comparable performance to state-of-the-art DRL algorithms using significantly fewer interactions in the environment and results in an interpretable control policy orders of magnitude smaller than a deep neural network policy.","sentences":["Deep reinforcement learning (DRL) has shown significant promise for uncovering sophisticated control policies that interact in environments with complicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak fusion reactor or minimizing the drag force exerted on an object in a fluid flow.","However, these algorithms require an abundance of training examples and may become prohibitively expensive for many applications.","In addition, the reliance on deep neural networks often results in an uninterpretable, black-box policy that may be too computationally expensive to use with certain embedded systems.","Recent advances in sparse dictionary learning, such as the sparse identification of nonlinear dynamics (SINDy), have shown promise for creating efficient and interpretable data-driven models in the low-data regime.","In this work we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to create efficient, interpretable, and trustworthy representations of the dynamics model, reward function, and control policy.","We demonstrate the effectiveness of our approaches on benchmark control environments and challenging fluids problems.","SINDy-RL achieves comparable performance to state-of-the-art DRL algorithms using significantly fewer interactions in the environment and results in an interpretable control policy orders of magnitude smaller than a deep neural network policy."],"url":"http://arxiv.org/abs/2403.09110v1","category":"cs.LG"}
{"created":"2024-03-14 05:09:22","title":"Temperature and Tautomeric Effects in High-Resolution Oxygen 1s X-ray Photoelectron Spectroscopy of Purines and Pyrimidines","abstract":"Purines and pyrimidines, crucial building blocks in biological systems, have attracted significant interest across molecular physics, biochemistry, pharmacology, and chemistry. Extensive spectroscopies have been employed for characterization, while the temperature and potential tautomeric effects can complicate the interpretation of underlying physics and chemistry. Here, we conducted first-principles simulations to analyze the vibrationally-resolved O1s X-ray photoelectron spectra of 6 common biomolecules at different temperatures, comprising 3 purine (xanthine, caffeine, and hypoxanthine) and 3 pyrimidine (thymine, 5F-uracil, and uracil) derivatives, and the tautomeric effect of hypoxanthine at varying temperatures. Using both time-independent (TI) and time-dependent (TD) methods under the Franck-Condon approximation, we obtained theoretical spectra that exhibited excellent agreement with experiments. Our analysis of these systems, all featuring carbonyl oxygens, unveiled distinctive characteristics of oxygen in N-CO-N (O2) compared to that within a N-CO-C structure (O1), showcasing higher O1s binding energy and total vibrational reorganization energy. We observed small differences in the zero-point vibration energies between the core-ionized and ground states, indicating a weak Duschinsky rotation effect. We consistently found that O1s ionization resulted in elongation of the O*=C bond length. The TI method facilitated the assignment of experimental spectra to different atoms or tautomers, where the atom-specific vibronic profiles of all 6 molecules exhibited similarity, with the 0-2 transitions dominating. TD enabled a more comprehensive exploration of the temperature effect, and the tautomeric effect of hypoxanthine by incorporating the Boltzmann population ratios of tautomers. We observed significant temperature dependence in the vibronic features present in these spectra.","sentences":["Purines and pyrimidines, crucial building blocks in biological systems, have attracted significant interest across molecular physics, biochemistry, pharmacology, and chemistry.","Extensive spectroscopies have been employed for characterization, while the temperature and potential tautomeric effects can complicate the interpretation of underlying physics and chemistry.","Here, we conducted first-principles simulations to analyze the vibrationally-resolved O1s X-ray photoelectron spectra of 6 common biomolecules at different temperatures, comprising 3 purine (xanthine, caffeine, and hypoxanthine) and 3 pyrimidine (thymine, 5F-uracil, and uracil) derivatives, and the tautomeric effect of hypoxanthine at varying temperatures.","Using both time-independent (TI) and time-dependent (TD) methods under the Franck-Condon approximation, we obtained theoretical spectra that exhibited excellent agreement with experiments.","Our analysis of these systems, all featuring carbonyl oxygens, unveiled distinctive characteristics of oxygen in N-CO-N (O2) compared to that within a N-CO-C structure (O1), showcasing higher O1s binding energy and total vibrational reorganization energy.","We observed small differences in the zero-point vibration energies between the core-ionized and ground states, indicating a weak Duschinsky rotation effect.","We consistently found that O1s ionization resulted in elongation of the O*=C bond length.","The TI method facilitated the assignment of experimental spectra to different atoms or tautomers, where the atom-specific vibronic profiles of all 6 molecules exhibited similarity, with the 0-2 transitions dominating.","TD enabled a more comprehensive exploration of the temperature effect, and the tautomeric effect of hypoxanthine by incorporating the Boltzmann population ratios of tautomers.","We observed significant temperature dependence in the vibronic features present in these spectra."],"url":"http://arxiv.org/abs/2403.09109v1","category":"physics.chem-ph"}
{"created":"2024-03-14 05:01:31","title":"CardioCaps: Attention-based Capsule Network for Class-Imbalanced Echocardiogram Classification","abstract":"Capsule Neural Networks (CapsNets) is a novel architecture that utilizes vector-wise representations formed by multiple neurons. Specifically, the Dynamic Routing CapsNets (DR-CapsNets) employ an affine matrix and dynamic routing mechanism to train capsules and acquire translation-equivariance properties, enhancing its robustness compared to traditional Convolutional Neural Networks (CNNs). Echocardiograms, which capture moving images of the heart, present unique challenges for traditional image classification methods. In this paper, we explore the potential of DR-CapsNets and propose CardioCaps, a novel attention-based DR-CapsNet architecture for class-imbalanced echocardiogram classification. CardioCaps comprises two key components: a weighted margin loss incorporating a regression auxiliary loss and an attention mechanism. First, the weighted margin loss prioritizes positive cases, supplemented by an auxiliary loss function based on the Ejection Fraction (EF) regression task, a crucial measure of cardiac function. This approach enhances the model's resilience in the face of class imbalance. Second, recognizing the quadratic complexity of dynamic routing leading to training inefficiencies, we adopt the attention mechanism as a more computationally efficient alternative. Our results demonstrate that CardioCaps surpasses traditional machine learning baseline methods, including Logistic Regression, Random Forest, and XGBoost with sampling methods and a class weight matrix. Furthermore, CardioCaps outperforms other deep learning baseline methods such as CNNs, ResNets, U-Nets, and ViTs, as well as advanced CapsNets methods such as EM-CapsNets and Efficient-CapsNets. Notably, our model demonstrates robustness to class imbalance, achieving high precision even in datasets with a substantial proportion of negative cases.","sentences":["Capsule Neural Networks (CapsNets) is a novel architecture that utilizes vector-wise representations formed by multiple neurons.","Specifically, the Dynamic Routing CapsNets (DR-CapsNets) employ an affine matrix and dynamic routing mechanism to train capsules and acquire translation-equivariance properties, enhancing its robustness compared to traditional Convolutional Neural Networks (CNNs).","Echocardiograms, which capture moving images of the heart, present unique challenges for traditional image classification methods.","In this paper, we explore the potential of DR-CapsNets and propose CardioCaps, a novel attention-based DR-CapsNet architecture for class-imbalanced echocardiogram classification.","CardioCaps comprises two key components: a weighted margin loss incorporating a regression auxiliary loss and an attention mechanism.","First, the weighted margin loss prioritizes positive cases, supplemented by an auxiliary loss function based on the Ejection Fraction (EF) regression task, a crucial measure of cardiac function.","This approach enhances the model's resilience in the face of class imbalance.","Second, recognizing the quadratic complexity of dynamic routing leading to training inefficiencies, we adopt the attention mechanism as a more computationally efficient alternative.","Our results demonstrate that CardioCaps surpasses traditional machine learning baseline methods, including Logistic Regression, Random Forest, and XGBoost with sampling methods and a class weight matrix.","Furthermore, CardioCaps outperforms other deep learning baseline methods such as CNNs, ResNets, U-Nets, and ViTs, as well as advanced CapsNets methods such as EM-CapsNets and Efficient-CapsNets.","Notably, our model demonstrates robustness to class imbalance, achieving high precision even in datasets with a substantial proportion of negative cases."],"url":"http://arxiv.org/abs/2403.09108v1","category":"cs.CV"}
{"created":"2024-03-14 04:51:17","title":"Alternant Hydrocarbon Diradicals as Optically Addressable Molecular Qubits","abstract":"High-spin molecules allow for bottom-up qubit design and are promising platforms for magnetic sensing and quantum information science. Optical addressability of molecular electron spins has also been proposed in first-row transition metal complexes via optically-detected magnetic resonance (ODMR) mechanisms analogous to the diamond-NV colour centre. However, significantly less progress has been made on the front of metal-free molecules, which can deliver lower costs and milder environmental impacts. At present, most luminescent open-shell organic molecules are {\\pi}-diradicals, but such systems often suffer from poor ground-state open-shell characters necessary to realise a stable molecular qubit. In this work, we propose the use of alternancy symmetry to selectively minimise radical-radical interactions in the ground state, generating {\\pi}-systems with high diradical characters. We call them m-dimers, referencing the need to covalently link two benzylic radicals at their meta carbon atoms for the desired symmetry. Through a detailed electronic structure analysis, we find that the excited states of alternant hydrocarbon m-diradicals contain important symmetries that can be used to construct ODMR mechanisms. The molecular parameters are set in the context of a tris(2,4,6-trichlorophenyl)methyl (TTM) radical dimer covalently tethered at the meta position, demonstrating the feasibility of realising a molecular colour centre with alternant {\\pi}-diradicals.","sentences":["High-spin molecules allow for bottom-up qubit design and are promising platforms for magnetic sensing and quantum information science.","Optical addressability of molecular electron spins has also been proposed in first-row transition metal complexes via optically-detected magnetic resonance (ODMR) mechanisms analogous to the diamond-NV colour centre.","However, significantly less progress has been made on the front of metal-free molecules, which can deliver lower costs and milder environmental impacts.","At present, most luminescent open-shell organic molecules are {\\pi}-diradicals, but such systems often suffer from poor ground-state open-shell characters necessary to realise a stable molecular qubit.","In this work, we propose the use of alternancy symmetry to selectively minimise radical-radical interactions in the ground state, generating {\\pi}-systems with high diradical characters.","We call them m-dimers, referencing the need to covalently link two benzylic radicals at their meta carbon atoms for the desired symmetry.","Through a detailed electronic structure analysis, we find that the excited states of alternant hydrocarbon m-diradicals contain important symmetries that can be used to construct ODMR mechanisms.","The molecular parameters are set in the context of a tris(2,4,6-trichlorophenyl)methyl (TTM) radical dimer covalently tethered at the meta position, demonstrating the feasibility of realising a molecular colour centre with alternant {\\pi}-diradicals."],"url":"http://arxiv.org/abs/2403.09102v1","category":"physics.chem-ph"}
{"created":"2024-03-14 04:48:06","title":"Virtual birefringence imaging and histological staining of amyloid deposits in label-free tissue using autofluorescence microscopy and deep learning","abstract":"Systemic amyloidosis is a group of diseases characterized by the deposition of misfolded proteins in various organs and tissues, leading to progressive organ dysfunction and failure. Congo red stain is the gold standard chemical stain for the visualization of amyloid deposits in tissue sections, as it forms complexes with the misfolded proteins and shows a birefringence pattern under polarized light microscopy. However, Congo red staining is tedious and costly to perform, and prone to false diagnoses due to variations in the amount of amyloid, staining quality and expert interpretation through manual examination of tissue under a polarization microscope. Here, we report the first demonstration of virtual birefringence imaging and virtual Congo red staining of label-free human tissue to show that a single trained neural network can rapidly transform autofluorescence images of label-free tissue sections into brightfield and polarized light microscopy equivalent images, matching the histochemically stained versions of the same samples. We demonstrate the efficacy of our method with blind testing and pathologist evaluations on cardiac tissue where the virtually stained images agreed well with the histochemically stained ground truth images. Our virtually stained polarization and brightfield images highlight amyloid birefringence patterns in a consistent, reproducible manner while mitigating diagnostic challenges due to variations in the quality of chemical staining and manual imaging processes as part of the clinical workflow.","sentences":["Systemic amyloidosis is a group of diseases characterized by the deposition of misfolded proteins in various organs and tissues, leading to progressive organ dysfunction and failure.","Congo red stain is the gold standard chemical stain for the visualization of amyloid deposits in tissue sections, as it forms complexes with the misfolded proteins and shows a birefringence pattern under polarized light microscopy.","However, Congo red staining is tedious and costly to perform, and prone to false diagnoses due to variations in the amount of amyloid, staining quality and expert interpretation through manual examination of tissue under a polarization microscope.","Here, we report the first demonstration of virtual birefringence imaging and virtual Congo red staining of label-free human tissue to show that a single trained neural network can rapidly transform autofluorescence images of label-free tissue sections into brightfield and polarized light microscopy equivalent images, matching the histochemically stained versions of the same samples.","We demonstrate the efficacy of our method with blind testing and pathologist evaluations on cardiac tissue where the virtually stained images agreed well with the histochemically stained ground truth images.","Our virtually stained polarization and brightfield images highlight amyloid birefringence patterns in a consistent, reproducible manner while mitigating diagnostic challenges due to variations in the quality of chemical staining and manual imaging processes as part of the clinical workflow."],"url":"http://arxiv.org/abs/2403.09100v1","category":"physics.med-ph"}
