{"created":"2024-05-07 17:59:50","title":"Tactile-Augmented Radiance Fields","abstract":"We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space. This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene. We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes. Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features. We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal. To evaluate our approach, we collect a dataset of TaRFs. This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal. We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks. Project page: https://dou-yiming.github.io/TaRF","sentences":["We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space.","This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene.","We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes.","Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features.","We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal.","To evaluate our approach, we collect a dataset of TaRFs.","This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal.","We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks.","Project page: https://dou-yiming.github.io/TaRF"],"url":"http://arxiv.org/abs/2405.04534v1","category":"cs.CV"}
{"created":"2024-05-07 17:59:31","title":"ChatHuman: Language-driven 3D Human Understanding with Retrieval-Augmented Tool Reasoning","abstract":"Numerous methods have been proposed to detect, estimate, and analyze properties of people in images, including the estimation of 3D pose, shape, contact, human-object interaction, emotion, and more. Each of these methods works in isolation instead of synergistically. Here we address this problem and build a language-driven human understanding system -- ChatHuman, which combines and integrates the skills of many different methods. To do so, we finetune a Large Language Model (LLM) to select and use a wide variety of existing tools in response to user inputs. In doing so, ChatHuman is able to combine information from multiple tools to solve problems more accurately than the individual tools themselves and to leverage tool output to improve its ability to reason about humans. The novel features of ChatHuman include leveraging academic publications to guide the application of 3D human-related tools, employing a retrieval-augmented generation model to generate in-context-learning examples for handling new tools, and discriminating and integrating tool results to enhance 3D human understanding. Our experiments show that ChatHuman outperforms existing models in both tool selection accuracy and performance across multiple 3D human-related tasks. ChatHuman is a step towards consolidating diverse methods for human analysis into a single, powerful, system for 3D human reasoning.","sentences":["Numerous methods have been proposed to detect, estimate, and analyze properties of people in images, including the estimation of 3D pose, shape, contact, human-object interaction, emotion, and more.","Each of these methods works in isolation instead of synergistically.","Here we address this problem and build a language-driven human understanding system -- ChatHuman, which combines and integrates the skills of many different methods.","To do so, we finetune a Large Language Model (LLM) to select and use a wide variety of existing tools in response to user inputs.","In doing so, ChatHuman is able to combine information from multiple tools to solve problems more accurately than the individual tools themselves and to leverage tool output to improve its ability to reason about humans.","The novel features of ChatHuman include leveraging academic publications to guide the application of 3D human-related tools, employing a retrieval-augmented generation model to generate in-context-learning examples for handling new tools, and discriminating and integrating tool results to enhance 3D human understanding.","Our experiments show that ChatHuman outperforms existing models in both tool selection accuracy and performance across multiple 3D human-related tasks.","ChatHuman is a step towards consolidating diverse methods for human analysis into a single, powerful, system for 3D human reasoning."],"url":"http://arxiv.org/abs/2405.04533v1","category":"cs.CV"}
{"created":"2024-05-07 17:59:30","title":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving","abstract":"Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.","sentences":["Quantization can accelerate large language model (LLM) inference.","Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4.","Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving.","We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs.","To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache.","QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin.","QoQ is implemented by the QServe inference library that achieves measured speedup.","The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores.","Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM.","Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization.","In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency.","We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization.","As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM.","Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100.","Thus, QServe effectively reduces the dollar cost of LLM serving by 3x.","Code is available at https://github.com/mit-han-lab/qserve."],"url":"http://arxiv.org/abs/2405.04532v1","category":"cs.CL"}
{"created":"2024-05-07 17:57:31","title":"PoW Security-Latency under Random Delays and the Effect of Transaction Fees","abstract":"Safety guarantees and security-latency problem of Nakamoto consensus have been extensively studied in the last decade with a bounded delay model. Recent studies have shown that PoW protocol is secure under random delay models as well. In this paper, we analyze the security-latency problem, i.e., how secure a block is, after it becomes k-deep in the blockchain, under general random delay distributions. We provide tight and explicit bounds which only require determining the distribution of the number of Poisson arrivals during the random delay. We further consider potential effects of recent Bitcoin halving on the security-latency problem by extending our results.","sentences":["Safety guarantees and security-latency problem of Nakamoto consensus have been extensively studied in the last decade with a bounded delay model.","Recent studies have shown that PoW protocol is secure under random delay models as well.","In this paper, we analyze the security-latency problem, i.e., how secure a block is, after it becomes k-deep in the blockchain, under general random delay distributions.","We provide tight and explicit bounds which only require determining the distribution of the number of Poisson arrivals during the random delay.","We further consider potential effects of recent Bitcoin halving on the security-latency problem by extending our results."],"url":"http://arxiv.org/abs/2405.04526v1","category":"cs.CR"}
{"created":"2024-05-07 17:56:12","title":"Neural network based deep learning analysis of semiconductor quantum dot qubits for automated control","abstract":"Machine learning offers a largely unexplored avenue for improving noisy disordered devices in physics using automated algorithms. Through simulations that include disorder in physical devices, particularly quantum devices, there is potential to learn about disordered landscapes and subsequently tune devices based on those insights. In this work, we introduce a novel methodology that employs machine learning, specifically convolutional neural networks (CNNs), to discern the disorder landscape in the parameters of the disordered extended Hubbard model underlying the semiconductor quantum dot spin qubit architectures. This technique takes advantage of experimentally obtainable charge stability diagrams from neighboring quantum dot pairs, enabling the CNN to accurately identify disorder in each parameter of the extended Hubbard model. Remarkably, our CNN can process site-specific disorder in Hubbard parameters, including variations in hopping constants, on-site potentials (gate voltages), and both intra-site and inter-site Coulomb terms. This advancement facilitates the prediction of spatially dependent disorder across all parameters simultaneously with high accuracy ($R^2>0.994$) and fewer parameter constraints, marking a significant improvement over previous methods that were focused only on analyzing on-site potentials at low coupling. Furthermore, our approach allows for the tuning of five or more quantum dots at a time, effectively addressing the often-overlooked issue of crosstalk. Not only does our method streamline the tuning process, potentially enabling fully automated adjustments, but it also introduces a \"no trust\" verification method to rigorously validate the neural network's predictions. Ultimately, this work aims to lay the groundwork for generalizing our method to tackle a broad spectrum of physical problems.","sentences":["Machine learning offers a largely unexplored avenue for improving noisy disordered devices in physics using automated algorithms.","Through simulations that include disorder in physical devices, particularly quantum devices, there is potential to learn about disordered landscapes and subsequently tune devices based on those insights.","In this work, we introduce a novel methodology that employs machine learning, specifically convolutional neural networks (CNNs), to discern the disorder landscape in the parameters of the disordered extended Hubbard model underlying the semiconductor quantum dot spin qubit architectures.","This technique takes advantage of experimentally obtainable charge stability diagrams from neighboring quantum dot pairs, enabling the CNN to accurately identify disorder in each parameter of the extended Hubbard model.","Remarkably, our CNN can process site-specific disorder in Hubbard parameters, including variations in hopping constants, on-site potentials (gate voltages), and both intra-site and inter-site Coulomb terms.","This advancement facilitates the prediction of spatially dependent disorder across all parameters simultaneously with high accuracy ($R^2>0.994$) and fewer parameter constraints, marking a significant improvement over previous methods that were focused only on analyzing on-site potentials at low coupling.","Furthermore, our approach allows for the tuning of five or more quantum dots at a time, effectively addressing the often-overlooked issue of crosstalk.","Not only does our method streamline the tuning process, potentially enabling fully automated adjustments, but it also introduces a \"no trust\" verification method to rigorously validate the neural network's predictions.","Ultimately, this work aims to lay the groundwork for generalizing our method to tackle a broad spectrum of physical problems."],"url":"http://arxiv.org/abs/2405.04524v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-07 17:52:51","title":"NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts","abstract":"Large language models (LLMs) have manifested strong ability to generate codes for productive activities. However, current benchmarks for code synthesis, such as HumanEval, MBPP, and DS-1000, are predominantly oriented towards introductory tasks on algorithm and data science, insufficiently satisfying challenging requirements prevalent in real-world coding. To fill this gap, we propose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks. NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains. Noting the extraordinary difficulty in creating testing cases for real-world queries, we also introduce a semi-automated pipeline to enhance the efficiency of test case construction. Comparing with manual solutions, it achieves an efficiency increase of more than 4 times. Our systematic experiments on 39 LLMs find that performance gaps on NCB between models with close HumanEval scores could still be significant, indicating a lack of focus on practical code synthesis scenarios or over-specified optimization on HumanEval. On the other hand, even the best-performing GPT-4 is still far from satisfying on NCB. The evaluation toolkit and development set are available at https://github.com/THUDM/NaturalCodeBench.","sentences":["Large language models (LLMs) have manifested strong ability to generate codes for productive activities.","However, current benchmarks for code synthesis, such as HumanEval, MBPP, and DS-1000, are predominantly oriented towards introductory tasks on algorithm and data science, insufficiently satisfying challenging requirements prevalent in real-world coding.","To fill this gap, we propose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks.","NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains.","Noting the extraordinary difficulty in creating testing cases for real-world queries, we also introduce a semi-automated pipeline to enhance the efficiency of test case construction.","Comparing with manual solutions, it achieves an efficiency increase of more than 4 times.","Our systematic experiments on 39 LLMs find that performance gaps on NCB between models with close HumanEval scores could still be significant, indicating a lack of focus on practical code synthesis scenarios or over-specified optimization on HumanEval.","On the other hand, even the best-performing GPT-4 is still far from satisfying on NCB.","The evaluation toolkit and development set are available at https://github.com/THUDM/NaturalCodeBench."],"url":"http://arxiv.org/abs/2405.04520v1","category":"cs.CL"}
{"created":"2024-05-07 17:51:10","title":"Local Advice and Local Decompression","abstract":"Algorithms with advice have received ample attention in the distributed and online settings, and they have recently proven useful also in dynamic settings. In this work we study local computation with advice: the goal is to solve a graph problem $\\Pi$ with a distributed algorithm in $f(\\Delta)$ communication rounds, for some function $f$ that only depends on the maximum degree $\\Delta$ of the graph, and the key question is how many bits of advice per node are needed. Our main results are:   - Any locally checkable labeling problem can be solved in graphs with sub-exponential growth with only $1$ bit of advice per node. Moreover, we can make the set of nodes that carry advice bits arbitrarily sparse, that is, we can make arbitrarily small the ratio between nodes carrying a 1 and the nodes carrying a 0. - The assumption of sub-exponential growth is necessary: assuming the Exponential-Time Hypothesis, there are LCLs that cannot be solved in general with any constant number of bits per node. - In any graph we can find an almost-balanced orientation (indegrees and outdegrees differ by at most one) with $1$ bit of advice per node, and again we can make the advice arbitrarily sparse. - As a corollary, we can also compress an arbitrary subset of edges so that a node of degree $d$ stores only $d/2 + 2$ bits, and we can decompress it locally, in $f(\\Delta)$ rounds. - In any graph of maximum degree $\\Delta$, we can find a $\\Delta$-coloring (if it exists) with $1$ bit of advice per node, and again, we can make the advice arbitrarily sparse. - In any $3$-colorable graph, we can find a $3$-coloring with $1$ bit of advice per node. Here, it remains open whether we can make the advice arbitrarily sparse.   Our work shows that for many problems the key threshold is not whether we can achieve, say, $1$ bit of advice per node, but whether we can make the advice arbitrarily sparse.","sentences":["Algorithms with advice have received ample attention in the distributed and online settings, and they have recently proven useful also in dynamic settings.","In this work we study local computation with advice: the goal is to solve a graph problem $\\Pi$ with a distributed algorithm in $f(\\Delta)$ communication rounds, for some function $f$ that only depends on the maximum degree $\\Delta$ of the graph, and the key question is how many bits of advice per node are needed.","Our main results are:   - Any locally checkable labeling problem can be solved in graphs with sub-exponential growth with only $1$ bit of advice per node.","Moreover, we can make the set of nodes that carry advice bits arbitrarily sparse, that is, we can make arbitrarily small the ratio between nodes carrying a 1 and the nodes carrying a 0. -","The assumption of sub-exponential growth is necessary: assuming the Exponential-Time Hypothesis, there are LCLs that cannot be solved in general with any constant number of bits per node.","- In any graph we can find an almost-balanced orientation (indegrees and outdegrees differ by at most one) with $1$ bit of advice per node, and again we can make the advice arbitrarily sparse.","- As a corollary, we can also compress an arbitrary subset of edges so that a node of degree $d$ stores only $d/2 + 2$ bits, and we can decompress it locally, in $f(\\Delta)$ rounds.","-","In any graph of maximum degree $\\Delta$, we can find a $\\Delta$-coloring (if it exists) with $1$ bit of advice per node, and again, we can make the advice arbitrarily sparse.","- In any $3$-colorable graph, we can find a $3$-coloring with $1$ bit of advice per node.","Here, it remains open whether we can make the advice arbitrarily sparse.   ","Our work shows that for many problems the key threshold is not whether we can achieve, say, $1$ bit of advice per node, but whether we can make the advice arbitrarily sparse."],"url":"http://arxiv.org/abs/2405.04519v1","category":"cs.DC"}
{"created":"2024-05-07 17:50:21","title":"xLSTM: Extended Long Short-Term Memory","abstract":"In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.","sentences":["In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM).","Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs).","However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.","We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?","Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques.","Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule.","Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures.","Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling."],"url":"http://arxiv.org/abs/2405.04517v1","category":"cs.LG"}
{"created":"2024-05-07 17:49:12","title":"Disorder free many-body localization transition in two quasiperiodically coupled Heisenberg spin chains","abstract":"Disorder free many-body localization (MBL) can occur in interacting systems that can dynamically generate their own disorder. We address the thermal-MBL phase transition of two isotropic Heisenberg spin chains that are quasi-periodically coupled to each other. The spin chains are incommensurate and are coupled through a short range exchange interaction of the $XXZ$ type that decays exponentially with the distance. Using exact diagonalization, matrix product states and density matrix renormalization group, we calculate the time evolution of the entanglement entropy at long times and extract the inverse participation ratio in the thermodynamic limit. We show that this system has a robust MBL phase. We establish the phase diagram with the onset of MBL as a function of the interchain exchange coupling and of the incommensuration between the spin chains. The Ising limit of the interchain interaction optimizes the stability of the MBL phase over a broad range of incommensurations above a given critical exchange coupling. Incorporation of interchain spin flips significantly enhances entanglement between the spin chains and produces delocalization, favoring a pre-thermal phase whose entanglement entropy grows logarithmically with time.","sentences":["Disorder free many-body localization (MBL) can occur in interacting systems that can dynamically generate their own disorder.","We address the thermal-MBL phase transition of two isotropic Heisenberg spin chains that are quasi-periodically coupled to each other.","The spin chains are incommensurate and are coupled through a short range exchange interaction of the $XXZ$ type that decays exponentially with the distance.","Using exact diagonalization, matrix product states and density matrix renormalization group, we calculate the time evolution of the entanglement entropy at long times and extract the inverse participation ratio in the thermodynamic limit.","We show that this system has a robust MBL phase.","We establish the phase diagram with the onset of MBL as a function of the interchain exchange coupling and of the incommensuration between the spin chains.","The Ising limit of the interchain interaction optimizes the stability of the MBL phase over a broad range of incommensurations above a given critical exchange coupling.","Incorporation of interchain spin flips significantly enhances entanglement between the spin chains and produces delocalization, favoring a pre-thermal phase whose entanglement entropy grows logarithmically with time."],"url":"http://arxiv.org/abs/2405.04516v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-07 17:44:54","title":"Switchable Decision: Dynamic Neural Generation Networks","abstract":"Auto-regressive generation models achieve competitive performance across many different NLP tasks such as summarization, question answering, and classifications. However, they are also known for being slow in inference, which makes them challenging to deploy in real-time applications. We propose a switchable decision to accelerate inference by dynamically assigning computation resources for each data instance. Automatically making decisions on where to skip and how to balance quality and computation cost with constrained optimization, our dynamic neural generation networks enforce the efficient inference path and determine the optimized trade-off. Experiments across question answering, summarization, and classification benchmarks show that our method benefits from less computation cost during inference while keeping the same accuracy. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many NLP tasks.","sentences":["Auto-regressive generation models achieve competitive performance across many different NLP tasks such as summarization, question answering, and classifications.","However, they are also known for being slow in inference, which makes them challenging to deploy in real-time applications.","We propose a switchable decision to accelerate inference by dynamically assigning computation resources for each data instance.","Automatically making decisions on where to skip and how to balance quality and computation cost with constrained optimization, our dynamic neural generation networks enforce the efficient inference path and determine the optimized trade-off.","Experiments across question answering, summarization, and classification benchmarks show that our method benefits from less computation cost during inference while keeping the same accuracy.","Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many NLP tasks."],"url":"http://arxiv.org/abs/2405.04513v1","category":"cs.CL"}
{"created":"2024-05-07 17:38:59","title":"Large mechanical squeezing through synthetic magnetism in optomechanics","abstract":"We propose a scheme to generate large amount of mechanical squeezing, far beyond the $\\rm{3dB}$ limit, which is based on synthetic magnetism in optomechanical system that hosts a Backward Stimulated Brillouin Scattering (BSBS) process. Our benckmark system consists of an acoustic mode coupled to two optical modes through the BSBS process, and a Duffing mechanical oscillator that couples to the same optical modes through the standard optomechanical radiation pressure. The synthetic magnetism comes from the modulation of the mechanical coupling between the acoustic and the mechanical modes. When there is no synthetic magnetism, a given amount of mechanical squeezing is generated in the system. This squeezing is mainly dependent on the BSBS process, and it is fragile against thermal noise. By switching on the synthetic magnetism, the degree of the generated squeezing is greatly enhanced and goes far beyond the limit of the $\\rm{3dB}$. This large magnetism induced squeezing persists even when there is no BSBS process in the system. Moreover, this generated squeezing is robust enough against thermal noise in comparison to the one induced when the synthetic magnetism is off. Furthermore, both the mechanical variance squeezing and effective phonon number exhibit series of peaks and dips depending on the phase modulation of the mechanical coupling. This oscillatory feature is reminscent of a sudden death and revival of squeezing phenomenon, which can be used to maintain a desired magnitude of squeezing by tuning this phase. Our proposal provides a path toward a flexible scheme that generates large amount of squeezing, far beyond the $\\rm{3dB}$ limit. Such a generated squeezed states can be use for quantum applications including quantum information processing, quantum sensing and metrology, and quantum computing.","sentences":["We propose a scheme to generate large amount of mechanical squeezing, far beyond the $\\rm{3dB}$ limit, which is based on synthetic magnetism in optomechanical system that hosts a Backward Stimulated Brillouin Scattering (BSBS) process.","Our benckmark system consists of an acoustic mode coupled to two optical modes through the BSBS process, and a Duffing mechanical oscillator that couples to the same optical modes through the standard optomechanical radiation pressure.","The synthetic magnetism comes from the modulation of the mechanical coupling between the acoustic and the mechanical modes.","When there is no synthetic magnetism, a given amount of mechanical squeezing is generated in the system.","This squeezing is mainly dependent on the BSBS process, and it is fragile against thermal noise.","By switching on the synthetic magnetism, the degree of the generated squeezing is greatly enhanced and goes far beyond the limit of the $\\rm{3dB}$. This large magnetism induced squeezing persists even when there is no BSBS process in the system.","Moreover, this generated squeezing is robust enough against thermal noise in comparison to the one induced when the synthetic magnetism is off.","Furthermore, both the mechanical variance squeezing and effective phonon number exhibit series of peaks and dips depending on the phase modulation of the mechanical coupling.","This oscillatory feature is reminscent of a sudden death and revival of squeezing phenomenon, which can be used to maintain a desired magnitude of squeezing by tuning this phase.","Our proposal provides a path toward a flexible scheme that generates large amount of squeezing, far beyond the $\\rm{3dB}$ limit.","Such a generated squeezed states can be use for quantum applications including quantum information processing, quantum sensing and metrology, and quantum computing."],"url":"http://arxiv.org/abs/2405.04508v1","category":"quant-ph"}
{"created":"2024-05-07 17:35:14","title":"$\\mathrm{D}$ and $\\mathrm{D_s}$ decay constants in $N_{\\rm f}=2+1$ QCD with Wilson fermions","abstract":"We present results for the leptonic decay constants of the D and D$_{\\rm s}$ mesons from $N_{\\rm f}=2+1$ lattice QCD. We employ a set of 49 high statistics gauge ensembles generated by the Coordinated Lattice Simulations (CLS) effort utilising non-perturbatively improved Wilson fermions and the tree-level Symanzik improved gauge action at six values of the lattice spacing in the range $a = 0.098\\,$fm down to $a = 0.039\\,$fm, with pion masses varying from around $420\\,$MeV down to below the physical point. The ensembles lie on three trajectories in the quark mass plane, two trajectories intersecting close to the physical quark mass point and the third one approaching the SU(3) chiral limit, enabling tight control of the light and strange quark mass dependence. We obtain $f_{\\mathrm{D_s}}=246.8(1.3)\\,$MeV, $f_\\mathrm{D}=208.4(1.5)\\,$MeV and $f_{\\mathrm{D_s}}/f_\\mathrm{D}=1.1842(36)$, where the precision of our results is mostly limited by the determination of the scale.","sentences":["We present results for the leptonic decay constants of the D and D$_{\\rm s}$ mesons from $N_{\\rm f}=2+1$ lattice QCD.","We employ a set of 49 high statistics gauge ensembles generated by the Coordinated Lattice Simulations (CLS) effort utilising non-perturbatively improved Wilson fermions and the tree-level Symanzik improved gauge action at six values of the lattice spacing in the range $a = 0.098\\,$fm down to $a = 0.039\\,$fm, with pion masses varying from around $420\\,$MeV down to below the physical point.","The ensembles lie on three trajectories in the quark mass plane, two trajectories intersecting close to the physical quark mass point and the third one approaching the SU(3) chiral limit, enabling tight control of the light and strange quark mass dependence.","We obtain $f_{\\mathrm{D_s}}=246.8(1.3)\\,$MeV, $f_\\mathrm{D}=208.4(1.5)\\,$MeV and $f_{\\mathrm{D_s}}/f_\\mathrm{D}=1.1842(36)$, where the precision of our results is mostly limited by the determination of the scale."],"url":"http://arxiv.org/abs/2405.04506v1","category":"hep-lat"}
{"created":"2024-05-07 17:14:42","title":"Generative Planning with Fast Collision Checks for High Speed Navigation","abstract":"Reasoning about large numbers of diverse plans to achieve high speed navigation in cluttered environments remains a challenge for robotic systems even in the case of perfect perceptual information. Often, this is tackled by methods that iteratively optimize around a prior seeded trajectory and consequently restrict to local optima. We present a novel planning method using normalizing flows (NFs) to encode expert-styled motion primitives. We also present an accelerated collision checking framework that enables rejecting samples from the prior distribution before running them through the NF model for rapid sampling of collision-free trajectories. The choice of an NF as the generator permits a flexible way to encode diverse multi-modal behavior distributions while maintaining a smooth relation to the input space which allows approximating collision checks on NF inputs rather than outputs. We show comparable performance to model predictive path integral control in random cluttered environments and improved exit rates in a cul-de-sac environment. We conclude by discussing our plans for future work to improve both safety and performance of our controller.","sentences":["Reasoning about large numbers of diverse plans to achieve high speed navigation in cluttered environments remains a challenge for robotic systems even in the case of perfect perceptual information.","Often, this is tackled by methods that iteratively optimize around a prior seeded trajectory and consequently restrict to local optima.","We present a novel planning method using normalizing flows (NFs) to encode expert-styled motion primitives.","We also present an accelerated collision checking framework that enables rejecting samples from the prior distribution before running them through the NF model for rapid sampling of collision-free trajectories.","The choice of an NF as the generator permits a flexible way to encode diverse multi-modal behavior distributions while maintaining a smooth relation to the input space which allows approximating collision checks on NF inputs rather than outputs.","We show comparable performance to model predictive path integral control in random cluttered environments and improved exit rates in a cul-de-sac environment.","We conclude by discussing our plans for future work to improve both safety and performance of our controller."],"url":"http://arxiv.org/abs/2405.04498v1","category":"cs.RO"}
{"created":"2024-05-07 17:10:31","title":"Unveiling Disparities in Web Task Handling Between Human and Web Agent","abstract":"With the advancement of Large-Language Models (LLMs) and Large Vision-Language Models (LVMs), agents have shown significant capabilities in various tasks, such as data analysis, gaming, or code generation. Recently, there has been a surge in research on web agents, capable of performing tasks within the web environment. However, the web poses unforeseeable scenarios, challenging the generalizability of these agents. This study investigates the disparities between human and web agents' performance in web tasks (e.g., information search) by concentrating on planning, action, and reflection aspects during task execution. We conducted a web task study with a think-aloud protocol, revealing distinct cognitive actions and operations on websites employed by humans. Comparative examination of existing agent structures and human behavior with thought processes highlighted differences in knowledge updating and ambiguity handling when performing the task. Humans demonstrated a propensity for exploring and modifying plans based on additional information and investigating reasons for failure. These findings offer insights into designing planning, reflection, and information discovery modules for web agents and designing the capturing method for implicit human knowledge in a web task.","sentences":["With the advancement of Large-Language Models (LLMs) and Large Vision-Language Models (LVMs), agents have shown significant capabilities in various tasks, such as data analysis, gaming, or code generation.","Recently, there has been a surge in research on web agents, capable of performing tasks within the web environment.","However, the web poses unforeseeable scenarios, challenging the generalizability of these agents.","This study investigates the disparities between human and web agents' performance in web tasks (e.g., information search) by concentrating on planning, action, and reflection aspects during task execution.","We conducted a web task study with a think-aloud protocol, revealing distinct cognitive actions and operations on websites employed by humans.","Comparative examination of existing agent structures and human behavior with thought processes highlighted differences in knowledge updating and ambiguity handling when performing the task.","Humans demonstrated a propensity for exploring and modifying plans based on additional information and investigating reasons for failure.","These findings offer insights into designing planning, reflection, and information discovery modules for web agents and designing the capturing method for implicit human knowledge in a web task."],"url":"http://arxiv.org/abs/2405.04497v1","category":"cs.HC"}
{"created":"2024-05-07 17:06:59","title":"Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video Motion Editing","abstract":"Existing diffusion-based video editing methods have achieved impressive results in motion editing. Most of the existing methods focus on the motion alignment between the edited video and the reference video. However, these methods do not constrain the background and object content of the video to remain unchanged, which makes it possible for users to generate unexpected videos. In this paper, we propose a one-shot video motion editing method called Edit-Your-Motion that requires only a single text-video pair for training. Specifically, we design the Detailed Prompt-Guided Learning Strategy (DPL) to decouple spatio-temporal features in space-time diffusion models. DPL separates learning object content and motion into two training stages. In the first training stage, we focus on learning the spatial features (the features of object content) and breaking down the temporal relationships in the video frames by shuffling them. We further propose Recurrent-Causal Attention (RC-Attn) to learn the consistent content features of the object from unordered video frames. In the second training stage, we restore the temporal relationship in video frames to learn the temporal feature (the features of the background and object's motion). We also adopt the Noise Constraint Loss to smooth out inter-frame differences. Finally, in the inference stage, we inject the content features of the source object into the editing branch through a two-branch structure (editing branch and reconstruction branch). With Edit-Your-Motion, users can edit the motion of objects in the source video to generate more exciting and diverse videos. Comprehensive qualitative experiments, quantitative experiments and user preference studies demonstrate that Edit-Your-Motion performs better than other methods.","sentences":["Existing diffusion-based video editing methods have achieved impressive results in motion editing.","Most of the existing methods focus on the motion alignment between the edited video and the reference video.","However, these methods do not constrain the background and object content of the video to remain unchanged, which makes it possible for users to generate unexpected videos.","In this paper, we propose a one-shot video motion editing method called Edit-Your-Motion that requires only a single text-video pair for training.","Specifically, we design the Detailed Prompt-Guided Learning Strategy (DPL) to decouple spatio-temporal features in space-time diffusion models.","DPL separates learning object content and motion into two training stages.","In the first training stage, we focus on learning the spatial features (the features of object content) and breaking down the temporal relationships in the video frames by shuffling them.","We further propose Recurrent-Causal Attention (RC-Attn) to learn the consistent content features of the object from unordered video frames.","In the second training stage, we restore the temporal relationship in video frames to learn the temporal feature (the features of the background and object's motion).","We also adopt the Noise Constraint Loss to smooth out inter-frame differences.","Finally, in the inference stage, we inject the content features of the source object into the editing branch through a two-branch structure (editing branch and reconstruction branch).","With Edit-Your-Motion, users can edit the motion of objects in the source video to generate more exciting and diverse videos.","Comprehensive qualitative experiments, quantitative experiments and user preference studies demonstrate that Edit-Your-Motion performs better than other methods."],"url":"http://arxiv.org/abs/2405.04496v1","category":"cs.CV"}
{"created":"2024-05-07 17:05:27","title":"Toward In-Context Teaching: Adapting Examples to Students' Misconceptions","abstract":"When a teacher provides examples for a student to study, these examples must be informative, enabling a student to progress from their current state toward a target concept or skill. Good teachers must therefore simultaneously infer what students already know and adapt their teaching to students' changing state of knowledge. There is increasing interest in using computational models, particularly large language models, as pedagogical tools. As students, language models in particular have shown a remarkable ability to adapt to new tasks given small numbers of examples. But how effectively can these models adapt as teachers to students of different types? To study this question, we introduce a suite of models and evaluation methods we call AdapT. AdapT has two components: (1) a collection of simulated Bayesian student models that can be used for evaluation of automated teaching methods; (2) a platform for evaluation with human students, to characterize the real-world effectiveness of these methods. We additionally introduce (3) AToM, a new probabilistic model for adaptive teaching that jointly infers students' past beliefs and optimizes for the correctness of future beliefs. In evaluations of simulated students across three learning domains (fraction arithmetic, English morphology, function learning), AToM systematically outperforms LLM-based and standard Bayesian teaching models. In human experiments, both AToM and LLMs outperform non-adaptive random example selection. Our results highlight both the difficulty of the adaptive teaching task and the potential of learned adaptive models for solving it.","sentences":["When a teacher provides examples for a student to study, these examples must be informative, enabling a student to progress from their current state toward a target concept or skill.","Good teachers must therefore simultaneously infer what students already know and adapt their teaching to students' changing state of knowledge.","There is increasing interest in using computational models, particularly large language models, as pedagogical tools.","As students, language models in particular have shown a remarkable ability to adapt to new tasks given small numbers of examples.","But how effectively can these models adapt as teachers to students of different types?","To study this question, we introduce a suite of models and evaluation methods we call AdapT. AdapT has two components: (1) a collection of simulated Bayesian student models that can be used for evaluation of automated teaching methods; (2) a platform for evaluation with human students, to characterize the real-world effectiveness of these methods.","We additionally introduce (3) AToM, a new probabilistic model for adaptive teaching that jointly infers students' past beliefs and optimizes for the correctness of future beliefs.","In evaluations of simulated students across three learning domains (fraction arithmetic, English morphology, function learning), AToM systematically outperforms LLM-based and standard Bayesian teaching models.","In human experiments, both AToM and LLMs outperform non-adaptive random example selection.","Our results highlight both the difficulty of the adaptive teaching task and the potential of learned adaptive models for solving it."],"url":"http://arxiv.org/abs/2405.04495v1","category":"cs.CL"}
{"created":"2024-05-07 17:03:46","title":"Probability of Presence Versus $\u03c8(x,t)^* \u03c8(x, t)$","abstract":"Postulating the identification of $\\psi^*(x, t) \\psi(x,t)$ with a physical probability density is unsatisfactory conceptually and overly limited practically. For electrons, there is a simple, calculable relativistic correction proportional to $\\nabla \\psi^* \\cdot \\nabla \\psi$. In particular, zeroes of the wave function do not indicate vanishing probability density of presence. Effects of this kind arise generically in Lagrangian-based theories implementing the particle concept.","sentences":["Postulating the identification of $\\psi^*(x, t) \\psi(x,t)$ with a physical probability density is unsatisfactory conceptually and overly limited practically.","For electrons, there is a simple, calculable relativistic correction proportional to $\\nabla \\psi^*","\\cdot \\nabla \\psi$.","In particular, zeroes of the wave function do not indicate vanishing probability density of presence.","Effects of this kind arise generically in Lagrangian-based theories implementing the particle concept."],"url":"http://arxiv.org/abs/2405.04493v1","category":"quant-ph"}
{"created":"2024-05-07 17:02:02","title":"TorchDriveEnv: A Reinforcement Learning Benchmark for Autonomous Driving with Reactive, Realistic, and Diverse Non-Playable Characters","abstract":"The training, testing, and deployment, of autonomous vehicles requires realistic and efficient simulators. Moreover, because of the high variability between different problems presented in different autonomous systems, these simulators need to be easy to use, and easy to modify. To address these problems we introduce TorchDriveSim and its benchmark extension TorchDriveEnv. TorchDriveEnv is a lightweight reinforcement learning benchmark programmed entirely in Python, which can be modified to test a number of different factors in learned vehicle behavior, including the effect of varying kinematic models, agent types, and traffic control patterns. Most importantly unlike many replay based simulation approaches, TorchDriveEnv is fully integrated with a state of the art behavioral simulation API. This allows users to train and evaluate driving models alongside data driven Non-Playable Characters (NPC) whose initializations and driving behavior are reactive, realistic, and diverse. We illustrate the efficiency and simplicity of TorchDriveEnv by evaluating common reinforcement learning baselines in both training and validation environments. Our experiments show that TorchDriveEnv is easy to use, but difficult to solve.","sentences":["The training, testing, and deployment, of autonomous vehicles requires realistic and efficient simulators.","Moreover, because of the high variability between different problems presented in different autonomous systems, these simulators need to be easy to use, and easy to modify.","To address these problems we introduce TorchDriveSim and its benchmark extension TorchDriveEnv.","TorchDriveEnv is a lightweight reinforcement learning benchmark programmed entirely in Python, which can be modified to test a number of different factors in learned vehicle behavior, including the effect of varying kinematic models, agent types, and traffic control patterns.","Most importantly unlike many replay based simulation approaches, TorchDriveEnv is fully integrated with a state of the art behavioral simulation API.","This allows users to train and evaluate driving models alongside data driven Non-Playable Characters (NPC) whose initializations and driving behavior are reactive, realistic, and diverse.","We illustrate the efficiency and simplicity of TorchDriveEnv by evaluating common reinforcement learning baselines in both training and validation environments.","Our experiments show that TorchDriveEnv is easy to use, but difficult to solve."],"url":"http://arxiv.org/abs/2405.04491v1","category":"cs.AI"}
{"created":"2024-05-07 16:55:00","title":"UQ state-dependent framework for seismic fragility assessment of industrial components","abstract":"In this study, we propose a novel surrogate modelling approach to efficiently and accurately approximate the response of complex dynamical systems driven by time-varying Recently, there has been increased interest in assessing the seismic fragility of industrial plants and process equipment. This is reflected in the growing number of studies, community-funded research projects and experimental campaigns on the matter.Nonetheless, the complexity of the problem and its inherent modelling, coupled with a general scarcity of available data on process equipment, has limited the development of risk assessment methods. In fact, these limitations have led to the creation of simplified and quick-to-run models. In this context, we propose an innovative framework for developing state-dependent fragility functions. This new methodology combines limited data with the power of metamodelling and statistical techniques, namely polynomial chaos expansions (PCE) and bootstrapping. Therefore, we validated the framework on a simplified and inexpensive-to-run MDoF system endowed with Bouc-Wen hysteresis.Then, we tested it on a real nonstructural industrial process component. Specifically, we applied the state-dependent fragility framework to a critical vertical tank of a multicomponent full-scale 3D steel braced frame (BF). The seismic performance of the BF endowed with process components was captured by means of shake table campaign within the European SPIF project. Finally, we derived state-dependent fragility functions based on the combination of PCE and bootstrap at a greatly reduced computational cost.","sentences":["In this study, we propose a novel surrogate modelling approach to efficiently and accurately approximate the response of complex dynamical systems driven by time-varying Recently, there has been increased interest in assessing the seismic fragility of industrial plants and process equipment.","This is reflected in the growing number of studies, community-funded research projects and experimental campaigns on the matter.","Nonetheless, the complexity of the problem and its inherent modelling, coupled with a general scarcity of available data on process equipment, has limited the development of risk assessment methods.","In fact, these limitations have led to the creation of simplified and quick-to-run models.","In this context, we propose an innovative framework for developing state-dependent fragility functions.","This new methodology combines limited data with the power of metamodelling and statistical techniques, namely polynomial chaos expansions (PCE) and bootstrapping.","Therefore, we validated the framework on a simplified and inexpensive-to-run MDoF system endowed with Bouc-Wen hysteresis.","Then, we tested it on a real nonstructural industrial process component.","Specifically, we applied the state-dependent fragility framework to a critical vertical tank of a multicomponent full-scale 3D steel braced frame (BF).","The seismic performance of the BF endowed with process components was captured by means of shake table campaign within the European SPIF project.","Finally, we derived state-dependent fragility functions based on the combination of PCE and bootstrap at a greatly reduced computational cost."],"url":"http://arxiv.org/abs/2405.04487v1","category":"stat.CO"}
{"created":"2024-05-07 16:53:29","title":"OptPDE: Discovering Novel Integrable Systems via AI-Human Collaboration","abstract":"Integrable partial differential equation (PDE) systems are of great interest in natural science, but are exceedingly rare and difficult to discover. To solve this, we introduce OptPDE, a first-of-its-kind machine learning approach that Optimizes PDEs' coefficients to maximize their number of conserved quantities, $n_{\\rm CQ}$, and thus discover new integrable systems. We discover four families of integrable PDEs, one of which was previously known, and three of which have at least one conserved quantity but are new to the literature to the best of our knowledge. We investigate more deeply the properties of one of these novel PDE families, $u_t = (u_x+a^2u_{xxx})^3$. Our paper offers a promising schema of AI-human collaboration for integrable system discovery: machine learning generates interpretable hypotheses for possible integrable systems, which human scientists can verify and analyze, to truly close the discovery loop.","sentences":["Integrable partial differential equation (PDE) systems are of great interest in natural science, but are exceedingly rare and difficult to discover.","To solve this, we introduce OptPDE, a first-of-its-kind machine learning approach that Optimizes PDEs' coefficients to maximize their number of conserved quantities, $n_{\\rm CQ}$, and thus discover new integrable systems.","We discover four families of integrable PDEs, one of which was previously known, and three of which have at least one conserved quantity but are new to the literature to the best of our knowledge.","We investigate more deeply the properties of one of these novel PDE families, $u_t = (u_x+a^2u_{xxx})^3$. Our paper offers a promising schema of AI-human collaboration for integrable system discovery: machine learning generates interpretable hypotheses for possible integrable systems, which human scientists can verify and analyze, to truly close the discovery loop."],"url":"http://arxiv.org/abs/2405.04484v1","category":"cs.LG"}
{"created":"2024-05-07 16:49:01","title":"CloudDiff: Super-resolution ensemble retrieval of cloud properties for all day using the generative diffusion model","abstract":"Clouds play a crucial role in the Earth's water and energy cycles, underscoring the importance of high spatiotemporal resolution data on cloud phase and properties for accurate numerical modeling and weather prediction. Currently, Moderate Resolution Imaging Spectroradiometer (MODIS) provides cloud products with a spatial resolution of 1 km. However, these products suffer from a lengthy revisit cycle. This study develops a generative diffusion model (donated as CloudDiff) for super-resolution retrieval of high spatiotemporal cloud phase and properties, applicable both day and night. Leveraging 2 km spatial resolution Himawari-8 Advanced Himawari Imager (AHI) thermal infrared (TIR) radiances and viewing geometry as condition, alongside daytime MODIS products as targets, the model can generate cloud phase (CLP), cloud top height (CTH), cloud optical thickness (COT), and cloud effective radius (CER) at 1 km spatial resolution and 10-minute temporal resolution. The conditional diffusion model can generate sharper images and capture finer local features than deterministic super-resolution approaches. It draws multiple samples based on the underlying probability distribution, enabling retrieval uncertainty assessment. Evaluations show agreement between cloud phase and properties derived from the CloudDiff and MODIS cloud products. The ensemble mean is found to enhance retrieval accuracy and credibility, outperforming the deterministic model.","sentences":["Clouds play a crucial role in the Earth's water and energy cycles, underscoring the importance of high spatiotemporal resolution data on cloud phase and properties for accurate numerical modeling and weather prediction.","Currently, Moderate Resolution Imaging Spectroradiometer (MODIS) provides cloud products with a spatial resolution of 1 km.","However, these products suffer from a lengthy revisit cycle.","This study develops a generative diffusion model (donated as CloudDiff) for super-resolution retrieval of high spatiotemporal cloud phase and properties, applicable both day and night.","Leveraging 2 km spatial resolution Himawari-8 Advanced Himawari Imager (AHI) thermal infrared (TIR) radiances and viewing geometry as condition, alongside daytime MODIS products as targets, the model can generate cloud phase (CLP), cloud top height (CTH), cloud optical thickness (COT), and cloud effective radius (CER) at 1 km spatial resolution and 10-minute temporal resolution.","The conditional diffusion model can generate sharper images and capture finer local features than deterministic super-resolution approaches.","It draws multiple samples based on the underlying probability distribution, enabling retrieval uncertainty assessment.","Evaluations show agreement between cloud phase and properties derived from the CloudDiff and MODIS cloud products.","The ensemble mean is found to enhance retrieval accuracy and credibility, outperforming the deterministic model."],"url":"http://arxiv.org/abs/2405.04483v1","category":"physics.ao-ph"}
{"created":"2024-05-07 16:45:15","title":"Concentration Tail-Bound Analysis of Coevolutionary and Bandit Learning Algorithms","abstract":"Runtime analysis, as a branch of the theory of AI, studies how the number of iterations algorithms take before finding a solution (its runtime) depends on the design of the algorithm and the problem structure. Drift analysis is a state-of-the-art tool for estimating the runtime of randomised algorithms, such as evolutionary and bandit algorithms. Drift refers roughly to the expected progress towards the optimum per iteration. This paper considers the problem of deriving concentration tail-bounds on the runtime/regret of algorithms. It provides a novel drift theorem that gives precise exponential tail-bounds given positive, weak, zero and even negative drift. Previously, such exponential tail bounds were missing in the case of weak, zero, or negative drift. Our drift theorem can be used to prove a strong concentration of the runtime/regret of algorithms in AI. For example, we prove that the regret of the \\rwab bandit algorithm is highly concentrated, while previous analyses only considered the expected regret. This means that the algorithm obtains the optimum within a given time frame with high probability, i.e. a form of algorithm reliability. Moreover, our theorem implies that the time needed by the co-evolutionary algorithm RLS-PD to obtain a Nash equilibrium in a \\bilinear max-min-benchmark problem is highly concentrated. However, we also prove that the algorithm forgets the Nash equilibrium, and the time until this occurs is highly concentrated. This highlights a weakness in the RLS-PD which should be addressed by future work.","sentences":["Runtime analysis, as a branch of the theory of AI, studies how the number of iterations algorithms take before finding a solution (its runtime) depends on the design of the algorithm and the problem structure.","Drift analysis is a state-of-the-art tool for estimating the runtime of randomised algorithms, such as evolutionary and bandit algorithms.","Drift refers roughly to the expected progress towards the optimum per iteration.","This paper considers the problem of deriving concentration tail-bounds on the runtime/regret of algorithms.","It provides a novel drift theorem that gives precise exponential tail-bounds given positive, weak, zero and even negative drift.","Previously, such exponential tail bounds were missing in the case of weak, zero, or negative drift.","Our drift theorem can be used to prove a strong concentration of the runtime/regret of algorithms in AI.","For example, we prove that the regret of the \\rwab bandit algorithm is highly concentrated, while previous analyses only considered the expected regret.","This means that the algorithm obtains the optimum within a given time frame with high probability, i.e. a form of algorithm reliability.","Moreover, our theorem implies that the time needed by the co-evolutionary algorithm RLS-PD to obtain a Nash equilibrium in a \\bilinear max-min-benchmark problem is highly concentrated.","However, we also prove that the algorithm forgets the Nash equilibrium, and the time until this occurs is highly concentrated.","This highlights a weakness in the RLS-PD which should be addressed by future work."],"url":"http://arxiv.org/abs/2405.04480v1","category":"cs.NE"}
{"created":"2024-05-07 16:42:48","title":"Designing an Objective-Driven Test Method for the Comparative Performance Evaluation of Commercial DTI Solutions for Counter UAS systems","abstract":"Unmanned Aerial Systems (UASs) or drones become more and more commercially available and cheap. There has been much emphasis on developing and deploying Counter-UAS systems (UASs) with Detection Tracking and Identification (DTI) solutions. However, the capabilities of these systems are hard to benchmark. Performance claims of these systems are currently not supported by evidence. In addition, no standard test methodologies are available for these DTI systems and different test methodologies make comparison of these systems hard or impossible. We report on the definition, development and verification of an objective-driven test method and corresponding comparative performance evaluation for commercial DTI solutions for C-UASs. The developed methodology is based on end-user scenarios that are operationally relevant. The test methodology is based on a generic DTI system lay-out and is detailed towards detection, tracking and identification, taking into account contextual information and end-user input. The comparative performance evaluation is developed to enable the use of the methodology in a relevant environment, thereby taking into account any potential environmental aspect that might influence DTI system performance. Validation of the work in a relevant environment has been done in three operational trials. The operational trial results show that the method allows for performance evaluation at component level (i.e., detection, tracking or identification component) and at system level (combinations of these components and integrated DTI system of system solutions).","sentences":["Unmanned Aerial Systems (UASs) or drones become more and more commercially available and cheap.","There has been much emphasis on developing and deploying Counter-UAS systems (UASs) with Detection Tracking and Identification (DTI) solutions.","However, the capabilities of these systems are hard to benchmark.","Performance claims of these systems are currently not supported by evidence.","In addition, no standard test methodologies are available for these DTI systems and different test methodologies make comparison of these systems hard or impossible.","We report on the definition, development and verification of an objective-driven test method and corresponding comparative performance evaluation for commercial DTI solutions for C-UASs.","The developed methodology is based on end-user scenarios that are operationally relevant.","The test methodology is based on a generic DTI system lay-out and is detailed towards detection, tracking and identification, taking into account contextual information and end-user input.","The comparative performance evaluation is developed to enable the use of the methodology in a relevant environment, thereby taking into account any potential environmental aspect that might influence DTI system performance.","Validation of the work in a relevant environment has been done in three operational trials.","The operational trial results show that the method allows for performance evaluation at component level (i.e., detection, tracking or identification component) and at system level (combinations of these components and integrated DTI system of system solutions)."],"url":"http://arxiv.org/abs/2405.04477v1","category":"cs.SE"}
{"created":"2024-05-07 16:39:53","title":"Thermodynamics and geometrothermodynamics of regular black holes","abstract":"We assume the validity of the Bekenstein-Hawking entropy, as given in terms of the horizon area of the Bardeen regular black hole, and consider it as the fundamental thermodynamic equation. We derive and investigate the behavior of the main thermodynamic variables. Using the formalism of geometrothermodynamics, we derive the geometric properties of the corresponding equilibrium space and show that the curvature contains information about the stability properties and phase transition structure of the black hole.","sentences":["We assume the validity of the Bekenstein-Hawking entropy, as given in terms of the horizon area of the Bardeen regular black hole, and consider it as the fundamental thermodynamic equation.","We derive and investigate the behavior of the main thermodynamic variables.","Using the formalism of geometrothermodynamics, we derive the geometric properties of the corresponding equilibrium space and show that the curvature contains information about the stability properties and phase transition structure of the black hole."],"url":"http://arxiv.org/abs/2405.04474v1","category":"gr-qc"}
{"created":"2024-05-07 16:32:26","title":"Universal Spatial Audio Transcoder","abstract":"This paper addresses the challenges associated with both the conversion between different spatial audio formats and the decoding of a spatial audio format to a specific loudspeaker layout. Existing approaches often rely on layout remapping tools, which may not guarantee optimal conversion from a psychoacoustic perspective. To overcome these challenges, we present the Universal Spatial Audio Transcoder(USAT) method and its corresponding open source implementation. USAT generates an optimal decoder or transcoder for any input spatial audio format, adapting it to any output format or 2D/3D loudspeaker configuration. Drawing upon optimization techniques based on psychoacoustic principles, the algorithm maximizes the preservation of spatial information. We present examples of the decoding and transcoding of several audio formats, and show that USAT approach is advantageous compared to the most common methods in the field.","sentences":["This paper addresses the challenges associated with both the conversion between different spatial audio formats and the decoding of a spatial audio format to a specific loudspeaker layout.","Existing approaches often rely on layout remapping tools, which may not guarantee optimal conversion from a psychoacoustic perspective.","To overcome these challenges, we present the Universal Spatial Audio Transcoder(USAT) method and its corresponding open source implementation.","USAT generates an optimal decoder or transcoder for any input spatial audio format, adapting it to any output format or 2D/3D loudspeaker configuration.","Drawing upon optimization techniques based on psychoacoustic principles, the algorithm maximizes the preservation of spatial information.","We present examples of the decoding and transcoding of several audio formats, and show that USAT approach is advantageous compared to the most common methods in the field."],"url":"http://arxiv.org/abs/2405.04471v1","category":"cs.SD"}
{"created":"2024-05-07 16:30:02","title":"A fully differentiable GNN-based PDE Solver: With Applications to Poisson and Navier-Stokes Equations","abstract":"In this study, we present a novel computational framework that integrates the finite volume method with graph neural networks to address the challenges in Physics-Informed Neural Networks(PINNs). Our approach leverages the flexibility of graph neural networks to adapt to various types of two-dimensional unstructured grids, enhancing the model's applicability across different physical equations and boundary conditions. The core innovation lies in the development of an unsupervised training algorithm that utilizes GPU parallel computing to implement a fully differentiable finite volume method discretization process. This method includes differentiable integral and gradient reconstruction algorithms, enabling the model to directly solve partial-differential equations(PDEs) during training without the need for pre-computed data. Our results demonstrate the model's superior mesh generalization and its capability to handle multiple boundary conditions simultaneously, significantly boosting its generalization capabilities. The proposed method not only shows potential for extensive applications in CFD but also establishes a new paradigm for integrating traditional numerical methods with deep learning technologies, offering a robust platform for solving complex physical problems.","sentences":["In this study, we present a novel computational framework that integrates the finite volume method with graph neural networks to address the challenges in Physics-Informed Neural Networks(PINNs).","Our approach leverages the flexibility of graph neural networks to adapt to various types of two-dimensional unstructured grids, enhancing the model's applicability across different physical equations and boundary conditions.","The core innovation lies in the development of an unsupervised training algorithm that utilizes GPU parallel computing to implement a fully differentiable finite volume method discretization process.","This method includes differentiable integral and gradient reconstruction algorithms, enabling the model to directly solve partial-differential equations(PDEs) during training without the need for pre-computed data.","Our results demonstrate the model's superior mesh generalization and its capability to handle multiple boundary conditions simultaneously, significantly boosting its generalization capabilities.","The proposed method not only shows potential for extensive applications in CFD but also establishes a new paradigm for integrating traditional numerical methods with deep learning technologies, offering a robust platform for solving complex physical problems."],"url":"http://arxiv.org/abs/2405.04466v1","category":"physics.flu-dyn"}
{"created":"2024-05-07 16:29:28","title":"Ekedahl-Oort strata and the supersingular locus in the GU(q-2,2) Shimura variety","abstract":"This paper concerns the characteristic-$p$ fibers of $\\mathsf{GU}(q-2,2)$ Shimura varieties, which classify abelian varieties with additional structure. These Shimura varieties admit two stratifications of interest: the Ekedahl-Oort stratification, based on the isomorphism class of the $p$-torsion subgroup scheme, and the Newton stratification, based on the isogeny class of the $p$-divisible group. It is natural to ask which Ekedahl-Oort strata intersect the unique closed Newton stratum, called the \\emph{supersingular locus}. In this paper, we present several novel techniques that give information about the interaction between the two stratifications for general signature $(q-2,2)$, and as an application, we completely answer this question for the signature $(3,2)$.","sentences":["This paper concerns the characteristic-$p$ fibers of $\\mathsf{GU}(q-2,2)$ Shimura varieties, which classify abelian varieties with additional structure.","These Shimura varieties admit two stratifications of interest: the Ekedahl-Oort stratification, based on the isomorphism class of the $p$-torsion subgroup scheme, and the Newton stratification, based on the isogeny class of the $p$-divisible group.","It is natural to ask which Ekedahl-Oort strata intersect the unique closed Newton stratum, called the \\emph{supersingular locus}.","In this paper, we present several novel techniques that give information about the interaction between the two stratifications for general signature $(q-2,2)$, and as an application, we completely answer this question for the signature $(3,2)$."],"url":"http://arxiv.org/abs/2405.04464v1","category":"math.NT"}
{"created":"2024-05-07 16:24:03","title":"A Significantly Better Class of Activation Functions Than ReLU Like Activation Functions","abstract":"This paper introduces a significantly better class of activation functions than the almost universally used ReLU like and Sigmoidal class of activation functions. Two new activation functions referred to as the Cone and Parabolic-Cone that differ drastically from popular activation functions and significantly outperform these on the CIFAR-10 and Imagenette benchmmarks are proposed. The cone activation functions are positive only on a finite interval and are strictly negative except at the end-points of the interval, where they become zero. Thus the set of inputs that produce a positive output for a neuron with cone activation functions is a hyperstrip and not a half-space as is the usual case. Since a hyper strip is the region between two parallel hyper-planes, it allows neurons to more finely divide the input feature space into positive and negative classes than with infinitely wide half-spaces. In particular the XOR function can be learn by a single neuron with cone-like activation functions. Both the cone and parabolic-cone activation functions are shown to achieve higher accuracies with significantly fewer neurons on benchmarks. The results presented in this paper indicate that many nonlinear real-world datasets may be separated with fewer hyperstrips than half-spaces. The Cone and Parabolic-Cone activation functions have larger derivatives than ReLU and are shown to significantly speedup training.","sentences":["This paper introduces a significantly better class of activation functions than the almost universally used ReLU like and Sigmoidal class of activation functions.","Two new activation functions referred to as the Cone and Parabolic-Cone that differ drastically from popular activation functions and significantly outperform these on the CIFAR-10 and Imagenette benchmmarks are proposed.","The cone activation functions are positive only on a finite interval and are strictly negative except at the end-points of the interval, where they become zero.","Thus the set of inputs that produce a positive output for a neuron with cone activation functions is a hyperstrip and not a half-space as is the usual case.","Since a hyper strip is the region between two parallel hyper-planes, it allows neurons to more finely divide the input feature space into positive and negative classes than with infinitely wide half-spaces.","In particular the XOR function can be learn by a single neuron with cone-like activation functions.","Both the cone and parabolic-cone activation functions are shown to achieve higher accuracies with significantly fewer neurons on benchmarks.","The results presented in this paper indicate that many nonlinear real-world datasets may be separated with fewer hyperstrips than half-spaces.","The Cone and Parabolic-Cone activation functions have larger derivatives than ReLU and are shown to significantly speedup training."],"url":"http://arxiv.org/abs/2405.04459v1","category":"cs.AI"}
{"created":"2024-05-07 16:23:16","title":"Metallic mean fractal systems and their tilings","abstract":"Fractals and quasiperiodic structures share self-similarity as a structural property. Motivated by the link between Fibonacci fractals and quasicrystals which are scaled by the golden mean ratio $\\frac{1+\\sqrt{5}}{2}$, we introduce and characterize a family of metallic-mean ratio fractals. We calculate the spatial properties of this generalized family, including their boundaries, which are also fractal. We then demonstrate how these fractals can be related to aperiodic tilings, and show how we can decorate them to produce new, fractal tilings.","sentences":["Fractals and quasiperiodic structures share self-similarity as a structural property.","Motivated by the link between Fibonacci fractals and quasicrystals which are scaled by the golden mean ratio $\\frac{1+\\sqrt{5}}{2}$, we introduce and characterize a family of metallic-mean ratio fractals.","We calculate the spatial properties of this generalized family, including their boundaries, which are also fractal.","We then demonstrate how these fractals can be related to aperiodic tilings, and show how we can decorate them to produce new, fractal tilings."],"url":"http://arxiv.org/abs/2405.04458v1","category":"cond-mat.other"}
{"created":"2024-05-07 16:23:06","title":"Towards Geographic Inclusion in the Evaluation of Text-to-Image Models","abstract":"Rapid progress in text-to-image generative models coupled with their deployment for visual content creation has magnified the importance of thoroughly evaluating their performance and identifying potential biases. In pursuit of models that generate images that are realistic, diverse, visually appealing, and consistent with the given prompt, researchers and practitioners often turn to automated metrics to facilitate scalable and cost-effective performance profiling. However, commonly-used metrics often fail to account for the full diversity of human preference; often even in-depth human evaluations face challenges with subjectivity, especially as interpretations of evaluation criteria vary across regions and cultures. In this work, we conduct a large, cross-cultural study to study how much annotators in Africa, Europe, and Southeast Asia vary in their perception of geographic representation, visual appeal, and consistency in real and generated images from state-of-the art public APIs. We collect over 65,000 image annotations and 20 survey responses. We contrast human annotations with common automated metrics, finding that human preferences vary notably across geographic location and that current metrics do not fully account for this diversity. For example, annotators in different locations often disagree on whether exaggerated, stereotypical depictions of a region are considered geographically representative. In addition, the utility of automatic evaluations is dependent on assumptions about their set-up, such as the alignment of feature extractors with human perception of object similarity or the definition of \"appeal\" captured in reference datasets used to ground evaluations. We recommend steps for improved automatic and human evaluations.","sentences":["Rapid progress in text-to-image generative models coupled with their deployment for visual content creation has magnified the importance of thoroughly evaluating their performance and identifying potential biases.","In pursuit of models that generate images that are realistic, diverse, visually appealing, and consistent with the given prompt, researchers and practitioners often turn to automated metrics to facilitate scalable and cost-effective performance profiling.","However, commonly-used metrics often fail to account for the full diversity of human preference; often even in-depth human evaluations face challenges with subjectivity, especially as interpretations of evaluation criteria vary across regions and cultures.","In this work, we conduct a large, cross-cultural study to study how much annotators in Africa, Europe, and Southeast Asia vary in their perception of geographic representation, visual appeal, and consistency in real and generated images from state-of-the art public APIs.","We collect over 65,000 image annotations and 20 survey responses.","We contrast human annotations with common automated metrics, finding that human preferences vary notably across geographic location and that current metrics do not fully account for this diversity.","For example, annotators in different locations often disagree on whether exaggerated, stereotypical depictions of a region are considered geographically representative.","In addition, the utility of automatic evaluations is dependent on assumptions about their set-up, such as the alignment of feature extractors with human perception of object similarity or the definition of \"appeal\" captured in reference datasets used to ground evaluations.","We recommend steps for improved automatic and human evaluations."],"url":"http://arxiv.org/abs/2405.04457v1","category":"cs.CV"}
{"created":"2024-05-07 16:16:00","title":"Towards Continual Knowledge Graph Embedding via Incremental Distillation","abstract":"Traditional knowledge graph embedding (KGE) methods typically require preserving the entire knowledge graph (KG) with significant training costs when new knowledge emerges. To address this issue, the continual knowledge graph embedding (CKGE) task has been proposed to train the KGE model by learning emerging knowledge efficiently while simultaneously preserving decent old knowledge. However, the explicit graph structure in KGs, which is critical for the above goal, has been heavily ignored by existing CKGE methods. On the one hand, existing methods usually learn new triples in a random order, destroying the inner structure of new KGs. On the other hand, old triples are preserved with equal priority, failing to alleviate catastrophic forgetting effectively. In this paper, we propose a competitive method for CKGE based on incremental distillation (IncDE), which considers the full use of the explicit graph structure in KGs. First, to optimize the learning order, we introduce a hierarchical strategy, ranking new triples for layer-by-layer learning. By employing the inter- and intra-hierarchical orders together, new triples are grouped into layers based on the graph structure features. Secondly, to preserve the old knowledge effectively, we devise a novel incremental distillation mechanism, which facilitates the seamless transfer of entity representations from the previous layer to the next one, promoting old knowledge preservation. Finally, we adopt a two-stage training paradigm to avoid the over-corruption of old knowledge influenced by under-trained new knowledge. Experimental results demonstrate the superiority of IncDE over state-of-the-art baselines. Notably, the incremental distillation mechanism contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR) score.","sentences":["Traditional knowledge graph embedding (KGE) methods typically require preserving the entire knowledge graph (KG) with significant training costs when new knowledge emerges.","To address this issue, the continual knowledge graph embedding (CKGE) task has been proposed to train the KGE model by learning emerging knowledge efficiently while simultaneously preserving decent old knowledge.","However, the explicit graph structure in KGs, which is critical for the above goal, has been heavily ignored by existing CKGE methods.","On the one hand, existing methods usually learn new triples in a random order, destroying the inner structure of new KGs.","On the other hand, old triples are preserved with equal priority, failing to alleviate catastrophic forgetting effectively.","In this paper, we propose a competitive method for CKGE based on incremental distillation (IncDE), which considers the full use of the explicit graph structure in KGs.","First, to optimize the learning order, we introduce a hierarchical strategy, ranking new triples for layer-by-layer learning.","By employing the inter- and intra-hierarchical orders together, new triples are grouped into layers based on the graph structure features.","Secondly, to preserve the old knowledge effectively, we devise a novel incremental distillation mechanism, which facilitates the seamless transfer of entity representations from the previous layer to the next one, promoting old knowledge preservation.","Finally, we adopt a two-stage training paradigm to avoid the over-corruption of old knowledge influenced by under-trained new knowledge.","Experimental results demonstrate the superiority of IncDE over state-of-the-art baselines.","Notably, the incremental distillation mechanism contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR) score."],"url":"http://arxiv.org/abs/2405.04453v1","category":"cs.AI"}
{"created":"2024-05-07 16:14:54","title":"A natural model for curved inflation","abstract":"Inflationary models with a non-zero background curvature are ill-defined in general relativity because scalar modes cannot be canonically quantized. Therefore, there is no consensus on the primordial power spectrum that should be considered at large scales in a curved Universe. In this letter, we propose a model of curved inflation where canonical quantization is possible for any curvature, and we unambiguously obtain the resulting primordial power spectra. The framework is a recently proposed modification of general relativity in which a non-dynamical topological term is added to the Einstein equation. The main strength of this model is that no additional degree of freedom compared to the standard model of cosmology is needed, giving a natural solution to the problem of constructing curved inflation, and at the same time providing an additional argument for this topological modification of general relativity.","sentences":["Inflationary models with a non-zero background curvature are ill-defined in general relativity because scalar modes cannot be canonically quantized.","Therefore, there is no consensus on the primordial power spectrum that should be considered at large scales in a curved Universe.","In this letter, we propose a model of curved inflation where canonical quantization is possible for any curvature, and we unambiguously obtain the resulting primordial power spectra.","The framework is a recently proposed modification of general relativity in which a non-dynamical topological term is added to the Einstein equation.","The main strength of this model is that no additional degree of freedom compared to the standard model of cosmology is needed, giving a natural solution to the problem of constructing curved inflation, and at the same time providing an additional argument for this topological modification of general relativity."],"url":"http://arxiv.org/abs/2405.04450v1","category":"gr-qc"}
{"created":"2024-05-07 16:13:45","title":"Exact solution of long-range stabilizer R\u00e9nyi entropy in the dual-unitary XXZ model","abstract":"Quantum systems can not be efficiently simulated classically due to the presence of entanglement and nonstabilizerness, also known as quantum magic. Here we study the generation of magic under evolution by a quantum circuit. To be able to provide exact solutions, we focus on the dual-unitary XXZ model and a measure of magic called stabilizer R\\'enyi entropy (SRE). Moreover, we focus also on long-range SRE, which cannot be removed by short-depth quantum circuits. To obtain exact solutions we use a ZX-calculus representation and graphical rules for the evaluation of the required expressions. We obtain exact results for SRE after short-time evolution in the thermodynamic limit and for long-range SRE for all times and all R\\'enyi parameters for a particular partition of the state. Since the numerical evaluation of these quantities is exponentially costly in the R\\'enyi parameter, we verify this numerically for low R\\'enyi parameters and accessible system sizes and provide numerical results for the long-range SRE in other bipartitions.","sentences":["Quantum systems can not be efficiently simulated classically due to the presence of entanglement and nonstabilizerness, also known as quantum magic.","Here we study the generation of magic under evolution by a quantum circuit.","To be able to provide exact solutions, we focus on the dual-unitary XXZ model and a measure of magic called stabilizer R\\'enyi entropy (SRE).","Moreover, we focus also on long-range SRE, which cannot be removed by short-depth quantum circuits.","To obtain exact solutions we use a ZX-calculus representation and graphical rules for the evaluation of the required expressions.","We obtain exact results for SRE after short-time evolution in the thermodynamic limit and for long-range SRE for all times and all R\\'enyi parameters for a particular partition of the state.","Since the numerical evaluation of these quantities is exponentially costly in the R\\'enyi parameter, we verify this numerically for low R\\'enyi parameters and accessible system sizes and provide numerical results for the long-range SRE in other bipartitions."],"url":"http://arxiv.org/abs/2405.04448v1","category":"quant-ph"}
{"created":"2024-05-07 16:11:51","title":"Causal Inference in the Multiverse of Hazard","abstract":"Hazard serves as a pivotal estimand in both practical applications and methodological frameworks. However, its causal interpretation poses notable challenges, including inherent selection biases and ill-defined populations to be compared between different treatment groups. In response, we propose a novel definition of counterfactual hazard within the framework of possible worlds. Instead of conditioning on prior survival status as a conditional probability, our new definition involves intervening in the prior status, treating it as a marginal probability. Using single-world intervention graphs, we demonstrate that the proposed counterfactual hazard is a type of controlled direct effect. Conceptually, intervening in survival status at each time point generates a new possible world, where the proposed hazards across time points represent risks in these hypothetical scenarios, forming a \"multiverse of hazard.\" The cumulative and average counterfactual hazards correspond to the sum and average of risks across this multiverse, respectively, with the actual world's risk lying between the two. This conceptual shift reframes hazards in the actual world as a collection of risks across possible worlds, marking a significant advancement in the causal interpretation of hazards.","sentences":["Hazard serves as a pivotal estimand in both practical applications and methodological frameworks.","However, its causal interpretation poses notable challenges, including inherent selection biases and ill-defined populations to be compared between different treatment groups.","In response, we propose a novel definition of counterfactual hazard within the framework of possible worlds.","Instead of conditioning on prior survival status as a conditional probability, our new definition involves intervening in the prior status, treating it as a marginal probability.","Using single-world intervention graphs, we demonstrate that the proposed counterfactual hazard is a type of controlled direct effect.","Conceptually, intervening in survival status at each time point generates a new possible world, where the proposed hazards across time points represent risks in these hypothetical scenarios, forming a \"multiverse of hazard.\"","The cumulative and average counterfactual hazards correspond to the sum and average of risks across this multiverse, respectively, with the actual world's risk lying between the two.","This conceptual shift reframes hazards in the actual world as a collection of risks across possible worlds, marking a significant advancement in the causal interpretation of hazards."],"url":"http://arxiv.org/abs/2405.04446v1","category":"stat.ME"}
{"created":"2024-05-07 16:09:12","title":"Realistic Channel and Delay Coefficient Generation for Dual Mobile Space-Ground Links - A Tutorial-","abstract":"Channel and delay coefficient are two essential parameters for the characterization of a multipath propagation environment. It is crucial to generate realistic channel and delay coefficient in order to study the channel characteristics that involves signals propagating through environments with severe multipath effects. While many deterministic channel models, such as ray-tracing (RT), face challenges like high computational complexity, data requirements for geometrical information, and inapplicability for space-ground links, and nongeometry-based stochastic channel models (NGSCMs) might lack spatial consistency and offer lower accuracy, we present a scalable tutorial for the channel modeling of dual mobile space-ground links in urban areas, utilizing the Quasi Deterministic Radio Channel Generator (QuaDRiGa), which adopts a geometry-based stochastic channel model (GSCM), in conjunction with an International Telecommunication Union (ITU) provided state duration model. This tutorial allows for the generation of realistic channel and delay coefficients in a multipath environment for dual mobile space-ground links. We validate the accuracy of the work by analyzing the generated channel and delay coefficient from several aspects, such as received signal power and amplitude, multipath delay distribution, delay spread and Doppler spectrum.","sentences":["Channel and delay coefficient are two essential parameters for the characterization of a multipath propagation environment.","It is crucial to generate realistic channel and delay coefficient in order to study the channel characteristics that involves signals propagating through environments with severe multipath effects.","While many deterministic channel models, such as ray-tracing (RT), face challenges like high computational complexity, data requirements for geometrical information, and inapplicability for space-ground links, and nongeometry-based stochastic channel models (NGSCMs) might lack spatial consistency and offer lower accuracy, we present a scalable tutorial for the channel modeling of dual mobile space-ground links in urban areas, utilizing the Quasi Deterministic Radio Channel Generator (QuaDRiGa), which adopts a geometry-based stochastic channel model (GSCM), in conjunction with an International Telecommunication Union (ITU) provided state duration model.","This tutorial allows for the generation of realistic channel and delay coefficients in a multipath environment for dual mobile space-ground links.","We validate the accuracy of the work by analyzing the generated channel and delay coefficient from several aspects, such as received signal power and amplitude, multipath delay distribution, delay spread and Doppler spectrum."],"url":"http://arxiv.org/abs/2405.04445v1","category":"eess.SP"}
{"created":"2024-05-07 16:07:29","title":"POV Learning: Individual Alignment of Multimodal Models using Human Perception","abstract":"Aligning machine learning systems with human expectations is mostly attempted by training with manually vetted human behavioral samples, typically explicit feedback. This is done on a population level since the context that is capturing the subjective Point-Of-View (POV) of a concrete person in a specific situational context is not retained in the data. However, we argue that alignment on an individual level can boost the subjective predictive performance for the individual user interacting with the system considerably. Since perception differs for each person, the same situation is observed differently. Consequently, the basis for decision making and the subsequent reasoning processes and observable reactions differ. We hypothesize that individual perception patterns can be used for improving the alignment on an individual level. We test this, by integrating perception information into machine learning systems and measuring their predictive performance wrt.~individual subjective assessments. For our empirical study, we collect a novel data set of multimodal stimuli and corresponding eye tracking sequences for the novel task of Perception-Guided Crossmodal Entailment and tackle it with our Perception-Guided Multimodal Transformer. Our findings suggest that exploiting individual perception signals for the machine learning of subjective human assessments provides a valuable cue for individual alignment. It does not only improve the overall predictive performance from the point-of-view of the individual user but might also contribute to steering AI systems towards every person's individual expectations and values.","sentences":["Aligning machine learning systems with human expectations is mostly attempted by training with manually vetted human behavioral samples, typically explicit feedback.","This is done on a population level since the context that is capturing the subjective Point-Of-View (POV) of a concrete person in a specific situational context is not retained in the data.","However, we argue that alignment on an individual level can boost the subjective predictive performance for the individual user interacting with the system considerably.","Since perception differs for each person, the same situation is observed differently.","Consequently, the basis for decision making and the subsequent reasoning processes and observable reactions differ.","We hypothesize that individual perception patterns can be used for improving the alignment on an individual level.","We test this, by integrating perception information into machine learning systems and measuring their predictive performance wrt.~individual subjective assessments.","For our empirical study, we collect a novel data set of multimodal stimuli and corresponding eye tracking sequences for the novel task of Perception-Guided Crossmodal Entailment and tackle it with our Perception-Guided Multimodal Transformer.","Our findings suggest that exploiting individual perception signals for the machine learning of subjective human assessments provides a valuable cue for individual alignment.","It does not only improve the overall predictive performance from the point-of-view of the individual user but might also contribute to steering AI systems towards every person's individual expectations and values."],"url":"http://arxiv.org/abs/2405.04443v1","category":"cs.AI"}
{"created":"2024-05-07 16:07:05","title":"AugmenTory: A Fast and Flexible Polygon Augmentation Library","abstract":"Data augmentation is a key technique for addressing the challenge of limited datasets, which have become a major component in the training procedures of image processing. Techniques such as geometric transformations and color space adjustments have been thoroughly tested for their ability to artificially expand training datasets and generate semi-realistic data for training purposes. Data augmentation is the most important key to addressing the challenge of limited datasets, which have become a major component of image processing training procedures. Data augmentation techniques, such as geometric transformations and color space adjustments, are thoroughly tested for their ability to artificially expand training datasets and generate semi-realistic data for training purposes. Polygons play a crucial role in instance segmentation and have seen a surge in use across advanced models, such as YOLOv8. Despite their growing popularity, the lack of specialized libraries hampers the polygon-augmentation process. This paper introduces a novel solution to this challenge, embodied in the newly developed AugmenTory library. Notably, AugmenTory offers reduced computational demands in both time and space compared to existing methods. Additionally, the library includes a postprocessing thresholding feature. The AugmenTory package is publicly available on GitHub, where interested users can access the source code: https://github.com/Smartory/AugmenTory","sentences":["Data augmentation is a key technique for addressing the challenge of limited datasets, which have become a major component in the training procedures of image processing.","Techniques such as geometric transformations and color space adjustments have been thoroughly tested for their ability to artificially expand training datasets and generate semi-realistic data for training purposes.","Data augmentation is the most important key to addressing the challenge of limited datasets, which have become a major component of image processing training procedures.","Data augmentation techniques, such as geometric transformations and color space adjustments, are thoroughly tested for their ability to artificially expand training datasets and generate semi-realistic data for training purposes.","Polygons play a crucial role in instance segmentation and have seen a surge in use across advanced models, such as YOLOv8.","Despite their growing popularity, the lack of specialized libraries hampers the polygon-augmentation process.","This paper introduces a novel solution to this challenge, embodied in the newly developed AugmenTory library.","Notably, AugmenTory offers reduced computational demands in both time and space compared to existing methods.","Additionally, the library includes a postprocessing thresholding feature.","The AugmenTory package is publicly available on GitHub, where interested users can access the source code:","https://github.com/Smartory/AugmenTory"],"url":"http://arxiv.org/abs/2405.04442v1","category":"cs.CV"}
{"created":"2024-05-07 16:05:06","title":"Designing, Developing, and Validating Network Intelligence for Scaling in Service-Based Architectures based on Deep Reinforcement Learning","abstract":"Automating network processes without human intervention is crucial for the complex 6G environment. This requires zero-touch management and orchestration, the integration of Network Intelligence (NI) into the network architecture, and the efficient lifecycle management of intelligent functions. Reinforcement Learning (RL) plays a key role in this context, offering intelligent decision-making capabilities suited to networks' dynamic nature. Despite its potential, integrating RL poses challenges in model development and application. To tackle those issues, we delve into designing, developing, and validating RL algorithms for scaling network functions in service-based network architectures such as Open Radio Access Network (O-RAN). It builds upon and expands previous research on RL lifecycle management by proposing several RL algorithms and Reward Functions (RFns). Our proposed methodology is anchored on a dual approach: firstly, it evaluates the training performance of these algorithms under varying RFns, and secondly, it validates their performance after being trained to discern the practical applicability in real-world settings. We show that, despite significant progress, the development stage of RL techniques for networking applications, particularly in scaling scenarios, still leaves room for significant improvements. This study underscores the importance of ongoing research and development to enhance the practicality and resilience of RL techniques in real-world networking environments.","sentences":["Automating network processes without human intervention is crucial for the complex 6G environment.","This requires zero-touch management and orchestration, the integration of Network Intelligence (NI) into the network architecture, and the efficient lifecycle management of intelligent functions.","Reinforcement Learning (RL) plays a key role in this context, offering intelligent decision-making capabilities suited to networks' dynamic nature.","Despite its potential, integrating RL poses challenges in model development and application.","To tackle those issues, we delve into designing, developing, and validating RL algorithms for scaling network functions in service-based network architectures such as Open Radio Access Network (O-RAN).","It builds upon and expands previous research on RL lifecycle management by proposing several RL algorithms and Reward Functions (RFns).","Our proposed methodology is anchored on a dual approach: firstly, it evaluates the training performance of these algorithms under varying RFns, and secondly, it validates their performance after being trained to discern the practical applicability in real-world settings.","We show that, despite significant progress, the development stage of RL techniques for networking applications, particularly in scaling scenarios, still leaves room for significant improvements.","This study underscores the importance of ongoing research and development to enhance the practicality and resilience of RL techniques in real-world networking environments."],"url":"http://arxiv.org/abs/2405.04441v1","category":"cs.NI"}
{"created":"2024-05-07 16:03:55","title":"Generalized classical Yang-Baxter equation and regular decompositions","abstract":"The focus of the paper is on constructing new solutions of the generalized classical Yang-Baxter equation (GCYBE) that are not skew-symmetric. Using regular decompositions of finite-dimensional simple Lie algebras, we construct Lie algebra decompositions of $\\mathfrak{g}(\\!(x)\\!) \\times \\mathfrak{g}[x]/x^m \\mathfrak{g}[x]$. The latter decompositions are in bijection with the solutions to the GCYBE. Under appropriate regularity conditions, we obtain a partial classification of such solutions. The paper is concluded with the presentations of the Gaudin-type models associated to these solutions.","sentences":["The focus of the paper is on constructing new solutions of the generalized classical Yang-Baxter equation (GCYBE) that are not skew-symmetric.","Using regular decompositions of finite-dimensional simple Lie algebras, we construct Lie algebra decompositions of $\\mathfrak{g}(\\!(x)\\!)","\\times \\mathfrak{g}[x]/x^m \\mathfrak{g}[x]$. The latter decompositions are in bijection with the solutions to the GCYBE.","Under appropriate regularity conditions, we obtain a partial classification of such solutions.","The paper is concluded with the presentations of the Gaudin-type models associated to these solutions."],"url":"http://arxiv.org/abs/2405.04440v1","category":"math.RA"}
{"created":"2024-05-07 16:01:47","title":"Brownian Motion on The Spider Like Quantum Graphs","abstract":"The paper contains the probabilistic analysis of the Brownian motion on the simplest quantum graph, spider: a system of N-half axis connected only at the graph's origin by the simplest (so-called Kirchhoff's) gluing conditions. The limit theorems for the diffusion on such a graph, especially if $N \\to \\infty$ are significantly different from the classical case $N = 2$ (full axis). Additional results concern the properties of the spectral measure of the spider Laplacian and the corresponding generalized Fourier transforms. The continuation of the paper will contain the study of the spectrum for the class of Schr\\\"odinger operators on the spider graphs: Laplacian perturbed by unbounded potential and related phase transitions.","sentences":["The paper contains the probabilistic analysis of the Brownian motion on the simplest quantum graph, spider: a system of N-half axis connected only at the graph's origin by the simplest (so-called Kirchhoff's) gluing conditions.","The limit theorems for the diffusion on such a graph, especially if $N \\to \\infty$ are significantly different from the classical case $N = 2$ (full axis).","Additional results concern the properties of the spectral measure of the spider Laplacian and the corresponding generalized Fourier transforms.","The continuation of the paper will contain the study of the spectrum for the class of Schr\\\"odinger operators on the spider graphs: Laplacian perturbed by unbounded potential and related phase transitions."],"url":"http://arxiv.org/abs/2405.04439v1","category":"math-ph"}
{"created":"2024-05-07 16:00:32","title":"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention","abstract":"Efficient use of GPU memory is essential for high throughput LLM inference. Prior systems reserved memory for the KV-cache ahead-of-time, resulting in wasted capacity due to internal fragmentation. Inspired by OS-based virtual memory systems, vLLM proposed PagedAttention to enable dynamic memory allocation for KV-cache. This approach eliminates fragmentation, enabling high-throughput LLM serving with larger batch sizes. However, to be able to allocate physical memory dynamically, PagedAttention changes the layout of KV-cache from contiguous virtual memory to non-contiguous virtual memory. This change requires attention kernels to be rewritten to support paging, and serving framework to implement a memory manager. Thus, the PagedAttention model leads to software complexity, portability issues, redundancy and inefficiency.   In this paper, we propose vAttention for dynamic KV-cache memory management. In contrast to PagedAttention, vAttention retains KV-cache in contiguous virtual memory and leverages low-level system support for demand paging, that already exists, to enable on-demand physical memory allocation. Thus, vAttention unburdens the attention kernel developer from having to explicitly support paging and avoids re-implementation of memory management in the serving framework. We show that vAttention enables seamless dynamic memory management for unchanged implementations of various attention kernels. vAttention also generates tokens up to 1.97x faster than vLLM, while processing input prompts up to 3.92x and 1.45x faster than the PagedAttention variants of FlashAttention and FlashInfer.","sentences":["Efficient use of GPU memory is essential for high throughput LLM inference.","Prior systems reserved memory for the KV-cache ahead-of-time, resulting in wasted capacity due to internal fragmentation.","Inspired by OS-based virtual memory systems, vLLM proposed PagedAttention to enable dynamic memory allocation for KV-cache.","This approach eliminates fragmentation, enabling high-throughput LLM serving with larger batch sizes.","However, to be able to allocate physical memory dynamically, PagedAttention changes the layout of KV-cache from contiguous virtual memory to non-contiguous virtual memory.","This change requires attention kernels to be rewritten to support paging, and serving framework to implement a memory manager.","Thus, the PagedAttention model leads to software complexity, portability issues, redundancy and inefficiency.   ","In this paper, we propose vAttention for dynamic KV-cache memory management.","In contrast to PagedAttention, vAttention retains KV-cache in contiguous virtual memory and leverages low-level system support for demand paging, that already exists, to enable on-demand physical memory allocation.","Thus, vAttention unburdens the attention kernel developer from having to explicitly support paging and avoids re-implementation of memory management in the serving framework.","We show that vAttention enables seamless dynamic memory management for unchanged implementations of various attention kernels.","vAttention also generates tokens up to 1.97x faster than vLLM, while processing input prompts up to 3.92x and 1.45x faster than the PagedAttention variants of FlashAttention and FlashInfer."],"url":"http://arxiv.org/abs/2405.04437v1","category":"cs.LG"}
{"created":"2024-05-07 15:57:39","title":"Fast Exact Retrieval for Nearest-neighbor Lookup (FERN)","abstract":"Exact nearest neighbor search is a computationally intensive process, and even its simpler sibling -- vector retrieval -- can be computationally complex. This is exacerbated when retrieving vectors which have high-dimension $d$ relative to the number of vectors, $N$, in the database. Exact nearest neighbor retrieval has been generally acknowledged to be a $O(Nd)$ problem with no sub-linear solutions. Attention has instead shifted towards Approximate Nearest-Neighbor (ANN) retrieval techniques, many of which have sub-linear or even logarithmic time complexities. However, if our intuition from binary search problems (e.g. $d=1$ vector retrieval) carries, there ought to be a way to retrieve an organized representation of vectors without brute-forcing our way to a solution. For low dimension (e.g. $d=2$ or $d=3$ cases), \\texttt{kd-trees} provide a $O(d\\log N)$ algorithm for retrieval. Unfortunately the algorithm deteriorates rapidly to a $O(dN)$ solution at high dimensions (e.g. $k=128$), in practice. We propose a novel algorithm for logarithmic Fast Exact Retrieval for Nearest-neighbor lookup (FERN), inspired by \\texttt{kd-trees}. The algorithm achieves $O(d\\log N)$ look-up with 100\\% recall on 10 million $d=128$ uniformly randomly generated vectors.\\footnote{Code available at https://github.com/RichardZhu123/ferns}","sentences":["Exact nearest neighbor search is a computationally intensive process, and even its simpler sibling -- vector retrieval -- can be computationally complex.","This is exacerbated when retrieving vectors which have high-dimension $d$ relative to the number of vectors, $N$, in the database.","Exact nearest neighbor retrieval has been generally acknowledged to be a $O(Nd)$ problem with no sub-linear solutions.","Attention has instead shifted towards Approximate Nearest-Neighbor (ANN) retrieval techniques, many of which have sub-linear or even logarithmic time complexities.","However, if our intuition from binary search problems (e.g. $d=1$ vector retrieval) carries, there ought to be a way to retrieve an organized representation of vectors without brute-forcing our way to a solution.","For low dimension (e.g. $d=2$ or $d=3$ cases), \\texttt{kd-trees} provide a $O(d\\log N)$ algorithm for retrieval.","Unfortunately the algorithm deteriorates rapidly to a $O(dN)$ solution at high dimensions (e.g. $k=128$), in practice.","We propose a novel algorithm for logarithmic Fast Exact Retrieval for Nearest-neighbor lookup (FERN), inspired by \\texttt{kd-trees}.","The algorithm achieves $O(d\\log N)$ look-up with 100\\% recall on 10 million $d=128$ uniformly randomly generated vectors.\\footnote{Code available at https://github.com/RichardZhu123/ferns}"],"url":"http://arxiv.org/abs/2405.04435v1","category":"cs.CL"}
{"created":"2024-05-07 15:56:43","title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model","abstract":"We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models. The model checkpoints are available at \"https://github.com/deepseek-ai/DeepSeek-V2\".","sentences":["We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference.","It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens.","DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation.","Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times.","We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential.","Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.","The model checkpoints are available at \"https://github.com/deepseek-ai/DeepSeek-V2\"."],"url":"http://arxiv.org/abs/2405.04434v1","category":"cs.CL"}
{"created":"2024-05-07 15:51:33","title":"Designing the Network Intelligence Stratum for 6G Networks","abstract":"As network complexity escalates, there is an increasing need for more sophisticated methods to manage and operate these networks, focusing on enhancing efficiency, reliability, and security. A wide range of Artificial Intelligence (AI)/Machine Learning (ML) models are being developed in response. These models are pivotal in automating decision-making, conducting predictive analyses, managing networks proactively, enhancing security, and optimizing network performance. They are foundational in shaping the future of networks, collectively forming what is known as Network Intelligence (NI). Prominent Standard-Defining Organizations (SDOs) are integrating NI into future network architectures, particularly emphasizing the closed-loop approach. However, existing methods for seamlessly integrating NI into network architectures are not yet fully effective. This paper introduces an in-depth architectural design for a Network Intelligence Stratum (NI Stratum). This stratum is supported by a novel end-to-end NI orchestrator that supports closed-loop NI operations across various network domains. The primary goal of this design is to streamline the deployment and coordination of NI throughout the entire network infrastructure, tackling issues related to scalability, conflict resolution, and effective data management. We detail exhaustive workflows for managing the NI lifecycle and demonstrate a reference implementation of the NI Stratum, focusing on its compatibility and integration with current network systems and open-source platforms such as Kubernetes and Kubeflow, as well as on its validation on real-world environments. The paper also outlines major challenges and open issues in deploying and managing NI.","sentences":["As network complexity escalates, there is an increasing need for more sophisticated methods to manage and operate these networks, focusing on enhancing efficiency, reliability, and security.","A wide range of Artificial Intelligence (AI)/Machine Learning (ML) models are being developed in response.","These models are pivotal in automating decision-making, conducting predictive analyses, managing networks proactively, enhancing security, and optimizing network performance.","They are foundational in shaping the future of networks, collectively forming what is known as Network Intelligence (NI).","Prominent Standard-Defining Organizations (SDOs) are integrating NI into future network architectures, particularly emphasizing the closed-loop approach.","However, existing methods for seamlessly integrating NI into network architectures are not yet fully effective.","This paper introduces an in-depth architectural design for a Network Intelligence Stratum (NI Stratum).","This stratum is supported by a novel end-to-end NI orchestrator that supports closed-loop NI operations across various network domains.","The primary goal of this design is to streamline the deployment and coordination of NI throughout the entire network infrastructure, tackling issues related to scalability, conflict resolution, and effective data management.","We detail exhaustive workflows for managing the NI lifecycle and demonstrate a reference implementation of the NI Stratum, focusing on its compatibility and integration with current network systems and open-source platforms such as Kubernetes and Kubeflow, as well as on its validation on real-world environments.","The paper also outlines major challenges and open issues in deploying and managing NI."],"url":"http://arxiv.org/abs/2405.04432v1","category":"cs.NI"}
{"created":"2024-05-07 15:49:29","title":"Landscape, Swampland, and Extra Dimensions","abstract":"By combining swampland conjectures with observational data, it was recently suggested that the cosmological hierarchy problem (i.e. the smallness of the dark energy in Planck units) could be understood as an asymptotic limit in field space, corresponding to a decompactification of one extra (dark) dimension of a size in the micron range. In these Proceedings we examine the fundamental setting of this framework and discuss general aspects of the effective low energy theory inherited from properties of the overarching string theory. We then explore some novel phenomenology encompassing the dark dimension by looking at potential dark matter candidates, decoding neutrino masses, and digging into new cosmological phenomena.","sentences":["By combining swampland conjectures with observational data, it was recently suggested that the cosmological hierarchy problem (i.e. the smallness of the dark energy in Planck units) could be understood as an asymptotic limit in field space, corresponding to a decompactification of one extra (dark) dimension of a size in the micron range.","In these Proceedings we examine the fundamental setting of this framework and discuss general aspects of the effective low energy theory inherited from properties of the overarching string theory.","We then explore some novel phenomenology encompassing the dark dimension by looking at potential dark matter candidates, decoding neutrino masses, and digging into new cosmological phenomena."],"url":"http://arxiv.org/abs/2405.04427v1","category":"hep-th"}
{"created":"2024-05-07 15:49:18","title":"On the classification of product-quotient surfaces with $q=0$, $p_g=3$ and their canonical map","abstract":"In this work we present new results to produce an algorithm that returns, for any fixed pair of natural integers $K^2$ and $\\chi$, all regular surfaces $S$ of general type with self-intersection $K_S^2=K^2$ and Euler characteristic $\\chi(\\mathcal O_S)=\\chi$, that are product-quotient surfaces. The key result we obtain is an algebraic characterization of all families of regular product-quotients surfaces, up to isomorphism, arising from a pair of $G$-coverings of $\\mathbb P^1$. As a consequence of our work, we provide a classification of all regular product-quotient surfaces of general type with $23\\leq K^2\\leq 32$ and $\\chi(\\mathcal O_S)=4$. Furthermore, we study their canonical map and present several new examples of surfaces of general type with a high degree of the canonical map.","sentences":["In this work we present new results to produce an algorithm that returns, for any fixed pair of natural integers $K^2$ and $\\chi$, all regular surfaces $S$ of general type with self-intersection $K_S^2=K^2$ and Euler characteristic $\\chi(\\mathcal O_S)=\\chi$, that are product-quotient surfaces.","The key result we obtain is an algebraic characterization of all families of regular product-quotients surfaces, up to isomorphism, arising from a pair of $G$-coverings of $\\mathbb P^1$.","As a consequence of our work, we provide a classification of all regular product-quotient surfaces of general type with $23\\leq K^2\\leq 32$ and $\\chi(\\mathcal O_S)=4$.","Furthermore, we study their canonical map and present several new examples of surfaces of general type with a high degree of the canonical map."],"url":"http://arxiv.org/abs/2405.04425v1","category":"math.AG"}
{"created":"2024-05-07 15:47:46","title":"Three-Wave Mixing between Continuous-Wave and Ultrafast Lasers","abstract":"Ultrafast optical frequency combs allow for both high spectral and temporal resolution in molecular spectroscopy and have become a powerful tool in many areas of chemistry and physics. Ultrafast lasers and frequency combs generated from ultrafast mode-locked lasers often need to be converted to other wavelengths. Commonly used wavelength conversions are optical parametric oscillators, which require an external optical cavity, and supercontinuum generation combined with optical parametric amplifiers. Whether commercial or home-built, these systems are complex and costly. Here we propose an alternative, simple, and easy-to-implement approach to tunable frequency comb ultrafast lasers enabled by new continuous-wave laser technology. Sum-frequency generation between a Nd:YAG continuous-wave laser and a Yb:fiber femtosecond frequency comb in a beta-barium borate (BBO) crystal is explored. The resulting sum-frequency beam is a pulsed frequency comb with the same repetition rate as the Yb:fiber source. SNLO simulation software was used to simulate the results and provide benchmarks for designing future system to achieve wavelength conversion and tunability in difficult spectral regions.","sentences":["Ultrafast optical frequency combs allow for both high spectral and temporal resolution in molecular spectroscopy and have become a powerful tool in many areas of chemistry and physics.","Ultrafast lasers and frequency combs generated from ultrafast mode-locked lasers often need to be converted to other wavelengths.","Commonly used wavelength conversions are optical parametric oscillators, which require an external optical cavity, and supercontinuum generation combined with optical parametric amplifiers.","Whether commercial or home-built, these systems are complex and costly.","Here we propose an alternative, simple, and easy-to-implement approach to tunable frequency comb ultrafast lasers enabled by new continuous-wave laser technology.","Sum-frequency generation between a Nd:YAG continuous-wave laser and a Yb:fiber femtosecond frequency comb in a beta-barium borate (BBO) crystal is explored.","The resulting sum-frequency beam is a pulsed frequency comb with the same repetition rate as the Yb:fiber source.","SNLO simulation software was used to simulate the results and provide benchmarks for designing future system to achieve wavelength conversion and tunability in difficult spectral regions."],"url":"http://arxiv.org/abs/2405.04424v1","category":"physics.optics"}
{"created":"2024-05-07 15:46:54","title":"First-principles and cluster expansion study of the effect of magnetism on short-range order in Fe-Ni-Cr austenitic stainless steels","abstract":"Short-range order (SRO) alters the mechanical properties of technologically relevant structural materials such as medium/high entropy alloys and austenitic stainless steels. In this study, we present a generalized spin cluster expansion (CE) model and show that magnetism is a primary factor influencing the level of SRO present in austenitic Fe-Ni-Cr alloys. The spin CE consists of a chemical cluster expansion combined with an Ising model for Fe-Ni-Cr alloys. It explicitly accounts for local magnetic exchange interactions, thereby capturing the effects of finite temperature magnetism on SRO. Model parameters are obtained by fitting to a first-principles data set comprising both chemically and magnetically diverse FCC configurations. The magnitude of the magnetic exchange interactions are found to be comparable to the chemical interactions. Compared to a conventional implicit magnetism CE built from only magnetic ground state configurations, the spin CE shows improved performance on several experimental benchmarks over a broad spectrum of compositions, particularly at higher temperatures due to the explicit treatment of magnetic disorder. We find that SRO is strongly influenced by alloy Cr content, since Cr atoms prefer to align antiferromagnetically with nearest neighbors but become magnetically frustrated with increasing Cr concentration. We predict that increasing the Cr concentration in typical austenitic stainless steels promotes the formation of SRO and increases order-disorder transition temperatures. This study underscores the significance of considering magnetic interactions explicitly when exploring the thermodynamic properties of complex transition metal alloys. It also highlights guidelines for customizing SRO through adjustments of alloy composition.","sentences":["Short-range order (SRO) alters the mechanical properties of technologically relevant structural materials such as medium/high entropy alloys and austenitic stainless steels.","In this study, we present a generalized spin cluster expansion (CE) model and show that magnetism is a primary factor influencing the level of SRO present in austenitic Fe-Ni-Cr alloys.","The spin CE consists of a chemical cluster expansion combined with an Ising model for Fe-Ni-Cr alloys.","It explicitly accounts for local magnetic exchange interactions, thereby capturing the effects of finite temperature magnetism on SRO.","Model parameters are obtained by fitting to a first-principles data set comprising both chemically and magnetically diverse FCC configurations.","The magnitude of the magnetic exchange interactions are found to be comparable to the chemical interactions.","Compared to a conventional implicit magnetism CE built from only magnetic ground state configurations, the spin CE shows improved performance on several experimental benchmarks over a broad spectrum of compositions, particularly at higher temperatures due to the explicit treatment of magnetic disorder.","We find that SRO is strongly influenced by alloy Cr content, since Cr atoms prefer to align antiferromagnetically with nearest neighbors but become magnetically frustrated with increasing Cr concentration.","We predict that increasing the Cr concentration in typical austenitic stainless steels promotes the formation of SRO and increases order-disorder transition temperatures.","This study underscores the significance of considering magnetic interactions explicitly when exploring the thermodynamic properties of complex transition metal alloys.","It also highlights guidelines for customizing SRO through adjustments of alloy composition."],"url":"http://arxiv.org/abs/2405.04423v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 15:45:23","title":"Homotopy braid groups are torsion-free","abstract":"We show that, for any number of components, the group of braids up to link-homotopy is torsion-free. This generalizes a result of Humphries up to six components, and provides an explicit solution to a question posed by Lin and addressed by Linell and Schick regarding the existence of non-abelian torsion-free quotients of the braid group. The proof relies on the diagrammatic theory of welded braids and uses the Artin representation. As a corollary, we obtain yet another proof that braid groups themselves are torsion-free.","sentences":["We show that, for any number of components, the group of braids up to link-homotopy is torsion-free.","This generalizes a result of Humphries up to six components, and provides an explicit solution to a question posed by Lin and addressed by Linell and Schick regarding the existence of non-abelian torsion-free quotients of the braid group.","The proof relies on the diagrammatic theory of welded braids and uses the Artin representation.","As a corollary, we obtain yet another proof that braid groups themselves are torsion-free."],"url":"http://arxiv.org/abs/2405.04422v1","category":"math.GT"}
{"created":"2024-05-07 15:43:21","title":"Mathematical Modeling of $^{18}$F-Fluoromisonidazole ($^{18}$F-FMISO) Radiopharmaceutical Transport in Vascularized Solid Tumors","abstract":"$^{18}$F-Fluoromisonidazole ($^{18}$F-FMISO) is a highly promising positron emission tomography radiopharmaceutical for identifying hypoxic regions in solid tumors. This research employs spatiotemporal multi-scale mathematical modeling to explore how different levels of angiogenesis influence the transport of radiopharmaceuticals within tumors. In this study, two tumor geometries with heterogeneous and uniform distributions of capillary networks were employed to incorporate varying degrees of microvascular density. The synthetic image of the heterogeneous and vascularized tumor was generated by simulating the angiogenesis process. The proposed multi-scale spatiotemporal model accounts for intricate physiological and biochemical factors within the tumor microenvironment, such as the transvascular transport of the radiopharmaceutical agent, its movement into the interstitial space by diffusion and convection mechanisms, and ultimately its uptake by tumor cells. Results showed that both quantitative and semi-quantitative metrics of $^{18}$F-FMISO uptake differ spatially and temporally at different stages during tumor growth. The presence of a high microvascular density in uniformly vascularized tumor increases cellular uptake, as it allows for more efficient release and rapid distribution of radiopharmaceutical molecules. This results in enhanced uptake compared to the heterogeneous vascularized tumor. In both heterogeneous and uniform distribution of microvessels in tumors, the diffusion transport mechanism has a more pronounced than convection. The findings of this study shed light on the transport phenomena behind $^{18}$F-FMISO radiopharmaceutical distribution and its delivery in the tumor microenvironment, aiding oncologists in their routine decision-making processes.","sentences":["$^{18}$F-Fluoromisonidazole ($^{18}$F-FMISO) is a highly promising positron emission tomography radiopharmaceutical for identifying hypoxic regions in solid tumors.","This research employs spatiotemporal multi-scale mathematical modeling to explore how different levels of angiogenesis influence the transport of radiopharmaceuticals within tumors.","In this study, two tumor geometries with heterogeneous and uniform distributions of capillary networks were employed to incorporate varying degrees of microvascular density.","The synthetic image of the heterogeneous and vascularized tumor was generated by simulating the angiogenesis process.","The proposed multi-scale spatiotemporal model accounts for intricate physiological and biochemical factors within the tumor microenvironment, such as the transvascular transport of the radiopharmaceutical agent, its movement into the interstitial space by diffusion and convection mechanisms, and ultimately its uptake by tumor cells.","Results showed that both quantitative and semi-quantitative metrics of $^{18}$F-FMISO uptake differ spatially and temporally at different stages during tumor growth.","The presence of a high microvascular density in uniformly vascularized tumor increases cellular uptake, as it allows for more efficient release and rapid distribution of radiopharmaceutical molecules.","This results in enhanced uptake compared to the heterogeneous vascularized tumor.","In both heterogeneous and uniform distribution of microvessels in tumors, the diffusion transport mechanism has a more pronounced than convection.","The findings of this study shed light on the transport phenomena behind $^{18}$F-FMISO radiopharmaceutical distribution and its delivery in the tumor microenvironment, aiding oncologists in their routine decision-making processes."],"url":"http://arxiv.org/abs/2405.04418v1","category":"physics.bio-ph"}
{"created":"2024-05-07 15:40:24","title":"100 Gbps Quantum-safe IPsec VPN Tunnels over 46 km Deployed Fiber","abstract":"We demonstrated for the first time quantum-safe high-speed 100 Gbps site-to-site IPsec tunnels secured using Quantum Key Distribution (QKD) technology. The demonstration was conducted between two JPMorgan Chase Data Centers (DCs) in an air-gapped environment over 46 km of deployed telecom fiber across Singapore achieving 45 days of continuous operation. Two different Virtual Private Network (VPN) tunnel configurations were tested: (1) a QKD-secured VPN tunnel configuration with a maximum throughput of 80 Gbps and (2) a multi-VPN tunnel configuration exhibiting 12 QKD-secured VPN tunnels with a throughput of 8.39 Gbps per tunnel resulting in an aggregated throughput of 99.62 Gbps for all tunnels. For the QKD system performance, we achieved an average Secret Key Rate (SKR) of 7.4 kbps (about 29 AES-256 keys per second), an average Quantum Bit Error Rate (QBER) of 0.8% and an average visibility of 98.6%. We utilized the ETSI-QKD-014 REST-based Application Programming Interface (API) to exchange the QKD generated keys between the key management server in the QKD system and the next-generation firewalls in order to encrypt and decrypt the data. The data was encrypted by the quantum-safe keys using the AES-256-GCM cipher suite with a key refresh rate of 120 seconds without affecting the VPN tunnel connectivity and performance","sentences":["We demonstrated for the first time quantum-safe high-speed 100 Gbps site-to-site IPsec tunnels secured using Quantum Key Distribution (QKD) technology.","The demonstration was conducted between two JPMorgan Chase Data Centers (DCs) in an air-gapped environment over 46 km of deployed telecom fiber across Singapore achieving 45 days of continuous operation.","Two different Virtual Private Network (VPN) tunnel configurations were tested: (1) a QKD-secured VPN tunnel configuration with a maximum throughput of 80 Gbps and (2) a multi-VPN tunnel configuration exhibiting 12 QKD-secured VPN tunnels with a throughput of 8.39 Gbps per tunnel resulting in an aggregated throughput of 99.62 Gbps for all tunnels.","For the QKD system performance, we achieved an average Secret Key Rate (SKR) of 7.4 kbps (about 29 AES-256 keys per second), an average Quantum Bit Error Rate (QBER) of 0.8% and an average visibility of 98.6%.","We utilized the ETSI-QKD-014 REST-based Application Programming Interface (API) to exchange the QKD generated keys between the key management server in the QKD system and the next-generation firewalls in order to encrypt and decrypt the data.","The data was encrypted by the quantum-safe keys using the AES-256-GCM cipher suite with a key refresh rate of 120 seconds without affecting the VPN tunnel connectivity and performance"],"url":"http://arxiv.org/abs/2405.04415v1","category":"quant-ph"}
{"created":"2024-05-07 15:40:05","title":"Generalized parton distributions from the pseudo-distribution approach on the lattice","abstract":"Generalized parton distributions (GPDs) are key quantities for the description of a hadron's three-dimensional structure. They are the current focus of all areas of hadronic physics -- phenomenological, experimental, and theoretical, including lattice QCD. Synergies between these areas are desirable and essential to achieve precise quantification and understanding of the structure of, particularly nucleons, as the basic ingredients of matter. In this paper, we investigate, for the first time, the numerical implementation of the pseudo-distribution approach for the extraction of zero-skewness GPDs for unpolarized quarks. Pseudo-distributions are Euclidean parton correlators computable in lattice QCD that can be perturbatively matched to the light-cone parton distributions of interest. Being closely related to the quasi-distributions and coming from the same lattice-extracted matrix elements, they are, however, subject to different systematic effects. We use the data previously utilized for quasi-GPDs and extend it with other momentum transfers and nucleon boosts, in particular a higher one ($P_3=1.67$ GeV) with eight-fold larger statistics than the largest one used for quasi-distributions ($P_3=1.25$ GeV). We renormalize the matrix elements with a ratio scheme and match the resulting Ioffe time distributions to the light cone in coordinate space. The matched distributions are then used to reconstruct the $x$-dependence with a fitting ansatz.We investigate some systematic effects related to this procedure, and we also compare the results with the ones obtained in the framework of quasi-GPDs. Our final results involve the invariant four-momentum transfer squared ($-t$) dependence of the flavor non-singlet ($u-d$) $H$ and $E$ GPDs.","sentences":["Generalized parton distributions (GPDs) are key quantities for the description of a hadron's three-dimensional structure.","They are the current focus of all areas of hadronic physics -- phenomenological, experimental, and theoretical, including lattice QCD.","Synergies between these areas are desirable and essential to achieve precise quantification and understanding of the structure of, particularly nucleons, as the basic ingredients of matter.","In this paper, we investigate, for the first time, the numerical implementation of the pseudo-distribution approach for the extraction of zero-skewness GPDs for unpolarized quarks.","Pseudo-distributions are Euclidean parton correlators computable in lattice QCD that can be perturbatively matched to the light-cone parton distributions of interest.","Being closely related to the quasi-distributions and coming from the same lattice-extracted matrix elements, they are, however, subject to different systematic effects.","We use the data previously utilized for quasi-GPDs and extend it with other momentum transfers and nucleon boosts, in particular a higher one ($P_3=1.67$ GeV) with eight-fold larger statistics than the largest one used for quasi-distributions ($P_3=1.25$ GeV).","We renormalize the matrix elements with a ratio scheme and match the resulting Ioffe time distributions to the light cone in coordinate space.","The matched distributions are then used to reconstruct the $x$-dependence with a fitting ansatz.","We investigate some systematic effects related to this procedure, and we also compare the results with the ones obtained in the framework of quasi-GPDs.","Our final results involve the invariant four-momentum transfer squared ($-t$) dependence of the flavor non-singlet ($u-d$) $H$ and $E$ GPDs."],"url":"http://arxiv.org/abs/2405.04414v1","category":"hep-lat"}
{"created":"2024-05-07 15:39:50","title":"Enhancement of terahertz emission during single-color filamentation by chirping laser pulse","abstract":"An experimental study of laser pulse duration influence on the terahertz emission during single-color filamentation is carried out. It is shown that for each terahertz frequency there is an optimal laser pulse duration providing maximal generation at constant pulse energy. It is demonstrated that longer pulses are required for stronger low-frequency terahertz emission, thus despite considerable laser peak power decreasing the terahertz radiation yield can be increased by more than 3 times.","sentences":["An experimental study of laser pulse duration influence on the terahertz emission during single-color filamentation is carried out.","It is shown that for each terahertz frequency there is an optimal laser pulse duration providing maximal generation at constant pulse energy.","It is demonstrated that longer pulses are required for stronger low-frequency terahertz emission, thus despite considerable laser peak power decreasing the terahertz radiation yield can be increased by more than 3 times."],"url":"http://arxiv.org/abs/2405.04413v1","category":"physics.optics"}
{"created":"2024-05-07 15:39:45","title":"The Silicone Ceiling: Auditing GPT's Race and Gender Biases in Hiring","abstract":"Large language models (LLMs) are increasingly being introduced in workplace settings, with the goals of improving efficiency and fairness. However, concerns have arisen regarding these models' potential to reflect or exacerbate social biases and stereotypes. This study explores the potential impact of LLMs on hiring practices. To do so, we conduct an algorithm audit of race and gender biases in one commonly-used LLM, OpenAI's GPT-3.5, taking inspiration from the history of traditional offline resume audits. We conduct two studies using names with varied race and gender connotations: resume assessment (Study 1) and resume generation (Study 2). In Study 1, we ask GPT to score resumes with 32 different names (4 names for each combination of the 2 gender and 4 racial groups) and two anonymous options across 10 occupations and 3 evaluation tasks (overall rating, willingness to interview, and hireability). We find that the model reflects some biases based on stereotypes. In Study 2, we prompt GPT to create resumes (10 for each name) for fictitious job candidates. When generating resumes, GPT reveals underlying biases; women's resumes had occupations with less experience, while Asian and Hispanic resumes had immigrant markers, such as non-native English and non-U.S. education and work experiences. Our findings contribute to a growing body of literature on LLM biases, in particular when used in workplace contexts.","sentences":["Large language models (LLMs) are increasingly being introduced in workplace settings, with the goals of improving efficiency and fairness.","However, concerns have arisen regarding these models' potential to reflect or exacerbate social biases and stereotypes.","This study explores the potential impact of LLMs on hiring practices.","To do so, we conduct an algorithm audit of race and gender biases in one commonly-used LLM, OpenAI's GPT-3.5, taking inspiration from the history of traditional offline resume audits.","We conduct two studies using names with varied race and gender connotations: resume assessment (Study 1) and resume generation (Study 2).","In Study 1, we ask GPT to score resumes with 32 different names (4 names for each combination of the 2 gender and 4 racial groups) and two anonymous options across 10 occupations and 3 evaluation tasks (overall rating, willingness to interview, and hireability).","We find that the model reflects some biases based on stereotypes.","In Study 2, we prompt GPT to create resumes (10 for each name) for fictitious job candidates.","When generating resumes, GPT reveals underlying biases; women's resumes had occupations with less experience, while Asian and Hispanic resumes had immigrant markers, such as non-native English and non-U.S. education and work experiences.","Our findings contribute to a growing body of literature on LLM biases, in particular when used in workplace contexts."],"url":"http://arxiv.org/abs/2405.04412v1","category":"cs.CY"}
{"created":"2024-05-07 15:39:42","title":"Accretion mediated spin-eccentricity correlations in LISA massive black hole binaries","abstract":"We examine expected effective spin ($\\chi_{{\\rm eff},1\\rm yr}$) and orbital eccentricity ($e_{1\\rm yr}$) correlations for a population of observable equal-mass massive black hole binaries (MBHBs) with total redshifted mass $M_z\\sim[10^{4.5},10^{7.5}]~{\\rm M}_\\odot$ embedded in a circumbinary disc (CBD), one-year before merging in the LISA band. We find a strong correlation between measurable eccentricity and negative effective spin for MBHBs that are carried to merger by retrograde accretion. This is due to the well-established eccentricity pumping of retrograde accretion and the formation of retrograde CBD-aligned mini-discs, as observed in hydrodynamical simulations. Conversely, prograde accretion channels result in positive $\\chi_{{\\rm eff},1\\rm yr}$ and non-measurable $e_{1\\rm yr}$. This clear contrast between the two CBD orientations -- and particularly the unique signature of retrograde configurations -- provides a promising way to unlock the mysteries of MBHB formation channels in the LISA era.","sentences":["We examine expected effective spin ($\\chi_{{\\rm eff},1\\rm yr}$) and orbital eccentricity ($e_{1\\rm yr}$) correlations for a population of observable equal-mass massive black hole binaries (MBHBs) with total redshifted mass $M_z\\sim[10^{4.5},10^{7.5}]~{\\rm M}_\\odot$ embedded in a circumbinary disc (CBD), one-year before merging in the LISA band.","We find a strong correlation between measurable eccentricity and negative effective spin for MBHBs that are carried to merger by retrograde accretion.","This is due to the well-established eccentricity pumping of retrograde accretion and the formation of retrograde CBD-aligned mini-discs, as observed in hydrodynamical simulations.","Conversely, prograde accretion channels result in positive $\\chi_{{\\rm eff},1\\rm yr}$ and non-measurable $e_{1\\rm yr}$.","This clear contrast between the two CBD orientations -- and particularly the unique signature of retrograde configurations -- provides a promising way to unlock the mysteries of MBHB formation channels in the LISA era."],"url":"http://arxiv.org/abs/2405.04411v1","category":"astro-ph.HE"}
{"created":"2024-05-07 15:36:30","title":"On Bias and Its Reduction via Standardization in Source Localization Problems","abstract":"In source localization problems, the aim is to locate the sources within a domain that causes given measurements on the boundary. In this type of problem, biasing of the solution is one of the main causes of mislocalization. A technique called standardization was developed to reduce biasing. However, the lack of a mathematical background for this method can cause difficulties in its application and confusion regarding the reliability of solutions. Here, we give a rigorous and generalized background for the technique using the Bayesian framework to shed light on the technique's abilities and limitations. In addition, we take a look at the noise robustness of the method that is widely reported in numerical studies. The paper starts by giving a gentle introduction to the problem and its bias and works its way toward standardization.","sentences":["In source localization problems, the aim is to locate the sources within a domain that causes given measurements on the boundary.","In this type of problem, biasing of the solution is one of the main causes of mislocalization.","A technique called standardization was developed to reduce biasing.","However, the lack of a mathematical background for this method can cause difficulties in its application and confusion regarding the reliability of solutions.","Here, we give a rigorous and generalized background for the technique using the Bayesian framework to shed light on the technique's abilities and limitations.","In addition, we take a look at the noise robustness of the method that is widely reported in numerical studies.","The paper starts by giving a gentle introduction to the problem and its bias and works its way toward standardization."],"url":"http://arxiv.org/abs/2405.04409v1","category":"math.OC"}
{"created":"2024-05-07 15:35:30","title":"Super-Exponential Regret for UCT, AlphaGo and Variants","abstract":"We improve the proofs of the lower bounds of Coquelin and Munos (2007) that demonstrate that UCT can have $\\exp(\\dots\\exp(1)\\dots)$ regret (with $\\Omega(D)$ exp terms) on the $D$-chain environment, and that a `polynomial' UCT variant has $\\exp_2(\\exp_2(D - O(\\log D)))$ regret on the same environment -- the original proofs contain an oversight for rewards bounded in $[0, 1]$, which we fix in the present draft. We also adapt the proofs to AlphaGo's MCTS and its descendants (e.g., AlphaZero, Leela Zero) to also show $\\exp_2(\\exp_2(D - O(\\log D)))$ regret.","sentences":["We improve the proofs of the lower bounds of Coquelin and Munos (2007) that demonstrate that UCT can have $\\exp(\\dots\\exp(1)\\dots)$ regret (with $\\Omega(D)$ exp terms) on the $D$-chain environment, and that a `polynomial' UCT variant has $\\exp_2(\\exp_2(D - O(\\log D)))$ regret on the same environment -- the original proofs contain an oversight for rewards bounded in $[0, 1]$, which we fix in the present draft.","We also adapt the proofs to AlphaGo's MCTS and its descendants (e.g., AlphaZero, Leela Zero) to also show $\\exp_2(\\exp_2(D - O(\\log D)))$ regret."],"url":"http://arxiv.org/abs/2405.04407v1","category":"cs.LG"}
{"created":"2024-05-07 15:30:14","title":"Vision Mamba: A Comprehensive Survey and Taxonomy","abstract":"State Space Model (SSM) is a mathematical model used to describe and analyze the behavior of dynamic systems. This model has witnessed numerous applications in several fields, including control theory, signal processing, economics and machine learning. In the field of deep learning, state space models are used to process sequence data, such as time series analysis, natural language processing (NLP) and video understanding. By mapping sequence data to state space, long-term dependencies in the data can be better captured. In particular, modern SSMs have shown strong representational capabilities in NLP, especially in long sequence modeling, while maintaining linear time complexity. Notably, based on the latest state-space models, Mamba merges time-varying parameters into SSMs and formulates a hardware-aware algorithm for efficient training and inference. Given its impressive efficiency and strong long-range dependency modeling capability, Mamba is expected to become a new AI architecture that may outperform Transformer. Recently, a number of works have attempted to study the potential of Mamba in various fields, such as general vision, multi-modal, medical image analysis and remote sensing image analysis, by extending Mamba from natural language domain to visual domain. To fully understand Mamba in the visual domain, we conduct a comprehensive survey and present a taxonomy study. This survey focuses on Mamba's application to a variety of visual tasks and data types, and discusses its predecessors, recent advances and far-reaching impact on a wide range of domains. Since Mamba is now on an upward trend, please actively notice us if you have new findings, and new progress on Mamba will be included in this survey in a timely manner and updated on the Mamba project at https://github.com/lx6c78/Vision-Mamba-A-Comprehensive-Survey-and-Taxonomy.","sentences":["State Space Model (SSM) is a mathematical model used to describe and analyze the behavior of dynamic systems.","This model has witnessed numerous applications in several fields, including control theory, signal processing, economics and machine learning.","In the field of deep learning, state space models are used to process sequence data, such as time series analysis, natural language processing (NLP) and video understanding.","By mapping sequence data to state space, long-term dependencies in the data can be better captured.","In particular, modern SSMs have shown strong representational capabilities in NLP, especially in long sequence modeling, while maintaining linear time complexity.","Notably, based on the latest state-space models, Mamba merges time-varying parameters into SSMs and formulates a hardware-aware algorithm for efficient training and inference.","Given its impressive efficiency and strong long-range dependency modeling capability, Mamba is expected to become a new AI architecture that may outperform Transformer.","Recently, a number of works have attempted to study the potential of Mamba in various fields, such as general vision, multi-modal, medical image analysis and remote sensing image analysis, by extending Mamba from natural language domain to visual domain.","To fully understand Mamba in the visual domain, we conduct a comprehensive survey and present a taxonomy study.","This survey focuses on Mamba's application to a variety of visual tasks and data types, and discusses its predecessors, recent advances and far-reaching impact on a wide range of domains.","Since Mamba is now on an upward trend, please actively notice us if you have new findings, and new progress on Mamba will be included in this survey in a timely manner and updated on the Mamba project at https://github.com/lx6c78/Vision-Mamba-A-Comprehensive-Survey-and-Taxonomy."],"url":"http://arxiv.org/abs/2405.04404v1","category":"cs.CV"}
{"created":"2024-05-07 15:26:51","title":"Data augmentation experiments with style-based quantum generative adversarial networks on trapped-ion and superconducting-qubit technologies","abstract":"In the current noisy intermediate scale quantum computing era, and after the significant progress of the quantum hardware we have seen in the past few years, it is of high importance to understand how different quantum algorithms behave on different types of hardware. This includes whether or not they can be implemented at all and, if so, what the quality of the results is. This work quantitatively demonstrates, for the first time, how the quantum generator architecture for the style-based quantum generative adversarial network (qGAN) can not only be implemented but also yield good results on two very different types of hardware for data augmentation: the IBM bm_torino quantum computer based on the Heron chip using superconducting transmon qubits and the aria-1 IonQ quantum computer based on trapped-ion qubits. The style-based qGAN, proposed in 2022, generalizes the state of the art for qGANs and allows for shallow-depth networks. The results obtained on both devices are of comparable quality, with the aria-1 device delivering somewhat more accurate results than the ibm_torino device, while the runtime on ibm_torino is significantly shorter than on aria-1. Parallelization of the circuits, using up to 48 qubits on IBM quantum systems and up to 24 qubits on the IonQ system, is also presented, reducing the number of submitted jobs and allowing for a substantial reduction of the runtime on the quantum processor to generate the total number of samples.","sentences":["In the current noisy intermediate scale quantum computing era, and after the significant progress of the quantum hardware we have seen in the past few years, it is of high importance to understand how different quantum algorithms behave on different types of hardware.","This includes whether or not they can be implemented at all and, if so, what the quality of the results is.","This work quantitatively demonstrates, for the first time, how the quantum generator architecture for the style-based quantum generative adversarial network (qGAN) can not only be implemented but also yield good results on two very different types of hardware for data augmentation: the IBM bm_torino quantum computer based on the Heron chip using superconducting transmon qubits and the aria-1 IonQ quantum computer based on trapped-ion qubits.","The style-based qGAN, proposed in 2022, generalizes the state of the art for qGANs and allows for shallow-depth networks.","The results obtained on both devices are of comparable quality, with the aria-1 device delivering somewhat more accurate results than the ibm_torino device, while the runtime on ibm_torino is significantly shorter than on aria-1.","Parallelization of the circuits, using up to 48 qubits on IBM quantum systems and up to 24 qubits on the IonQ system, is also presented, reducing the number of submitted jobs and allowing for a substantial reduction of the runtime on the quantum processor to generate the total number of samples."],"url":"http://arxiv.org/abs/2405.04401v1","category":"quant-ph"}
{"created":"2024-05-07 15:20:33","title":"Primordial monopoles, black holes and gravitational waves","abstract":"We show how topologically stable superheavy magnetic monopoles and primordial black holes can be generated at observable levels by the waterfall field in hybrid inflation models based on grand unified theories. In $SU(5) \\times U(1)_\\chi$ grand unification, the monopole mass is of order $4 \\times 10^{17}$ GeV, and it carries a single unit ($2 \\pi /e$) of Dirac magnetic charge as well as screened color magnetic charge. The monopole density is partially diluted to an observable value, and accompanied with the production of primordial black holes with mass of order $10^{17}$-$10^{19}$ g which may make up the entire dark matter in the universe. The tensor to scalar ratio $r$ is predicted to be of order $10^{-5}$ - $10^{-4}$ which should be testable in the next generation of CMB experiments such as CMB-S4 and LiteBIRD. The gravitational wave spectrum generated during the waterfall transition is also presented.","sentences":["We show how topologically stable superheavy magnetic monopoles and primordial black holes can be generated at observable levels by the waterfall field in hybrid inflation models based on grand unified theories.","In $SU(5) \\times U(1)_\\chi$ grand unification, the monopole mass is of order $4 \\times 10^{17}$ GeV, and it carries a single unit ($2 \\pi /e$) of Dirac magnetic charge as well as screened color magnetic charge.","The monopole density is partially diluted to an observable value, and accompanied with the production of primordial black holes with mass of order $10^{17}$-$10^{19}$ g which may make up the entire dark matter in the universe.","The tensor to scalar ratio $r$ is predicted to be of order $10^{-5}$ - $10^{-4}$ which should be testable in the next generation of CMB experiments such as CMB-S4 and LiteBIRD.","The gravitational wave spectrum generated during the waterfall transition is also presented."],"url":"http://arxiv.org/abs/2405.04397v1","category":"hep-ph"}
{"created":"2024-05-07 15:18:21","title":"Predicting Transonic Flowfields in Non-Homogeneous Unstructured Grids Using Autoencoder Graph Convolutional Networks","abstract":"This paper focuses on addressing challenges posed by non-homogeneous unstructured grids, commonly used in Computational Fluid Dynamics (CFD). Their prevalence in CFD scenarios has motivated the exploration of innovative approaches for generating reduced-order models. The core of our approach centers on geometric deep learning, specifically the utilization of graph convolutional network (GCN). The novel Autoencoder GCN architecture enhances prediction accuracy by propagating information to distant nodes and emphasizing influential points. This architecture, with GCN layers and encoding/decoding modules, reduces dimensionality based on pressure-gradient values. The autoencoder structure improves the network capability to identify key features, contributing to a more robust and accurate predictive model. To validate the proposed methodology, we analyzed two different test cases: wing-only model and wing--body configuration. Precise reconstruction of steady-state distributed quantities within a two-dimensional parametric space underscores the reliability and versatility of the implemented approach.","sentences":["This paper focuses on addressing challenges posed by non-homogeneous unstructured grids, commonly used in Computational Fluid Dynamics (CFD).","Their prevalence in CFD scenarios has motivated the exploration of innovative approaches for generating reduced-order models.","The core of our approach centers on geometric deep learning, specifically the utilization of graph convolutional network (GCN).","The novel Autoencoder GCN architecture enhances prediction accuracy by propagating information to distant nodes and emphasizing influential points.","This architecture, with GCN layers and encoding/decoding modules, reduces dimensionality based on pressure-gradient values.","The autoencoder structure improves the network capability to identify key features, contributing to a more robust and accurate predictive model.","To validate the proposed methodology, we analyzed two different test cases: wing-only model and wing--body configuration.","Precise reconstruction of steady-state distributed quantities within a two-dimensional parametric space underscores the reliability and versatility of the implemented approach."],"url":"http://arxiv.org/abs/2405.04396v1","category":"cs.CE"}
{"created":"2024-05-07 15:18:10","title":"PACIFISTA: Conflict Evaluation and Management in Open RAN","abstract":"The O-RAN ALLIANCE is defining architectures, interfaces, operations, and security requirements for cellular networks based on Open Radio Access Network (RAN) principles. In this context, O-RAN introduced the RAN Intelligent Controllers (RICs) to enable dynamic control of cellular networks via data-driven applications referred to as rApps and xApps. RICs enable for the first time truly intelligent and self-organizing cellular networks. However, enabling the execution of many Artificial Intelligence (AI) algorithms taking autonomous control decisions to fulfill diverse (and possibly conflicting) goals poses unprecedented challenges. For instance, the execution of one xApp aiming at maximizing throughput and one aiming at minimizing energy consumption would inevitably result in diametrically opposed resource allocation strategies. Therefore, conflict management becomes a crucial component of any functional intelligent O-RAN system. This article studies the problem of conflict mitigation in O-RAN and proposes PACIFISTA, a framework to detect, characterize, and mitigate conflicts. PACIFISTA leverages a profiling pipeline to tests O-RAN applications in a sandbox environment, and combines hierarchical graphs with statistical models to detect the existence of conflicts and evaluate their severity. Experiments on Colosseum and OpenRAN Gym demonstrate PACIFISTA's ability to predict conflicts and provide valuable information before potentially conflicting xApps are deployed in production systems. We demonstrate that even O-RAN applications with similar goals can result in 16% throughput loss, and show how applications with conflicting goals might cause severe instability and result in up to 30% performance degradation. We also show that PACIFISTA can help operators to identify coexisting applications and maintain performance degradation below a tolerable threshold.","sentences":["The O-RAN ALLIANCE is defining architectures, interfaces, operations, and security requirements for cellular networks based on Open Radio Access Network (RAN) principles.","In this context, O-RAN introduced the RAN Intelligent Controllers (RICs) to enable dynamic control of cellular networks via data-driven applications referred to as rApps and xApps.","RICs enable for the first time truly intelligent and self-organizing cellular networks.","However, enabling the execution of many Artificial Intelligence (AI) algorithms taking autonomous control decisions to fulfill diverse (and possibly conflicting) goals poses unprecedented challenges.","For instance, the execution of one xApp aiming at maximizing throughput and one aiming at minimizing energy consumption would inevitably result in diametrically opposed resource allocation strategies.","Therefore, conflict management becomes a crucial component of any functional intelligent O-RAN system.","This article studies the problem of conflict mitigation in O-RAN and proposes PACIFISTA, a framework to detect, characterize, and mitigate conflicts.","PACIFISTA leverages a profiling pipeline to tests O-RAN applications in a sandbox environment, and combines hierarchical graphs with statistical models to detect the existence of conflicts and evaluate their severity.","Experiments on Colosseum and OpenRAN Gym demonstrate PACIFISTA's ability to predict conflicts and provide valuable information before potentially conflicting xApps are deployed in production systems.","We demonstrate that even O-RAN applications with similar goals can result in 16% throughput loss, and show how applications with conflicting goals might cause severe instability and result in up to 30% performance degradation.","We also show that PACIFISTA can help operators to identify coexisting applications and maintain performance degradation below a tolerable threshold."],"url":"http://arxiv.org/abs/2405.04395v1","category":"cs.NI"}
{"created":"2024-05-07 15:14:51","title":"Efficient Online Set-valued Classification with Bandit Feedback","abstract":"Conformal prediction is a distribution-free method that wraps a given machine learning model and returns a set of plausible labels that contain the true label with a prescribed coverage rate. In practice, the empirical coverage achieved highly relies on fully observed label information from data both in the training phase for model fitting and the calibration phase for quantile estimation. This dependency poses a challenge in the context of online learning with bandit feedback, where a learner only has access to the correctness of actions (i.e., pulled an arm) but not the full information of the true label. In particular, when the pulled arm is incorrect, the learner only knows that the pulled one is not the true class label, but does not know which label is true. Additionally, bandit feedback further results in a smaller labeled dataset for calibration, limited to instances with correct actions, thereby affecting the accuracy of quantile estimation. To address these limitations, we propose Bandit Class-specific Conformal Prediction (BCCP), offering coverage guarantees on a class-specific granularity. Using an unbiased estimation of an estimand involving the true label, BCCP trains the model and makes set-valued inferences through stochastic gradient descent. Our approach overcomes the challenges of sparsely labeled data in each iteration and generalizes the reliability and applicability of conformal prediction to online decision-making environments.","sentences":["Conformal prediction is a distribution-free method that wraps a given machine learning model and returns a set of plausible labels that contain the true label with a prescribed coverage rate.","In practice, the empirical coverage achieved highly relies on fully observed label information from data both in the training phase for model fitting and the calibration phase for quantile estimation.","This dependency poses a challenge in the context of online learning with bandit feedback, where a learner only has access to the correctness of actions (i.e., pulled an arm) but not the full information of the true label.","In particular, when the pulled arm is incorrect, the learner only knows that the pulled one is not the true class label, but does not know which label is true.","Additionally, bandit feedback further results in a smaller labeled dataset for calibration, limited to instances with correct actions, thereby affecting the accuracy of quantile estimation.","To address these limitations, we propose Bandit Class-specific Conformal Prediction (BCCP), offering coverage guarantees on a class-specific granularity.","Using an unbiased estimation of an estimand involving the true label, BCCP trains the model and makes set-valued inferences through stochastic gradient descent.","Our approach overcomes the challenges of sparsely labeled data in each iteration and generalizes the reliability and applicability of conformal prediction to online decision-making environments."],"url":"http://arxiv.org/abs/2405.04393v1","category":"stat.ML"}
{"created":"2024-05-07 15:14:49","title":"BILTS: A novel bi-invariant local trajectory-shape descriptor for rigid-body motion","abstract":"Measuring the similarity between motions and established motion models is crucial for motion analysis, recognition, generation, and adaptation. To enhance similarity measurement across diverse contexts, invariant motion descriptors have been proposed. However, for rigid-body motion, few invariant descriptors exist that are bi-invariant, meaning invariant to both the body and world reference frames used to describe the motion. Moreover, their robustness to singularities is limited. This paper introduces a novel Bi-Invariant Local Trajectory-Shape descriptor (BILTS) and a corresponding dissimilarity measure. Mathematical relationships between BILTS and existing descriptors are derived, providing new insights into their properties. The paper also includes an algorithm to reproduce the motion from the BILTS descriptor, demonstrating its bidirectionality and usefulness for trajectory generation. Experimental validation using datasets of daily-life activities shows the higher robustness of the BILTS descriptor compared to the bi-invariant ISA descriptor. This higher robustness supports the further application of bi-invariant descriptors for motion recognition and generalization.","sentences":["Measuring the similarity between motions and established motion models is crucial for motion analysis, recognition, generation, and adaptation.","To enhance similarity measurement across diverse contexts, invariant motion descriptors have been proposed.","However, for rigid-body motion, few invariant descriptors exist that are bi-invariant, meaning invariant to both the body and world reference frames used to describe the motion.","Moreover, their robustness to singularities is limited.","This paper introduces a novel Bi-Invariant Local Trajectory-Shape descriptor (BILTS) and a corresponding dissimilarity measure.","Mathematical relationships between BILTS and existing descriptors are derived, providing new insights into their properties.","The paper also includes an algorithm to reproduce the motion from the BILTS descriptor, demonstrating its bidirectionality and usefulness for trajectory generation.","Experimental validation using datasets of daily-life activities shows the higher robustness of the BILTS descriptor compared to the bi-invariant ISA descriptor.","This higher robustness supports the further application of bi-invariant descriptors for motion recognition and generalization."],"url":"http://arxiv.org/abs/2405.04392v1","category":"cs.RO"}
{"created":"2024-05-07 15:11:42","title":"Pragmatist Intelligence: Where the Principle of Usefulness Can Take ANNs","abstract":"Artificial neural networks (ANNs) perform extraordinarily on numerous tasks including classification or prediction, e.g., speech processing and image classification. These new functions are based on a computational model that is enabled to select freely all necessary internal model parameters as long as it eventually delivers the functionality it is supposed to exhibit. Here, we review the connection between the model parameter selection in machine learning (ML) algorithms running on ANNs and the epistemological theory of neopragmatism focusing on the theory's utility and anti-representationalist aspects. To understand the consequences of the model parameter selection of an ANN, we suggest using neopragmatist theories whose implications are well studied. Incidentally, neopragmatism's notion of optimization is also based on utility considerations. This means that applying this approach elegantly reveals the inherent connections between optimization in ML, using a numerical method during the learning phase, and optimization in the ethical theory of consequentialism, where it occurs as a maxim of action. We suggest that these connections originate from the way relevance is calculated in ML systems. This could ultimately reveal a tendency for specific actions in ML systems.","sentences":["Artificial neural networks (ANNs) perform extraordinarily on numerous tasks including classification or prediction, e.g., speech processing and image classification.","These new functions are based on a computational model that is enabled to select freely all necessary internal model parameters as long as it eventually delivers the functionality it is supposed to exhibit.","Here, we review the connection between the model parameter selection in machine learning (ML) algorithms running on ANNs and the epistemological theory of neopragmatism focusing on the theory's utility and anti-representationalist aspects.","To understand the consequences of the model parameter selection of an ANN, we suggest using neopragmatist theories whose implications are well studied.","Incidentally, neopragmatism's notion of optimization is also based on utility considerations.","This means that applying this approach elegantly reveals the inherent connections between optimization in ML, using a numerical method during the learning phase, and optimization in the ethical theory of consequentialism, where it occurs as a maxim of action.","We suggest that these connections originate from the way relevance is calculated in ML systems.","This could ultimately reveal a tendency for specific actions in ML systems."],"url":"http://arxiv.org/abs/2405.04386v1","category":"cs.AI"}
{"created":"2024-05-07 15:06:53","title":"Geodesic connectivity and rooftop envelopes in the Cegrell classes","abstract":"This study examines geodesics and plurisubharmonic envelopes within the Cegrell classes on bounded hyperconvex domains in $\\mathbb{C}^n$. We establish that solutions possessing comparable singularities to the complex Monge-Amp\\`ere equation are identical, affirmatively addressing a longstanding open question raised by Cegrell. This achievement furnishes the most general form of the Bedford-Taylor comparison principle within the Cegrell classes. Building on this foundational result, we explore plurisubharmonic geodesics, broadening the criteria for geodesic connectivity among plurisubharmonic functions with connectable boundary values. Our investigation also delves into the notion of rooftop envelopes, revealing that the rooftop equality condition and the idempotency conjecture are valid under substantially weaker conditions than previously established, a finding made possible by our proven uniqueness result. The paper concludes by discussing the core open problems within the Cegrell classes related to the complex Monge-Amp\\`ere equation.","sentences":["This study examines geodesics and plurisubharmonic envelopes within the Cegrell classes on bounded hyperconvex domains in $\\mathbb{C}^n$. We establish that solutions possessing comparable singularities to the complex Monge-Amp\\`ere equation are identical, affirmatively addressing a longstanding open question raised by Cegrell.","This achievement furnishes the most general form of the Bedford-Taylor comparison principle within the Cegrell classes.","Building on this foundational result, we explore plurisubharmonic geodesics, broadening the criteria for geodesic connectivity among plurisubharmonic functions with connectable boundary values.","Our investigation also delves into the notion of rooftop envelopes, revealing that the rooftop equality condition and the idempotency conjecture are valid under substantially weaker conditions than previously established, a finding made possible by our proven uniqueness result.","The paper concludes by discussing the core open problems within the Cegrell classes related to the complex Monge-Amp\\`ere equation."],"url":"http://arxiv.org/abs/2405.04384v1","category":"math.CV"}
{"created":"2024-05-07 15:05:23","title":"Large Language Models Cannot Explain Themselves","abstract":"Large language models can be prompted to produce text. They can also be prompted to produce \"explanations\" of their output. But these are not really explanations, because they do not accurately reflect the mechanical process underlying the prediction. The illusion that they reflect the reasoning process can result in significant harms. These \"explanations\" can be valuable, but for promoting critical thinking rather than for understanding the model. I propose a recontextualisation of these \"explanations\", using the term \"exoplanations\" to draw attention to their exogenous nature. I discuss some implications for design and technology, such as the inclusion of appropriate guardrails and responses when models are prompted to generate explanations.","sentences":["Large language models can be prompted to produce text.","They can also be prompted to produce \"explanations\" of their output.","But these are not really explanations, because they do not accurately reflect the mechanical process underlying the prediction.","The illusion that they reflect the reasoning process can result in significant harms.","These \"explanations\" can be valuable, but for promoting critical thinking rather than for understanding the model.","I propose a recontextualisation of these \"explanations\", using the term \"exoplanations\" to draw attention to their exogenous nature.","I discuss some implications for design and technology, such as the inclusion of appropriate guardrails and responses when models are prompted to generate explanations."],"url":"http://arxiv.org/abs/2405.04382v1","category":"cs.HC"}
{"created":"2024-05-07 15:05:15","title":"Limits to positional information in boundary-driven systems","abstract":"Chemical gradients can be used by a particle to determine its position. This \\textit{positional information} is of crucial importance, for example in developmental biology in the formation of patterns in an embryo. The central goal of this paper is to study the fundamental physical limits on how much positional information can be stored inside a system. To achieve this, we study positional information for general boundary-driven systems, and derive, in the near-equilibrium regime, a universal expression involving only the chemical potential and density gradients of the system. We also conjecture that this expression serves as an upper bound on the positional information of boundary driven systems beyond linear response. To support this claim, we test it on a broad range of solvable boundary-driven systems.","sentences":["Chemical gradients can be used by a particle to determine its position.","This \\textit{positional information} is of crucial importance, for example in developmental biology in the formation of patterns in an embryo.","The central goal of this paper is to study the fundamental physical limits on how much positional information can be stored inside a system.","To achieve this, we study positional information for general boundary-driven systems, and derive, in the near-equilibrium regime, a universal expression involving only the chemical potential and density gradients of the system.","We also conjecture that this expression serves as an upper bound on the positional information of boundary driven systems beyond linear response.","To support this claim, we test it on a broad range of solvable boundary-driven systems."],"url":"http://arxiv.org/abs/2405.04381v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-07 15:02:58","title":"Preserving Nonlinear Constraints in Variational Flow Filtering Data Assimilation","abstract":"Data assimilation aims to estimate the states of a dynamical system by optimally combining sparse and noisy observations of the physical system with uncertain forecasts produced by a computational model. The states of many dynamical systems of interest obey nonlinear physical constraints, and the corresponding dynamics is confined to a certain sub-manifold of the state space. Standard data assimilation techniques applied to such systems yield posterior states lying outside the manifold, violating the physical constraints. This work focuses on particle flow filters which use stochastic differential equations to evolve state samples from a prior distribution to samples from an observation-informed posterior distribution. The variational Fokker-Planck (VFP) -- a generic particle flow filtering framework -- is extended to incorporate non-linear, equality state constraints in the analysis. To this end, two algorithmic approaches that modify the VFP stochastic differential equation are discussed: (i) VFPSTAB, to inexactly preserve constraints with the addition of a stabilizing drift term, and (ii) VFPDAE, to exactly preserve constraints by treating the VFP dynamics as a stochastic differential-algebraic equation (SDAE). Additionally, an implicit-explicit time integrator is developed to evolve the VFPDAE dynamics. The strength of the proposed approach for constraint preservation in data assimilation is demonstrated on three test problems: the double pendulum, Korteweg-de-Vries, and the incompressible Navier-Stokes equations.","sentences":["Data assimilation aims to estimate the states of a dynamical system by optimally combining sparse and noisy observations of the physical system with uncertain forecasts produced by a computational model.","The states of many dynamical systems of interest obey nonlinear physical constraints, and the corresponding dynamics is confined to a certain sub-manifold of the state space.","Standard data assimilation techniques applied to such systems yield posterior states lying outside the manifold, violating the physical constraints.","This work focuses on particle flow filters which use stochastic differential equations to evolve state samples from a prior distribution to samples from an observation-informed posterior distribution.","The variational Fokker-Planck (VFP) -- a generic particle flow filtering framework -- is extended to incorporate non-linear, equality state constraints in the analysis.","To this end, two algorithmic approaches that modify the VFP stochastic differential equation are discussed: (i) VFPSTAB, to inexactly preserve constraints with the addition of a stabilizing drift term, and (ii) VFPDAE, to exactly preserve constraints by treating the VFP dynamics as a stochastic differential-algebraic equation (SDAE).","Additionally, an implicit-explicit time integrator is developed to evolve the VFPDAE dynamics.","The strength of the proposed approach for constraint preservation in data assimilation is demonstrated on three test problems: the double pendulum, Korteweg-de-Vries, and the incompressible Navier-Stokes equations."],"url":"http://arxiv.org/abs/2405.04380v1","category":"math.OC"}
{"created":"2024-05-07 15:02:21","title":"Dressing fields for supersymmetry: The cases of the Rarita-Schwinger and gravitino fields","abstract":"In this paper we argue that the gauge-fixing conditions typically used to extract the (off-shell) degrees of freedom of the Rarita-Schwinger spinor-vector and gravitino, respectively in rigid supersymmetric field theory and supergravity, are actually instances of the dressing field method of symmetry reduction. Since the latter has a natural relation interpretation, solving the ``gauge-fixing condition\" -- or, better, ``dressing functional constraints\" -- actually realises the Rarita-Schwinger spinor-vector and the gravitino fields as (non-local) relational variables. To the best of our knowledge, this is the first application of the dressing field method to supersymmetric theories.","sentences":["In this paper we argue that the gauge-fixing conditions typically used to extract the (off-shell) degrees of freedom of the Rarita-Schwinger spinor-vector and gravitino, respectively in rigid supersymmetric field theory and supergravity, are actually instances of the dressing field method of symmetry reduction.","Since the latter has a natural relation interpretation, solving the ``gauge-fixing condition\" -- or, better, ``dressing functional constraints\" -- actually realises the Rarita-Schwinger spinor-vector and the gravitino fields as (non-local) relational variables.","To the best of our knowledge, this is the first application of the dressing field method to supersymmetric theories."],"url":"http://arxiv.org/abs/2405.04379v1","category":"hep-th"}
{"created":"2024-05-07 15:00:19","title":"$\\textbf{Splat-MOVER}$: Multi-Stage, Open-Vocabulary Robotic Manipulation via Editable Gaussian Splatting","abstract":"We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic manipulation, which leverages the editability of Gaussian Splatting (GSplat) scene representations to enable multi-stage manipulation tasks. Splat-MOVER consists of: (i) $\\textit{ASK-Splat}$, a GSplat representation that distills latent codes for language semantics and grasp affordance into the 3D scene. ASK-Splat enables geometric, semantic, and affordance understanding of 3D scenes, which is critical for many robotics tasks; (ii) $\\textit{SEE-Splat}$, a real-time scene-editing module using 3D semantic masking and infilling to visualize the motions of objects that result from robot interactions in the real-world. SEE-Splat creates a \"digital twin\" of the evolving environment throughout the manipulation task; and (iii) $\\textit{Grasp-Splat}$, a grasp generation module that uses ASK-Splat and SEE-Splat to propose candidate grasps for open-world objects. ASK-Splat is trained in real-time from RGB images in a brief scanning phase prior to operation, while SEE-Splat and Grasp-Splat run in real-time during operation. We demonstrate the superior performance of Splat-MOVER in hardware experiments on a Kinova robot compared to two recent baselines in four single-stage, open-vocabulary manipulation tasks, as well as in four multi-stage manipulation tasks using the edited scene to reflect scene changes due to prior manipulation stages, which is not possible with the existing baselines. Code for this project and a link to the project page will be made available soon.","sentences":["We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic manipulation, which leverages the editability of Gaussian Splatting (GSplat) scene representations to enable multi-stage manipulation tasks.","Splat-MOVER consists of: (i) $\\textit{ASK-Splat}$, a GSplat representation that distills latent codes for language semantics and grasp affordance into the 3D scene.","ASK-Splat enables geometric, semantic, and affordance understanding of 3D scenes, which is critical for many robotics tasks; (ii) $\\textit{SEE-Splat}$, a real-time scene-editing module using 3D semantic masking and infilling to visualize the motions of objects that result from robot interactions in the real-world.","SEE-Splat creates a \"digital twin\" of the evolving environment throughout the manipulation task; and (iii) $\\textit{Grasp-Splat}$, a grasp generation module that uses ASK-Splat and SEE-Splat to propose candidate grasps for open-world objects.","ASK-Splat is trained in real-time from RGB images in a brief scanning phase prior to operation, while SEE-Splat and Grasp-Splat run in real-time during operation.","We demonstrate the superior performance of Splat-MOVER in hardware experiments on a Kinova robot compared to two recent baselines in four single-stage, open-vocabulary manipulation tasks, as well as in four multi-stage manipulation tasks using the edited scene to reflect scene changes due to prior manipulation stages, which is not possible with the existing baselines.","Code for this project and a link to the project page will be made available soon."],"url":"http://arxiv.org/abs/2405.04378v1","category":"cs.RO"}
{"created":"2024-05-07 14:58:08","title":"Coherent distributions: Hilbert space approach and duality","abstract":"Let $X$ be a Bernoulli random variable with the success probability $p$. We are interested in tight bounds on $\\mathbb{E}[f(X_1,X_2)]$, where $X_i=\\mathbb{E}[X| \\mathcal{F}_i]$ and $\\mathcal{F}_i$ are some sigma-algebras. This problem is closely related to understanding extreme points of the set of coherent distributions. A distribution on $[0,1]^2$ is called $\\textit{coherent}$ if it can be obtained as the joint distribution of $(X_1, X_2)$ for some choice of $\\mathcal{F}_i$. By treating random variables as vectors in a Hilbert space, we establish an upper bound for quadratic $f$, characterize $f$ for which this bound is tight, and show that such $f$ result in exposed coherent distributions with arbitrarily large support. As a corollary, we get a tight bound on $\\mathrm{cov}\\,(X_1,X_2)$ for $p\\in [1/3,\\,2/3]$. To obtain a tight bound on $\\mathrm{cov}\\,(X_1,X_2)$ for all $p$, we develop an approach based on linear programming duality. Its generality is illustrated by tight bounds on $\\mathbb{E}[|X_1-X_2|^\\alpha]$ for any $\\alpha>0$ and $p=1/2$.","sentences":["Let $X$ be a Bernoulli random variable with the success probability $p$. We are interested in tight bounds on $\\mathbb{E}[f(X_1,X_2)]$, where $X_i=\\mathbb{E}[X| \\mathcal{F}_i]$ and $\\mathcal{F}_i$ are some sigma-algebras.","This problem is closely related to understanding extreme points of the set of coherent distributions.","A distribution on $[0,1]^2$ is called $\\textit{coherent}$ if it can be obtained as the joint distribution of $(X_1, X_2)$ for some choice of $\\mathcal{F}_i$. By treating random variables as vectors in a Hilbert space, we establish an upper bound for quadratic $f$, characterize $f$ for which this bound is tight, and show that such $f$ result in exposed coherent distributions with arbitrarily large support.","As a corollary, we get a tight bound on $\\mathrm{cov}\\,(X_1,X_2)$ for $p\\in [1/3,\\,2/3]$. To obtain a tight bound on $\\mathrm{cov}\\,(X_1,X_2)$ for all $p$, we develop an approach based on linear programming duality.","Its generality is illustrated by tight bounds on $\\mathbb{E}[|X_1-X_2|^\\alpha]$ for any $\\alpha>0$ and $p=1/2$."],"url":"http://arxiv.org/abs/2405.04375v1","category":"math.PR"}
{"created":"2024-05-07 14:57:24","title":"Leveraging LSTM and GAN for Modern Malware Detection","abstract":"The malware booming is a cyberspace equal to the effect of climate change to ecosystems in terms of danger. In the case of significant investments in cybersecurity technologies and staff training, the global community has become locked up in the eternal war with cyber security threats. The multi-form and changing faces of malware are continuously pushing the boundaries of the cybersecurity practitioners employ various approaches like detection and mitigate in coping with this issue. Some old mannerisms like signature-based detection and behavioral analysis are slow to adapt to the speedy evolution of malware types. Consequently, this paper proposes the utilization of the Deep Learning Model, LSTM networks, and GANs to amplify malware detection accuracy and speed. A fast-growing, state-of-the-art technology that leverages raw bytestream-based data and deep learning architectures, the AI technology provides better accuracy and performance than the traditional methods. Integration of LSTM and GAN model is the technique that is used for the synthetic generation of data, leading to the expansion of the training datasets, and as a result, the detection accuracy is improved. The paper uses the VirusShare dataset which has more than one million unique samples of the malware as the training and evaluation set for the presented models. Through thorough data preparation including tokenization, augmentation, as well as model training, the LSTM and GAN models convey the better performance in the tasks compared to straight classifiers. The research outcomes come out with 98% accuracy that shows the efficiency of deep learning plays a decisive role in proactive cybersecurity defense. Aside from that, the paper studies the output of ensemble learning and model fusion methods as a way to reduce biases and lift model complexity.","sentences":["The malware booming is a cyberspace equal to the effect of climate change to ecosystems in terms of danger.","In the case of significant investments in cybersecurity technologies and staff training, the global community has become locked up in the eternal war with cyber security threats.","The multi-form and changing faces of malware are continuously pushing the boundaries of the cybersecurity practitioners employ various approaches like detection and mitigate in coping with this issue.","Some old mannerisms like signature-based detection and behavioral analysis are slow to adapt to the speedy evolution of malware types.","Consequently, this paper proposes the utilization of the Deep Learning Model, LSTM networks, and GANs to amplify malware detection accuracy and speed.","A fast-growing, state-of-the-art technology that leverages raw bytestream-based data and deep learning architectures, the AI technology provides better accuracy and performance than the traditional methods.","Integration of LSTM and GAN model is the technique that is used for the synthetic generation of data, leading to the expansion of the training datasets, and as a result, the detection accuracy is improved.","The paper uses the VirusShare dataset which has more than one million unique samples of the malware as the training and evaluation set for the presented models.","Through thorough data preparation including tokenization, augmentation, as well as model training, the LSTM and GAN models convey the better performance in the tasks compared to straight classifiers.","The research outcomes come out with 98% accuracy that shows the efficiency of deep learning plays a decisive role in proactive cybersecurity defense.","Aside from that, the paper studies the output of ensemble learning and model fusion methods as a way to reduce biases and lift model complexity."],"url":"http://arxiv.org/abs/2405.04373v1","category":"cs.CR"}
{"created":"2024-05-07 14:55:42","title":"Explainable machine learning for predicting shellfish toxicity in the Adriatic Sea using long-term monitoring data of HABs","abstract":"In this study, explainable machine learning techniques are applied to predict the toxicity of mussels in the Gulf of Trieste (Adriatic Sea) caused by harmful algal blooms. By analysing a newly created 28-year dataset containing records of toxic phytoplankton in mussel farming areas and toxin concentrations in mussels (Mytilus galloprovincialis), we train and evaluate the performance of ML models to accurately predict diarrhetic shellfish poisoning (DSP) events. The random forest model provided the best prediction of positive toxicity results based on the F1 score. Explainability methods such as permutation importance and SHAP identified key species (Dinophysis fortii and D. caudata) and environmental factors (salinity, river discharge and precipitation) as the best predictors of DSP outbreaks. These findings are important for improving early warning systems and supporting sustainable aquaculture practices.","sentences":["In this study, explainable machine learning techniques are applied to predict the toxicity of mussels in the Gulf of Trieste (Adriatic Sea) caused by harmful algal blooms.","By analysing a newly created 28-year dataset containing records of toxic phytoplankton in mussel farming areas and toxin concentrations in mussels (Mytilus galloprovincialis), we train and evaluate the performance of ML models to accurately predict diarrhetic shellfish poisoning (DSP) events.","The random forest model provided the best prediction of positive toxicity results based on the F1 score.","Explainability methods such as permutation importance and SHAP identified key species (Dinophysis fortii and D. caudata) and environmental factors (salinity, river discharge and precipitation) as the best predictors of DSP outbreaks.","These findings are important for improving early warning systems and supporting sustainable aquaculture practices."],"url":"http://arxiv.org/abs/2405.04372v1","category":"cs.LG"}
{"created":"2024-05-07 14:52:34","title":"Community Detection for Heterogeneous Multiple Social Networks","abstract":"The community plays a crucial role in understanding user behavior and network characteristics in social networks. Some users can use multiple social networks at once for a variety of objectives. These users are called overlapping users who bridge different social networks. Detecting communities across multiple social networks is vital for interaction mining, information diffusion, and behavior migration analysis among networks. This paper presents a community detection method based on nonnegative matrix tri-factorization for multiple heterogeneous social networks, which formulates a common consensus matrix to represent the global fused community. Specifically, the proposed method involves creating adjacency matrices based on network structure and content similarity, followed by alignment matrices which distinguish overlapping users in different social networks. With the generated alignment matrices, the method could enhance the fusion degree of the global community by detecting overlapping user communities across networks. The effectiveness of the proposed method is evaluated with new metrics on Twitter, Instagram, and Tumblr datasets. The results of the experiments demonstrate its superior performance in terms of community quality and community fusion.","sentences":["The community plays a crucial role in understanding user behavior and network characteristics in social networks.","Some users can use multiple social networks at once for a variety of objectives.","These users are called overlapping users who bridge different social networks.","Detecting communities across multiple social networks is vital for interaction mining, information diffusion, and behavior migration analysis among networks.","This paper presents a community detection method based on nonnegative matrix tri-factorization for multiple heterogeneous social networks, which formulates a common consensus matrix to represent the global fused community.","Specifically, the proposed method involves creating adjacency matrices based on network structure and content similarity, followed by alignment matrices which distinguish overlapping users in different social networks.","With the generated alignment matrices, the method could enhance the fusion degree of the global community by detecting overlapping user communities across networks.","The effectiveness of the proposed method is evaluated with new metrics on Twitter, Instagram, and Tumblr datasets.","The results of the experiments demonstrate its superior performance in terms of community quality and community fusion."],"url":"http://arxiv.org/abs/2405.04371v1","category":"cs.SI"}
{"created":"2024-05-07 14:51:05","title":"Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos","abstract":"Understanding how humans would behave during hand-object interaction is vital for applications in service robot manipulation and extended reality. To achieve this, some recent works have been proposed to simultaneously predict hand trajectories and object affordances on human egocentric videos. They are regarded as the representation of future hand-object interactions, indicating potential human motion and motivation. However, the existing approaches mostly adopt the autoregressive paradigm for unidirectional prediction, which lacks mutual constraints within the holistic future sequence, and accumulates errors along the time axis. Meanwhile, these works basically overlook the effect of camera egomotion on first-person view predictions. To address these limitations, we propose a novel diffusion-based interaction prediction method, namely Diff-IP2D, to forecast future hand trajectories and object affordances concurrently in an iterative non-autoregressive manner. We transform the sequential 2D images into latent feature space and design a denoising diffusion model to predict future latent interaction features conditioned on past ones. Motion features are further integrated into the conditional denoising process to enable Diff-IP2D aware of the camera wearer's dynamics for more accurate interaction prediction. The experimental results show that our method significantly outperforms the state-of-the-art baselines on both the off-the-shelf metrics and our proposed new evaluation protocol. This highlights the efficacy of leveraging a generative paradigm for 2D hand-object interaction prediction. The code of Diff-IP2D will be released at https://github.com/IRMVLab/Diff-IP2D.","sentences":["Understanding how humans would behave during hand-object interaction is vital for applications in service robot manipulation and extended reality.","To achieve this, some recent works have been proposed to simultaneously predict hand trajectories and object affordances on human egocentric videos.","They are regarded as the representation of future hand-object interactions, indicating potential human motion and motivation.","However, the existing approaches mostly adopt the autoregressive paradigm for unidirectional prediction, which lacks mutual constraints within the holistic future sequence, and accumulates errors along the time axis.","Meanwhile, these works basically overlook the effect of camera egomotion on first-person view predictions.","To address these limitations, we propose a novel diffusion-based interaction prediction method, namely Diff-IP2D, to forecast future hand trajectories and object affordances concurrently in an iterative non-autoregressive manner.","We transform the sequential 2D images into latent feature space and design a denoising diffusion model to predict future latent interaction features conditioned on past ones.","Motion features are further integrated into the conditional denoising process to enable Diff-IP2D aware of the camera wearer's dynamics for more accurate interaction prediction.","The experimental results show that our method significantly outperforms the state-of-the-art baselines on both the off-the-shelf metrics and our proposed new evaluation protocol.","This highlights the efficacy of leveraging a generative paradigm for 2D hand-object interaction prediction.","The code of Diff-IP2D will be released at https://github.com/IRMVLab/Diff-IP2D."],"url":"http://arxiv.org/abs/2405.04370v1","category":"cs.CV"}
{"created":"2024-05-07 14:48:42","title":"Quantum Circuit for Imputation of Missing Data","abstract":"The imputation of missing data is a common procedure in data analysis that consists in predicting missing values of incomplete data points. In this work we analyse a variational quantum circuit for the imputation of missing data. We construct variational quantum circuits with gates complexity $O(N)$ and $O(N^2)$ that return the last missing bit of a binary string for a specific distribution. We train and test the performance of the algorithms on a series of datasets finding good convergence of the results. Finally, we test the circuit for generalization to unseen data. For simple systems, we are able to describe the circuit analytically, making possible to skip the tedious and unresolved problem of training the circuit with repetitive measurements. We find beforehand the optimal values of the parameters and we make use of them to construct an optimal circuit suited to the generation of truly random data.","sentences":["The imputation of missing data is a common procedure in data analysis that consists in predicting missing values of incomplete data points.","In this work we analyse a variational quantum circuit for the imputation of missing data.","We construct variational quantum circuits with gates complexity $O(N)$ and $O(N^2)$ that return the last missing bit of a binary string for a specific distribution.","We train and test the performance of the algorithms on a series of datasets finding good convergence of the results.","Finally, we test the circuit for generalization to unseen data.","For simple systems, we are able to describe the circuit analytically, making possible to skip the tedious and unresolved problem of training the circuit with repetitive measurements.","We find beforehand the optimal values of the parameters and we make use of them to construct an optimal circuit suited to the generation of truly random data."],"url":"http://arxiv.org/abs/2405.04367v1","category":"quant-ph"}
{"created":"2024-05-07 14:45:43","title":"Detailed Gender Wage Gap Decompositions: Controlling for Worker Unobserved Heterogeneity Using Network Theory","abstract":"Recent advances in the literature of decomposition methods in economics have allowed for the identification and estimation of detailed wage gap decompositions. In this context, building reliable counterfactuals requires using tighter controls to ensure that similar workers are correctly identified by making sure that important unobserved variables such as skills are controlled for, as well as comparing only workers with similar observable characteristics. This paper contributes to the wage decomposition literature in two main ways: (i) developing an economic principled network based approach to control for unobserved worker skills heterogeneity in the presence of potential discrimination; and (ii) extending existing generic decomposition tools to accommodate for potential lack of overlapping supports in covariates between groups being compared, which is likely to be the norm in more detailed decompositions. We illustrate the methodology by decomposing the gender wage gap in Brazil.","sentences":["Recent advances in the literature of decomposition methods in economics have allowed for the identification and estimation of detailed wage gap decompositions.","In this context, building reliable counterfactuals requires using tighter controls to ensure that similar workers are correctly identified by making sure that important unobserved variables such as skills are controlled for, as well as comparing only workers with similar observable characteristics.","This paper contributes to the wage decomposition literature in two main ways: (i) developing an economic principled network based approach to control for unobserved worker skills heterogeneity in the presence of potential discrimination; and (ii) extending existing generic decomposition tools to accommodate for potential lack of overlapping supports in covariates between groups being compared, which is likely to be the norm in more detailed decompositions.","We illustrate the methodology by decomposing the gender wage gap in Brazil."],"url":"http://arxiv.org/abs/2405.04365v1","category":"econ.EM"}
{"created":"2024-05-07 14:33:45","title":"Global Scale Self-Supervised Channel Charting with Sensor Fusion","abstract":"The sensing and positioning capabilities foreseen in 6G have great potential for technology advancements in various domains, such as future smart cities and industrial use cases. Channel charting has emerged as a promising technology in recent years for radio frequency-based sensing and localization. However, the accuracy of these techniques is yet far behind the numbers envisioned in 6G. To reduce this gap, in this paper, we propose a novel channel charting technique capitalizing on the time of arrival measurements from surrounding Transmission Reception Points (TRPs) along with their locations and leveraging sensor fusion in channel charting by incorporating laser scanner data during the training phase of our algorithm. The proposed algorithm remains self-supervised during training and test phases, requiring no geometrical models or user position ground truth. Simulation results validate the achievement of a sub-meter level localization accuracy using our algorithm 90% of the time, outperforming the state-of-the-art channel charting techniques and the traditional triangulation-based approaches.","sentences":["The sensing and positioning capabilities foreseen in 6G have great potential for technology advancements in various domains, such as future smart cities and industrial use cases.","Channel charting has emerged as a promising technology in recent years for radio frequency-based sensing and localization.","However, the accuracy of these techniques is yet far behind the numbers envisioned in 6G. To reduce this gap, in this paper, we propose a novel channel charting technique capitalizing on the time of arrival measurements from surrounding Transmission Reception Points (TRPs) along with their locations and leveraging sensor fusion in channel charting by incorporating laser scanner data during the training phase of our algorithm.","The proposed algorithm remains self-supervised during training and test phases, requiring no geometrical models or user position ground truth.","Simulation results validate the achievement of a sub-meter level localization accuracy using our algorithm 90% of the time, outperforming the state-of-the-art channel charting techniques and the traditional triangulation-based approaches."],"url":"http://arxiv.org/abs/2405.04357v1","category":"cs.IT"}
{"created":"2024-05-07 14:33:40","title":"Diffusion-driven GAN Inversion for Multi-Modal Face Image Generation","abstract":"We present a new multi-modal face image generation method that converts a text prompt and a visual input, such as a semantic mask or scribble map, into a photo-realistic face image. To do this, we combine the strengths of Generative Adversarial networks (GANs) and diffusion models (DMs) by employing the multi-modal features in the DM into the latent space of the pre-trained GANs. We present a simple mapping and a style modulation network to link two models and convert meaningful representations in feature maps and attention maps into latent codes. With GAN inversion, the estimated latent codes can be used to generate 2D or 3D-aware facial images. We further present a multi-step training strategy that reflects textual and structural representations into the generated image. Our proposed network produces realistic 2D, multi-view, and stylized face images, which align well with inputs. We validate our method by using pre-trained 2D and 3D GANs, and our results outperform existing methods. Our project page is available at https://github.com/1211sh/Diffusion-driven_GAN-Inversion/.","sentences":["We present a new multi-modal face image generation method that converts a text prompt and a visual input, such as a semantic mask or scribble map, into a photo-realistic face image.","To do this, we combine the strengths of Generative Adversarial networks (GANs) and diffusion models (DMs) by employing the multi-modal features in the DM into the latent space of the pre-trained GANs.","We present a simple mapping and a style modulation network to link two models and convert meaningful representations in feature maps and attention maps into latent codes.","With GAN inversion, the estimated latent codes can be used to generate 2D or 3D-aware facial images.","We further present a multi-step training strategy that reflects textual and structural representations into the generated image.","Our proposed network produces realistic 2D, multi-view, and stylized face images, which align well with inputs.","We validate our method by using pre-trained 2D and 3D GANs, and our results outperform existing methods.","Our project page is available at https://github.com/1211sh/Diffusion-driven_GAN-Inversion/."],"url":"http://arxiv.org/abs/2405.04356v1","category":"cs.CV"}
{"created":"2024-05-07 14:31:26","title":"A transversality theorem for semi-algebraic sets with application to signal recovery from the second moment and cryo-EM","abstract":"Semi-algebraic priors are ubiquitous in signal processing and machine learning. Prevalent examples include a) linear models where the signal lies in a low-dimensional subspace; b) sparse models where the signal can be represented by only a few coefficients under a suitable basis; and c) a large family of neural network generative models. In this paper, we prove a transversality theorem for semi-algebraic sets in orthogonal or unitary representations of groups: with a suitable dimension bound, a generic translate of any semi-algebraic set is transverse to the orbits of the group action. This, in turn, implies that if a signal lies in a low-dimensional semi-algebraic set, then it can be recovered uniquely from measurements that separate orbits.   As an application, we consider the implications of the transversality theorem to the problem of recovering signals that are translated by random group actions from their second moment. As a special case, we discuss cryo-EM: a leading technology to constitute the spatial structure of biological molecules, which serves as our prime motivation. In particular, we derive explicit bounds for recovering a molecular structure from the second moment under a semi-algebraic prior and deduce information-theoretic implications. We also obtain information-theoretic bounds for three additional applications: factoring Gram matrices, multi-reference alignment, and phase retrieval. Finally, we deduce bounds for designing permutation invariant separators in machine learning.","sentences":["Semi-algebraic priors are ubiquitous in signal processing and machine learning.","Prevalent examples include a) linear models where the signal lies in a low-dimensional subspace; b) sparse models where the signal can be represented by only a few coefficients under a suitable basis; and c) a large family of neural network generative models.","In this paper, we prove a transversality theorem for semi-algebraic sets in orthogonal or unitary representations of groups: with a suitable dimension bound, a generic translate of any semi-algebraic set is transverse to the orbits of the group action.","This, in turn, implies that if a signal lies in a low-dimensional semi-algebraic set, then it can be recovered uniquely from measurements that separate orbits.   ","As an application, we consider the implications of the transversality theorem to the problem of recovering signals that are translated by random group actions from their second moment.","As a special case, we discuss cryo-EM: a leading technology to constitute the spatial structure of biological molecules, which serves as our prime motivation.","In particular, we derive explicit bounds for recovering a molecular structure from the second moment under a semi-algebraic prior and deduce information-theoretic implications.","We also obtain information-theoretic bounds for three additional applications: factoring Gram matrices, multi-reference alignment, and phase retrieval.","Finally, we deduce bounds for designing permutation invariant separators in machine learning."],"url":"http://arxiv.org/abs/2405.04354v1","category":"cs.IT"}
{"created":"2024-05-07 14:31:18","title":"Return to Office and the Tenure Distribution","abstract":"With the official end of the COVID-19 pandemic, debates about the return to office have taken center stage among companies and employees. Despite their ubiquity, the economic implications of return to office policies are not fully understood. Using 260 million resumes matched to company data, we analyze the causal effects of such policies on employees' tenure and seniority levels at three of the largest US tech companies: Microsoft, SpaceX, and Apple. Our estimation procedure is nonparametric and captures the full heterogeneity of tenure and seniority of employees in a distributional synthetic controls framework. We estimate a reduction in counterfactual tenure that increases for employees with longer tenure. Similarly, we document a leftward shift in the seniority distribution towards positions below the senior level. These shifts appear to be driven by employees leaving to larger firms that are direct competitors. Our results suggest that return to office policies can lead to an outflow of senior employees, posing a potential threat to the productivity, innovation, and competitiveness of the wider firm.","sentences":["With the official end of the COVID-19 pandemic, debates about the return to office have taken center stage among companies and employees.","Despite their ubiquity, the economic implications of return to office policies are not fully understood.","Using 260 million resumes matched to company data, we analyze the causal effects of such policies on employees' tenure and seniority levels at three of the largest US tech companies: Microsoft, SpaceX, and Apple.","Our estimation procedure is nonparametric and captures the full heterogeneity of tenure and seniority of employees in a distributional synthetic controls framework.","We estimate a reduction in counterfactual tenure that increases for employees with longer tenure.","Similarly, we document a leftward shift in the seniority distribution towards positions below the senior level.","These shifts appear to be driven by employees leaving to larger firms that are direct competitors.","Our results suggest that return to office policies can lead to an outflow of senior employees, posing a potential threat to the productivity, innovation, and competitiveness of the wider firm."],"url":"http://arxiv.org/abs/2405.04352v1","category":"econ.GN"}
{"created":"2024-05-07 14:29:12","title":"Study of Particle Acceleration using Fine Structures and Oscillations in Microwaves from Electron Cyclotron Maser","abstract":"The accelerated electrons during solar flares produce radio bursts and nonthermal X-ray signatures. The quasi-periodic pulsations (QPPs) and fine structures in spatial-spectral-temporal space in radio bursts depend on the emission mechanism and the local conditions, such as magnetic fields, electron density, and pitch angle distribution. Radio burst observations with high frequency-time resolution imaging provide excellent diagnostics. In converging magnetic field geometries, the radio bursts can be produced via the electron-cyclotron maser (ECM). Recently, using observations made by the Karl G. Jansky Very Large Array (VLA) at 1--2 GHz, \\cite{Yu2023} reported a discovery of long-lasting auroral-like radio bursts persistent over a sunspot and interpreted them as ECM-generated emission. Here, we investigate the detailed second and sub-second temporal variability of this continuous ECM source. We study the association of 5-second period QPPs with a concurrent GOES C1.5-class flare, utilizing VLA's imaging spectroscopy capability with an extremely high temporal resolution (50 ms). We use the density and magnetic field extrapolation model to constrain the ECM emission to the second harmonic o-mode. Using the delay of QPPs from X-ray emission times, combined with X-ray spectroscopy and magnetic extrapolation, we constrain the energies and pitch angles of the ECM-emitting electrons to $\\approx$4-8 keV and $>26^{\\circ}$. Our analysis shows that the loss-cone diffusion continuously fuels the ECM via Coulomb collisions and magnetic turbulence between a 5 Mm--100 Mm length scale. We conclude that the QPP occurs via the Lotka-Volterra system, where the electron from solar flares saturates the continuously operating ECM and causes temporary oscillations.","sentences":["The accelerated electrons during solar flares produce radio bursts and nonthermal X-ray signatures.","The quasi-periodic pulsations (QPPs) and fine structures in spatial-spectral-temporal space in radio bursts depend on the emission mechanism and the local conditions, such as magnetic fields, electron density, and pitch angle distribution.","Radio burst observations with high frequency-time resolution imaging provide excellent diagnostics.","In converging magnetic field geometries, the radio bursts can be produced via the electron-cyclotron maser (ECM).","Recently, using observations made by the Karl G. Jansky Very Large Array (VLA) at 1--2 GHz, \\cite{Yu2023} reported a discovery of long-lasting auroral-like radio bursts persistent over a sunspot and interpreted them as ECM-generated emission.","Here, we investigate the detailed second and sub-second temporal variability of this continuous ECM source.","We study the association of 5-second period QPPs with a concurrent GOES C1.5-class flare, utilizing VLA's imaging spectroscopy capability with an extremely high temporal resolution (50 ms).","We use the density and magnetic field extrapolation model to constrain the ECM emission to the second harmonic o-mode.","Using the delay of QPPs from X-ray emission times, combined with X-ray spectroscopy and magnetic extrapolation, we constrain the energies and pitch angles of the ECM-emitting electrons to $\\approx$4-8 keV and $>26^{\\circ}$.","Our analysis shows that the loss-cone diffusion continuously fuels the ECM via Coulomb collisions and magnetic turbulence between a 5 Mm--100 Mm length scale.","We conclude that the QPP occurs via the Lotka-Volterra system, where the electron from solar flares saturates the continuously operating ECM and causes temporary oscillations."],"url":"http://arxiv.org/abs/2405.04351v1","category":"astro-ph.SR"}
{"created":"2024-05-07 14:23:22","title":"Revisiting character-level adversarial attacks","abstract":"Adversarial attacks in Natural Language Processing apply perturbations in the character or token levels. Token-level attacks, gaining prominence for their use of gradient-based methods, are susceptible to altering sentence semantics, leading to invalid adversarial examples. While character-level attacks easily maintain semantics, they have received less attention as they cannot easily adopt popular gradient-based methods, and are thought to be easy to defend. Challenging these beliefs, we introduce Charmer, an efficient query-based adversarial attack capable of achieving high attack success rate (ASR) while generating highly similar adversarial examples. Our method successfully targets both small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2, Charmer improves the ASR in 4.84% points and the USE similarity in 8% points with respect to the previous art. Our implementation is available in https://github.com/LIONS-EPFL/Charmer.","sentences":["Adversarial attacks in Natural Language Processing apply perturbations in the character or token levels.","Token-level attacks, gaining prominence for their use of gradient-based methods, are susceptible to altering sentence semantics, leading to invalid adversarial examples.","While character-level attacks easily maintain semantics, they have received less attention as they cannot easily adopt popular gradient-based methods, and are thought to be easy to defend.","Challenging these beliefs, we introduce Charmer, an efficient query-based adversarial attack capable of achieving high attack success rate (ASR) while generating highly similar adversarial examples.","Our method successfully targets both small (BERT) and large (Llama 2) models.","Specifically, on BERT with SST-2, Charmer improves the ASR in 4.84% points and the USE similarity in 8% points with respect to the previous art.","Our implementation is available in https://github.com/LIONS-EPFL/Charmer."],"url":"http://arxiv.org/abs/2405.04346v1","category":"cs.LG"}
{"created":"2024-05-07 14:22:32","title":"Novel View Synthesis with Neural Radiance Fields for Industrial Robot Applications","abstract":"Neural Radiance Fields (NeRFs) have become a rapidly growing research field with the potential to revolutionize typical photogrammetric workflows, such as those used for 3D scene reconstruction. As input, NeRFs require multi-view images with corresponding camera poses as well as the interior orientation. In the typical NeRF workflow, the camera poses and the interior orientation are estimated in advance with Structure from Motion (SfM). But the quality of the resulting novel views, which depends on different parameters such as the number and distribution of available images, as well as the accuracy of the related camera poses and interior orientation, is difficult to predict. In addition, SfM is a time-consuming pre-processing step, and its quality strongly depends on the image content. Furthermore, the undefined scaling factor of SfM hinders subsequent steps in which metric information is required. In this paper, we evaluate the potential of NeRFs for industrial robot applications. We propose an alternative to SfM pre-processing: we capture the input images with a calibrated camera that is attached to the end effector of an industrial robot and determine accurate camera poses with metric scale based on the robot kinematics. We then investigate the quality of the novel views by comparing them to ground truth, and by computing an internal quality measure based on ensemble methods. For evaluation purposes, we acquire multiple datasets that pose challenges for reconstruction typical of industrial applications, like reflective objects, poor texture, and fine structures. We show that the robot-based pose determination reaches similar accuracy as SfM in non-demanding cases, while having clear advantages in more challenging scenarios. Finally, we present first results of applying the ensemble method to estimate the quality of the synthetic novel view in the absence of a ground truth.","sentences":["Neural Radiance Fields (NeRFs) have become a rapidly growing research field with the potential to revolutionize typical photogrammetric workflows, such as those used for 3D scene reconstruction.","As input, NeRFs require multi-view images with corresponding camera poses as well as the interior orientation.","In the typical NeRF workflow, the camera poses and the interior orientation are estimated in advance with Structure from Motion (SfM).","But the quality of the resulting novel views, which depends on different parameters such as the number and distribution of available images, as well as the accuracy of the related camera poses and interior orientation, is difficult to predict.","In addition, SfM is a time-consuming pre-processing step, and its quality strongly depends on the image content.","Furthermore, the undefined scaling factor of SfM hinders subsequent steps in which metric information is required.","In this paper, we evaluate the potential of NeRFs for industrial robot applications.","We propose an alternative to SfM pre-processing: we capture the input images with a calibrated camera that is attached to the end effector of an industrial robot and determine accurate camera poses with metric scale based on the robot kinematics.","We then investigate the quality of the novel views by comparing them to ground truth, and by computing an internal quality measure based on ensemble methods.","For evaluation purposes, we acquire multiple datasets that pose challenges for reconstruction typical of industrial applications, like reflective objects, poor texture, and fine structures.","We show that the robot-based pose determination reaches similar accuracy as SfM in non-demanding cases, while having clear advantages in more challenging scenarios.","Finally, we present first results of applying the ensemble method to estimate the quality of the synthetic novel view in the absence of a ground truth."],"url":"http://arxiv.org/abs/2405.04345v1","category":"cs.CV"}
{"created":"2024-05-07 14:19:09","title":"Enhancing Scalability of Metric Differential Privacy via Secret Dataset Partitioning and Benders Decomposition","abstract":"Metric Differential Privacy (mDP) extends the concept of Differential Privacy (DP) to serve as a new paradigm of data perturbation. It is designed to protect secret data represented in general metric space, such as text data encoded as word embeddings or geo-location data on the road network or grid maps. To derive an optimal data perturbation mechanism under mDP, a widely used method is linear programming (LP), which, however, might suffer from a polynomial explosion of decision variables, rendering it impractical in large-scale mDP.   In this paper, our objective is to develop a new computation framework to enhance the scalability of the LP-based mDP. Considering the connections established by the mDP constraints among the secret records, we partition the original secret dataset into various subsets. Building upon the partition, we reformulate the LP problem for mDP and solve it via Benders Decomposition, which is composed of two stages: (1) a master program to manage the perturbation calculation across subsets and (2) a set of subproblems, each managing the perturbation derivation within a subset. Our experimental results on multiple datasets, including geo-location data in the road network/grid maps, text data, and synthetic data, underscore our proposed mechanism's superior scalability and efficiency.","sentences":["Metric Differential Privacy (mDP) extends the concept of Differential Privacy (DP) to serve as a new paradigm of data perturbation.","It is designed to protect secret data represented in general metric space, such as text data encoded as word embeddings or geo-location data on the road network or grid maps.","To derive an optimal data perturbation mechanism under mDP, a widely used method is linear programming (LP), which, however, might suffer from a polynomial explosion of decision variables, rendering it impractical in large-scale mDP.   ","In this paper, our objective is to develop a new computation framework to enhance the scalability of the LP-based mDP.","Considering the connections established by the mDP constraints among the secret records, we partition the original secret dataset into various subsets.","Building upon the partition, we reformulate the LP problem for mDP and solve it via Benders Decomposition, which is composed of two stages: (1) a master program to manage the perturbation calculation across subsets and (2) a set of subproblems, each managing the perturbation derivation within a subset.","Our experimental results on multiple datasets, including geo-location data in the road network/grid maps, text data, and synthetic data, underscore our proposed mechanism's superior scalability and efficiency."],"url":"http://arxiv.org/abs/2405.04344v1","category":"cs.AI"}
{"created":"2024-05-07 14:16:33","title":"Essential freeness, allostery and $\\mathcal{Z}$-stability of crossed products","abstract":"We explore classifiability of crossed products of actions of countable amenable groups on compact, metrizable spaces. It is completely understood when such crossed products are simple, separable, unital, nuclear and satisfy the UCT: these properties are equivalent to the combination of minimality and topological freeness, and the challenge in this context is establishing $\\mathcal{Z}$-stability. While most of the existing results in this direction assume freeness of the action, there exist numerous natural examples of minimal, topologically free (but not free) actions whose crossed products are classifiable.   In this work, we take the first steps towards a systematic study of $\\mathcal{Z}$-stability for crossed products beyond the free case, extending the available machinery around the small boundary property and almost finiteness to a more general setting. Among others, for actions of groups of polynomial growth with the small boundary property, we show that minimality and topological freeness are not just necessary, but also \\emph{sufficient} conditions for classifiability of the crossed product.   Our most general results apply to actions that are essentially free, a property weaker than freeness but stronger than topological freeness in the minimal setting. Very recently, M. Joseph produced the first examples of minimal actions of amenable groups which are topologically free and not essentially free. While the current machinery does not give any information for his examples, we develop ad-hoc methods to show that his actions have classifiable crossed products.","sentences":["We explore classifiability of crossed products of actions of countable amenable groups on compact, metrizable spaces.","It is completely understood when such crossed products are simple, separable, unital, nuclear and satisfy the UCT: these properties are equivalent to the combination of minimality and topological freeness, and the challenge in this context is establishing $\\mathcal{Z}$-stability.","While most of the existing results in this direction assume freeness of the action, there exist numerous natural examples of minimal, topologically free (but not free) actions whose crossed products are classifiable.   ","In this work, we take the first steps towards a systematic study of $\\mathcal{Z}$-stability for crossed products beyond the free case, extending the available machinery around the small boundary property and almost finiteness to a more general setting.","Among others, for actions of groups of polynomial growth with the small boundary property, we show that minimality and topological freeness are not just necessary, but also \\emph{sufficient} conditions for classifiability of the crossed product.   ","Our most general results apply to actions that are essentially free, a property weaker than freeness but stronger than topological freeness in the minimal setting.","Very recently, M. Joseph produced the first examples of minimal actions of amenable groups which are topologically free and not essentially free.","While the current machinery does not give any information for his examples, we develop ad-hoc methods to show that his actions have classifiable crossed products."],"url":"http://arxiv.org/abs/2405.04343v1","category":"math.OA"}
{"created":"2024-05-07 14:14:50","title":"The Curse of Diversity in Ensemble-Based Exploration","abstract":"We uncover a surprising phenomenon in deep reinforcement learning: training a diverse ensemble of data-sharing agents -- a well-established exploration strategy -- can significantly impair the performance of the individual ensemble members when compared to standard single-agent training. Through careful analysis, we attribute the degradation in performance to the low proportion of self-generated data in the shared training data for each ensemble member, as well as the inefficiency of the individual ensemble members to learn from such highly off-policy data. We thus name this phenomenon the curse of diversity. We find that several intuitive solutions -- such as a larger replay buffer or a smaller ensemble size -- either fail to consistently mitigate the performance loss or undermine the advantages of ensembling. Finally, we demonstrate the potential of representation learning to counteract the curse of diversity with a novel method named Cross-Ensemble Representation Learning (CERL) in both discrete and continuous control domains. Our work offers valuable insights into an unexpected pitfall in ensemble-based exploration and raises important caveats for future applications of similar approaches.","sentences":["We uncover a surprising phenomenon in deep reinforcement learning: training a diverse ensemble of data-sharing agents -- a well-established exploration strategy -- can significantly impair the performance of the individual ensemble members when compared to standard single-agent training.","Through careful analysis, we attribute the degradation in performance to the low proportion of self-generated data in the shared training data for each ensemble member, as well as the inefficiency of the individual ensemble members to learn from such highly off-policy data.","We thus name this phenomenon the curse of diversity.","We find that several intuitive solutions -- such as a larger replay buffer or a smaller ensemble size -- either fail to consistently mitigate the performance loss or undermine the advantages of ensembling.","Finally, we demonstrate the potential of representation learning to counteract the curse of diversity with a novel method named Cross-Ensemble Representation Learning (CERL) in both discrete and continuous control domains.","Our work offers valuable insights into an unexpected pitfall in ensemble-based exploration and raises important caveats for future applications of similar approaches."],"url":"http://arxiv.org/abs/2405.04342v1","category":"cs.LG"}
{"created":"2024-05-07 14:08:57","title":"Temporal and Heterogeneous Graph Neural Network for Remaining Useful Life Prediction","abstract":"Predicting Remaining Useful Life (RUL) plays a crucial role in the prognostics and health management of industrial systems that involve a variety of interrelated sensors. Given a constant stream of time series sensory data from such systems, deep learning models have risen to prominence at identifying complex, nonlinear temporal dependencies in these data. In addition to the temporal dependencies of individual sensors, spatial dependencies emerge as important correlations among these sensors, which can be naturally modelled by a temporal graph that describes time-varying spatial relationships. However, the majority of existing studies have relied on capturing discrete snapshots of this temporal graph, a coarse-grained approach that leads to loss of temporal information. Moreover, given the variety of heterogeneous sensors, it becomes vital that such inherent heterogeneity is leveraged for RUL prediction in temporal sensor graphs. To capture the nuances of the temporal and spatial relationships and heterogeneous characteristics in an interconnected graph of sensors, we introduce a novel model named Temporal and Heterogeneous Graph Neural Networks (THGNN). Specifically, THGNN aggregates historical data from neighboring nodes to accurately capture the temporal dynamics and spatial correlations within the stream of sensor data in a fine-grained manner. Moreover, the model leverages Feature-wise Linear Modulation (FiLM) to address the diversity of sensor types, significantly improving the model's capacity to learn the heterogeneity in the data sources. Finally, we have validated the effectiveness of our approach through comprehensive experiments. Our empirical findings demonstrate significant advancements on the N-CMAPSS dataset, achieving improvements of up to 19.2% and 31.6% in terms of two different evaluation metrics over state-of-the-art methods.","sentences":["Predicting Remaining Useful Life (RUL) plays a crucial role in the prognostics and health management of industrial systems that involve a variety of interrelated sensors.","Given a constant stream of time series sensory data from such systems, deep learning models have risen to prominence at identifying complex, nonlinear temporal dependencies in these data.","In addition to the temporal dependencies of individual sensors, spatial dependencies emerge as important correlations among these sensors, which can be naturally modelled by a temporal graph that describes time-varying spatial relationships.","However, the majority of existing studies have relied on capturing discrete snapshots of this temporal graph, a coarse-grained approach that leads to loss of temporal information.","Moreover, given the variety of heterogeneous sensors, it becomes vital that such inherent heterogeneity is leveraged for RUL prediction in temporal sensor graphs.","To capture the nuances of the temporal and spatial relationships and heterogeneous characteristics in an interconnected graph of sensors, we introduce a novel model named Temporal and Heterogeneous Graph Neural Networks (THGNN).","Specifically, THGNN aggregates historical data from neighboring nodes to accurately capture the temporal dynamics and spatial correlations within the stream of sensor data in a fine-grained manner.","Moreover, the model leverages Feature-wise Linear Modulation (FiLM) to address the diversity of sensor types, significantly improving the model's capacity to learn the heterogeneity in the data sources.","Finally, we have validated the effectiveness of our approach through comprehensive experiments.","Our empirical findings demonstrate significant advancements on the N-CMAPSS dataset, achieving improvements of up to 19.2% and 31.6% in terms of two different evaluation metrics over state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.04336v1","category":"cs.AI"}
{"created":"2024-05-07 14:02:40","title":"Statistical estimation theory detection limits for label-free imaging","abstract":"The emergence of label-free microscopy techniques has significantly improved our ability to precisely characterize biochemical targets, enabling non-invasive visualization of cellular organelles and tissue organization. Each label-free method has specific benefits, drawbacks, and varied varied sensitivity under measurement conditions across different types of specimens. To link all these disparate label-free optical interactions together and to compare detection sensitivity of these modalities, we investigate their sensitivity within the framework of statistical estimation theory. This paper introduces a comprehensive unified framework for evaluating the bounds for signal detection with label-free microscopy methods, including second harmonic generation (SHG), third harmonic generation (THG), coherent anti-Stokes Raman scattering (CARS), coherent Stokes Raman scattering (CSRS), stimulated Raman loss (SRL), stimulated Raman gain (SRG), stimulated emission (SE), impulsive stimulated Raman scattering (ISRS), transient absorption (TA), and photothermal effect (PTE). A general model for signal generation induced by optical scattering is developed. Based on this model, the information obtained is quantitatively analyzed using Fisher information, and the fundamental constraints on estimation precision are evaluated through the Cram\\'er-Rao Lower Bound (CRLB).","sentences":["The emergence of label-free microscopy techniques has significantly improved our ability to precisely characterize biochemical targets, enabling non-invasive visualization of cellular organelles and tissue organization.","Each label-free method has specific benefits, drawbacks, and varied varied sensitivity under measurement conditions across different types of specimens.","To link all these disparate label-free optical interactions together and to compare detection sensitivity of these modalities, we investigate their sensitivity within the framework of statistical estimation theory.","This paper introduces a comprehensive unified framework for evaluating the bounds for signal detection with label-free microscopy methods, including second harmonic generation (SHG), third harmonic generation (THG), coherent anti-Stokes Raman scattering (CARS), coherent Stokes Raman scattering (CSRS), stimulated Raman loss (SRL), stimulated Raman gain (SRG), stimulated emission (SE), impulsive stimulated Raman scattering (ISRS), transient absorption (TA), and photothermal effect (PTE).","A general model for signal generation induced by optical scattering is developed.","Based on this model, the information obtained is quantitatively analyzed using Fisher information, and the fundamental constraints on estimation precision are evaluated through the Cram\\'er-Rao Lower Bound (CRLB)."],"url":"http://arxiv.org/abs/2405.04334v1","category":"physics.optics"}
{"created":"2024-05-07 14:01:33","title":"A Fourth Wave of Open Data? Exploring the Spectrum of Scenarios for Open Data and Generative AI","abstract":"Since late 2022, generative AI has taken the world by storm, with widespread use of tools including ChatGPT, Gemini, and Claude. Generative AI and large language model (LLM) applications are transforming how individuals find and access data and knowledge. However, the intricate relationship between open data and generative AI, and the vast potential it holds for driving innovation in this field remain underexplored areas. This white paper seeks to unpack the relationship between open data and generative AI and explore possible components of a new Fourth Wave of Open Data: Is open data becoming AI ready? Is open data moving towards a data commons approach? Is generative AI making open data more conversational? Will generative AI improve open data quality and provenance? Towards this end, we provide a new Spectrum of Scenarios framework. This framework outlines a range of scenarios in which open data and generative AI could intersect and what is required from a data quality and provenance perspective to make open data ready for those specific scenarios. These scenarios include: pertaining, adaptation, inference and insight generation, data augmentation, and open-ended exploration. Through this process, we found that in order for data holders to embrace generative AI to improve open data access and develop greater insights from open data, they first must make progress around five key areas: enhance transparency and documentation, uphold quality and integrity, promote interoperability and standards, improve accessibility and useability, and address ethical considerations.","sentences":["Since late 2022, generative AI has taken the world by storm, with widespread use of tools including ChatGPT, Gemini, and Claude.","Generative AI and large language model (LLM) applications are transforming how individuals find and access data and knowledge.","However, the intricate relationship between open data and generative AI, and the vast potential it holds for driving innovation in this field remain underexplored areas.","This white paper seeks to unpack the relationship between open data and generative AI and explore possible components of a new Fourth Wave of Open Data: Is open data becoming AI ready?","Is open data moving towards a data commons approach?","Is generative AI making open data more conversational?","Will generative AI improve open data quality and provenance?","Towards this end, we provide a new Spectrum of Scenarios framework.","This framework outlines a range of scenarios in which open data and generative AI could intersect and what is required from a data quality and provenance perspective to make open data ready for those specific scenarios.","These scenarios include: pertaining, adaptation, inference and insight generation, data augmentation, and open-ended exploration.","Through this process, we found that in order for data holders to embrace generative AI to improve open data access and develop greater insights from open data, they first must make progress around five key areas: enhance transparency and documentation, uphold quality and integrity, promote interoperability and standards, improve accessibility and useability, and address ethical considerations."],"url":"http://arxiv.org/abs/2405.04333v1","category":"cs.AI"}
{"created":"2024-05-07 13:55:50","title":"Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation","abstract":"In the task of talking face generation, the objective is to generate a face video with lips synchronized to the corresponding audio while preserving visual details and identity information. Current methods face the challenge of learning accurate lip synchronization while avoiding detrimental effects on visual quality, as well as robustly evaluating such synchronization. To tackle these problems, we propose utilizing an audio-visual speech representation expert (AV-HuBERT) for calculating lip synchronization loss during training. Moreover, leveraging AV-HuBERT's features, we introduce three novel lip synchronization evaluation metrics, aiming to provide a comprehensive assessment of lip synchronization performance. Experimental results, along with a detailed ablation study, demonstrate the effectiveness of our approach and the utility of the proposed evaluation metrics.","sentences":["In the task of talking face generation, the objective is to generate a face video with lips synchronized to the corresponding audio while preserving visual details and identity information.","Current methods face the challenge of learning accurate lip synchronization while avoiding detrimental effects on visual quality, as well as robustly evaluating such synchronization.","To tackle these problems, we propose utilizing an audio-visual speech representation expert (AV-HuBERT) for calculating lip synchronization loss during training.","Moreover, leveraging AV-HuBERT's features, we introduce three novel lip synchronization evaluation metrics, aiming to provide a comprehensive assessment of lip synchronization performance.","Experimental results, along with a detailed ablation study, demonstrate the effectiveness of our approach and the utility of the proposed evaluation metrics."],"url":"http://arxiv.org/abs/2405.04327v1","category":"cs.CV"}
{"created":"2024-05-07 13:55:11","title":"Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation","abstract":"Recent developments in large language models (LLMs), while offering a powerful foundation for developing natural language agents, raise safety concerns about them and the autonomous agents built upon them. Deception is one potential capability of AI agents of particular concern, which we refer to as an act or statement that misleads, hides the truth, or promotes a belief that is not true in its entirety or in part. We move away from the conventional understanding of deception through straight-out lying, making objective selfish decisions, or giving false information, as seen in previous AI safety research. We target a specific category of deception achieved through obfuscation and equivocation. We broadly explain the two types of deception by analogizing them with the rabbit-out-of-hat magic trick, where (i) the rabbit either comes out of a hidden trap door or (ii) (our focus) the audience is completely distracted to see the magician bring out the rabbit right in front of them using sleight of hand or misdirection. Our novel testbed framework displays intrinsic deception capabilities of LLM agents in a goal-driven environment when directed to be deceptive in their natural language generations in a two-agent adversarial dialogue system built upon the legislative task of \"lobbying\" for a bill. Along the lines of a goal-driven environment, we show developing deceptive capacity through a reinforcement learning setup, building it around the theories of language philosophy and cognitive psychology. We find that the lobbyist agent increases its deceptive capabilities by ~ 40% (relative) through subsequent reinforcement trials of adversarial interactions, and our deception detection mechanism shows a detection capability of up to 92%. Our results highlight potential issues in agent-human interaction, with agents potentially manipulating humans towards its programmed end-goal.","sentences":["Recent developments in large language models (LLMs), while offering a powerful foundation for developing natural language agents, raise safety concerns about them and the autonomous agents built upon them.","Deception is one potential capability of AI agents of particular concern, which we refer to as an act or statement that misleads, hides the truth, or promotes a belief that is not true in its entirety or in part.","We move away from the conventional understanding of deception through straight-out lying, making objective selfish decisions, or giving false information, as seen in previous AI safety research.","We target a specific category of deception achieved through obfuscation and equivocation.","We broadly explain the two types of deception by analogizing them with the rabbit-out-of-hat magic trick, where (i) the rabbit either comes out of a hidden trap door or (ii) (our focus) the audience is completely distracted to see the magician bring out the rabbit right in front of them using sleight of hand or misdirection.","Our novel testbed framework displays intrinsic deception capabilities of LLM agents in a goal-driven environment when directed to be deceptive in their natural language generations in a two-agent adversarial dialogue system built upon the legislative task of \"lobbying\" for a bill.","Along the lines of a goal-driven environment, we show developing deceptive capacity through a reinforcement learning setup, building it around the theories of language philosophy and cognitive psychology.","We find that the lobbyist agent increases its deceptive capabilities by ~ 40% (relative) through subsequent reinforcement trials of adversarial interactions, and our deception detection mechanism shows a detection capability of up to 92%.","Our results highlight potential issues in agent-human interaction, with agents potentially manipulating humans towards its programmed end-goal."],"url":"http://arxiv.org/abs/2405.04325v1","category":"cs.CL"}
{"created":"2024-05-07 13:50:40","title":"Granite Code Models: A Family of Open Foundation Models for Code Intelligence","abstract":"Large Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs. The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile all around code model. We release all our Granite Code models under an Apache 2.0 license for both research and commercial use.","sentences":["Large Language Models (LLMs) trained on code are revolutionizing the software development process.","Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously.","Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more.","In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages.","The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases.","Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs.","The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile all around code model.","We release all our Granite Code models under an Apache 2.0 license for both research and commercial use."],"url":"http://arxiv.org/abs/2405.04324v1","category":"cs.AI"}
{"created":"2024-05-07 13:49:59","title":"Beyond human subjectivity and error: a novel AI grading system","abstract":"The grading of open-ended questions is a high-effort, high-impact task in education. Automating this task promises a significant reduction in workload for education professionals, as well as more consistent grading outcomes for students, by circumventing human subjectivity and error. While recent breakthroughs in AI technology might facilitate such automation, this has not been demonstrated at scale. It this paper, we introduce a novel automatic short answer grading (ASAG) system. The system is based on a fine-tuned open-source transformer model which we trained on large set of exam data from university courses across a large range of disciplines. We evaluated the trained model's performance against held-out test data in a first experiment and found high accuracy levels across a broad spectrum of unseen questions, even in unseen courses. We further compared the performance of our model with that of certified human domain experts in a second experiment: we first assembled another test dataset from real historical exams - the historic grades contained in that data were awarded to students in a regulated, legally binding examination process; we therefore considered them as ground truth for our experiment. We then asked certified human domain experts and our model to grade the historic student answers again without disclosing the historic grades. Finally, we compared the hence obtained grades with the historic grades (our ground truth). We found that for the courses examined, the model deviated less from the official historic grades than the human re-graders - the model's median absolute error was 44 % smaller than the human re-graders', implying that the model is more consistent than humans in grading. These results suggest that leveraging AI enhanced grading can reduce human subjectivity, improve consistency and thus ultimately increase fairness.","sentences":["The grading of open-ended questions is a high-effort, high-impact task in education.","Automating this task promises a significant reduction in workload for education professionals, as well as more consistent grading outcomes for students, by circumventing human subjectivity and error.","While recent breakthroughs in AI technology might facilitate such automation, this has not been demonstrated at scale.","It this paper, we introduce a novel automatic short answer grading (ASAG) system.","The system is based on a fine-tuned open-source transformer model which we trained on large set of exam data from university courses across a large range of disciplines.","We evaluated the trained model's performance against held-out test data in a first experiment and found high accuracy levels across a broad spectrum of unseen questions, even in unseen courses.","We further compared the performance of our model with that of certified human domain experts in a second experiment: we first assembled another test dataset from real historical exams - the historic grades contained in that data were awarded to students in a regulated, legally binding examination process; we therefore considered them as ground truth for our experiment.","We then asked certified human domain experts and our model to grade the historic student answers again without disclosing the historic grades.","Finally, we compared the hence obtained grades with the historic grades (our ground truth).","We found that for the courses examined, the model deviated less from the official historic grades than the human re-graders - the model's median absolute error was 44 % smaller than the human re-graders', implying that the model is more consistent than humans in grading.","These results suggest that leveraging AI enhanced grading can reduce human subjectivity, improve consistency and thus ultimately increase fairness."],"url":"http://arxiv.org/abs/2405.04323v1","category":"cs.AI"}
{"created":"2024-05-07 13:37:40","title":"The average number of Goldbach representations over multiples of $q$","abstract":"We discuss the evaluation of the average number of Goldbach representations for integers which are multiples of $q$ introduced by Granville. We improve an estimate given by Granville under the generalized Riemann hypothesis.","sentences":["We discuss the evaluation of the average number of Goldbach representations for integers which are multiples of $q$ introduced by Granville.","We improve an estimate given by Granville under the generalized Riemann hypothesis."],"url":"http://arxiv.org/abs/2405.04315v1","category":"math.NT"}
{"created":"2024-05-07 13:36:33","title":"Deck of Cards method for Hierarchical, Robust and Stochastic Ordinal Regression","abstract":"In this paper, we consider the recently introduced application of the Deck of Cards Method (DCM) to ordinal regression proposing an extension to Robust Ordinal Regression and Stochastic Multiattribute Acceptability Analysis. In Multiple Criteria Decision Aiding context, the proposed methodology permits to assign a value to each alternative evaluated on a set of criteria hierarchically structured. The Decision Maker can provide precise or imprecise information at different levels of the hierarchy of criteria using the classical DCM framework. This information is therefore used to infer a value function compatible with it. The compatible value function can be a simple weighted sum, a piecewise linear value function, a general monotonic value function, or a Choquet integral. To provide robust recommendations to the Decision Maker, we consider the Robust Ordinal Regression and the Stochastic Multicriteria Acceptability Analysis because, even if in different ways, both of them take into account the whole set of models compatible with the preference information provided by the Decision Maker. The applicability of the proposed methodology is shown by a didactic example in which Italian regions are evaluated on criteria representing Circular Economy, Innovation Driven Development and Smart Specialization Strategies.","sentences":["In this paper, we consider the recently introduced application of the Deck of Cards Method (DCM) to ordinal regression proposing an extension to Robust Ordinal Regression and Stochastic Multiattribute Acceptability Analysis.","In Multiple Criteria Decision Aiding context, the proposed methodology permits to assign a value to each alternative evaluated on a set of criteria hierarchically structured.","The Decision Maker can provide precise or imprecise information at different levels of the hierarchy of criteria using the classical DCM framework.","This information is therefore used to infer a value function compatible with it.","The compatible value function can be a simple weighted sum, a piecewise linear value function, a general monotonic value function, or a Choquet integral.","To provide robust recommendations to the Decision Maker, we consider the Robust Ordinal Regression and the Stochastic Multicriteria Acceptability Analysis because, even if in different ways, both of them take into account the whole set of models compatible with the preference information provided by the Decision Maker.","The applicability of the proposed methodology is shown by a didactic example in which Italian regions are evaluated on criteria representing Circular Economy, Innovation Driven Development and Smart Specialization Strategies."],"url":"http://arxiv.org/abs/2405.04313v1","category":"math.OC"}
{"created":"2024-05-07 13:35:58","title":"Inf-DiT: Upsampling Any-Resolution Image with Memory-Efficient Diffusion Transformer","abstract":"Diffusion models have shown remarkable performance in image generation in recent years. However, due to a quadratic increase in memory during generating ultra-high-resolution images (e.g. 4096*4096), the resolution of generated images is often limited to 1024*1024. In this work. we propose a unidirectional block attention mechanism that can adaptively adjust the memory overhead during the inference process and handle global dependencies. Building on this module, we adopt the DiT structure for upsampling and develop an infinite super-resolution model capable of upsampling images of various shapes and resolutions. Comprehensive experiments show that our model achieves SOTA performance in generating ultra-high-resolution images in both machine and human evaluation. Compared to commonly used UNet structures, our model can save more than 5x memory when generating 4096*4096 images. The project URL is https://github.com/THUDM/Inf-DiT.","sentences":["Diffusion models have shown remarkable performance in image generation in recent years.","However, due to a quadratic increase in memory during generating ultra-high-resolution images (e.g. 4096*4096), the resolution of generated images is often limited to 1024*1024.","In this work.","we propose a unidirectional block attention mechanism that can adaptively adjust the memory overhead during the inference process and handle global dependencies.","Building on this module, we adopt the DiT structure for upsampling and develop an infinite super-resolution model capable of upsampling images of various shapes and resolutions.","Comprehensive experiments show that our model achieves SOTA performance in generating ultra-high-resolution images in both machine and human evaluation.","Compared to commonly used UNet structures, our model can save more than 5x memory when generating 4096*4096 images.","The project URL is https://github.com/THUDM/Inf-DiT."],"url":"http://arxiv.org/abs/2405.04312v1","category":"cs.CV"}
{"created":"2024-05-07 13:35:51","title":"Cross-IQA: Unsupervised Learning for Image Quality Assessment","abstract":"Automatic perception of image quality is a challenging problem that impacts billions of Internet and social media users daily. To advance research in this field, we propose a no-reference image quality assessment (NR-IQA) method termed Cross-IQA based on vision transformer(ViT) model. The proposed Cross-IQA method can learn image quality features from unlabeled image data. We construct the pretext task of synthesized image reconstruction to unsupervised extract the image quality information based ViT block. The pretrained encoder of Cross-IQA is used to fine-tune a linear regression model for score prediction. Experimental results show that Cross-IQA can achieve state-of-the-art performance in assessing the low-frequency degradation information (e.g., color change, blurring, etc.) of images compared with the classical full-reference IQA and NR-IQA under the same datasets.","sentences":["Automatic perception of image quality is a challenging problem that impacts billions of Internet and social media users daily.","To advance research in this field, we propose a no-reference image quality assessment (NR-IQA) method termed Cross-IQA based on vision transformer(ViT) model.","The proposed Cross-IQA method can learn image quality features from unlabeled image data.","We construct the pretext task of synthesized image reconstruction to unsupervised extract the image quality information based ViT block.","The pretrained encoder of Cross-IQA is used to fine-tune a linear regression model for score prediction.","Experimental results show that Cross-IQA can achieve state-of-the-art performance in assessing the low-frequency degradation information (e.g., color change, blurring, etc.) of images compared with the classical full-reference IQA and NR-IQA under the same datasets."],"url":"http://arxiv.org/abs/2405.04311v1","category":"cs.CV"}
{"created":"2024-05-07 13:29:41","title":"Improving Offline Reinforcement Learning with Inaccurate Simulators","abstract":"Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment. However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process. In many robotic applications, an inaccurate simulator is often available. However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment. To address these issues, we propose a novel approach to combine the offline dataset and the inaccurate simulation data in a better manner. Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset. Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator. Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods.","sentences":["Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment.","However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process.","In many robotic applications, an inaccurate simulator is often available.","However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment.","To address these issues, we propose a novel approach to combine the offline dataset and the inaccurate simulation data in a better manner.","Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset.","Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator.","Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.04307v1","category":"cs.RO"}
{"created":"2024-05-07 13:28:27","title":"Tropicalization of linear series and tilings by polymatroids","abstract":"We show that tropicalization of linear series on curves gives rise to two-parameter families of tilings by polymatroids, with one parameter arising from the theory of divisors on tropical curves and the other from the reduction of linear series of rational functions in non-Archimedean geometry. In order to do this, we introduce a general framework that produces tilings of vector spaces and their subsets by polymatroids. We furthermore show that these tilings are regular and relate them to work by Kapranov and Lafforgue on Chow quotients of Grassmannians.","sentences":["We show that tropicalization of linear series on curves gives rise to two-parameter families of tilings by polymatroids, with one parameter arising from the theory of divisors on tropical curves and the other from the reduction of linear series of rational functions in non-Archimedean geometry.","In order to do this, we introduce a general framework that produces tilings of vector spaces and their subsets by polymatroids.","We furthermore show that these tilings are regular and relate them to work by Kapranov and Lafforgue on Chow quotients of Grassmannians."],"url":"http://arxiv.org/abs/2405.04306v1","category":"math.AG"}
{"created":"2024-05-07 13:27:58","title":"A New Dataset and Comparative Study for Aphid Cluster Detection and Segmentation in Sorghum Fields","abstract":"Aphid infestations are one of the primary causes of extensive damage to wheat and sorghum fields and are one of the most common vectors for plant viruses, resulting in significant agricultural yield losses. To address this problem, farmers often employ the inefficient use of harmful chemical pesticides that have negative health and environmental impacts. As a result, a large amount of pesticide is wasted on areas without significant pest infestation. This brings to attention the urgent need for an intelligent autonomous system that can locate and spray sufficiently large infestations selectively within the complex crop canopies. We have developed a large multi-scale dataset for aphid cluster detection and segmentation, collected from actual sorghum fields and meticulously annotated to include clusters of aphids. Our dataset comprises a total of 54,742 image patches, showcasing a variety of viewpoints, diverse lighting conditions, and multiple scales, highlighting its effectiveness for real-world applications. In this study, we trained and evaluated four real-time semantic segmentation models and three object detection models specifically for aphid cluster segmentation and detection. Considering the balance between accuracy and efficiency, Fast-SCNN delivered the most effective segmentation results, achieving 80.46% mean precision, 81.21% mean recall, and 91.66 frames per second (FPS). For object detection, RT-DETR exhibited the best overall performance with a 61.63% mean average precision (mAP), 92.6% mean recall, and 72.55 on an NVIDIA V100 GPU. Our experiments further indicate that aphid cluster segmentation is more suitable for assessing aphid infestations than using detection models.","sentences":["Aphid infestations are one of the primary causes of extensive damage to wheat and sorghum fields and are one of the most common vectors for plant viruses, resulting in significant agricultural yield losses.","To address this problem, farmers often employ the inefficient use of harmful chemical pesticides that have negative health and environmental impacts.","As a result, a large amount of pesticide is wasted on areas without significant pest infestation.","This brings to attention the urgent need for an intelligent autonomous system that can locate and spray sufficiently large infestations selectively within the complex crop canopies.","We have developed a large multi-scale dataset for aphid cluster detection and segmentation, collected from actual sorghum fields and meticulously annotated to include clusters of aphids.","Our dataset comprises a total of 54,742 image patches, showcasing a variety of viewpoints, diverse lighting conditions, and multiple scales, highlighting its effectiveness for real-world applications.","In this study, we trained and evaluated four real-time semantic segmentation models and three object detection models specifically for aphid cluster segmentation and detection.","Considering the balance between accuracy and efficiency, Fast-SCNN delivered the most effective segmentation results, achieving 80.46% mean precision, 81.21% mean recall, and 91.66 frames per second (FPS).","For object detection, RT-DETR exhibited the best overall performance with a 61.63% mean average precision (mAP), 92.6% mean recall, and 72.55 on an NVIDIA V100 GPU.","Our experiments further indicate that aphid cluster segmentation is more suitable for assessing aphid infestations than using detection models."],"url":"http://arxiv.org/abs/2405.04305v1","category":"cs.CV"}
{"created":"2024-05-07 13:27:52","title":"Accelerating Speculative Decoding using Dynamic Speculation Length","abstract":"Speculative decoding is a promising method for reducing the inference latency of large language models. The effectiveness of the method depends on the speculation length (SL) - the number of tokens generated by the draft model at each iteration. The vast majority of speculative decoding approaches use the same SL for all iterations. In this work, we show that this practice is suboptimal. We introduce DISCO, a DynamIc SpeCulation length Optimization method that uses a classifier to dynamically adjust the SL at each iteration, while provably preserving the decoding quality. Experiments with four benchmarks demonstrate average speedup gains of 10.3% relative to our best baselines.","sentences":["Speculative decoding is a promising method for reducing the inference latency of large language models.","The effectiveness of the method depends on the speculation length (SL) - the number of tokens generated by the draft model at each iteration.","The vast majority of speculative decoding approaches use the same SL for all iterations.","In this work, we show that this practice is suboptimal.","We introduce DISCO, a DynamIc SpeCulation length Optimization method that uses a classifier to dynamically adjust the SL at each iteration, while provably preserving the decoding quality.","Experiments with four benchmarks demonstrate average speedup gains of 10.3% relative to our best baselines."],"url":"http://arxiv.org/abs/2405.04304v1","category":"cs.CL"}
{"created":"2024-05-07 13:18:22","title":"Behaviour Planning: A Toolkit for Diverse Planning","abstract":"Diverse planning is the problem of generating plans with distinct characteristics. This is valuable for many real-world scenarios, including applications related to plan recognition and business process automation. In this work, we introduce \\emph{Behaviour Planning}, a diverse planning toolkit that can characterise and generate diverse plans based on modular diversity models. We present a qualitative framework for describing diversity models, a planning approach for generating plans aligned with any given diversity model, and provide a practical implementation of an SMT-based behaviour planner. We showcase how the qualitative approach offered by Behaviour Planning allows it to overcome various challenges faced by previous approaches. Finally, the experimental evaluation shows the effectiveness of Behaviour Planning in generating diverse plans compared to state-of-the-art approaches.","sentences":["Diverse planning is the problem of generating plans with distinct characteristics.","This is valuable for many real-world scenarios, including applications related to plan recognition and business process automation.","In this work, we introduce \\emph{Behaviour Planning}, a diverse planning toolkit that can characterise and generate diverse plans based on modular diversity models.","We present a qualitative framework for describing diversity models, a planning approach for generating plans aligned with any given diversity model, and provide a practical implementation of an SMT-based behaviour planner.","We showcase how the qualitative approach offered by Behaviour Planning allows it to overcome various challenges faced by previous approaches.","Finally, the experimental evaluation shows the effectiveness of Behaviour Planning in generating diverse plans compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2405.04300v1","category":"cs.AI"}
{"created":"2024-05-07 13:12:49","title":"Certifying Phase Abstraction","abstract":"Certification helps to increase trust in formal verification of safety-critical systems which require assurance on their correctness. In hardware model checking, a widely used formal verification technique, phase abstraction is considered one of the most commonly used preprocessing techniques. We present an approach to certify an extended form of phase abstraction using a generic certificate format. As in earlier works our approach involves constructing a witness circuit with an inductive invariant property that certifies the correctness of the entire model checking process, which is then validated by an independent certificate checker. We have implemented and evaluated the proposed approach including certification for various preprocessing configurations on hardware model checking competition benchmarks. As an improvement on previous work in this area, the proposed method is able to efficiently complete certification with an overhead of a fraction of model checking time.","sentences":["Certification helps to increase trust in formal verification of safety-critical systems which require assurance on their correctness.","In hardware model checking, a widely used formal verification technique, phase abstraction is considered one of the most commonly used preprocessing techniques.","We present an approach to certify an extended form of phase abstraction using a generic certificate format.","As in earlier works our approach involves constructing a witness circuit with an inductive invariant property that certifies the correctness of the entire model checking process, which is then validated by an independent certificate checker.","We have implemented and evaluated the proposed approach including certification for various preprocessing configurations on hardware model checking competition benchmarks.","As an improvement on previous work in this area, the proposed method is able to efficiently complete certification with an overhead of a fraction of model checking time."],"url":"http://arxiv.org/abs/2405.04297v1","category":"cs.SC"}
{"created":"2024-05-07 13:11:37","title":"Open Implementation and Study of BEST-RQ for Speech Processing","abstract":"Self-Supervised Learning (SSL) has proven to be useful in various speech tasks. However, these methods are generally very demanding in terms of data, memory, and computational resources. BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ), is an SSL method that has shown great performance on Automatic Speech Recognition (ASR) while being simpler than other SSL methods, such as wav2vec 2.0. Despite BEST-RQ's great performance, details are lacking in the original paper, such as the amount of GPU/TPU hours used in pre-training, and there is no official easy-to-use open-source implementation. Furthermore, BEST-RQ has not been evaluated on other downstream tasks aside from ASR and speech translation. In this work, we describe a re-implementation of a Random-projection quantizer and perform a preliminary study with a comparison to wav2vec 2.0 on four downstream tasks. We discuss the details and differences of our implementation. We show that a random projection quantizer can achieve similar downstream performance as wav2vec 2.0 while decreasing training time by over a factor of two.","sentences":["Self-Supervised Learning (SSL) has proven to be useful in various speech tasks.","However, these methods are generally very demanding in terms of data, memory, and computational resources.","BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ), is an SSL method that has shown great performance on Automatic Speech Recognition (ASR) while being simpler than other SSL methods, such as wav2vec 2.0.","Despite BEST-RQ's great performance, details are lacking in the original paper, such as the amount of GPU/TPU hours used in pre-training, and there is no official easy-to-use open-source implementation.","Furthermore, BEST-RQ has not been evaluated on other downstream tasks aside from ASR and speech translation.","In this work, we describe a re-implementation of a Random-projection quantizer and perform a preliminary study with a comparison to wav2vec 2.0 on four downstream tasks.","We discuss the details and differences of our implementation.","We show that a random projection quantizer can achieve similar downstream performance as wav2vec 2.0 while decreasing training time by over a factor of two."],"url":"http://arxiv.org/abs/2405.04296v1","category":"cs.CL"}
{"created":"2024-05-07 13:11:08","title":"Semi-Supervised Disease Classification based on Limited Medical Image Data","abstract":"In recent years, significant progress has been made in the field of learning from positive and unlabeled examples (PU learning), particularly in the context of advancing image and text classification tasks. However, applying PU learning to semi-supervised disease classification remains a formidable challenge, primarily due to the limited availability of labeled medical images. In the realm of medical image-aided diagnosis algorithms, numerous theoretical and practical obstacles persist. The research on PU learning for medical image-assisted diagnosis holds substantial importance, as it aims to reduce the time spent by professional experts in classifying images. Unlike natural images, medical images are typically accompanied by a scarcity of annotated data, while an abundance of unlabeled cases exists. Addressing these challenges, this paper introduces a novel generative model inspired by H\\\"older divergence, specifically designed for semi-supervised disease classification using positive and unlabeled medical image data. In this paper, we present a comprehensive formulation of the problem and establish its theoretical feasibility through rigorous mathematical analysis. To evaluate the effectiveness of our proposed approach, we conduct extensive experiments on five benchmark datasets commonly used in PU medical learning: BreastMNIST, PneumoniaMNIST, BloodMNIST, OCTMNIST, and AMD. The experimental results clearly demonstrate the superiority of our method over existing approaches based on KL divergence. Notably, our approach achieves state-of-the-art performance on all five disease classification benchmarks.   By addressing the limitations imposed by limited labeled data and harnessing the untapped potential of unlabeled medical images, our novel generative model presents a promising direction for enhancing semi-supervised disease classification in the field of medical image analysis.","sentences":["In recent years, significant progress has been made in the field of learning from positive and unlabeled examples (PU learning), particularly in the context of advancing image and text classification tasks.","However, applying PU learning to semi-supervised disease classification remains a formidable challenge, primarily due to the limited availability of labeled medical images.","In the realm of medical image-aided diagnosis algorithms, numerous theoretical and practical obstacles persist.","The research on PU learning for medical image-assisted diagnosis holds substantial importance, as it aims to reduce the time spent by professional experts in classifying images.","Unlike natural images, medical images are typically accompanied by a scarcity of annotated data, while an abundance of unlabeled cases exists.","Addressing these challenges, this paper introduces a novel generative model inspired by H\\\"older divergence, specifically designed for semi-supervised disease classification using positive and unlabeled medical image data.","In this paper, we present a comprehensive formulation of the problem and establish its theoretical feasibility through rigorous mathematical analysis.","To evaluate the effectiveness of our proposed approach, we conduct extensive experiments on five benchmark datasets commonly used in PU medical learning: BreastMNIST, PneumoniaMNIST, BloodMNIST, OCTMNIST, and AMD.","The experimental results clearly demonstrate the superiority of our method over existing approaches based on KL divergence.","Notably, our approach achieves state-of-the-art performance on all five disease classification benchmarks.   ","By addressing the limitations imposed by limited labeled data and harnessing the untapped potential of unlabeled medical images, our novel generative model presents a promising direction for enhancing semi-supervised disease classification in the field of medical image analysis."],"url":"http://arxiv.org/abs/2405.04295v1","category":"eess.IV"}
{"created":"2024-05-07 13:09:49","title":"Enhancing the Efficiency and Accuracy of Underlying Asset Reviews in Structured Finance: The Application of Multi-agent Framework","abstract":"Structured finance, which involves restructuring diverse assets into securities like MBS, ABS, and CDOs, enhances capital market efficiency but presents significant due diligence challenges. This study explores the integration of artificial intelligence (AI) with traditional asset review processes to improve efficiency and accuracy in structured finance. Using both open-sourced and close-sourced large language models (LLMs), we demonstrate that AI can automate the verification of information between loan applications and bank statements effectively. While close-sourced models such as GPT-4 show superior performance, open-sourced models like LLAMA3 offer a cost-effective alternative. Dual-agent systems further increase accuracy, though this comes with higher operational costs. This research highlights AI's potential to minimize manual errors and streamline due diligence, suggesting a broader application of AI in financial document analysis and risk management.","sentences":["Structured finance, which involves restructuring diverse assets into securities like MBS, ABS, and CDOs, enhances capital market efficiency but presents significant due diligence challenges.","This study explores the integration of artificial intelligence (AI) with traditional asset review processes to improve efficiency and accuracy in structured finance.","Using both open-sourced and close-sourced large language models (LLMs), we demonstrate that AI can automate the verification of information between loan applications and bank statements effectively.","While close-sourced models such as GPT-4 show superior performance, open-sourced models like LLAMA3 offer a cost-effective alternative.","Dual-agent systems further increase accuracy, though this comes with higher operational costs.","This research highlights AI's potential to minimize manual errors and streamline due diligence, suggesting a broader application of AI in financial document analysis and risk management."],"url":"http://arxiv.org/abs/2405.04294v1","category":"cs.AI"}
{"created":"2024-05-07 13:09:25","title":"Mitigating Clickbait: An Approach to Spoiler Generation Using Multitask Learning","abstract":"This study introduces 'clickbait spoiling', a novel technique designed to detect, categorize, and generate spoilers as succinct text responses, countering the curiosity induced by clickbait content. By leveraging a multi-task learning framework, our model's generalization capabilities are significantly enhanced, effectively addressing the pervasive issue of clickbait. The crux of our research lies in generating appropriate spoilers, be it a phrase, an extended passage, or multiple, depending on the spoiler type required. Our methodology integrates two crucial techniques: a refined spoiler categorization method and a modified version of the Question Answering (QA) mechanism, incorporated within a multi-task learning paradigm for optimized spoiler extraction from context. Notably, we have included fine-tuning methods for models capable of handling longer sequences to accommodate the generation of extended spoilers. This research highlights the potential of sophisticated text processing techniques in tackling the omnipresent issue of clickbait, promising an enhanced user experience in the digital realm.","sentences":["This study introduces 'clickbait spoiling', a novel technique designed to detect, categorize, and generate spoilers as succinct text responses, countering the curiosity induced by clickbait content.","By leveraging a multi-task learning framework, our model's generalization capabilities are significantly enhanced, effectively addressing the pervasive issue of clickbait.","The crux of our research lies in generating appropriate spoilers, be it a phrase, an extended passage, or multiple, depending on the spoiler type required.","Our methodology integrates two crucial techniques: a refined spoiler categorization method and a modified version of the Question Answering (QA) mechanism, incorporated within a multi-task learning paradigm for optimized spoiler extraction from context.","Notably, we have included fine-tuning methods for models capable of handling longer sequences to accommodate the generation of extended spoilers.","This research highlights the potential of sophisticated text processing techniques in tackling the omnipresent issue of clickbait, promising an enhanced user experience in the digital realm."],"url":"http://arxiv.org/abs/2405.04292v1","category":"cs.CL"}
{"created":"2024-05-07 13:06:14","title":"Spiral Attractors in a Reduced Mean-Field Model of Neuron-Glial Interaction","abstract":"It is well known that bursting activity plays an important role in the processes of transmission of neural signals. In terms of population dynamics, macroscopic bursting can be described using a mean-field approach. Mean field theory provides a useful tool for analysis of collective behavior of a large populations of interacting units, allowing to reduce the description of corresponding dynamics to just a few equations. Recently a new phenomenological model was proposed that describes bursting population activity of a big group of excitatory neurons, taking into account short-term synaptic plasticity and the astrocytic modulation of the synaptic dynamics [1]. The purpose of the present study is to investigate various bifurcation scenarios of the appearance of bursting activity in the phenomenological model. We show that the birth of bursting population pattern can be connected both with the cascade of period doubling bifurcations and further development of chaos according to the Shilnikov scenario, which leads to the appearance of a homoclinic attractor containing a homoclinic loop of a saddle-focus equilibrium with the two-dimensional unstable invariant manifold. We also show that the homoclinic spiral attractors observed in the system under study generate several types of bursting activity with different properties.","sentences":["It is well known that bursting activity plays an important role in the processes of transmission of neural signals.","In terms of population dynamics, macroscopic bursting can be described using a mean-field approach.","Mean field theory provides a useful tool for analysis of collective behavior of a large populations of interacting units, allowing to reduce the description of corresponding dynamics to just a few equations.","Recently a new phenomenological model was proposed that describes bursting population activity of a big group of excitatory neurons, taking into account short-term synaptic plasticity and the astrocytic modulation of the synaptic dynamics","[1].","The purpose of the present study is to investigate various bifurcation scenarios of the appearance of bursting activity in the phenomenological model.","We show that the birth of bursting population pattern can be connected both with the cascade of period doubling bifurcations and further development of chaos according to the Shilnikov scenario, which leads to the appearance of a homoclinic attractor containing a homoclinic loop of a saddle-focus equilibrium with the two-dimensional unstable invariant manifold.","We also show that the homoclinic spiral attractors observed in the system under study generate several types of bursting activity with different properties."],"url":"http://arxiv.org/abs/2405.04291v1","category":"nlin.CD"}
{"created":"2024-05-07 13:04:29","title":"Bayesian Simultaneous Localization and Multi-Lane Tracking Using Onboard Sensors and a SD Map","abstract":"High-definition map with accurate lane-level information is crucial for autonomous driving, but the creation of these maps is a resource-intensive process. To this end, we present a cost-effective solution to create lane-level roadmaps using only the global navigation satellite system (GNSS) and a camera on customer vehicles. Our proposed solution utilizes a prior standard-definition (SD) map, GNSS measurements, visual odometry, and lane marking edge detection points, to simultaneously estimate the vehicle's 6D pose, its position within a SD map, and also the 3D geometry of traffic lines. This is achieved using a Bayesian simultaneous localization and multi-object tracking filter, where the estimation of traffic lines is formulated as a multiple extended object tracking problem, solved using a trajectory Poisson multi-Bernoulli mixture (TPMBM) filter. In TPMBM filtering, traffic lines are modeled using B-spline trajectories, and each trajectory is parameterized by a sequence of control points. The proposed solution has been evaluated using experimental data collected by a test vehicle driving on highway. Preliminary results show that the traffic line estimates, overlaid on the satellite image, generally align with the lane markings up to some lateral offsets.","sentences":["High-definition map with accurate lane-level information is crucial for autonomous driving, but the creation of these maps is a resource-intensive process.","To this end, we present a cost-effective solution to create lane-level roadmaps using only the global navigation satellite system (GNSS) and a camera on customer vehicles.","Our proposed solution utilizes a prior standard-definition (SD) map, GNSS measurements, visual odometry, and lane marking edge detection points, to simultaneously estimate the vehicle's 6D pose, its position within a SD map, and also the 3D geometry of traffic lines.","This is achieved using a Bayesian simultaneous localization and multi-object tracking filter, where the estimation of traffic lines is formulated as a multiple extended object tracking problem, solved using a trajectory Poisson multi-Bernoulli mixture (TPMBM) filter.","In TPMBM filtering, traffic lines are modeled using B-spline trajectories, and each trajectory is parameterized by a sequence of control points.","The proposed solution has been evaluated using experimental data collected by a test vehicle driving on highway.","Preliminary results show that the traffic line estimates, overlaid on the satellite image, generally align with the lane markings up to some lateral offsets."],"url":"http://arxiv.org/abs/2405.04290v1","category":"cs.RO"}
{"created":"2024-05-07 12:58:37","title":"Asymmetry of Frequency Distribution in Power Systems: Sources, Impact and Control","abstract":"This letter analyses the sources of asymmetry of frequency probability distributions (PDs) and their impact on the dynamic behaviour of power systems. The letter also discusses on how secondary control can reduce this asymmetry. We also propose an asymmetry index based on the difference between the left and right-hand side standard deviations of the frequency PDs. The IEEE 9-bus system and real-world data obtained from the Irish transmission system serve to show that losses, saturation's and wind generation lead to asymmetric PDs. A relevant result is that the droop-based frequency support provided by wind generation using a tight deadband of 15 mHz leads to significantly increase the asymmetry of the frequency PDs.","sentences":["This letter analyses the sources of asymmetry of frequency probability distributions (PDs) and their impact on the dynamic behaviour of power systems.","The letter also discusses on how secondary control can reduce this asymmetry.","We also propose an asymmetry index based on the difference between the left and right-hand side standard deviations of the frequency PDs.","The IEEE 9-bus system and real-world data obtained from the Irish transmission system serve to show that losses, saturation's and wind generation lead to asymmetric PDs.","A relevant result is that the droop-based frequency support provided by wind generation using a tight deadband of 15 mHz leads to significantly increase the asymmetry of the frequency PDs."],"url":"http://arxiv.org/abs/2405.04287v1","category":"eess.SY"}
{"created":"2024-05-07 12:57:01","title":"Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore","abstract":"The efficacy of an large language model (LLM) generated text detector depends substantially on the availability of sizable training data. White-box zero-shot detectors, which require no such data, are nonetheless limited by the accessibility of the source model of the LLM-generated text. In this paper, we propose an simple but effective black-box zero-shot detection approach, predicated on the observation that human-written texts typically contain more grammatical errors than LLM-generated texts. This approach entails computing the Grammar Error Correction Score (GECScore) for the given text to distinguish between human-written and LLM-generated text. Extensive experimental results show that our method outperforms current state-of-the-art (SOTA) zero-shot and supervised methods, achieving an average AUROC of 98.7% and showing strong robustness against paraphrase and adversarial perturbation attacks.","sentences":["The efficacy of an large language model (LLM) generated text detector depends substantially on the availability of sizable training data.","White-box zero-shot detectors, which require no such data, are nonetheless limited by the accessibility of the source model of the LLM-generated text.","In this paper, we propose an simple but effective black-box zero-shot detection approach, predicated on the observation that human-written texts typically contain more grammatical errors than LLM-generated texts.","This approach entails computing the Grammar Error Correction Score (GECScore) for the given text to distinguish between human-written and LLM-generated text.","Extensive experimental results show that our method outperforms current state-of-the-art (SOTA) zero-shot and supervised methods, achieving an average AUROC of 98.7% and showing strong robustness against paraphrase and adversarial perturbation attacks."],"url":"http://arxiv.org/abs/2405.04286v1","category":"cs.CL"}
{"created":"2024-05-07 12:54:54","title":"On the Foundations of Earth and Climate Foundation Models","abstract":"Foundation models have enormous potential in advancing Earth and climate sciences, however, current approaches may not be optimal as they focus on a few basic features of a desirable Earth and climate foundation model. Crafting the ideal Earth foundation model, we define eleven features which would allow such a foundation model to be beneficial for any geoscientific downstream application in an environmental- and human-centric manner.We further shed light on the way forward to achieve the ideal model and to evaluate Earth foundation models. What comes after foundation models? Energy efficient adaptation, adversarial defenses, and interpretability are among the emerging directions.","sentences":["Foundation models have enormous potential in advancing Earth and climate sciences, however, current approaches may not be optimal as they focus on a few basic features of a desirable Earth and climate foundation model.","Crafting the ideal Earth foundation model, we define eleven features which would allow such a foundation model to be beneficial for any geoscientific downstream application in an environmental- and human-centric manner.","We further shed light on the way forward to achieve the ideal model and to evaluate Earth foundation models.","What comes after foundation models?","Energy efficient adaptation, adversarial defenses, and interpretability are among the emerging directions."],"url":"http://arxiv.org/abs/2405.04285v1","category":"cs.AI"}
{"created":"2024-05-07 12:52:18","title":"Quasi-stationary distributions for subcritical branching Markov chains","abstract":"Consider a subcritical branching Markov chain. Let $Z_n$ denote the counting measure of particles of generation $n$. Under some conditions, we give a probabilistic proof for the existence of the Yaglom limit of $(Z_n)_{n\\in\\mathbb{N}}$ by the moment method, based on the spinal decomposition and the many-to-few formula. As a result, we give explicit integral representations of all quasi-stationary distributions of $(Z_n)_{n\\in\\mathbb{N}}$, whose proofs are direct and probabilistic, and don't rely on Martin boundary theory.","sentences":["Consider a subcritical branching Markov chain.","Let $Z_n$ denote the counting measure of particles of generation $n$. Under some conditions, we give a probabilistic proof for the existence of the Yaglom limit of $(Z_n)_{n\\in\\mathbb{N}}$ by the moment method, based on the spinal decomposition and the many-to-few formula.","As a result, we give explicit integral representations of all quasi-stationary distributions of $(Z_n)_{n\\in\\mathbb{N}}$, whose proofs are direct and probabilistic, and don't rely on Martin boundary theory."],"url":"http://arxiv.org/abs/2405.04284v1","category":"math.PR"}
{"created":"2024-05-07 12:48:37","title":"Modal Folding: Discovering Smooth Folding Patterns for Sheet Materials using Strain-Space Modes","abstract":"Folding can transform mundane objects such as napkins into stunning works of art. However, finding new folding transformations for sheet materials is a challenging problem that requires expertise and real-world experimentation. In this paper, we present Modal Folding -- an automated approach for discovering energetically optimal folding transformations, i.e., large deformations that require little mechanical work. For small deformations, minimizing internal energy for fixed displacement magnitudes leads to the well-known elastic eigenmodes. While linear modes provide promising directions for bending, they cannot capture the rotational motion required for folding. To overcome this limitation, we introduce strain-space modes -- nonlinear analogues of elastic eigenmodes that operate on per-element curvatures instead of vertices. Using strain-space modes to determine target curvatures for bending elements, we can generate complex nonlinear folding motions by simply minimizing the sheet's internal energy. Our modal folding approach offers a systematic and automated way to create complex designs. We demonstrate the effectiveness of our method with simulation results for a range of shapes and materials, and validate our designs with physical prototypes.","sentences":["Folding can transform mundane objects such as napkins into stunning works of art.","However, finding new folding transformations for sheet materials is a challenging problem that requires expertise and real-world experimentation.","In this paper, we present Modal Folding -- an automated approach for discovering energetically optimal folding transformations, i.e., large deformations that require little mechanical work.","For small deformations, minimizing internal energy for fixed displacement magnitudes leads to the well-known elastic eigenmodes.","While linear modes provide promising directions for bending, they cannot capture the rotational motion required for folding.","To overcome this limitation, we introduce strain-space modes -- nonlinear analogues of elastic eigenmodes that operate on per-element curvatures instead of vertices.","Using strain-space modes to determine target curvatures for bending elements, we can generate complex nonlinear folding motions by simply minimizing the sheet's internal energy.","Our modal folding approach offers a systematic and automated way to create complex designs.","We demonstrate the effectiveness of our method with simulation results for a range of shapes and materials, and validate our designs with physical prototypes."],"url":"http://arxiv.org/abs/2405.04280v1","category":"cs.GR"}
{"created":"2024-05-07 12:45:44","title":"Antiferromagnetic and spin spiral correlations in the doped two-dimensional Hubbard model: gauge symmetry, Ward identities, and dynamical mean-field theory analysis","abstract":"We reconsider the derivation of Ward identities for spin stiffnesses, which determine the non-linear sigma model of magnetic degrees of freedom of interacting electrons in the presence of antiferromagnetic or incommensurate correlations. We emphasize that in the approaches, which do not break explicitly spin symmetry of the action, the spatial components of gauge kernel, which is used to obtain spin stiffnesses, remain gauge invariant even in case of spontaneous spin symmetry breaking. We derive the corrected Ward identities, which account for this gauge invariance. We emphasize that the frequency dependence of temporal spin stiffnesses is not fixed by the obtained identities, and show that the infinitesimally small external staggered field is crucially important to obtain finite static uniform transverse susceptibility. On the other hand, we find that the spatial spin stiffnesses are determined by the gauge kernel of the Legendre transformed theory, which is in general {\\it different} from the gauge kernel of the original theory and obtain an explicit expressions for spatial spin stiffnesses through susceptibilities and current correlation functions. We verify numerically the obtained results within dynamic mean field theory, and obtain doping dependencies of the resulting spin-wave stiffnesses for antiferromagnetic and incommensurate phase.","sentences":["We reconsider the derivation of Ward identities for spin stiffnesses, which determine the non-linear sigma model of magnetic degrees of freedom of interacting electrons in the presence of antiferromagnetic or incommensurate correlations.","We emphasize that in the approaches, which do not break explicitly spin symmetry of the action, the spatial components of gauge kernel, which is used to obtain spin stiffnesses, remain gauge invariant even in case of spontaneous spin symmetry breaking.","We derive the corrected Ward identities, which account for this gauge invariance.","We emphasize that the frequency dependence of temporal spin stiffnesses is not fixed by the obtained identities, and show that the infinitesimally small external staggered field is crucially important to obtain finite static uniform transverse susceptibility.","On the other hand, we find that the spatial spin stiffnesses are determined by the gauge kernel of the Legendre transformed theory, which is in general {\\it different} from the gauge kernel of the original theory and obtain an explicit expressions for spatial spin stiffnesses through susceptibilities and current correlation functions.","We verify numerically the obtained results within dynamic mean field theory, and obtain doping dependencies of the resulting spin-wave stiffnesses for antiferromagnetic and incommensurate phase."],"url":"http://arxiv.org/abs/2405.04277v1","category":"cond-mat.str-el"}
{"created":"2024-05-07 12:42:23","title":"Group-aware Parameter-efficient Updating for Content-Adaptive Neural Video Compression","abstract":"Content-adaptive compression is crucial for enhancing the adaptability of the pre-trained neural codec for various contents. Although these methods have been very practical in neural image compression (NIC), their application in neural video compression (NVC) is still limited due to two main aspects: 1), video compression relies heavily on temporal redundancy, therefore updating just one or a few frames can lead to significant errors accumulating over time; 2), NVC frameworks are generally more complex, with many large components that are not easy to update quickly during encoding. To address the previously mentioned challenges, we have developed a content-adaptive NVC technique called Group-aware Parameter-Efficient Updating (GPU). Initially, to minimize error accumulation, we adopt a group-aware approach for updating encoder parameters. This involves adopting a patch-based Group of Pictures (GoP) training strategy to segment a video into patch-based GoPs, which will be updated to facilitate a globally optimized domain-transferable solution. Subsequently, we introduce a parameter-efficient delta-tuning strategy, which is achieved by integrating several light-weight adapters into each coding component of the encoding process by both serial and parallel configuration. Such architecture-agnostic modules stimulate the components with large parameters, thereby reducing both the update cost and the encoding time. We incorporate our GPU into the latest NVC framework and conduct comprehensive experiments, whose results showcase outstanding video compression efficiency across four video benchmarks and adaptability of one medical image benchmark.","sentences":["Content-adaptive compression is crucial for enhancing the adaptability of the pre-trained neural codec for various contents.","Although these methods have been very practical in neural image compression (NIC), their application in neural video compression (NVC) is still limited due to two main aspects: 1), video compression relies heavily on temporal redundancy, therefore updating just one or a few frames can lead to significant errors accumulating over time; 2), NVC frameworks are generally more complex, with many large components that are not easy to update quickly during encoding.","To address the previously mentioned challenges, we have developed a content-adaptive NVC technique called Group-aware Parameter-Efficient Updating (GPU).","Initially, to minimize error accumulation, we adopt a group-aware approach for updating encoder parameters.","This involves adopting a patch-based Group of Pictures (GoP) training strategy to segment a video into patch-based GoPs, which will be updated to facilitate a globally optimized domain-transferable solution.","Subsequently, we introduce a parameter-efficient delta-tuning strategy, which is achieved by integrating several light-weight adapters into each coding component of the encoding process by both serial and parallel configuration.","Such architecture-agnostic modules stimulate the components with large parameters, thereby reducing both the update cost and the encoding time.","We incorporate our GPU into the latest NVC framework and conduct comprehensive experiments, whose results showcase outstanding video compression efficiency across four video benchmarks and adaptability of one medical image benchmark."],"url":"http://arxiv.org/abs/2405.04274v1","category":"eess.IV"}
{"created":"2024-05-07 12:41:31","title":"BUDDy: Single-Channel Blind Unsupervised Dereverberation with Diffusion Models","abstract":"In this paper, we present an unsupervised single-channel method for joint blind dereverberation and room impulse response estimation, based on posterior sampling with diffusion models. We parameterize the reverberation operator using a filter with exponential decay for each frequency subband, and iteratively estimate the corresponding parameters as the speech utterance gets refined along the reverse diffusion trajectory. A measurement consistency criterion enforces the fidelity of the generated speech with the reverberant measurement, while an unconditional diffusion model implements a strong prior for clean speech generation. Without any knowledge of the room impulse response nor any coupled reverberant-anechoic data, we can successfully perform dereverberation in various acoustic scenarios. Our method significantly outperforms previous blind unsupervised baselines, and we demonstrate its increased robustness to unseen acoustic conditions in comparison to blind supervised methods. Audio samples and code are available online.","sentences":["In this paper, we present an unsupervised single-channel method for joint blind dereverberation and room impulse response estimation, based on posterior sampling with diffusion models.","We parameterize the reverberation operator using a filter with exponential decay for each frequency subband, and iteratively estimate the corresponding parameters as the speech utterance gets refined along the reverse diffusion trajectory.","A measurement consistency criterion enforces the fidelity of the generated speech with the reverberant measurement, while an unconditional diffusion model implements a strong prior for clean speech generation.","Without any knowledge of the room impulse response nor any coupled reverberant-anechoic data, we can successfully perform dereverberation in various acoustic scenarios.","Our method significantly outperforms previous blind unsupervised baselines, and we demonstrate its increased robustness to unseen acoustic conditions in comparison to blind supervised methods.","Audio samples and code are available online."],"url":"http://arxiv.org/abs/2405.04272v1","category":"eess.AS"}
{"created":"2024-05-07 12:40:59","title":"Generating Feature Vectors from Phonetic Transcriptions in Cross-Linguistic Data Formats","abstract":"When comparing speech sounds across languages, scholars often make use of feature representations of individual sounds in order to determine fine-grained sound similarities. Although binary feature systems for large numbers of speech sounds have been proposed, large-scale computational applications often face the challenges that the proposed feature systems -- even if they list features for several thousand sounds -- only cover a smaller part of the numerous speech sounds reflected in actual cross-linguistic data. In order to address the problem of missing data for attested speech sounds, we propose a new approach that can create binary feature vectors dynamically for all sounds that can be represented in the the standardized version of the International Phonetic Alphabet proposed by the Cross-Linguistic Transcription Systems (CLTS) reference catalog. Since CLTS is actively used in large data collections, covering more than 2,000 distinct language varieties, our procedure for the generation of binary feature vectors provides immediate access to a very large collection of multilingual wordlists. Testing our feature system in different ways on different datasets proves that the system is not only useful to provide a straightforward means to compare the similarity of speech sounds, but also illustrates its potential to be used in future cross-linguistic machine learning applications.","sentences":["When comparing speech sounds across languages, scholars often make use of feature representations of individual sounds in order to determine fine-grained sound similarities.","Although binary feature systems for large numbers of speech sounds have been proposed, large-scale computational applications often face the challenges that the proposed feature systems -- even if they list features for several thousand sounds -- only cover a smaller part of the numerous speech sounds reflected in actual cross-linguistic data.","In order to address the problem of missing data for attested speech sounds, we propose a new approach that can create binary feature vectors dynamically for all sounds that can be represented in the the standardized version of the International Phonetic Alphabet proposed by the Cross-Linguistic Transcription Systems (CLTS) reference catalog.","Since CLTS is actively used in large data collections, covering more than 2,000 distinct language varieties, our procedure for the generation of binary feature vectors provides immediate access to a very large collection of multilingual wordlists.","Testing our feature system in different ways on different datasets proves that the system is not only useful to provide a straightforward means to compare the similarity of speech sounds, but also illustrates its potential to be used in future cross-linguistic machine learning applications."],"url":"http://arxiv.org/abs/2405.04271v1","category":"cs.CL"}
{"created":"2024-05-07 12:33:12","title":"Quenches in the Sherrington-Kirkpatrick model","abstract":"The Sherrington-Kirkpatrick (SK) model is a prototype of a complex non-convex energy landscape. Dynamical processes evolving on such landscapes and locally aiming to reach minima are generally poorly understood. Here, we study quenches, i.e. dynamics that locally aim to decrease energy. We analyse the energy at convergence for two distinct algorithmic classes, single-spin flip and synchronous dynamics, focusing on greedy and reluctant strategies. We provide precise numerical analysis of the finite size effects and conclude that, perhaps counter-intuitively, the reluctant algorithm is compatible with converging to the ground state energy density, while the greedy strategy is not. Inspired by the single-spin reluctant and greedy algorithms, we investigate two synchronous time algorithms, the sync-greedy and sync-reluctant algorithms. These synchronous processes can be analysed using dynamical mean field theory (DMFT), and a new backtracking version of DMFT. Notably, this is the first time the backtracking DMFT is applied to study dynamical convergence properties in fully connected disordered models. The analysis suggests that the sync-greedy algorithm can also achieve energies compatible with the ground state, and that it undergoes a dynamical phase transition.","sentences":["The Sherrington-Kirkpatrick (SK) model is a prototype of a complex non-convex energy landscape.","Dynamical processes evolving on such landscapes and locally aiming to reach minima are generally poorly understood.","Here, we study quenches, i.e. dynamics that locally aim to decrease energy.","We analyse the energy at convergence for two distinct algorithmic classes, single-spin flip and synchronous dynamics, focusing on greedy and reluctant strategies.","We provide precise numerical analysis of the finite size effects and conclude that, perhaps counter-intuitively, the reluctant algorithm is compatible with converging to the ground state energy density, while the greedy strategy is not.","Inspired by the single-spin reluctant and greedy algorithms, we investigate two synchronous time algorithms, the sync-greedy and sync-reluctant algorithms.","These synchronous processes can be analysed using dynamical mean field theory (DMFT), and a new backtracking version of DMFT.","Notably, this is the first time the backtracking DMFT is applied to study dynamical convergence properties in fully connected disordered models.","The analysis suggests that the sync-greedy algorithm can also achieve energies compatible with the ground state, and that it undergoes a dynamical phase transition."],"url":"http://arxiv.org/abs/2405.04267v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-07 12:30:37","title":"CMB spectrum in unified EFT of dark energy: scalar-tensor and vector-tensor theories","abstract":"We study the cosmic microwave background (CMB) radiation in the unified description of the effective field theory (EFT) of dark energy that accommodates both scalar-tensor and vector-tensor theories. The boundaries of different classes of theories are universally parameterised by a new EFT parameter $\\alpha_V$ characterising the vectorial nature of dark energy and a set of consistency relations associated with the global/local shift symmetry. After implementing the equations of motion in a Boltzmann code, as a demonstration, we compute the CMB power spectrum based on the $w$CDM background with the EFT parameterisation of perturbations and a concrete Horndeski/generalised Proca theory. We show that the vectorial nature generically prevents modifications of gravity in the CMB spectrum. On the other hand, while the shift symmetry is less significant in the perturbation equations unless the background is close to the $\\Lambda$CDM, it requires that the effective equation of state of dark energy is in the phantom region $w_{\\rm DE}<-1$. The latter is particularly interesting in light of the latest result of the DESI+CMB combination as the observational verification of $w_{\\rm DE}>-1$ can rule out shift-symmetric theories including vector-tensor theories in one shot.","sentences":["We study the cosmic microwave background (CMB) radiation in the unified description of the effective field theory (EFT) of dark energy that accommodates both scalar-tensor and vector-tensor theories.","The boundaries of different classes of theories are universally parameterised by a new EFT parameter $\\alpha_V$ characterising the vectorial nature of dark energy and a set of consistency relations associated with the global/local shift symmetry.","After implementing the equations of motion in a Boltzmann code, as a demonstration, we compute the CMB power spectrum based on the $w$CDM background with the EFT parameterisation of perturbations and a concrete Horndeski/generalised Proca theory.","We show that the vectorial nature generically prevents modifications of gravity in the CMB spectrum.","On the other hand, while the shift symmetry is less significant in the perturbation equations unless the background is close to the $\\Lambda$CDM, it requires that the effective equation of state of dark energy is in the phantom region $w_{\\rm DE}<-1$.","The latter is particularly interesting in light of the latest result of the DESI+CMB combination as the observational verification of $w_{\\rm DE}>-1$ can rule out shift-symmetric theories including vector-tensor theories in one shot."],"url":"http://arxiv.org/abs/2405.04265v1","category":"astro-ph.CO"}
{"created":"2024-05-07 12:21:42","title":"Graph Reconstruction from Noisy Random Subgraphs","abstract":"We consider the problem of reconstructing an undirected graph $G$ on $n$ vertices given multiple random noisy subgraphs or \"traces\". Specifically, a trace is generated by sampling each vertex with probability $p_v$, then taking the resulting induced subgraph on the sampled vertices, and then adding noise in the form of either (a) deleting each edge in the subgraph with probability $1-p_e$, or (b) deleting each edge with probability $f_e$ and transforming a non-edge into an edge with probability $f_e$. We show that, under mild assumptions on $p_v$, $p_e$ and $f_e$, if $G$ is selected uniformly at random, then $O(p_e^{-1} p_v^{-2} \\log n)$ or $O((f_e-1/2)^{-2} p_v^{-2} \\log n)$ traces suffice to reconstruct $G$ with high probability. In contrast, if $G$ is arbitrary, then $\\exp(\\Omega(n))$ traces are necessary even when $p_v=1, p_e=1/2$.","sentences":["We consider the problem of reconstructing an undirected graph $G$ on $n$ vertices given multiple random noisy subgraphs or \"traces\".","Specifically, a trace is generated by sampling each vertex with probability $p_v$, then taking the resulting induced subgraph on the sampled vertices, and then adding noise in the form of either (a) deleting each edge in the subgraph with probability $1-p_e$, or (b) deleting each edge with probability $f_e$ and transforming a non-edge into an edge with probability $f_e$. We show that, under mild assumptions on $p_v$, $p_e$ and $f_e$, if $G$ is selected uniformly at random, then $O(p_e^{-1} p_v^{-2} \\log n)$ or $O((f_e-1/2)^{-2} p_v^{-2} \\log n)$ traces suffice to reconstruct $G$ with high probability.","In contrast, if $G$ is arbitrary, then $\\exp(\\Omega(n))$ traces are necessary even when $p_v=1, p_e=1/2$."],"url":"http://arxiv.org/abs/2405.04261v1","category":"cs.IT"}
{"created":"2024-05-07 12:20:12","title":"Verified Neural Compressed Sensing","abstract":"We develop the first (to the best of our knowledge) provably correct neural networks for a precise computational task, with the proof of correctness generated by an automated verification algorithm without any human input. Prior work on neural network verification has focused on partial specifications that, even when satisfied, are not sufficient to ensure that a neural network never makes errors. We focus on applying neural network verification to computational tasks with a precise notion of correctness, where a verifiably correct neural network provably solves the task at hand with no caveats. In particular, we develop an approach to train and verify the first provably correct neural networks for compressed sensing, i.e., recovering sparse vectors from a number of measurements smaller than the dimension of the vector. We show that for modest problem dimensions (up to 50), we can train neural networks that provably recover a sparse vector from linear and binarized linear measurements. Furthermore, we show that the complexity of the network (number of neurons/layers) can be adapted to the problem difficulty and solve problems where traditional compressed sensing methods are not known to provably work.","sentences":["We develop the first (to the best of our knowledge) provably correct neural networks for a precise computational task, with the proof of correctness generated by an automated verification algorithm without any human input.","Prior work on neural network verification has focused on partial specifications that, even when satisfied, are not sufficient to ensure that a neural network never makes errors.","We focus on applying neural network verification to computational tasks with a precise notion of correctness, where a verifiably correct neural network provably solves the task at hand with no caveats.","In particular, we develop an approach to train and verify the first provably correct neural networks for compressed sensing, i.e., recovering sparse vectors from a number of measurements smaller than the dimension of the vector.","We show that for modest problem dimensions (up to 50), we can train neural networks that provably recover a sparse vector from linear and binarized linear measurements.","Furthermore, we show that the complexity of the network (number of neurons/layers) can be adapted to the problem difficulty and solve problems where traditional compressed sensing methods are not known to provably work."],"url":"http://arxiv.org/abs/2405.04260v1","category":"cs.LG"}
{"created":"2024-05-07 12:15:02","title":"Distributed variable screening for generalized linear models","abstract":"In this article, we develop a distributed variable screening method for generalized linear models. This method is designed to handle situations where both the sample size and the number of covariates are large. Specifically, the proposed method selects relevant covariates by using a sparsity-restricted surrogate likelihood estimator. It takes into account the joint effects of the covariates rather than just the marginal effect, and this characteristic enhances the reliability of the screening results. We establish the sure screening property of the proposed method, which ensures that with a high probability, the true model is included in the selected model. Simulation studies are conducted to evaluate the finite sample performance of the proposed method, and an application to a real dataset showcases its practical utility.","sentences":["In this article, we develop a distributed variable screening method for generalized linear models.","This method is designed to handle situations where both the sample size and the number of covariates are large.","Specifically, the proposed method selects relevant covariates by using a sparsity-restricted surrogate likelihood estimator.","It takes into account the joint effects of the covariates rather than just the marginal effect, and this characteristic enhances the reliability of the screening results.","We establish the sure screening property of the proposed method, which ensures that with a high probability, the true model is included in the selected model.","Simulation studies are conducted to evaluate the finite sample performance of the proposed method, and an application to a real dataset showcases its practical utility."],"url":"http://arxiv.org/abs/2405.04254v1","category":"stat.ME"}
{"created":"2024-05-07 12:13:11","title":"VAEneu: A New Avenue for VAE Application on Probabilistic Forecasting","abstract":"This paper presents VAEneu, an innovative autoregressive method for multistep ahead univariate probabilistic time series forecasting. We employ the conditional VAE framework and optimize the lower bound of the predictive distribution likelihood function by adopting the Continuous Ranked Probability Score (CRPS), a strictly proper scoring rule, as the loss function. This novel pipeline results in forecasting sharp and well-calibrated predictive distribution. Through a comprehensive empirical study, VAEneu is rigorously benchmarked against 12 baseline models across 12 datasets. The results unequivocally demonstrate VAEneu's remarkable forecasting performance. VAEneu provides a valuable tool for quantifying future uncertainties, and our extensive empirical study lays the foundation for future comparative studies for univariate multistep ahead probabilistic forecasting.","sentences":["This paper presents VAEneu, an innovative autoregressive method for multistep ahead univariate probabilistic time series forecasting.","We employ the conditional VAE framework and optimize the lower bound of the predictive distribution likelihood function by adopting the Continuous Ranked Probability Score (CRPS), a strictly proper scoring rule, as the loss function.","This novel pipeline results in forecasting sharp and well-calibrated predictive distribution.","Through a comprehensive empirical study, VAEneu is rigorously benchmarked against 12 baseline models across 12 datasets.","The results unequivocally demonstrate VAEneu's remarkable forecasting performance.","VAEneu provides a valuable tool for quantifying future uncertainties, and our extensive empirical study lays the foundation for future comparative studies for univariate multistep ahead probabilistic forecasting."],"url":"http://arxiv.org/abs/2405.04252v1","category":"cs.LG"}
{"created":"2024-05-07 12:11:15","title":"A General Model for Detecting Learner Engagement: Implementation and Evaluation","abstract":"Considering learner engagement has a mutual benefit for both learners and instructors. Instructors can help learners increase their attention, involvement, motivation, and interest. On the other hand, instructors can improve their instructional performance by evaluating the cumulative results of all learners and upgrading their training programs. This paper proposes a general, lightweight model for selecting and processing features to detect learners' engagement levels while preserving the sequential temporal relationship over time. During training and testing, we analyzed the videos from the publicly available DAiSEE dataset to capture the dynamic essence of learner engagement. We have also proposed an adaptation policy to find new labels that utilize the affective states of this dataset related to education, thereby improving the models' judgment. The suggested model achieves an accuracy of 68.57\\% in a specific implementation and outperforms the studied state-of-the-art models detecting learners' engagement levels.","sentences":["Considering learner engagement has a mutual benefit for both learners and instructors.","Instructors can help learners increase their attention, involvement, motivation, and interest.","On the other hand, instructors can improve their instructional performance by evaluating the cumulative results of all learners and upgrading their training programs.","This paper proposes a general, lightweight model for selecting and processing features to detect learners' engagement levels while preserving the sequential temporal relationship over time.","During training and testing, we analyzed the videos from the publicly available DAiSEE dataset to capture the dynamic essence of learner engagement.","We have also proposed an adaptation policy to find new labels that utilize the affective states of this dataset related to education, thereby improving the models' judgment.","The suggested model achieves an accuracy of 68.57\\% in a specific implementation and outperforms the studied state-of-the-art models detecting learners' engagement levels."],"url":"http://arxiv.org/abs/2405.04251v1","category":"cs.CV"}
{"created":"2024-05-07 12:07:06","title":"Federated Learning for Cooperative Inference Systems: The Case of Early Exit Networks","abstract":"As Internet of Things (IoT) technology advances, end devices like sensors and smartphones are progressively equipped with AI models tailored to their local memory and computational constraints. Local inference reduces communication costs and latency; however, these smaller models typically underperform compared to more sophisticated models deployed on edge servers or in the cloud. Cooperative Inference Systems (CISs) address this performance trade-off by enabling smaller devices to offload part of their inference tasks to more capable devices. These systems often deploy hierarchical models that share numerous parameters, exemplified by Deep Neural Networks (DNNs) that utilize strategies like early exits or ordered dropout. In such instances, Federated Learning (FL) may be employed to jointly train the models within a CIS. Yet, traditional training methods have overlooked the operational dynamics of CISs during inference, particularly the potential high heterogeneity in serving rates across clients. To address this gap, we propose a novel FL approach designed explicitly for use in CISs that accounts for these variations in serving rates. Our framework not only offers rigorous theoretical guarantees, but also surpasses state-of-the-art (SOTA) training algorithms for CISs, especially in scenarios where inference request rates or data availability are uneven among clients.","sentences":["As Internet of Things (IoT) technology advances, end devices like sensors and smartphones are progressively equipped with AI models tailored to their local memory and computational constraints.","Local inference reduces communication costs and latency; however, these smaller models typically underperform compared to more sophisticated models deployed on edge servers or in the cloud.","Cooperative Inference Systems (CISs) address this performance trade-off by enabling smaller devices to offload part of their inference tasks to more capable devices.","These systems often deploy hierarchical models that share numerous parameters, exemplified by Deep Neural Networks (DNNs) that utilize strategies like early exits or ordered dropout.","In such instances, Federated Learning (FL) may be employed to jointly train the models within a CIS.","Yet, traditional training methods have overlooked the operational dynamics of CISs during inference, particularly the potential high heterogeneity in serving rates across clients.","To address this gap, we propose a novel FL approach designed explicitly for use in CISs that accounts for these variations in serving rates.","Our framework not only offers rigorous theoretical guarantees, but also surpasses state-of-the-art (SOTA) training algorithms for CISs, especially in scenarios where inference request rates or data availability are uneven among clients."],"url":"http://arxiv.org/abs/2405.04249v1","category":"cs.LG"}
{"created":"2024-05-07 12:02:23","title":"Exploring Correlations of Self-supervised Tasks for Graphs","abstract":"Graph self-supervised learning has sparked a research surge in training informative representations without accessing any labeled data. However, our understanding of graph self-supervised learning remains limited, and the inherent relationships between various self-supervised tasks are still unexplored. Our paper aims to provide a fresh understanding of graph self-supervised learning based on task correlations. Specifically, we evaluate the performance of the representations trained by one specific task on other tasks and define correlation values to quantify task correlations. Through this process, we unveil the task correlations between various self-supervised tasks and can measure their expressive capabilities, which are closely related to downstream performance. By analyzing the correlation values between tasks across various datasets, we reveal the complexity of task correlations and the limitations of existing multi-task learning methods. To obtain more capable representations, we propose Graph Task Correlation Modeling (GraphTCM) to illustrate the task correlations and utilize it to enhance graph self-supervised training. The experimental results indicate that our method significantly outperforms existing methods across various downstream tasks.","sentences":["Graph self-supervised learning has sparked a research surge in training informative representations without accessing any labeled data.","However, our understanding of graph self-supervised learning remains limited, and the inherent relationships between various self-supervised tasks are still unexplored.","Our paper aims to provide a fresh understanding of graph self-supervised learning based on task correlations.","Specifically, we evaluate the performance of the representations trained by one specific task on other tasks and define correlation values to quantify task correlations.","Through this process, we unveil the task correlations between various self-supervised tasks and can measure their expressive capabilities, which are closely related to downstream performance.","By analyzing the correlation values between tasks across various datasets, we reveal the complexity of task correlations and the limitations of existing multi-task learning methods.","To obtain more capable representations, we propose Graph Task Correlation Modeling (GraphTCM) to illustrate the task correlations and utilize it to enhance graph self-supervised training.","The experimental results indicate that our method significantly outperforms existing methods across various downstream tasks."],"url":"http://arxiv.org/abs/2405.04245v1","category":"cs.LG"}
{"created":"2024-05-07 11:58:34","title":"Exploring the Potential of Robot-Collected Data for Training Gesture Classification Systems","abstract":"Sensors and Artificial Intelligence (AI) have revolutionized the analysis of human movement, but the scarcity of specific samples presents a significant challenge in training intelligent systems, particularly in the context of diagnosing neurodegenerative diseases. This study investigates the feasibility of utilizing robot-collected data to train classification systems traditionally trained with human-collected data. As a proof of concept, we recorded a database of numeric characters using an ABB robotic arm and an Apple Watch. We compare the classification performance of the trained systems using both human-recorded and robot-recorded data. Our primary objective is to determine the potential for accurate identification of human numeric characters wearing a smartwatch using robotic movement as training data. The findings of this study offer valuable insights into the feasibility of using robot-collected data for training classification systems. This research holds broad implications across various domains that require reliable identification, particularly in scenarios where access to human-specific data is limited.","sentences":["Sensors and Artificial Intelligence (AI) have revolutionized the analysis of human movement, but the scarcity of specific samples presents a significant challenge in training intelligent systems, particularly in the context of diagnosing neurodegenerative diseases.","This study investigates the feasibility of utilizing robot-collected data to train classification systems traditionally trained with human-collected data.","As a proof of concept, we recorded a database of numeric characters using an ABB robotic arm and an Apple Watch.","We compare the classification performance of the trained systems using both human-recorded and robot-recorded data.","Our primary objective is to determine the potential for accurate identification of human numeric characters wearing a smartwatch using robotic movement as training data.","The findings of this study offer valuable insights into the feasibility of using robot-collected data for training classification systems.","This research holds broad implications across various domains that require reliable identification, particularly in scenarios where access to human-specific data is limited."],"url":"http://arxiv.org/abs/2405.04241v1","category":"cs.RO"}
{"created":"2024-05-07 11:55:39","title":"Exploring relaxation dynamics in warm dense plasmas by tailoring non-thermal electron distributions with a free electron laser","abstract":"Knowing the characteristic relaxation time of free electrons in a dense plasma is crucial to our understanding of plasma equilibration and transport. However, experimental investigations of electron relaxation dynamics have been hindered by the ultra-fast, sub-femtosecond time scales on which these interactions typically take place. Here we propose a novel approach that uses x-rays from a free electron laser to generate well-defined non-thermal electron distributions, which can then be tracked via emission spectroscopy from radiative recombination as they thermalize. Collisional radiative simulations reveal how this method can enable the measurement of electron relaxation time scales {\\it in situ}, shedding light on the applicability and accuracy of the Coulomb Logarithm framework for modelling collisions in dense plasmas.","sentences":["Knowing the characteristic relaxation time of free electrons in a dense plasma is crucial to our understanding of plasma equilibration and transport.","However, experimental investigations of electron relaxation dynamics have been hindered by the ultra-fast, sub-femtosecond time scales on which these interactions typically take place.","Here we propose a novel approach that uses x-rays from a free electron laser to generate well-defined non-thermal electron distributions, which can then be tracked via emission spectroscopy from radiative recombination as they thermalize.","Collisional radiative simulations reveal how this method can enable the measurement of electron relaxation time scales {\\it in situ}, shedding light on the applicability and accuracy of the Coulomb Logarithm framework for modelling collisions in dense plasmas."],"url":"http://arxiv.org/abs/2405.04240v1","category":"physics.plasm-ph"}
{"created":"2024-05-07 11:54:22","title":"LTLDoG: Satisfying Temporally-Extended Symbolic Constraints for Safe Diffusion-based Planning","abstract":"Operating effectively in complex environments while complying with specified constraints is crucial for the safe and successful deployment of robots that interact with and operate around people. In this work, we focus on generating long-horizon trajectories that adhere to novel static and temporally-extended constraints/instructions at test time. We propose a data-driven diffusion-based framework, LTLDoG, that modifies the inference steps of the reverse process given an instruction specified using finite linear temporal logic ($\\text{LTL}_f$). LTLDoG leverages a satisfaction value function on $\\text{LTL}_f$ and guides the sampling steps using its gradient field. This value function can also be trained to generalize to new instructions not observed during training, enabling flexible test-time adaptability. Experiments in robot navigation and manipulation illustrate that the method is able to generate trajectories that satisfy formulae that specify obstacle avoidance and visitation sequences.","sentences":["Operating effectively in complex environments while complying with specified constraints is crucial for the safe and successful deployment of robots that interact with and operate around people.","In this work, we focus on generating long-horizon trajectories that adhere to novel static and temporally-extended constraints/instructions at test time.","We propose a data-driven diffusion-based framework, LTLDoG, that modifies the inference steps of the reverse process given an instruction specified using finite linear temporal logic ($\\text{LTL}_f$).","LTLDoG leverages a satisfaction value function on $\\text{LTL}_f$ and guides the sampling steps using its gradient field.","This value function can also be trained to generalize to new instructions not observed during training, enabling flexible test-time adaptability.","Experiments in robot navigation and manipulation illustrate that the method is able to generate trajectories that satisfy formulae that specify obstacle avoidance and visitation sequences."],"url":"http://arxiv.org/abs/2405.04235v1","category":"cs.RO"}
{"created":"2024-05-07 11:52:49","title":"Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models","abstract":"We introduce Vidu, a high-performance text-to-video generator that is capable of producing 1080p videos up to 16 seconds in a single generation. Vidu is a diffusion model with U-ViT as its backbone, which unlocks the scalability and the capability for handling long videos. Vidu exhibits strong coherence and dynamism, and is capable of generating both realistic and imaginative videos, as well as understanding some professional photography techniques, on par with Sora -- the most powerful reported text-to-video generator. Finally, we perform initial experiments on other controllable video generation, including canny-to-video generation, video prediction and subject-driven generation, which demonstrate promising results.","sentences":["We introduce Vidu, a high-performance text-to-video generator that is capable of producing 1080p videos up to 16 seconds in a single generation.","Vidu is a diffusion model with U-ViT as its backbone, which unlocks the scalability and the capability for handling long videos.","Vidu exhibits strong coherence and dynamism, and is capable of generating both realistic and imaginative videos, as well as understanding some professional photography techniques, on par with Sora -- the most powerful reported text-to-video generator.","Finally, we perform initial experiments on other controllable video generation, including canny-to-video generation, video prediction and subject-driven generation, which demonstrate promising results."],"url":"http://arxiv.org/abs/2405.04233v1","category":"cs.CV"}
{"created":"2024-05-07 11:52:14","title":"Probing the polarized photon content of the proton in $ep$ collisions at the EIC","abstract":"We study the single-inclusive production of prompt photons in electron proton collisions, $ep \\to \\gamma X $, for kinematics relevant at the Electron-Ion Collider (EIC). We perform a perturbative calculation of the differential cross section to next-to-leading order in QCD and to lowest order in QED. We consider unpolarized collisions as well as scattering of longitudinally polarized incident electrons and protons. We show that the cross sections are sensitive to the parton distribution functions of photons inside the proton, which we find to generate the dominant contributions in certain kinematical regions at the EIC. We also investigate the effects of photon isolation on the unpolarized and polarized cross sections.","sentences":["We study the single-inclusive production of prompt photons in electron proton collisions, $ep \\to \\gamma X $, for kinematics relevant at the Electron-Ion Collider (EIC).","We perform a perturbative calculation of the differential cross section to next-to-leading order in QCD and to lowest order in QED.","We consider unpolarized collisions as well as scattering of longitudinally polarized incident electrons and protons.","We show that the cross sections are sensitive to the parton distribution functions of photons inside the proton, which we find to generate the dominant contributions in certain kinematical regions at the EIC.","We also investigate the effects of photon isolation on the unpolarized and polarized cross sections."],"url":"http://arxiv.org/abs/2405.04232v1","category":"hep-ph"}
{"created":"2024-05-07 11:52:03","title":"Nonlinear dynamics driving the conversion of gravitational and electromagnetic waves in cylindrically symmetric spacetime","abstract":"Using the ``composite harmonic mapping method,\" we construct exact solutions for cylindrically symmetric gravitational and electromagnetic waves within the Einstein-Maxwell system, focusing on the conversion dynamics between these types of waves. In this approach, we employs two types of geodesic surfaces in ${\\mathbb H}^{2}_{C}$: (a) the complex line and (b) the totally real Lagrangian plane, applied to two different vacuum seed solutions: (i) a vacuum solution previously utilized in our studies and (ii) the solitonic vacuum solution constructed previously by Economou and Tsoubelis. We study three scenarios: case (a) with seeds (i) and (ii), and case (b) with seed (ii). In all cases (a) and (b), solutions demonstrate notable mode conversions near the symmetric axis. In case (a) with seed (i) or seed (ii), we show that any change in the occupancy of the gravitational or electromagnetic mode relative to the C-energy near the axis always reverts to its initial state once the wave moves away from the axis. Particularly in case (b) with seed (ii), nontrivial conversions occur even when the wave moves away from the axis. In this case, the amplification factors of electromagnetic modes range from an upper limit of approximately $2.4$ to a lower limit of about $0.4$, when comparing the contributions of electromagnetic mode to C-energy at past and future null infinities.","sentences":["Using the ``composite harmonic mapping method,\" we construct exact solutions for cylindrically symmetric gravitational and electromagnetic waves within the Einstein-Maxwell system, focusing on the conversion dynamics between these types of waves.","In this approach, we employs two types of geodesic surfaces in ${\\mathbb H}^{2}_{C}$: (a) the complex line and (b) the totally real Lagrangian plane, applied to two different vacuum seed solutions: (i) a vacuum solution previously utilized in our studies and (ii) the solitonic vacuum solution constructed previously by Economou and Tsoubelis.","We study three scenarios: case (a) with seeds (i) and (ii), and case (b) with seed (ii).","In all cases (a) and (b), solutions demonstrate notable mode conversions near the symmetric axis.","In case (a) with seed (i) or seed (ii), we show that any change in the occupancy of the gravitational or electromagnetic mode relative to the C-energy near the axis always reverts to its initial state once the wave moves away from the axis.","Particularly in case (b) with seed (ii), nontrivial conversions occur even when the wave moves away from the axis.","In this case, the amplification factors of electromagnetic modes range from an upper limit of approximately $2.4$ to a lower limit of about $0.4$, when comparing the contributions of electromagnetic mode to C-energy at past and future null infinities."],"url":"http://arxiv.org/abs/2405.04231v1","category":"gr-qc"}
{"created":"2024-05-07 11:50:25","title":"Unveiling the optimization process of Physics Informed Neural Networks: How accurate and competitive can PINNs be?","abstract":"This study investigates the potential accuracy boundaries of physics-informed neural networks, contrasting their approach with previous similar works and traditional numerical methods. We find that selecting improved optimization algorithms significantly enhances the accuracy of the results. Simple modifications to the loss function may also improve precision, offering an additional avenue for enhancement. Despite optimization algorithms having a greater impact on convergence than adjustments to the loss function, practical considerations often favor tweaking the latter due to ease of implementation. On a global scale, the integration of an enhanced optimizer and a marginally adjusted loss function enables a reduction in the loss function by several orders of magnitude across diverse physical problems. Consequently, our results obtained using compact networks (typically comprising 2 or 3 layers of 20-30 neurons) achieve accuracies comparable to finite difference schemes employing thousands of grid points. This study encourages the continued advancement of PINNs and associated optimization techniques for broader applications across various fields.","sentences":["This study investigates the potential accuracy boundaries of physics-informed neural networks, contrasting their approach with previous similar works and traditional numerical methods.","We find that selecting improved optimization algorithms significantly enhances the accuracy of the results.","Simple modifications to the loss function may also improve precision, offering an additional avenue for enhancement.","Despite optimization algorithms having a greater impact on convergence than adjustments to the loss function, practical considerations often favor tweaking the latter due to ease of implementation.","On a global scale, the integration of an enhanced optimizer and a marginally adjusted loss function enables a reduction in the loss function by several orders of magnitude across diverse physical problems.","Consequently, our results obtained using compact networks (typically comprising 2 or 3 layers of 20-30 neurons) achieve accuracies comparable to finite difference schemes employing thousands of grid points.","This study encourages the continued advancement of PINNs and associated optimization techniques for broader applications across various fields."],"url":"http://arxiv.org/abs/2405.04230v1","category":"physics.comp-ph"}
{"created":"2024-05-07 11:44:30","title":"Long-term usage of the off-grid photovoltaic system with lithium-ion battery-based energy storage system on high mountains: A case study in Payiun Lodge on Mt. Jade in Taiwan","abstract":"Energy supply on high mountains remains an open issue since grid connection is unavailable. In the past, diesel generators with lead-acid battery energy storage systems (ESSs) are applied in most cases. Recently, photovoltaic (PV) system with lithium-ion (Li-ion) battery ESS is an appropriate method for solving this problem in a greener way. In 2016, an off-grid PV system with Li-ion battery ESS has been installed in Paiyun Lodge on Mt. Jade (the highest lodge in Taiwan). After operation for more than 7 years, the aging problem of the whole electric power system becomes a critical issue for long-term usage. In this work, a method is established for analyzing the massive energy data (over 7 million rows) and estimating the health of the Li-ion battery system, such as daily operation patterns as well as C-rate, temperature, and accumulated energy distributions. The accomplished electric power improvement project dealing with the power system aging is reported. Based on the long-term usage experience, a simple cost analysis model between lead-acid and Li-ion battery systems is built, explaining that the expensive Li-ion batteries can compete with the cheap lead-acid batteries for long-term usage on high mountains. This case study provides engineers and researchers a fundamental understanding of the long-term usage of off-grid PV ESSs and engineering on high mountains.","sentences":["Energy supply on high mountains remains an open issue since grid connection is unavailable.","In the past, diesel generators with lead-acid battery energy storage systems (ESSs) are applied in most cases.","Recently, photovoltaic (PV) system with lithium-ion (Li-ion) battery ESS is an appropriate method for solving this problem in a greener way.","In 2016, an off-grid PV system with Li-ion battery ESS has been installed in Paiyun Lodge on Mt. Jade (the highest lodge in Taiwan).","After operation for more than 7 years, the aging problem of the whole electric power system becomes a critical issue for long-term usage.","In this work, a method is established for analyzing the massive energy data (over 7 million rows) and estimating the health of the Li-ion battery system, such as daily operation patterns as well as C-rate, temperature, and accumulated energy distributions.","The accomplished electric power improvement project dealing with the power system aging is reported.","Based on the long-term usage experience, a simple cost analysis model between lead-acid and Li-ion battery systems is built, explaining that the expensive Li-ion batteries can compete with the cheap lead-acid batteries for long-term usage on high mountains.","This case study provides engineers and researchers a fundamental understanding of the long-term usage of off-grid PV ESSs and engineering on high mountains."],"url":"http://arxiv.org/abs/2405.04225v1","category":"eess.SY"}
{"created":"2024-05-07 11:40:35","title":"ChemPlasKin: a general-purpose program for unified gas and plasma kinetics simulations","abstract":"This work introduces ChemPlasKin, a freely accessible solver optimized for zero-dimensional (0D) simulations of chemical kinetics of neutral gas in non-equilibrium plasma environments. By integrating the electron Boltzmann equation solver, CppBOLOS, with the open-source combustion library, Cantera, at the source code level, ChemPlasKin computes time-resolved evolution of species concentration and gas temperature in a unified gas-plasma kinetics framework. The model allows high fidelity predictions of both chemical thermal effects and plasma-induced heating, including fast gas heating and slower vibrational-translational relaxation processes. Additionally, a new heat loss model is developed for nanosecond pulsed discharges, specifically within pin-pin electrode configurations. With its versatility, ChemPlasKin is well-suited for a wide range of applications, from plasma-assisted combustion (PAC) to fuel reforming. In this paper, the reliability, accuracy and efficiency of ChemPlasKin are validated through a number of test problems, demonstrating its utility in advancing gas-plasma kinetic studies.","sentences":["This work introduces ChemPlasKin, a freely accessible solver optimized for zero-dimensional (0D) simulations of chemical kinetics of neutral gas in non-equilibrium plasma environments.","By integrating the electron Boltzmann equation solver, CppBOLOS, with the open-source combustion library, Cantera, at the source code level, ChemPlasKin computes time-resolved evolution of species concentration and gas temperature in a unified gas-plasma kinetics framework.","The model allows high fidelity predictions of both chemical thermal effects and plasma-induced heating, including fast gas heating and slower vibrational-translational relaxation processes.","Additionally, a new heat loss model is developed for nanosecond pulsed discharges, specifically within pin-pin electrode configurations.","With its versatility, ChemPlasKin is well-suited for a wide range of applications, from plasma-assisted combustion (PAC) to fuel reforming.","In this paper, the reliability, accuracy and efficiency of ChemPlasKin are validated through a number of test problems, demonstrating its utility in advancing gas-plasma kinetic studies."],"url":"http://arxiv.org/abs/2405.04224v1","category":"physics.plasm-ph"}
{"created":"2024-05-07 11:36:31","title":"Two dimensional semiconductors: optical and electronic properties","abstract":"In the last decade atomically thin 2D materials have emerged as a perfect platform for studying and tuning light-matter interaction and electronic properties in nanostructures. The optoelectronic properties in layered materials such as transition-metal-dichalcogenides (TMDs) are governed by excitons, Coulomb bound electron-hole pairs, even at room temperature. The energy, wave function extension, spin and valley properties of optically excited conduction electrons and valence holes are controllable via multiple experimentally accessible knobs, such as lattice strain, varying atomic registries, dielectric engineering as well as electric and magnetic fields. This results in a multitude of fascinating physical phenomena in optics and transport linked to excitons with very specific properties, such as bright and dark excitons, interlayer and charge transfer excitons as well as hybrid and moir\\'e excitons. In this book chapter we introduce general optoelectronic properties of 2D materials and energy landscapes in TMD monolayers as well as their vertical and lateral heterostructures, including twisted TMD hetero- and homobilayer bilayers with moir\\'e excitons and lattice recombination effects. We review the recently gained insights and open questions on exciton diffusion, strain- and field-induced exciton drift. We discuss intriguing non-linear many-particle effects, such as exciton halo formation, negative and anomalous diffusion, the surprising anti-funneling of dark excitons.","sentences":["In the last decade atomically thin 2D materials have emerged as a perfect platform for studying and tuning light-matter interaction and electronic properties in nanostructures.","The optoelectronic properties in layered materials such as transition-metal-dichalcogenides (TMDs) are governed by excitons, Coulomb bound electron-hole pairs, even at room temperature.","The energy, wave function extension, spin and valley properties of optically excited conduction electrons and valence holes are controllable via multiple experimentally accessible knobs, such as lattice strain, varying atomic registries, dielectric engineering as well as electric and magnetic fields.","This results in a multitude of fascinating physical phenomena in optics and transport linked to excitons with very specific properties, such as bright and dark excitons, interlayer and charge transfer excitons as well as hybrid and moir\\'e excitons.","In this book chapter we introduce general optoelectronic properties of 2D materials and energy landscapes in TMD monolayers as well as their vertical and lateral heterostructures, including twisted TMD hetero- and homobilayer bilayers with moir\\'e excitons and lattice recombination effects.","We review the recently gained insights and open questions on exciton diffusion, strain- and field-induced exciton drift.","We discuss intriguing non-linear many-particle effects, such as exciton halo formation, negative and anomalous diffusion, the surprising anti-funneling of dark excitons."],"url":"http://arxiv.org/abs/2405.04222v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-07 11:33:49","title":"Iterative Experience Refinement of Software-Developing Agents","abstract":"Autonomous agents powered by large language models (LLMs) show significant potential for achieving high autonomy in various scenarios such as software development. Recent research has shown that LLM agents can leverage past experiences to reduce errors and enhance efficiency. However, the static experience paradigm, reliant on a fixed collection of past experiences acquired heuristically, lacks iterative refinement and thus hampers agents' adaptability. In this paper, we introduce the Iterative Experience Refinement framework, enabling LLM agents to refine experiences iteratively during task execution. We propose two fundamental patterns: the successive pattern, refining based on nearest experiences within a task batch, and the cumulative pattern, acquiring experiences across all previous task batches. Augmented with our heuristic experience elimination, the method prioritizes high-quality and frequently-used experiences, effectively managing the experience space and enhancing efficiency. Extensive experiments show that while the successive pattern may yield superior results, the cumulative pattern provides more stable performance. Moreover, experience elimination facilitates achieving better performance using just 11.54% of a high-quality subset.","sentences":["Autonomous agents powered by large language models (LLMs) show significant potential for achieving high autonomy in various scenarios such as software development.","Recent research has shown that LLM agents can leverage past experiences to reduce errors and enhance efficiency.","However, the static experience paradigm, reliant on a fixed collection of past experiences acquired heuristically, lacks iterative refinement and thus hampers agents' adaptability.","In this paper, we introduce the Iterative Experience Refinement framework, enabling LLM agents to refine experiences iteratively during task execution.","We propose two fundamental patterns: the successive pattern, refining based on nearest experiences within a task batch, and the cumulative pattern, acquiring experiences across all previous task batches.","Augmented with our heuristic experience elimination, the method prioritizes high-quality and frequently-used experiences, effectively managing the experience space and enhancing efficiency.","Extensive experiments show that while the successive pattern may yield superior results, the cumulative pattern provides more stable performance.","Moreover, experience elimination facilitates achieving better performance using just 11.54% of a high-quality subset."],"url":"http://arxiv.org/abs/2405.04219v1","category":"cs.CL"}
{"created":"2024-05-07 11:27:13","title":"NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions","abstract":"Today's classical planners are powerful, but modeling input tasks in formats such as PDDL is tedious and error-prone. In contrast, planning with Large Language Models (LLMs) allows for almost any input text, but offers no guarantees on plan quality or even soundness. In an attempt to merge the best of these two approaches, some work has begun to use LLMs to automate parts of the PDDL creation process. However, these methods still require various degrees of expert input. We present NL2Plan, the first domain-agnostic offline LLM-driven planning system. NL2Plan uses an LLM to incrementally extract the necessary information from a short text prompt before creating a complete PDDL description of both the domain and the problem, which is finally solved by a classical planner. We evaluate NL2Plan on four planning domains and find that it solves 10 out of 15 tasks - a clear improvement over a plain chain-of-thought reasoning LLM approach, which only solves 2 tasks. Moreover, in two out of the five failure cases, instead of returning an invalid plan, NL2Plan reports that it failed to solve the task. In addition to using NL2Plan in end-to-end mode, users can inspect and correct all of its intermediate results, such as the PDDL representation, increasing explainability and making it an assistive tool for PDDL creation.","sentences":["Today's classical planners are powerful, but modeling input tasks in formats such as PDDL is tedious and error-prone.","In contrast, planning with Large Language Models (LLMs) allows for almost any input text, but offers no guarantees on plan quality or even soundness.","In an attempt to merge the best of these two approaches, some work has begun to use LLMs to automate parts of the PDDL creation process.","However, these methods still require various degrees of expert input.","We present NL2Plan, the first domain-agnostic offline LLM-driven planning system.","NL2Plan uses an LLM to incrementally extract the necessary information from a short text prompt before creating a complete PDDL description of both the domain and the problem, which is finally solved by a classical planner.","We evaluate NL2Plan on four planning domains and find that it solves 10 out of 15 tasks - a clear improvement over a plain chain-of-thought reasoning LLM approach, which only solves 2 tasks.","Moreover, in two out of the five failure cases, instead of returning an invalid plan, NL2Plan reports that it failed to solve the task.","In addition to using NL2Plan in end-to-end mode, users can inspect and correct all of its intermediate results, such as the PDDL representation, increasing explainability and making it an assistive tool for PDDL creation."],"url":"http://arxiv.org/abs/2405.04215v1","category":"cs.AI"}
{"created":"2024-05-07 11:24:56","title":"Green Tsetlin Redefining Efficiency in Tsetlin Machine Frameworks","abstract":"Green Tsetlin (GT) is a Tsetlin Machine (TM) framework developed to solve real-world problems using TMs. Several frameworks already exist that provide access to TM implementations. However, these either lack features or have a research-first focus. GT is an easy-to-use framework that aims to lower the complexity and provide a production-ready TM implementation that is great for experienced practitioners and beginners. To this end, GT establishes a clear separation between training and inference. A C++ backend with a Python interface provides competitive training and inference performance, with the option of running in pure Python. It also integrates support for critical components such as exporting trained models, hyper-parameter search, and cross-validation out-of-the-box.","sentences":["Green Tsetlin (GT) is a Tsetlin Machine (TM) framework developed to solve real-world problems using TMs.","Several frameworks already exist that provide access to TM implementations.","However, these either lack features or have a research-first focus.","GT is an easy-to-use framework that aims to lower the complexity and provide a production-ready TM implementation that is great for experienced practitioners and beginners.","To this end, GT establishes a clear separation between training and inference.","A C++ backend with a Python interface provides competitive training and inference performance, with the option of running in pure Python.","It also integrates support for critical components such as exporting trained models, hyper-parameter search, and cross-validation out-of-the-box."],"url":"http://arxiv.org/abs/2405.04212v1","category":"cs.AI"}
{"created":"2024-05-07 11:22:35","title":"Extracting parity-violating gravitational waves from projected tidal force tensor in three dimensions","abstract":"Gravitational waves (GWs) may be produced by various mechanisms in the early universe. In particular, if parity is violated, it may lead to the production of parity-violating GWs. In this paper, we focus on GWs on the scale of the large-scale structure. Since GWs induce tidal deformations of the shape of galaxies, one can extract such GW signals by observing images of galaxies in galaxy surveys. Conventionally the detection of such signals is discussed by considering the three-dimensional power spectra of the $E/B$-modes. Here, we develop a complementary new technique to estimate the contribution of GWs to the tidal force tensor field projected on the celestial sphere, which is a directly observable quantity. We introduce two two-dimensional vector fields constructed by taking the divergence and curl of the projected tidal field in three dimensions. Their auto-correlation functions naturally contain contributions of the scalar-type tidal field. However, we find that the divergence of the curl of the projected tidal field, which is a pseudo-scalar quantity, is free from the scalar contribution and thus enables us to extract GW signals. We also find that we can detect parity-violating signals in the GWs by observing the nonzero cross-correlation between the divergence of the projected tidal field and the curl of it. It roughly corresponds to measuring the cross-power spectrum of E and B-modes, but these are complementary to each other in the sense that our estimator can be naturally defined locally in position space. Finally we present expressions of the correlation functions in the form of Fourier integrals, and discuss the properties of the kernels specific to the GW case, which we call the overlap reduction function, borrowing the terminology used in the pulsar timing array experiments.","sentences":["Gravitational waves (GWs) may be produced by various mechanisms in the early universe.","In particular, if parity is violated, it may lead to the production of parity-violating GWs.","In this paper, we focus on GWs on the scale of the large-scale structure.","Since GWs induce tidal deformations of the shape of galaxies, one can extract such GW signals by observing images of galaxies in galaxy surveys.","Conventionally the detection of such signals is discussed by considering the three-dimensional power spectra of the $E/B$-modes.","Here, we develop a complementary new technique to estimate the contribution of GWs to the tidal force tensor field projected on the celestial sphere, which is a directly observable quantity.","We introduce two two-dimensional vector fields constructed by taking the divergence and curl of the projected tidal field in three dimensions.","Their auto-correlation functions naturally contain contributions of the scalar-type tidal field.","However, we find that the divergence of the curl of the projected tidal field, which is a pseudo-scalar quantity, is free from the scalar contribution and thus enables us to extract GW signals.","We also find that we can detect parity-violating signals in the GWs by observing the nonzero cross-correlation between the divergence of the projected tidal field and the curl of it.","It roughly corresponds to measuring the cross-power spectrum of E and B-modes, but these are complementary to each other in the sense that our estimator can be naturally defined locally in position space.","Finally we present expressions of the correlation functions in the form of Fourier integrals, and discuss the properties of the kernels specific to the GW case, which we call the overlap reduction function, borrowing the terminology used in the pulsar timing array experiments."],"url":"http://arxiv.org/abs/2405.04210v1","category":"astro-ph.CO"}
{"created":"2024-05-07 11:20:37","title":"On High-Dimensional Twin-Field Quantum Key Distribution","abstract":"Twin-Field Quantum Key Distribution (QKD) is a QKD protocol that uses single-photon interference to perform QKD over long distances. QKD protocols that encode information using high-dimensional quantum states can benefit from increased key rates and higher noise resilience. We define the essence of Twin-Field QKD and explore its generalization to higher dimensions. Further, we show that, ultimately, the Twin-Field protocol cannot be generalized to higher dimensions in accordance with our definition.","sentences":["Twin-Field Quantum Key Distribution (QKD) is a QKD protocol that uses single-photon interference to perform QKD over long distances.","QKD protocols that encode information using high-dimensional quantum states can benefit from increased key rates and higher noise resilience.","We define the essence of Twin-Field QKD and explore its generalization to higher dimensions.","Further, we show that, ultimately, the Twin-Field protocol cannot be generalized to higher dimensions in accordance with our definition."],"url":"http://arxiv.org/abs/2405.04207v1","category":"quant-ph"}
{"created":"2024-05-07 11:20:10","title":"NOVA: NoC-based Vector Unit for Mapping Attention Layers on a CNN Accelerator","abstract":"Attention mechanisms are becoming increasingly popular, being used in neural network models in multiple domains such as natural language processing (NLP) and vision applications, especially at the edge. However, attention layers are difficult to map onto existing neuro accelerators since they have a much higher density of non-linear operations, which lead to inefficient utilization of today's vector units. This work introduces NOVA, a NoC-based Vector Unit that can perform non-linear operations within the NoC of the accelerators, and can be overlaid onto existing neuro accelerators to map attention layers at the edge. Our results show that the NOVA architecture is up to 37.8x more power-efficient than state-of-the-art hardware approximators when running existing attention-based neural networks.","sentences":["Attention mechanisms are becoming increasingly popular, being used in neural network models in multiple domains such as natural language processing (NLP) and vision applications, especially at the edge.","However, attention layers are difficult to map onto existing neuro accelerators since they have a much higher density of non-linear operations, which lead to inefficient utilization of today's vector units.","This work introduces NOVA, a NoC-based Vector Unit that can perform non-linear operations within the NoC of the accelerators, and can be overlaid onto existing neuro accelerators to map attention layers at the edge.","Our results show that the NOVA architecture is up to 37.8x more power-efficient than state-of-the-art hardware approximators when running existing attention-based neural networks."],"url":"http://arxiv.org/abs/2405.04206v1","category":"cs.AR"}
{"created":"2024-05-07 11:20:04","title":"Darboux's Theorem, Lie series and the standardization of the Salerno and Ablowitz-Ladik models","abstract":"In the framework of nonlinear Hamiltonian lattices, we revisit the proof of Moser-Darboux's Theorem, in order to present a general scheme for its constructive applicability to Hamiltonian models with non-standard symplectic structures. We take as a guiding example the Salerno and Ablowitz-Ladik (AL) models: we justify the form of a well-known change of coordinates which is adapted to the Gauge symmetry, by showing that it comes out in a natural way within the general strategy outlined in the proof. Moreover, the full or truncated Lie-series technique in the extended phase-space is used to transform the Salerno model, at leading orders in the Darboux coordinates: thus the dNLS Hamiltonian turns out to be a normal form of the Salerno and AL models; as a byproduct we also get estimates of the dynamics of these models by means of dNLS one. We also stress that, once it is cast into the perturbative approach, the method allows to deal with the cases where the explicit trasformation is not known, or even worse it is not writable in terms of elementary functions.","sentences":["In the framework of nonlinear Hamiltonian lattices, we revisit the proof of Moser-Darboux's Theorem, in order to present a general scheme for its constructive applicability to Hamiltonian models with non-standard symplectic structures.","We take as a guiding example the Salerno and Ablowitz-Ladik (AL) models: we justify the form of a well-known change of coordinates which is adapted to the Gauge symmetry, by showing that it comes out in a natural way within the general strategy outlined in the proof.","Moreover, the full or truncated Lie-series technique in the extended phase-space is used to transform the Salerno model, at leading orders in the Darboux coordinates: thus the dNLS Hamiltonian turns out to be a normal form of the Salerno and AL models; as a byproduct we also get estimates of the dynamics of these models by means of dNLS one.","We also stress that, once it is cast into the perturbative approach, the method allows to deal with the cases where the explicit trasformation is not known, or even worse it is not writable in terms of elementary functions."],"url":"http://arxiv.org/abs/2405.04205v1","category":"math-ph"}
{"created":"2024-05-07 11:15:12","title":"Asymptotics of the partition function for beta-ensembles at high temperature","abstract":"We consider a model for a gas of $N$ confined particles interacting via a two-body logarithmic interaction, namely the real $\\beta$-ensembles. We are interested in the regime where the inverse temperature scales as $N\\beta=2P$ with $P$ a fixed positive parameter; this is called the high-temperature regime. The confining potential is of the form $x^2+\\phi$ with bounded smooth function $\\phi$. We establish for this model, the existence of a large-$N$ asymptotic expansion for the associated partition function. We also prove the existence of a large-$N$ asymptotic expansion of linear statistics for general confining potentials. Our method is based on the analysis of the loop equations. Finally, we establish a continuity result for the equilibrium density with respect to the potential dependence.","sentences":["We consider a model for a gas of $N$ confined particles interacting via a two-body logarithmic interaction, namely the real $\\beta$-ensembles.","We are interested in the regime where the inverse temperature scales as $N\\beta=2P$ with $P$ a fixed positive parameter; this is called the high-temperature regime.","The confining potential is of the form $x^2+\\phi$ with bounded smooth function $\\phi$. We establish for this model, the existence of a large-$N$ asymptotic expansion for the associated partition function.","We also prove the existence of a large-$N$ asymptotic expansion of linear statistics for general confining potentials.","Our method is based on the analysis of the loop equations.","Finally, we establish a continuity result for the equilibrium density with respect to the potential dependence."],"url":"http://arxiv.org/abs/2405.04199v1","category":"math.PR"}
{"created":"2024-05-07 11:13:17","title":"Enhancing Physical Layer Communication Security through Generative AI with Mixture of Experts","abstract":"AI technologies have become more widely adopted in wireless communications. As an emerging type of AI technologies, the generative artificial intelligence (GAI) gains lots of attention in communication security. Due to its powerful learning ability, GAI models have demonstrated superiority over conventional AI methods. However, GAI still has several limitations, including high computational complexity and limited adaptability. Mixture of Experts (MoE), which uses multiple expert models for prediction through a gate mechanism, proposes possible solutions. Firstly, we review GAI model's applications in physical layer communication security, discuss limitations, and explore how MoE can help GAI overcome these limitations. Furthermore, we propose an MoE-enabled GAI framework for network optimization problems for communication security. To demonstrate the framework's effectiveness, we provide a case study in a cooperative friendly jamming scenario. The experimental results show that the MoE-enabled framework effectively assists the GAI algorithm, solves its limitations, and enhances communication security.","sentences":["AI technologies have become more widely adopted in wireless communications.","As an emerging type of AI technologies, the generative artificial intelligence (GAI) gains lots of attention in communication security.","Due to its powerful learning ability, GAI models have demonstrated superiority over conventional AI methods.","However, GAI still has several limitations, including high computational complexity and limited adaptability.","Mixture of Experts (MoE), which uses multiple expert models for prediction through a gate mechanism, proposes possible solutions.","Firstly, we review GAI model's applications in physical layer communication security, discuss limitations, and explore how MoE can help GAI overcome these limitations.","Furthermore, we propose an MoE-enabled GAI framework for network optimization problems for communication security.","To demonstrate the framework's effectiveness, we provide a case study in a cooperative friendly jamming scenario.","The experimental results show that the MoE-enabled framework effectively assists the GAI algorithm, solves its limitations, and enhances communication security."],"url":"http://arxiv.org/abs/2405.04198v1","category":"cs.CR"}
{"created":"2024-05-07 11:10:01","title":"Foreground biases in strong gravitational lensing","abstract":"Strong gravitational lensing is a competitive tool to probe the dark matter and energy content of the universe. However, significant uncertainties can arise from the choice of lens model, and in particular the parameterisation of the line of sight. In this work, we consider the consequences of ignoring the contribution of foreground perturbers in lens modelling. We derive the explicit form of the degeneracy between the foreground shear and the ellipticity of a power law lens, which renders both quantities effectively unmeasurable from strong lensing observables. Nonetheless, we demonstrate that this degeneracy does not affect measurements of the Einstein radius. Foreground tidal effects are also not expected to bias the slope of the potential, and further that any biases in this slope should not affect the recovery of the Hubble constant. The foreground convergence term adds an additional uncertainty to the measurement of $H_0$, and we show that this uncertainty will be on the order of $1\\%$ for lensing systems located along random lines of sight. There is evidence to indicate that the probability of strong lensing is higher towards overdense lines of sight, and this could result in a small systematic bias towards overestimations of $H_0$.","sentences":["Strong gravitational lensing is a competitive tool to probe the dark matter and energy content of the universe.","However, significant uncertainties can arise from the choice of lens model, and in particular the parameterisation of the line of sight.","In this work, we consider the consequences of ignoring the contribution of foreground perturbers in lens modelling.","We derive the explicit form of the degeneracy between the foreground shear and the ellipticity of a power law lens, which renders both quantities effectively unmeasurable from strong lensing observables.","Nonetheless, we demonstrate that this degeneracy does not affect measurements of the Einstein radius.","Foreground tidal effects are also not expected to bias the slope of the potential, and further that any biases in this slope should not affect the recovery of the Hubble constant.","The foreground convergence term adds an additional uncertainty to the measurement of $H_0$, and we show that this uncertainty will be on the order of $1\\%$ for lensing systems located along random lines of sight.","There is evidence to indicate that the probability of strong lensing is higher towards overdense lines of sight, and this could result in a small systematic bias towards overestimations of $H_0$."],"url":"http://arxiv.org/abs/2405.04194v1","category":"astro-ph.CO"}
{"created":"2024-05-07 11:05:52","title":"A generalized ordinal quasi-symmetry model and its separability for analyzing multi-way tables","abstract":"This paper addresses the challenge of modeling multi-way contingency tables for matched set data with ordinal categories. Although the complete symmetry and marginal homogeneity models are well established, they may not always provide a satisfactory fit to the data. To address this issue, we propose a generalized ordinal quasi-symmetry model that offers increased flexibility when the complete symmetry model fails to capture the underlying structure. We investigate the properties of this new model and provide an information-theoretic interpretation, elucidating its relationship to the ordinal quasi-symmetry model. Moreover, we revisit Agresti's findings and present a new necessary and sufficient condition for the complete symmetry model, proving that the proposed model and the marginal moment equality model are separable hypotheses. The separability of the proposed model and marginal moment equality model is a significant development in the analysis of multi-way contingency tables. It enables researchers to examine the symmetry structure in the data with greater precision, providing a more thorough understanding of the underlying patterns. This powerful framework equips researchers with the necessary tools to explore the complexities of ordinal variable relationships in matched set data, paving the way for new discoveries and insights.","sentences":["This paper addresses the challenge of modeling multi-way contingency tables for matched set data with ordinal categories.","Although the complete symmetry and marginal homogeneity models are well established, they may not always provide a satisfactory fit to the data.","To address this issue, we propose a generalized ordinal quasi-symmetry model that offers increased flexibility when the complete symmetry model fails to capture the underlying structure.","We investigate the properties of this new model and provide an information-theoretic interpretation, elucidating its relationship to the ordinal quasi-symmetry model.","Moreover, we revisit Agresti's findings and present a new necessary and sufficient condition for the complete symmetry model, proving that the proposed model and the marginal moment equality model are separable hypotheses.","The separability of the proposed model and marginal moment equality model is a significant development in the analysis of multi-way contingency tables.","It enables researchers to examine the symmetry structure in the data with greater precision, providing a more thorough understanding of the underlying patterns.","This powerful framework equips researchers with the necessary tools to explore the complexities of ordinal variable relationships in matched set data, paving the way for new discoveries and insights."],"url":"http://arxiv.org/abs/2405.04193v1","category":"stat.ME"}
{"created":"2024-05-07 10:53:20","title":"Effective and Robust Adversarial Training against Data and Label Corruptions","abstract":"Corruptions due to data perturbations and label noise are prevalent in the datasets from unreliable sources, which poses significant threats to model training. Despite existing efforts in developing robust models, current learning methods commonly overlook the possible co-existence of both corruptions, limiting the effectiveness and practicability of the model. In this paper, we develop an Effective and Robust Adversarial Training (ERAT) framework to simultaneously handle two types of corruption (i.e., data and label) without prior knowledge of their specifics. We propose a hybrid adversarial training surrounding multiple potential adversarial perturbations, alongside a semi-supervised learning based on class-rebalancing sample selection to enhance the resilience of the model for dual corruption. On the one hand, in the proposed adversarial training, the perturbation generation module learns multiple surrogate malicious data perturbations by taking a DNN model as the victim, while the model is trained to maintain semantic consistency between the original data and the hybrid perturbed data. It is expected to enable the model to cope with unpredictable perturbations in real-world data corruption. On the other hand, a class-rebalancing data selection strategy is designed to fairly differentiate clean labels from noisy labels. Semi-supervised learning is performed accordingly by discarding noisy labels. Extensive experiments demonstrate the superiority of the proposed ERAT framework.","sentences":["Corruptions due to data perturbations and label noise are prevalent in the datasets from unreliable sources, which poses significant threats to model training.","Despite existing efforts in developing robust models, current learning methods commonly overlook the possible co-existence of both corruptions, limiting the effectiveness and practicability of the model.","In this paper, we develop an Effective and Robust Adversarial Training (ERAT) framework to simultaneously handle two types of corruption (i.e., data and label) without prior knowledge of their specifics.","We propose a hybrid adversarial training surrounding multiple potential adversarial perturbations, alongside a semi-supervised learning based on class-rebalancing sample selection to enhance the resilience of the model for dual corruption.","On the one hand, in the proposed adversarial training, the perturbation generation module learns multiple surrogate malicious data perturbations by taking a DNN model as the victim, while the model is trained to maintain semantic consistency between the original data and the hybrid perturbed data.","It is expected to enable the model to cope with unpredictable perturbations in real-world data corruption.","On the other hand, a class-rebalancing data selection strategy is designed to fairly differentiate clean labels from noisy labels.","Semi-supervised learning is performed accordingly by discarding noisy labels.","Extensive experiments demonstrate the superiority of the proposed ERAT framework."],"url":"http://arxiv.org/abs/2405.04191v1","category":"cs.LG"}
{"created":"2024-05-07 10:49:10","title":"Artificial Intelligence-powered fossil shark tooth identification: Unleashing the potential of Convolutional Neural Networks","abstract":"All fields of knowledge are being impacted by Artificial Intelligence. In particular, the Deep Learning paradigm enables the development of data analysis tools that support subject matter experts in a variety of sectors, from physics up to the recognition of ancient languages. Palaeontology is now observing this trend as well. This study explores the capability of Convolutional Neural Networks (CNNs), a particular class of Deep Learning algorithms specifically crafted for computer vision tasks, to classify images of isolated fossil shark teeth gathered from online datasets as well as from the authors$'$ experience on Peruvian Miocene and Italian Pliocene fossil assemblages. The shark taxa that are included in the final, composite dataset (which consists of more than one thousand images) are representative of both extinct and extant genera, namely, Carcharhinus, Carcharias, Carcharocles, Chlamydoselachus, Cosmopolitodus, Galeocerdo, Hemipristis, Notorynchus, Prionace and Squatina. We developed a CNN, named SharkNet-X, specifically tailored on our recognition task, reaching a 5-fold cross validated mean accuracy of 0.85 to identify images containing a single shark tooth. Furthermore, we elaborated a visualization of the features extracted from images using the last dense layer of the CNN, achieved through the application of the clustering technique t-SNE. In addition, in order to understand and explain the behaviour of the CNN while giving a paleontological point of view on the results, we introduced the explainability method SHAP. To the best of our knowledge, this is the first instance in which this method is applied to the field of palaeontology. The main goal of this work is to showcase how Deep Learning techniques can aid in identifying isolated fossil shark teeth, paving the way for developing new information tools for automating the recognition and classification of fossils.","sentences":["All fields of knowledge are being impacted by Artificial Intelligence.","In particular, the Deep Learning paradigm enables the development of data analysis tools that support subject matter experts in a variety of sectors, from physics up to the recognition of ancient languages.","Palaeontology is now observing this trend as well.","This study explores the capability of Convolutional Neural Networks (CNNs), a particular class of Deep Learning algorithms specifically crafted for computer vision tasks, to classify images of isolated fossil shark teeth gathered from online datasets as well as from the authors$'$ experience on Peruvian Miocene and Italian Pliocene fossil assemblages.","The shark taxa that are included in the final, composite dataset (which consists of more than one thousand images) are representative of both extinct and extant genera, namely, Carcharhinus, Carcharias, Carcharocles, Chlamydoselachus, Cosmopolitodus, Galeocerdo, Hemipristis, Notorynchus, Prionace and Squatina.","We developed a CNN, named SharkNet-X, specifically tailored on our recognition task, reaching a 5-fold cross validated mean accuracy of 0.85 to identify images containing a single shark tooth.","Furthermore, we elaborated a visualization of the features extracted from images using the last dense layer of the CNN, achieved through the application of the clustering technique t-SNE.","In addition, in order to understand and explain the behaviour of the CNN while giving a paleontological point of view on the results, we introduced the explainability method SHAP.","To the best of our knowledge, this is the first instance in which this method is applied to the field of palaeontology.","The main goal of this work is to showcase how Deep Learning techniques can aid in identifying isolated fossil shark teeth, paving the way for developing new information tools for automating the recognition and classification of fossils."],"url":"http://arxiv.org/abs/2405.04189v1","category":"cs.CV"}
{"created":"2024-05-07 10:48:37","title":"Behavioral Manifolds: Representing the Landscape of Grasp Affordances in the Relative Pose Space","abstract":"The use of machine learning to investigate grasp affordances has received extensive attention over the past several decades. The existing literature provides a robust basis to build upon, though a number of aspects may be improved. Results commonly work in terms of grasp configuration, with little consideration for the manner in which the grasp may be (re-)produced from a reachability and trajectory planning perspective. In addition, the majority of existing learning approaches focus of producing a single viable grasp, offering little transparency on how the result was reached, or insights on its robustness. We propose a different perspective on grasp affordance learning, explicitly accounting for grasp synthesis; that is, the manner in which manipulator kinematics are used to allow materialization of grasps. The approach allows to explicitly map the grasp policy space in terms of generated grasp types and associated grasp quality. Results of numerical simulations illustrate merit of the method and highlight the manner in which it may promote a greater degree of explainability for otherwise intransparent reinforcement processes.","sentences":["The use of machine learning to investigate grasp affordances has received extensive attention over the past several decades.","The existing literature provides a robust basis to build upon, though a number of aspects may be improved.","Results commonly work in terms of grasp configuration, with little consideration for the manner in which the grasp may be (re-)produced from a reachability and trajectory planning perspective.","In addition, the majority of existing learning approaches focus of producing a single viable grasp, offering little transparency on how the result was reached, or insights on its robustness.","We propose a different perspective on grasp affordance learning, explicitly accounting for grasp synthesis; that is, the manner in which manipulator kinematics are used to allow materialization of grasps.","The approach allows to explicitly map the grasp policy space in terms of generated grasp types and associated grasp quality.","Results of numerical simulations illustrate merit of the method and highlight the manner in which it may promote a greater degree of explainability for otherwise intransparent reinforcement processes."],"url":"http://arxiv.org/abs/2405.04188v1","category":"cs.RO"}
{"created":"2024-05-07 10:46:20","title":"Detectability of eccentric binary black holes with PyCBC and cWB pipelines during the third observing run of LIGO-Virgo-KAGRA","abstract":"Detecting binary black hole (BBH) mergers with quantifiable orbital eccentricity would confirm the existence of a dynamical formation channel for these binaries. The current state-of-the-art gravitational wave searches of LIGO-Virgo-KAGRA strain data focus more on quasicircular mergers due to increased dimensionality and lack of efficient eccentric waveform models. In this work, we compare the sensitivities of two search pipelines, the matched filter-based \\texttt{PyCBC} and the unmodelled coherent Wave Burst (\\texttt{cWB}) algorithms towards the spinning eccentric BBH mergers, using a multipolar nonprecessing-spin eccentric signal model, \\texttt{SEOBNRv4EHM}. Our findings show that neglecting eccentricity leads to missed opportunities for detecting eccentric BBH mergers, with \\texttt{PyCBC} exhibiting a $10-20\\, \\%$ sensitivity loss for eccentricities exceeding $0.2$ defined at $10$ Hz. In contrast, \\texttt{cWB} is resilient, with a $10\\, \\%$ sensitivity increase for heavier ($\\mathcal{M} \\ge 30 \\, \\text{M}_{\\odot}$) eccentric BBH mergers, but is significantly less sensitive than \\texttt{PyCBC} for lighter BBH mergers. Our fitting factor study confirmed that neglecting eccentricity biases the estimation of chirp mass, mass ratio, and effective spin parameter, skewing our understanding of astrophysical BBH populations, fundamental physics, and precision cosmology. Our results demonstrate that the current search pipelines are not sufficiently sensitive to eccentric BBH mergers, necessitating the development of a dedicated matched-filter search for these binaries. Whereas, burst searches should be optimized to detect lower chirp mass BBH mergers as eccentricity does not affect their search sensitivity significantly.","sentences":["Detecting binary black hole (BBH) mergers with quantifiable orbital eccentricity would confirm the existence of a dynamical formation channel for these binaries.","The current state-of-the-art gravitational wave searches of LIGO-Virgo-KAGRA strain data focus more on quasicircular mergers due to increased dimensionality and lack of efficient eccentric waveform models.","In this work, we compare the sensitivities of two search pipelines, the matched filter-based \\texttt{PyCBC} and the unmodelled coherent Wave Burst (\\texttt{cWB}) algorithms towards the spinning eccentric BBH mergers, using a multipolar nonprecessing-spin eccentric signal model, \\texttt{SEOBNRv4EHM}.","Our findings show that neglecting eccentricity leads to missed opportunities for detecting eccentric BBH mergers, with \\texttt{PyCBC} exhibiting a $10-20\\, \\%$ sensitivity loss for eccentricities exceeding $0.2$ defined at $10$ Hz.","In contrast, \\texttt{cWB} is resilient, with a $10\\, \\%$ sensitivity increase for heavier ($\\mathcal{M} \\ge 30 \\, \\text{M}_{\\odot}$) eccentric BBH mergers, but is significantly less sensitive than \\texttt{PyCBC} for lighter BBH mergers.","Our fitting factor study confirmed that neglecting eccentricity biases the estimation of chirp mass, mass ratio, and effective spin parameter, skewing our understanding of astrophysical BBH populations, fundamental physics, and precision cosmology.","Our results demonstrate that the current search pipelines are not sufficiently sensitive to eccentric BBH mergers, necessitating the development of a dedicated matched-filter search for these binaries.","Whereas, burst searches should be optimized to detect lower chirp mass BBH mergers as eccentricity does not affect their search sensitivity significantly."],"url":"http://arxiv.org/abs/2405.04186v1","category":"gr-qc"}
{"created":"2024-05-07 10:45:52","title":"Research on signalized intersection mixed traffic flow platoon control method considering Backward-looking effect","abstract":"Connected and Autonomous Vehicles (CAVs) technology facilitates the advancement of intelligent transportation. However, intelligent control techniques for mixed traffic flow at signalized intersections involving both CAVs and Human-Driven Vehicles (HDVs) require further investigation into the impact of backward-looking effect. This paper proposes the concept of 1+n+1 mixed platoon considering the backward-looking effect, consisting of one leading CAV, n following HDVs, and one trailing CAV. The leading and trailing CAVs collectively guide the movement of intermediate HDVs at intersections, forming an optimal control framework for platoon-based CAVs at signalized intersections. Initially, a linearized dynamic model for the 1+n+1 mixed platoon is established and compared with a benchmark model focusing solely on controlling the lead vehicle. Subsequently, constraints are formulated for the optimal control framework, aiming to enhance overall intersection traffic efficiency and fuel economy by directly controlling the leading and trailing CAVs in the platoon. Finally, extensive numerical simulations compare vehicle throughput and fuel consumption at signalized intersections under different mixed platoon control methods, validating that considering both front and backward-looking effects in the mixed platoon control method outperforms traditional methods focusing solely on the lead CAV.","sentences":["Connected and Autonomous Vehicles (CAVs) technology facilitates the advancement of intelligent transportation.","However, intelligent control techniques for mixed traffic flow at signalized intersections involving both CAVs and Human-Driven Vehicles (HDVs) require further investigation into the impact of backward-looking effect.","This paper proposes the concept of 1+n+1 mixed platoon considering the backward-looking effect, consisting of one leading CAV, n following HDVs, and one trailing CAV.","The leading and trailing CAVs collectively guide the movement of intermediate HDVs at intersections, forming an optimal control framework for platoon-based CAVs at signalized intersections.","Initially, a linearized dynamic model for the 1+n+1 mixed platoon is established and compared with a benchmark model focusing solely on controlling the lead vehicle.","Subsequently, constraints are formulated for the optimal control framework, aiming to enhance overall intersection traffic efficiency and fuel economy by directly controlling the leading and trailing CAVs in the platoon.","Finally, extensive numerical simulations compare vehicle throughput and fuel consumption at signalized intersections under different mixed platoon control methods, validating that considering both front and backward-looking effects in the mixed platoon control method outperforms traditional methods focusing solely on the lead CAV."],"url":"http://arxiv.org/abs/2405.04185v1","category":"physics.app-ph"}
{"created":"2024-05-07 10:39:58","title":"Super-suppression of long wavelength phonons in constricted nanoporous geometries","abstract":"In a typical semiconductor material, the majority of heat is carried by long wavelength, long mean-free-path phonons. Nanostructuring strategies to reduce thermal conductivity, a promising direction in the field of thermoelectrics, place scattering centers of size and spatial separation comparable to the mean-free-paths of the dominant phonons to selectively scatter them. The resultant thermal conductivity is in most cases well predicted using Matthiessens rule. In general, however, long wavelength phonons are not as effectively scattered as the rest of the phonon spectrum. In this work, using large-scale Molecular Dynamics simulations, Non-Equilibrium Greens Function simulations, and Monte Carlo simulations, we show that specific nanoporous geometries, which create narrow constrictions in the passage of phonons, lead to anticorrelated heat currents in the phonon spectrum. This results in super-suppression of long-wavelength phonons due to heat trapping, and reductions in the thermal conductivity well below what is predicted by Matthiessens rule.","sentences":["In a typical semiconductor material, the majority of heat is carried by long wavelength, long mean-free-path phonons.","Nanostructuring strategies to reduce thermal conductivity, a promising direction in the field of thermoelectrics, place scattering centers of size and spatial separation comparable to the mean-free-paths of the dominant phonons to selectively scatter them.","The resultant thermal conductivity is in most cases well predicted using Matthiessens rule.","In general, however, long wavelength phonons are not as effectively scattered as the rest of the phonon spectrum.","In this work, using large-scale Molecular Dynamics simulations, Non-Equilibrium Greens Function simulations, and Monte Carlo simulations, we show that specific nanoporous geometries, which create narrow constrictions in the passage of phonons, lead to anticorrelated heat currents in the phonon spectrum.","This results in super-suppression of long-wavelength phonons due to heat trapping, and reductions in the thermal conductivity well below what is predicted by Matthiessens rule."],"url":"http://arxiv.org/abs/2405.04183v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-07 10:39:19","title":"Detecting music deepfakes is easy but actually hard","abstract":"In the face of a new era of generative models, the detection of artificially generated content has become a matter of utmost importance. The ability to create credible minute-long music deepfakes in a few seconds on user-friendly platforms poses a real threat of fraud on streaming services and unfair competition to human artists. This paper demonstrates the possibility (and surprising ease) of training classifiers on datasets comprising real audio and fake reconstructions, achieving a convincing accuracy of 99.8%. To our knowledge, this marks the first publication of a music deepfake detector, a tool that will help in the regulation of music forgery. Nevertheless, informed by decades of literature on forgery detection in other fields, we stress that a good test score is not the end of the story. We step back from the straightforward ML framework and expose many facets that could be problematic with such a deployed detector: calibration, robustness to audio manipulation, generalisation to unseen models, interpretability and possibility for recourse. This second part acts as a position for future research steps in the field and a caveat to a flourishing market of fake content checkers.","sentences":["In the face of a new era of generative models, the detection of artificially generated content has become a matter of utmost importance.","The ability to create credible minute-long music deepfakes in a few seconds on user-friendly platforms poses a real threat of fraud on streaming services and unfair competition to human artists.","This paper demonstrates the possibility (and surprising ease) of training classifiers on datasets comprising real audio and fake reconstructions, achieving a convincing accuracy of 99.8%.","To our knowledge, this marks the first publication of a music deepfake detector, a tool that will help in the regulation of music forgery.","Nevertheless, informed by decades of literature on forgery detection in other fields, we stress that a good test score is not the end of the story.","We step back from the straightforward ML framework and expose many facets that could be problematic with such a deployed detector: calibration, robustness to audio manipulation, generalisation to unseen models, interpretability and possibility for recourse.","This second part acts as a position for future research steps in the field and a caveat to a flourishing market of fake content checkers."],"url":"http://arxiv.org/abs/2405.04181v1","category":"cs.SD"}
{"created":"2024-05-07 10:39:14","title":"Sora Detector: A Unified Hallucination Detection for Large Text-to-Video Models","abstract":"The rapid advancement in text-to-video (T2V) generative models has enabled the synthesis of high-fidelity video content guided by textual descriptions. Despite this significant progress, these models are often susceptible to hallucination, generating contents that contradict the input text, which poses a challenge to their reliability and practical deployment. To address this critical issue, we introduce the SoraDetector, a novel unified framework designed to detect hallucinations across diverse large T2V models, including the cutting-edge Sora model. Our framework is built upon a comprehensive analysis of hallucination phenomena, categorizing them based on their manifestation in the video content. Leveraging the state-of-the-art keyframe extraction techniques and multimodal large language models, SoraDetector first evaluates the consistency between extracted video content summary and textual prompts, then constructs static and dynamic knowledge graphs (KGs) from frames to detect hallucination both in single frames and across frames. Sora Detector provides a robust and quantifiable measure of consistency, static and dynamic hallucination. In addition, we have developed the Sora Detector Agent to automate the hallucination detection process and generate a complete video quality report for each input video. Lastly, we present a novel meta-evaluation benchmark, T2VHaluBench, meticulously crafted to facilitate the evaluation of advancements in T2V hallucination detection. Through extensive experiments on videos generated by Sora and other large T2V models, we demonstrate the efficacy of our approach in accurately detecting hallucinations. The code and dataset can be accessed via GitHub.","sentences":["The rapid advancement in text-to-video (T2V) generative models has enabled the synthesis of high-fidelity video content guided by textual descriptions.","Despite this significant progress, these models are often susceptible to hallucination, generating contents that contradict the input text, which poses a challenge to their reliability and practical deployment.","To address this critical issue, we introduce the SoraDetector, a novel unified framework designed to detect hallucinations across diverse large T2V models, including the cutting-edge Sora model.","Our framework is built upon a comprehensive analysis of hallucination phenomena, categorizing them based on their manifestation in the video content.","Leveraging the state-of-the-art keyframe extraction techniques and multimodal large language models, SoraDetector first evaluates the consistency between extracted video content summary and textual prompts, then constructs static and dynamic knowledge graphs (KGs) from frames to detect hallucination both in single frames and across frames.","Sora Detector provides a robust and quantifiable measure of consistency, static and dynamic hallucination.","In addition, we have developed the Sora Detector Agent to automate the hallucination detection process and generate a complete video quality report for each input video.","Lastly, we present a novel meta-evaluation benchmark, T2VHaluBench, meticulously crafted to facilitate the evaluation of advancements in T2V hallucination detection.","Through extensive experiments on videos generated by Sora and other large T2V models, we demonstrate the efficacy of our approach in accurately detecting hallucinations.","The code and dataset can be accessed via GitHub."],"url":"http://arxiv.org/abs/2405.04180v1","category":"cs.LG"}
{"created":"2024-05-07 10:35:52","title":"A fixed point results of Kannan-type for multi-valued mapping on fuzzy metric spaces","abstract":"We prove a Kannan-type fixed point theorem for multi-valued mappings on G-complete fuzzy metric spaces. The proof uses the Hausdorff fuzzy metric space which was introduced by Rodriguez-Lopez and Romaguera [19].","sentences":["We prove a Kannan-type fixed point theorem for multi-valued mappings on G-complete fuzzy metric spaces.","The proof uses the Hausdorff fuzzy metric space which was introduced by Rodriguez-Lopez and Romaguera [19]."],"url":"http://arxiv.org/abs/2405.04179v1","category":"math.GN"}
{"created":"2024-05-07 10:21:23","title":"Topicwise Separable Sentence Retrieval for Medical Report Generation","abstract":"Automated radiology reporting holds immense clinical potential in alleviating the burdensome workload of radiologists and mitigating diagnostic bias. Recently, retrieval-based report generation methods have garnered increasing attention due to their inherent advantages in terms of the quality and consistency of generated reports. However, due to the long-tail distribution of the training data, these models tend to learn frequently occurring sentences and topics, overlooking the rare topics. Regrettably, in many cases, the descriptions of rare topics often indicate critical findings that should be mentioned in the report. To address this problem, we introduce a Topicwise Separable Sentence Retrieval (Teaser) for medical report generation. To ensure comprehensive learning of both common and rare topics, we categorize queries into common and rare types to learn differentiated topics, and then propose Topic Contrastive Loss to effectively align topics and queries in the latent space. Moreover, we integrate an Abstractor module following the extraction of visual features, which aids the topic decoder in gaining a deeper understanding of the visual observational intent. Experiments on the MIMIC-CXR and IU X-ray datasets demonstrate that Teaser surpasses state-of-the-art models, while also validating its capability to effectively represent rare topics and establish more dependable correspondences between queries and topics.","sentences":["Automated radiology reporting holds immense clinical potential in alleviating the burdensome workload of radiologists and mitigating diagnostic bias.","Recently, retrieval-based report generation methods have garnered increasing attention due to their inherent advantages in terms of the quality and consistency of generated reports.","However, due to the long-tail distribution of the training data, these models tend to learn frequently occurring sentences and topics, overlooking the rare topics.","Regrettably, in many cases, the descriptions of rare topics often indicate critical findings that should be mentioned in the report.","To address this problem, we introduce a Topicwise Separable Sentence Retrieval (Teaser) for medical report generation.","To ensure comprehensive learning of both common and rare topics, we categorize queries into common and rare types to learn differentiated topics, and then propose Topic Contrastive Loss to effectively align topics and queries in the latent space.","Moreover, we integrate an Abstractor module following the extraction of visual features, which aids the topic decoder in gaining a deeper understanding of the visual observational intent.","Experiments on the MIMIC-CXR and IU X-ray datasets demonstrate that Teaser surpasses state-of-the-art models, while also validating its capability to effectively represent rare topics and establish more dependable correspondences between queries and topics."],"url":"http://arxiv.org/abs/2405.04175v1","category":"cs.CV"}
{"created":"2024-05-07 10:19:18","title":"Gravity Rainbow Effects on Higher Curvature Modification of $R^2$ inflation","abstract":"In this work, we study several extensions of the higher curvature modification of $R^{2}$ inflation in the context of gravity's rainbow. We modify the $(R+R^{2})$ model by adding an $f_{1}R^3$-term, an $f_{2}R^4$-term, and an $f_{3}R^{3/2}$-term to the original model. We calculate the inflationary observables and confront them using the latest observational bounds from Planck 2018 data. We assume the rainbow function of the form $\\tilde{f}=1+\\left(\\frac{H}{M}\\right)^{\\lambda }$ with $\\lambda$ being a rainbow parameter and $M$ a mass-dimensional parameter. We demonstrate that the power spectrum of curvature perturbation relies on the dimensionless coefficient $f_{i},\\,i=1,2,3$, a rainbow parameter $\\lambda$ and a ratio $H/M$. Likewise, the scalar spectral index $n_s$ is affected by both $f_{i}$ and the rainbow parameter. Moreover, the tensor-to-scalar ratio $r$ is solely determined by the rainbow parameter. Interestingly, by ensuring that $n_s$ aligns with the Planck collaboration's findings at the $1\\sigma$ confidence level, the tensor-to-scalar ratio could reach up to $r\\sim 0.01$, which is possibly measurable for detection in forthcoming Stage IV CMB ground experiments and is certainly feasible for future dedicated space missions.","sentences":["In this work, we study several extensions of the higher curvature modification of $R^{2}$ inflation in the context of gravity's rainbow.","We modify the $(R+R^{2})$ model by adding an $f_{1}R^3$-term, an $f_{2}R^4$-term, and an $f_{3}R^{3/2}$-term to the original model.","We calculate the inflationary observables and confront them using the latest observational bounds from Planck 2018 data.","We assume the rainbow function of the form $\\tilde{f}=1+\\left(\\frac{H}{M}\\right)^{\\lambda }$ with $\\lambda$ being a rainbow parameter and $M$ a mass-dimensional parameter.","We demonstrate that the power spectrum of curvature perturbation relies on the dimensionless coefficient $f_{i},\\,i=1,2,3$, a rainbow parameter $\\lambda$ and a ratio $H/M$. Likewise, the scalar spectral index $n_s$ is affected by both $f_{i}$ and the rainbow parameter.","Moreover, the tensor-to-scalar ratio $r$ is solely determined by the rainbow parameter.","Interestingly, by ensuring that $n_s$ aligns with the Planck collaboration's findings at the $1\\sigma$ confidence level, the tensor-to-scalar ratio could reach up to $r\\sim 0.01$, which is possibly measurable for detection in forthcoming Stage IV CMB ground experiments and is certainly feasible for future dedicated space missions."],"url":"http://arxiv.org/abs/2405.04174v1","category":"gr-qc"}
{"created":"2024-05-07 10:14:33","title":"An efficient active-set method with applications to sparse approximations and risk minimization","abstract":"In this paper we present an efficient active-set method for the solution of convex quadratic programming problems with general piecewise-linear terms in the objective, with applications to sparse approximations and risk-minimization. The algorithm is derived by combining a proximal method of multipliers (PMM) with a standard semismooth Newton method (SSN), and is shown to be globally convergent under minimal assumptions. Further local linear (and potentially superlinear) convergence is shown under standard additional conditions. The major computational bottleneck of the proposed approach arises from the solution of the associated SSN linear systems. These are solved using a Krylov-subspace method, accelerated by certain novel general-purpose preconditioners which are shown to be optimal with respect to the proximal penalty parameters. The preconditioners are easy to store and invert, since they exploit the structure of the nonsmooth terms appearing in the problem's objective to significantly reduce their memory requirements. We showcase the efficiency, robustness, and scalability of the proposed solver on a variety of problems arising in risk-averse portfolio selection, $L^1$-regularized partial differential equation constrained optimization, quantile regression, and binary classification via linear support vector machines. We provide computational evidence, on real-world datasets, to demonstrate the ability of the solver to efficiently and competitively handle a diverse set of medium- and large-scale optimization instances.","sentences":["In this paper we present an efficient active-set method for the solution of convex quadratic programming problems with general piecewise-linear terms in the objective, with applications to sparse approximations and risk-minimization.","The algorithm is derived by combining a proximal method of multipliers (PMM) with a standard semismooth Newton method (SSN), and is shown to be globally convergent under minimal assumptions.","Further local linear (and potentially superlinear) convergence is shown under standard additional conditions.","The major computational bottleneck of the proposed approach arises from the solution of the associated SSN linear systems.","These are solved using a Krylov-subspace method, accelerated by certain novel general-purpose preconditioners which are shown to be optimal with respect to the proximal penalty parameters.","The preconditioners are easy to store and invert, since they exploit the structure of the nonsmooth terms appearing in the problem's objective to significantly reduce their memory requirements.","We showcase the efficiency, robustness, and scalability of the proposed solver on a variety of problems arising in risk-averse portfolio selection, $L^1$-regularized partial differential equation constrained optimization, quantile regression, and binary classification via linear support vector machines.","We provide computational evidence, on real-world datasets, to demonstrate the ability of the solver to efficiently and competitively handle a diverse set of medium- and large-scale optimization instances."],"url":"http://arxiv.org/abs/2405.04172v1","category":"math.OC"}
{"created":"2024-05-07 10:11:42","title":"FedStale: leveraging stale client updates in federated learning","abstract":"Federated learning algorithms, such as FedAvg, are negatively affected by data heterogeneity and partial client participation. To mitigate the latter problem, global variance reduction methods, like FedVARP, leverage stale model updates for non-participating clients. These methods are effective under homogeneous client participation. Yet, this paper shows that, when some clients participate much less than others, aggregating updates with different levels of staleness can detrimentally affect the training process. Motivated by this observation, we introduce FedStale, a novel algorithm that updates the global model in each round through a convex combination of \"fresh\" updates from participating clients and \"stale\" updates from non-participating ones. By adjusting the weight in the convex combination, FedStale interpolates between FedAvg, which only uses fresh updates, and FedVARP, which treats fresh and stale updates equally. Our analysis of FedStale convergence yields the following novel findings: i) it integrates and extends previous FedAvg and FedVARP analyses to heterogeneous client participation; ii) it underscores how the least participating client influences convergence error; iii) it provides practical guidelines to best exploit stale updates, showing that their usefulness diminishes as data heterogeneity decreases and participation heterogeneity increases. Extensive experiments featuring diverse levels of client data and participation heterogeneity not only confirm these findings but also show that FedStale outperforms both FedAvg and FedVARP in many settings.","sentences":["Federated learning algorithms, such as FedAvg, are negatively affected by data heterogeneity and partial client participation.","To mitigate the latter problem, global variance reduction methods, like FedVARP, leverage stale model updates for non-participating clients.","These methods are effective under homogeneous client participation.","Yet, this paper shows that, when some clients participate much less than others, aggregating updates with different levels of staleness can detrimentally affect the training process.","Motivated by this observation, we introduce FedStale, a novel algorithm that updates the global model in each round through a convex combination of \"fresh\" updates from participating clients and \"stale\" updates from non-participating ones.","By adjusting the weight in the convex combination, FedStale interpolates between FedAvg, which only uses fresh updates, and FedVARP, which treats fresh and stale updates equally.","Our analysis of FedStale convergence yields the following novel findings: i) it integrates and extends previous FedAvg and FedVARP analyses to heterogeneous client participation; ii) it underscores how the least participating client influences convergence error; iii) it provides practical guidelines to best exploit stale updates, showing that their usefulness diminishes as data heterogeneity decreases and participation heterogeneity increases.","Extensive experiments featuring diverse levels of client data and participation heterogeneity not only confirm these findings but also show that FedStale outperforms both FedAvg and FedVARP in many settings."],"url":"http://arxiv.org/abs/2405.04171v1","category":"cs.LG"}
{"created":"2024-05-07 10:09:41","title":"D-TrAttUnet: Toward Hybrid CNN-Transformer Architecture for Generic and Subtle Segmentation in Medical Images","abstract":"Over the past two decades, machine analysis of medical imaging has advanced rapidly, opening up significant potential for several important medical applications. As complicated diseases increase and the number of cases rises, the role of machine-based imaging analysis has become indispensable. It serves as both a tool and an assistant to medical experts, providing valuable insights and guidance. A particularly challenging task in this area is lesion segmentation, a task that is challenging even for experienced radiologists. The complexity of this task highlights the urgent need for robust machine learning approaches to support medical staff. In response, we present our novel solution: the D-TrAttUnet architecture. This framework is based on the observation that different diseases often target specific organs. Our architecture includes an encoder-decoder structure with a composite Transformer-CNN encoder and dual decoders. The encoder includes two paths: the Transformer path and the Encoders Fusion Module path. The Dual-Decoder configuration uses two identical decoders, each with attention gates. This allows the model to simultaneously segment lesions and organs and integrate their segmentation losses.   To validate our approach, we performed evaluations on the Covid-19 and Bone Metastasis segmentation tasks. We also investigated the adaptability of the model by testing it without the second decoder in the segmentation of glands and nuclei. The results confirmed the superiority of our approach, especially in Covid-19 infections and the segmentation of bone metastases. In addition, the hybrid encoder showed exceptional performance in the segmentation of glands and nuclei, solidifying its role in modern medical image analysis.","sentences":["Over the past two decades, machine analysis of medical imaging has advanced rapidly, opening up significant potential for several important medical applications.","As complicated diseases increase and the number of cases rises, the role of machine-based imaging analysis has become indispensable.","It serves as both a tool and an assistant to medical experts, providing valuable insights and guidance.","A particularly challenging task in this area is lesion segmentation, a task that is challenging even for experienced radiologists.","The complexity of this task highlights the urgent need for robust machine learning approaches to support medical staff.","In response, we present our novel solution: the D-TrAttUnet architecture.","This framework is based on the observation that different diseases often target specific organs.","Our architecture includes an encoder-decoder structure with a composite Transformer-CNN encoder and dual decoders.","The encoder includes two paths: the Transformer path and the Encoders Fusion Module path.","The Dual-Decoder configuration uses two identical decoders, each with attention gates.","This allows the model to simultaneously segment lesions and organs and integrate their segmentation losses.   ","To validate our approach, we performed evaluations on the Covid-19 and Bone Metastasis segmentation tasks.","We also investigated the adaptability of the model by testing it without the second decoder in the segmentation of glands and nuclei.","The results confirmed the superiority of our approach, especially in Covid-19 infections and the segmentation of bone metastases.","In addition, the hybrid encoder showed exceptional performance in the segmentation of glands and nuclei, solidifying its role in modern medical image analysis."],"url":"http://arxiv.org/abs/2405.04169v1","category":"eess.IV"}
{"created":"2024-05-07 10:07:33","title":"Bridging the Synthetic-to-Authentic Gap: Distortion-Guided Unsupervised Domain Adaptation for Blind Image Quality Assessment","abstract":"The annotation of blind image quality assessment (BIQA) is labor-intensive and time-consuming, especially for authentic images. Training on synthetic data is expected to be beneficial, but synthetically trained models often suffer from poor generalization in real domains due to domain gaps. In this work, we make a key observation that introducing more distortion types in the synthetic dataset may not improve or even be harmful to generalizing authentic image quality assessment. To solve this challenge, we propose distortion-guided unsupervised domain adaptation for BIQA (DGQA), a novel framework that leverages adaptive multi-domain selection via prior knowledge from distortion to match the data distribution between the source domains and the target domain, thereby reducing negative transfer from the outlier source domains. Extensive experiments on two cross-domain settings (synthetic distortion to authentic distortion and synthetic distortion to algorithmic distortion) have demonstrated the effectiveness of our proposed DGQA. Besides, DGQA is orthogonal to existing model-based BIQA methods, and can be used in combination with such models to improve performance with less training data.","sentences":["The annotation of blind image quality assessment (BIQA) is labor-intensive and time-consuming, especially for authentic images.","Training on synthetic data is expected to be beneficial, but synthetically trained models often suffer from poor generalization in real domains due to domain gaps.","In this work, we make a key observation that introducing more distortion types in the synthetic dataset may not improve or even be harmful to generalizing authentic image quality assessment.","To solve this challenge, we propose distortion-guided unsupervised domain adaptation for BIQA (DGQA), a novel framework that leverages adaptive multi-domain selection via prior knowledge from distortion to match the data distribution between the source domains and the target domain, thereby reducing negative transfer from the outlier source domains.","Extensive experiments on two cross-domain settings (synthetic distortion to authentic distortion and synthetic distortion to algorithmic distortion) have demonstrated the effectiveness of our proposed DGQA.","Besides, DGQA is orthogonal to existing model-based BIQA methods, and can be used in combination with such models to improve performance with less training data."],"url":"http://arxiv.org/abs/2405.04167v1","category":"cs.CV"}
{"created":"2024-05-07 10:05:36","title":"Structure and Kinematics of Star-forming Elliptical Galaxies in SDSS-MaNGA","abstract":"Using spatially resolved spectroscopy, we investigated the characteristics and different modes of formation of stars in elliptical galaxies. We identified an unusual population of 59 star-forming elliptical (SF-E) galaxies in SDSS-MaNGA, our primary sample. To identify these rare star-forming ellipticals, we combined GSWLC-A2 containing outputs of stellar population synthesis models with morphological results from the deep-learning catalogue and resolved and integrated properties from the MaNGA Pipe3D value-added catalogue. We have also constructed two control samples of star-forming spirals (SF-Sps; 2419 galaxies) and quenched ellipticals (Q-Es; 684 galaxies) to compare with our primary sample of SF-Es. H$\\alpha$ emission line flux of SF-Es is similar to spiral galaxies. The D4000 spectral index indicates that SF-Es have a mixture of old and young stellar populations. Mass-weighted stellar age and metallicity for the SF-Es are lower than the Q-Es. 67\\% of stellar and gas velocity maps of the primary sample show signs of kinematic disturbance. All of these indicate that SF-Es have acquired metal-poor gas through recent mergers or interactions with other galaxies and are forming a new generation of stars. Further, we subdivide our primary sample of SF-Es into four classes based on their $B/T$ and $\\lambda_{re}$. These four classes have their distinct evolutionary history and modes of formation. Based on these results, we suggest that the Hubble diagram does not accurately capture galaxy evolution processes, and we need a revised morphology diagram like the comb morphology diagram to get a complete picture of the galaxy evolution processes.","sentences":["Using spatially resolved spectroscopy, we investigated the characteristics and different modes of formation of stars in elliptical galaxies.","We identified an unusual population of 59 star-forming elliptical (SF-E) galaxies in SDSS-MaNGA, our primary sample.","To identify these rare star-forming ellipticals, we combined GSWLC-A2 containing outputs of stellar population synthesis models with morphological results from the deep-learning catalogue and resolved and integrated properties from the MaNGA Pipe3D value-added catalogue.","We have also constructed two control samples of star-forming spirals (SF-Sps; 2419 galaxies) and quenched ellipticals (Q-Es; 684 galaxies) to compare with our primary sample of SF-Es. H$\\alpha$ emission line flux of SF-Es is similar to spiral galaxies.","The D4000 spectral index indicates that SF-Es have a mixture of old and young stellar populations.","Mass-weighted stellar age and metallicity for the SF-Es are lower than the Q-Es.","67\\% of stellar and gas velocity maps of the primary sample show signs of kinematic disturbance.","All of these indicate that SF-Es have acquired metal-poor gas through recent mergers or interactions with other galaxies and are forming a new generation of stars.","Further, we subdivide our primary sample of SF-Es into four classes based on their $B/T$ and $\\lambda_{re}$. These four classes have their distinct evolutionary history and modes of formation.","Based on these results, we suggest that the Hubble diagram does not accurately capture galaxy evolution processes, and we need a revised morphology diagram like the comb morphology diagram to get a complete picture of the galaxy evolution processes."],"url":"http://arxiv.org/abs/2405.04166v1","category":"astro-ph.GA"}
{"created":"2024-05-07 10:00:00","title":"MEDVOC: Vocabulary Adaptation for Fine-tuning Pre-trained Language Models on Medical Text Summarization","abstract":"This work presents a dynamic vocabulary adaptation strategy, MEDVOC, for fine-tuning pre-trained language models (PLMs) like BertSumAbs, BART, and PEGASUS for improved medical text summarization. In contrast to existing domain adaptation approaches in summarization, MEDVOC treats vocabulary as an optimizable parameter and optimizes the PLM vocabulary based on fragment score conditioned only on the downstream task's reference summaries. Unlike previous works on vocabulary adaptation (limited only to classification tasks), optimizing vocabulary based on summarization tasks requires an extremely costly intermediate fine-tuning step on large summarization datasets. To that end, our novel fragment score-based hyperparameter search very significantly reduces this fine-tuning time -- from 450 days to less than 2 days on average. Furthermore, while previous works on vocabulary adaptation are often primarily tied to single PLMs, MEDVOC is designed to be deployable across multiple PLMs (with varying model vocabulary sizes, pre-training objectives, and model sizes) -- bridging the limited vocabulary overlap between the biomedical literature domain and PLMs. MEDVOC outperforms baselines by 15.74% in terms of Rouge-L in zero-shot setting and shows gains of 17.29% in high Out-Of-Vocabulary (OOV) concentrations. Our human evaluation shows MEDVOC generates more faithful medical summaries (88% compared to 59% in baselines). We make the codebase publicly available at https://github.com/gb-kgp/MEDVOC.","sentences":["This work presents a dynamic vocabulary adaptation strategy, MEDVOC, for fine-tuning pre-trained language models (PLMs) like BertSumAbs, BART, and PEGASUS for improved medical text summarization.","In contrast to existing domain adaptation approaches in summarization, MEDVOC treats vocabulary as an optimizable parameter and optimizes the PLM vocabulary based on fragment score conditioned only on the downstream task's reference summaries.","Unlike previous works on vocabulary adaptation (limited only to classification tasks), optimizing vocabulary based on summarization tasks requires an extremely costly intermediate fine-tuning step on large summarization datasets.","To that end, our novel fragment score-based hyperparameter search very significantly reduces this fine-tuning time -- from 450 days to less than 2 days on average.","Furthermore, while previous works on vocabulary adaptation are often primarily tied to single PLMs, MEDVOC is designed to be deployable across multiple PLMs (with varying model vocabulary sizes, pre-training objectives, and model sizes) -- bridging the limited vocabulary overlap between the biomedical literature domain and PLMs.","MEDVOC outperforms baselines by 15.74% in terms of Rouge-L in zero-shot setting and shows gains of 17.29% in high Out-Of-Vocabulary (OOV) concentrations.","Our human evaluation shows MEDVOC generates more faithful medical summaries (88% compared to 59% in baselines).","We make the codebase publicly available at https://github.com/gb-kgp/MEDVOC."],"url":"http://arxiv.org/abs/2405.04163v1","category":"cs.CL"}
{"created":"2024-05-07 09:58:02","title":"Opportunities for machine learning in scientific discovery","abstract":"Technological advancements have substantially increased computational power and data availability, enabling the application of powerful machine-learning (ML) techniques across various fields. However, our ability to leverage ML methods for scientific discovery, {\\it i.e.} to obtain fundamental and formalized knowledge about natural processes, is still in its infancy. In this review, we explore how the scientific community can increasingly leverage ML techniques to achieve scientific discoveries. We observe that the applicability and opportunity of ML depends strongly on the nature of the problem domain, and whether we have full ({\\it e.g.}, turbulence), partial ({\\it e.g.}, computational biochemistry), or no ({\\it e.g.}, neuroscience) {\\it a-priori} knowledge about the governing equations and physical properties of the system. Although challenges remain, principled use of ML is opening up new avenues for fundamental scientific discoveries. Throughout these diverse fields, there is a theme that ML is enabling researchers to embrace complexity in observational data that was previously intractable to classic analysis and numerical investigations.","sentences":["Technological advancements have substantially increased computational power and data availability, enabling the application of powerful machine-learning (ML) techniques across various fields.","However, our ability to leverage ML methods for scientific discovery, {\\it i.e.} to obtain fundamental and formalized knowledge about natural processes, is still in its infancy.","In this review, we explore how the scientific community can increasingly leverage ML techniques to achieve scientific discoveries.","We observe that the applicability and opportunity of ML depends strongly on the nature of the problem domain, and whether we have full ({\\it e.g.}, turbulence), partial ({\\it e.g.}, computational biochemistry), or no ({\\it e.g.}, neuroscience) {\\it a-priori} knowledge about the governing equations and physical properties of the system.","Although challenges remain, principled use of ML is opening up new avenues for fundamental scientific discoveries.","Throughout these diverse fields, there is a theme that ML is enabling researchers to embrace complexity in observational data that was previously intractable to classic analysis and numerical investigations."],"url":"http://arxiv.org/abs/2405.04161v1","category":"cs.LG"}
{"created":"2024-05-07 09:55:05","title":"A Causal Explainable Guardrails for Large Language Models","abstract":"Large Language Models (LLMs) have shown impressive performance in natural language tasks, but their outputs can exhibit undesirable attributes or biases. Existing methods for steering LLMs towards desired attributes often assume unbiased representations and rely solely on steering prompts. However, the representations learned from pre-training can introduce semantic biases that influence the steering process, leading to suboptimal results. We propose LLMGuardaril, a novel framework that incorporates causal analysis and adversarial learning to obtain unbiased steering representations in LLMs. LLMGuardaril systematically identifies and blocks the confounding effects of biases, enabling the extraction of unbiased steering representations. Additionally, it includes an explainable component that provides insights into the alignment between the generated output and the desired direction. Experiments demonstrate LLMGuardaril's effectiveness in steering LLMs towards desired attributes while mitigating biases. Our work contributes to the development of safe and reliable LLMs that align with desired attributes. We discuss the limitations and future research directions, highlighting the need for ongoing research to address the ethical implications of large language models.","sentences":["Large Language Models (LLMs) have shown impressive performance in natural language tasks, but their outputs can exhibit undesirable attributes or biases.","Existing methods for steering LLMs towards desired attributes often assume unbiased representations and rely solely on steering prompts.","However, the representations learned from pre-training can introduce semantic biases that influence the steering process, leading to suboptimal results.","We propose LLMGuardaril, a novel framework that incorporates causal analysis and adversarial learning to obtain unbiased steering representations in LLMs.","LLMGuardaril systematically identifies and blocks the confounding effects of biases, enabling the extraction of unbiased steering representations.","Additionally, it includes an explainable component that provides insights into the alignment between the generated output and the desired direction.","Experiments demonstrate LLMGuardaril's effectiveness in steering LLMs towards desired attributes while mitigating biases.","Our work contributes to the development of safe and reliable LLMs that align with desired attributes.","We discuss the limitations and future research directions, highlighting the need for ongoing research to address the ethical implications of large language models."],"url":"http://arxiv.org/abs/2405.04160v1","category":"cs.CL"}
{"created":"2024-05-07 09:50:57","title":"How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability","abstract":"Transformer-based language models are treated as black-boxes because of their large number of parameters and complex internal interactions, which is a serious safety concern. Mechanistic Interpretability (MI) intends to reverse-engineer neural network behaviors in terms of human-understandable components. In this work, we focus on understanding how GPT-2 Small performs the task of predicting three-letter acronyms. Previous works in the MI field have focused so far on tasks that predict a single token. To the best of our knowledge, this is the first work that tries to mechanistically understand a behavior involving the prediction of multiple consecutive tokens. We discover that the prediction is performed by a circuit composed of 8 attention heads (~5% of the total heads) which we classified in three groups according to their role. We also demonstrate that these heads concentrate the acronym prediction functionality. In addition, we mechanistically interpret the most relevant heads of the circuit and find out that they use positional information which is propagated via the causal mask mechanism. We expect this work to lay the foundation for understanding more complex behaviors involving multiple-token predictions.","sentences":["Transformer-based language models are treated as black-boxes because of their large number of parameters and complex internal interactions, which is a serious safety concern.","Mechanistic Interpretability (MI) intends to reverse-engineer neural network behaviors in terms of human-understandable components.","In this work, we focus on understanding how GPT-2 Small performs the task of predicting three-letter acronyms.","Previous works in the MI field have focused so far on tasks that predict a single token.","To the best of our knowledge, this is the first work that tries to mechanistically understand a behavior involving the prediction of multiple consecutive tokens.","We discover that the prediction is performed by a circuit composed of 8 attention heads (~5% of the total heads) which we classified in three groups according to their role.","We also demonstrate that these heads concentrate the acronym prediction functionality.","In addition, we mechanistically interpret the most relevant heads of the circuit and find out that they use positional information which is propagated via the causal mask mechanism.","We expect this work to lay the foundation for understanding more complex behaviors involving multiple-token predictions."],"url":"http://arxiv.org/abs/2405.04156v1","category":"cs.LG"}
{"created":"2024-05-07 09:46:05","title":"Thermal convection in a higher velocity gradient and higher temperature gradient fluid","abstract":"We analyse a model for thermal convection in a class of generalized Navier-Stokes equations containing fourth order spatial derivatives of the velocity and of the temperature. The work generalises the isothermal model of A. Musesti. We derive critical Rayleigh and wavenumbers for the onset of convective fluid motion paying careful attention to the variation of coefficients of the highest derivatives. In addition to linear instability theory we include an analysis of fully nonlinear stability theory. The theory analysed possesses a bi-Laplacian term for the velocity field and also for the temperature field. It was pointed out by E. Fried and M. Gurtin that higher order terms represent micro-length effects and these phenomena are very important in flows in microfluidic situations. We introduce temperature into the theory via a Boussinesq approximation where the density of the body force term is allowed to depend upon temperature to account for buoyancy effects which arise due to expansion of the fluid when this is heated. We analyse a meaningful set of boundary conditions which are introduced by Fried and Gurtin as conditions of strong adherence, and these are crucial to understand the effect of the higher order derivatives upon convective motion in a microfluidic scenario where micro-length effects are paramount. The basic steady state is the one of zero velocity, but in contrast to the classical theory the temperature field is nonlinear in the vertical coordinate. This requires care especially dealing with nonlinear theory and also leads to some novel effects.","sentences":["We analyse a model for thermal convection in a class of generalized Navier-Stokes equations containing fourth order spatial derivatives of the velocity and of the temperature.","The work generalises the isothermal model of A. Musesti.","We derive critical Rayleigh and wavenumbers for the onset of convective fluid motion paying careful attention to the variation of coefficients of the highest derivatives.","In addition to linear instability theory we include an analysis of fully nonlinear stability theory.","The theory analysed possesses a bi-Laplacian term for the velocity field and also for the temperature field.","It was pointed out by E. Fried and M. Gurtin that higher order terms represent micro-length effects and these phenomena are very important in flows in microfluidic situations.","We introduce temperature into the theory via a Boussinesq approximation where the density of the body force term is allowed to depend upon temperature to account for buoyancy effects which arise due to expansion of the fluid when this is heated.","We analyse a meaningful set of boundary conditions which are introduced by Fried and Gurtin as conditions of strong adherence, and these are crucial to understand the effect of the higher order derivatives upon convective motion in a microfluidic scenario where micro-length effects are paramount.","The basic steady state is the one of zero velocity, but in contrast to the classical theory the temperature field is nonlinear in the vertical coordinate.","This requires care especially dealing with nonlinear theory and also leads to some novel effects."],"url":"http://arxiv.org/abs/2405.04155v1","category":"physics.flu-dyn"}
{"created":"2024-05-07 09:44:07","title":"On the convergence of zeta functions of prehomogeneous vector spaces","abstract":"We prove a general convergence result for zeta functions of prehomogeneous vector spaces extending results of H. Saito, F. Sato and Yukie. Our analysis points to certain subspaces which yield boundary terms. We study it further in the setup arising from nilpotent orbits. In certain cases we determine the residue at the rightmost pole of the zeta function.","sentences":["We prove a general convergence result for zeta functions of prehomogeneous vector spaces extending results of H. Saito, F. Sato and Yukie.","Our analysis points to certain subspaces which yield boundary terms.","We study it further in the setup arising from nilpotent orbits.","In certain cases we determine the residue at the rightmost pole of the zeta function."],"url":"http://arxiv.org/abs/2405.04153v1","category":"math.NT"}
{"created":"2024-05-07 09:25:39","title":"pFedLVM: A Large Vision Model (LVM)-Driven and Latent Feature-Based Personalized Federated Learning Framework in Autonomous Driving","abstract":"Deep learning-based Autonomous Driving (AD) models often exhibit poor generalization due to data heterogeneity in an ever domain-shifting environment. While Federated Learning (FL) could improve the generalization of an AD model (known as FedAD system), conventional models often struggle with under-fitting as the amount of accumulated training data progressively increases. To address this issue, instead of conventional small models, employing Large Vision Models (LVMs) in FedAD is a viable option for better learning of representations from a vast volume of data. However, implementing LVMs in FedAD introduces three challenges: (I) the extremely high communication overheads associated with transmitting LVMs between participating vehicles and a central server; (II) lack of computing resource to deploy LVMs on each vehicle; (III) the performance drop due to LVM focusing on shared features but overlooking local vehicle characteristics. To overcome these challenges, we propose pFedLVM, a LVM-Driven, Latent Feature-Based Personalized Federated Learning framework. In this approach, the LVM is deployed only on central server, which effectively alleviates the computational burden on individual vehicles. Furthermore, the exchange between central server and vehicles are the learned features rather than the LVM parameters, which significantly reduces communication overhead. In addition, we utilize both shared features from all participating vehicles and individual characteristics from each vehicle to establish a personalized learning mechanism. This enables each vehicle's model to learn features from others while preserving its personalized characteristics, thereby outperforming globally shared models trained in general FL. Extensive experiments demonstrate that pFedLVM outperforms the existing state-of-the-art approaches.","sentences":["Deep learning-based Autonomous Driving (AD) models often exhibit poor generalization due to data heterogeneity in an ever domain-shifting environment.","While Federated Learning (FL) could improve the generalization of an AD model (known as FedAD system), conventional models often struggle with under-fitting as the amount of accumulated training data progressively increases.","To address this issue, instead of conventional small models, employing Large Vision Models (LVMs) in FedAD is a viable option for better learning of representations from a vast volume of data.","However, implementing LVMs in FedAD introduces three challenges: (I) the extremely high communication overheads associated with transmitting LVMs between participating vehicles and a central server; (II) lack of computing resource to deploy LVMs on each vehicle; (III) the performance drop due to LVM focusing on shared features but overlooking local vehicle characteristics.","To overcome these challenges, we propose pFedLVM, a LVM-Driven, Latent Feature-Based Personalized Federated Learning framework.","In this approach, the LVM is deployed only on central server, which effectively alleviates the computational burden on individual vehicles.","Furthermore, the exchange between central server and vehicles are the learned features rather than the LVM parameters, which significantly reduces communication overhead.","In addition, we utilize both shared features from all participating vehicles and individual characteristics from each vehicle to establish a personalized learning mechanism.","This enables each vehicle's model to learn features from others while preserving its personalized characteristics, thereby outperforming globally shared models trained in general FL.","Extensive experiments demonstrate that pFedLVM outperforms the existing state-of-the-art approaches."],"url":"http://arxiv.org/abs/2405.04146v1","category":"cs.RO"}
{"created":"2024-05-07 09:14:07","title":"Element orders in extraspecial groups","abstract":"By using the structure and some properties of extraspecial and generalized/almost extraspecial $p$-groups, we explicitly determine the number of elements of specific orders in such groups. As a consequence, one may find the number of cyclic subgroups of any (generalized/almost) extraspecial group. For a finite group $G$, the ratio of the number of cyclic subgroups to the number of subgroups is called the cyclicity degree of $G$ and is denoted by $cdeg(G)$. We show that the set containing the cyclicity degrees of all finite groups is dense in $[0, 1]$. This is equivalent to giving an affirmative answer to the following question posed by T\\'{o}th and T\\u{a}rn\\u{a}uceanu: ``For every $a\\in [0, 1]$, does there exist a sequence $(G_n)_{n\\geq 1}$ of finite groups such that $\\displaystyle\\lim_{n\\to\\infty} cdeg(G_n)=a$?\". We show that such sequences are formed of finite direct products of extraspecial groups of a specific type.","sentences":["By using the structure and some properties of extraspecial and generalized/almost extraspecial $p$-groups, we explicitly determine the number of elements of specific orders in such groups.","As a consequence, one may find the number of cyclic subgroups of any (generalized/almost) extraspecial group.","For a finite group $G$, the ratio of the number of cyclic subgroups to the number of subgroups is called the cyclicity degree of $G$ and is denoted by $cdeg(G)$. We show that the set containing the cyclicity degrees of all finite groups is dense in $[0, 1]$.","This is equivalent to giving an affirmative answer to the following question posed by T\\'{o}th and T\\u{a}rn\\u{a}uceanu: ``For every $a\\in [0, 1]$, does there exist a sequence $(G_n)_{n\\geq 1}$ of finite groups such that $\\displaystyle\\lim_{n\\to\\infty} cdeg(G_n)=a$?\".","We show that such sequences are formed of finite direct products of extraspecial groups of a specific type."],"url":"http://arxiv.org/abs/2405.04141v1","category":"math.GR"}
{"created":"2024-05-07 09:08:00","title":"GPT-Enabled Cybersecurity Training: A Tailored Approach for Effective Awareness","abstract":"This study explores the limitations of traditional Cybersecurity Awareness and Training (CSAT) programs and proposes an innovative solution using Generative Pre-Trained Transformers (GPT) to address these shortcomings. Traditional approaches lack personalization and adaptability to individual learning styles. To overcome these challenges, the study integrates GPT models to deliver highly tailored and dynamic cybersecurity learning expe-riences. Leveraging natural language processing capabilities, the proposed approach personalizes training modules based on individual trainee pro-files, helping to ensure engagement and effectiveness. An experiment using a GPT model to provide a real-time and adaptive CSAT experience through generating customized training content. The findings have demonstrated a significant improvement over traditional programs, addressing issues of en-gagement, dynamicity, and relevance. GPT-powered CSAT programs offer a scalable and effective solution to enhance cybersecurity awareness, provid-ing personalized training content that better prepares individuals to miti-gate cybersecurity risks in their specific roles within the organization.","sentences":["This study explores the limitations of traditional Cybersecurity Awareness and Training (CSAT) programs and proposes an innovative solution using Generative Pre-Trained Transformers (GPT) to address these shortcomings.","Traditional approaches lack personalization and adaptability to individual learning styles.","To overcome these challenges, the study integrates GPT models to deliver highly tailored and dynamic cybersecurity learning expe-riences.","Leveraging natural language processing capabilities, the proposed approach personalizes training modules based on individual trainee pro-files, helping to ensure engagement and effectiveness.","An experiment using a GPT model to provide a real-time and adaptive CSAT experience through generating customized training content.","The findings have demonstrated a significant improvement over traditional programs, addressing issues of en-gagement, dynamicity, and relevance.","GPT-powered CSAT programs offer a scalable and effective solution to enhance cybersecurity awareness, provid-ing personalized training content that better prepares individuals to miti-gate cybersecurity risks in their specific roles within the organization."],"url":"http://arxiv.org/abs/2405.04138v1","category":"cs.CR"}
{"created":"2024-05-07 09:05:37","title":"Deformed quantum vertex algebra modules associated with braidings","abstract":"We introduce the notion of deformed quantum vertex algebra module associated with a braiding map. We construct two families of braiding maps over the Etingof-Kazhdan quantum vertex algebras associated with the rational $R$-matrices of classical types. We investigate their properties and demonstrate the applications of the corresponding deformed modules to the (generalized) Yangians and reflection algebras.","sentences":["We introduce the notion of deformed quantum vertex algebra module associated with a braiding map.","We construct two families of braiding maps over the Etingof-Kazhdan quantum vertex algebras associated with the rational $R$-matrices of classical types.","We investigate their properties and demonstrate the applications of the corresponding deformed modules to the (generalized) Yangians and reflection algebras."],"url":"http://arxiv.org/abs/2405.04137v1","category":"math.QA"}
{"created":"2024-05-07 09:05:20","title":"Enriched BERT Embeddings for Scholarly Publication Classification","abstract":"With the rapid expansion of academic literature and the proliferation of preprints, researchers face growing challenges in manually organizing and labeling large volumes of articles. The NSLP 2024 FoRC Shared Task I addresses this challenge organized as a competition. The goal is to develop a classifier capable of predicting one of 123 predefined classes from the Open Research Knowledge Graph (ORKG) taxonomy of research fields for a given article.This paper presents our results. Initially, we enrich the dataset (containing English scholarly articles sourced from ORKG and arXiv), then leverage different pre-trained language Models (PLMs), specifically BERT, and explore their efficacy in transfer learning for this downstream task. Our experiments encompass feature-based and fine-tuned transfer learning approaches using diverse PLMs, optimized for scientific tasks, including SciBERT, SciNCL, and SPECTER2. We conduct hyperparameter tuning and investigate the impact of data augmentation from bibliographic databases such as OpenAlex, Semantic Scholar, and Crossref. Our results demonstrate that fine-tuning pre-trained models substantially enhances classification performance, with SPECTER2 emerging as the most accurate model. Moreover, enriching the dataset with additional metadata improves classification outcomes significantly, especially when integrating information from S2AG, OpenAlex and Crossref. Our best-performing approach achieves a weighted F1-score of 0.7415. Overall, our study contributes to the advancement of reliable automated systems for scholarly publication categorization, offering a potential solution to the laborious manual curation process, thereby facilitating researchers in efficiently locating relevant resources.","sentences":["With the rapid expansion of academic literature and the proliferation of preprints, researchers face growing challenges in manually organizing and labeling large volumes of articles.","The NSLP 2024 FoRC Shared Task I addresses this challenge organized as a competition.","The goal is to develop a classifier capable of predicting one of 123 predefined classes from the Open Research Knowledge Graph (ORKG) taxonomy of research fields for a given article.","This paper presents our results.","Initially, we enrich the dataset (containing English scholarly articles sourced from ORKG and arXiv), then leverage different pre-trained language Models (PLMs), specifically BERT, and explore their efficacy in transfer learning for this downstream task.","Our experiments encompass feature-based and fine-tuned transfer learning approaches using diverse PLMs, optimized for scientific tasks, including SciBERT, SciNCL, and SPECTER2.","We conduct hyperparameter tuning and investigate the impact of data augmentation from bibliographic databases such as OpenAlex, Semantic Scholar, and Crossref.","Our results demonstrate that fine-tuning pre-trained models substantially enhances classification performance, with SPECTER2 emerging as the most accurate model.","Moreover, enriching the dataset with additional metadata improves classification outcomes significantly, especially when integrating information from S2AG, OpenAlex and Crossref.","Our best-performing approach achieves a weighted F1-score of 0.7415.","Overall, our study contributes to the advancement of reliable automated systems for scholarly publication categorization, offering a potential solution to the laborious manual curation process, thereby facilitating researchers in efficiently locating relevant resources."],"url":"http://arxiv.org/abs/2405.04136v1","category":"cs.AI"}
{"created":"2024-05-07 09:04:52","title":"In-context Learning for Automated Driving Scenarios","abstract":"One of the key challenges in current Reinforcement Learning (RL)-based Automated Driving (AD) agents is achieving flexible, precise, and human-like behavior cost-effectively. This paper introduces an innovative approach utilizing Large Language Models (LLMs) to intuitively and effectively optimize RL reward functions in a human-centric way. We developed a framework where instructions and dynamic environment descriptions are input into the LLM. The LLM then utilizes this information to assist in generating rewards, thereby steering the behavior of RL agents towards patterns that more closely resemble human driving. The experimental results demonstrate that this approach not only makes RL agents more anthropomorphic but also reaches better performance. Additionally, various strategies for reward-proxy and reward-shaping are investigated, revealing the significant impact of prompt design on shaping an AD vehicle's behavior. These findings offer a promising direction for the development of more advanced and human-like automated driving systems. Our experimental data and source code can be found here.","sentences":["One of the key challenges in current Reinforcement Learning (RL)-based Automated Driving (AD) agents is achieving flexible, precise, and human-like behavior cost-effectively.","This paper introduces an innovative approach utilizing Large Language Models (LLMs) to intuitively and effectively optimize RL reward functions in a human-centric way.","We developed a framework where instructions and dynamic environment descriptions are input into the LLM.","The LLM then utilizes this information to assist in generating rewards, thereby steering the behavior of RL agents towards patterns that more closely resemble human driving.","The experimental results demonstrate that this approach not only makes RL agents more anthropomorphic but also reaches better performance.","Additionally, various strategies for reward-proxy and reward-shaping are investigated, revealing the significant impact of prompt design on shaping an AD vehicle's behavior.","These findings offer a promising direction for the development of more advanced and human-like automated driving systems.","Our experimental data and source code can be found here."],"url":"http://arxiv.org/abs/2405.04135v1","category":"cs.AI"}
{"created":"2024-05-07 09:00:09","title":"Exposing AI-generated Videos: A Benchmark Dataset and a Local-and-Global Temporal Defect Based Detection Method","abstract":"The generative model has made significant advancements in the creation of realistic videos, which causes security issues. However, this emerging risk has not been adequately addressed due to the absence of a benchmark dataset for AI-generated videos. In this paper, we first construct a video dataset using advanced diffusion-based video generation algorithms with various semantic contents. Besides, typical video lossy operations over network transmission are adopted to generate degraded samples. Then, by analyzing local and global temporal defects of current AI-generated videos, a novel detection framework by adaptively learning local motion information and global appearance variation is constructed to expose fake videos. Finally, experiments are conducted to evaluate the generalization and robustness of different spatial and temporal domain detection methods, where the results can serve as the baseline and demonstrate the research challenge for future studies.","sentences":["The generative model has made significant advancements in the creation of realistic videos, which causes security issues.","However, this emerging risk has not been adequately addressed due to the absence of a benchmark dataset for AI-generated videos.","In this paper, we first construct a video dataset using advanced diffusion-based video generation algorithms with various semantic contents.","Besides, typical video lossy operations over network transmission are adopted to generate degraded samples.","Then, by analyzing local and global temporal defects of current AI-generated videos, a novel detection framework by adaptively learning local motion information and global appearance variation is constructed to expose fake videos.","Finally, experiments are conducted to evaluate the generalization and robustness of different spatial and temporal domain detection methods, where the results can serve as the baseline and demonstrate the research challenge for future studies."],"url":"http://arxiv.org/abs/2405.04133v1","category":"cs.CV"}
{"created":"2024-05-07 08:54:38","title":"Cosmological implications of the Weyl geometric gravity theory","abstract":"We consider cosmological implications of the Weyl geometric gravity theory. The basic action of the model is obtained from the simplest conformally invariant gravitational action, constructed, in Weyl geometry, from the square of the Weyl scalar, the strength of the Weyl vector, and a matter term, respectively. The total action is linearized in the Weyl scalar by introducing an auxiliary scalar field. To maintain the conformal invariance of the action the trace condition is imposed on the matter energy-momentum tensor, thus making the matter sector of the action conformally invariant. The field equations are derived by varying the action with respect to the metric tensor, the Weyl vector field, and the scalar field, respectively. We investigate the cosmological implications of the theory, and we obtain first the cosmological evolution equations for a flat, homogeneous and isotropic geometry, described by Friedmann-Lemaitre-Robertson-Walker metric, which generalize the Friedmann equations of standard general relativity. In this context we consider two cosmological models, corresponding to the vacuum state, and to the presence of matter described by a linear barotropic equation of state. In both cases we perform a detailed comparison of the predictions of the theory with the cosmological observational data, and with the standard $\\Lambda$ CDM model. By assuming that the presence of the Weyl geometric effects induce small perturbations in the homogeneous and isotropic cosmological background, and that the anisotropy parameter is small, the equations of the cosmological perturbations due to the presence of the Weyl geometric effects are derived. The time evolution of the metric and matter perturbations are explicitly obtained. So, if Weyl geometric effects are present, the Universe would acquire some anisotropic characteristics, and its geometry will deviate from the standard FLRW one.","sentences":["We consider cosmological implications of the Weyl geometric gravity theory.","The basic action of the model is obtained from the simplest conformally invariant gravitational action, constructed, in Weyl geometry, from the square of the Weyl scalar, the strength of the Weyl vector, and a matter term, respectively.","The total action is linearized in the Weyl scalar by introducing an auxiliary scalar field.","To maintain the conformal invariance of the action the trace condition is imposed on the matter energy-momentum tensor, thus making the matter sector of the action conformally invariant.","The field equations are derived by varying the action with respect to the metric tensor, the Weyl vector field, and the scalar field, respectively.","We investigate the cosmological implications of the theory, and we obtain first the cosmological evolution equations for a flat, homogeneous and isotropic geometry, described by Friedmann-Lemaitre-Robertson-Walker metric, which generalize the Friedmann equations of standard general relativity.","In this context we consider two cosmological models, corresponding to the vacuum state, and to the presence of matter described by a linear barotropic equation of state.","In both cases we perform a detailed comparison of the predictions of the theory with the cosmological observational data, and with the standard $\\Lambda$ CDM model.","By assuming that the presence of the Weyl geometric effects induce small perturbations in the homogeneous and isotropic cosmological background, and that the anisotropy parameter is small, the equations of the cosmological perturbations due to the presence of the Weyl geometric effects are derived.","The time evolution of the metric and matter perturbations are explicitly obtained.","So, if Weyl geometric effects are present, the Universe would acquire some anisotropic characteristics, and its geometry will deviate from the standard FLRW one."],"url":"http://arxiv.org/abs/2405.04129v1","category":"gr-qc"}
{"created":"2024-05-07 08:47:40","title":"Comparative Study of Recurrent Neural Networks for Virtual Analog Audio Effects Modeling","abstract":"Analog electronic circuits are at the core of an important category of musical devices. The nonlinear features of their electronic components give analog musical devices a distinctive timbre and sound quality, making them highly desirable. Artificial neural networks have rapidly gained popularity for the emulation of analog audio effects circuits, particularly recurrent networks. While neural approaches have been successful in accurately modeling distortion circuits, they require architectural improvements that account for parameter conditioning and low latency response. In this article, we explore the application of recent machine learning advancements for virtual analog modeling. We compare State Space models and Linear Recurrent Units against the more common Long Short Term Memory networks. These have shown promising ability in sequence to sequence modeling tasks, showing a notable improvement in signal history encoding. Our comparative study uses these black box neural modeling techniques with a variety of audio effects. We evaluate the performance and limitations using multiple metrics aiming to assess the models' ability to accurately replicate energy envelopes, frequency contents, and transients in the audio signal. To incorporate control parameters we employ the Feature wise Linear Modulation method. Long Short Term Memory networks exhibit better accuracy in emulating distortions and equalizers, while the State Space model, followed by Long Short Term Memory networks when integrated in an encoder decoder structure, outperforms others in emulating saturation and compression. When considering long time variant characteristics, the State Space model demonstrates the greatest accuracy. The Long Short Term Memory and, in particular, Linear Recurrent Unit networks present more tendency to introduce audio artifacts.","sentences":["Analog electronic circuits are at the core of an important category of musical devices.","The nonlinear features of their electronic components give analog musical devices a distinctive timbre and sound quality, making them highly desirable.","Artificial neural networks have rapidly gained popularity for the emulation of analog audio effects circuits, particularly recurrent networks.","While neural approaches have been successful in accurately modeling distortion circuits, they require architectural improvements that account for parameter conditioning and low latency response.","In this article, we explore the application of recent machine learning advancements for virtual analog modeling.","We compare State Space models and Linear Recurrent Units against the more common Long Short Term Memory networks.","These have shown promising ability in sequence to sequence modeling tasks, showing a notable improvement in signal history encoding.","Our comparative study uses these black box neural modeling techniques with a variety of audio effects.","We evaluate the performance and limitations using multiple metrics aiming to assess the models' ability to accurately replicate energy envelopes, frequency contents, and transients in the audio signal.","To incorporate control parameters we employ the Feature wise Linear Modulation method.","Long Short Term Memory networks exhibit better accuracy in emulating distortions and equalizers, while the State Space model, followed by Long Short Term Memory networks when integrated in an encoder decoder structure, outperforms others in emulating saturation and compression.","When considering long time variant characteristics, the State Space model demonstrates the greatest accuracy.","The Long Short Term Memory and, in particular, Linear Recurrent Unit networks present more tendency to introduce audio artifacts."],"url":"http://arxiv.org/abs/2405.04124v1","category":"cs.SD"}
{"created":"2024-05-07 08:46:50","title":"Two uniqueness results in the inverse boundary value problem for the weighted p-Laplace equation","abstract":"In this paper we prove a general uniqueness result in the inverse boundary value problem for the weighted p-Laplace equation in the plane, with smooth weights. We also prove a uniqueness result in dimension 3 and higher, for real analytic weights that are subject to a smallness condition on one of their directional derivatives. Both results are obtained by linearizing the equation at a solution without critical points. This unknown solution is then recovered, together with the unknown weight.","sentences":["In this paper we prove a general uniqueness result in the inverse boundary value problem for the weighted p-Laplace equation in the plane, with smooth weights.","We also prove a uniqueness result in dimension 3 and higher, for real analytic weights that are subject to a smallness condition on one of their directional derivatives.","Both results are obtained by linearizing the equation at a solution without critical points.","This unknown solution is then recovered, together with the unknown weight."],"url":"http://arxiv.org/abs/2405.04123v1","category":"math.AP"}
{"created":"2024-05-07 08:44:13","title":"ELiTe: Efficient Image-to-LiDAR Knowledge Transfer for Semantic Segmentation","abstract":"Cross-modal knowledge transfer enhances point cloud representation learning in LiDAR semantic segmentation. Despite its potential, the \\textit{weak teacher challenge} arises due to repetitive and non-diverse car camera images and sparse, inaccurate ground truth labels. To address this, we propose the Efficient Image-to-LiDAR Knowledge Transfer (ELiTe) paradigm. ELiTe introduces Patch-to-Point Multi-Stage Knowledge Distillation, transferring comprehensive knowledge from the Vision Foundation Model (VFM), extensively trained on diverse open-world images. This enables effective knowledge transfer to a lightweight student model across modalities. ELiTe employs Parameter-Efficient Fine-Tuning to strengthen the VFM teacher and expedite large-scale model training with minimal costs. Additionally, we introduce the Segment Anything Model based Pseudo-Label Generation approach to enhance low-quality image labels, facilitating robust semantic representations. Efficient knowledge transfer in ELiTe yields state-of-the-art results on the SemanticKITTI benchmark, outperforming real-time inference models. Our approach achieves this with significantly fewer parameters, confirming its effectiveness and efficiency.","sentences":["Cross-modal knowledge transfer enhances point cloud representation learning in LiDAR semantic segmentation.","Despite its potential, the \\textit{weak teacher challenge} arises due to repetitive and non-diverse car camera images and sparse, inaccurate ground truth labels.","To address this, we propose the Efficient Image-to-LiDAR Knowledge Transfer (ELiTe) paradigm.","ELiTe introduces Patch-to-Point Multi-Stage Knowledge Distillation, transferring comprehensive knowledge from the Vision Foundation Model (VFM), extensively trained on diverse open-world images.","This enables effective knowledge transfer to a lightweight student model across modalities.","ELiTe employs Parameter-Efficient Fine-Tuning to strengthen the VFM teacher and expedite large-scale model training with minimal costs.","Additionally, we introduce the Segment Anything Model based Pseudo-Label Generation approach to enhance low-quality image labels, facilitating robust semantic representations.","Efficient knowledge transfer in ELiTe yields state-of-the-art results on the SemanticKITTI benchmark, outperforming real-time inference models.","Our approach achieves this with significantly fewer parameters, confirming its effectiveness and efficiency."],"url":"http://arxiv.org/abs/2405.04121v1","category":"cs.CV"}
{"created":"2024-05-07 08:40:21","title":"Policy Learning with a Language Bottleneck","abstract":"Modern AI systems such as self-driving cars and game-playing agents achieve superhuman performance, but often lack human-like features such as generalization, interpretability and human inter-operability. Inspired by the rich interactions between language and decision-making in humans, we introduce Policy Learning with a Language Bottleneck (PLLB), a framework enabling AI agents to generate linguistic rules that capture the strategies underlying their most rewarding behaviors. PLLB alternates between a rule generation step guided by language models, and an update step where agents learn new policies guided by rules. In a two-player communication game, a maze solving task, and two image reconstruction tasks, we show that PLLB agents are not only able to learn more interpretable and generalizable behaviors, but can also share the learned rules with human users, enabling more effective human-AI coordination.","sentences":["Modern AI systems such as self-driving cars and game-playing agents achieve superhuman performance, but often lack human-like features such as generalization, interpretability and human inter-operability.","Inspired by the rich interactions between language and decision-making in humans, we introduce Policy Learning with a Language Bottleneck (PLLB), a framework enabling AI agents to generate linguistic rules that capture the strategies underlying their most rewarding behaviors.","PLLB alternates between a rule generation step guided by language models, and an update step where agents learn new policies guided by rules.","In a two-player communication game, a maze solving task, and two image reconstruction tasks, we show that PLLB agents are not only able to learn more interpretable and generalizable behaviors, but can also share the learned rules with human users, enabling more effective human-AI coordination."],"url":"http://arxiv.org/abs/2405.04118v1","category":"cs.LG"}
{"created":"2024-05-07 08:39:53","title":"Semi-implicit Lagrangian Voronoi Approximation for the incompressible Navier-Stokes equations","abstract":"We introduce Semi-Implicit Lagrangian Voronoi Approximation (SILVA), a novel numerical method for the solution of the incompressible Euler and Navier-Stokes equations, which combines the efficiency of semi-implicit time marching schemes with the robustness of time-dependent Voronoi tessellations. In SILVA, the numerical solution is stored at particles, which move with the fluid velocity and also play the role of the generators of the computational mesh. The Voronoi mesh is rapidly regenerated at each time step, allowing large deformations with topology changes. As opposed to the reconnection-based Arbitrary-Lagrangian-Eulerian schemes, we need no remapping stage. A semi-implicit scheme is devised in the context of moving Voronoi meshes to project the velocity field onto a divergence-free manifold. We validate SILVA by illustrative benchmarks, including viscous, inviscid, and multi-phase flows. Compared to its closest competitor, the Incompressible Smoothed Particle Hydrodynamics (ISPH) method, SILVA offers a sparser stiffness matrix and facilitates the implementation of no-slip and free-slip boundary conditions.","sentences":["We introduce Semi-Implicit Lagrangian Voronoi Approximation (SILVA), a novel numerical method for the solution of the incompressible Euler and Navier-Stokes equations, which combines the efficiency of semi-implicit time marching schemes with the robustness of time-dependent Voronoi tessellations.","In SILVA, the numerical solution is stored at particles, which move with the fluid velocity and also play the role of the generators of the computational mesh.","The Voronoi mesh is rapidly regenerated at each time step, allowing large deformations with topology changes.","As opposed to the reconnection-based Arbitrary-Lagrangian-Eulerian schemes, we need no remapping stage.","A semi-implicit scheme is devised in the context of moving Voronoi meshes to project the velocity field onto a divergence-free manifold.","We validate SILVA by illustrative benchmarks, including viscous, inviscid, and multi-phase flows.","Compared to its closest competitor, the Incompressible Smoothed Particle Hydrodynamics (ISPH) method, SILVA offers a sparser stiffness matrix and facilitates the implementation of no-slip and free-slip boundary conditions."],"url":"http://arxiv.org/abs/2405.04116v1","category":"math.NA"}
{"created":"2024-05-07 08:34:33","title":"Acceleration Algorithms in GNNs: A Survey","abstract":"Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph-based tasks. However, their inefficiency in training and inference presents challenges for scaling up to real-world and large-scale graph applications. To address the critical challenges, a range of algorithms have been proposed to accelerate training and inference of GNNs, attracting increasing attention from the research community. In this paper, we present a systematic review of acceleration algorithms in GNNs, which can be categorized into three main topics based on their purpose: training acceleration, inference acceleration, and execution acceleration. Specifically, we summarize and categorize the existing approaches for each main topic, and provide detailed characterizations of the approaches within each category. Additionally, we review several libraries related to acceleration algorithms in GNNs and discuss our Scalable Graph Learning (SGL) library. Finally, we propose promising directions for future research. A complete summary is presented in our GitHub repository: https://github.com/PKU-DAIR/SGL/blob/main/Awsome-GNN-Acceleration.md.","sentences":["Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph-based tasks.","However, their inefficiency in training and inference presents challenges for scaling up to real-world and large-scale graph applications.","To address the critical challenges, a range of algorithms have been proposed to accelerate training and inference of GNNs, attracting increasing attention from the research community.","In this paper, we present a systematic review of acceleration algorithms in GNNs, which can be categorized into three main topics based on their purpose: training acceleration, inference acceleration, and execution acceleration.","Specifically, we summarize and categorize the existing approaches for each main topic, and provide detailed characterizations of the approaches within each category.","Additionally, we review several libraries related to acceleration algorithms in GNNs and discuss our Scalable Graph Learning (SGL) library.","Finally, we propose promising directions for future research.","A complete summary is presented in our GitHub repository: https://github.com/PKU-DAIR/SGL/blob/main/Awsome-GNN-Acceleration.md."],"url":"http://arxiv.org/abs/2405.04114v1","category":"cs.LG"}
{"created":"2024-05-07 08:32:45","title":"Logarithmic lattice models for flows with boundaries","abstract":"Many fundamental problems in fluid dynamics are related to the effects of solid boundaries. In general, they install sharp gradients and contribute to the developement of small-scale structures, which are computationally expensive to resolve with numerical simulations. A way to access extremely fine scales with a reduced number of degrees of freedom is to consider the equations on logarithmic lattices in Fourier space. Here we introduce new toy models for flows with walls, by showing how to add boundaries to the logarithmic lattice framework. The resulting equations retain many important properties of the original systems, such as the conserved quantities, the symmetries and the boundary effects. We apply this technique to many flows, with emphasis on the inviscid limit of the Navier-Stokes equations. For this setup, simulations reach impressively large Reynolds numbers and disclose interesting insights about the original problem.","sentences":["Many fundamental problems in fluid dynamics are related to the effects of solid boundaries.","In general, they install sharp gradients and contribute to the developement of small-scale structures, which are computationally expensive to resolve with numerical simulations.","A way to access extremely fine scales with a reduced number of degrees of freedom is to consider the equations on logarithmic lattices in Fourier space.","Here we introduce new toy models for flows with walls, by showing how to add boundaries to the logarithmic lattice framework.","The resulting equations retain many important properties of the original systems, such as the conserved quantities, the symmetries and the boundary effects.","We apply this technique to many flows, with emphasis on the inviscid limit of the Navier-Stokes equations.","For this setup, simulations reach impressively large Reynolds numbers and disclose interesting insights about the original problem."],"url":"http://arxiv.org/abs/2405.04112v1","category":"physics.flu-dyn"}
{"created":"2024-05-07 08:25:12","title":"The Malware as a Service ecosystem","abstract":"The goal of this chapter is to illuminate the operational frameworks, key actors, and significant cybersecurity implications of the Malware as a Service (MaaS) ecosystem. Highlighting the transformation of malware proliferation into a service-oriented model, the chapter discusses how MaaS democratises access to sophisticated cyberattack capabilities, enabling even those with minimal technical knowledge to execute catastrophic cyberattacks. The discussion extends to the roles within the MaaS ecosystem, including malware developers, affiliates, initial access brokers, and the essential infrastructure providers that support these nefarious activities. The study emphasises the profound challenges MaaS poses to traditional cybersecurity defences, rendered ineffective against the constantly evolving and highly adaptable threats generated by MaaS platforms. With the increase in malware sophistication, there is a parallel call for a paradigm shift in defensive strategies, advocating for dynamic analysis, behavioural detection, and the integration of AI and machine learning techniques. By exploring the intricacies of the MaaS ecosystem, including the economic motivations driving its growth and the blurred lines between legitimate service models and cyber crime, the chapter presents a comprehensive overview intended to foster a deeper understanding among researchers and cybersecurity professionals. The ultimate goal is to aid in developing more effective strategies for combating the spread of commoditised malware threats and safeguarding against the increasing accessibility and scalability of cyberattacks facilitated by the MaaS model.","sentences":["The goal of this chapter is to illuminate the operational frameworks, key actors, and significant cybersecurity implications of the Malware as a Service (MaaS) ecosystem.","Highlighting the transformation of malware proliferation into a service-oriented model, the chapter discusses how MaaS democratises access to sophisticated cyberattack capabilities, enabling even those with minimal technical knowledge to execute catastrophic cyberattacks.","The discussion extends to the roles within the MaaS ecosystem, including malware developers, affiliates, initial access brokers, and the essential infrastructure providers that support these nefarious activities.","The study emphasises the profound challenges MaaS poses to traditional cybersecurity defences, rendered ineffective against the constantly evolving and highly adaptable threats generated by MaaS platforms.","With the increase in malware sophistication, there is a parallel call for a paradigm shift in defensive strategies, advocating for dynamic analysis, behavioural detection, and the integration of AI and machine learning techniques.","By exploring the intricacies of the MaaS ecosystem, including the economic motivations driving its growth and the blurred lines between legitimate service models and cyber crime, the chapter presents a comprehensive overview intended to foster a deeper understanding among researchers and cybersecurity professionals.","The ultimate goal is to aid in developing more effective strategies for combating the spread of commoditised malware threats and safeguarding against the increasing accessibility and scalability of cyberattacks facilitated by the MaaS model."],"url":"http://arxiv.org/abs/2405.04109v1","category":"cs.CR"}
{"created":"2024-05-07 08:24:50","title":"A2-DIDM: Privacy-preserving Accumulator-enabled Auditing for Distributed Identity of DNN Model","abstract":"Recent booming development of Generative Artificial Intelligence (GenAI) has facilitated an emerging model commercialization for the purpose of reinforcement on model performance, such as licensing or trading Deep Neural Network (DNN) models. However, DNN model trading may trigger concerns of the unauthorized replications or misuses over the model, so that the benefit of the model ownership will be violated. Model identity auditing is a challenging issue in protecting intellectual property of DNN models and verifying the integrity and ownership of models for guaranteeing trusts in transactions is one of the critical obstacles. In this paper, we focus on the above issue and propose a novel Accumulator-enabled Auditing for Distributed Identity of DNN Model (A2-DIDM) that utilizes blockchain and zero-knowledge techniques to protect data and function privacy while ensuring the lightweight on-chain ownership verification. The proposed model presents a scheme of identity records via configuring model weight checkpoints with corresponding zero-knowledge proofs, which incorporates predicates to capture incremental state changes in model weight checkpoints. Our scheme ensures both computational integrity of DNN training process and programmability, so that the uniqueness of the weight checkpoint sequence in a DNN model is preserved, ensuring the correctness of the model identity auditing. In addition, A2-DIDM also addresses privacy protections in distributed identity via a proposed method of accumulators. We systematically analyze the security and robustness of our proposed model and further evaluate the effectiveness and usability of auditing DNN model identities.","sentences":["Recent booming development of Generative Artificial Intelligence (GenAI) has facilitated an emerging model commercialization for the purpose of reinforcement on model performance, such as licensing or trading Deep Neural Network (DNN) models.","However, DNN model trading may trigger concerns of the unauthorized replications or misuses over the model, so that the benefit of the model ownership will be violated.","Model identity auditing is a challenging issue in protecting intellectual property of DNN models and verifying the integrity and ownership of models for guaranteeing trusts in transactions is one of the critical obstacles.","In this paper, we focus on the above issue and propose a novel Accumulator-enabled Auditing for Distributed Identity of DNN Model (A2-DIDM) that utilizes blockchain and zero-knowledge techniques to protect data and function privacy while ensuring the lightweight on-chain ownership verification.","The proposed model presents a scheme of identity records via configuring model weight checkpoints with corresponding zero-knowledge proofs, which incorporates predicates to capture incremental state changes in model weight checkpoints.","Our scheme ensures both computational integrity of DNN training process and programmability, so that the uniqueness of the weight checkpoint sequence in a DNN model is preserved, ensuring the correctness of the model identity auditing.","In addition, A2-DIDM also addresses privacy protections in distributed identity via a proposed method of accumulators.","We systematically analyze the security and robustness of our proposed model and further evaluate the effectiveness and usability of auditing DNN model identities."],"url":"http://arxiv.org/abs/2405.04108v1","category":"cs.CR"}
{"created":"2024-05-07 08:24:08","title":"Counting core sets in matrix rings over finite fields","abstract":"Let $R$ be a commutative ring and $M_n(R)$ be the ring of $n \\times n$ matrices with entries from $R$. For each $S \\subseteq M_n(R)$, we consider its (generalized) null ideal $N(S)$, which is the set of all polynomials $f$ with coefficients from $M_n(R)$ with the property that $f(A) = 0$ for all $A \\in S$. The set $S$ is said to be core if $N(S)$ is a two-sided ideal of $M_n(R)[x]$. It is not known how common core sets are among all subsets of $M_n(R)$. We study this problem for $2 \\times 2$ matrices over $\\mathbb{F}_q$, where $\\mathbb{F}_q$ is the finite field with $q$ elements. We provide exact counts for the number of core subsets of each similarity class of $M_2(\\mathbb{F}_q)$. While not every subset of $M_2(\\mathbb{F}_q)$ is core, we prove that as $q \\to \\infty$, the probability that a subset of $M_2(\\mathbb{F}_q)$ is core approaches 1. Thus, asymptotically in~$q$, almost all subsets of $M_2(\\mathbb{F}_q)$ are core.","sentences":["Let $R$ be a commutative ring and $M_n(R)$ be the ring of $n \\times n$ matrices with entries from $R$. For each $S \\subseteq M_n(R)$, we consider its (generalized) null ideal $N(S)$, which is the set of all polynomials $f$ with coefficients from $M_n(R)$ with the property that $f(A) = 0$ for all $A \\in S$.","The set $S$ is said to be core if $N(S)$ is a two-sided ideal of $M_n(R)[x]$. It is not known how common core sets are among all subsets of $M_n(R)$. We study this problem for $2 \\times 2$ matrices over $\\mathbb{F}_q$, where $\\mathbb{F}_q$ is the finite field with $q$ elements.","We provide exact counts for the number of core subsets of each similarity class of $M_2(\\mathbb{F}_q)$. While not every subset of $M_2(\\mathbb{F}_q)$ is core, we prove that as $q \\to \\infty$, the probability that a subset of $M_2(\\mathbb{F}_q)$ is core approaches 1.","Thus, asymptotically in~$q$, almost all subsets of $M_2(\\mathbb{F}_q)$ are core."],"url":"http://arxiv.org/abs/2405.04106v1","category":"math.RA"}
{"created":"2024-05-07 08:16:13","title":"COM3D: Leveraging Cross-View Correspondence and Cross-Modal Mining for 3D Retrieval","abstract":"In this paper, we investigate an open research task of cross-modal retrieval between 3D shapes and textual descriptions. Previous approaches mainly rely on point cloud encoders for feature extraction, which may ignore key inherent features of 3D shapes, including depth, spatial hierarchy, geometric continuity, etc. To address this issue, we propose COM3D, making the first attempt to exploit the cross-view correspondence and cross-modal mining to enhance the retrieval performance. Notably, we augment the 3D features through a scene representation transformer, to generate cross-view correspondence features of 3D shapes, which enrich the inherent features and enhance their compatibility with text matching. Furthermore, we propose to optimize the cross-modal matching process based on the semi-hard negative example mining method, in an attempt to improve the learning efficiency. Extensive quantitative and qualitative experiments demonstrate the superiority of our proposed COM3D, achieving state-of-the-art results on the Text2Shape dataset.","sentences":["In this paper, we investigate an open research task of cross-modal retrieval between 3D shapes and textual descriptions.","Previous approaches mainly rely on point cloud encoders for feature extraction, which may ignore key inherent features of 3D shapes, including depth, spatial hierarchy, geometric continuity, etc.","To address this issue, we propose COM3D, making the first attempt to exploit the cross-view correspondence and cross-modal mining to enhance the retrieval performance.","Notably, we augment the 3D features through a scene representation transformer, to generate cross-view correspondence features of 3D shapes, which enrich the inherent features and enhance their compatibility with text matching.","Furthermore, we propose to optimize the cross-modal matching process based on the semi-hard negative example mining method, in an attempt to improve the learning efficiency.","Extensive quantitative and qualitative experiments demonstrate the superiority of our proposed COM3D, achieving state-of-the-art results on the Text2Shape dataset."],"url":"http://arxiv.org/abs/2405.04103v1","category":"cs.CV"}
{"created":"2024-05-07 08:15:48","title":"Continual Learning in the Presence of Repetition","abstract":"Continual learning (CL) provides a framework for training models in ever-evolving environments. Although re-occurrence of previously seen objects or tasks is common in real-world problems, the concept of repetition in the data stream is not often considered in standard benchmarks for CL. Unlike with the rehearsal mechanism in buffer-based strategies, where sample repetition is controlled by the strategy, repetition in the data stream naturally stems from the environment. This report provides a summary of the CLVision challenge at CVPR 2023, which focused on the topic of repetition in class-incremental learning. The report initially outlines the challenge objective and then describes three solutions proposed by finalist teams that aim to effectively exploit the repetition in the stream to learn continually. The experimental results from the challenge highlight the effectiveness of ensemble-based solutions that employ multiple versions of similar modules, each trained on different but overlapping subsets of classes. This report underscores the transformative potential of taking a different perspective in CL by employing repetition in the data stream to foster innovative strategy design.","sentences":["Continual learning (CL) provides a framework for training models in ever-evolving environments.","Although re-occurrence of previously seen objects or tasks is common in real-world problems, the concept of repetition in the data stream is not often considered in standard benchmarks for CL.","Unlike with the rehearsal mechanism in buffer-based strategies, where sample repetition is controlled by the strategy, repetition in the data stream naturally stems from the environment.","This report provides a summary of the CLVision challenge at CVPR 2023, which focused on the topic of repetition in class-incremental learning.","The report initially outlines the challenge objective and then describes three solutions proposed by finalist teams that aim to effectively exploit the repetition in the stream to learn continually.","The experimental results from the challenge highlight the effectiveness of ensemble-based solutions that employ multiple versions of similar modules, each trained on different but overlapping subsets of classes.","This report underscores the transformative potential of taking a different perspective in CL by employing repetition in the data stream to foster innovative strategy design."],"url":"http://arxiv.org/abs/2405.04101v1","category":"cs.LG"}
{"created":"2024-05-07 07:57:15","title":"Unmasking Illusions: Understanding Human Perception of Audiovisual Deepfakes","abstract":"The emergence of contemporary deepfakes has attracted significant attention in machine learning research, as artificial intelligence (AI) generated synthetic media increases the incidence of misinterpretation and is difficult to distinguish from genuine content. Currently, machine learning techniques have been extensively studied for automatically detecting deepfakes. However, human perception has been less explored. Malicious deepfakes could ultimately cause public and social problems. Can we humans correctly perceive the authenticity of the content of the videos we watch? The answer is obviously uncertain; therefore, this paper aims to evaluate the human ability to discern deepfake videos through a subjective study. We present our findings by comparing human observers to five state-ofthe-art audiovisual deepfake detection models. To this end, we used gamification concepts to provide 110 participants (55 native English speakers and 55 non-native English speakers) with a webbased platform where they could access a series of 40 videos (20 real and 20 fake) to determine their authenticity. Each participant performed the experiment twice with the same 40 videos in different random orders. The videos are manually selected from the FakeAVCeleb dataset. We found that all AI models performed better than humans when evaluated on the same 40 videos. The study also reveals that while deception is not impossible, humans tend to overestimate their detection capabilities. Our experimental results may help benchmark human versus machine performance, advance forensics analysis, and enable adaptive countermeasures.","sentences":["The emergence of contemporary deepfakes has attracted significant attention in machine learning research, as artificial intelligence (AI) generated synthetic media increases the incidence of misinterpretation and is difficult to distinguish from genuine content.","Currently, machine learning techniques have been extensively studied for automatically detecting deepfakes.","However, human perception has been less explored.","Malicious deepfakes could ultimately cause public and social problems.","Can we humans correctly perceive the authenticity of the content of the videos we watch?","The answer is obviously uncertain; therefore, this paper aims to evaluate the human ability to discern deepfake videos through a subjective study.","We present our findings by comparing human observers to five state-ofthe-art audiovisual deepfake detection models.","To this end, we used gamification concepts to provide 110 participants (55 native English speakers and 55 non-native English speakers) with a webbased platform where they could access a series of 40 videos (20 real and 20 fake) to determine their authenticity.","Each participant performed the experiment twice with the same 40 videos in different random orders.","The videos are manually selected from the FakeAVCeleb dataset.","We found that all AI models performed better than humans when evaluated on the same 40 videos.","The study also reveals that while deception is not impossible, humans tend to overestimate their detection capabilities.","Our experimental results may help benchmark human versus machine performance, advance forensics analysis, and enable adaptive countermeasures."],"url":"http://arxiv.org/abs/2405.04097v1","category":"cs.CV"}
{"created":"2024-05-07 07:55:45","title":"Going Proactive and Explanatory Against Malware Concept Drift","abstract":"Deep learning-based malware classifiers face significant challenges due to concept drift. The rapid evolution of malware, especially with new families, can depress classification accuracy to near-random levels. Previous research has primarily focused on detecting drift samples, relying on expert-led analysis and labeling for model retraining. However, these methods often lack a comprehensive understanding of malware concepts and provide limited guidance for effective drift adaptation, leading to unstable detection performance and high human labeling costs.   To address these limitations, we introduce DREAM, a novel system designed to surpass the capabilities of existing drift detectors and to establish an explanatory drift adaptation process. DREAM enhances drift detection through model sensitivity and data autonomy. The detector, trained in a semi-supervised approach, proactively captures malware behavior concepts through classifier feedback. During testing, it utilizes samples generated by the detector itself, eliminating reliance on extensive training data. For drift adaptation, DREAM enlarges human intervention, enabling revisions of malware labels and concept explanations embedded within the detector's latent space. To ensure a comprehensive response to concept drift, it facilitates a coordinated update process for both the classifier and the detector. Our evaluation shows that DREAM can effectively improve the drift detection accuracy and reduce the expert analysis effort in adaptation across different malware datasets and classifiers.","sentences":["Deep learning-based malware classifiers face significant challenges due to concept drift.","The rapid evolution of malware, especially with new families, can depress classification accuracy to near-random levels.","Previous research has primarily focused on detecting drift samples, relying on expert-led analysis and labeling for model retraining.","However, these methods often lack a comprehensive understanding of malware concepts and provide limited guidance for effective drift adaptation, leading to unstable detection performance and high human labeling costs.   ","To address these limitations, we introduce DREAM, a novel system designed to surpass the capabilities of existing drift detectors and to establish an explanatory drift adaptation process.","DREAM enhances drift detection through model sensitivity and data autonomy.","The detector, trained in a semi-supervised approach, proactively captures malware behavior concepts through classifier feedback.","During testing, it utilizes samples generated by the detector itself, eliminating reliance on extensive training data.","For drift adaptation, DREAM enlarges human intervention, enabling revisions of malware labels and concept explanations embedded within the detector's latent space.","To ensure a comprehensive response to concept drift, it facilitates a coordinated update process for both the classifier and the detector.","Our evaluation shows that DREAM can effectively improve the drift detection accuracy and reduce the expert analysis effort in adaptation across different malware datasets and classifiers."],"url":"http://arxiv.org/abs/2405.04095v1","category":"cs.CR"}
{"created":"2024-05-07 07:51:28","title":"DCNN: Dual Cross-current Neural Networks Realized Using An Interactive Deep Learning Discriminator for Fine-grained Objects","abstract":"Accurate classification of fine-grained images remains a challenge in backbones based on convolutional operations or self-attention mechanisms. This study proposes novel dual-current neural networks (DCNN), which combine the advantages of convolutional operations and self-attention mechanisms to improve the accuracy of fine-grained image classification. The main novel design features for constructing a weakly supervised learning backbone model DCNN include (a) extracting heterogeneous data, (b) keeping the feature map resolution unchanged, (c) expanding the receptive field, and (d) fusing global representations and local features. Experimental results demonstrated that using DCNN as the backbone network for classifying certain fine-grained benchmark datasets achieved performance advantage improvements of 13.5--19.5% and 2.2--12.9%, respectively, compared to other advanced convolution or attention-based fine-grained backbones.","sentences":["Accurate classification of fine-grained images remains a challenge in backbones based on convolutional operations or self-attention mechanisms.","This study proposes novel dual-current neural networks (DCNN), which combine the advantages of convolutional operations and self-attention mechanisms to improve the accuracy of fine-grained image classification.","The main novel design features for constructing a weakly supervised learning backbone model DCNN include (a) extracting heterogeneous data, (b) keeping the feature map resolution unchanged, (c) expanding the receptive field, and (d) fusing global representations and local features.","Experimental results demonstrated that using DCNN as the backbone network for classifying certain fine-grained benchmark datasets achieved performance advantage improvements of 13.5--19.5% and 2.2--12.9%, respectively, compared to other advanced convolution or attention-based fine-grained backbones."],"url":"http://arxiv.org/abs/2405.04093v1","category":"cs.CV"}
{"created":"2024-05-07 07:45:22","title":"Randomized iterative methods for generalized absolute value equations: Solvability and error bounds","abstract":"Randomized iterative methods, such as the Kaczmarz method and its variants, have gained growing attention due to their simplicity and efficiency in solving large-scale linear systems. Meanwhile, absolute value equations (AVE) have attracted increasing interest due to their connection with the linear complementarity problem. In this paper, we investigate the application of randomized iterative methods to generalized AVE (GAVE). Our approach differs from most existing works in that we tackle GAVE with non-square coefficient matrices. We establish more comprehensive sufficient and necessary conditions for characterizing the solvability of GAVE and propose precise error bound conditions. Furthermore, we introduce a flexible and efficient randomized iterative algorithmic framework for solving GAVE, which employs sampling matrices drawn from user-specified distributions. This framework is capable of encompassing many well-known methods, including the Picard iteration method and the randomized Kaczmarz method. Leveraging our findings on solvability and error bounds, we establish both almost sure convergence and linear convergence rates for this versatile algorithmic framework. Finally, we present numerical examples to illustrate the advantages of the new algorithms.","sentences":["Randomized iterative methods, such as the Kaczmarz method and its variants, have gained growing attention due to their simplicity and efficiency in solving large-scale linear systems.","Meanwhile, absolute value equations (AVE) have attracted increasing interest due to their connection with the linear complementarity problem.","In this paper, we investigate the application of randomized iterative methods to generalized AVE (GAVE).","Our approach differs from most existing works in that we tackle GAVE with non-square coefficient matrices.","We establish more comprehensive sufficient and necessary conditions for characterizing the solvability of GAVE and propose precise error bound conditions.","Furthermore, we introduce a flexible and efficient randomized iterative algorithmic framework for solving GAVE, which employs sampling matrices drawn from user-specified distributions.","This framework is capable of encompassing many well-known methods, including the Picard iteration method and the randomized Kaczmarz method.","Leveraging our findings on solvability and error bounds, we establish both almost sure convergence and linear convergence rates for this versatile algorithmic framework.","Finally, we present numerical examples to illustrate the advantages of the new algorithms."],"url":"http://arxiv.org/abs/2405.04091v1","category":"math.NA"}
{"created":"2024-05-07 07:42:59","title":"Parametric set-theoretic Yang-Baxter equation: p-racks, solutions & quantum algebras","abstract":"The theory of the parametric set-theoretic Yang-Baxter equation is established from a purely algebraic point of view. The first step towards this objective is the introduction of certain generalizations of the familiar shelves and racks called parametric (p)-shelves and racks. These objects satisfy a parametric self-distributivity condition and lead to solutions of the Yang-Baxter equation. Novel, non-reversible solutions are obtained from p-shelve/rack solutions by a suitable parametric twist, whereas all reversible set-theoretic solutions are reduced to the identity map via a parametric twist. The universal algebras associated to both p-rack and generic parametric, set-theoretic solutions are next presented and the corresponding universal R-matrices are derived. The admissible universal Drinfel'd twist is constructed allowing the derivation of the general set-theoretic universal R-matrix. By introducing the concept of a parametric coproduct we prove the existence of a parametric co-associativity. We show that the parametric coproduct is an algebra homomorphsim and the universal R-matrices satisfy intertwining relations with the algebra coproducts.","sentences":["The theory of the parametric set-theoretic Yang-Baxter equation is established from a purely algebraic point of view.","The first step towards this objective is the introduction of certain generalizations of the familiar shelves and racks called parametric (p)-shelves and racks.","These objects satisfy a parametric self-distributivity condition and lead to solutions of the Yang-Baxter equation.","Novel, non-reversible solutions are obtained from p-shelve/rack solutions by a suitable parametric twist, whereas all reversible set-theoretic solutions are reduced to the identity map via a parametric twist.","The universal algebras associated to both p-rack and generic parametric, set-theoretic solutions are next presented and the corresponding universal R-matrices are derived.","The admissible universal Drinfel'd twist is constructed allowing the derivation of the general set-theoretic universal R-matrix.","By introducing the concept of a parametric coproduct we prove the existence of a parametric co-associativity.","We show that the parametric coproduct is an algebra homomorphsim and the universal R-matrices satisfy intertwining relations with the algebra coproducts."],"url":"http://arxiv.org/abs/2405.04088v1","category":"math-ph"}
{"created":"2024-05-07 07:40:32","title":"Solving Maxwell's Equations Using Polarimetry Alone","abstract":"Maxwell's equations are solved when the amplitude and phase of the electromagnetic field are determined at all points in space. Generally, the Stokes parameters can only capture the amplitude and polarization state of the electromagnetic field in the radiation (far) zone. Therefore, the measurement of the Stokes parameters is, in general, insufficient to solve Maxwell's equations. In this Letter, we solve Maxwell's equations for a set of objects widely used in Nanophotonics using the Stokes parameters alone. Our method for solving Maxwell's equations endows the Stokes parameters an even more fundamental role in the electromagnetic scattering theory.","sentences":["Maxwell's equations are solved when the amplitude and phase of the electromagnetic field are determined at all points in space.","Generally, the Stokes parameters can only capture the amplitude and polarization state of the electromagnetic field in the radiation (far) zone.","Therefore, the measurement of the Stokes parameters is, in general, insufficient to solve Maxwell's equations.","In this Letter, we solve Maxwell's equations for a set of objects widely used in Nanophotonics using the Stokes parameters alone.","Our method for solving Maxwell's equations endows the Stokes parameters an even more fundamental role in the electromagnetic scattering theory."],"url":"http://arxiv.org/abs/2405.04087v1","category":"physics.optics"}
{"created":"2024-05-07 07:39:15","title":"Optimizing Language Model's Reasoning Abilities with Weak Supervision","abstract":"While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations poses scalability challenges, particularly as models and data requirements grow. To mitigate this, we explore the potential of enhancing LLMs' reasoning abilities with minimal human supervision. In this work, we introduce self-reinforcement, which begins with Supervised Fine-Tuning (SFT) of the model using a small collection of annotated questions. Then it iteratively improves LLMs by learning from the differences in responses from the SFT and unfinetuned models on unlabeled questions. Our approach provides an efficient approach without relying heavily on extensive human-annotated explanations. However, current reasoning benchmarks typically only include golden-reference answers or rationales. Therefore, we present \\textsc{PuzzleBen}, a weakly supervised benchmark that comprises 25,147 complex questions, answers, and human-generated rationales across various domains, such as brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks. A unique aspect of our dataset is the inclusion of 10,000 unannotated questions, enabling us to explore utilizing fewer supersized data to boost LLMs' inference capabilities. Our experiments underscore the significance of \\textsc{PuzzleBen}, as well as the effectiveness of our methodology as a promising direction in future endeavors. Our dataset and code will be published soon on \\texttt{Anonymity Link}.","sentences":["While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts.","However, this reliance on fully-supervised annotations poses scalability challenges, particularly as models and data requirements grow.","To mitigate this, we explore the potential of enhancing LLMs' reasoning abilities with minimal human supervision.","In this work, we introduce self-reinforcement, which begins with Supervised Fine-Tuning (SFT) of the model using a small collection of annotated questions.","Then it iteratively improves LLMs by learning from the differences in responses from the SFT and unfinetuned models on unlabeled questions.","Our approach provides an efficient approach without relying heavily on extensive human-annotated explanations.","However, current reasoning benchmarks typically only include golden-reference answers or rationales.","Therefore, we present \\textsc{PuzzleBen}, a weakly supervised benchmark that comprises 25,147 complex questions, answers, and human-generated rationales across various domains, such as brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks.","A unique aspect of our dataset is the inclusion of 10,000 unannotated questions, enabling us to explore utilizing fewer supersized data to boost LLMs' inference capabilities.","Our experiments underscore the significance of \\textsc{PuzzleBen}, as well as the effectiveness of our methodology as a promising direction in future endeavors.","Our dataset and code will be published soon on \\texttt{Anonymity Link}."],"url":"http://arxiv.org/abs/2405.04086v1","category":"cs.CL"}
{"created":"2024-05-07 07:38:28","title":"Solution of the mean-field Hubbard model of graphene rectangulenes","abstract":"We present a complete analytical solution of the mean-field Hubbard model of undoped and doped graphene rectangulenes. These are non-chiral ribbons of arbitrary length and width, whose dimensions range from simple short acene molecules all the way up to the bulk limit. We rewrite the Hubbard model in the basis of bulk and edge non-interacting eigen-states, and provide explicit expressions for the Coulomb matrix elements. We present a general mean-field decoupling of the Hamiltonian, and discuss in detail the paramagnetic, ferromagnetic and antiferromagnetic mean-field solutions. We calculate the eigen-energies, occupations, spin densities and addition energies of rectangulenes with lengths and widths ranging from a nanometer to several hundreds of them. We rewrite the exact mean-field tight-binding Hamiltonian back in the site-occupation basis, that can be used to model electronic, thermo-electric, transport and optical properties of experimental-size graphene flakes.","sentences":["We present a complete analytical solution of the mean-field Hubbard model of undoped and doped graphene rectangulenes.","These are non-chiral ribbons of arbitrary length and width, whose dimensions range from simple short acene molecules all the way up to the bulk limit.","We rewrite the Hubbard model in the basis of bulk and edge non-interacting eigen-states, and provide explicit expressions for the Coulomb matrix elements.","We present a general mean-field decoupling of the Hamiltonian, and discuss in detail the paramagnetic, ferromagnetic and antiferromagnetic mean-field solutions.","We calculate the eigen-energies, occupations, spin densities and addition energies of rectangulenes with lengths and widths ranging from a nanometer to several hundreds of them.","We rewrite the exact mean-field tight-binding Hamiltonian back in the site-occupation basis, that can be used to model electronic, thermo-electric, transport and optical properties of experimental-size graphene flakes."],"url":"http://arxiv.org/abs/2405.04085v1","category":"cond-mat.str-el"}
{"created":"2024-05-07 17:38:39","title":"New allometric models for the USA create a step-change in forest carbon estimation, modeling, and mapping","abstract":"The United States national forest inventory (NFI) serves as the foundation for forest aboveground biomass (AGB) and carbon accounting across the nation. These data enable design-based estimates of forest carbon stocks and stock-changes at state and regional levels, but also serve as inputs to model-based approaches for characterizing forest carbon stocks and stock-changes at finer resolutions. Although NFI tree and plot-level data are often treated as truth in these models, they are in fact estimates based on regional species-group models known collectively as the Component Ratio Method (CRM). In late 2023 the Forest Inventory and Analysis (FIA) program introduced a new National Scale Volume and Biomass Estimators (NSVB) system to replace CRM nationwide and offer more precise and accurate representations of forest AGB and carbon. Given the prevalence of model-based AGB studies relying on FIA, there is concern about the transferability of methods from CRM to NSVB models, as well as the comparability of existing CRM AGB products (e.g. maps) to new and forthcoming NSVB AGB products. To begin addressing these concerns we compared previously published CRM AGB maps to new maps produced using identical methods with NSVB AGB reference data. Our results suggest that models relying on passive satellite imagery (e.g. Landsat) provide acceptable estimates of point-in-time NSVB AGB and carbon stocks, but fail to accurately quantify growth in mature closed-canopy forests. We highlight that existing estimates, models, and maps based on FIA reference data are no longer compatible with NSVB, and recommend new methods as well as updated models and maps for accommodating this step-change. Our collective ability to adopt NSVB in our modeling and mapping workflows will help us provide the most accurate spatial forest carbon data possible in order to better inform local management and decision making.","sentences":["The United States national forest inventory (NFI) serves as the foundation for forest aboveground biomass (AGB) and carbon accounting across the nation.","These data enable design-based estimates of forest carbon stocks and stock-changes at state and regional levels, but also serve as inputs to model-based approaches for characterizing forest carbon stocks and stock-changes at finer resolutions.","Although NFI tree and plot-level data are often treated as truth in these models, they are in fact estimates based on regional species-group models known collectively as the Component Ratio Method (CRM).","In late 2023 the Forest Inventory and Analysis (FIA) program introduced a new National Scale Volume and Biomass Estimators (NSVB) system to replace CRM nationwide and offer more precise and accurate representations of forest AGB and carbon.","Given the prevalence of model-based AGB studies relying on FIA, there is concern about the transferability of methods from CRM to NSVB models, as well as the comparability of existing CRM AGB products (e.g. maps) to new and forthcoming NSVB AGB products.","To begin addressing these concerns we compared previously published CRM AGB maps to new maps produced using identical methods with NSVB AGB reference data.","Our results suggest that models relying on passive satellite imagery (e.g. Landsat) provide acceptable estimates of point-in-time NSVB AGB and carbon stocks, but fail to accurately quantify growth in mature closed-canopy forests.","We highlight that existing estimates, models, and maps based on FIA reference data are no longer compatible with NSVB, and recommend new methods as well as updated models and maps for accommodating this step-change.","Our collective ability to adopt NSVB in our modeling and mapping workflows will help us provide the most accurate spatial forest carbon data possible in order to better inform local management and decision making."],"url":"http://arxiv.org/abs/2405.04507v1","category":"stat.AP"}
{"created":"2024-05-07 12:50:28","title":"CoqPyt: Proof Navigation in Python in the Era of LLMs","abstract":"Proof assistants enable users to develop machine-checked proofs regarding software-related properties. Unfortunately, the interactive nature of these proof assistants imposes most of the proof burden on the user, making formal verification a complex, and time-consuming endeavor. Recent automation techniques based on neural methods address this issue, but require good programmatic support for collecting data and interacting with proof assistants. This paper presents CoqPyt, a Python tool for interacting with the Coq proof assistant. CoqPyt improves on other Coq-related tools by providing novel features, such as the extraction of rich premise data. We expect our work to aid development of tools and techniques, especially LLM-based, designed for proof synthesis and repair. A video describing and demonstrating CoqPyt is available at: https://youtu.be/fk74o0rePM8.","sentences":["Proof assistants enable users to develop machine-checked proofs regarding software-related properties.","Unfortunately, the interactive nature of these proof assistants imposes most of the proof burden on the user, making formal verification a complex, and time-consuming endeavor.","Recent automation techniques based on neural methods address this issue, but require good programmatic support for collecting data and interacting with proof assistants.","This paper presents CoqPyt, a Python tool for interacting with the Coq proof assistant.","CoqPyt improves on other Coq-related tools by providing novel features, such as the extraction of rich premise data.","We expect our work to aid development of tools and techniques, especially LLM-based, designed for proof synthesis and repair.","A video describing and demonstrating CoqPyt is available at: https://youtu.be/fk74o0rePM8."],"url":"http://arxiv.org/abs/2405.04282v1","category":"cs.SE"}
{"created":"2024-05-07 12:18:23","title":"A Weighted Least-Squares Method for Non-Asymptotic Identification of Markov Parameters from Multiple Trajectories","abstract":"Markov parameters play a key role in system identification. There exists many algorithms where these parameters are estimated using least-squares in a first, pre-processing, step, including subspace identification and multi-step least-squares algorithms, such as Weighted Null-Space Fitting. Recently, there has been an increasing interest in non-asymptotic analysis of estimation algorithms. In this contribution we identify the Markov parameters using weighted least-squares and present non-asymptotic analysis for such estimator. To cover both stable and unstable systems, multiple trajectories are collected. We show that with the optimal weighting matrix, weighted least-squares gives a tighter error bound than ordinary least-squares for the case of non-uniformly distributed measurement errors. Moreover, as the optimal weighting matrix depends on the system's true parameters, we introduce two methods to consistently estimate the optimal weighting matrix, where the convergence rate of these estimates is also provided. Numerical experiments demonstrate improvements of weighted least-squares over ordinary least-squares in finite sample settings.","sentences":["Markov parameters play a key role in system identification.","There exists many algorithms where these parameters are estimated using least-squares in a first, pre-processing, step, including subspace identification and multi-step least-squares algorithms, such as Weighted Null-Space Fitting.","Recently, there has been an increasing interest in non-asymptotic analysis of estimation algorithms.","In this contribution we identify the Markov parameters using weighted least-squares and present non-asymptotic analysis for such estimator.","To cover both stable and unstable systems, multiple trajectories are collected.","We show that with the optimal weighting matrix, weighted least-squares gives a tighter error bound than ordinary least-squares for the case of non-uniformly distributed measurement errors.","Moreover, as the optimal weighting matrix depends on the system's true parameters, we introduce two methods to consistently estimate the optimal weighting matrix, where the convergence rate of these estimates is also provided.","Numerical experiments demonstrate improvements of weighted least-squares over ordinary least-squares in finite sample settings."],"url":"http://arxiv.org/abs/2405.04258v1","category":"eess.SY"}
{"created":"2024-05-07 12:06:12","title":"Neurocomputational Phenotypes in Female and Male Autistic Individuals","abstract":"Autism Spectrum Disorder (ASD) is characterized by an altered phenotype in social interaction and communication. Additionally, autism typically manifests differently in females as opposed to males: a phenomenon that has likely led to long-term problems in diagnostics of autism in females. These sex-based differences in communicative behavior may originate from differences in neurocomputational properties of brain organization. The present study looked to examine the relationship between one neurocomputational measure of brain organization, the local power-law exponent, in autistic vs. neurotypical, as well as male vs. female participants. To investigate the autistic phenotype in neural organization based on biological sex, we collected continuous resting-state EEG data for 19 autistic young adults (10 F), and 23 controls (14 F), using a 64-channel Net Station EEG acquisition system. The data was analyzed to quantify the 1/f power spectrum. Correlations between power-law exponent and behavioral measures were calculated in a between-group (female vs. male; autistic vs. neurotypical) design. On average, the power-law exponent was significantly greater in the male ASD group than in the female ASD group in fronto-central regions. The differences were more pronounced over the left hemisphere, suggesting neural organization differences in regions responsible for language complexity. These differences provide a potential explanation for behavioral variances in female vs. male autistic young adults.","sentences":["Autism Spectrum Disorder (ASD) is characterized by an altered phenotype in social interaction and communication.","Additionally, autism typically manifests differently in females as opposed to males: a phenomenon that has likely led to long-term problems in diagnostics of autism in females.","These sex-based differences in communicative behavior may originate from differences in neurocomputational properties of brain organization.","The present study looked to examine the relationship between one neurocomputational measure of brain organization, the local power-law exponent, in autistic vs. neurotypical, as well as male vs. female participants.","To investigate the autistic phenotype in neural organization based on biological sex, we collected continuous resting-state EEG data for 19 autistic young adults (10 F), and 23 controls (14 F), using a 64-channel Net Station EEG acquisition system.","The data was analyzed to quantify the 1/f power spectrum.","Correlations between power-law exponent and behavioral measures were calculated in a between-group (female vs. male; autistic vs. neurotypical) design.","On average, the power-law exponent was significantly greater in the male ASD group than in the female ASD group in fronto-central regions.","The differences were more pronounced over the left hemisphere, suggesting neural organization differences in regions responsible for language complexity.","These differences provide a potential explanation for behavioral variances in female vs. male autistic young adults."],"url":"http://arxiv.org/abs/2405.04248v1","category":"q-bio.NC"}
{"created":"2024-05-07 11:13:12","title":"Resonant structure for improved directionality and extraction of single photons","abstract":"Fluorescent atomic defects, especially in dielectric materials, such as diamond are quite promising for several emerging quantum applications. However, efficient light extraction, directional emission, and narrow spectral emission are key challenges. We have designed dielectric metasurface exploiting Mie-resonance and the Kerker condition to address these issues. Our designed diamond metasurface, tailored for nitrogen-vacancy (NV) defect centers in diamond, predicts up to 500x improvement in the collection of 637 nm (zero phonon line) photons over that from the bare diamond. Our design achieves highly directional emission, predominantly emitting in a 20 degree lobe in the forward direction. This makes light collection more efficient, including for fiber-based collection. The predicted results are stable against the position of the emitter placed in the metaelement, thus alleviating the challenging fabrication requirement of precise positioning of the defect center. Equally importantly, our design approach can be applied to enhance single photon emission also from other defects such as SiV, other materials such as hBN, and other sources such as quantum dots.","sentences":["Fluorescent atomic defects, especially in dielectric materials, such as diamond are quite promising for several emerging quantum applications.","However, efficient light extraction, directional emission, and narrow spectral emission are key challenges.","We have designed dielectric metasurface exploiting Mie-resonance and the Kerker condition to address these issues.","Our designed diamond metasurface, tailored for nitrogen-vacancy (NV) defect centers in diamond, predicts up to 500x improvement in the collection of 637 nm (zero phonon line) photons over that from the bare diamond.","Our design achieves highly directional emission, predominantly emitting in a 20 degree lobe in the forward direction.","This makes light collection more efficient, including for fiber-based collection.","The predicted results are stable against the position of the emitter placed in the metaelement, thus alleviating the challenging fabrication requirement of precise positioning of the defect center.","Equally importantly, our design approach can be applied to enhance single photon emission also from other defects such as SiV, other materials such as hBN, and other sources such as quantum dots."],"url":"http://arxiv.org/abs/2405.04197v1","category":"physics.optics"}
{"created":"2024-05-07 10:18:28","title":"\u03c3/\u03c0 character of H-bonding in water clusters","abstract":"Hydrogen bonds are typically treated as sufficiently localized directional intermolecular bonds, in which dispersion and electrostatic contributions can be distinguished. However, being formed chiefly due to the overlapping of p orbitals of electronegative atoms, the corresponding electronic bonds are characterized by both {\\sigma}- and {\\pi}-kind binding, the former determining the directionality of bonds, while the latter, the coupling of molecules and the collective effects in H-bond networks. The latter contribution was never considered previously and is predetermined by overlapping pre-lone pair orbitals of oxygen atoms. This is manifested in the peculiarities of the electron density distribution, which are quantified based on the analysis of magnetic shielding tensors of oxygen and bridge hydrogen nuclei and illustrated by the shapes of cluster orbitals of water aggregates.","sentences":["Hydrogen bonds are typically treated as sufficiently localized directional intermolecular bonds, in which dispersion and electrostatic contributions can be distinguished.","However, being formed chiefly due to the overlapping of p orbitals of electronegative atoms, the corresponding electronic bonds are characterized by both {\\sigma}- and {\\pi}-kind binding, the former determining the directionality of bonds, while the latter, the coupling of molecules and the collective effects in H-bond networks.","The latter contribution was never considered previously and is predetermined by overlapping pre-lone pair orbitals of oxygen atoms.","This is manifested in the peculiarities of the electron density distribution, which are quantified based on the analysis of magnetic shielding tensors of oxygen and bridge hydrogen nuclei and illustrated by the shapes of cluster orbitals of water aggregates."],"url":"http://arxiv.org/abs/2405.04173v1","category":"physics.chem-ph"}
{"created":"2024-05-07 09:41:39","title":"Gas Source Localization Using physics Guided Neural Networks","abstract":"This work discusses a novel method for estimating the location of a gas source based on spatially distributed concentration measurements taken, e.g., by a mobile robot or flying platform that follows a predefined trajectory to collect samples. The proposed approach uses a Physics-Guided Neural Network to approximate the gas dispersion with the source location as an additional network input. After an initial offline training phase, the neural network can be used to efficiently solve the inverse problem of localizing the gas source based on measurements. The proposed approach allows avoiding rather costly numerical simulations of gas physics needed for solving inverse problems. Our experiments show that the method localizes the source well, even when dealing with measurements affected by noise.","sentences":["This work discusses a novel method for estimating the location of a gas source based on spatially distributed concentration measurements taken, e.g., by a mobile robot or flying platform that follows a predefined trajectory to collect samples.","The proposed approach uses a Physics-Guided Neural Network to approximate the gas dispersion with the source location as an additional network input.","After an initial offline training phase, the neural network can be used to efficiently solve the inverse problem of localizing the gas source based on measurements.","The proposed approach allows avoiding rather costly numerical simulations of gas physics needed for solving inverse problems.","Our experiments show that the method localizes the source well, even when dealing with measurements affected by noise."],"url":"http://arxiv.org/abs/2405.04151v1","category":"cs.LG"}
{"created":"2024-05-07 09:33:08","title":"Reconstructing the spacetime dual to a free matrix","abstract":"In this paper we consider the collective field theory description of the singlet sector of a free matrix field in 2+1 dimensions. This necessarily involves the study of $k$-local collective fields, which are functions of $2k+1$ coordinates. We argue that these coordinates have a natural interpretation: the $k$-local collective field is a field defined on an AdS$_4\\times$S$^{k-2}\\times$S$^{k-1}$ spacetime. The modes of a harmonic expansion on the S$^{k-2}\\times$S$^{k-1}$ portion of the spacetime leads to the spinning bulk fields of the dual gravity theory.","sentences":["In this paper we consider the collective field theory description of the singlet sector of a free matrix field in 2+1 dimensions.","This necessarily involves the study of $k$-local collective fields, which are functions of $2k+1$ coordinates.","We argue that these coordinates have a natural interpretation: the $k$-local collective field is a field defined on an AdS$_4\\times$S$^{k-2}\\times$S$^{k-1}$ spacetime.","The modes of a harmonic expansion on the S$^{k-2}\\times$S$^{k-1}$ portion of the spacetime leads to the spinning bulk fields of the dual gravity theory."],"url":"http://arxiv.org/abs/2405.04148v1","category":"hep-th"}
{"created":"2024-05-07 08:15:37","title":"ESP: Extro-Spective Prediction for Long-term Behavior Reasoning in Emergency Scenarios","abstract":"Emergent-scene safety is the key milestone for fully autonomous driving, and reliable on-time prediction is essential to maintain safety in emergency scenarios. However, these emergency scenarios are long-tailed and hard to collect, which restricts the system from getting reliable predictions. In this paper, we build a new dataset, which aims at the long-term prediction with the inconspicuous state variation in history for the emergency event, named the Extro-Spective Prediction (ESP) problem. Based on the proposed dataset, a flexible feature encoder for ESP is introduced to various prediction methods as a seamless plug-in, and its consistent performance improvement underscores its efficacy. Furthermore, a new metric named clamped temporal error (CTE) is proposed to give a more comprehensive evaluation of prediction performance, especially in time-sensitive emergency events of subseconds. Interestingly, as our ESP features can be described in human-readable language naturally, the application of integrating into ChatGPT also shows huge potential. The ESP-dataset and all benchmarks are released at https://dingrui-wang.github.io/ESP-Dataset/.","sentences":["Emergent-scene safety is the key milestone for fully autonomous driving, and reliable on-time prediction is essential to maintain safety in emergency scenarios.","However, these emergency scenarios are long-tailed and hard to collect, which restricts the system from getting reliable predictions.","In this paper, we build a new dataset, which aims at the long-term prediction with the inconspicuous state variation in history for the emergency event, named the Extro-Spective Prediction (ESP) problem.","Based on the proposed dataset, a flexible feature encoder for ESP is introduced to various prediction methods as a seamless plug-in, and its consistent performance improvement underscores its efficacy.","Furthermore, a new metric named clamped temporal error (CTE) is proposed to give a more comprehensive evaluation of prediction performance, especially in time-sensitive emergency events of subseconds.","Interestingly, as our ESP features can be described in human-readable language naturally, the application of integrating into ChatGPT also shows huge potential.","The ESP-dataset and all benchmarks are released at https://dingrui-wang.github.io/ESP-Dataset/."],"url":"http://arxiv.org/abs/2405.04100v1","category":"cs.CV"}
{"created":"2024-05-07 07:27:27","title":"Counterfactual and Semifactual Explanations in Abstract Argumentation: Formal Foundations, Complexity and Computation","abstract":"Explainable Artificial Intelligence and Formal Argumentation have received significant attention in recent years. Argumentation-based systems often lack explainability while supporting decision-making processes. Counterfactual and semifactual explanations are interpretability techniques that provide insights into the outcome of a model by generating alternative hypothetical instances. While there has been important work on counterfactual and semifactual explanations for Machine Learning models, less attention has been devoted to these kinds of problems in argumentation. In this paper, we explore counterfactual and semifactual reasoning in abstract Argumentation Framework. We investigate the computational complexity of counterfactual- and semifactual-based reasoning problems, showing that they are generally harder than classical argumentation problems such as credulous and skeptical acceptance. Finally, we show that counterfactual and semifactual queries can be encoded in weak-constrained Argumentation Framework, and provide a computational strategy through ASP solvers.","sentences":["Explainable Artificial Intelligence and Formal Argumentation have received significant attention in recent years.","Argumentation-based systems often lack explainability while supporting decision-making processes.","Counterfactual and semifactual explanations are interpretability techniques that provide insights into the outcome of a model by generating alternative hypothetical instances.","While there has been important work on counterfactual and semifactual explanations for Machine Learning models, less attention has been devoted to these kinds of problems in argumentation.","In this paper, we explore counterfactual and semifactual reasoning in abstract Argumentation Framework.","We investigate the computational complexity of counterfactual- and semifactual-based reasoning problems, showing that they are generally harder than classical argumentation problems such as credulous and skeptical acceptance.","Finally, we show that counterfactual and semifactual queries can be encoded in weak-constrained Argumentation Framework, and provide a computational strategy through ASP solvers."],"url":"http://arxiv.org/abs/2405.04081v1","category":"cs.AI"}
{"created":"2024-05-07 07:21:20","title":"WISER: Weak supervISion and supErvised Representation learning to improve drug response prediction in cancer","abstract":"Cancer, a leading cause of death globally, occurs due to genomic changes and manifests heterogeneously across patients. To advance research on personalized treatment strategies, the effectiveness of various drugs on cells derived from cancers (`cell lines') is experimentally determined in laboratory settings. Nevertheless, variations in the distribution of genomic data and drug responses between cell lines and humans arise due to biological and environmental differences. Moreover, while genomic profiles of many cancer patients are readily available, the scarcity of corresponding drug response data limits the ability to train machine learning models that can predict drug response in patients effectively. Recent cancer drug response prediction methods have largely followed the paradigm of unsupervised domain-invariant representation learning followed by a downstream drug response classification step. Introducing supervision in both stages is challenging due to heterogeneous patient response to drugs and limited drug response data. This paper addresses these challenges through a novel representation learning method in the first phase and weak supervision in the second. Experimental results on real patient data demonstrate the efficacy of our method (WISER) over state-of-the-art alternatives on predicting personalized drug response.","sentences":["Cancer, a leading cause of death globally, occurs due to genomic changes and manifests heterogeneously across patients.","To advance research on personalized treatment strategies, the effectiveness of various drugs on cells derived from cancers (`cell lines') is experimentally determined in laboratory settings.","Nevertheless, variations in the distribution of genomic data and drug responses between cell lines and humans arise due to biological and environmental differences.","Moreover, while genomic profiles of many cancer patients are readily available, the scarcity of corresponding drug response data limits the ability to train machine learning models that can predict drug response in patients effectively.","Recent cancer drug response prediction methods have largely followed the paradigm of unsupervised domain-invariant representation learning followed by a downstream drug response classification step.","Introducing supervision in both stages is challenging due to heterogeneous patient response to drugs and limited drug response data.","This paper addresses these challenges through a novel representation learning method in the first phase and weak supervision in the second.","Experimental results on real patient data demonstrate the efficacy of our method (WISER) over state-of-the-art alternatives on predicting personalized drug response."],"url":"http://arxiv.org/abs/2405.04078v1","category":"cs.LG"}
{"created":"2024-05-07 07:20:15","title":"A simple theory for training response of deep neural networks","abstract":"Deep neural networks give us a powerful method to model the training dataset's relationship between input and output. We can regard that as a complex adaptive system consisting of many artificial neurons that work as an adaptive memory as a whole. The network's behavior is training dynamics with a feedback loop from the evaluation of the loss function. We already know the training response can be constant or shows power law-like aging in some ideal situations. However, we still have gaps between those findings and other complex phenomena, like network fragility. To fill the gap, we introduce a very simple network and analyze it. We show the training response consists of some different factors based on training stages, activation functions, or training methods. In addition, we show feature space reduction as an effect of stochastic training dynamics, which can result in network fragility. Finally, we discuss some complex phenomena of deep networks.","sentences":["Deep neural networks give us a powerful method to model the training dataset's relationship between input and output.","We can regard that as a complex adaptive system consisting of many artificial neurons that work as an adaptive memory as a whole.","The network's behavior is training dynamics with a feedback loop from the evaluation of the loss function.","We already know the training response can be constant or shows power law-like aging in some ideal situations.","However, we still have gaps between those findings and other complex phenomena, like network fragility.","To fill the gap, we introduce a very simple network and analyze it.","We show the training response consists of some different factors based on training stages, activation functions, or training methods.","In addition, we show feature space reduction as an effect of stochastic training dynamics, which can result in network fragility.","Finally, we discuss some complex phenomena of deep networks."],"url":"http://arxiv.org/abs/2405.04074v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-07 07:10:44","title":"MFA-Net: Multi-Scale feature fusion attention network for liver tumor segmentation","abstract":"Segmentation of organs of interest in medical CT images is beneficial for diagnosis of diseases. Though recent methods based on Fully Convolutional Neural Networks (F-CNNs) have shown success in many segmentation tasks, fusing features from images with different scales is still a challenge: (1) Due to the lack of spatial awareness, F-CNNs share the same weights at different spatial locations. (2) F-CNNs can only obtain surrounding information through local receptive fields. To address the above challenge, we propose a new segmentation framework based on attention mechanisms, named MFA-Net (Multi-Scale Feature Fusion Attention Network). The proposed framework can learn more meaningful feature maps among multiple scales and result in more accurate automatic segmentation. We compare our proposed MFA-Net with SOTA methods on two 2D liver CT datasets. The experimental results show that our MFA-Net produces more precise segmentation on images with different scales.","sentences":["Segmentation of organs of interest in medical CT images is beneficial for diagnosis of diseases.","Though recent methods based on Fully Convolutional Neural Networks (F-CNNs) have shown success in many segmentation tasks, fusing features from images with different scales is still a challenge: (1) Due to the lack of spatial awareness, F-CNNs share the same weights at different spatial locations.","(2) F-CNNs can only obtain surrounding information through local receptive fields.","To address the above challenge, we propose a new segmentation framework based on attention mechanisms, named MFA-Net (Multi-Scale Feature Fusion Attention Network).","The proposed framework can learn more meaningful feature maps among multiple scales and result in more accurate automatic segmentation.","We compare our proposed MFA-Net with SOTA methods on two 2D liver CT datasets.","The experimental results show that our MFA-Net produces more precise segmentation on images with different scales."],"url":"http://arxiv.org/abs/2405.04064v1","category":"cs.AI"}
{"created":"2024-05-07 07:07:44","title":"Generalized Cauchy-Schwarz Divergence and Its Deep Learning Applications","abstract":"Divergence measures play a central role in machine learning and become increasingly essential in deep learning. However, valid and computationally efficient divergence measures for multiple (more than two) distributions are scarcely investigated. This becomes particularly crucial in areas where the simultaneous management of multiple distributions is both unavoidable and essential. Examples include clustering, multi-source domain adaptation or generalization, and multi-view learning, among others. Although calculating the mean of pairwise distances between any two distributions serves as a common way to quantify the total divergence among multiple distributions, it is crucial to acknowledge that this approach is not straightforward and requires significant computational resources. In this study, we introduce a new divergence measure for multiple distributions named the generalized Cauchy-Schwarz divergence (GCSD), which is inspired by the classic Cauchy-Schwarz divergence. Additionally, we provide a closed-form sample estimator based on kernel density estimation, making it convenient and straightforward to use in various machine-learning applications. Finally, we apply the proposed GCSD to two challenging machine learning tasks, namely deep learning-based clustering and the problem of multi-source domain adaptation. The experimental results showcase the impressive performance of GCSD in both tasks, highlighting its potential application in machine-learning areas that involve quantifying multiple distributions.","sentences":["Divergence measures play a central role in machine learning and become increasingly essential in deep learning.","However, valid and computationally efficient divergence measures for multiple (more than two) distributions are scarcely investigated.","This becomes particularly crucial in areas where the simultaneous management of multiple distributions is both unavoidable and essential.","Examples include clustering, multi-source domain adaptation or generalization, and multi-view learning, among others.","Although calculating the mean of pairwise distances between any two distributions serves as a common way to quantify the total divergence among multiple distributions, it is crucial to acknowledge that this approach is not straightforward and requires significant computational resources.","In this study, we introduce a new divergence measure for multiple distributions named the generalized Cauchy-Schwarz divergence (GCSD), which is inspired by the classic Cauchy-Schwarz divergence.","Additionally, we provide a closed-form sample estimator based on kernel density estimation, making it convenient and straightforward to use in various machine-learning applications.","Finally, we apply the proposed GCSD to two challenging machine learning tasks, namely deep learning-based clustering and the problem of multi-source domain adaptation.","The experimental results showcase the impressive performance of GCSD in both tasks, highlighting its potential application in machine-learning areas that involve quantifying multiple distributions."],"url":"http://arxiv.org/abs/2405.04061v1","category":"cs.LG"}
{"created":"2024-05-07 06:52:34","title":"Evaluating Text Summaries Generated by Large Language Models Using OpenAI's GPT","abstract":"This research examines the effectiveness of OpenAI's GPT models as independent evaluators of text summaries generated by six transformer-based models from Hugging Face: DistilBART, BERT, ProphetNet, T5, BART, and PEGASUS. We evaluated these summaries based on essential properties of high-quality summary - conciseness, relevance, coherence, and readability - using traditional metrics such as ROUGE and Latent Semantic Analysis (LSA). Uniquely, we also employed GPT not as a summarizer but as an evaluator, allowing it to independently assess summary quality without predefined metrics. Our analysis revealed significant correlations between GPT evaluations and traditional metrics, particularly in assessing relevance and coherence. The results demonstrate GPT's potential as a robust tool for evaluating text summaries, offering insights that complement established metrics and providing a basis for comparative analysis of transformer-based models in natural language processing tasks.","sentences":["This research examines the effectiveness of OpenAI's GPT models as independent evaluators of text summaries generated by six transformer-based models from Hugging Face: DistilBART, BERT, ProphetNet, T5, BART, and PEGASUS.","We evaluated these summaries based on essential properties of high-quality summary - conciseness, relevance, coherence, and readability - using traditional metrics such as ROUGE and Latent Semantic Analysis (LSA).","Uniquely, we also employed GPT not as a summarizer but as an evaluator, allowing it to independently assess summary quality without predefined metrics.","Our analysis revealed significant correlations between GPT evaluations and traditional metrics, particularly in assessing relevance and coherence.","The results demonstrate GPT's potential as a robust tool for evaluating text summaries, offering insights that complement established metrics and providing a basis for comparative analysis of transformer-based models in natural language processing tasks."],"url":"http://arxiv.org/abs/2405.04053v1","category":"cs.CL"}
{"created":"2024-05-07 06:47:12","title":"Learning Linear Block Error Correction Codes","abstract":"Error correction codes are a crucial part of the physical communication layer, ensuring the reliable transfer of data over noisy channels. The design of optimal linear block codes capable of being efficiently decoded is of major concern, especially for short block lengths. While neural decoders have recently demonstrated their advantage over classical decoding techniques, the neural design of the codes remains a challenge. In this work, we propose for the first time a unified encoder-decoder training of binary linear block codes. To this end, we adapt the coding setting to support efficient and differentiable training of the code for end-to-end optimization over the order two Galois field. We also propose a novel Transformer model in which the self-attention masking is performed in a differentiable fashion for the efficient backpropagation of the code gradient. Our results show that (i) the proposed decoder outperforms existing neural decoding on conventional codes, (ii) the suggested framework generates codes that outperform the {analogous} conventional codes, and (iii) the codes we developed not only excel with our decoder but also show enhanced performance with traditional decoding techniques.","sentences":["Error correction codes are a crucial part of the physical communication layer, ensuring the reliable transfer of data over noisy channels.","The design of optimal linear block codes capable of being efficiently decoded is of major concern, especially for short block lengths.","While neural decoders have recently demonstrated their advantage over classical decoding techniques, the neural design of the codes remains a challenge.","In this work, we propose for the first time a unified encoder-decoder training of binary linear block codes.","To this end, we adapt the coding setting to support efficient and differentiable training of the code for end-to-end optimization over the order two Galois field.","We also propose a novel Transformer model in which the self-attention masking is performed in a differentiable fashion for the efficient backpropagation of the code gradient.","Our results show that (i) the proposed decoder outperforms existing neural decoding on conventional codes, (ii) the suggested framework generates codes that outperform the {analogous} conventional codes, and (iii) the codes we developed not only excel with our decoder but also show enhanced performance with traditional decoding techniques."],"url":"http://arxiv.org/abs/2405.04050v1","category":"cs.IT"}
{"created":"2024-05-07 06:39:47","title":"Philosophy of Cognitive Science in the Age of Deep Learning","abstract":"Deep learning has enabled major advances across most areas of artificial intelligence research. This remarkable progress extends beyond mere engineering achievements and holds significant relevance for the philosophy of cognitive science. Deep neural networks have made significant strides in overcoming the limitations of older connectionist models that once occupied the centre stage of philosophical debates about cognition. This development is directly relevant to long-standing theoretical debates in the philosophy of cognitive science. Furthermore, ongoing methodological challenges related to the comparative evaluation of deep neural networks stand to benefit greatly from interdisciplinary collaboration with philosophy and cognitive science. The time is ripe for philosophers to explore foundational issues related to deep learning and cognition; this perspective paper surveys key areas where their contributions can be especially fruitful.","sentences":["Deep learning has enabled major advances across most areas of artificial intelligence research.","This remarkable progress extends beyond mere engineering achievements and holds significant relevance for the philosophy of cognitive science.","Deep neural networks have made significant strides in overcoming the limitations of older connectionist models that once occupied the centre stage of philosophical debates about cognition.","This development is directly relevant to long-standing theoretical debates in the philosophy of cognitive science.","Furthermore, ongoing methodological challenges related to the comparative evaluation of deep neural networks stand to benefit greatly from interdisciplinary collaboration with philosophy and cognitive science.","The time is ripe for philosophers to explore foundational issues related to deep learning and cognition; this perspective paper surveys key areas where their contributions can be especially fruitful."],"url":"http://arxiv.org/abs/2405.04048v1","category":"cs.CL"}
{"created":"2024-05-07 06:26:30","title":"Space-time Reinforcement Network for Video Object Segmentation","abstract":"Recently, video object segmentation (VOS) networks typically use memory-based methods: for each query frame, the mask is predicted by space-time matching to memory frames. Despite these methods having superior performance, they suffer from two issues: 1) Challenging data can destroy the space-time coherence between adjacent video frames. 2) Pixel-level matching will lead to undesired mismatching caused by the noises or distractors. To address the aforementioned issues, we first propose to generate an auxiliary frame between adjacent frames, serving as an implicit short-temporal reference for the query one. Next, we learn a prototype for each video object and prototype-level matching can be implemented between the query and memory. The experiment demonstrated that our network outperforms the state-of-the-art method on the DAVIS 2017, achieving a J&F score of 86.4%, and attains a competitive result 85.0% on YouTube VOS 2018. In addition, our network exhibits a high inference speed of 32+ FPS.","sentences":["Recently, video object segmentation (VOS) networks typically use memory-based methods: for each query frame, the mask is predicted by space-time matching to memory frames.","Despite these methods having superior performance, they suffer from two issues: 1) Challenging data can destroy the space-time coherence between adjacent video frames.","2) Pixel-level matching will lead to undesired mismatching caused by the noises or distractors.","To address the aforementioned issues, we first propose to generate an auxiliary frame between adjacent frames, serving as an implicit short-temporal reference for the query one.","Next, we learn a prototype for each video object and prototype-level matching can be implemented between the query and memory.","The experiment demonstrated that our network outperforms the state-of-the-art method on the DAVIS 2017, achieving a J&F score of 86.4%, and attains a competitive result 85.0% on YouTube VOS 2018.","In addition, our network exhibits a high inference speed of 32+ FPS."],"url":"http://arxiv.org/abs/2405.04042v1","category":"cs.CV"}
{"created":"2024-05-07 06:25:49","title":"Feature Map Convergence Evaluation for Functional Module","abstract":"Autonomous driving perception models are typically composed of multiple functional modules that interact through complex relationships to accomplish environment understanding. However, perception models are predominantly optimized as a black box through end-to-end training, lacking independent evaluation of functional modules, which poses difficulties for interpretability and optimization. Pioneering in the issue, we propose an evaluation method based on feature map analysis to gauge the convergence of model, thereby assessing functional modules' training maturity. We construct a quantitative metric named as the Feature Map Convergence Score (FMCS) and develop Feature Map Convergence Evaluation Network (FMCE-Net) to measure and predict the convergence degree of models respectively. FMCE-Net achieves remarkable predictive accuracy for FMCS across multiple image classification experiments, validating the efficacy and robustness of the introduced approach. To the best of our knowledge, this is the first independent evaluation method for functional modules, offering a new paradigm for the training assessment towards perception models.","sentences":["Autonomous driving perception models are typically composed of multiple functional modules that interact through complex relationships to accomplish environment understanding.","However, perception models are predominantly optimized as a black box through end-to-end training, lacking independent evaluation of functional modules, which poses difficulties for interpretability and optimization.","Pioneering in the issue, we propose an evaluation method based on feature map analysis to gauge the convergence of model, thereby assessing functional modules' training maturity.","We construct a quantitative metric named as the Feature Map Convergence Score (FMCS) and develop Feature Map Convergence Evaluation Network (FMCE-Net) to measure and predict the convergence degree of models respectively.","FMCE-Net achieves remarkable predictive accuracy for FMCS across multiple image classification experiments, validating the efficacy and robustness of the introduced approach.","To the best of our knowledge, this is the first independent evaluation method for functional modules, offering a new paradigm for the training assessment towards perception models."],"url":"http://arxiv.org/abs/2405.04041v1","category":"cs.AI"}
{"created":"2024-05-07 06:23:02","title":"Utilizing GPT to Enhance Text Summarization: A Strategy to Minimize Hallucinations","abstract":"In this research, we uses the DistilBERT model to generate extractive summary and the T5 model to generate abstractive summaries. Also, we generate hybrid summaries by combining both DistilBERT and T5 models. Central to our research is the implementation of GPT-based refining process to minimize the common problem of hallucinations that happens in AI-generated summaries. We evaluate unrefined summaries and, after refining, we also assess refined summaries using a range of traditional and novel metrics, demonstrating marked improvements in the accuracy and reliability of the summaries. Results highlight significant improvements in reducing hallucinatory content, thereby increasing the factual integrity of the summaries.","sentences":["In this research, we uses the DistilBERT model to generate extractive summary and the T5 model to generate abstractive summaries.","Also, we generate hybrid summaries by combining both DistilBERT and T5 models.","Central to our research is the implementation of GPT-based refining process to minimize the common problem of hallucinations that happens in AI-generated summaries.","We evaluate unrefined summaries and, after refining, we also assess refined summaries using a range of traditional and novel metrics, demonstrating marked improvements in the accuracy and reliability of the summaries.","Results highlight significant improvements in reducing hallucinatory content, thereby increasing the factual integrity of the summaries."],"url":"http://arxiv.org/abs/2405.04039v1","category":"cs.CL"}
{"created":"2024-05-07 06:05:43","title":"Locally Differentially Private In-Context Learning","abstract":"Large pretrained language models (LLMs) have shown surprising In-Context Learning (ICL) ability. An important application in deploying large language models is to augment LLMs with a private database for some specific task. The main problem with this promising commercial use is that LLMs have been shown to memorize their training data and their prompt data are vulnerable to membership inference attacks (MIA) and prompt leaking attacks. In order to deal with this problem, we treat LLMs as untrusted in privacy and propose a locally differentially private framework of in-context learning(LDP-ICL) in the settings where labels are sensitive. Considering the mechanisms of in-context learning in Transformers by gradient descent, we provide an analysis of the trade-off between privacy and utility in such LDP-ICL for classification. Moreover, we apply LDP-ICL to the discrete distribution estimation problem. In the end, we perform several experiments to demonstrate our analysis results.","sentences":["Large pretrained language models (LLMs) have shown surprising In-Context Learning (ICL) ability.","An important application in deploying large language models is to augment LLMs with a private database for some specific task.","The main problem with this promising commercial use is that LLMs have been shown to memorize their training data and their prompt data are vulnerable to membership inference attacks (MIA) and prompt leaking attacks.","In order to deal with this problem, we treat LLMs as untrusted in privacy and propose a locally differentially private framework of in-context learning(LDP-ICL) in the settings where labels are sensitive.","Considering the mechanisms of in-context learning in Transformers by gradient descent, we provide an analysis of the trade-off between privacy and utility in such LDP-ICL for classification.","Moreover, we apply LDP-ICL to the discrete distribution estimation problem.","In the end, we perform several experiments to demonstrate our analysis results."],"url":"http://arxiv.org/abs/2405.04032v1","category":"cs.CR"}
{"created":"2024-05-07 05:59:10","title":"Federated Control in Markov Decision Processes","abstract":"We study problems of federated control in Markov Decision Processes. To solve an MDP with large state space, multiple learning agents are introduced to collaboratively learn its optimal policy without communication of locally collected experience. In our settings, these agents have limited capabilities, which means they are restricted within different regions of the overall state space during the training process. In face of the difference among restricted regions, we firstly introduce concepts of leakage probabilities to understand how such heterogeneity affects the learning process, and then propose a novel communication protocol that we call Federated-Q protocol (FedQ), which periodically aggregates agents' knowledge of their restricted regions and accordingly modifies their learning problems for further training. In terms of theoretical analysis, we justify the correctness of FedQ as a communication protocol, then give a general result on sample complexity of derived algorithms FedQ-X with the RL oracle , and finally conduct a thorough study on the sample complexity of FedQ-SynQ. Specifically, FedQ-X has been shown to enjoy linear speedup in terms of sample complexity when workload is uniformly distributed among agents. Moreover, we carry out experiments in various environments to justify the efficiency of our methods.","sentences":["We study problems of federated control in Markov Decision Processes.","To solve an MDP with large state space, multiple learning agents are introduced to collaboratively learn its optimal policy without communication of locally collected experience.","In our settings, these agents have limited capabilities, which means they are restricted within different regions of the overall state space during the training process.","In face of the difference among restricted regions, we firstly introduce concepts of leakage probabilities to understand how such heterogeneity affects the learning process, and then propose a novel communication protocol that we call Federated-Q protocol (FedQ), which periodically aggregates agents' knowledge of their restricted regions and accordingly modifies their learning problems for further training.","In terms of theoretical analysis, we justify the correctness of FedQ as a communication protocol, then give a general result on sample complexity of derived algorithms FedQ-X with the RL oracle , and finally conduct a thorough study on the sample complexity of FedQ-SynQ. Specifically, FedQ-X has been shown to enjoy linear speedup in terms of sample complexity when workload is uniformly distributed among agents.","Moreover, we carry out experiments in various environments to justify the efficiency of our methods."],"url":"http://arxiv.org/abs/2405.04026v1","category":"stat.ML"}
{"created":"2024-05-07 05:23:56","title":"Certified Policy Verification and Synthesis for MDPs under Distributional Reach-avoidance Properties","abstract":"Markov Decision Processes (MDPs) are a classical model for decision making in the presence of uncertainty. Often they are viewed as state transformers with planning objectives defined with respect to paths over MDP states. An increasingly popular alternative is to view them as distribution transformers, giving rise to a sequence of probability distributions over MDP states. For instance, reachability and safety properties in modeling robot swarms or chemical reaction networks are naturally defined in terms of probability distributions over states. Verifying such distributional properties is known to be hard and often beyond the reach of classical state-based verification techniques.   In this work, we consider the problems of certified policy (i.e. controller) verification and synthesis in MDPs under distributional reach-avoidance specifications. By certified we mean that, along with a policy, we also aim to synthesize a (checkable) certificate ensuring that the MDP indeed satisfies the property. Thus, given the target set of distributions and an unsafe set of distributions over MDP states, our goal is to either synthesize a certificate for a given policy or synthesize a policy along with a certificate, proving that the target distribution can be reached while avoiding unsafe distributions. To solve this problem, we introduce the novel notion of distributional reach-avoid certificates and present automated procedures for (1) synthesizing a certificate for a given policy, and (2) synthesizing a policy together with the certificate, both providing formal guarantees on certificate correctness. Our experimental evaluation demonstrates the ability of our method to solve several non-trivial examples, including a multi-agent robot-swarm model, to synthesize certified policies and to certify existing policies.","sentences":["Markov Decision Processes (MDPs) are a classical model for decision making in the presence of uncertainty.","Often they are viewed as state transformers with planning objectives defined with respect to paths over MDP states.","An increasingly popular alternative is to view them as distribution transformers, giving rise to a sequence of probability distributions over MDP states.","For instance, reachability and safety properties in modeling robot swarms or chemical reaction networks are naturally defined in terms of probability distributions over states.","Verifying such distributional properties is known to be hard and often beyond the reach of classical state-based verification techniques.   ","In this work, we consider the problems of certified policy (i.e. controller) verification and synthesis in MDPs under distributional reach-avoidance specifications.","By certified we mean that, along with a policy, we also aim to synthesize a (checkable) certificate ensuring that the MDP indeed satisfies the property.","Thus, given the target set of distributions and an unsafe set of distributions over MDP states, our goal is to either synthesize a certificate for a given policy or synthesize a policy along with a certificate, proving that the target distribution can be reached while avoiding unsafe distributions.","To solve this problem, we introduce the novel notion of distributional reach-avoid certificates and present automated procedures for (1) synthesizing a certificate for a given policy, and (2) synthesizing a policy together with the certificate, both providing formal guarantees on certificate correctness.","Our experimental evaluation demonstrates the ability of our method to solve several non-trivial examples, including a multi-agent robot-swarm model, to synthesize certified policies and to certify existing policies."],"url":"http://arxiv.org/abs/2405.04015v1","category":"cs.AI"}
{"created":"2024-05-07 04:57:25","title":"Structured Click Control in Transformer-based Interactive Segmentation","abstract":"Click-point-based interactive segmentation has received widespread attention due to its efficiency. However, it's hard for existing algorithms to obtain precise and robust responses after multiple clicks. In this case, the segmentation results tend to have little change or are even worse than before. To improve the robustness of the response, we propose a structured click intent model based on graph neural networks, which adaptively obtains graph nodes via the global similarity of user-clicked Transformer tokens. Then the graph nodes will be aggregated to obtain structured interaction features. Finally, the dual cross-attention will be used to inject structured interaction features into vision Transformer features, thereby enhancing the control of clicks over segmentation results. Extensive experiments demonstrated the proposed algorithm can serve as a general structure in improving Transformer-based interactive segmenta?tion performance. The code and data will be released at https://github.com/hahamyt/scc.","sentences":["Click-point-based interactive segmentation has received widespread attention due to its efficiency.","However, it's hard for existing algorithms to obtain precise and robust responses after multiple clicks.","In this case, the segmentation results tend to have little change or are even worse than before.","To improve the robustness of the response, we propose a structured click intent model based on graph neural networks, which adaptively obtains graph nodes via the global similarity of user-clicked Transformer tokens.","Then the graph nodes will be aggregated to obtain structured interaction features.","Finally, the dual cross-attention will be used to inject structured interaction features into vision Transformer features, thereby enhancing the control of clicks over segmentation results.","Extensive experiments demonstrated the proposed algorithm can serve as a general structure in improving Transformer-based interactive segmenta?tion performance.","The code and data will be released at https://github.com/hahamyt/scc."],"url":"http://arxiv.org/abs/2405.04009v1","category":"cs.CV"}
{"created":"2024-05-07 04:55:47","title":"SEED-Data-Edit Technical Report: A Hybrid Dataset for Instructional Image Editing","abstract":"In this technical report, we introduce SEED-Data-Edit: a unique hybrid dataset for instruction-guided image editing, which aims to facilitate image manipulation using open-form language. SEED-Data-Edit is composed of three distinct types of data: (1) High-quality editing data produced by an automated pipeline, ensuring a substantial volume of diverse image editing pairs. (2) Real-world scenario data collected from the internet, which captures the intricacies of user intentions for promoting the practical application of image editing in the real world. (3) High-precision multi-turn editing data annotated by humans, which involves multiple rounds of edits for simulating iterative editing processes. The combination of these diverse data sources makes SEED-Data-Edit a comprehensive and versatile dataset for training language-guided image editing model. We fine-tune a pretrained Multimodal Large Language Model (MLLM) that unifies comprehension and generation with SEED-Data-Edit. The instruction tuned model demonstrates promising results, indicating the potential and effectiveness of SEED-Data-Edit in advancing the field of instructional image editing. The datasets are released in https://huggingface.co/datasets/AILab-CVC/SEED-Data-Edit.","sentences":["In this technical report, we introduce SEED-Data-Edit: a unique hybrid dataset for instruction-guided image editing, which aims to facilitate image manipulation using open-form language.","SEED-Data-Edit is composed of three distinct types of data: (1) High-quality editing data produced by an automated pipeline, ensuring a substantial volume of diverse image editing pairs.","(2) Real-world scenario data collected from the internet, which captures the intricacies of user intentions for promoting the practical application of image editing in the real world.","(3) High-precision multi-turn editing data annotated by humans, which involves multiple rounds of edits for simulating iterative editing processes.","The combination of these diverse data sources makes SEED-Data-Edit a comprehensive and versatile dataset for training language-guided image editing model.","We fine-tune a pretrained Multimodal Large Language Model (MLLM) that unifies comprehension and generation with SEED-Data-Edit.","The instruction tuned model demonstrates promising results, indicating the potential and effectiveness of SEED-Data-Edit in advancing the field of instructional image editing.","The datasets are released in https://huggingface.co/datasets/AILab-CVC/SEED-Data-Edit."],"url":"http://arxiv.org/abs/2405.04007v1","category":"cs.CV"}
{"created":"2024-05-07 04:08:49","title":"TrimCaching: Parameter-sharing AI Model Caching in Wireless Edge Networks","abstract":"Next-generation mobile networks are expected to facilitate fast AI model downloading to end users. By caching models on edge servers, mobile networks can deliver models to end users with low latency, resulting in a paradigm called edge model caching. In this paper, we develop a novel model placement scheme, called parameter-sharing model caching (TrimCaching). TrimCaching exploits the key observation that a wide range of AI models, such as convolutional neural networks or large language models, can share a significant proportion of parameter blocks containing reusable knowledge, thereby improving storage efficiency. To this end, we formulate a parameter-sharing model placement problem to maximize the cache hit ratio in multi-edge wireless networks by balancing the fundamental tradeoff between storage efficiency and service latency. We show that the formulated problem is a submodular maximization problem with submodular constraints, for which no polynomial-time approximation algorithm exists. To overcome this challenge, we study an important special case, where a small fixed number of parameter blocks are shared across models, which often holds in practice. In such a case, a polynomial-time algorithm with $\\left(1-\\epsilon\\right)/2$-approximation guarantee is developed. Subsequently, we address the original problem for the general case by developing a greedy algorithm. Simulation results demonstrate that the proposed TrimCaching framework significantly improves the cache hit ratio compared with state-of-the-art content caching without exploiting shared parameters in AI models.","sentences":["Next-generation mobile networks are expected to facilitate fast AI model downloading to end users.","By caching models on edge servers, mobile networks can deliver models to end users with low latency, resulting in a paradigm called edge model caching.","In this paper, we develop a novel model placement scheme, called parameter-sharing model caching (TrimCaching).","TrimCaching exploits the key observation that a wide range of AI models, such as convolutional neural networks or large language models, can share a significant proportion of parameter blocks containing reusable knowledge, thereby improving storage efficiency.","To this end, we formulate a parameter-sharing model placement problem to maximize the cache hit ratio in multi-edge wireless networks by balancing the fundamental tradeoff between storage efficiency and service latency.","We show that the formulated problem is a submodular maximization problem with submodular constraints, for which no polynomial-time approximation algorithm exists.","To overcome this challenge, we study an important special case, where a small fixed number of parameter blocks are shared across models, which often holds in practice.","In such a case, a polynomial-time algorithm with $\\left(1-\\epsilon\\right)/2$-approximation guarantee is developed.","Subsequently, we address the original problem for the general case by developing a greedy algorithm.","Simulation results demonstrate that the proposed TrimCaching framework significantly improves the cache hit ratio compared with state-of-the-art content caching without exploiting shared parameters in AI models."],"url":"http://arxiv.org/abs/2405.03990v1","category":"cs.NI"}
{"created":"2024-05-07 04:00:30","title":"Knowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application","abstract":"Contemporary recommender systems predominantly rely on collaborative filtering techniques, employing ID-embedding to capture latent associations among users and items. However, this approach overlooks the wealth of semantic information embedded within textual descriptions of items, leading to suboptimal performance in cold-start scenarios and long-tail user recommendations. Leveraging the capabilities of Large Language Models (LLMs) pretrained on massive text corpus presents a promising avenue for enhancing recommender systems by integrating open-world domain knowledge. In this paper, we propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world knowledge with collaborative knowledge. We address computational complexity concerns by utilizing pretrained LLMs as item encoders and freezing LLM parameters to avoid catastrophic forgetting and preserve open-world knowledge. To bridge the gap between the open-world and collaborative domains, we design a twin-tower structure supervised by the recommendation task and tailored for practical industrial application. Through offline experiments on the large-scale industrial dataset and online experiments on A/B tests, we demonstrate the efficacy of our approach.","sentences":["Contemporary recommender systems predominantly rely on collaborative filtering techniques, employing ID-embedding to capture latent associations among users and items.","However, this approach overlooks the wealth of semantic information embedded within textual descriptions of items, leading to suboptimal performance in cold-start scenarios and long-tail user recommendations.","Leveraging the capabilities of Large Language Models (LLMs) pretrained on massive text corpus presents a promising avenue for enhancing recommender systems by integrating open-world domain knowledge.","In this paper, we propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world knowledge with collaborative knowledge.","We address computational complexity concerns by utilizing pretrained LLMs as item encoders and freezing LLM parameters to avoid catastrophic forgetting and preserve open-world knowledge.","To bridge the gap between the open-world and collaborative domains, we design a twin-tower structure supervised by the recommendation task and tailored for practical industrial application.","Through offline experiments on the large-scale industrial dataset and online experiments on A/B tests, we demonstrate the efficacy of our approach."],"url":"http://arxiv.org/abs/2405.03988v1","category":"cs.IR"}
{"created":"2024-05-07 03:55:32","title":"Factors Influencing User Willingness To Use SORA","abstract":"Sora promises to redefine the way visual content is created. Despite its numerous forecasted benefits, the drivers of user willingness to use the text-to-video (T2V) model are unknown. This study extends the extended unified theory of acceptance and use of technology (UTAUT2) with perceived realism and novelty value. Using a purposive sampling method, we collected data from 940 respondents in the US and analyzed the sample using covariance-based structural equation modeling and fuzzy set qualitative comparative analysis (fsQCA). The findings reveal that all hypothesized relationships are supported, with perceived realism emerging as the most influential driver, followed by novelty value. Moreover, fsQCA identifies five configurations leading to high and low willingness to use, and the model demonstrates high predictive validity, contributing to theory advancement. Our study provides valuable insights for developers and marketers, offering guidance for strategic decisions to promote the widespread adoption of T2V models.","sentences":["Sora promises to redefine the way visual content is created.","Despite its numerous forecasted benefits, the drivers of user willingness to use the text-to-video (T2V) model are unknown.","This study extends the extended unified theory of acceptance and use of technology (UTAUT2) with perceived realism and novelty value.","Using a purposive sampling method, we collected data from 940 respondents in the US and analyzed the sample using covariance-based structural equation modeling and fuzzy set qualitative comparative analysis (fsQCA).","The findings reveal that all hypothesized relationships are supported, with perceived realism emerging as the most influential driver, followed by novelty value.","Moreover, fsQCA identifies five configurations leading to high and low willingness to use, and the model demonstrates high predictive validity, contributing to theory advancement.","Our study provides valuable insights for developers and marketers, offering guidance for strategic decisions to promote the widespread adoption of T2V models."],"url":"http://arxiv.org/abs/2405.03986v1","category":"cs.AI"}
{"created":"2024-05-07 03:29:11","title":"Can citations tell us about a paper's reproducibility? A case study of machine learning papers","abstract":"The iterative character of work in machine learning (ML) and artificial intelligence (AI) and reliance on comparisons against benchmark datasets emphasize the importance of reproducibility in that literature. Yet, resource constraints and inadequate documentation can make running replications particularly challenging. Our work explores the potential of using downstream citation contexts as a signal of reproducibility. We introduce a sentiment analysis framework applied to citation contexts from papers involved in Machine Learning Reproducibility Challenges in order to interpret the positive or negative outcomes of reproduction attempts. Our contributions include training classifiers for reproducibility-related contexts and sentiment analysis, and exploring correlations between citation context sentiment and reproducibility scores. Study data, software, and an artifact appendix are publicly available at https://github.com/lamps-lab/ccair-ai-reproducibility .","sentences":["The iterative character of work in machine learning (ML) and artificial intelligence (AI) and reliance on comparisons against benchmark datasets emphasize the importance of reproducibility in that literature.","Yet, resource constraints and inadequate documentation can make running replications particularly challenging.","Our work explores the potential of using downstream citation contexts as a signal of reproducibility.","We introduce a sentiment analysis framework applied to citation contexts from papers involved in Machine Learning Reproducibility Challenges in order to interpret the positive or negative outcomes of reproduction attempts.","Our contributions include training classifiers for reproducibility-related contexts and sentiment analysis, and exploring correlations between citation context sentiment and reproducibility scores.","Study data, software, and an artifact appendix are publicly available at https://github.com/lamps-lab/ccair-ai-reproducibility ."],"url":"http://arxiv.org/abs/2405.03977v1","category":"cs.DL"}
{"created":"2024-05-07 03:08:30","title":"TBNet: A Neural Architectural Defense Framework Facilitating DNN Model Protection in Trusted Execution Environments","abstract":"Trusted Execution Environments (TEEs) have become a promising solution to secure DNN models on edge devices. However, the existing solutions either provide inadequate protection or introduce large performance overhead. Taking both security and performance into consideration, this paper presents TBNet, a TEE-based defense framework that protects DNN model from a neural architectural perspective. Specifically, TBNet generates a novel Two-Branch substitution model, to respectively exploit (1) the computational resources in the untrusted Rich Execution Environment (REE) for latency reduction and (2) the physically-isolated TEE for model protection. Experimental results on a Raspberry Pi across diverse DNN model architectures and datasets demonstrate that TBNet achieves efficient model protection at a low cost.","sentences":["Trusted Execution Environments (TEEs) have become a promising solution to secure DNN models on edge devices.","However, the existing solutions either provide inadequate protection or introduce large performance overhead.","Taking both security and performance into consideration, this paper presents TBNet, a TEE-based defense framework that protects DNN model from a neural architectural perspective.","Specifically, TBNet generates a novel Two-Branch substitution model, to respectively exploit (1) the computational resources in the untrusted Rich Execution Environment (REE) for latency reduction and (2) the physically-isolated TEE for model protection.","Experimental results on a Raspberry Pi across diverse DNN model architectures and datasets demonstrate that TBNet achieves efficient model protection at a low cost."],"url":"http://arxiv.org/abs/2405.03974v1","category":"cs.CR"}
{"created":"2024-05-07 03:05:37","title":"Contextualization with SPLADE for High Recall Retrieval","abstract":"High Recall Retrieval (HRR), such as eDiscovery and medical systematic review, is a search problem that optimizes the cost of retrieving most relevant documents in a given collection. Iterative approaches, such as iterative relevance feedback and uncertainty sampling, are shown to be effective under various operational scenarios. Despite neural models demonstrating success in other text-related tasks, linear models such as logistic regression, in general, are still more effective and efficient in HRR since the model is trained and retrieves documents from the same fixed collection. In this work, we leverage SPLADE, an efficient retrieval model that transforms documents into contextualized sparse vectors, for HRR. Our approach combines the best of both worlds, leveraging both the contextualization from pretrained language models and the efficiency of linear models. It reduces 10% and 18% of the review cost in two HRR evaluation collections under a one-phase review workflow with a target recall of 80%. The experiment is implemented with TARexp and is available at https://github.com/eugene-yang/LSR-for-TAR.","sentences":["High Recall Retrieval (HRR), such as eDiscovery and medical systematic review, is a search problem that optimizes the cost of retrieving most relevant documents in a given collection.","Iterative approaches, such as iterative relevance feedback and uncertainty sampling, are shown to be effective under various operational scenarios.","Despite neural models demonstrating success in other text-related tasks, linear models such as logistic regression, in general, are still more effective and efficient in HRR since the model is trained and retrieves documents from the same fixed collection.","In this work, we leverage SPLADE, an efficient retrieval model that transforms documents into contextualized sparse vectors, for HRR.","Our approach combines the best of both worlds, leveraging both the contextualization from pretrained language models and the efficiency of linear models.","It reduces 10% and 18% of the review cost in two HRR evaluation collections under a one-phase review workflow with a target recall of 80%.","The experiment is implemented with TARexp and is available at https://github.com/eugene-yang/LSR-for-TAR."],"url":"http://arxiv.org/abs/2405.03972v1","category":"cs.IR"}
{"created":"2024-05-07 02:58:29","title":"Speak the Same Language: Global LiDAR Registration on BIM Using Pose Hough Transform","abstract":"The construction and robotic sensing data originate from disparate sources and are associated with distinct frames of reference. The primary objective of this study is to align LiDAR point clouds with building information modeling (BIM) using a global point cloud registration approach, aimed at establishing a shared understanding between the two modalities, i.e., ``speak the same language''. To achieve this, we design a cross-modality registration method, spanning from front end the back end. At the front end, we extract descriptors by identifying walls and capturing the intersected corners. Subsequently, for the back-end pose estimation, we employ the Hough transform for pose estimation and estimate multiple pose candidates. The final pose is verified by wall-pixel correlation. To evaluate the effectiveness of our method, we conducted real-world multi-session experiments in a large-scale university building, involving two different types of LiDAR sensors. We also report our findings and plan to make our collected dataset open-sourced.","sentences":["The construction and robotic sensing data originate from disparate sources and are associated with distinct frames of reference.","The primary objective of this study is to align LiDAR point clouds with building information modeling (BIM) using a global point cloud registration approach, aimed at establishing a shared understanding between the two modalities, i.e., ``speak the same language''.","To achieve this, we design a cross-modality registration method, spanning from front end the back end.","At the front end, we extract descriptors by identifying walls and capturing the intersected corners.","Subsequently, for the back-end pose estimation, we employ the Hough transform for pose estimation and estimate multiple pose candidates.","The final pose is verified by wall-pixel correlation.","To evaluate the effectiveness of our method, we conducted real-world multi-session experiments in a large-scale university building, involving two different types of LiDAR sensors.","We also report our findings and plan to make our collected dataset open-sourced."],"url":"http://arxiv.org/abs/2405.03969v1","category":"cs.RO"}
{"created":"2024-05-07 02:49:59","title":"ERATTA: Extreme RAG for Table To Answers with Large Language Models","abstract":"Large language models (LLMs) with residual augmented-generation (RAG) have been the optimal choice for scalable generative AI solutions in the recent past. However, the choice of use-cases that incorporate RAG with LLMs have been either generic or extremely domain specific, thereby questioning the scalability and generalizability of RAG-LLM approaches. In this work, we propose a unique LLM-based system where multiple LLMs can be invoked to enable data authentication, user query routing, data retrieval and custom prompting for question answering capabilities from data tables that are highly varying and large in size. Our system is tuned to extract information from Enterprise-level data products and furnish real time responses under 10 seconds. One prompt manages user-to-data authentication followed by three prompts to route, fetch data and generate a customizable prompt natural language responses. Additionally, we propose a five metric scoring module that detects and reports hallucinations in the LLM responses. Our proposed system and scoring metrics achieve >90% confidence scores across hundreds of user queries in the sustainability, financial health and social media domains. Extensions to the proposed extreme RAG architectures can enable heterogeneous source querying using LLMs.","sentences":["Large language models (LLMs) with residual augmented-generation (RAG) have been the optimal choice for scalable generative AI solutions in the recent past.","However, the choice of use-cases that incorporate RAG with LLMs have been either generic or extremely domain specific, thereby questioning the scalability and generalizability of RAG-LLM approaches.","In this work, we propose a unique LLM-based system where multiple LLMs can be invoked to enable data authentication, user query routing, data retrieval and custom prompting for question answering capabilities from data tables that are highly varying and large in size.","Our system is tuned to extract information from Enterprise-level data products and furnish real time responses under 10 seconds.","One prompt manages user-to-data authentication followed by three prompts to route, fetch data and generate a customizable prompt natural language responses.","Additionally, we propose a five metric scoring module that detects and reports hallucinations in the LLM responses.","Our proposed system and scoring metrics achieve >90% confidence scores across hundreds of user queries in the sustainability, financial health and social media domains.","Extensions to the proposed extreme RAG architectures can enable heterogeneous source querying using LLMs."],"url":"http://arxiv.org/abs/2405.03963v1","category":"cs.AI"}
{"created":"2024-05-07 02:45:28","title":"Simple Drop-in LoRA Conditioning on Attention Layers Will Improve Your Diffusion Model","abstract":"Current state-of-the-art diffusion models employ U-Net architectures containing convolutional and (qkv) self-attention layers. The U-Net processes images while being conditioned on the time embedding input for each sampling step and the class or caption embedding input corresponding to the desired conditional generation. Such conditioning involves scale-and-shift operations to the convolutional layers but does not directly affect the attention layers. While these standard architectural choices are certainly effective, not conditioning the attention layers feels arbitrary and potentially suboptimal. In this work, we show that simply adding LoRA conditioning to the attention layers without changing or tuning the other parts of the U-Net architecture improves the image generation quality. For example, a drop-in addition of LoRA conditioning to EDM diffusion model yields FID scores of 1.91/1.75 for unconditional and class-conditional CIFAR-10 generation, improving upon the baseline of 1.97/1.79.","sentences":["Current state-of-the-art diffusion models employ U-Net architectures containing convolutional and (qkv) self-attention layers.","The U-Net processes images while being conditioned on the time embedding input for each sampling step and the class or caption embedding input corresponding to the desired conditional generation.","Such conditioning involves scale-and-shift operations to the convolutional layers but does not directly affect the attention layers.","While these standard architectural choices are certainly effective, not conditioning the attention layers feels arbitrary and potentially suboptimal.","In this work, we show that simply adding LoRA conditioning to the attention layers without changing or tuning the other parts of the U-Net architecture improves the image generation quality.","For example, a drop-in addition of LoRA conditioning to EDM diffusion model yields FID scores of 1.91/1.75 for unconditional and class-conditional CIFAR-10 generation, improving upon the baseline of 1.97/1.79."],"url":"http://arxiv.org/abs/2405.03958v1","category":"cs.CV"}
{"created":"2024-05-07 02:29:41","title":"IPFed: Identity protected federated learning for user authentication","abstract":"With the development of laws and regulations related to privacy preservation, it has become difficult to collect personal data to perform machine learning. In this context, federated learning, which is distributed learning without sharing personal data, has been proposed. In this paper, we focus on federated learning for user authentication. We show that it is difficult to achieve both privacy preservation and high accuracy with existing methods. To address these challenges, we propose IPFed which is privacy-preserving federated learning using random projection for class embedding. Furthermore, we prove that IPFed is capable of learning equivalent to the state-of-the-art method. Experiments on face image datasets show that IPFed can protect the privacy of personal data while maintaining the accuracy of the state-of-the-art method.","sentences":["With the development of laws and regulations related to privacy preservation, it has become difficult to collect personal data to perform machine learning.","In this context, federated learning, which is distributed learning without sharing personal data, has been proposed.","In this paper, we focus on federated learning for user authentication.","We show that it is difficult to achieve both privacy preservation and high accuracy with existing methods.","To address these challenges, we propose IPFed","which is privacy-preserving federated learning using random projection for class embedding.","Furthermore, we prove that IPFed is capable of learning equivalent to the state-of-the-art method.","Experiments on face image datasets show that IPFed can protect the privacy of personal data while maintaining the accuracy of the state-of-the-art method."],"url":"http://arxiv.org/abs/2405.03955v1","category":"cs.CV"}
{"created":"2024-05-07 02:24:44","title":"Intelligent Cardiac Auscultation for Murmur Detection via Parallel-Attentive Models with Uncertainty Estimation","abstract":"Heart murmurs are a common manifestation of cardiovascular diseases and can provide crucial clues to early cardiac abnormalities. While most current research methods primarily focus on the accuracy of models, they often overlook other important aspects such as the interpretability of machine learning algorithms and the uncertainty of predictions. This paper introduces a heart murmur detection method based on a parallel-attentive model, which consists of two branches: One is based on a self-attention module and the other one is based on a convolutional network. Unlike traditional approaches, this structure is better equipped to handle long-term dependencies in sequential data, and thus effectively captures the local and global features of heart murmurs. Additionally, we acknowledge the significance of understanding the uncertainty of model predictions in the medical field for clinical decision-making. Therefore, we have incorporated an effective uncertainty estimation method based on Monte Carlo Dropout into our model. Furthermore, we have employed temperature scaling to calibrate the predictions of our probabilistic model, enhancing its reliability. In experiments conducted on the CirCor Digiscope dataset for heart murmur detection, our proposed method achieves a weighted accuracy of 79.8% and an F1 of 65.1%, representing state-of-the-art results.","sentences":["Heart murmurs are a common manifestation of cardiovascular diseases and can provide crucial clues to early cardiac abnormalities.","While most current research methods primarily focus on the accuracy of models, they often overlook other important aspects such as the interpretability of machine learning algorithms and the uncertainty of predictions.","This paper introduces a heart murmur detection method based on a parallel-attentive model, which consists of two branches: One is based on a self-attention module and the other one is based on a convolutional network.","Unlike traditional approaches, this structure is better equipped to handle long-term dependencies in sequential data, and thus effectively captures the local and global features of heart murmurs.","Additionally, we acknowledge the significance of understanding the uncertainty of model predictions in the medical field for clinical decision-making.","Therefore, we have incorporated an effective uncertainty estimation method based on Monte Carlo Dropout into our model.","Furthermore, we have employed temperature scaling to calibrate the predictions of our probabilistic model, enhancing its reliability.","In experiments conducted on the CirCor Digiscope dataset for heart murmur detection, our proposed method achieves a weighted accuracy of 79.8% and an F1 of 65.1%, representing state-of-the-art results."],"url":"http://arxiv.org/abs/2405.03953v1","category":"cs.SD"}
{"created":"2024-05-07 02:16:54","title":"Relating-Up: Advancing Graph Neural Networks through Inter-Graph Relationships","abstract":"Graph Neural Networks (GNNs) have excelled in learning from graph-structured data, especially in understanding the relationships within a single graph, i.e., intra-graph relationships. Despite their successes, GNNs are limited by neglecting the context of relationships across graphs, i.e., inter-graph relationships. Recognizing the potential to extend this capability, we introduce Relating-Up, a plug-and-play module that enhances GNNs by exploiting inter-graph relationships. This module incorporates a relation-aware encoder and a feedback training strategy. The former enables GNNs to capture relationships across graphs, enriching relation-aware graph representation through collective context. The latter utilizes a feedback loop mechanism for the recursively refinement of these representations, leveraging insights from refining inter-graph dynamics to conduct feedback loop. The synergy between these two innovations results in a robust and versatile module. Relating-Up enhances the expressiveness of GNNs, enabling them to encapsulate a wider spectrum of graph relationships with greater precision. Our evaluations across 16 benchmark datasets demonstrate that integrating Relating-Up into GNN architectures substantially improves performance, positioning Relating-Up as a formidable choice for a broad spectrum of graph representation learning tasks.","sentences":["Graph Neural Networks (GNNs) have excelled in learning from graph-structured data, especially in understanding the relationships within a single graph, i.e., intra-graph relationships.","Despite their successes, GNNs are limited by neglecting the context of relationships across graphs, i.e., inter-graph relationships.","Recognizing the potential to extend this capability, we introduce Relating-Up, a plug-and-play module that enhances GNNs by exploiting inter-graph relationships.","This module incorporates a relation-aware encoder and a feedback training strategy.","The former enables GNNs to capture relationships across graphs, enriching relation-aware graph representation through collective context.","The latter utilizes a feedback loop mechanism for the recursively refinement of these representations, leveraging insights from refining inter-graph dynamics to conduct feedback loop.","The synergy between these two innovations results in a robust and versatile module.","Relating-Up enhances the expressiveness of GNNs, enabling them to encapsulate a wider spectrum of graph relationships with greater precision.","Our evaluations across 16 benchmark datasets demonstrate that integrating Relating-Up into GNN architectures substantially improves performance, positioning Relating-Up as a formidable choice for a broad spectrum of graph representation learning tasks."],"url":"http://arxiv.org/abs/2405.03950v1","category":"cs.LG"}
{"created":"2024-05-07 02:12:17","title":"The Fault in Our Recommendations: On the Perils of Optimizing the Measurable","abstract":"Recommendation systems are widespread, and through customized recommendations, promise to match users with options they will like. To that end, data on engagement is collected and used. Most recommendation systems are ranking-based, where they rank and recommend items based on their predicted engagement. However, the engagement signals are often only a crude proxy for utility, as data on the latter is rarely collected or available. This paper explores the following question: By optimizing for measurable proxies, are recommendation systems at risk of significantly under-delivering on utility? If so, how can one improve utility which is seldom measured? To study these questions, we introduce a model of repeated user consumption in which, at each interaction, users select between an outside option and the best option from a recommendation set. Our model accounts for user heterogeneity, with the majority preferring ``popular'' content, and a minority favoring ``niche'' content. The system initially lacks knowledge of individual user preferences but can learn them through observations of users' choices over time. Our theoretical and numerical analysis demonstrate that optimizing for engagement can lead to significant utility losses. Instead, we propose a utility-aware policy that initially recommends a mix of popular and niche content. As the platform becomes more forward-looking, our utility-aware policy achieves the best of both worlds: near-optimal utility and near-optimal engagement simultaneously. Our study elucidates an important feature of recommendation systems; given the ability to suggest multiple items, one can perform significant exploration without incurring significant reductions in engagement. By recommending high-risk, high-reward items alongside popular items, systems can enhance discovery of high utility items without significantly affecting engagement.","sentences":["Recommendation systems are widespread, and through customized recommendations, promise to match users with options they will like.","To that end, data on engagement is collected and used.","Most recommendation systems are ranking-based, where they rank and recommend items based on their predicted engagement.","However, the engagement signals are often only a crude proxy for utility, as data on the latter is rarely collected or available.","This paper explores the following question: By optimizing for measurable proxies, are recommendation systems at risk of significantly under-delivering on utility?","If so, how can one improve utility which is seldom measured?","To study these questions, we introduce a model of repeated user consumption in which, at each interaction, users select between an outside option and the best option from a recommendation set.","Our model accounts for user heterogeneity, with the majority preferring ``popular'' content, and a minority favoring ``niche'' content.","The system initially lacks knowledge of individual user preferences but can learn them through observations of users' choices over time.","Our theoretical and numerical analysis demonstrate that optimizing for engagement can lead to significant utility losses.","Instead, we propose a utility-aware policy that initially recommends a mix of popular and niche content.","As the platform becomes more forward-looking, our utility-aware policy achieves the best of both worlds: near-optimal utility and near-optimal engagement simultaneously.","Our study elucidates an important feature of recommendation systems; given the ability to suggest multiple items, one can perform significant exploration without incurring significant reductions in engagement.","By recommending high-risk, high-reward items alongside popular items, systems can enhance discovery of high utility items without significantly affecting engagement."],"url":"http://arxiv.org/abs/2405.03948v1","category":"cs.IR"}
{"created":"2024-05-07 02:10:30","title":"Role of Sensing and Computer Vision in 6G Wireless Communications","abstract":"Recently, we are witnessing the remarkable progress and widespread adoption of sensing technologies in autonomous driving, robotics, and metaverse. Considering the rapid advancement of computer vision (CV) technology to analyze the sensing information, we anticipate a proliferation of wireless applications exploiting the sensing and CV technologies in 6G. In this article, we provide a holistic overview of the sensing and CV-aided wireless communications (SVWC) framework for 6G. By analyzing the high-resolution sensing information through the powerful CV techniques, SVWC can quickly and accurately understand the wireless environments and then perform the wireless tasks. To demonstrate the efficacy of SVWC, we design the whole process of SVWC including the sensing dataset collection, DL model training, and execution of realistic wireless tasks. From the numerical evaluations on 6G communication scenarios, we show that SVWC achieves considerable performance gains over the conventional 5G systems in terms of positioning accuracy, data rate, and access latency.","sentences":["Recently, we are witnessing the remarkable progress and widespread adoption of sensing technologies in autonomous driving, robotics, and metaverse.","Considering the rapid advancement of computer vision (CV) technology to analyze the sensing information, we anticipate a proliferation of wireless applications exploiting the sensing and CV technologies in 6G. In this article, we provide a holistic overview of the sensing and CV-aided wireless communications (SVWC) framework for 6G. By analyzing the high-resolution sensing information through the powerful CV techniques, SVWC can quickly and accurately understand the wireless environments and then perform the wireless tasks.","To demonstrate the efficacy of SVWC, we design the whole process of SVWC including the sensing dataset collection, DL model training, and execution of realistic wireless tasks.","From the numerical evaluations on 6G communication scenarios, we show that SVWC achieves considerable performance gains over the conventional 5G systems in terms of positioning accuracy, data rate, and access latency."],"url":"http://arxiv.org/abs/2405.03945v1","category":"cs.CV"}
{"created":"2024-05-07 02:05:30","title":"Predictive Modeling with Temporal Graphical Representation on Electronic Health Records","abstract":"Deep learning-based predictive models, leveraging Electronic Health Records (EHR), are receiving increasing attention in healthcare. An effective representation of a patient's EHR should hierarchically encompass both the temporal relationships between historical visits and medical events, and the inherent structural information within these elements. Existing patient representation methods can be roughly categorized into sequential representation and graphical representation. The sequential representation methods focus only on the temporal relationships among longitudinal visits. On the other hand, the graphical representation approaches, while adept at extracting the graph-structured relationships between various medical events, fall short in effectively integrate temporal information. To capture both types of information, we model a patient's EHR as a novel temporal heterogeneous graph. This graph includes historical visits nodes and medical events nodes. It propagates structured information from medical event nodes to visit nodes and utilizes time-aware visit nodes to capture changes in the patient's health status. Furthermore, we introduce a novel temporal graph transformer (TRANS) that integrates temporal edge features, global positional encoding, and local structural encoding into heterogeneous graph convolution, capturing both temporal and structural information. We validate the effectiveness of TRANS through extensive experiments on three real-world datasets. The results show that our proposed approach achieves state-of-the-art performance.","sentences":["Deep learning-based predictive models, leveraging Electronic Health Records (EHR), are receiving increasing attention in healthcare.","An effective representation of a patient's EHR should hierarchically encompass both the temporal relationships between historical visits and medical events, and the inherent structural information within these elements.","Existing patient representation methods can be roughly categorized into sequential representation and graphical representation.","The sequential representation methods focus only on the temporal relationships among longitudinal visits.","On the other hand, the graphical representation approaches, while adept at extracting the graph-structured relationships between various medical events, fall short in effectively integrate temporal information.","To capture both types of information, we model a patient's EHR as a novel temporal heterogeneous graph.","This graph includes historical visits nodes and medical events nodes.","It propagates structured information from medical event nodes to visit nodes and utilizes time-aware visit nodes to capture changes in the patient's health status.","Furthermore, we introduce a novel temporal graph transformer (TRANS) that integrates temporal edge features, global positional encoding, and local structural encoding into heterogeneous graph convolution, capturing both temporal and structural information.","We validate the effectiveness of TRANS through extensive experiments on three real-world datasets.","The results show that our proposed approach achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.03943v1","category":"cs.LG"}
{"created":"2024-05-07 02:03:07","title":"Collaborative Intelligence in Sequential Experiments: A Human-in-the-Loop Framework for Drug Discovery","abstract":"Drug discovery is a complex process that involves sequentially screening and examining a vast array of molecules to identify those with the target properties. This process, also referred to as sequential experimentation, faces challenges due to the vast search space, the rarity of target molecules, and constraints imposed by limited data and experimental budgets. To address these challenges, we introduce a human-in-the-loop framework for sequential experiments in drug discovery. This collaborative approach combines human expert knowledge with deep learning algorithms, enhancing the discovery of target molecules within a specified experimental budget. The proposed algorithm processes experimental data to recommend both promising molecules and those that could improve its performance to human experts. Human experts retain the final decision-making authority based on these recommendations and their domain expertise, including the ability to override algorithmic recommendations. We applied our method to drug discovery tasks using real-world data and found that it consistently outperforms all baseline methods, including those which rely solely on human or algorithmic input. This demonstrates the complementarity between human experts and the algorithm. Our results provide key insights into the levels of humans' domain knowledge, the importance of meta-knowledge, and effective work delegation strategies. Our findings suggest that such a framework can significantly accelerate the development of new vaccines and drugs by leveraging the best of both human and artificial intelligence.","sentences":["Drug discovery is a complex process that involves sequentially screening and examining a vast array of molecules to identify those with the target properties.","This process, also referred to as sequential experimentation, faces challenges due to the vast search space, the rarity of target molecules, and constraints imposed by limited data and experimental budgets.","To address these challenges, we introduce a human-in-the-loop framework for sequential experiments in drug discovery.","This collaborative approach combines human expert knowledge with deep learning algorithms, enhancing the discovery of target molecules within a specified experimental budget.","The proposed algorithm processes experimental data to recommend both promising molecules and those that could improve its performance to human experts.","Human experts retain the final decision-making authority based on these recommendations and their domain expertise, including the ability to override algorithmic recommendations.","We applied our method to drug discovery tasks using real-world data and found that it consistently outperforms all baseline methods, including those which rely solely on human or algorithmic input.","This demonstrates the complementarity between human experts and the algorithm.","Our results provide key insights into the levels of humans' domain knowledge, the importance of meta-knowledge, and effective work delegation strategies.","Our findings suggest that such a framework can significantly accelerate the development of new vaccines and drugs by leveraging the best of both human and artificial intelligence."],"url":"http://arxiv.org/abs/2405.03942v1","category":"cs.AI"}
{"created":"2024-05-07 01:47:47","title":"Roadside Units Assisted Localized Automated Vehicle Maneuvering: An Offline Reinforcement Learning Approach","abstract":"Traffic intersections present significant challenges for the safe and efficient maneuvering of connected and automated vehicles (CAVs). This research proposes an innovative roadside unit (RSU)-assisted cooperative maneuvering system aimed at enhancing road safety and traveling efficiency at intersections for CAVs. We utilize RSUs for real-time traffic data acquisition and train an offline reinforcement learning (RL) algorithm based on human driving data. Evaluation results obtained from hardware-in-loop autonomous driving simulations show that our approach employing the twin delayed deep deterministic policy gradient and behavior cloning (TD3+BC), achieves performance comparable to state-of-the-art autonomous driving systems in terms of safety measures while significantly enhancing travel efficiency by up to 17.38% in intersection areas. This paper makes a pivotal contribution to the field of intelligent transportation systems, presenting a breakthrough solution for improving urban traffic flow and safety at intersections.","sentences":["Traffic intersections present significant challenges for the safe and efficient maneuvering of connected and automated vehicles (CAVs).","This research proposes an innovative roadside unit (RSU)-assisted cooperative maneuvering system aimed at enhancing road safety and traveling efficiency at intersections for CAVs.","We utilize RSUs for real-time traffic data acquisition and train an offline reinforcement learning (RL) algorithm based on human driving data.","Evaluation results obtained from hardware-in-loop autonomous driving simulations show that our approach employing the twin delayed deep deterministic policy gradient and behavior cloning (TD3+BC), achieves performance comparable to state-of-the-art autonomous driving systems in terms of safety measures while significantly enhancing travel efficiency by up to 17.38% in intersection areas.","This paper makes a pivotal contribution to the field of intelligent transportation systems, presenting a breakthrough solution for improving urban traffic flow and safety at intersections."],"url":"http://arxiv.org/abs/2405.03935v1","category":"eess.SY"}
{"created":"2024-05-07 01:40:23","title":"CleanGraph: Human-in-the-loop Knowledge Graph Refinement and Completion","abstract":"This paper presents CleanGraph, an interactive web-based tool designed to facilitate the refinement and completion of knowledge graphs. Maintaining the reliability of knowledge graphs, which are grounded in high-quality and error-free facts, is crucial for real-world applications such as question-answering and information retrieval systems. These graphs are often automatically assembled from textual sources by extracting semantic triples via information extraction. However, assuring the quality of these extracted triples, especially when dealing with large or low-quality datasets, can pose a significant challenge and adversely affect the performance of downstream applications. CleanGraph allows users to perform Create, Read, Update, and Delete (CRUD) operations on their graphs, as well as apply models in the form of plugins for graph refinement and completion tasks. These functionalities enable users to enhance the integrity and reliability of their graph data. A demonstration of CleanGraph and its source code can be accessed at https://github.com/nlp-tlp/CleanGraph under the MIT License.","sentences":["This paper presents CleanGraph, an interactive web-based tool designed to facilitate the refinement and completion of knowledge graphs.","Maintaining the reliability of knowledge graphs, which are grounded in high-quality and error-free facts, is crucial for real-world applications such as question-answering and information retrieval systems.","These graphs are often automatically assembled from textual sources by extracting semantic triples via information extraction.","However, assuring the quality of these extracted triples, especially when dealing with large or low-quality datasets, can pose a significant challenge and adversely affect the performance of downstream applications.","CleanGraph allows users to perform Create, Read, Update, and Delete (CRUD) operations on their graphs, as well as apply models in the form of plugins for graph refinement and completion tasks.","These functionalities enable users to enhance the integrity and reliability of their graph data.","A demonstration of CleanGraph and its source code can be accessed at https://github.com/nlp-tlp/CleanGraph under the MIT License."],"url":"http://arxiv.org/abs/2405.03932v1","category":"cs.AI"}
{"created":"2024-05-07 01:17:06","title":"Unicorn: U-Net for Sea Ice Forecasting with Convolutional Neural Ordinary Differential Equations","abstract":"Sea ice at the North Pole is vital to global climate dynamics. However, accurately forecasting sea ice poses a significant challenge due to the intricate interaction among multiple variables. Leveraging the capability to integrate multiple inputs and powerful performances seamlessly, many studies have turned to neural networks for sea ice forecasting. This paper introduces a novel deep architecture named Unicorn, designed to forecast weekly sea ice. Our model integrates multiple time series images within its architecture to enhance its forecasting performance. Moreover, we incorporate a bottleneck layer within the U-Net architecture, serving as neural ordinary differential equations with convolution operations, to capture the spatiotemporal dynamics of latent variables. Through real data analysis with datasets spanning from 1998 to 2021, our proposed model demonstrates significant improvements over state-of-the-art models in the sea ice concentration forecasting task. It achieves an average MAE improvement of 12% compared to benchmark models. Additionally, our method outperforms existing approaches in sea ice extent forecasting, achieving a classification performance improvement of approximately 18%. These experimental results show the superiority of our proposed model.","sentences":["Sea ice at the North Pole is vital to global climate dynamics.","However, accurately forecasting sea ice poses a significant challenge due to the intricate interaction among multiple variables.","Leveraging the capability to integrate multiple inputs and powerful performances seamlessly, many studies have turned to neural networks for sea ice forecasting.","This paper introduces a novel deep architecture named Unicorn, designed to forecast weekly sea ice.","Our model integrates multiple time series images within its architecture to enhance its forecasting performance.","Moreover, we incorporate a bottleneck layer within the U-Net architecture, serving as neural ordinary differential equations with convolution operations, to capture the spatiotemporal dynamics of latent variables.","Through real data analysis with datasets spanning from 1998 to 2021, our proposed model demonstrates significant improvements over state-of-the-art models in the sea ice concentration forecasting task.","It achieves an average MAE improvement of 12% compared to benchmark models.","Additionally, our method outperforms existing approaches in sea ice extent forecasting, achieving a classification performance improvement of approximately 18%.","These experimental results show the superiority of our proposed model."],"url":"http://arxiv.org/abs/2405.03929v1","category":"cs.AI"}
{"created":"2024-05-07 00:51:48","title":"NeurDB: An AI-powered Autonomous Data System","abstract":"In the wake of rapid advancements in artificial intelligence (AI), we stand on the brink of a transformative leap in data systems. The imminent fusion of AI and DB (AIxDB) promises a new generation of data systems, which will relieve the burden on end-users across all industry sectors by featuring AI-enhanced functionalities, such as personalized and automated in-database AI-powered analytics, self-driving capabilities for improved system performance, etc. In this paper, we explore the evolution of data systems with a focus on deepening the fusion of AI and DB. We present NeurDB, our next-generation data system designed to fully embrace AI design in each major system component and provide in-database AI-powered analytics. We outline the conceptual and architectural overview of NeurDB, discuss its design choices and key components, and report its current development and future plan.","sentences":["In the wake of rapid advancements in artificial intelligence (AI), we stand on the brink of a transformative leap in data systems.","The imminent fusion of AI and DB (AIxDB) promises a new generation of data systems, which will relieve the burden on end-users across all industry sectors by featuring AI-enhanced functionalities, such as personalized and automated in-database AI-powered analytics, self-driving capabilities for improved system performance, etc.","In this paper, we explore the evolution of data systems with a focus on deepening the fusion of AI and DB.","We present NeurDB, our next-generation data system designed to fully embrace AI design in each major system component and provide in-database AI-powered analytics.","We outline the conceptual and architectural overview of NeurDB, discuss its design choices and key components, and report its current development and future plan."],"url":"http://arxiv.org/abs/2405.03924v1","category":"cs.DB"}
{"created":"2024-05-07 00:38:34","title":"A Roadmap for Multilingual, Multimodal Domain Independent Deception Detection","abstract":"Deception, a prevalent aspect of human communication, has undergone a significant transformation in the digital age. With the globalization of online interactions, individuals are communicating in multiple languages and mixing languages on social media, with varied data becoming available in each language and dialect. At the same time, the techniques for detecting deception are similar across the board. Recent studies have shown the possibility of the existence of universal linguistic cues to deception across domains within the English language; however, the existence of such cues in other languages remains unknown. Furthermore, the practical task of deception detection in low-resource languages is not a well-studied problem due to the lack of labeled data. Another dimension of deception is multimodality. For example, a picture with an altered caption in fake news or disinformation may exist. This paper calls for a comprehensive investigation into the complexities of deceptive language across linguistic boundaries and modalities within the realm of computer security and natural language processing and the possibility of using multilingual transformer models and labeled data in various languages to universally address the task of deception detection.","sentences":["Deception, a prevalent aspect of human communication, has undergone a significant transformation in the digital age.","With the globalization of online interactions, individuals are communicating in multiple languages and mixing languages on social media, with varied data becoming available in each language and dialect.","At the same time, the techniques for detecting deception are similar across the board.","Recent studies have shown the possibility of the existence of universal linguistic cues to deception across domains within the English language; however, the existence of such cues in other languages remains unknown.","Furthermore, the practical task of deception detection in low-resource languages is not a well-studied problem due to the lack of labeled data.","Another dimension of deception is multimodality.","For example, a picture with an altered caption in fake news or disinformation may exist.","This paper calls for a comprehensive investigation into the complexities of deceptive language across linguistic boundaries and modalities within the realm of computer security and natural language processing and the possibility of using multilingual transformer models and labeled data in various languages to universally address the task of deception detection."],"url":"http://arxiv.org/abs/2405.03920v1","category":"cs.CL"}
{"created":"2024-05-07 00:08:15","title":"Federated Graph Condensation with Information Bottleneck Principles","abstract":"Graph condensation, which reduces the size of a large-scale graph by synthesizing a small-scale condensed graph as its substitution, has immediately benefited various graph learning tasks. However, existing graph condensation methods rely on centralized data storage, which is unfeasible for real-world decentralized data distribution, and overlook data holders' privacy-preserving requirements. To bridge the gap, we propose and study the novel problem of federated graph condensation for graph neural networks (GNNs). Specifically, we first propose a general framework for federated graph condensation, in which we decouple the typical gradient matching process for graph condensation into client-side gradient calculation and server-side gradient matching. In this way, the burdensome computation cost in client-side is largely alleviated. Besides, our empirical studies show that under the federated setting, the condensed graph will consistently leak data membership privacy, i.e., the condensed graph during the federated training can be utilized to steal the training data under the membership inference attacks (MIA). To tackle this issue, we innovatively incorporate information bottleneck principles into the federated graph condensation, which only needs to extract partial node features in one local pre-training step and utilize the features during federated training. Extensive experiments on real-world datasets demonstrate that our framework can consistently protect membership privacy during training. Meanwhile, it also achieves comparable and even superior performance against existing centralized graph condensation and federated graph learning methods.","sentences":["Graph condensation, which reduces the size of a large-scale graph by synthesizing a small-scale condensed graph as its substitution, has immediately benefited various graph learning tasks.","However, existing graph condensation methods rely on centralized data storage, which is unfeasible for real-world decentralized data distribution, and overlook data holders' privacy-preserving requirements.","To bridge the gap, we propose and study the novel problem of federated graph condensation for graph neural networks (GNNs).","Specifically, we first propose a general framework for federated graph condensation, in which we decouple the typical gradient matching process for graph condensation into client-side gradient calculation and server-side gradient matching.","In this way, the burdensome computation cost in client-side is largely alleviated.","Besides, our empirical studies show that under the federated setting, the condensed graph will consistently leak data membership privacy, i.e., the condensed graph during the federated training can be utilized to steal the training data under the membership inference attacks (MIA).","To tackle this issue, we innovatively incorporate information bottleneck principles into the federated graph condensation, which only needs to extract partial node features in one local pre-training step and utilize the features during federated training.","Extensive experiments on real-world datasets demonstrate that our framework can consistently protect membership privacy during training.","Meanwhile, it also achieves comparable and even superior performance against existing centralized graph condensation and federated graph learning methods."],"url":"http://arxiv.org/abs/2405.03911v1","category":"cs.LG"}
{"created":"2024-05-06 23:33:52","title":"Unified Locational Differential Privacy Framework","abstract":"Aggregating statistics over geographical regions is important for many applications, such as analyzing income, election results, and disease spread. However, the sensitive nature of this data necessitates strong privacy protections to safeguard individuals. In this work, we present a unified locational differential privacy (DP) framework to enable private aggregation of various data types, including one-hot encoded, boolean, float, and integer arrays, over geographical regions. Our framework employs local DP mechanisms such as randomized response, the exponential mechanism, and the Gaussian mechanism. We evaluate our approach on four datasets representing significant location data aggregation scenarios. Results demonstrate the utility of our framework in providing formal DP guarantees while enabling geographical data analysis.","sentences":["Aggregating statistics over geographical regions is important for many applications, such as analyzing income, election results, and disease spread.","However, the sensitive nature of this data necessitates strong privacy protections to safeguard individuals.","In this work, we present a unified locational differential privacy (DP) framework to enable private aggregation of various data types, including one-hot encoded, boolean, float, and integer arrays, over geographical regions.","Our framework employs local DP mechanisms such as randomized response, the exponential mechanism, and the Gaussian mechanism.","We evaluate our approach on four datasets representing significant location data aggregation scenarios.","Results demonstrate the utility of our framework in providing formal DP guarantees while enabling geographical data analysis."],"url":"http://arxiv.org/abs/2405.03903v1","category":"cs.AI"}
{"created":"2024-05-06 23:11:00","title":"OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs","abstract":"The progression to \"Pervasive Augmented Reality\" envisions easy access to multimodal information continuously. However, in many everyday scenarios, users are occupied physically, cognitively or socially. This may increase the friction to act upon the multimodal information that users encounter in the world. To reduce such friction, future interactive interfaces should intelligently provide quick access to digital actions based on users' context. To explore the range of possible digital actions, we conducted a diary study that required participants to capture and share the media that they intended to perform actions on (e.g., images or audio), along with their desired actions and other contextual information. Using this data, we generated a holistic design space of digital follow-up actions that could be performed in response to different types of multimodal sensory inputs. We then designed OmniActions, a pipeline powered by large language models (LLMs) that processes multimodal sensory inputs and predicts follow-up actions on the target information grounded in the derived design space. Using the empirical data collected in the diary study, we performed quantitative evaluations on three variations of LLM techniques (intent classification, in-context learning and finetuning) and identified the most effective technique for our task. Additionally, as an instantiation of the pipeline, we developed an interactive prototype and reported preliminary user feedback about how people perceive and react to the action predictions and its errors.","sentences":["The progression to \"Pervasive Augmented Reality\" envisions easy access to multimodal information continuously.","However, in many everyday scenarios, users are occupied physically, cognitively or socially.","This may increase the friction to act upon the multimodal information that users encounter in the world.","To reduce such friction, future interactive interfaces should intelligently provide quick access to digital actions based on users' context.","To explore the range of possible digital actions, we conducted a diary study that required participants to capture and share the media that they intended to perform actions on (e.g., images or audio), along with their desired actions and other contextual information.","Using this data, we generated a holistic design space of digital follow-up actions that could be performed in response to different types of multimodal sensory inputs.","We then designed OmniActions, a pipeline powered by large language models (LLMs) that processes multimodal sensory inputs and predicts follow-up actions on the target information grounded in the derived design space.","Using the empirical data collected in the diary study, we performed quantitative evaluations on three variations of LLM techniques (intent classification, in-context learning and finetuning) and identified the most effective technique for our task.","Additionally, as an instantiation of the pipeline, we developed an interactive prototype and reported preliminary user feedback about how people perceive and react to the action predictions and its errors."],"url":"http://arxiv.org/abs/2405.03901v1","category":"cs.HC"}
{"created":"2024-05-06 23:05:58","title":"With or Without Permission: Site-Specific Augmented Reality for Social Justice","abstract":"Movements for social change are often tied to a particular locale. This makes Augmented Reality (AR), which changes how people perceive their surroundings, a promising technology for social justice. Site-specific AR empowers activists to re-tell the story of a place, with or without permission of its owner. It has been used, for example, to reveal hidden histories, re-imagine problematic monuments, and celebrate minority cultures. However, challenges remain concerning technological ownership and accessibility, scalability, sustainability, and navigating collaborations with marginalized communities and across disciplinary boundaries. This half-day workshop at CHI 2024 seeks to bring together an interdisciplinary group of activists, computer scientists, designers, media scholars, and more to identify opportunities and challenges across domains. To anchor the discussion, participants will each share one example of an artifact used in speculating, designing, and/or delivering site-specific AR experiences. This collection of artifacts will inaugurate an interactive database that can inspire a new wave of activists to leverage AR for social justice.","sentences":["Movements for social change are often tied to a particular locale.","This makes Augmented Reality (AR), which changes how people perceive their surroundings, a promising technology for social justice.","Site-specific AR empowers activists to re-tell the story of a place, with or without permission of its owner.","It has been used, for example, to reveal hidden histories, re-imagine problematic monuments, and celebrate minority cultures.","However, challenges remain concerning technological ownership and accessibility, scalability, sustainability, and navigating collaborations with marginalized communities and across disciplinary boundaries.","This half-day workshop at CHI 2024 seeks to bring together an interdisciplinary group of activists, computer scientists, designers, media scholars, and more to identify opportunities and challenges across domains.","To anchor the discussion, participants will each share one example of an artifact used in speculating, designing, and/or delivering site-specific AR experiences.","This collection of artifacts will inaugurate an interactive database that can inspire a new wave of activists to leverage AR for social justice."],"url":"http://arxiv.org/abs/2405.03898v1","category":"cs.HC"}
{"created":"2024-05-06 22:44:32","title":"Out-of-Distribution Adaptation in Offline RL: Counterfactual Reasoning via Causal Normalizing Flows","abstract":"Despite notable successes of Reinforcement Learning (RL), the prevalent use of an online learning paradigm prevents its widespread adoption, especially in hazardous or costly scenarios. Offline RL has emerged as an alternative solution, learning from pre-collected static datasets. However, this offline learning introduces a new challenge known as distributional shift, degrading the performance when the policy is evaluated on scenarios that are Out-Of-Distribution (OOD) from the training dataset. Most existing offline RL resolves this issue by regularizing policy learning within the information supported by the given dataset. However, such regularization overlooks the potential for high-reward regions that may exist beyond the dataset. This motivates exploring novel offline learning techniques that can make improvements beyond the data support without compromising policy performance, potentially by learning causation (cause-and-effect) instead of correlation from the dataset. In this paper, we propose the MOOD-CRL (Model-based Offline OOD-Adapting Causal RL) algorithm, which aims to address the challenge of extrapolation for offline policy training through causal inference instead of policy-regularizing methods. Specifically, Causal Normalizing Flow (CNF) is developed to learn the transition and reward functions for data generation and augmentation in offline policy evaluation and training. Based on the data-invariant, physics-based qualitative causal graph and the observational data, we develop a novel learning scheme for CNF to learn the quantitative structural causal model. As a result, CNF gains predictive and counterfactual reasoning capabilities for sequential decision-making tasks, revealing a high potential for OOD adaptation. Our CNF-based offline RL approach is validated through empirical evaluations, outperforming model-free and model-based methods by a significant margin.","sentences":["Despite notable successes of Reinforcement Learning (RL), the prevalent use of an online learning paradigm prevents its widespread adoption, especially in hazardous or costly scenarios.","Offline RL has emerged as an alternative solution, learning from pre-collected static datasets.","However, this offline learning introduces a new challenge known as distributional shift, degrading the performance when the policy is evaluated on scenarios that are Out-Of-Distribution (OOD) from the training dataset.","Most existing offline RL resolves this issue by regularizing policy learning within the information supported by the given dataset.","However, such regularization overlooks the potential for high-reward regions that may exist beyond the dataset.","This motivates exploring novel offline learning techniques that can make improvements beyond the data support without compromising policy performance, potentially by learning causation (cause-and-effect) instead of correlation from the dataset.","In this paper, we propose the MOOD-CRL (Model-based Offline OOD-Adapting Causal RL) algorithm, which aims to address the challenge of extrapolation for offline policy training through causal inference instead of policy-regularizing methods.","Specifically, Causal Normalizing Flow (CNF) is developed to learn the transition and reward functions for data generation and augmentation in offline policy evaluation and training.","Based on the data-invariant, physics-based qualitative causal graph and the observational data, we develop a novel learning scheme for CNF to learn the quantitative structural causal model.","As a result, CNF gains predictive and counterfactual reasoning capabilities for sequential decision-making tasks, revealing a high potential for OOD adaptation.","Our CNF-based offline RL approach is validated through empirical evaluations, outperforming model-free and model-based methods by a significant margin."],"url":"http://arxiv.org/abs/2405.03892v1","category":"cs.LG"}
{"created":"2024-05-06 22:27:24","title":"Enhancing O-RAN Security: Evasion Attacks and Robust Defenses for Graph Reinforcement Learning-based Connection Management","abstract":"Adversarial machine learning, focused on studying various attacks and defenses on machine learning (ML) models, is rapidly gaining importance as ML is increasingly being adopted for optimizing wireless systems such as Open Radio Access Networks (O-RAN). A comprehensive modeling of the security threats and the demonstration of adversarial attacks and defenses on practical AI based O-RAN systems is still in its nascent stages. We begin by conducting threat modeling to pinpoint attack surfaces in O-RAN using an ML-based Connection management application (xApp) as an example. The xApp uses a Graph Neural Network trained using Deep Reinforcement Learning and achieves on average 54% improvement in the coverage rate measured as the 5th percentile user data rates. We then formulate and demonstrate evasion attacks that degrade the coverage rates by as much as 50% through injecting bounded noise at different threat surfaces including the open wireless medium itself. Crucially, we also compare and contrast the effectiveness of such attacks on the ML-based xApp and a non-ML based heuristic. We finally develop and demonstrate robust training-based defenses against the challenging physical/jamming-based attacks and show a 15% improvement in the coverage rates when compared to employing no defense over a range of noise budgets","sentences":["Adversarial machine learning, focused on studying various attacks and defenses on machine learning (ML) models, is rapidly gaining importance as ML is increasingly being adopted for optimizing wireless systems such as Open Radio Access Networks (O-RAN).","A comprehensive modeling of the security threats and the demonstration of adversarial attacks and defenses on practical AI based O-RAN systems is still in its nascent stages.","We begin by conducting threat modeling to pinpoint attack surfaces in O-RAN using an ML-based Connection management application (xApp) as an example.","The xApp uses a Graph Neural Network trained using Deep Reinforcement Learning and achieves on average 54% improvement in the coverage rate measured as the 5th percentile user data rates.","We then formulate and demonstrate evasion attacks that degrade the coverage rates by as much as 50% through injecting bounded noise at different threat surfaces including the open wireless medium itself.","Crucially, we also compare and contrast the effectiveness of such attacks on the ML-based xApp and a non-ML based heuristic.","We finally develop and demonstrate robust training-based defenses against the challenging physical/jamming-based attacks and show a 15% improvement in the coverage rates when compared to employing no defense over a range of noise budgets"],"url":"http://arxiv.org/abs/2405.03891v1","category":"cs.CR"}
{"created":"2024-05-06 21:57:35","title":"Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer","abstract":"Motivated by the huge success of Transformers in the field of natural language processing (NLP), Vision Transformers (ViTs) have been rapidly developed and achieved remarkable performance in various computer vision tasks. However, their huge model sizes and intensive computations hinder ViTs' deployment on embedded devices, calling for effective model compression methods, such as quantization. Unfortunately, due to the existence of hardware-unfriendly and quantization-sensitive non-linear operations, particularly {Softmax}, it is non-trivial to completely quantize all operations in ViTs, yielding either significant accuracy drops or non-negligible hardware costs. In response to challenges associated with \\textit{standard ViTs}, we focus our attention towards the quantization and acceleration for \\textit{efficient ViTs}, which not only eliminate the troublesome Softmax but also integrate linear attention with low computational complexity, and propose \\emph{Trio-ViT} accordingly. Specifically, at the algorithm level, we develop a {tailored post-training quantization engine} taking the unique activation distributions of Softmax-free efficient ViTs into full consideration, aiming to boost quantization accuracy. Furthermore, at the hardware level, we build an accelerator dedicated to the specific Convolution-Transformer hybrid architecture of efficient ViTs, thereby enhancing hardware efficiency. Extensive experimental results consistently prove the effectiveness of our Trio-ViT framework. {Particularly, we can gain up to $\\uparrow$$\\mathbf{7.2}\\times$ and $\\uparrow$$\\mathbf{14.6}\\times$ FPS under comparable accuracy over state-of-the-art ViT accelerators, as well as $\\uparrow$$\\mathbf{5.9}\\times$ and $\\uparrow$$\\mathbf{2.0}\\times$ DSP efficiency.} Codes will be released publicly upon acceptance.","sentences":["Motivated by the huge success of Transformers in the field of natural language processing (NLP), Vision Transformers (ViTs) have been rapidly developed and achieved remarkable performance in various computer vision tasks.","However, their huge model sizes and intensive computations hinder ViTs' deployment on embedded devices, calling for effective model compression methods, such as quantization.","Unfortunately, due to the existence of hardware-unfriendly and quantization-sensitive non-linear operations, particularly {Softmax}, it is non-trivial to completely quantize all operations in ViTs, yielding either significant accuracy drops or non-negligible hardware costs.","In response to challenges associated with \\textit{standard ViTs}, we focus our attention towards the quantization and acceleration for \\textit{efficient ViTs}, which not only eliminate the troublesome Softmax but also integrate linear attention with low computational complexity, and propose \\emph{Trio-ViT} accordingly.","Specifically, at the algorithm level, we develop a {tailored post-training quantization engine} taking the unique activation distributions of Softmax-free efficient ViTs into full consideration, aiming to boost quantization accuracy.","Furthermore, at the hardware level, we build an accelerator dedicated to the specific Convolution-Transformer hybrid architecture of efficient ViTs, thereby enhancing hardware efficiency.","Extensive experimental results consistently prove the effectiveness of our Trio-ViT framework.","{Particularly, we can gain up to $\\uparrow$$\\mathbf{7.2}\\times$ and $\\uparrow$$\\mathbf{14.6}\\times$ FPS under comparable accuracy over state-of-the-art ViT accelerators, as well as $\\uparrow$$\\mathbf{5.9}\\times$ and $\\uparrow$$\\mathbf{2.0}\\times$ DSP efficiency.}","Codes will be released publicly upon acceptance."],"url":"http://arxiv.org/abs/2405.03882v1","category":"cs.CV"}
{"created":"2024-05-06 21:39:25","title":"Investigating Personalized Driving Behaviors in Dilemma Zones: Analysis and Prediction of Stop-or-Go Decisions","abstract":"Dilemma zones at signalized intersections present a commonly occurring but unsolved challenge for both drivers and traffic operators. Onsets of the yellow lights prompt varied responses from different drivers: some may brake abruptly, compromising the ride comfort, while others may accelerate, increasing the risk of red-light violations and potential safety hazards. Such diversity in drivers' stop-or-go decisions may result from not only surrounding traffic conditions, but also personalized driving behaviors. To this end, identifying personalized driving behaviors and integrating them into advanced driver assistance systems (ADAS) to mitigate the dilemma zone problem presents an intriguing scientific question. In this study, we employ a game engine-based (i.e., CARLA-enabled) driving simulator to collect high-resolution vehicle trajectories, incoming traffic signal phase and timing information, and stop-or-go decisions from four subject drivers in various scenarios. This approach allows us to analyze personalized driving behaviors in dilemma zones and develop a Personalized Transformer Encoder to predict individual drivers' stop-or-go decisions. The results show that the Personalized Transformer Encoder improves the accuracy of predicting driver decision-making in the dilemma zone by 3.7% to 12.6% compared to the Generic Transformer Encoder, and by 16.8% to 21.6% over the binary logistic regression model.","sentences":["Dilemma zones at signalized intersections present a commonly occurring but unsolved challenge for both drivers and traffic operators.","Onsets of the yellow lights prompt varied responses from different drivers: some may brake abruptly, compromising the ride comfort, while others may accelerate, increasing the risk of red-light violations and potential safety hazards.","Such diversity in drivers' stop-or-go decisions may result from not only surrounding traffic conditions, but also personalized driving behaviors.","To this end, identifying personalized driving behaviors and integrating them into advanced driver assistance systems (ADAS) to mitigate the dilemma zone problem presents an intriguing scientific question.","In this study, we employ a game engine-based (i.e., CARLA-enabled) driving simulator to collect high-resolution vehicle trajectories, incoming traffic signal phase and timing information, and stop-or-go decisions from four subject drivers in various scenarios.","This approach allows us to analyze personalized driving behaviors in dilemma zones and develop a Personalized Transformer Encoder to predict individual drivers' stop-or-go decisions.","The results show that the Personalized Transformer Encoder improves the accuracy of predicting driver decision-making in the dilemma zone by 3.7% to 12.6% compared to the Generic Transformer Encoder, and by 16.8% to 21.6% over the binary logistic regression model."],"url":"http://arxiv.org/abs/2405.03873v1","category":"cs.AI"}
{"created":"2024-05-06 21:36:45","title":"AI-Driven Frameworks for Enhancing Data Quality in Big Data Ecosystems: Error_Detection, Correction, and Metadata Integration","abstract":"The widespread adoption of big data has ushered in a new era of data-driven decision-making, transforming numerous industries and sectors. However, the efficacy of these decisions hinges on the quality of the underlying data. Poor data quality can result in inaccurate analyses and deceptive conclusions. Managing the vast volume, velocity, and variety of data sources presents significant challenges, heightening the importance of addressing big data quality issues. While there has been increased attention from both academia and industry, current approaches often lack comprehensiveness and universality. They tend to focus on limited metrics, neglecting other dimensions of data quality. Moreover, existing methods are often context-specific, limiting their applicability across different domains. There is a clear need for intelligent, automated approaches leveraging artificial intelligence (AI) for advanced data quality corrections.   To bridge these gaps, this Ph.D. thesis proposes a novel set of interconnected frameworks aimed at enhancing big data quality comprehensively. Firstly, we introduce new quality metrics and a weighted scoring system for precise data quality assessment. Secondly, we present a generic framework for detecting various quality anomalies using AI models. Thirdly, we propose an innovative framework for correcting detected anomalies through predictive modeling. Additionally, we address metadata quality enhancement within big data ecosystems. These frameworks are rigorously tested on diverse datasets, demonstrating their efficacy in improving big data quality. Finally, the thesis concludes with insights and suggestions for future research directions.","sentences":["The widespread adoption of big data has ushered in a new era of data-driven decision-making, transforming numerous industries and sectors.","However, the efficacy of these decisions hinges on the quality of the underlying data.","Poor data quality can result in inaccurate analyses and deceptive conclusions.","Managing the vast volume, velocity, and variety of data sources presents significant challenges, heightening the importance of addressing big data quality issues.","While there has been increased attention from both academia and industry, current approaches often lack comprehensiveness and universality.","They tend to focus on limited metrics, neglecting other dimensions of data quality.","Moreover, existing methods are often context-specific, limiting their applicability across different domains.","There is a clear need for intelligent, automated approaches leveraging artificial intelligence (AI) for advanced data quality corrections.   ","To bridge these gaps, this Ph.D. thesis proposes a novel set of interconnected frameworks aimed at enhancing big data quality comprehensively.","Firstly, we introduce new quality metrics and a weighted scoring system for precise data quality assessment.","Secondly, we present a generic framework for detecting various quality anomalies using AI models.","Thirdly, we propose an innovative framework for correcting detected anomalies through predictive modeling.","Additionally, we address metadata quality enhancement within big data ecosystems.","These frameworks are rigorously tested on diverse datasets, demonstrating their efficacy in improving big data quality.","Finally, the thesis concludes with insights and suggestions for future research directions."],"url":"http://arxiv.org/abs/2405.03870v1","category":"cs.AI"}
{"created":"2024-05-06 21:34:46","title":"Outlier Gradient Analysis: Efficiently Improving Deep Learning Model Performance via Hessian-Free Influence Functions","abstract":"Influence functions offer a robust framework for assessing the impact of each training data sample on model predictions, serving as a prominent tool in data-centric learning. Despite their widespread use in various tasks, the strong convexity assumption on the model and the computational cost associated with calculating the inverse of the Hessian matrix pose constraints, particularly when analyzing large deep models. This paper focuses on a classical data-centric scenario--trimming detrimental samples--and addresses both challenges within a unified framework. Specifically, we establish an equivalence transformation between identifying detrimental training samples via influence functions and outlier gradient detection. This transformation not only presents a straightforward and Hessian-free formulation but also provides profound insights into the role of the gradient in sample impact. Moreover, it relaxes the convexity assumption of influence functions, extending their applicability to non-convex deep models. Through systematic empirical evaluations, we first validate the correctness of our proposed outlier gradient analysis on synthetic datasets and then demonstrate its effectiveness in detecting mislabeled samples in vision models, selecting data samples for improving performance of transformer models for natural language processing, and identifying influential samples for fine-tuned Large Language Models.","sentences":["Influence functions offer a robust framework for assessing the impact of each training data sample on model predictions, serving as a prominent tool in data-centric learning.","Despite their widespread use in various tasks, the strong convexity assumption on the model and the computational cost associated with calculating the inverse of the Hessian matrix pose constraints, particularly when analyzing large deep models.","This paper focuses on a classical data-centric scenario--trimming detrimental samples--and addresses both challenges within a unified framework.","Specifically, we establish an equivalence transformation between identifying detrimental training samples via influence functions and outlier gradient detection.","This transformation not only presents a straightforward and Hessian-free formulation but also provides profound insights into the role of the gradient in sample impact.","Moreover, it relaxes the convexity assumption of influence functions, extending their applicability to non-convex deep models.","Through systematic empirical evaluations, we first validate the correctness of our proposed outlier gradient analysis on synthetic datasets and then demonstrate its effectiveness in detecting mislabeled samples in vision models, selecting data samples for improving performance of transformer models for natural language processing, and identifying influential samples for fine-tuned Large Language Models."],"url":"http://arxiv.org/abs/2405.03869v1","category":"cs.LG"}
{"created":"2024-05-06 21:25:51","title":"Information-driven Affordance Discovery for Efficient Robotic Manipulation","abstract":"Robotic affordances, providing information about what actions can be taken in a given situation, can aid robotic manipulation. However, learning about affordances requires expensive large annotated datasets of interactions or demonstrations. In this work, we argue that well-directed interactions with the environment can mitigate this problem and propose an information-based measure to augment the agent's objective and accelerate the affordance discovery process. We provide a theoretical justification of our approach and we empirically validate the approach both in simulation and real-world tasks. Our method, which we dub IDA, enables the efficient discovery of visual affordances for several action primitives, such as grasping, stacking objects, or opening drawers, strongly improving data efficiency in simulation, and it allows us to learn grasping affordances in a small number of interactions, on a real-world setup with a UFACTORY XArm 6 robot arm.","sentences":["Robotic affordances, providing information about what actions can be taken in a given situation, can aid robotic manipulation.","However, learning about affordances requires expensive large annotated datasets of interactions or demonstrations.","In this work, we argue that well-directed interactions with the environment can mitigate this problem and propose an information-based measure to augment the agent's objective and accelerate the affordance discovery process.","We provide a theoretical justification of our approach and we empirically validate the approach both in simulation and real-world tasks.","Our method, which we dub IDA, enables the efficient discovery of visual affordances for several action primitives, such as grasping, stacking objects, or opening drawers, strongly improving data efficiency in simulation, and it allows us to learn grasping affordances in a small number of interactions, on a real-world setup with a UFACTORY XArm 6 robot arm."],"url":"http://arxiv.org/abs/2405.03865v1","category":"cs.RO"}
{"created":"2024-05-06 21:24:22","title":"Learning Planning Abstractions from Language","abstract":"This paper presents a framework for learning state and action abstractions in sequential decision-making domains. Our framework, planning abstraction from language (PARL), utilizes language-annotated demonstrations to automatically discover a symbolic and abstract action space and induce a latent state abstraction based on it. PARL consists of three stages: 1) recovering object-level and action concepts, 2) learning state abstractions, abstract action feasibility, and transition models, and 3) applying low-level policies for abstract actions. During inference, given the task description, PARL first makes abstract action plans using the latent transition and feasibility functions, then refines the high-level plan using low-level policies. PARL generalizes across scenarios involving novel object instances and environments, unseen concept compositions, and tasks that require longer planning horizons than settings it is trained on.","sentences":["This paper presents a framework for learning state and action abstractions in sequential decision-making domains.","Our framework, planning abstraction from language (PARL), utilizes language-annotated demonstrations to automatically discover a symbolic and abstract action space and induce a latent state abstraction based on it.","PARL consists of three stages: 1) recovering object-level and action concepts, 2) learning state abstractions, abstract action feasibility, and transition models, and 3) applying low-level policies for abstract actions.","During inference, given the task description, PARL first makes abstract action plans using the latent transition and feasibility functions, then refines the high-level plan using low-level policies.","PARL generalizes across scenarios involving novel object instances and environments, unseen concept compositions, and tasks that require longer planning horizons than settings it is trained on."],"url":"http://arxiv.org/abs/2405.03864v1","category":"cs.RO"}
{"created":"2024-05-06 21:20:48","title":"Resource Optimization in UAV-assisted IoT Networks: The Role of Generative AI","abstract":"We investigate how generative Artificial Intelligence (AI) can be used to optimize resources in Unmanned Aerial Vehicle (UAV)-assisted Internet of Things (IoT) networks. In particular, generative AI models for real-time decision-making have been used in public safety scenarios. This work describes how generative AI models can improve resource management within UAV-assisted networks. Furthermore, this work presents generative AI in UAV-assisted networks to demonstrate its practical applications and highlight its broader capabilities. We demonstrate a real-life case study for public safety, demonstrating how generative AI can enhance real-time decision-making and improve training datasets. By leveraging generative AI in UAV- assisted networks, we can design more intelligent, adaptive, and efficient ecosystems to meet the evolving demands of wireless networks and diverse applications. Finally, we discuss challenges and future research directions associated with generative AI for resource optimization in UAV-assisted networks.","sentences":["We investigate how generative Artificial Intelligence (AI) can be used to optimize resources in Unmanned Aerial Vehicle (UAV)-assisted Internet of Things (IoT) networks.","In particular, generative AI models for real-time decision-making have been used in public safety scenarios.","This work describes how generative AI models can improve resource management within UAV-assisted networks.","Furthermore, this work presents generative AI in UAV-assisted networks to demonstrate its practical applications and highlight its broader capabilities.","We demonstrate a real-life case study for public safety, demonstrating how generative AI can enhance real-time decision-making and improve training datasets.","By leveraging generative AI in UAV- assisted networks, we can design more intelligent, adaptive, and efficient ecosystems to meet the evolving demands of wireless networks and diverse applications.","Finally, we discuss challenges and future research directions associated with generative AI for resource optimization in UAV-assisted networks."],"url":"http://arxiv.org/abs/2405.03863v1","category":"eess.SY"}
{"created":"2024-05-06 21:20:35","title":"Conformity, Confabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Collaboration","abstract":"This study explores the sources of instability in maintaining cultural personas and opinions within multi-agent LLM systems. Drawing on simulations of inter-cultural collaboration and debate, we analyze agents' pre- and post-discussion private responses alongside chat transcripts to assess the stability of cultural personas and the impact of opinion diversity on group outcomes. Our findings suggest that multi-agent discussions can encourage collective decisions that reflect diverse perspectives, yet this benefit is tempered by the agents' susceptibility to conformity due to perceived peer pressure and challenges in maintaining consistent personas and opinions. Counterintuitively, instructions that encourage debate in support of one's opinions increase the rate of inconstancy. Without addressing the factors we identify, the full potential of multi-agent frameworks for producing more culturally diverse AI outputs will remain untapped.","sentences":["This study explores the sources of instability in maintaining cultural personas and opinions within multi-agent LLM systems.","Drawing on simulations of inter-cultural collaboration and debate, we analyze agents' pre- and post-discussion private responses alongside chat transcripts to assess the stability of cultural personas and the impact of opinion diversity on group outcomes.","Our findings suggest that multi-agent discussions can encourage collective decisions that reflect diverse perspectives, yet this benefit is tempered by the agents' susceptibility to conformity due to perceived peer pressure and challenges in maintaining consistent personas and opinions.","Counterintuitively, instructions that encourage debate in support of one's opinions increase the rate of inconstancy.","Without addressing the factors we identify, the full potential of multi-agent frameworks for producing more culturally diverse AI outputs will remain untapped."],"url":"http://arxiv.org/abs/2405.03862v1","category":"cs.AI"}
{"created":"2024-05-06 21:04:06","title":"Strategies for Increasing Corporate Responsible AI Prioritization","abstract":"Responsible artificial intelligence (RAI) is increasingly recognized as a critical concern. However, the level of corporate RAI prioritization has not kept pace. In this work, we conduct 16 semi-structured interviews with practitioners to investigate what has historically motivated companies to increase the prioritization of RAI. What emerges is a complex story of conflicting and varied factors, but we bring structure to the narrative by highlighting the different strategies available to employ, and point to the actors with access to each. While there are no guaranteed steps for increasing RAI prioritization, we paint the current landscape of motivators so that practitioners can learn from each other, and put forth our own selection of promising directions forward.","sentences":["Responsible artificial intelligence (RAI) is increasingly recognized as a critical concern.","However, the level of corporate RAI prioritization has not kept pace.","In this work, we conduct 16 semi-structured interviews with practitioners to investigate what has historically motivated companies to increase the prioritization of RAI.","What emerges is a complex story of conflicting and varied factors, but we bring structure to the narrative by highlighting the different strategies available to employ, and point to the actors with access to each.","While there are no guaranteed steps for increasing RAI prioritization, we paint the current landscape of motivators so that practitioners can learn from each other, and put forth our own selection of promising directions forward."],"url":"http://arxiv.org/abs/2405.03855v1","category":"cs.CY"}
{"created":"2024-05-06 20:59:45","title":"VSA4VQA: Scaling a Vector Symbolic Architecture to Visual Question Answering on Natural Images","abstract":"While Vector Symbolic Architectures (VSAs) are promising for modelling spatial cognition, their application is currently limited to artificially generated images and simple spatial queries. We propose VSA4VQA - a novel 4D implementation of VSAs that implements a mental representation of natural images for the challenging task of Visual Question Answering (VQA). VSA4VQA is the first model to scale a VSA to complex spatial queries. Our method is based on the Semantic Pointer Architecture (SPA) to encode objects in a hyperdimensional vector space. To encode natural images, we extend the SPA to include dimensions for object's width and height in addition to their spatial location. To perform spatial queries we further introduce learned spatial query masks and integrate a pre-trained vision-language model for answering attribute-related questions. We evaluate our method on the GQA benchmark dataset and show that it can effectively encode natural images, achieving competitive performance to state-of-the-art deep learning methods for zero-shot VQA.","sentences":["While Vector Symbolic Architectures (VSAs) are promising for modelling spatial cognition, their application is currently limited to artificially generated images and simple spatial queries.","We propose VSA4VQA - a novel 4D implementation of VSAs that implements a mental representation of natural images for the challenging task of Visual Question Answering (VQA).","VSA4VQA is the first model to scale a VSA to complex spatial queries.","Our method is based on the Semantic Pointer Architecture (SPA) to encode objects in a hyperdimensional vector space.","To encode natural images, we extend the SPA to include dimensions for object's width and height in addition to their spatial location.","To perform spatial queries we further introduce learned spatial query masks and integrate a pre-trained vision-language model for answering attribute-related questions.","We evaluate our method on the GQA benchmark dataset and show that it can effectively encode natural images, achieving competitive performance to state-of-the-art deep learning methods for zero-shot VQA."],"url":"http://arxiv.org/abs/2405.03852v1","category":"cs.CV"}
{"created":"2024-05-06 20:50:17","title":"Self-Improving Customer Review Response Generation Based on LLMs","abstract":"Previous studies have demonstrated that proactive interaction with user reviews has a positive impact on the perception of app users and encourages them to submit revised ratings. Nevertheless, developers encounter challenges in managing a high volume of reviews, particularly in the case of popular apps with a substantial influx of daily reviews. Consequently, there is a demand for automated solutions aimed at streamlining the process of responding to user reviews. To address this, we have developed a new system for generating automatic responses by leveraging user-contributed documents with the help of retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs). Our solution, named SCRABLE, represents an adaptive customer review response automation that enhances itself with self-optimizing prompts and a judging mechanism based on LLMs. Additionally, we introduce an automatic scoring mechanism that mimics the role of a human evaluator to assess the quality of responses generated in customer review domains. Extensive experiments and analyses conducted on real-world datasets reveal that our method is effective in producing high-quality responses, yielding improvement of more than 8.5% compared to the baseline. Further validation through manual examination of the generated responses underscores the efficacy our proposed system.","sentences":["Previous studies have demonstrated that proactive interaction with user reviews has a positive impact on the perception of app users and encourages them to submit revised ratings.","Nevertheless, developers encounter challenges in managing a high volume of reviews, particularly in the case of popular apps with a substantial influx of daily reviews.","Consequently, there is a demand for automated solutions aimed at streamlining the process of responding to user reviews.","To address this, we have developed a new system for generating automatic responses by leveraging user-contributed documents with the help of retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs).","Our solution, named SCRABLE, represents an adaptive customer review response automation that enhances itself with self-optimizing prompts and a judging mechanism based on LLMs.","Additionally, we introduce an automatic scoring mechanism that mimics the role of a human evaluator to assess the quality of responses generated in customer review domains.","Extensive experiments and analyses conducted on real-world datasets reveal that our method is effective in producing high-quality responses, yielding improvement of more than 8.5% compared to the baseline.","Further validation through manual examination of the generated responses underscores the efficacy our proposed system."],"url":"http://arxiv.org/abs/2405.03845v1","category":"cs.CL"}
{"created":"2024-05-06 20:44:58","title":"A Novel Cross-band CSI Prediction Scheme for Multi-band Fingerprint based Localization","abstract":"Because of the advantages of computation complexity compared with traditional localization algorithms, fingerprint based localization is getting increasing demand. Expanding the fingerprint database from the frequency domain by channel reconstruction can improve localization accuracy. However, in a mobility environment, the channel reconstruction accuracy is limited by the time-varying parameters. In this paper, we proposed a system to extract the time-varying parameters based on space-alternating generalized expectation maximization (SAGE) algorithm, then used variational auto-encoder (VAE) to reconstruct the channel state information on another channel. The proposed scheme is tested on the data generated by the deep-MIMO channel model. Mathematical analysis for the viability of our system is also shown in this paper.","sentences":["Because of the advantages of computation complexity compared with traditional localization algorithms, fingerprint based localization is getting increasing demand.","Expanding the fingerprint database from the frequency domain by channel reconstruction can improve localization accuracy.","However, in a mobility environment, the channel reconstruction accuracy is limited by the time-varying parameters.","In this paper, we proposed a system to extract the time-varying parameters based on space-alternating generalized expectation maximization (SAGE) algorithm, then used variational auto-encoder (VAE) to reconstruct the channel state information on another channel.","The proposed scheme is tested on the data generated by the deep-MIMO channel model.","Mathematical analysis for the viability of our system is also shown in this paper."],"url":"http://arxiv.org/abs/2405.03842v1","category":"cs.NI"}
{"created":"2024-05-06 20:30:14","title":"Guylingo: The Republic of Guyana Creole Corpora","abstract":"While major languages often enjoy substantial attention and resources, the linguistic diversity across the globe encompasses a multitude of smaller, indigenous, and regional languages that lack the same level of computational support. One such region is the Caribbean. While commonly labeled as \"English speaking\", the ex-British Caribbean region consists of a myriad of Creole languages thriving alongside English. In this paper, we present Guylingo: a comprehensive corpus designed for advancing NLP research in the domain of Creolese (Guyanese English-lexicon Creole), the most widely spoken language in the culturally rich nation of Guyana. We first outline our framework for gathering and digitizing this diverse corpus, inclusive of colloquial expressions, idioms, and regional variations in a low-resource language. We then demonstrate the challenges of training and evaluating NLP models for machine translation in Creole. Lastly, we discuss the unique opportunities presented by recent NLP advancements for accelerating the formal adoption of Creole languages as official languages in the Caribbean.","sentences":["While major languages often enjoy substantial attention and resources, the linguistic diversity across the globe encompasses a multitude of smaller, indigenous, and regional languages that lack the same level of computational support.","One such region is the Caribbean.","While commonly labeled as \"English speaking\", the ex-British Caribbean region consists of a myriad of Creole languages thriving alongside English.","In this paper, we present Guylingo: a comprehensive corpus designed for advancing NLP research in the domain of Creolese (Guyanese English-lexicon Creole), the most widely spoken language in the culturally rich nation of Guyana.","We first outline our framework for gathering and digitizing this diverse corpus, inclusive of colloquial expressions, idioms, and regional variations in a low-resource language.","We then demonstrate the challenges of training and evaluating NLP models for machine translation in Creole.","Lastly, we discuss the unique opportunities presented by recent NLP advancements for accelerating the formal adoption of Creole languages as official languages in the Caribbean."],"url":"http://arxiv.org/abs/2405.03832v1","category":"cs.CL"}
{"created":"2024-05-06 20:20:06","title":"Unsupervised Machine Learning Identifies Latent Ultradian States in Multi-Modal Wearable Sensor Signals","abstract":"Wearable sensors such as smartwatches have become ubiquitous in recent years, allowing the easy and continual measurement of physiological parameters such as heart rate, physical activity, body temperature, and blood glucose in an every-day setting. This multi-modal data offers the potential to identify latent states occurring across physiological measures, which may represent important bio-behavioural states that could not be observed in any single measure. Here we present an approach, utilising a hidden semi-Markov model, to identify such states in data collected using a smartwatch, electrocardiogram, and blood glucose monitor, over two weeks from a sample of 9 participants. We found 26 latent ultradian states across the sample, with many occurring at particular times of day. Here we describe some of these, as well as their association with subjective mood and time use diaries. These methods provide a novel avenue for developing insights into the physiology of everyday life.","sentences":["Wearable sensors such as smartwatches have become ubiquitous in recent years, allowing the easy and continual measurement of physiological parameters such as heart rate, physical activity, body temperature, and blood glucose in an every-day setting.","This multi-modal data offers the potential to identify latent states occurring across physiological measures, which may represent important bio-behavioural states that could not be observed in any single measure.","Here we present an approach, utilising a hidden semi-Markov model, to identify such states in data collected using a smartwatch, electrocardiogram, and blood glucose monitor, over two weeks from a sample of 9 participants.","We found 26 latent ultradian states across the sample, with many occurring at particular times of day.","Here we describe some of these, as well as their association with subjective mood and time use diaries.","These methods provide a novel avenue for developing insights into the physiology of everyday life."],"url":"http://arxiv.org/abs/2405.03829v1","category":"q-bio.NC"}
{"created":"2024-05-06 20:15:45","title":"Organizing a Society of Language Models: Structures and Mechanisms for Enhanced Collective Intelligence","abstract":"Recent developments in Large Language Models (LLMs) have significantly expanded their applications across various domains. However, the effectiveness of LLMs is often constrained when operating individually in complex environments. This paper introduces a transformative approach by organizing LLMs into community-based structures, aimed at enhancing their collective intelligence and problem-solving capabilities. We investigate different organizational models-hierarchical, flat, dynamic, and federated-each presenting unique benefits and challenges for collaborative AI systems. Within these structured communities, LLMs are designed to specialize in distinct cognitive tasks, employ advanced interaction mechanisms such as direct communication, voting systems, and market-based approaches, and dynamically adjust their governance structures to meet changing demands. The implementation of such communities holds substantial promise for improve problem-solving capabilities in AI, prompting an in-depth examination of their ethical considerations, management strategies, and scalability potential. This position paper seeks to lay the groundwork for future research, advocating a paradigm shift from isolated to synergistic operational frameworks in AI research and application.","sentences":["Recent developments in Large Language Models (LLMs) have significantly expanded their applications across various domains.","However, the effectiveness of LLMs is often constrained when operating individually in complex environments.","This paper introduces a transformative approach by organizing LLMs into community-based structures, aimed at enhancing their collective intelligence and problem-solving capabilities.","We investigate different organizational models-hierarchical, flat, dynamic, and federated-each presenting unique benefits and challenges for collaborative AI systems.","Within these structured communities, LLMs are designed to specialize in distinct cognitive tasks, employ advanced interaction mechanisms such as direct communication, voting systems, and market-based approaches, and dynamically adjust their governance structures to meet changing demands.","The implementation of such communities holds substantial promise for improve problem-solving capabilities in AI, prompting an in-depth examination of their ethical considerations, management strategies, and scalability potential.","This position paper seeks to lay the groundwork for future research, advocating a paradigm shift from isolated to synergistic operational frameworks in AI research and application."],"url":"http://arxiv.org/abs/2405.03825v1","category":"cs.AI"}
{"created":"2024-05-06 20:07:45","title":"Breaking Barriers: Investigating the Sense of Belonging Among Women and Non-Binary Students in Software Engineering","abstract":"Women in computing were among the first programmers in the early 20th century and were substantial contributors to the industry. Today, men dominate the software engineering industry. Research and data show that women are far less likely to pursue a career in this industry, and those that do are less likely than men to stay in it. Reasons for women and other underrepresented minorities to leave the industry are a lack of opportunities for growth and advancement, unfair treatment and workplace culture. This research explores how the potential to cultivate or uphold an industry unfavourable to women and non-binary individuals manifests in software engineering education at the university level. For this purpose, the study includes surveys and interviews. We use gender name perception as a survey instrument, and the results show small differences in perceptions of software engineering students based on their gender. Particularly, the survey respondents anchor the values of the male software engineer (Hans) to a variety of technical and non-technical skills, while the same description for a female software engineer (Hanna) is anchored mainly by her managerial skills. With interviews with women and non-binary students, we gain insight on the main barriers to their sense of ambient belonging. The collected data shows that some known barriers from the literature such as tokenism, and stereotype threat, do still exist. However, we find positive factors such as role models and encouragement that strengthen the sense of belonging among these students.","sentences":["Women in computing were among the first programmers in the early 20th century and were substantial contributors to the industry.","Today, men dominate the software engineering industry.","Research and data show that women are far less likely to pursue a career in this industry, and those that do are less likely than men to stay in it.","Reasons for women and other underrepresented minorities to leave the industry are a lack of opportunities for growth and advancement, unfair treatment and workplace culture.","This research explores how the potential to cultivate or uphold an industry unfavourable to women and non-binary individuals manifests in software engineering education at the university level.","For this purpose, the study includes surveys and interviews.","We use gender name perception as a survey instrument, and the results show small differences in perceptions of software engineering students based on their gender.","Particularly, the survey respondents anchor the values of the male software engineer (Hans) to a variety of technical and non-technical skills, while the same description for a female software engineer (Hanna) is anchored mainly by her managerial skills.","With interviews with women and non-binary students, we gain insight on the main barriers to their sense of ambient belonging.","The collected data shows that some known barriers from the literature such as tokenism, and stereotype threat, do still exist.","However, we find positive factors such as role models and encouragement that strengthen the sense of belonging among these students."],"url":"http://arxiv.org/abs/2405.03824v1","category":"cs.SE"}
{"created":"2024-05-06 20:04:53","title":"Thoughtful Things: Building Human-Centric Smart Devices with Small Language Models","abstract":"Everyday devices like light bulbs and kitchen appliances are now embedded with so many features and automated behaviors that they have become complicated to actually use. While such \"smart\" capabilities can better support users' goals, the task of learning the \"ins and outs\" of different devices is daunting. Voice assistants aim to solve this problem by providing a natural language interface to devices, yet such assistants cannot understand loosely-constrained commands, they lack the ability to reason about and explain devices' behaviors to users, and they rely on connectivity to intrusive cloud infrastructure. Toward addressing these issues, we propose thoughtful things: devices that leverage lightweight, on-device language models to take actions and explain their behaviors in response to unconstrained user commands. We propose an end-to-end framework that leverages formal modeling, automated training data synthesis, and generative language models to create devices that are both capable and thoughtful in the presence of unconstrained user goals and inquiries. Our framework requires no labeled data and can be deployed on-device, with no cloud dependency. We implement two thoughtful things (a lamp and a thermostat) and deploy them on real hardware, evaluating their practical performance.","sentences":["Everyday devices like light bulbs and kitchen appliances are now embedded with so many features and automated behaviors that they have become complicated to actually use.","While such \"smart\" capabilities can better support users' goals, the task of learning the \"ins and outs\" of different devices is daunting.","Voice assistants aim to solve this problem by providing a natural language interface to devices, yet such assistants cannot understand loosely-constrained commands, they lack the ability to reason about and explain devices' behaviors to users, and they rely on connectivity to intrusive cloud infrastructure.","Toward addressing these issues, we propose thoughtful things: devices that leverage lightweight, on-device language models to take actions and explain their behaviors in response to unconstrained user commands.","We propose an end-to-end framework that leverages formal modeling, automated training data synthesis, and generative language models to create devices that are both capable and thoughtful in the presence of unconstrained user goals and inquiries.","Our framework requires no labeled data and can be deployed on-device, with no cloud dependency.","We implement two thoughtful things (a lamp and a thermostat) and deploy them on real hardware, evaluating their practical performance."],"url":"http://arxiv.org/abs/2405.03821v1","category":"cs.HC"}
{"created":"2024-05-06 20:02:07","title":"False Sense of Security in Explainable Artificial Intelligence (XAI)","abstract":"A cautious interpretation of AI regulations and policy in the EU and the USA place explainability as a central deliverable of compliant AI systems. However, from a technical perspective, explainable AI (XAI) remains an elusive and complex target where even state of the art methods often reach erroneous, misleading, and incomplete explanations. \"Explainability\" has multiple meanings which are often used interchangeably, and there are an even greater number of XAI methods - none of which presents a clear edge. Indeed, there are multiple failure modes for each XAI method, which require application-specific development and continuous evaluation. In this paper, we analyze legislative and policy developments in the United States and the European Union, such as the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, the AI Act, the AI Liability Directive, and the General Data Protection Regulation (GDPR) from a right to explanation perspective. We argue that these AI regulations and current market conditions threaten effective AI governance and safety because the objective of trustworthy, accountable, and transparent AI is intrinsically linked to the questionable ability of AI operators to provide meaningful explanations. Unless governments explicitly tackle the issue of explainability through clear legislative and policy statements that take into account technical realities, AI governance risks becoming a vacuous \"box-ticking\" exercise where scientific standards are replaced with legalistic thresholds, providing only a false sense of security in XAI.","sentences":["A cautious interpretation of AI regulations and policy in the EU and the USA place explainability as a central deliverable of compliant AI systems.","However, from a technical perspective, explainable AI (XAI) remains an elusive and complex target where even state of the art methods often reach erroneous, misleading, and incomplete explanations.","\"Explainability\" has multiple meanings which are often used interchangeably, and there are an even greater number of XAI methods - none of which presents a clear edge.","Indeed, there are multiple failure modes for each XAI method, which require application-specific development and continuous evaluation.","In this paper, we analyze legislative and policy developments in the United States and the European Union, such as the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, the AI Act, the AI Liability Directive, and the General Data Protection Regulation (GDPR) from a right to explanation perspective.","We argue that these AI regulations and current market conditions threaten effective AI governance and safety because the objective of trustworthy, accountable, and transparent AI is intrinsically linked to the questionable ability of AI operators to provide meaningful explanations.","Unless governments explicitly tackle the issue of explainability through clear legislative and policy statements that take into account technical realities, AI governance risks becoming a vacuous \"box-ticking\" exercise where scientific standards are replaced with legalistic thresholds, providing only a false sense of security in XAI."],"url":"http://arxiv.org/abs/2405.03820v1","category":"cs.CY"}
{"created":"2024-05-06 19:47:23","title":"SocialFormer: Social Interaction Modeling with Edge-enhanced Heterogeneous Graph Transformers for Trajectory Prediction","abstract":"Accurate trajectory prediction is crucial for ensuring safe and efficient autonomous driving. However, most existing methods overlook complex interactions between traffic participants that often govern their future trajectories. In this paper, we propose SocialFormer, an agent interaction-aware trajectory prediction method that leverages the semantic relationship between the target vehicle and surrounding vehicles by making use of the road topology. We also introduce an edge-enhanced heterogeneous graph transformer (EHGT) as the aggregator in a graph neural network (GNN) to encode the semantic and spatial agent interaction information. Additionally, we introduce a temporal encoder based on gated recurrent units (GRU) to model the temporal social behavior of agent movements. Finally, we present an information fusion framework that integrates agent encoding, lane encoding, and agent interaction encoding for a holistic representation of the traffic scene. We evaluate SocialFormer for the trajectory prediction task on the popular nuScenes benchmark and achieve state-of-the-art performance.","sentences":["Accurate trajectory prediction is crucial for ensuring safe and efficient autonomous driving.","However, most existing methods overlook complex interactions between traffic participants that often govern their future trajectories.","In this paper, we propose SocialFormer, an agent interaction-aware trajectory prediction method that leverages the semantic relationship between the target vehicle and surrounding vehicles by making use of the road topology.","We also introduce an edge-enhanced heterogeneous graph transformer (EHGT) as the aggregator in a graph neural network (GNN) to encode the semantic and spatial agent interaction information.","Additionally, we introduce a temporal encoder based on gated recurrent units (GRU) to model the temporal social behavior of agent movements.","Finally, we present an information fusion framework that integrates agent encoding, lane encoding, and agent interaction encoding for a holistic representation of the traffic scene.","We evaluate SocialFormer for the trajectory prediction task on the popular nuScenes benchmark and achieve state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.03809v1","category":"cs.AI"}
{"created":"2024-05-06 19:33:32","title":"The Future of Office and Administrative Support Occupations in the Era of Artificial Intelligence: A Bibliometric Analysis","abstract":"The U.S. Bureau of Labor Statistics projects that by the year 2029, the United States will lose a million jobs in the office and administrative support occupations because technology, automation, and artificial intelligence (AI) have the potential to substitute or replace the office and administrative functions performed by office workers. Despite the potential impact AI will have on office work and the important role office workers play in the American economy, we have limited knowledge of the state of the art research in office work at the intersection of emerging artificial intelligence technologies. In this study, we conducted a bibliometric analysis of the scholarly literature at the intersection of office work and artificial intelligence. We extracted literature sources from Compendex and Scopus databases and used VOSviewer for visualizing and quantifying our bibliometric analyses. Our findings from keywords analysis indicate that office automation, humans, human-computer interaction, and artificial intelligence occurred more frequently in the scholarly literature and had high link strengths. Keyword clusters from co-occurrence analysis indicate that intelligent buildings, robotics, and the internet of things are emerging topics in the office work domain. The two clusters related to ergonomics, worker characteristics, human performance, and safety indicate the types of human factors concerns that are more widely studied in office work settings. In summary, our findings on the state-of-the-art research in office work indicate that more studies have been conducted on smart buildings, robotics, and technology development for office work, compared to studies on office workers and their professional development.","sentences":["The U.S. Bureau of Labor Statistics projects that by the year 2029, the United States will lose a million jobs in the office and administrative support occupations because technology, automation, and artificial intelligence (AI) have the potential to substitute or replace the office and administrative functions performed by office workers.","Despite the potential impact AI will have on office work and the important role office workers play in the American economy, we have limited knowledge of the state of the art research in office work at the intersection of emerging artificial intelligence technologies.","In this study, we conducted a bibliometric analysis of the scholarly literature at the intersection of office work and artificial intelligence.","We extracted literature sources from Compendex and Scopus databases and used VOSviewer for visualizing and quantifying our bibliometric analyses.","Our findings from keywords analysis indicate that office automation, humans, human-computer interaction, and artificial intelligence occurred more frequently in the scholarly literature and had high link strengths.","Keyword clusters from co-occurrence analysis indicate that intelligent buildings, robotics, and the internet of things are emerging topics in the office work domain.","The two clusters related to ergonomics, worker characteristics, human performance, and safety indicate the types of human factors concerns that are more widely studied in office work settings.","In summary, our findings on the state-of-the-art research in office work indicate that more studies have been conducted on smart buildings, robotics, and technology development for office work, compared to studies on office workers and their professional development."],"url":"http://arxiv.org/abs/2405.03808v1","category":"cs.CY"}
{"created":"2024-05-06 19:28:58","title":"A snapshot review on soft-materials assembly design utilizing machine learning methods","abstract":"Since the surge of data in materials science research and the advancement in machine learning methods, an increasing number of researchers are introducing machine learning techniques into the next generation of materials discovery, ranging from neural-network learned potentials to automated characterization techniques for experimental images. In this snapshot review, we first summarize the landscape of techniques for soft materials assembly design that do not employ machine learning or artificial intelligence and then discuss specific machine-learning and artificial-intelligence-based methods that enhance the design pipeline, such as high-throughput crystal-structure characterization and the inverse design of building blocks for materials assembly and properties. Additionally, we survey the landscape of current developments of scientific software, especially in the context of their compatibility with traditional molecular dynamics engines such as LAMMPS and HOOMD-blue.","sentences":["Since the surge of data in materials science research and the advancement in machine learning methods, an increasing number of researchers are introducing machine learning techniques into the next generation of materials discovery, ranging from neural-network learned potentials to automated characterization techniques for experimental images.","In this snapshot review, we first summarize the landscape of techniques for soft materials assembly design that do not employ machine learning or artificial intelligence and then discuss specific machine-learning and artificial-intelligence-based methods that enhance the design pipeline, such as high-throughput crystal-structure characterization and the inverse design of building blocks for materials assembly and properties.","Additionally, we survey the landscape of current developments of scientific software, especially in the context of their compatibility with traditional molecular dynamics engines such as LAMMPS and HOOMD-blue."],"url":"http://arxiv.org/abs/2405.03805v1","category":"cond-mat.soft"}
{"created":"2024-05-06 19:09:37","title":"Synthetic Data from Diffusion Models Improve Drug Discovery Prediction","abstract":"Artificial intelligence (AI) is increasingly used in every stage of drug development. Continuing breakthroughs in AI-based methods for drug discovery require the creation, improvement, and refinement of drug discovery data. We posit a new data challenge that slows the advancement of drug discovery AI: datasets are often collected independently from each other, often with little overlap, creating data sparsity. Data sparsity makes data curation difficult for researchers looking to answer key research questions requiring values posed across multiple datasets. We propose a novel diffusion GNN model Syngand capable of generating ligand and pharmacokinetic data end-to-end. We show and provide a methodology for sampling pharmacokinetic data for existing ligands using our Syngand model. We show the initial promising results on the efficacy of the Syngand-generated synthetic target property data on downstream regression tasks with AqSolDB, LD50, and hERG central. Using our proposed model and methodology, researchers can easily generate synthetic ligand data to help them explore research questions that require data spanning multiple datasets.","sentences":["Artificial intelligence (AI) is increasingly used in every stage of drug development.","Continuing breakthroughs in AI-based methods for drug discovery require the creation, improvement, and refinement of drug discovery data.","We posit a new data challenge that slows the advancement of drug discovery AI: datasets are often collected independently from each other, often with little overlap, creating data sparsity.","Data sparsity makes data curation difficult for researchers looking to answer key research questions requiring values posed across multiple datasets.","We propose a novel diffusion GNN model Syngand capable of generating ligand and pharmacokinetic data end-to-end.","We show and provide a methodology for sampling pharmacokinetic data for existing ligands using our Syngand model.","We show the initial promising results on the efficacy of the Syngand-generated synthetic target property data on downstream regression tasks with AqSolDB, LD50, and hERG central.","Using our proposed model and methodology, researchers can easily generate synthetic ligand data to help them explore research questions that require data spanning multiple datasets."],"url":"http://arxiv.org/abs/2405.03799v1","category":"cs.LG"}
{"created":"2024-05-06 18:53:44","title":"Prize-Collecting Steiner Tree: A 1.79 Approximation","abstract":"Prize-Collecting Steiner Tree (PCST) is a generalization of the Steiner Tree problem, a fundamental problem in computer science. In the classic Steiner Tree problem, we aim to connect a set of vertices known as terminals using the minimum-weight tree in a given weighted graph. In this generalized version, each vertex has a penalty, and there is flexibility to decide whether to connect each vertex or pay its associated penalty, making the problem more realistic and practical.   Both the Steiner Tree problem and its Prize-Collecting version had long-standing $2$-approximation algorithms, matching the integrality gap of the natural LP formulations for both. This barrier for both problems has been surpassed, with algorithms achieving approximation factors below $2$. While research on the Steiner Tree problem has led to a series of reductions in the approximation ratio below $2$, culminating in a $\\ln(4)+\\epsilon$ approximation by Byrka, Grandoni, Rothvo{\\ss}, and Sanit\\`a, the Prize-Collecting version has not seen improvements in the past 15 years since the work of Archer, Bateni, Hajiaghayi, and Karloff, which reduced the approximation factor for this problem from $2$ to $1.9672$. Interestingly, even the Prize-Collecting TSP approximation, which was first improved below $2$ in the same paper, has seen several advancements since then.   In this paper, we reduce the approximation factor for the PCST problem substantially to 1.7994 via a novel iterative approach.","sentences":["Prize-Collecting Steiner Tree (PCST) is a generalization of the Steiner Tree problem, a fundamental problem in computer science.","In the classic Steiner Tree problem, we aim to connect a set of vertices known as terminals using the minimum-weight tree in a given weighted graph.","In this generalized version, each vertex has a penalty, and there is flexibility to decide whether to connect each vertex or pay its associated penalty, making the problem more realistic and practical.   ","Both the Steiner Tree problem and its Prize-Collecting version had long-standing $2$-approximation algorithms, matching the integrality gap of the natural LP formulations for both.","This barrier for both problems has been surpassed, with algorithms achieving approximation factors below $2$. While research on the Steiner Tree problem has led to a series of reductions in the approximation ratio below $2$, culminating in a $\\ln(4)+\\epsilon$ approximation by Byrka, Grandoni, Rothvo{\\ss}, and Sanit\\`a, the Prize-Collecting version has not seen improvements in the past 15 years since the work of Archer, Bateni, Hajiaghayi, and Karloff, which reduced the approximation factor for this problem from $2$ to $1.9672$. Interestingly, even the Prize-Collecting TSP approximation, which was first improved below $2$ in the same paper, has seen several advancements since then.   ","In this paper, we reduce the approximation factor for the PCST problem substantially to 1.7994 via a novel iterative approach."],"url":"http://arxiv.org/abs/2405.03792v1","category":"cs.DS"}
{"created":"2024-05-06 18:45:18","title":"On Adversarial Examples for Text Classification by Perturbing Latent Representations","abstract":"Recently, with the advancement of deep learning, several applications in text classification have advanced significantly. However, this improvement comes with a cost because deep learning is vulnerable to adversarial examples. This weakness indicates that deep learning is not very robust. Fortunately, the input of a text classifier is discrete. Hence, it can prevent the classifier from state-of-the-art attacks. Nonetheless, previous works have generated black-box attacks that successfully manipulate the discrete values of the input to find adversarial examples. Therefore, instead of changing the discrete values, we transform the input into its embedding vector containing real values to perform the state-of-the-art white-box attacks. Then, we convert the perturbed embedding vector back into a text and name it an adversarial example. In summary, we create a framework that measures the robustness of a text classifier by using the gradients of the classifier.","sentences":["Recently, with the advancement of deep learning, several applications in text classification have advanced significantly.","However, this improvement comes with a cost because deep learning is vulnerable to adversarial examples.","This weakness indicates that deep learning is not very robust.","Fortunately, the input of a text classifier is discrete.","Hence, it can prevent the classifier from state-of-the-art attacks.","Nonetheless, previous works have generated black-box attacks that successfully manipulate the discrete values of the input to find adversarial examples.","Therefore, instead of changing the discrete values, we transform the input into its embedding vector containing real values to perform the state-of-the-art white-box attacks.","Then, we convert the perturbed embedding vector back into a text and name it an adversarial example.","In summary, we create a framework that measures the robustness of a text classifier by using the gradients of the classifier."],"url":"http://arxiv.org/abs/2405.03789v1","category":"cs.LG"}
{"created":"2024-05-06 18:21:41","title":"Interpretable Data Fusion for Distributed Learning: A Representative Approach via Gradient Matching","abstract":"This paper introduces a representative-based approach for distributed learning that transforms multiple raw data points into a virtual representation. Unlike traditional distributed learning methods such as Federated Learning, which do not offer human interpretability, our method makes complex machine learning processes accessible and comprehensible. It achieves this by condensing extensive datasets into digestible formats, thus fostering intuitive human-machine interactions. Additionally, this approach maintains privacy and communication efficiency, and it matches the training performance of models using raw data. Simulation results show that our approach is competitive with or outperforms traditional Federated Learning in accuracy and convergence, especially in scenarios with complex models and a higher number of clients. This framework marks a step forward in integrating human intuition with machine intelligence, which potentially enhances human-machine learning interfaces and collaborative efforts.","sentences":["This paper introduces a representative-based approach for distributed learning that transforms multiple raw data points into a virtual representation.","Unlike traditional distributed learning methods such as Federated Learning, which do not offer human interpretability, our method makes complex machine learning processes accessible and comprehensible.","It achieves this by condensing extensive datasets into digestible formats, thus fostering intuitive human-machine interactions.","Additionally, this approach maintains privacy and communication efficiency, and it matches the training performance of models using raw data.","Simulation results show that our approach is competitive with or outperforms traditional Federated Learning in accuracy and convergence, especially in scenarios with complex models and a higher number of clients.","This framework marks a step forward in integrating human intuition with machine intelligence, which potentially enhances human-machine learning interfaces and collaborative efforts."],"url":"http://arxiv.org/abs/2405.03782v1","category":"cs.LG"}
{"created":"2024-05-06 18:19:01","title":"Is ReLU Adversarially Robust?","abstract":"The efficacy of deep learning models has been called into question by the presence of adversarial examples. Addressing the vulnerability of deep learning models to adversarial examples is crucial for ensuring their continued development and deployment. In this work, we focus on the role of rectified linear unit (ReLU) activation functions in the generation of adversarial examples. ReLU functions are commonly used in deep learning models because they facilitate the training process. However, our empirical analysis demonstrates that ReLU functions are not robust against adversarial examples. We propose a modified version of the ReLU function, which improves robustness against adversarial examples. Our results are supported by an experiment, which confirms the effectiveness of our proposed modification. Additionally, we demonstrate that applying adversarial training to our customized model further enhances its robustness compared to a general model.","sentences":["The efficacy of deep learning models has been called into question by the presence of adversarial examples.","Addressing the vulnerability of deep learning models to adversarial examples is crucial for ensuring their continued development and deployment.","In this work, we focus on the role of rectified linear unit (ReLU) activation functions in the generation of adversarial examples.","ReLU functions are commonly used in deep learning models because they facilitate the training process.","However, our empirical analysis demonstrates that ReLU functions are not robust against adversarial examples.","We propose a modified version of the ReLU function, which improves robustness against adversarial examples.","Our results are supported by an experiment, which confirms the effectiveness of our proposed modification.","Additionally, we demonstrate that applying adversarial training to our customized model further enhances its robustness compared to a general model."],"url":"http://arxiv.org/abs/2405.03777v1","category":"cs.LG"}
{"created":"2024-05-06 18:17:27","title":"Secure Inference for Vertically Partitioned Data Using Multiparty Homomorphic Encryption","abstract":"We propose a secure inference protocol for a distributed setting involving a single server node and multiple client nodes. We assume that the observed data vector is partitioned across multiple client nodes while the deep learning model is located at the server node. Each client node is required to encrypt its portion of the data vector and transmit the resulting ciphertext to the server node. The server node is required to collect the ciphertexts and perform inference in the encrypted domain. We demonstrate an application of multi-party homomorphic encryption (MPHE) to satisfy these requirements. We propose a packing scheme, that enables the server to form the ciphertext of the complete data by aggregating the ciphertext of data subsets encrypted using MPHE. While our proposed protocol builds upon prior horizontal federated training protocol~\\cite{sav2020poseidon}, we focus on the inference for vertically partitioned data and avoid the transmission of (encrypted) model weights from the server node to the client nodes.","sentences":["We propose a secure inference protocol for a distributed setting involving a single server node and multiple client nodes.","We assume that the observed data vector is partitioned across multiple client nodes while the deep learning model is located at the server node.","Each client node is required to encrypt its portion of the data vector and transmit the resulting ciphertext to the server node.","The server node is required to collect the ciphertexts and perform inference in the encrypted domain.","We demonstrate an application of multi-party homomorphic encryption (MPHE) to satisfy these requirements.","We propose a packing scheme, that enables the server to form the ciphertext of the complete data by aggregating the ciphertext of data subsets encrypted using MPHE.","While our proposed protocol builds upon prior horizontal federated training protocol~\\cite{sav2020poseidon}, we focus on the inference for vertically partitioned data and avoid the transmission of (encrypted) model weights from the server node to the client nodes."],"url":"http://arxiv.org/abs/2405.03775v1","category":"cs.CR"}
{"created":"2024-05-06 17:57:58","title":"JWST Observations of Starbursts: Cold Clouds and Plumes Launching in the M82 Outflow","abstract":"In this paper we study the filamentary substructure of 3.3 $\\mu$m PAH emission from JWST/NIRCam observations in the base of the M82 star-burst driven wind. We identify plume-like substructure within the PAH emission with widths of $\\sim$50 pc. Several of the plumes extend to the edge of the field-of-view, and thus are at least 200-300 pc in length. In this region of the outflow, the vast majority ($\\sim$70\\%) of PAH emission is associated with the plumes. We show that those structures contain smaller scale \"clouds\" with widths that are $\\sim$5-15 pc, and they are morphologically similar to the results of \"cloud-crushing\" simulations. We estimate the cloud-crushing time-scales of $\\sim$0.5-3 Myr, depending on assumptions. We show this time scale is consistent with a picture in which these observed PAH clouds survived break-out from the disk rather than being destroyed by the hot wind. The PAH emission in both the midplane and the outflow is shown to tightly correlate with that of Pa$\\alpha$ emission (from HST/NICMOS data), at the scale of both plumes and clouds, though the ratio of PAH-to-Pa$\\alpha$ increases at further distances from the midplane. Finally, we show that the outflow PAH emission is suppressed in regions of the M82 wind that are bright in X-ray emission. Overall, our results are broadly consistent with a picture in which cold gas in galactic outflows is launched via hierarchically structured plumes, and those small scale clouds are more likely to survive the wind environment when collected into the larger plume structure.","sentences":["In this paper we study the filamentary substructure of 3.3 $\\mu$m PAH emission from JWST/NIRCam observations in the base of the M82 star-burst driven wind.","We identify plume-like substructure within the PAH emission with widths of $\\sim$50 pc.","Several of the plumes extend to the edge of the field-of-view, and thus are at least 200-300 pc in length.","In this region of the outflow, the vast majority ($\\sim$70\\%) of PAH emission is associated with the plumes.","We show that those structures contain smaller scale \"clouds\" with widths that are $\\sim$5-15 pc, and they are morphologically similar to the results of \"cloud-crushing\" simulations.","We estimate the cloud-crushing time-scales of $\\sim$0.5-3 Myr, depending on assumptions.","We show this time scale is consistent with a picture in which these observed PAH clouds survived break-out from the disk rather than being destroyed by the hot wind.","The PAH emission in both the midplane and the outflow is shown to tightly correlate with that of Pa$\\alpha$ emission (from HST/NICMOS data), at the scale of both plumes and clouds, though the ratio of PAH-to-Pa$\\alpha$ increases at further distances from the midplane.","Finally, we show that the outflow PAH emission is suppressed in regions of the M82 wind that are bright in X-ray emission.","Overall, our results are broadly consistent with a picture in which cold gas in galactic outflows is launched via hierarchically structured plumes, and those small scale clouds are more likely to survive the wind environment when collected into the larger plume structure."],"url":"http://arxiv.org/abs/2405.03686v1","category":"astro-ph.GA"}
{"created":"2024-05-06 17:57:27","title":"Language-Image Models with 3D Understanding","abstract":"Multi-modal large language models (MLLMs) have shown incredible capabilities in a variety of 2D vision and language tasks. We extend MLLMs' perceptual capabilities to ground and reason about images in 3-dimensional space. To that end, we first develop a large-scale pre-training dataset for 2D and 3D called LV3D by combining multiple existing 2D and 3D recognition datasets under a common task formulation: as multi-turn question-answering. Next, we introduce a new MLLM named Cube-LLM and pre-train it on LV3D. We show that pure data scaling makes a strong 3D perception capability without 3D specific architectural design or training objective. Cube-LLM exhibits intriguing properties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting to improve 3D understanding from 2D context information. (2) Cube-LLM can follow complex and diverse instructions and adapt to versatile input and output formats. (3) Cube-LLM can be visually prompted such as 2D box or a set of candidate 3D boxes from specialists. Our experiments on outdoor benchmarks demonstrate that Cube-LLM significantly outperforms existing baselines by 21.3 points of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7 points on the DriveLM dataset for complex reasoning about driving scenarios, respectively. Cube-LLM also shows competitive results in general MLLM benchmarks such as refCOCO for 2D grounding with (87.0) average score, as well as visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for complex reasoning. Our project is available at https://janghyuncho.github.io/Cube-LLM.","sentences":["Multi-modal large language models (MLLMs) have shown incredible capabilities in a variety of 2D vision and language tasks.","We extend MLLMs' perceptual capabilities to ground and reason about images in 3-dimensional space.","To that end, we first develop a large-scale pre-training dataset for 2D and 3D called LV3D by combining multiple existing 2D and 3D recognition datasets under a common task formulation: as multi-turn question-answering.","Next, we introduce a new MLLM named Cube-LLM and pre-train it on LV3D.","We show that pure data scaling makes a strong 3D perception capability without 3D specific architectural design or training objective.","Cube-LLM exhibits intriguing properties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting to improve 3D understanding from 2D context information.","(2) Cube-LLM can follow complex and diverse instructions and adapt to versatile input and output formats.","(3) Cube-LLM can be visually prompted such as 2D box or a set of candidate 3D boxes from specialists.","Our experiments on outdoor benchmarks demonstrate that Cube-LLM significantly outperforms existing baselines by 21.3 points of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7 points on the DriveLM dataset for complex reasoning about driving scenarios, respectively.","Cube-LLM also shows competitive results in general MLLM benchmarks such as refCOCO for 2D grounding with (87.0) average score, as well as visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for complex reasoning.","Our project is available at https://janghyuncho.github.io/Cube-LLM."],"url":"http://arxiv.org/abs/2405.03685v1","category":"cs.CV"}
{"created":"2024-05-06 17:57:06","title":"All-in-One Deep Learning Framework for MR Image Reconstruction","abstract":"We introduce a novel, all-in-one deep learning framework for MR image reconstruction, enabling a single model to enhance image quality across multiple aspects of k-space sampling and to be effective across a wide range of clinical and technical scenarios. This DICOM-based algorithm serves as the core of SwiftMR (AIRS Medical, Seoul, Korea), which is FDA-cleared, CE-certified, and commercially available. We first detail the comprehensive development process of the model, including data collection, training pair preparation, model architecture design, and DICOM inference. We then assess the model's capability to enhance image quality in a multi-dimensional manner, specifically across various aspects of k-space sampling. Subsequently, we evaluate several features of the multi-dimensional enhancement: the accuracy of tunable denoising, the effectiveness of super-resolution in each encoding direction, and the reduction of artifacts that become more prominent at lower spatial resolutions. Additionally, we assess its compatibility with various scan parameter sets and its generalizability across scanner vendors not seen during training. Finally, we present specific cases demonstrating the model's utility in reducing scan time across anatomical regions in conjunction with protocol optimization. The proposed model is compatible with a broad spectrum of scenarios, including various vendors, pulse sequences, scan parameters, and anatomical regions. Its DICOM-based operation particularly enhances its applicability for real-world applications. Given its demonstrated effectiveness and versatility, we expect its use to expand in the field of clinical MRI.","sentences":["We introduce a novel, all-in-one deep learning framework for MR image reconstruction, enabling a single model to enhance image quality across multiple aspects of k-space sampling and to be effective across a wide range of clinical and technical scenarios.","This DICOM-based algorithm serves as the core of SwiftMR (AIRS Medical, Seoul, Korea), which is FDA-cleared, CE-certified, and commercially available.","We first detail the comprehensive development process of the model, including data collection, training pair preparation, model architecture design, and DICOM inference.","We then assess the model's capability to enhance image quality in a multi-dimensional manner, specifically across various aspects of k-space sampling.","Subsequently, we evaluate several features of the multi-dimensional enhancement: the accuracy of tunable denoising, the effectiveness of super-resolution in each encoding direction, and the reduction of artifacts that become more prominent at lower spatial resolutions.","Additionally, we assess its compatibility with various scan parameter sets and its generalizability across scanner vendors not seen during training.","Finally, we present specific cases demonstrating the model's utility in reducing scan time across anatomical regions in conjunction with protocol optimization.","The proposed model is compatible with a broad spectrum of scenarios, including various vendors, pulse sequences, scan parameters, and anatomical regions.","Its DICOM-based operation particularly enhances its applicability for real-world applications.","Given its demonstrated effectiveness and versatility, we expect its use to expand in the field of clinical MRI."],"url":"http://arxiv.org/abs/2405.03684v1","category":"eess.IV"}
{"created":"2024-05-06 17:53:33","title":"Towards A Human-in-the-Loop LLM Approach to Collaborative Discourse Analysis","abstract":"LLMs have demonstrated proficiency in contextualizing their outputs using human input, often matching or beating human-level performance on a variety of tasks. However, LLMs have not yet been used to characterize synergistic learning in students' collaborative discourse. In this exploratory work, we take a first step towards adopting a human-in-the-loop prompt engineering approach with GPT-4-Turbo to summarize and categorize students' synergistic learning during collaborative discourse. Our preliminary findings suggest GPT-4-Turbo may be able to characterize students' synergistic learning in a manner comparable to humans and that our approach warrants further investigation.","sentences":["LLMs have demonstrated proficiency in contextualizing their outputs using human input, often matching or beating human-level performance on a variety of tasks.","However, LLMs have not yet been used to characterize synergistic learning in students' collaborative discourse.","In this exploratory work, we take a first step towards adopting a human-in-the-loop prompt engineering approach with GPT-4-Turbo to summarize and categorize students' synergistic learning during collaborative discourse.","Our preliminary findings suggest GPT-4-Turbo may be able to characterize students' synergistic learning in a manner comparable to humans and that our approach warrants further investigation."],"url":"http://arxiv.org/abs/2405.03677v1","category":"cs.CL"}
{"created":"2024-05-06 17:49:31","title":"MemoryMamba: Memory-Augmented State Space Model for Defect Recognition","abstract":"As automation advances in manufacturing, the demand for precise and sophisticated defect detection technologies grows. Existing vision models for defect recognition methods are insufficient for handling the complexities and variations of defects in contemporary manufacturing settings. These models especially struggle in scenarios involving limited or imbalanced defect data. In this work, we introduce MemoryMamba, a novel memory-augmented state space model (SSM), designed to overcome the limitations of existing defect recognition models. MemoryMamba integrates the state space model with the memory augmentation mechanism, enabling the system to maintain and retrieve essential defect-specific information in training. Its architecture is designed to capture dependencies and intricate defect characteristics, which are crucial for effective defect detection. In the experiments, MemoryMamba was evaluated across four industrial datasets with diverse defect types and complexities. The model consistently outperformed other methods, demonstrating its capability to adapt to various defect recognition scenarios.","sentences":["As automation advances in manufacturing, the demand for precise and sophisticated defect detection technologies grows.","Existing vision models for defect recognition methods are insufficient for handling the complexities and variations of defects in contemporary manufacturing settings.","These models especially struggle in scenarios involving limited or imbalanced defect data.","In this work, we introduce MemoryMamba, a novel memory-augmented state space model (SSM), designed to overcome the limitations of existing defect recognition models.","MemoryMamba integrates the state space model with the memory augmentation mechanism, enabling the system to maintain and retrieve essential defect-specific information in training.","Its architecture is designed to capture dependencies and intricate defect characteristics, which are crucial for effective defect detection.","In the experiments, MemoryMamba was evaluated across four industrial datasets with diverse defect types and complexities.","The model consistently outperformed other methods, demonstrating its capability to adapt to various defect recognition scenarios."],"url":"http://arxiv.org/abs/2405.03673v1","category":"cs.CV"}
{"created":"2024-05-06 17:48:10","title":"Prompting Task Trees using Gemini: Methodologies and Insights","abstract":"Robots are the future of every technology where every advanced technology eventually will be used to make robots which are more efficient. The major challenge today is to train the robots exactly and empathetically using knowledge representation. This paper gives you insights of how we can use unstructured knowledge representation and convert them to meaningful structured representation with the help of prompt engineering which can be eventually used in the robots to make help them understand how human brain can make wonders with the minimal data or objects can providing to them.","sentences":["Robots are the future of every technology where every advanced technology eventually will be used to make robots which are more efficient.","The major challenge today is to train the robots exactly and empathetically using knowledge representation.","This paper gives you insights of how we can use unstructured knowledge representation and convert them to meaningful structured representation with the help of prompt engineering which can be eventually used in the robots to make help them understand how human brain can make wonders with the minimal data or objects can providing to them."],"url":"http://arxiv.org/abs/2405.03671v1","category":"cs.RO"}
{"created":"2024-05-06 17:43:34","title":"ScrewMimic: Bimanual Imitation from Human Videos with Screw Space Projection","abstract":"Bimanual manipulation is a longstanding challenge in robotics due to the large number of degrees of freedom and the strict spatial and temporal synchronization required to generate meaningful behavior. Humans learn bimanual manipulation skills by watching other humans and by refining their abilities through play. In this work, we aim to enable robots to learn bimanual manipulation behaviors from human video demonstrations and fine-tune them through interaction. Inspired by seminal work in psychology and biomechanics, we propose modeling the interaction between two hands as a serial kinematic linkage -- as a screw motion, in particular, that we use to define a new action space for bimanual manipulation: screw actions. We introduce ScrewMimic, a framework that leverages this novel action representation to facilitate learning from human demonstration and self-supervised policy fine-tuning. Our experiments demonstrate that ScrewMimic is able to learn several complex bimanual behaviors from a single human video demonstration, and that it outperforms baselines that interpret demonstrations and fine-tune directly in the original space of motion of both arms. For more information and video results, https://robin-lab.cs.utexas.edu/ScrewMimic/","sentences":["Bimanual manipulation is a longstanding challenge in robotics due to the large number of degrees of freedom and the strict spatial and temporal synchronization required to generate meaningful behavior.","Humans learn bimanual manipulation skills by watching other humans and by refining their abilities through play.","In this work, we aim to enable robots to learn bimanual manipulation behaviors from human video demonstrations and fine-tune them through interaction.","Inspired by seminal work in psychology and biomechanics, we propose modeling the interaction between two hands as a serial kinematic linkage -- as a screw motion, in particular, that we use to define a new action space for bimanual manipulation: screw actions.","We introduce ScrewMimic, a framework that leverages this novel action representation to facilitate learning from human demonstration and self-supervised policy fine-tuning.","Our experiments demonstrate that ScrewMimic is able to learn several complex bimanual behaviors from a single human video demonstration, and that it outperforms baselines that interpret demonstrations and fine-tune directly in the original space of motion of both arms.","For more information and video results, https://robin-lab.cs.utexas.edu/ScrewMimic/"],"url":"http://arxiv.org/abs/2405.03666v1","category":"cs.RO"}
{"created":"2024-05-06 17:39:53","title":"Diffeomorphic Template Registration for Atmospheric Turbulence Mitigation","abstract":"We describe a method for recovering the irradiance underlying a collection of images corrupted by atmospheric turbulence. Since supervised data is often technically impossible to obtain, assumptions and biases have to be imposed to solve this inverse problem, and we choose to model them explicitly. Rather than initializing a latent irradiance (\"template\") by heuristics to estimate deformation, we select one of the images as a reference, and model the deformation in this image by the aggregation of the optical flow from it to other images, exploiting a prior imposed by Central Limit Theorem. Then with a novel flow inversion module, the model registers each image TO the template but WITHOUT the template, avoiding artifacts related to poor template initialization. To illustrate the robustness of the method, we simply (i) select the first frame as the reference and (ii) use the simplest optical flow to estimate the warpings, yet the improvement in registration is decisive in the final reconstruction, as we achieve state-of-the-art performance despite its simplicity. The method establishes a strong baseline that can be further improved by integrating it seamlessly into more sophisticated pipelines, or with domain-specific methods if so desired.","sentences":["We describe a method for recovering the irradiance underlying a collection of images corrupted by atmospheric turbulence.","Since supervised data is often technically impossible to obtain, assumptions and biases have to be imposed to solve this inverse problem, and we choose to model them explicitly.","Rather than initializing a latent irradiance (\"template\") by heuristics to estimate deformation, we select one of the images as a reference, and model the deformation in this image by the aggregation of the optical flow from it to other images, exploiting a prior imposed by Central Limit Theorem.","Then with a novel flow inversion module, the model registers each image TO the template but WITHOUT the template, avoiding artifacts related to poor template initialization.","To illustrate the robustness of the method, we simply (i) select the first frame as the reference and (ii) use the simplest optical flow to estimate the warpings, yet the improvement in registration is decisive in the final reconstruction, as we achieve state-of-the-art performance despite its simplicity.","The method establishes a strong baseline that can be further improved by integrating it seamlessly into more sophisticated pipelines, or with domain-specific methods if so desired."],"url":"http://arxiv.org/abs/2405.03662v1","category":"cs.CV"}
{"created":"2024-05-06 17:26:34","title":"Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent","abstract":"To demonstrate and address the underlying maliciousness, we propose a theoretical hypothesis and analytical approach, and introduce a new black-box jailbreak attack methodology named IntentObfuscator, exploiting this identified flaw by obfuscating the true intentions behind user prompts.This approach compels LLMs to inadvertently generate restricted content, bypassing their built-in content security measures. We detail two implementations under this framework: \"Obscure Intention\" and \"Create Ambiguity\", which manipulate query complexity and ambiguity to evade malicious intent detection effectively. We empirically validate the effectiveness of the IntentObfuscator method across several models, including ChatGPT-3.5, ChatGPT-4, Qwen and Baichuan, achieving an average jailbreak success rate of 69.21\\%. Notably, our tests on ChatGPT-3.5, which claims 100 million weekly active users, achieved a remarkable success rate of 83.65\\%. We also extend our validation to diverse types of sensitive content like graphic violence, racism, sexism, political sensitivity, cybersecurity threats, and criminal skills, further proving the substantial impact of our findings on enhancing 'Red Team' strategies against LLM content security frameworks.","sentences":["To demonstrate and address the underlying maliciousness, we propose a theoretical hypothesis and analytical approach, and introduce a new black-box jailbreak attack methodology named IntentObfuscator, exploiting this identified flaw by obfuscating the true intentions behind user prompts.","This approach compels LLMs to inadvertently generate restricted content, bypassing their built-in content security measures.","We detail two implementations under this framework: \"Obscure Intention\" and \"Create Ambiguity\", which manipulate query complexity and ambiguity to evade malicious intent detection effectively.","We empirically validate the effectiveness of the IntentObfuscator method across several models, including ChatGPT-3.5, ChatGPT-4, Qwen and Baichuan, achieving an average jailbreak success rate of 69.21\\%.","Notably, our tests on ChatGPT-3.5, which claims 100 million weekly active users, achieved a remarkable success rate of 83.65\\%.","We also extend our validation to diverse types of sensitive content like graphic violence, racism, sexism, political sensitivity, cybersecurity threats, and criminal skills, further proving the substantial impact of our findings on enhancing 'Red Team' strategies against LLM content security frameworks."],"url":"http://arxiv.org/abs/2405.03654v2","category":"cs.CR"}
{"created":"2024-05-06 17:14:09","title":"Generated Contents Enrichment","abstract":"In this paper, we investigate a novel artificial intelligence generation task, termed as generated contents enrichment (GCE). Different from conventional artificial intelligence contents generation task that enriches the given textual description implicitly with limited semantics for generating visually real content, our proposed GCE strives to perform content enrichment explicitly on both the visual and textual domain, from which the enriched contents are visually real, structurally reasonable, and semantically abundant. Towards to solve GCE, we propose a deep end-to-end method that explicitly explores the semantics and inter-semantic relationships during the enrichment. Specifically, we first model the input description as a semantic graph, wherein each node represents an object and each edge corresponds to the inter-object relationship. We then adopt Graph Convolutional Networks on top of the input scene description to predict the enriching objects and their relationships with the input objects. Finally, the enriched graph is fed into an image synthesis model to carry out the visual contents generation. Our experiments conducted on the Visual Genome dataset exhibit promising and visually plausible results.","sentences":["In this paper, we investigate a novel artificial intelligence generation task, termed as generated contents enrichment (GCE).","Different from conventional artificial intelligence contents generation task that enriches the given textual description implicitly with limited semantics for generating visually real content, our proposed GCE strives to perform content enrichment explicitly on both the visual and textual domain, from which the enriched contents are visually real, structurally reasonable, and semantically abundant.","Towards to solve GCE, we propose a deep end-to-end method that explicitly explores the semantics and inter-semantic relationships during the enrichment.","Specifically, we first model the input description as a semantic graph, wherein each node represents an object and each edge corresponds to the inter-object relationship.","We then adopt Graph Convolutional Networks on top of the input scene description to predict the enriching objects and their relationships with the input objects.","Finally, the enriched graph is fed into an image synthesis model to carry out the visual contents generation.","Our experiments conducted on the Visual Genome dataset exhibit promising and visually plausible results."],"url":"http://arxiv.org/abs/2405.03650v1","category":"cs.CV"}
{"created":"2024-05-06 17:07:28","title":"When LLMs Meet Cybersecurity: A Systematic Literature Review","abstract":"The rapid advancements in large language models (LLMs) have opened new avenues across various fields, including cybersecurity, which faces an ever-evolving threat landscape and need for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper bridge this gap by providing a systematic literature review, encompassing an analysis of over 180 works, spanning across 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three critical research questions: the construction of cybersecurity-oriented LLMs, LLMs' applications in various cybersecurity tasks, and the existing challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices, and serve as a valuable resource for applying LLMs in this doamin. We also maintain and regularly updated list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.","sentences":["The rapid advancements in large language models (LLMs) have opened new avenues across various fields, including cybersecurity, which faces an ever-evolving threat landscape and need for innovative technologies.","Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area.","This paper bridge this gap by providing a systematic literature review, encompassing an analysis of over 180 works, spanning across 25 LLMs and more than 10 downstream scenarios.","Our comprehensive overview addresses three critical research questions: the construction of cybersecurity-oriented LLMs, LLMs' applications in various cybersecurity tasks, and the existing challenges and further research in this area.","This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices, and serve as a valuable resource for applying LLMs in this doamin.","We also maintain and regularly updated list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity."],"url":"http://arxiv.org/abs/2405.03644v1","category":"cs.CR"}
{"created":"2024-05-06 17:06:32","title":"Collecting Consistently High Quality Object Tracks with Minimal Human Involvement by Using Self-Supervised Learning to Detect Tracker Errors","abstract":"We propose a hybrid framework for consistently producing high-quality object tracks by combining an automated object tracker with little human input. The key idea is to tailor a module for each dataset to intelligently decide when an object tracker is failing and so humans should be brought in to re-localize an object for continued tracking. Our approach leverages self-supervised learning on unlabeled videos to learn a tailored representation for a target object that is then used to actively monitor its tracked region and decide when the tracker fails. Since labeled data is not needed, our approach can be applied to novel object categories. Experiments on three datasets demonstrate our method outperforms existing approaches, especially for small, fast moving, or occluded objects.","sentences":["We propose a hybrid framework for consistently producing high-quality object tracks by combining an automated object tracker with little human input.","The key idea is to tailor a module for each dataset to intelligently decide when an object tracker is failing and so humans should be brought in to re-localize an object for continued tracking.","Our approach leverages self-supervised learning on unlabeled videos to learn a tailored representation for a target object that is then used to actively monitor its tracked region and decide when the tracker fails.","Since labeled data is not needed, our approach can be applied to novel object categories.","Experiments on three datasets demonstrate our method outperforms existing approaches, especially for small, fast moving, or occluded objects."],"url":"http://arxiv.org/abs/2405.03643v1","category":"cs.CV"}
{"created":"2024-05-06 16:35:56","title":"Detecting Android Malware: From Neural Embeddings to Hands-On Validation with BERTroid","abstract":"As cyber threats and malware attacks increasingly alarm both individuals and businesses, the urgency for proactive malware countermeasures intensifies. This has driven a rising interest in automated machine learning solutions. Transformers, a cutting-edge category of attention-based deep learning methods, have demonstrated remarkable success. In this paper, we present BERTroid, an innovative malware detection model built on the BERT architecture. Overall, BERTroid emerged as a promising solution for combating Android malware. Its ability to outperform state-of-the-art solutions demonstrates its potential as a proactive defense mechanism against malicious software attacks. Additionally, we evaluate BERTroid on multiple datasets to assess its performance across diverse scenarios. In the dynamic landscape of cybersecurity, our approach has demonstrated promising resilience against the rapid evolution of malware on Android systems. While the machine learning model captures broad patterns, we emphasize the role of manual validation for deeper comprehension and insight into these behaviors. This human intervention is critical for discerning intricate and context-specific behaviors, thereby validating and reinforcing the model's findings.","sentences":["As cyber threats and malware attacks increasingly alarm both individuals and businesses, the urgency for proactive malware countermeasures intensifies.","This has driven a rising interest in automated machine learning solutions.","Transformers, a cutting-edge category of attention-based deep learning methods, have demonstrated remarkable success.","In this paper, we present BERTroid, an innovative malware detection model built on the BERT architecture.","Overall, BERTroid emerged as a promising solution for combating Android malware.","Its ability to outperform state-of-the-art solutions demonstrates its potential as a proactive defense mechanism against malicious software attacks.","Additionally, we evaluate BERTroid on multiple datasets to assess its performance across diverse scenarios.","In the dynamic landscape of cybersecurity, our approach has demonstrated promising resilience against the rapid evolution of malware on Android systems.","While the machine learning model captures broad patterns, we emphasize the role of manual validation for deeper comprehension and insight into these behaviors.","This human intervention is critical for discerning intricate and context-specific behaviors, thereby validating and reinforcing the model's findings."],"url":"http://arxiv.org/abs/2405.03620v1","category":"cs.CR"}
{"created":"2024-05-06 16:32:29","title":"A Controlled Experiment on the Energy Efficiency of the Source Code Generated by Code Llama","abstract":"Context. Nowadays, 83% of software developers use Large Language Models (LLMs) to generate code. LLMs recently became essential to increase the productivity of software developers and decrease the time and cost of software development. Developers ranging from novices to experts use LLM tools not only to detect and patch bugs, but also to integrate generated code into their software. However, as of today there is no objective assessment of the energy efficiency of the source code generated by LLM tools. Released in August 2023, Code Llama is one of the most recent LLM tools.   Goal. In this paper, we present an empirical study that assesses the energy efficiency of Code Llama with respect to human-written source code.   Method. We design an experiment involving three human-written benchmarks implemented in C++, JavaScript, and Python. We ask Code Llama to generate the code of the benchmarks using different prompts and temperatures. Therefore, we execute both implementations and profile their energy efficiency.   Results. Our study shows that the energy efficiency of code generated by Code Llama is heavily-dependent on the chosen programming language and the specific code problem at hand. Also, human implementations tend to be more energy efficient overall, with generated JavaScript code outperforming its human counterpart. Moreover, explicitly asking Code Llama to generate energy-efficient code results in an equal or worse energy efficiency, as well as using different temperatures seems not to affect the energy efficiency of generated code.   Conclusions. According to our results, code generated using Code Llama does not guarantee energy efficiency, even when prompted to do so. Therefore, software developers should evaluate the energy efficiency of generated code before integrating it into the software system under development.","sentences":["Context.","Nowadays, 83% of software developers use Large Language Models (LLMs) to generate code.","LLMs recently became essential to increase the productivity of software developers and decrease the time and cost of software development.","Developers ranging from novices to experts use LLM tools not only to detect and patch bugs, but also to integrate generated code into their software.","However, as of today there is no objective assessment of the energy efficiency of the source code generated by LLM tools.","Released in August 2023, Code Llama is one of the most recent LLM tools.   Goal.","In this paper, we present an empirical study that assesses the energy efficiency of Code Llama with respect to human-written source code.   ","Method.","We design an experiment involving three human-written benchmarks implemented in C++, JavaScript, and Python.","We ask Code Llama to generate the code of the benchmarks using different prompts and temperatures.","Therefore, we execute both implementations and profile their energy efficiency.   Results.","Our study shows that the energy efficiency of code generated by Code Llama is heavily-dependent on the chosen programming language and the specific code problem at hand.","Also, human implementations tend to be more energy efficient overall, with generated JavaScript code outperforming its human counterpart.","Moreover, explicitly asking Code Llama to generate energy-efficient code results in an equal or worse energy efficiency, as well as using different temperatures seems not to affect the energy efficiency of generated code.   ","Conclusions.","According to our results, code generated using Code Llama does not guarantee energy efficiency, even when prompted to do so.","Therefore, software developers should evaluate the energy efficiency of generated code before integrating it into the software system under development."],"url":"http://arxiv.org/abs/2405.03616v1","category":"cs.SE"}
{"created":"2024-05-06 16:04:03","title":"GREEN: Generative Radiology Report Evaluation and Error Notation","abstract":"Evaluating radiology reports is a challenging problem as factual correctness is extremely important due to the need for accurate medical communication about medical images. Existing automatic evaluation metrics either suffer from failing to consider factual correctness (e.g., BLEU and ROUGE) or are limited in their interpretability (e.g., F1CheXpert and F1RadGraph). In this paper, we introduce GREEN (Generative Radiology Report Evaluation and Error Notation), a radiology report generation metric that leverages the natural language understanding of language models to identify and explain clinically significant errors in candidate reports, both quantitatively and qualitatively. Compared to current metrics, GREEN offers: 1) a score aligned with expert preferences, 2) human interpretable explanations of clinically significant errors, enabling feedback loops with end-users, and 3) a lightweight open-source method that reaches the performance of commercial counterparts. We validate our GREEN metric by comparing it to GPT-4, as well as to error counts of 6 experts and preferences of 2 experts. Our method demonstrates not only higher correlation with expert error counts, but simultaneously higher alignment with expert preferences when compared to previous approaches.\"","sentences":["Evaluating radiology reports is a challenging problem as factual correctness is extremely important due to the need for accurate medical communication about medical images.","Existing automatic evaluation metrics either suffer from failing to consider factual correctness (e.g., BLEU and ROUGE) or are limited in their interpretability (e.g., F1CheXpert and F1RadGraph).","In this paper, we introduce GREEN (Generative Radiology Report Evaluation and Error Notation), a radiology report generation metric that leverages the natural language understanding of language models to identify and explain clinically significant errors in candidate reports, both quantitatively and qualitatively.","Compared to current metrics, GREEN offers: 1) a score aligned with expert preferences, 2) human interpretable explanations of clinically significant errors, enabling feedback loops with end-users, and 3) a lightweight open-source method that reaches the performance of commercial counterparts.","We validate our GREEN metric by comparing it to GPT-4, as well as to error counts of 6 experts and preferences of 2 experts.","Our method demonstrates not only higher correlation with expert error counts, but simultaneously higher alignment with expert preferences when compared to previous approaches.\""],"url":"http://arxiv.org/abs/2405.03595v1","category":"cs.CL"}
{"created":"2024-05-06 16:03:32","title":"Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment","abstract":"Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks. We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset. We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling. In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization. Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality. This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy.","sentences":["Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks.","We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity.","We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset.","We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling.","In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine.","The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization.","Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x.","We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality.","This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy."],"url":"http://arxiv.org/abs/2405.03594v1","category":"cs.CL"}
{"created":"2024-05-06 15:59:44","title":"Non-detectable patterns hidden within sequences of bits","abstract":"In this paper we construct families of bit sequences using combinatorial methods. Each sequence is derived by con- verting a collection of numbers encoding certain combinatorial nu- merics from objects exhibiting symmetry in various dimensions. Using the algorithms first described in [1] we show that the NIST testing suite described in publication 800-22 does not detect these symmetries hidden within these sequences.","sentences":["In this paper we construct families of bit sequences using combinatorial methods.","Each sequence is derived by con- verting a collection of numbers encoding certain combinatorial nu- merics from objects exhibiting symmetry in various dimensions.","Using the algorithms first described in [1] we show that the NIST testing suite described in publication 800-22 does not detect these symmetries hidden within these sequences."],"url":"http://arxiv.org/abs/2405.03587v1","category":"math.CO"}
{"created":"2024-05-06 15:48:24","title":"Select to Perfect: Imitating desired behavior from large multi-agent data","abstract":"AI agents are commonly trained with large datasets of demonstrations of human behavior. However, not all behaviors are equally safe or desirable. Desired characteristics for an AI agent can be expressed by assigning desirability scores, which we assume are not assigned to individual behaviors but to collective trajectories. For example, in a dataset of vehicle interactions, these scores might relate to the number of incidents that occurred. We first assess the effect of each individual agent's behavior on the collective desirability score, e.g., assessing how likely an agent is to cause incidents. This allows us to selectively imitate agents with a positive effect, e.g., only imitating agents that are unlikely to cause incidents. To enable this, we propose the concept of an agent's Exchange Value, which quantifies an individual agent's contribution to the collective desirability score. The Exchange Value is the expected change in desirability score when substituting the agent for a randomly selected agent. We propose additional methods for estimating Exchange Values from real-world datasets, enabling us to learn desired imitation policies that outperform relevant baselines. The project website can be found at https://tinyurl.com/select-to-perfect.","sentences":["AI agents are commonly trained with large datasets of demonstrations of human behavior.","However, not all behaviors are equally safe or desirable.","Desired characteristics for an AI agent can be expressed by assigning desirability scores, which we assume are not assigned to individual behaviors but to collective trajectories.","For example, in a dataset of vehicle interactions, these scores might relate to the number of incidents that occurred.","We first assess the effect of each individual agent's behavior on the collective desirability score, e.g., assessing how likely an agent is to cause incidents.","This allows us to selectively imitate agents with a positive effect, e.g., only imitating agents that are unlikely to cause incidents.","To enable this, we propose the concept of an agent's Exchange Value, which quantifies an individual agent's contribution to the collective desirability score.","The Exchange Value is the expected change in desirability score when substituting the agent for a randomly selected agent.","We propose additional methods for estimating Exchange Values from real-world datasets, enabling us to learn desired imitation policies that outperform relevant baselines.","The project website can be found at https://tinyurl.com/select-to-perfect."],"url":"http://arxiv.org/abs/2405.03735v1","category":"cs.LG"}
{"created":"2024-05-06 15:41:41","title":"Deep Space Separable Distillation for Lightweight Acoustic Scene Classification","abstract":"Acoustic scene classification (ASC) is highly important in the real world. Recently, deep learning-based methods have been widely employed for acoustic scene classification. However, these methods are currently not lightweight enough as well as their performance is not satisfactory. To solve these problems, we propose a deep space separable distillation network. Firstly, the network performs high-low frequency decomposition on the log-mel spectrogram, significantly reducing computational complexity while maintaining model performance. Secondly, we specially design three lightweight operators for ASC, including Separable Convolution (SC), Orthonormal Separable Convolution (OSC), and Separable Partial Convolution (SPC). These operators exhibit highly efficient feature extraction capabilities in acoustic scene classification tasks. The experimental results demonstrate that the proposed method achieves a performance gain of 9.8% compared to the currently popular deep learning methods, while also having smaller parameter count and computational complexity.","sentences":["Acoustic scene classification (ASC) is highly important in the real world.","Recently, deep learning-based methods have been widely employed for acoustic scene classification.","However, these methods are currently not lightweight enough as well as their performance is not satisfactory.","To solve these problems, we propose a deep space separable distillation network.","Firstly, the network performs high-low frequency decomposition on the log-mel spectrogram, significantly reducing computational complexity while maintaining model performance.","Secondly, we specially design three lightweight operators for ASC, including Separable Convolution (SC), Orthonormal Separable Convolution (OSC), and Separable Partial Convolution (SPC).","These operators exhibit highly efficient feature extraction capabilities in acoustic scene classification tasks.","The experimental results demonstrate that the proposed method achieves a performance gain of 9.8% compared to the currently popular deep learning methods, while also having smaller parameter count and computational complexity."],"url":"http://arxiv.org/abs/2405.03567v1","category":"cs.SD"}
{"created":"2024-05-06 15:25:48","title":"A Comprehensive Overview and Survey of O-RAN: Exploring Slicing-aware Architecture, Deployment Options, and Use Cases","abstract":"Open-radio access network (O-RAN) seeks to establish principles of openness, programmability, automation, intelligence, and hardware-software disaggregation with interoperable interfaces. It advocates for multi-vendorism and multi-stakeholderism within a cloudified and virtualized wireless infrastructure, aimed at enhancing the deployment, operation, and maintenance of RAN architecture. This enhancement promises increased flexibility, performance optimization, service innovation, energy efficiency, and cost efficiency in fifth-generation (5G), sixth-generation (6G), and future networks. One of the key features of the O-RAN architecture is its support for network slicing, which entails interaction with other slicing domains within a mobile network, notably the transport network (TN) domain and the core network (CN) domain, to realize end-to-end (E2E) network slicing. The study of this feature requires exploring the stances and contributions of diverse standards development organizations (SDOs). In this context, we note that despite the ongoing industrial deployments and standardization efforts, the research and standardization communities have yet to comprehensively address network slicing in O-RAN. To address this gap, this survey paper provides a comprehensive exploration of network slicing in O-RAN through an in-depth review of specification documents from O-RAN Alliance and research papers from leading industry and academic institutions. The paper commences with an overview of the ongoing standardization efforts and open-source contributions associated with O-RAN, subsequently delving into the latest O-RAN architecture with an emphasis on its slicing aspects. Further, the paper explores deployment scenarios for network slicing within O-RAN, examining options for the deployment and orchestration of O-RAN and TN network slice subnets...","sentences":["Open-radio access network (O-RAN) seeks to establish principles of openness, programmability, automation, intelligence, and hardware-software disaggregation with interoperable interfaces.","It advocates for multi-vendorism and multi-stakeholderism within a cloudified and virtualized wireless infrastructure, aimed at enhancing the deployment, operation, and maintenance of RAN architecture.","This enhancement promises increased flexibility, performance optimization, service innovation, energy efficiency, and cost efficiency in fifth-generation (5G), sixth-generation (6G), and future networks.","One of the key features of the O-RAN architecture is its support for network slicing, which entails interaction with other slicing domains within a mobile network, notably the transport network (TN) domain and the core network (CN) domain, to realize end-to-end (E2E) network slicing.","The study of this feature requires exploring the stances and contributions of diverse standards development organizations (SDOs).","In this context, we note that despite the ongoing industrial deployments and standardization efforts, the research and standardization communities have yet to comprehensively address network slicing in O-RAN.","To address this gap, this survey paper provides a comprehensive exploration of network slicing in O-RAN through an in-depth review of specification documents from O-RAN Alliance and research papers from leading industry and academic institutions.","The paper commences with an overview of the ongoing standardization efforts and open-source contributions associated with O-RAN, subsequently delving into the latest O-RAN architecture with an emphasis on its slicing aspects.","Further, the paper explores deployment scenarios for network slicing within O-RAN, examining options for the deployment and orchestration of O-RAN and TN network slice subnets..."],"url":"http://arxiv.org/abs/2405.03555v2","category":"cs.NI"}
{"created":"2024-05-06 15:20:30","title":"AlphaMath Almost Zero: process Supervision without process","abstract":"Recent advancements in large language models (LLMs) have substantially enhanced their mathematical reasoning abilities. However, these models still struggle with complex problems that require multiple reasoning steps, frequently leading to logical or numerical errors. While numerical mistakes can largely be addressed by integrating a code interpreter, identifying logical errors within intermediate steps is more challenging. Moreover, manually annotating these steps for training is not only expensive but also demands specialized expertise. In this study, we introduce an innovative approach that eliminates the need for manual annotation by leveraging the Monte Carlo Tree Search (MCTS) framework to generate both the process supervision and evaluation signals automatically. Essentially, when a LLM is well pre-trained, only the mathematical questions and their final answers are required to generate our training data, without requiring the solutions. We proceed to train a step-level value model designed to improve the LLM's inference process in mathematical domains. Our experiments indicate that using automatically generated solutions by LLMs enhanced with MCTS significantly improves the model's proficiency in dealing with intricate mathematical reasoning tasks.","sentences":["Recent advancements in large language models (LLMs) have substantially enhanced their mathematical reasoning abilities.","However, these models still struggle with complex problems that require multiple reasoning steps, frequently leading to logical or numerical errors.","While numerical mistakes can largely be addressed by integrating a code interpreter, identifying logical errors within intermediate steps is more challenging.","Moreover, manually annotating these steps for training is not only expensive but also demands specialized expertise.","In this study, we introduce an innovative approach that eliminates the need for manual annotation by leveraging the Monte Carlo Tree Search (MCTS) framework to generate both the process supervision and evaluation signals automatically.","Essentially, when a LLM is well pre-trained, only the mathematical questions and their final answers are required to generate our training data, without requiring the solutions.","We proceed to train a step-level value model designed to improve the LLM's inference process in mathematical domains.","Our experiments indicate that using automatically generated solutions by LLMs enhanced with MCTS significantly improves the model's proficiency in dealing with intricate mathematical reasoning tasks."],"url":"http://arxiv.org/abs/2405.03553v1","category":"cs.CL"}
{"created":"2024-05-06 15:11:05","title":"FOKE: A Personalized and Explainable Education Framework Integrating Foundation Models, Knowledge Graphs, and Prompt Engineering","abstract":"Integrating large language models (LLMs) and knowledge graphs (KGs) holds great promise for revolutionizing intelligent education, but challenges remain in achieving personalization, interactivity, and explainability. We propose FOKE, a Forest Of Knowledge and Education framework that synergizes foundation models, knowledge graphs, and prompt engineering to address these challenges. FOKE introduces three key innovations: (1) a hierarchical knowledge forest for structured domain knowledge representation; (2) a multi-dimensional user profiling mechanism for comprehensive learner modeling; and (3) an interactive prompt engineering scheme for generating precise and tailored learning guidance.   We showcase FOKE's application in programming education, homework assessment, and learning path planning, demonstrating its effectiveness and practicality. Additionally, we implement Scholar Hero, a real-world instantiation of FOKE. Our research highlights the potential of integrating foundation models, knowledge graphs, and prompt engineering to revolutionize intelligent education practices, ultimately benefiting learners worldwide. FOKE provides a principled and unified approach to harnessing cutting-edge AI technologies for personalized, interactive, and explainable educational services, paving the way for further research and development in this critical direction.","sentences":["Integrating large language models (LLMs) and knowledge graphs (KGs) holds great promise for revolutionizing intelligent education, but challenges remain in achieving personalization, interactivity, and explainability.","We propose FOKE, a Forest Of Knowledge and Education framework that synergizes foundation models, knowledge graphs, and prompt engineering to address these challenges.","FOKE introduces three key innovations: (1) a hierarchical knowledge forest for structured domain knowledge representation; (2) a multi-dimensional user profiling mechanism for comprehensive learner modeling; and (3) an interactive prompt engineering scheme for generating precise and tailored learning guidance.   ","We showcase FOKE's application in programming education, homework assessment, and learning path planning, demonstrating its effectiveness and practicality.","Additionally, we implement Scholar Hero, a real-world instantiation of FOKE.","Our research highlights the potential of integrating foundation models, knowledge graphs, and prompt engineering to revolutionize intelligent education practices, ultimately benefiting learners worldwide.","FOKE provides a principled and unified approach to harnessing cutting-edge AI technologies for personalized, interactive, and explainable educational services, paving the way for further research and development in this critical direction."],"url":"http://arxiv.org/abs/2405.03734v1","category":"cs.HC"}
{"created":"2024-05-06 15:10:46","title":"Position Paper: Leveraging Foundational Models for Black-Box Optimization: Benefits, Challenges, and Future Directions","abstract":"Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave of innovation in the machine learning research domain, resulting in substantial impact across diverse fields such as reinforcement learning, robotics, and computer vision. Their incorporation has been rapid and transformative, marking a significant paradigm shift in the field of machine learning research.   However, the field of experimental design, grounded on black-box optimization, has been much less affected by such a paradigm shift, even though integrating LLMs with optimization presents a unique landscape ripe for exploration. In this position paper, we frame the field of black-box optimization around sequence-based foundation models and organize their relationship with previous literature. We discuss the most promising ways foundational language models can revolutionize optimization, which include harnessing the vast wealth of information encapsulated in free-form text to enrich task comprehension, utilizing highly flexible sequence models such as Transformers to engineer superior optimization strategies, and enhancing performance prediction over previously unseen search spaces.","sentences":["Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave of innovation in the machine learning research domain, resulting in substantial impact across diverse fields such as reinforcement learning, robotics, and computer vision.","Their incorporation has been rapid and transformative, marking a significant paradigm shift in the field of machine learning research.   ","However, the field of experimental design, grounded on black-box optimization, has been much less affected by such a paradigm shift, even though integrating LLMs with optimization presents a unique landscape ripe for exploration.","In this position paper, we frame the field of black-box optimization around sequence-based foundation models and organize their relationship with previous literature.","We discuss the most promising ways foundational language models can revolutionize optimization, which include harnessing the vast wealth of information encapsulated in free-form text to enrich task comprehension, utilizing highly flexible sequence models such as Transformers to engineer superior optimization strategies, and enhancing performance prediction over previously unseen search spaces."],"url":"http://arxiv.org/abs/2405.03547v1","category":"cs.LG"}
{"created":"2024-05-06 15:02:16","title":"RepVGG-GELAN: Enhanced GELAN with VGG-STYLE ConvNets for Brain Tumour Detection","abstract":"Object detection algorithms particularly those based on YOLO have demonstrated remarkable efficiency in balancing speed and accuracy. However, their application in brain tumour detection remains underexplored. This study proposes RepVGG-GELAN, a novel YOLO architecture enhanced with RepVGG, a reparameterized convolutional approach for object detection tasks particularly focusing on brain tumour detection within medical images. RepVGG-GELAN leverages the RepVGG architecture to improve both speed and accuracy in detecting brain tumours. Integrating RepVGG into the YOLO framework aims to achieve a balance between computational efficiency and detection performance. This study includes a spatial pyramid pooling-based Generalized Efficient Layer Aggregation Network (GELAN) architecture which further enhances the capability of RepVGG. Experimental evaluation conducted on a brain tumour dataset demonstrates the effectiveness of RepVGG-GELAN surpassing existing RCS-YOLO in terms of precision and speed. Specifically, RepVGG-GELAN achieves an increased precision of 4.91% and an increased AP50 of 2.54% over the latest existing approach while operating at 240.7 GFLOPs. The proposed RepVGG-GELAN with GELAN architecture presents promising results establishing itself as a state-of-the-art solution for accurate and efficient brain tumour detection in medical images. The implementation code is publicly available at https://github.com/ThensiB/RepVGG-GELAN.","sentences":["Object detection algorithms particularly those based on YOLO have demonstrated remarkable efficiency in balancing speed and accuracy.","However, their application in brain tumour detection remains underexplored.","This study proposes RepVGG-GELAN, a novel YOLO architecture enhanced with RepVGG, a reparameterized convolutional approach for object detection tasks particularly focusing on brain tumour detection within medical images.","RepVGG-GELAN leverages the RepVGG architecture to improve both speed and accuracy in detecting brain tumours.","Integrating RepVGG into the YOLO framework aims to achieve a balance between computational efficiency and detection performance.","This study includes a spatial pyramid pooling-based Generalized Efficient Layer Aggregation Network (GELAN) architecture which further enhances the capability of RepVGG.","Experimental evaluation conducted on a brain tumour dataset demonstrates the effectiveness of RepVGG-GELAN surpassing existing RCS-YOLO in terms of precision and speed.","Specifically, RepVGG-GELAN achieves an increased precision of 4.91% and an increased AP50 of 2.54% over the latest existing approach while operating at 240.7 GFLOPs.","The proposed RepVGG-GELAN with GELAN architecture presents promising results establishing itself as a state-of-the-art solution for accurate and efficient brain tumour detection in medical images.","The implementation code is publicly available at https://github.com/ThensiB/RepVGG-GELAN."],"url":"http://arxiv.org/abs/2405.03541v1","category":"cs.CV"}
{"created":"2024-05-06 14:55:37","title":"Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation","abstract":"Web phishing poses a dynamic threat, requiring detection systems to quickly adapt to the latest tactics. Traditional approaches of accumulating data and periodically retraining models are outpaced. We propose a novel paradigm combining federated learning and continual learning, enabling distributed nodes to continually update models on streams of new phishing data, without accumulating data. These locally adapted models are then aggregated at a central server via federated learning. To enhance detection, we introduce a custom attention-based classifier model with residual connections, tailored for web phishing, leveraging attention mechanisms to capture intricate phishing patterns. We evaluate our hybrid learning paradigm across continual learning strategies (cumulative, replay, MIR, LwF) and model architectures through an empirical investigation. Our main contributions are: (1) a new hybrid federated-continual learning paradigm for robust web phishing detection, and (2) a novel attention + residual connections based model explicitly designed for this task, attaining 0.93 accuracy, 0.90 precision, 0.96 recall and 0.93 f1-score with the LwF strategy, outperforming traditional approaches in detecting emerging phishing threats while retaining past knowledge.","sentences":["Web phishing poses a dynamic threat, requiring detection systems to quickly adapt to the latest tactics.","Traditional approaches of accumulating data and periodically retraining models are outpaced.","We propose a novel paradigm combining federated learning and continual learning, enabling distributed nodes to continually update models on streams of new phishing data, without accumulating data.","These locally adapted models are then aggregated at a central server via federated learning.","To enhance detection, we introduce a custom attention-based classifier model with residual connections, tailored for web phishing, leveraging attention mechanisms to capture intricate phishing patterns.","We evaluate our hybrid learning paradigm across continual learning strategies (cumulative, replay, MIR, LwF) and model architectures through an empirical investigation.","Our main contributions are: (1) a new hybrid federated-continual learning paradigm for robust web phishing detection, and (2) a novel attention + residual connections based model explicitly designed for this task, attaining 0.93 accuracy, 0.90 precision, 0.96 recall and 0.93 f1-score with the LwF strategy, outperforming traditional approaches in detecting emerging phishing threats while retaining past knowledge."],"url":"http://arxiv.org/abs/2405.03537v1","category":"cs.LG"}
{"created":"2024-05-06 14:52:23","title":"Meta-Evolve: Continuous Robot Evolution for One-to-many Policy Transfer","abstract":"We investigate the problem of transferring an expert policy from a source robot to multiple different robots. To solve this problem, we propose a method named $Meta$-$Evolve$ that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of tree-structured evolutionary robot sequences. The robot evolution tree allows the robot evolution paths to be shared, so our approach can significantly outperform naive one-to-one policy transfer. We present a heuristic approach to determine an optimized robot evolution tree. Experiments have shown that our method is able to improve the efficiency of one-to-three transfer of manipulation policy by up to 3.2$\\times$ and one-to-six transfer of agile locomotion policy by 2.4$\\times$ in terms of simulation cost over the baseline of launching multiple independent one-to-one policy transfers.","sentences":["We investigate the problem of transferring an expert policy from a source robot to multiple different robots.","To solve this problem, we propose a method named $Meta$-$Evolve$ that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of tree-structured evolutionary robot sequences.","The robot evolution tree allows the robot evolution paths to be shared, so our approach can significantly outperform naive one-to-one policy transfer.","We present a heuristic approach to determine an optimized robot evolution tree.","Experiments have shown that our method is able to improve the efficiency of one-to-three transfer of manipulation policy by up to 3.2$\\times$ and one-to-six transfer of agile locomotion policy by 2.4$\\times$ in terms of simulation cost over the baseline of launching multiple independent one-to-one policy transfers."],"url":"http://arxiv.org/abs/2405.03534v1","category":"cs.RO"}
{"created":"2024-05-06 14:40:50","title":"Exploring knowledge graph-based neural-symbolic system from application perspective","abstract":"The rapid advancement in artificial intelligence (AI), particularly through deep neural networks, has catalyzed significant progress in fields such as vision and text processing. Nonetheless, the pursuit of AI systems that exhibit human-like reasoning and interpretability continues to pose a substantial challenge. The Neural-Symbolic paradigm, which integrates the deep learning prowess of neural networks with the reasoning capabilities of symbolic systems, presents a promising pathway toward developing more transparent and comprehensible AI systems. Within this paradigm, the Knowledge Graph (KG) emerges as a crucial element, offering a structured and dynamic method for representing knowledge through interconnected entities and relationships, predominantly utilizing the triple (subject, predicate, object). This paper explores recent advancements in neural-symbolic integration based on KG, elucidating how KG underpins this integration across three key categories: enhancing the reasoning and interpretability of neural networks through the incorporation of symbolic knowledge (Symbol for Neural), refining the completeness and accuracy of symbolic systems via neural network methodologies (Neural for Symbol), and facilitating their combined application in Hybrid Neural-Symbolic Integration. It highlights current trends and proposes directions for future research in the domain of Neural-Symbolic AI.","sentences":["The rapid advancement in artificial intelligence (AI), particularly through deep neural networks, has catalyzed significant progress in fields such as vision and text processing.","Nonetheless, the pursuit of AI systems that exhibit human-like reasoning and interpretability continues to pose a substantial challenge.","The Neural-Symbolic paradigm, which integrates the deep learning prowess of neural networks with the reasoning capabilities of symbolic systems, presents a promising pathway toward developing more transparent and comprehensible AI systems.","Within this paradigm, the Knowledge Graph (KG) emerges as a crucial element, offering a structured and dynamic method for representing knowledge through interconnected entities and relationships, predominantly utilizing the triple (subject, predicate, object).","This paper explores recent advancements in neural-symbolic integration based on KG, elucidating how KG underpins this integration across three key categories: enhancing the reasoning and interpretability of neural networks through the incorporation of symbolic knowledge (Symbol for Neural), refining the completeness and accuracy of symbolic systems via neural network methodologies (Neural for Symbol), and facilitating their combined application in Hybrid Neural-Symbolic Integration.","It highlights current trends and proposes directions for future research in the domain of Neural-Symbolic AI."],"url":"http://arxiv.org/abs/2405.03524v1","category":"cs.AI"}
{"created":"2024-05-06 14:38:43","title":"Optimisation challenge for superconducting adiabatic neural network implementing XOR and OR boolean functions","abstract":"In this article, we consider designs of simple analog artificial neural networks based on adiabatic Josephson cells with a sigmoid activation function. A new approach based on the gradient descent method is developed to adjust the circuit parameters, allowing efficient signal transmission between the network layers. The proposed solution is demonstrated on the example of the system implementing XOR and OR logical operations.","sentences":["In this article, we consider designs of simple analog artificial neural networks based on adiabatic Josephson cells with a sigmoid activation function.","A new approach based on the gradient descent method is developed to adjust the circuit parameters, allowing efficient signal transmission between the network layers.","The proposed solution is demonstrated on the example of the system implementing XOR and OR logical operations."],"url":"http://arxiv.org/abs/2405.03521v1","category":"cond-mat.supr-con"}
{"created":"2024-05-06 14:37:07","title":"Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond","abstract":"General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems. Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws. In this survey, we embark on a comprehensive exploration of the latest advancements in world models. Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content. Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility. Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts. At last, we examine challenges and limitations of world models, and discuss their potential future directions. We hope this survey can serve as a foundational reference for the research community and inspire continued innovation. This survey will be regularly updated at: https://github.com/GigaAI-research/General-World-Models-Survey.","sentences":["General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems.","Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws.","In this survey, we embark on a comprehensive exploration of the latest advancements in world models.","Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content.","Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility.","Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts.","At last, we examine challenges and limitations of world models, and discuss their potential future directions.","We hope this survey can serve as a foundational reference for the research community and inspire continued innovation.","This survey will be regularly updated at: https://github.com/GigaAI-research/General-World-Models-Survey."],"url":"http://arxiv.org/abs/2405.03520v1","category":"cs.CV"}
{"created":"2024-05-06 14:13:38","title":"Boosting Single Positive Multi-label Classification with Generalized Robust Loss","abstract":"Multi-label learning (MLL) requires comprehensive multi-semantic annotations that is hard to fully obtain, thus often resulting in missing labels scenarios. In this paper, we investigate Single Positive Multi-label Learning (SPML), where each image is associated with merely one positive label. Existing SPML methods only focus on designing losses using mechanisms such as hard pseudo-labeling and robust losses, mostly leading to unacceptable false negatives. To address this issue, we first propose a generalized loss framework based on expected risk minimization to provide soft pseudo labels, and point out that the former losses can be seamlessly converted into our framework. In particular, we design a novel robust loss based on our framework, which enjoys flexible coordination between false positives and false negatives, and can additionally deal with the imbalance between positive and negative samples. Extensive experiments show that our approach can significantly improve SPML performance and outperform the vast majority of state-of-the-art methods on all the four benchmarks.","sentences":["Multi-label learning (MLL) requires comprehensive multi-semantic annotations that is hard to fully obtain, thus often resulting in missing labels scenarios.","In this paper, we investigate Single Positive Multi-label Learning (SPML), where each image is associated with merely one positive label.","Existing SPML methods only focus on designing losses using mechanisms such as hard pseudo-labeling and robust losses, mostly leading to unacceptable false negatives.","To address this issue, we first propose a generalized loss framework based on expected risk minimization to provide soft pseudo labels, and point out that the former losses can be seamlessly converted into our framework.","In particular, we design a novel robust loss based on our framework, which enjoys flexible coordination between false positives and false negatives, and can additionally deal with the imbalance between positive and negative samples.","Extensive experiments show that our approach can significantly improve SPML performance and outperform the vast majority of state-of-the-art methods on all the four benchmarks."],"url":"http://arxiv.org/abs/2405.03501v1","category":"cs.LG"}
{"created":"2024-05-06 14:11:36","title":"A Rate-Distortion-Classification Approach for Lossy Image Compression","abstract":"In lossy image compression, the objective is to achieve minimal signal distortion while compressing images to a specified bit rate. The increasing demand for visual analysis applications, particularly in classification tasks, has emphasized the significance of considering semantic distortion in compressed images. To bridge the gap between image compression and visual analysis, we propose a Rate-Distortion-Classification (RDC) model for lossy image compression, offering a unified framework to optimize the trade-off between rate, distortion, and classification accuracy. The RDC model is extensively analyzed both statistically on a multi-distribution source and experimentally on the widely used MNIST dataset. The findings reveal that the RDC model exhibits desirable properties, including monotonic non-increasing and convex functions, under certain conditions. This work provides insights into the development of human-machine friendly compression methods and Video Coding for Machine (VCM) approaches, paving the way for end-to-end image compression techniques in real-world applications.","sentences":["In lossy image compression, the objective is to achieve minimal signal distortion while compressing images to a specified bit rate.","The increasing demand for visual analysis applications, particularly in classification tasks, has emphasized the significance of considering semantic distortion in compressed images.","To bridge the gap between image compression and visual analysis, we propose a Rate-Distortion-Classification (RDC) model for lossy image compression, offering a unified framework to optimize the trade-off between rate, distortion, and classification accuracy.","The RDC model is extensively analyzed both statistically on a multi-distribution source and experimentally on the widely used MNIST dataset.","The findings reveal that the RDC model exhibits desirable properties, including monotonic non-increasing and convex functions, under certain conditions.","This work provides insights into the development of human-machine friendly compression methods and Video Coding for Machine (VCM) approaches, paving the way for end-to-end image compression techniques in real-world applications."],"url":"http://arxiv.org/abs/2405.03500v1","category":"cs.MM"}
{"created":"2024-05-06 14:02:59","title":"Jointly Learning Cost and Constraints from Demonstrations for Safe Trajectory Generation","abstract":"Learning from Demonstration allows robots to mimic human actions. However, these methods do not model constraints crucial to ensure safety of the learned skill. Moreover, even when explicitly modelling constraints, they rely on the assumption of a known cost function, which limits their practical usability for task with unknown cost. In this work we propose a two-step optimization process that allow to estimate cost and constraints by decoupling the learning of cost functions from the identification of unknown constraints within the demonstrated trajectories. Initially, we identify the cost function by isolating the effect of constraints on parts of the demonstrations. Subsequently, a constraint leaning method is used to identify the unknown constraints. Our approach is validated both on simulated trajectories and a real robotic manipulation task. Our experiments show the impact that incorrect cost estimation has on the learned constraints and illustrate how the proposed method is able to infer unknown constraints, such as obstacles, from demonstrated trajectories without any initial knowledge of the cost.","sentences":["Learning from Demonstration allows robots to mimic human actions.","However, these methods do not model constraints crucial to ensure safety of the learned skill.","Moreover, even when explicitly modelling constraints, they rely on the assumption of a known cost function, which limits their practical usability for task with unknown cost.","In this work we propose a two-step optimization process that allow to estimate cost and constraints by decoupling the learning of cost functions from the identification of unknown constraints within the demonstrated trajectories.","Initially, we identify the cost function by isolating the effect of constraints on parts of the demonstrations.","Subsequently, a constraint leaning method is used to identify the unknown constraints.","Our approach is validated both on simulated trajectories and a real robotic manipulation task.","Our experiments show the impact that incorrect cost estimation has on the learned constraints and illustrate how the proposed method is able to infer unknown constraints, such as obstacles, from demonstrated trajectories without any initial knowledge of the cost."],"url":"http://arxiv.org/abs/2405.03491v1","category":"cs.RO"}
{"created":"2024-05-06 13:53:03","title":"Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search","abstract":"The future of conversational agents will provide users with personalized information responses. However, a significant challenge in developing models is the lack of large-scale dialogue datasets that span multiple sessions and reflect real-world user preferences. Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks. Our method, LAPS, addresses this by using large language models (LLMs) to guide a single human worker in generating personalized dialogues. This method has proven to speed up the creation process and improve quality. LAPS can collect large-scale, human-written, multi-session, and multi-domain conversations, including extracting user preferences. When compared to existing datasets, LAPS-produced conversations are as natural and diverse as expert-created ones, which stays in contrast with fully synthetic methods. The collected dataset is suited to train preference extraction and personalized response generation. Our results show that responses generated explicitly using extracted preferences better match user's actual preferences, highlighting the value of using extracted preferences over simple dialogue history. Overall, LAPS introduces a new method to leverage LLMs to create realistic personalized conversational data more efficiently and effectively than previous methods.","sentences":["The future of conversational agents will provide users with personalized information responses.","However, a significant challenge in developing models is the lack of large-scale dialogue datasets that span multiple sessions and reflect real-world user preferences.","Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks.","Our method, LAPS, addresses this by using large language models (LLMs) to guide a single human worker in generating personalized dialogues.","This method has proven to speed up the creation process and improve quality.","LAPS can collect large-scale, human-written, multi-session, and multi-domain conversations, including extracting user preferences.","When compared to existing datasets, LAPS-produced conversations are as natural and diverse as expert-created ones, which stays in contrast with fully synthetic methods.","The collected dataset is suited to train preference extraction and personalized response generation.","Our results show that responses generated explicitly using extracted preferences better match user's actual preferences, highlighting the value of using extracted preferences over simple dialogue history.","Overall, LAPS introduces a new method to leverage LLMs to create realistic personalized conversational data more efficiently and effectively than previous methods."],"url":"http://arxiv.org/abs/2405.03480v1","category":"cs.IR"}
{"created":"2024-05-06 13:51:02","title":"DexSkills: Skill Segmentation Using Haptic Data for Learning Autonomous Long-Horizon Robotic Manipulation Tasks","abstract":"Effective execution of long-horizon tasks with dexterous robotic hands remains a significant challenge in real-world problems. While learning from human demonstrations have shown encouraging results, they require extensive data collection for training. Hence, decomposing long-horizon tasks into reusable primitive skills is a more efficient approach. To achieve so, we developed DexSkills, a novel supervised learning framework that addresses long-horizon dexterous manipulation tasks using primitive skills. DexSkills is trained to recognize and replicate a select set of skills using human demonstration data, which can then segment a demonstrated long-horizon dexterous manipulation task into a sequence of primitive skills to achieve one-shot execution by the robot directly. Significantly, DexSkills operates solely on proprioceptive and tactile data, i.e., haptic data. Our real-world robotic experiments show that DexSkills can accurately segment skills, thereby enabling autonomous robot execution of a diverse range of tasks.","sentences":["Effective execution of long-horizon tasks with dexterous robotic hands remains a significant challenge in real-world problems.","While learning from human demonstrations have shown encouraging results, they require extensive data collection for training.","Hence, decomposing long-horizon tasks into reusable primitive skills is a more efficient approach.","To achieve so, we developed DexSkills, a novel supervised learning framework that addresses long-horizon dexterous manipulation tasks using primitive skills.","DexSkills is trained to recognize and replicate a select set of skills using human demonstration data, which can then segment a demonstrated long-horizon dexterous manipulation task into a sequence of primitive skills to achieve one-shot execution by the robot directly.","Significantly, DexSkills operates solely on proprioceptive and tactile data, i.e., haptic data.","Our real-world robotic experiments show that DexSkills can accurately segment skills, thereby enabling autonomous robot execution of a diverse range of tasks."],"url":"http://arxiv.org/abs/2405.03476v1","category":"cs.RO"}
{"created":"2024-05-06 13:44:42","title":"Welfare Loss in Connected Resource Allocation","abstract":"We study the allocation of indivisible goods that form an undirected graph and investigate the worst-case welfare loss when requiring that each agent must receive a connected subgraph. Our focus is on both egalitarian and utilitarian welfare. Specifically, we introduce the concept of egalitarian (resp., utilitarian) price of connectivity, which captures the worst-case ratio between the optimal egalitarian (resp., utilitarian) welfare among all allocations and that among the connected allocations. We provide tight or asymptotically tight bounds on the price of connectivity for various large classes of graphs when there are two agents as well as for paths, stars and cycles in the general case. Many of our results are supplemented with algorithms which find connected allocations with a welfare guarantee corresponding to the price of connectivity.","sentences":["We study the allocation of indivisible goods that form an undirected graph and investigate the worst-case welfare loss when requiring that each agent must receive a connected subgraph.","Our focus is on both egalitarian and utilitarian welfare.","Specifically, we introduce the concept of egalitarian (resp., utilitarian) price of connectivity, which captures the worst-case ratio between the optimal egalitarian (resp., utilitarian) welfare among all allocations and that among the connected allocations.","We provide tight or asymptotically tight bounds on the price of connectivity for various large classes of graphs when there are two agents as well as for paths, stars and cycles in the general case.","Many of our results are supplemented with algorithms which find connected allocations with a welfare guarantee corresponding to the price of connectivity."],"url":"http://arxiv.org/abs/2405.03467v1","category":"cs.GT"}
{"created":"2024-05-06 13:33:38","title":"A Lightweight Neural Architecture Search Model for Medical Image Classification","abstract":"Accurate classification of medical images is essential for modern diagnostics. Deep learning advancements led clinicians to increasingly use sophisticated models to make faster and more accurate decisions, sometimes replacing human judgment. However, model development is costly and repetitive. Neural Architecture Search (NAS) provides solutions by automating the design of deep learning architectures. This paper presents ZO-DARTS+, a differentiable NAS algorithm that improves search efficiency through a novel method of generating sparse probabilities by bi-level optimization. Experiments on five public medical datasets show that ZO-DARTS+ matches the accuracy of state-of-the-art solutions while reducing search times by up to three times.","sentences":["Accurate classification of medical images is essential for modern diagnostics.","Deep learning advancements led clinicians to increasingly use sophisticated models to make faster and more accurate decisions, sometimes replacing human judgment.","However, model development is costly and repetitive.","Neural Architecture Search (NAS) provides solutions by automating the design of deep learning architectures.","This paper presents ZO-DARTS+, a differentiable NAS algorithm that improves search efficiency through a novel method of generating sparse probabilities by bi-level optimization.","Experiments on five public medical datasets show that ZO-DARTS+ matches the accuracy of state-of-the-art solutions while reducing search times by up to three times."],"url":"http://arxiv.org/abs/2405.03462v1","category":"cs.CV"}
{"created":"2024-05-06 13:32:53","title":"Variations on a Theme of Makowski","abstract":"We distinguish the axiomatic study of proofs in geometry from study about geometry from general axioms for mathematics. We briefly report on an abuse of that distinction and its unfortunate effect on US high school education. We review a number of 20th century approaches to synthetic geometry. In doing so, we disambiguate (in the Wikipedia sense) the terms: metric, orthogonal, isotropic and hyperbolic. With some of these systems we are able to axiomatize `affine geometry' over the complex field (The argument is trivial from [Wu94] or [Szm78], but not remarked by either of them.). We examine the general question of the connections between axioms for Affine geometries and the stability classification of associated complete first order theories of fields. We conclude with reminiscences of a half-century friendship with Jan\\'{o}s.","sentences":["We distinguish the axiomatic study of proofs in geometry from study about geometry from general axioms for mathematics.","We briefly report on an abuse of that distinction and its unfortunate effect on US high school education.","We review a number of 20th century approaches to synthetic geometry.","In doing so, we disambiguate (in the Wikipedia sense) the terms: metric, orthogonal, isotropic and hyperbolic.","With some of these systems we are able to axiomatize `affine geometry' over the complex field (The argument is trivial from [Wu94] or [Szm78], but not remarked by either of them.).","We examine the general question of the connections between axioms for Affine geometries and the stability classification of associated complete first order theories of fields.","We conclude with reminiscences of a half-century friendship with Jan\\'{o}s."],"url":"http://arxiv.org/abs/2405.03461v1","category":"math.MG"}
{"created":"2024-05-06 13:23:57","title":"Large Language Models (LLMs) as Agents for Augmented Democracy","abstract":"We explore the capabilities of an augmented democracy system built on off-the-shelf LLMs fine-tuned on data summarizing individual preferences across 67 policy proposals collected during the 2022 Brazilian presidential elections. We use a train-test cross-validation setup to estimate the accuracy with which the LLMs predict both: a subject's individual political choices and the aggregate preferences of the full sample of participants. At the individual level, the accuracy of the out of sample predictions lie in the range 69%-76% and are significantly better at predicting the preferences of liberal and college educated participants. At the population level, we aggregate preferences using an adaptation of the Borda score and compare the ranking of policy proposals obtained from a probabilistic sample of participants and from data augmented using LLMs. We find that the augmented data predicts the preferences of the full population of participants better than probabilistic samples alone when these represent less than 30% to 40% of the total population. These results indicate that LLMs are potentially useful for the construction of systems of augmented democracy.","sentences":["We explore the capabilities of an augmented democracy system built on off-the-shelf LLMs fine-tuned on data summarizing individual preferences across 67 policy proposals collected during the 2022 Brazilian presidential elections.","We use a train-test cross-validation setup to estimate the accuracy with which the LLMs predict both: a subject's individual political choices and the aggregate preferences of the full sample of participants.","At the individual level, the accuracy of the out of sample predictions lie in the range 69%-76% and are significantly better at predicting the preferences of liberal and college educated participants.","At the population level, we aggregate preferences using an adaptation of the Borda score and compare the ranking of policy proposals obtained from a probabilistic sample of participants and from data augmented using LLMs.","We find that the augmented data predicts the preferences of the full population of participants better than probabilistic samples alone when these represent less than 30% to 40% of the total population.","These results indicate that LLMs are potentially useful for the construction of systems of augmented democracy."],"url":"http://arxiv.org/abs/2405.03452v2","category":"cs.CY"}
{"created":"2024-05-06 13:17:43","title":"SEvenLLM: Benchmarking, Eliciting, and Enhancing Abilities of Large Language Models in Cyber Threat Intelligence","abstract":"To address the increasing complexity and frequency of cybersecurity incidents emphasized by the recent cybersecurity threat reports with over 10 billion instances, cyber threat intelligence (CTI) plays a critical role in the modern cybersecurity landscape by offering the insights required to understand and combat the constantly evolving nature of cyber threats. Inspired by the powerful capability of large language models (LLMs) in handling complex tasks, in this paper, we introduce a framework to benchmark, elicit, and improve cybersecurity incident analysis and response abilities in LLMs for Security Events (SEvenLLM). Specifically, we create a high-quality bilingual instruction corpus by crawling cybersecurity raw text from cybersecurity websites to overcome the lack of effective data for information extraction. Then, we design a pipeline to auto-select tasks from the tasks pool and convert the raw text into supervised corpora comprised of question and response. The instruction dataset SEvenLLM-Instruct is used to train cybersecurity LLMs with the multi-task learning objective (27 well-designed tasks) for augmenting the analysis of cybersecurity events. Extensive experiments in our curated benchmark (SEvenLLM-bench) demonstrate that SEvenLLM performs more sophisticated threat analysis and fortifies defenses against the evolving landscape of cyber threats.","sentences":["To address the increasing complexity and frequency of cybersecurity incidents emphasized by the recent cybersecurity threat reports with over 10 billion instances, cyber threat intelligence (CTI) plays a critical role in the modern cybersecurity landscape by offering the insights required to understand and combat the constantly evolving nature of cyber threats.","Inspired by the powerful capability of large language models (LLMs) in handling complex tasks, in this paper, we introduce a framework to benchmark, elicit, and improve cybersecurity incident analysis and response abilities in LLMs for Security Events (SEvenLLM).","Specifically, we create a high-quality bilingual instruction corpus by crawling cybersecurity raw text from cybersecurity websites to overcome the lack of effective data for information extraction.","Then, we design a pipeline to auto-select tasks from the tasks pool and convert the raw text into supervised corpora comprised of question and response.","The instruction dataset SEvenLLM-Instruct is used to train cybersecurity LLMs with the multi-task learning objective (27 well-designed tasks) for augmenting the analysis of cybersecurity events.","Extensive experiments in our curated benchmark (SEvenLLM-bench) demonstrate that SEvenLLM performs more sophisticated threat analysis and fortifies defenses against the evolving landscape of cyber threats."],"url":"http://arxiv.org/abs/2405.03446v1","category":"cs.CR"}
{"created":"2024-05-06 13:12:25","title":"Robotic Constrained Imitation Learning for the Peg Transfer Task in Fundamentals of Laparoscopic Surgery","abstract":"In this study, we present an implementation strategy for a robot that performs peg transfer tasks in Fundamentals of Laparoscopic Surgery (FLS) via imitation learning, aimed at the development of an autonomous robot for laparoscopic surgery. Robotic laparoscopic surgery presents two main challenges: (1) the need to manipulate forceps using ports established on the body surface as fulcrums, and (2) difficulty in perceiving depth information when working with a monocular camera that displays its images on a monitor. Especially, regarding issue (2), most prior research has assumed the availability of depth images or models of a target to be operated on. Therefore, in this study, we achieve more accurate imitation learning with only monocular images by extracting motion constraints from one exemplary motion of skilled operators, collecting data based on these constraints, and conducting imitation learning based on the collected data. We implemented an overall system using two Franka Emika Panda Robot Arms and validated its effectiveness.","sentences":["In this study, we present an implementation strategy for a robot that performs peg transfer tasks in Fundamentals of Laparoscopic Surgery (FLS) via imitation learning, aimed at the development of an autonomous robot for laparoscopic surgery.","Robotic laparoscopic surgery presents two main challenges: (1) the need to manipulate forceps using ports established on the body surface as fulcrums, and (2) difficulty in perceiving depth information when working with a monocular camera that displays its images on a monitor.","Especially, regarding issue (2), most prior research has assumed the availability of depth images or models of a target to be operated on.","Therefore, in this study, we achieve more accurate imitation learning with only monocular images by extracting motion constraints from one exemplary motion of skilled operators, collecting data based on these constraints, and conducting imitation learning based on the collected data.","We implemented an overall system using two Franka Emika Panda Robot Arms and validated its effectiveness."],"url":"http://arxiv.org/abs/2405.03440v1","category":"cs.RO"}
{"created":"2024-05-06 12:58:48","title":"A method for quantifying the generalization capabilities of generative models for solving Ising models","abstract":"For Ising models with complex energy landscapes, whether the ground state can be found by neural networks depends heavily on the Hamming distance between the training datasets and the ground state. Despite the fact that various recently proposed generative models have shown good performance in solving Ising models, there is no adequate discussion on how to quantify their generalization capabilities. Here we design a Hamming distance regularizer in the framework of a class of generative models, variational autoregressive networks (VAN), to quantify the generalization capabilities of various network architectures combined with VAN. The regularizer can control the size of the overlaps between the ground state and the training datasets generated by networks, which, together with the success rates of finding the ground state, form a quantitative metric to quantify their generalization capabilities. We conduct numerical experiments on several prototypical network architectures combined with VAN, including feed-forward neural networks, recurrent neural networks, and graph neural networks, to quantify their generalization capabilities when solving Ising models. Moreover, considering the fact that the quantification of the generalization capabilities of networks on small-scale problems can be used to predict their relative performance on large-scale problems, our method is of great significance for assisting in the Neural Architecture Search field of searching for the optimal network architectures when solving large-scale Ising models.","sentences":["For Ising models with complex energy landscapes, whether the ground state can be found by neural networks depends heavily on the Hamming distance between the training datasets and the ground state.","Despite the fact that various recently proposed generative models have shown good performance in solving Ising models, there is no adequate discussion on how to quantify their generalization capabilities.","Here we design a Hamming distance regularizer in the framework of a class of generative models, variational autoregressive networks (VAN), to quantify the generalization capabilities of various network architectures combined with VAN.","The regularizer can control the size of the overlaps between the ground state and the training datasets generated by networks, which, together with the success rates of finding the ground state, form a quantitative metric to quantify their generalization capabilities.","We conduct numerical experiments on several prototypical network architectures combined with VAN, including feed-forward neural networks, recurrent neural networks, and graph neural networks, to quantify their generalization capabilities when solving Ising models.","Moreover, considering the fact that the quantification of the generalization capabilities of networks on small-scale problems can be used to predict their relative performance on large-scale problems, our method is of great significance for assisting in the Neural Architecture Search field of searching for the optimal network architectures when solving large-scale Ising models."],"url":"http://arxiv.org/abs/2405.03435v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-06 12:48:34","title":"ReCycle: Fast and Efficient Long Time Series Forecasting with Residual Cyclic Transformers","abstract":"Transformers have recently gained prominence in long time series forecasting by elevating accuracies in a variety of use cases. Regrettably, in the race for better predictive performance the overhead of model architectures has grown onerous, leading to models with computational demand infeasible for most practical applications. To bridge the gap between high method complexity and realistic computational resources, we introduce the Residual Cyclic Transformer, ReCycle. ReCycle utilizes primary cycle compression to address the computational complexity of the attention mechanism in long time series. By learning residuals from refined smoothing average techniques, ReCycle surpasses state-of-the-art accuracy in a variety of application use cases. The reliable and explainable fallback behavior ensured by simple, yet robust, smoothing average techniques additionally lowers the barrier for user acceptance. At the same time, our approach reduces the run time and energy consumption by more than an order of magnitude, making both training and inference feasible on low-performance, low-power and edge computing devices. Code is available at https://github.com/Helmholtz-AI-Energy/ReCycle","sentences":["Transformers have recently gained prominence in long time series forecasting by elevating accuracies in a variety of use cases.","Regrettably, in the race for better predictive performance the overhead of model architectures has grown onerous, leading to models with computational demand infeasible for most practical applications.","To bridge the gap between high method complexity and realistic computational resources, we introduce the Residual Cyclic Transformer, ReCycle.","ReCycle utilizes primary cycle compression to address the computational complexity of the attention mechanism in long time series.","By learning residuals from refined smoothing average techniques, ReCycle surpasses state-of-the-art accuracy in a variety of application use cases.","The reliable and explainable fallback behavior ensured by simple, yet robust, smoothing average techniques additionally lowers the barrier for user acceptance.","At the same time, our approach reduces the run time and energy consumption by more than an order of magnitude, making both training and inference feasible on low-performance, low-power and edge computing devices.","Code is available at https://github.com/Helmholtz-AI-Energy/ReCycle"],"url":"http://arxiv.org/abs/2405.03429v1","category":"cs.LG"}
{"created":"2024-05-06 12:46:43","title":"EdgeAlpha: Bringing Process Discovery to the Data Sources","abstract":"Process Mining is moving beyond mining traditional event logs and nowadays includes, for example, data sourced from sensors in the Internet of Things (IoT). The volume and velocity of data generated by such sensors makes it increasingly challenging for traditional process discovery algorithms to store and mine such data in traditional event logs. Further, privacy considerations often prevent data collection at a central location in the first place. To address this challenge, this paper introduces EdgeAlpha, a distributed algorithm for process discovery operating directly on sensor nodes and edge devices on a stream of real-time event data. Based on the Alpha Miner, EdgeAlpha tracks each event and its predecessor and successor events directly on the sensor node where the event is sensed and recorded. From this local view, each node in EdgeAlpha derives a partial footprint matrix, which we then merge at a central location, whenever we query the system to compute a process model. EdgeAlpha enables (a) scalable mining, as a node, for each event, only interacts with its predecessors and, when queried, only exchanges aggregates, i.e., partial footprint matrices, with the central location and (b) privacy preserving process mining, as nodes only store their own as well as predecessor and successor events. On the Sepsis Cases event log, for example, a node queries on average 18.7% of all nodes. For the Hospital Log, we can even reduce the overall querying to 3.87% of the nodes.","sentences":["Process Mining is moving beyond mining traditional event logs and nowadays includes, for example, data sourced from sensors in the Internet of Things (IoT).","The volume and velocity of data generated by such sensors makes it increasingly challenging for traditional process discovery algorithms to store and mine such data in traditional event logs.","Further, privacy considerations often prevent data collection at a central location in the first place.","To address this challenge, this paper introduces EdgeAlpha, a distributed algorithm for process discovery operating directly on sensor nodes and edge devices on a stream of real-time event data.","Based on the Alpha Miner, EdgeAlpha tracks each event and its predecessor and successor events directly on the sensor node where the event is sensed and recorded.","From this local view, each node in EdgeAlpha derives a partial footprint matrix, which we then merge at a central location, whenever we query the system to compute a process model.","EdgeAlpha enables (a) scalable mining, as a node, for each event, only interacts with its predecessors and, when queried, only exchanges aggregates, i.e., partial footprint matrices, with the central location and (b) privacy preserving process mining, as nodes only store their own as well as predecessor and successor events.","On the Sepsis Cases event log, for example, a node queries on average 18.7% of all nodes.","For the Hospital Log, we can even reduce the overall querying to 3.87% of the nodes."],"url":"http://arxiv.org/abs/2405.03426v1","category":"cs.DB"}
{"created":"2024-05-06 12:16:53","title":"Automated Computation of Therapies Using Failure Mode and Effects Analysis in the Medical Domain","abstract":"Failure mode and effects analysis (FMEA) is a systematic approach to identify and analyse potential failures and their effects in a system or process. The FMEA approach, however, requires domain experts to manually analyse the FMEA model to derive risk-reducing actions that should be applied. In this paper, we provide a formal framework to allow for automatic planning and acting in FMEA models. More specifically, we cast the FMEA model into a Markov decision process which can then be solved by existing solvers. We show that the FMEA approach can not only be used to support medical experts during the modelling process but also to automatically derive optimal therapies for the treatment of patients.","sentences":["Failure mode and effects analysis (FMEA) is a systematic approach to identify and analyse potential failures and their effects in a system or process.","The FMEA approach, however, requires domain experts to manually analyse the FMEA model to derive risk-reducing actions that should be applied.","In this paper, we provide a formal framework to allow for automatic planning and acting in FMEA models.","More specifically, we cast the FMEA model into a Markov decision process which can then be solved by existing solvers.","We show that the FMEA approach can not only be used to support medical experts during the modelling process but also to automatically derive optimal therapies for the treatment of patients."],"url":"http://arxiv.org/abs/2405.03406v1","category":"cs.AI"}
{"created":"2024-05-06 12:11:46","title":"E2GNN: Efficient Graph Neural Network Ensembles for Semi-Supervised Classification","abstract":"This work studies ensemble learning for graph neural networks (GNNs) under the popular semi-supervised setting. Ensemble learning has shown superiority in improving the accuracy and robustness of traditional machine learning by combining the outputs of multiple weak learners. However, adopting a similar idea to integrate different GNN models is challenging because of two reasons. First, GNN is notorious for its poor inference ability, so naively assembling multiple GNN models would deteriorate the inference efficiency. Second, when GNN models are trained with few labeled nodes, their performance are limited. In this case, the vanilla ensemble approach, e.g., majority vote, may be sub-optimal since most base models, i.e., GNNs, may make the wrong predictions. To this end, in this paper, we propose an efficient ensemble learner--E2GNN to assemble multiple GNNs in a learnable way by leveraging both labeled and unlabeled nodes. Specifically, we first pre-train different GNN models on a given data scenario according to the labeled nodes. Next, instead of directly combing their outputs for label inference, we train a simple multi-layer perceptron--MLP model to mimic their predictions on both labeled and unlabeled nodes. Then the unified MLP model is deployed to infer labels for unlabeled or new nodes. Since the predictions of unlabeled nodes from different GNN models may be incorrect, we develop a reinforced discriminator to effectively filter out those wrongly predicted nodes to boost the performance of MLP. By doing this, we suggest a principled approach to tackle the inference issues of GNN ensembles and maintain the merit of ensemble learning: improved performance. Comprehensive experiments over both transductive and inductive settings, across different GNN backbones and 8 benchmark datasets, demonstrate the superiority of E2GNN.","sentences":["This work studies ensemble learning for graph neural networks (GNNs) under the popular semi-supervised setting.","Ensemble learning has shown superiority in improving the accuracy and robustness of traditional machine learning by combining the outputs of multiple weak learners.","However, adopting a similar idea to integrate different GNN models is challenging because of two reasons.","First, GNN is notorious for its poor inference ability, so naively assembling multiple GNN models would deteriorate the inference efficiency.","Second, when GNN models are trained with few labeled nodes, their performance are limited.","In this case, the vanilla ensemble approach, e.g., majority vote, may be sub-optimal since most base models, i.e., GNNs, may make the wrong predictions.","To this end, in this paper, we propose an efficient ensemble learner--E2GNN to assemble multiple GNNs in a learnable way by leveraging both labeled and unlabeled nodes.","Specifically, we first pre-train different GNN models on a given data scenario according to the labeled nodes.","Next, instead of directly combing their outputs for label inference, we train a simple multi-layer perceptron--MLP model to mimic their predictions on both labeled and unlabeled nodes.","Then the unified MLP model is deployed to infer labels for unlabeled or new nodes.","Since the predictions of unlabeled nodes from different GNN models may be incorrect, we develop a reinforced discriminator to effectively filter out those wrongly predicted nodes to boost the performance of MLP.","By doing this, we suggest a principled approach to tackle the inference issues of GNN ensembles and maintain the merit of ensemble learning: improved performance.","Comprehensive experiments over both transductive and inductive settings, across different GNN backbones and 8 benchmark datasets, demonstrate the superiority of E2GNN."],"url":"http://arxiv.org/abs/2405.03401v1","category":"cs.LG"}
{"created":"2024-05-06 11:51:09","title":"Don't Waste Your Time: Early Stopping Cross-Validation","abstract":"State-of-the-art automated machine learning systems for tabular data often employ cross-validation; ensuring that measured performances generalize to unseen data, or that subsequent ensembling does not overfit. However, using k-fold cross-validation instead of holdout validation drastically increases the computational cost of validating a single configuration. While ensuring better generalization and, by extension, better performance, the additional cost is often prohibitive for effective model selection within a time budget. We aim to make model selection with cross-validation more effective. Therefore, we study early stopping the process of cross-validation during model selection. We investigate the impact of early stopping on random search for two algorithms, MLP and random forest, across 36 classification datasets. We further analyze the impact of the number of folds by considering 3-, 5-, and 10-folds. In addition, we investigate the impact of early stopping with Bayesian optimization instead of random search and also repeated cross-validation. Our exploratory study shows that even a simple-to-understand and easy-to-implement method consistently allows model selection to converge faster; in ~94% of all datasets, on average by ~214%. Moreover, stopping cross-validation enables model selection to explore the search space more exhaustively by considering +167% configurations on average within one hour, while also obtaining better overall performance.","sentences":["State-of-the-art automated machine learning systems for tabular data often employ cross-validation; ensuring that measured performances generalize to unseen data, or that subsequent ensembling does not overfit.","However, using k-fold cross-validation instead of holdout validation drastically increases the computational cost of validating a single configuration.","While ensuring better generalization and, by extension, better performance, the additional cost is often prohibitive for effective model selection within a time budget.","We aim to make model selection with cross-validation more effective.","Therefore, we study early stopping the process of cross-validation during model selection.","We investigate the impact of early stopping on random search for two algorithms, MLP and random forest, across 36 classification datasets.","We further analyze the impact of the number of folds by considering 3-, 5-, and 10-folds.","In addition, we investigate the impact of early stopping with Bayesian optimization instead of random search and also repeated cross-validation.","Our exploratory study shows that even a simple-to-understand and easy-to-implement method consistently allows model selection to converge faster; in ~94% of all datasets, on average by ~214%.","Moreover, stopping cross-validation enables model selection to explore the search space more exhaustively by considering +167% configurations on average within one hour, while also obtaining better overall performance."],"url":"http://arxiv.org/abs/2405.03389v1","category":"cs.LG"}
{"created":"2024-05-06 11:45:59","title":"The high dimensional psychological profile and cultural bias of ChatGPT","abstract":"Given the rapid advancement of large-scale language models, artificial intelligence (AI) models, like ChatGPT, are playing an increasingly prominent role in human society. However, to ensure that artificial intelligence models benefit human society, we must first fully understand the similarities and differences between the human-like characteristics exhibited by artificial intelligence models and real humans, as well as the cultural stereotypes and biases that artificial intelligence models may exhibit in the process of interacting with humans. This study first measured ChatGPT in 84 dimensions of psychological characteristics, revealing differences between ChatGPT and human norms in most dimensions as well as in high-dimensional psychological representations. Additionally, through the measurement of ChatGPT in 13 dimensions of cultural values, it was revealed that ChatGPT's cultural value patterns are dissimilar to those of various countries/regions worldwide. Finally, an analysis of ChatGPT's performance in eight decision-making tasks involving interactions with humans from different countries/regions revealed that ChatGPT exhibits clear cultural stereotypes in most decision-making tasks and shows significant cultural bias in third-party punishment and ultimatum games. The findings indicate that, compared to humans, ChatGPT exhibits a distinct psychological profile and cultural value orientation, and it also shows cultural biases and stereotypes in interpersonal decision-making. Future research endeavors should emphasize enhanced technical oversight and augmented transparency in the database and algorithmic training procedures to foster more efficient cross-cultural communication and mitigate social disparities.","sentences":["Given the rapid advancement of large-scale language models, artificial intelligence (AI) models, like ChatGPT, are playing an increasingly prominent role in human society.","However, to ensure that artificial intelligence models benefit human society, we must first fully understand the similarities and differences between the human-like characteristics exhibited by artificial intelligence models and real humans, as well as the cultural stereotypes and biases that artificial intelligence models may exhibit in the process of interacting with humans.","This study first measured ChatGPT in 84 dimensions of psychological characteristics, revealing differences between ChatGPT and human norms in most dimensions as well as in high-dimensional psychological representations.","Additionally, through the measurement of ChatGPT in 13 dimensions of cultural values, it was revealed that ChatGPT's cultural value patterns are dissimilar to those of various countries/regions worldwide.","Finally, an analysis of ChatGPT's performance in eight decision-making tasks involving interactions with humans from different countries/regions revealed that ChatGPT exhibits clear cultural stereotypes in most decision-making tasks and shows significant cultural bias in third-party punishment and ultimatum games.","The findings indicate that, compared to humans, ChatGPT exhibits a distinct psychological profile and cultural value orientation, and it also shows cultural biases and stereotypes in interpersonal decision-making.","Future research endeavors should emphasize enhanced technical oversight and augmented transparency in the database and algorithmic training procedures to foster more efficient cross-cultural communication and mitigate social disparities."],"url":"http://arxiv.org/abs/2405.03387v1","category":"cs.CL"}
{"created":"2024-05-06 11:43:01","title":"GLIP: Electromagnetic Field Exposure Map Completion by Deep Generative Networks","abstract":"In Spectrum cartography (SC), the generation of exposure maps for radio frequency electromagnetic fields (RF-EMF) spans dimensions of frequency, space, and time, which relies on a sparse collection of sensor data, posing a challenging ill-posed inverse problem. Cartography methods based on models integrate designed priors, such as sparsity and low-rank structures, to refine the solution of this inverse problem. In our previous work, EMF exposure map reconstruction was achieved by Generative Adversarial Networks (GANs) where physical laws or structural constraints were employed as a prior, but they require a large amount of labeled data or simulated full maps for training to produce efficient results. In this paper, we present a method to reconstruct EMF exposure maps using only the generator network in GANs which does not require explicit training, thus overcoming the limitations of GANs, such as using reference full exposure maps. This approach uses a prior from sensor data as Local Image Prior (LIP) captured by deep convolutional generative networks independent of learning the network parameters from images in an urban environment. Experimental results show that, even when only sparse sensor data are available, our method can produce accurate estimates.","sentences":["In Spectrum cartography (SC), the generation of exposure maps for radio frequency electromagnetic fields (RF-EMF) spans dimensions of frequency, space, and time, which relies on a sparse collection of sensor data, posing a challenging ill-posed inverse problem.","Cartography methods based on models integrate designed priors, such as sparsity and low-rank structures, to refine the solution of this inverse problem.","In our previous work, EMF exposure map reconstruction was achieved by Generative Adversarial Networks (GANs) where physical laws or structural constraints were employed as a prior, but they require a large amount of labeled data or simulated full maps for training to produce efficient results.","In this paper, we present a method to reconstruct EMF exposure maps using only the generator network in GANs which does not require explicit training, thus overcoming the limitations of GANs, such as using reference full exposure maps.","This approach uses a prior from sensor data as Local Image Prior (LIP) captured by deep convolutional generative networks independent of learning the network parameters from images in an urban environment.","Experimental results show that, even when only sparse sensor data are available, our method can produce accurate estimates."],"url":"http://arxiv.org/abs/2405.03384v1","category":"cs.LG"}
{"created":"2024-05-06 11:33:12","title":"Reverse Forward Curriculum Learning for Extreme Sample and Demonstration Efficiency in Reinforcement Learning","abstract":"Reinforcement learning (RL) presents a promising framework to learn policies through environment interaction, but often requires an infeasible amount of interaction data to solve complex tasks from sparse rewards. One direction includes augmenting RL with offline data demonstrating desired tasks, but past work often require a lot of high-quality demonstration data that is difficult to obtain, especially for domains such as robotics. Our approach consists of a reverse curriculum followed by a forward curriculum. Unique to our approach compared to past work is the ability to efficiently leverage more than one demonstration via a per-demonstration reverse curriculum generated via state resets. The result of our reverse curriculum is an initial policy that performs well on a narrow initial state distribution and helps overcome difficult exploration problems. A forward curriculum is then used to accelerate the training of the initial policy to perform well on the full initial state distribution of the task and improve demonstration and sample efficiency. We show how the combination of a reverse curriculum and forward curriculum in our method, RFCL, enables significant improvements in demonstration and sample efficiency compared against various state-of-the-art learning-from-demonstration baselines, even solving previously unsolvable tasks that require high precision and control.","sentences":["Reinforcement learning (RL) presents a promising framework to learn policies through environment interaction, but often requires an infeasible amount of interaction data to solve complex tasks from sparse rewards.","One direction includes augmenting RL with offline data demonstrating desired tasks, but past work often require a lot of high-quality demonstration data that is difficult to obtain, especially for domains such as robotics.","Our approach consists of a reverse curriculum followed by a forward curriculum.","Unique to our approach compared to past work is the ability to efficiently leverage more than one demonstration via a per-demonstration reverse curriculum generated via state resets.","The result of our reverse curriculum is an initial policy that performs well on a narrow initial state distribution and helps overcome difficult exploration problems.","A forward curriculum is then used to accelerate the training of the initial policy to perform well on the full initial state distribution of the task and improve demonstration and sample efficiency.","We show how the combination of a reverse curriculum and forward curriculum in our method, RFCL, enables significant improvements in demonstration and sample efficiency compared against various state-of-the-art learning-from-demonstration baselines, even solving previously unsolvable tasks that require high precision and control."],"url":"http://arxiv.org/abs/2405.03379v1","category":"cs.LG"}
{"created":"2024-05-06 11:25:59","title":"Snake Learning: A Communication- and Computation-Efficient Distributed Learning Framework for 6G","abstract":"In the evolution towards 6G, integrating Artificial Intelligence (AI) with advanced network infrastructure emerges as a pivotal strategy for enhancing network intelligence and resource utilization. Existing distributed learning frameworks like Federated Learning and Split Learning often struggle with significant challenges in dynamic network environments including high synchronization demands, costly communication overheads, severe computing resource consumption, and data heterogeneity across network nodes. These obstacles hinder the applications of ubiquitous computing capabilities of 6G networks, especially in light of the trend of escalating model parameters and training data volumes. To address these challenges effectively, this paper introduces \"Snake Learning\", a cost-effective distributed learning framework. Specifically, Snake Learning respects the heterogeneity of inter-node computing capability and local data distribution in 6G networks, and sequentially trains the designated part of model layers on individual nodes. This layer-by-layer serpentine update mechanism contributes to significantly reducing the requirements for storage, memory and communication during the model training phase, and demonstrates superior adaptability and efficiency for both Computer Vision (CV) training and Large Language Model (LLM) fine-tuning tasks across homogeneous and heterogeneous data distributions.","sentences":["In the evolution towards 6G, integrating Artificial Intelligence (AI) with advanced network infrastructure emerges as a pivotal strategy for enhancing network intelligence and resource utilization.","Existing distributed learning frameworks like Federated Learning and Split Learning often struggle with significant challenges in dynamic network environments including high synchronization demands, costly communication overheads, severe computing resource consumption, and data heterogeneity across network nodes.","These obstacles hinder the applications of ubiquitous computing capabilities of 6G networks, especially in light of the trend of escalating model parameters and training data volumes.","To address these challenges effectively, this paper introduces \"Snake Learning\", a cost-effective distributed learning framework.","Specifically, Snake Learning respects the heterogeneity of inter-node computing capability and local data distribution in 6G networks, and sequentially trains the designated part of model layers on individual nodes.","This layer-by-layer serpentine update mechanism contributes to significantly reducing the requirements for storage, memory and communication during the model training phase, and demonstrates superior adaptability and efficiency for both Computer Vision (CV) training and Large Language Model (LLM) fine-tuning tasks across homogeneous and heterogeneous data distributions."],"url":"http://arxiv.org/abs/2405.03372v1","category":"cs.NI"}
{"created":"2024-05-06 11:12:19","title":"Embedded Distributed Inference of Deep Neural Networks: A Systematic Review","abstract":"Embedded distributed inference of Neural Networks has emerged as a promising approach for deploying machine-learning models on resource-constrained devices in an efficient and scalable manner. The inference task is distributed across a network of embedded devices, with each device contributing to the overall computation by performing a portion of the workload. In some cases, more powerful devices such as edge or cloud servers can be part of the system to be responsible of the most demanding layers of the network. As the demand for intelligent systems and the complexity of the deployed neural network models increases, this approach is becoming more relevant in a variety of applications such as robotics, autonomous vehicles, smart cities, Industry 4.0 and smart health. We present a systematic review of papers published during the last six years which describe techniques and methods to distribute Neural Networks across these kind of systems. We provide an overview of the current state-of-the-art by analysing more than 100 papers, present a new taxonomy to characterize them, and discuss trends and challenges in the field.","sentences":["Embedded distributed inference of Neural Networks has emerged as a promising approach for deploying machine-learning models on resource-constrained devices in an efficient and scalable manner.","The inference task is distributed across a network of embedded devices, with each device contributing to the overall computation by performing a portion of the workload.","In some cases, more powerful devices such as edge or cloud servers can be part of the system to be responsible of the most demanding layers of the network.","As the demand for intelligent systems and the complexity of the deployed neural network models increases, this approach is becoming more relevant in a variety of applications such as robotics, autonomous vehicles, smart cities, Industry 4.0 and smart health.","We present a systematic review of papers published during the last six years which describe techniques and methods to distribute Neural Networks across these kind of systems.","We provide an overview of the current state-of-the-art by analysing more than 100 papers, present a new taxonomy to characterize them, and discuss trends and challenges in the field."],"url":"http://arxiv.org/abs/2405.03360v1","category":"cs.DC"}
{"created":"2024-05-07 17:59:21","title":"Stochastic Gradient MCMC for Massive Geostatistical Data","abstract":"Gaussian processes (GPs) are commonly used for prediction and inference for spatial data analyses. However, since estimation and prediction tasks have cubic time and quadratic memory complexity in number of locations, GPs are difficult to scale to large spatial datasets. The Vecchia approximation induces sparsity in the dependence structure and is one of several methods proposed to scale GP inference. Our work adds to the substantial research in this area by developing a stochastic gradient Markov chain Monte Carlo (SGMCMC) framework for efficient computation in GPs. At each step, the algorithm subsamples a minibatch of locations and subsequently updates process parameters through a Vecchia-approximated GP likelihood. Since the Vecchia-approximated GP has a time complexity that is linear in the number of locations, this results in scalable estimation in GPs. Through simulation studies, we demonstrate that SGMCMC is competitive with state-of-the-art scalable GP algorithms in terms of computational time and parameter estimation. An application of our method is also provided using the Argo dataset of ocean temperature measurements.","sentences":["Gaussian processes (GPs) are commonly used for prediction and inference for spatial data analyses.","However, since estimation and prediction tasks have cubic time and quadratic memory complexity in number of locations, GPs are difficult to scale to large spatial datasets.","The Vecchia approximation induces sparsity in the dependence structure and is one of several methods proposed to scale GP inference.","Our work adds to the substantial research in this area by developing a stochastic gradient Markov chain Monte Carlo (SGMCMC) framework for efficient computation in GPs.","At each step, the algorithm subsamples a minibatch of locations and subsequently updates process parameters through a Vecchia-approximated GP likelihood.","Since the Vecchia-approximated GP has a time complexity that is linear in the number of locations, this results in scalable estimation in GPs.","Through simulation studies, we demonstrate that SGMCMC is competitive with state-of-the-art scalable GP algorithms in terms of computational time and parameter estimation.","An application of our method is also provided using the Argo dataset of ocean temperature measurements."],"url":"http://arxiv.org/abs/2405.04531v1","category":"stat.ME"}
{"created":"2024-05-07 17:57:32","title":"The six-planet resonant chain of HD 110067","abstract":"HD 110067 is the brightest star known to have six transiting planets. Each adjacent pair of planets has a period ratio that is nearly equal to a ratio of small integers, suggesting the planets are in a chain of mean-motion resonances, but the limited timespan of the available data has prevented firm conclusions. Here, we show that the requirement of long-term dynamical stability implies that all six planets are very likely to form a resonant chain. Dynamical simulations of non-resonant systems with initial conditions compatible with the available data almost always suffer an instability within $25$ Myr ($\\sim 0.3 \\%$ of the system's age). Assuming the system is in resonance, we place upper limits on the planets' eccentricities, and lower limits on the masses of the planets that have not yet been measured. We also predict the characteristics of transit timing variations and the values of the three-body libration centers.","sentences":["HD 110067 is the brightest star known to have six transiting planets.","Each adjacent pair of planets has a period ratio that is nearly equal to a ratio of small integers, suggesting the planets are in a chain of mean-motion resonances, but the limited timespan of the available data has prevented firm conclusions.","Here, we show that the requirement of long-term dynamical stability implies that all six planets are very likely to form a resonant chain.","Dynamical simulations of non-resonant systems with initial conditions compatible with the available data almost always suffer an instability within $25$ Myr ($\\sim 0.3 \\%$ of the system's age).","Assuming the system is in resonance, we place upper limits on the planets' eccentricities, and lower limits on the masses of the planets that have not yet been measured.","We also predict the characteristics of transit timing variations and the values of the three-body libration centers."],"url":"http://arxiv.org/abs/2405.04527v1","category":"astro-ph.EP"}
{"created":"2024-05-07 17:53:12","title":"SCExAO/CHARIS Multi-Wavelength, High-Contrast Imaging of the BD+45$^\\circ$598 Debris Disk","abstract":"We present a multi-wavelength (1.16$\\mu$m-2.37$\\mu$m) view of the debris disk around BD+45$^\\circ$598, using the Subaru Coronagraphic Extreme Adaptive Optics system paired with the Coronagraphic High Angular Resolution Imaging Spectrograph. With an assumed age of 23 Myr, this source allows us to study the early evolution of debris disks and search for forming planets. We fit a scattered light model to our disk using a differential evolution algorithm, and constrain its geometry. We find the disk to have a peak density radius of $R_0 = 109.6$ au, an inclination of $i = 88.1^\\circ$, and position angle $PA = 111.0^\\circ$. While we do not detect a substellar companion in the disk, our calculated contrast limits indicate sensitivity to planets as small as $\\sim 10 M_{\\rm Jup}$ at a projected separation of 12 au of the star, and as small as $\\sim 4 M_{\\rm Jup}$ beyond 38 au. When measuring intensity as a function of wavelength, the disk color constrains the minimum dust grain size within a range of $\\sim0.13$ to 1.01$\\mu$m.","sentences":["We present a multi-wavelength (1.16$\\mu$m-2.37$\\mu$m) view of the debris disk around BD+45$^\\circ$598, using the Subaru Coronagraphic Extreme Adaptive Optics system paired with the Coronagraphic High Angular Resolution Imaging Spectrograph.","With an assumed age of 23 Myr, this source allows us to study the early evolution of debris disks and search for forming planets.","We fit a scattered light model to our disk using a differential evolution algorithm, and constrain its geometry.","We find the disk to have a peak density radius of $R_0 = 109.6$ au, an inclination of $i = 88.1^\\circ$, and position angle $PA = 111.0^\\circ$.","While we do not detect a substellar companion in the disk, our calculated contrast limits indicate sensitivity to planets as small as $\\sim 10 M_{\\rm Jup}$ at a projected separation of 12 au of the star, and as small as $\\sim 4 M_{\\rm Jup}$ beyond 38 au.","When measuring intensity as a function of wavelength, the disk color constrains the minimum dust grain size within a range of $\\sim0.13$ to 1.01$\\mu$m."],"url":"http://arxiv.org/abs/2405.04521v1","category":"astro-ph.EP"}
{"created":"2024-05-07 17:45:53","title":"Scalable Circuit Cutting and Scheduling in a Resource-constrained and Distributed Quantum System","abstract":"Despite quantum computing's rapid development, current systems remain limited in practical applications due to their limited qubit count and quality. Various technologies, such as superconducting, trapped ions, and neutral atom quantum computing technologies are progressing towards a fault tolerant era, however they all face a diverse set of challenges in scalability and control. Recent efforts have focused on multi-node quantum systems that connect multiple smaller quantum devices to execute larger circuits. Future demonstrations hope to use quantum channels to couple systems, however current demonstrations can leverage classical communication with circuit cutting techniques. This involves cutting large circuits into smaller subcircuits and reconstructing them post-execution. However, existing cutting methods are hindered by lengthy search times as the number of qubits and gates increases. Additionally, they often fail to effectively utilize the resources of various worker configurations in a multi-node system. To address these challenges, we introduce FitCut, a novel approach that transforms quantum circuits into weighted graphs and utilizes a community-based, bottom-up approach to cut circuits according to resource constraints, e.g., qubit counts, on each worker. FitCut also includes a scheduling algorithm that optimizes resource utilization across workers. Implemented with Qiskit and evaluated extensively, FitCut significantly outperforms the Qiskit Circuit Knitting Toolbox, reducing time costs by factors ranging from 3 to 2000 and improving resource utilization rates by up to 3.88 times on the worker side, achieving a system-wide improvement of 2.86 times.","sentences":["Despite quantum computing's rapid development, current systems remain limited in practical applications due to their limited qubit count and quality.","Various technologies, such as superconducting, trapped ions, and neutral atom quantum computing technologies are progressing towards a fault tolerant era, however they all face a diverse set of challenges in scalability and control.","Recent efforts have focused on multi-node quantum systems that connect multiple smaller quantum devices to execute larger circuits.","Future demonstrations hope to use quantum channels to couple systems, however current demonstrations can leverage classical communication with circuit cutting techniques.","This involves cutting large circuits into smaller subcircuits and reconstructing them post-execution.","However, existing cutting methods are hindered by lengthy search times as the number of qubits and gates increases.","Additionally, they often fail to effectively utilize the resources of various worker configurations in a multi-node system.","To address these challenges, we introduce FitCut, a novel approach that transforms quantum circuits into weighted graphs and utilizes a community-based, bottom-up approach to cut circuits according to resource constraints, e.g., qubit counts, on each worker.","FitCut also includes a scheduling algorithm that optimizes resource utilization across workers.","Implemented with Qiskit and evaluated extensively, FitCut significantly outperforms the Qiskit Circuit Knitting Toolbox, reducing time costs by factors ranging from 3 to 2000 and improving resource utilization rates by up to 3.88 times on the worker side, achieving a system-wide improvement of 2.86 times."],"url":"http://arxiv.org/abs/2405.04514v1","category":"quant-ph"}
{"created":"2024-05-07 17:41:14","title":"Macroscopic flow out of a segment for Activated Random Walks in dimension 1","abstract":"Activated Random Walk is a system of interacting particles which presents a phase transition and a conjectured phenomenon of self-organized criticality. In this note, we prove that, in dimension 1, in the supercritical case, when a segment is stabilized with particles being killed when they jump out of the segment, a positive fraction of the particles exits the segment with positive probability.   This was already known to be a sufficient condition for being in the active phase of the model, and the result of this paper shows that this condition is also necessary, except maybe precisely at the critical point. This result can also be seen as a partial answer to some of the many conjectures which connect the different points of view on the phase transition of the model.","sentences":["Activated Random Walk is a system of interacting particles which presents a phase transition and a conjectured phenomenon of self-organized criticality.","In this note, we prove that, in dimension 1, in the supercritical case, when a segment is stabilized with particles being killed when they jump out of the segment, a positive fraction of the particles exits the segment with positive probability.   ","This was already known to be a sufficient condition for being in the active phase of the model, and the result of this paper shows that this condition is also necessary, except maybe precisely at the critical point.","This result can also be seen as a partial answer to some of the many conjectures which connect the different points of view on the phase transition of the model."],"url":"http://arxiv.org/abs/2405.04510v1","category":"math.PR"}
{"created":"2024-05-07 17:34:19","title":"A density-dependent metapopulation model: Extinction, persistence and source-sink dynamics","abstract":"We consider a nonlinear coupled discrete-time model of population dynamics. This model describes the movement of populations within a heterogeneous landscape, where the growth of subpopulations are modelled by (possibly different) bounded Kolmogorov maps and coupling terms are defined by nonlinear functions taking values in $(0,1)$. These couplings describe the proportions of individuals dispersing between regions. We first give a brief survey of similar discrete-time dispersal models. We then derive sufficient conditions for the stability/instability of the extinction equilibrium, for the existence of a positive fixed point and for ensuring uniform strong persistence. Finally we numerically explore a planar version of our model in a source-sink context, to show some of the qualitative behaviour that the model we consider can capture: for example, periodic behaviour and dynamics reminiscent of chaos.","sentences":["We consider a nonlinear coupled discrete-time model of population dynamics.","This model describes the movement of populations within a heterogeneous landscape, where the growth of subpopulations are modelled by (possibly different) bounded Kolmogorov maps and coupling terms are defined by nonlinear functions taking values in $(0,1)$. These couplings describe the proportions of individuals dispersing between regions.","We first give a brief survey of similar discrete-time dispersal models.","We then derive sufficient conditions for the stability/instability of the extinction equilibrium, for the existence of a positive fixed point and for ensuring uniform strong persistence.","Finally we numerically explore a planar version of our model in a source-sink context, to show some of the qualitative behaviour that the model we consider can capture: for example, periodic behaviour and dynamics reminiscent of chaos."],"url":"http://arxiv.org/abs/2405.04505v1","category":"math.DS"}
{"created":"2024-05-07 17:24:20","title":"Diatomic Molecules in deSitter and Anti-deSitter Spaces","abstract":"The Schr\\\"odinger equation for diatomic molecules in deSitter and anti-deSitter spaces is studied using the extended uncertainty principle formulation. The equations are solved by the Nikiforov-Uvarov method for both the Kratzer potential and the pseudoharmonic oscillator. The energy eigenvalues of the system have been derived analytically, and the exact expressions of the eigenfunctions are provided in terms of Romanovski and Jacobi polynomials. The impact of the spatial deformation parameter on the bound states is also examined, with experimental results used to establish an upper limit for this parameter.","sentences":["The Schr\\\"odinger equation for diatomic molecules in deSitter and anti-deSitter spaces is studied using the extended uncertainty principle formulation.","The equations are solved by the Nikiforov-Uvarov method for both the Kratzer potential and the pseudoharmonic oscillator.","The energy eigenvalues of the system have been derived analytically, and the exact expressions of the eigenfunctions are provided in terms of Romanovski and Jacobi polynomials.","The impact of the spatial deformation parameter on the bound states is also examined, with experimental results used to establish an upper limit for this parameter."],"url":"http://arxiv.org/abs/2405.04502v1","category":"math-ph"}
{"created":"2024-05-07 17:18:05","title":"The Chemical Composition of Ryugu: Prospects as a Reference Material for Solar System Composition","abstract":"The Hayabusa 2 spacecraft sampled approximately 5.4 g of asteroid material from the Cb-type asteroid Ryugu. Initial analysis of the Ryugu materials revealed a mineralogical, chemical, and isotopic kinship to the CI chondrites. The pristine nature of Ryugu makes the returned samples ideal for constraining the composition of the Solar System. However, some elements (e.g., P, Ca, Mn, and rare earth elements) show large relative dispersions compared to the other elements in the returned materials studied so far, most likely due to the presence of aqueously formed secondary minerals (e.g., carbonates, phosphates) in Ryugu. Therefore, the estimation of the Solar System composition using currently available Ryugu data is challenging due to the so-called nugget effect of carbonates, phosphates, and possibly other accessory minerals. The nugget effect can be mitigated by analyzing a homogenized, relatively large amount of sample. We estimate that for approximately 0.1 g of Ryugu sample, the dispersion (2SD) of the bulk Mn/Cr and Rb/Sr ratios are +/-13% and +/-15%, respectively, while they will be improved to be better than +/-5% for approximately 1 g of homogenized Ryugu sample. To further constrain the Solar System composition and to evaluate if previous estimates based on CI chondrites stored in museums for decades to centuries are reliable, it is strongly recommended to determine the chemical and isotopic compositions of Ryugu using a homogenized sample prepared from relatively large (approx. 1 g) returned material. Determining Ryugu reference compositions will be used by multidisciplinary communities, including Earth and planetary sciences, astronomy, physics, and chemistry.","sentences":["The Hayabusa 2 spacecraft sampled approximately 5.4 g of asteroid material from the Cb-type asteroid Ryugu.","Initial analysis of the Ryugu materials revealed a mineralogical, chemical, and isotopic kinship to the CI chondrites.","The pristine nature of Ryugu makes the returned samples ideal for constraining the composition of the Solar System.","However, some elements (e.g., P, Ca, Mn, and rare earth elements) show large relative dispersions compared to the other elements in the returned materials studied so far, most likely due to the presence of aqueously formed secondary minerals (e.g., carbonates, phosphates) in Ryugu.","Therefore, the estimation of the Solar System composition using currently available Ryugu data is challenging due to the so-called nugget effect of carbonates, phosphates, and possibly other accessory minerals.","The nugget effect can be mitigated by analyzing a homogenized, relatively large amount of sample.","We estimate that for approximately 0.1 g of Ryugu sample, the dispersion (2SD) of the bulk Mn/Cr and Rb/Sr ratios are +/-13% and +/-15%, respectively, while they will be improved to be better than +/-5% for approximately 1 g of homogenized Ryugu sample.","To further constrain the Solar System composition and to evaluate if previous estimates based on CI chondrites stored in museums for decades to centuries are reliable, it is strongly recommended to determine the chemical and isotopic compositions of Ryugu using a homogenized sample prepared from relatively large (approx.","1 g) returned material.","Determining Ryugu reference compositions will be used by multidisciplinary communities, including Earth and planetary sciences, astronomy, physics, and chemistry."],"url":"http://arxiv.org/abs/2405.04500v1","category":"astro-ph.EP"}
{"created":"2024-05-07 17:15:58","title":"Benchmarking Optimizers for Qumode State Preparation with Variational Quantum Algorithms","abstract":"Quantum state preparation involves preparing a target state from an initial system, a process integral to applications such as quantum machine learning and solving systems of linear equations. Recently, there has been a growing interest in qumodes due to advancements in the field and their potential applications. However there is a notable gap in the literature specifically addressing this area. This paper aims to bridge this gap by providing performance benchmarks of various optimizers used in state preparation with Variational Quantum Algorithms. We conducted extensive testing across multiple scenarios, including different target states, both ideal and sampling simulations, and varying numbers of basis gate layers. Our evaluations offer insights into the complexity of learning each type of target state and demonstrate that some optimizers perform better than others in this context. Notably, the Powell optimizer was found to be exceptionally robust against sampling errors, making it a preferred choice in scenarios prone to such inaccuracies. Additionally, the Simultaneous Perturbation Stochastic Approximation optimizer was distinguished for its efficiency and ability to handle increased parameter dimensionality effectively.","sentences":["Quantum state preparation involves preparing a target state from an initial system, a process integral to applications such as quantum machine learning and solving systems of linear equations.","Recently, there has been a growing interest in qumodes due to advancements in the field and their potential applications.","However there is a notable gap in the literature specifically addressing this area.","This paper aims to bridge this gap by providing performance benchmarks of various optimizers used in state preparation with Variational Quantum Algorithms.","We conducted extensive testing across multiple scenarios, including different target states, both ideal and sampling simulations, and varying numbers of basis gate layers.","Our evaluations offer insights into the complexity of learning each type of target state and demonstrate that some optimizers perform better than others in this context.","Notably, the Powell optimizer was found to be exceptionally robust against sampling errors, making it a preferred choice in scenarios prone to such inaccuracies.","Additionally, the Simultaneous Perturbation Stochastic Approximation optimizer was distinguished for its efficiency and ability to handle increased parameter dimensionality effectively."],"url":"http://arxiv.org/abs/2405.04499v1","category":"quant-ph"}
{"created":"2024-05-07 17:02:44","title":"Geometric Structures for the $G_2'$-Hitchin Component","abstract":"We give an explicit geometric structures interpretation of the $G_2'$-Hitchin component $Hit(S, G_2') \\subset \\chi(\\pi_1S,G_2')$ of a closed oriented surface $S$ of genus $g \\geq 2$. In particular, we prove $Hit(S, G_2')$ is naturally homeomorphic to a moduli space $\\mathscr{M}$ of $(G,X)$-structures for $G = G_2'$ and $X = Ein^{2,3}$ on a fiber bundle $\\mathscr{C}$ over $S$ via the descended holonomy map. Explicitly, $\\mathscr{C}$ is the direct sum of fiber bundles $\\mathscr{C} = UTS \\oplus UTS \\oplus \\underline{\\mathbb{R}_+}$ with fiber $\\mathscr{C}_p = UT_p S \\times UT_p S \\times \\mathbb{R}_+$, where $UT S$ denotes the unit tangent bundle.   The geometric structure associated to a $G_2'$-Hitchin representation $\\rho$ is explicitly constructed from the unique associated $\\rho$-equivariant alternating almost-complex curve $\\hat{\\nu}: \\tilde{S} \\rightarrow \\hat{\\mathbb{S}}^{2,4}$; we critically use recent work of Collier-Toulisse on the moduli space of such curves. Our explicit geometric structures are examined in the $G_2'$-Fuchsian case and shown to be unrelated to the $(G_2', Ein^{2,3})$-structures of Guichard-Wienhard.","sentences":["We give an explicit geometric structures interpretation of the $G_2'$-Hitchin component $Hit(S, G_2')","\\subset \\chi(\\pi_1S,G_2')$ of a closed oriented surface $S$ of genus $g \\geq 2$.","In particular, we prove $Hit(S, G_2')$ is naturally homeomorphic to a moduli space $\\mathscr{M}$ of $(G,X)$-structures for $G = G_2'$ and $X = Ein^{2,3}$ on a fiber bundle $\\mathscr{C}$ over $S$ via the descended holonomy map.","Explicitly, $\\mathscr{C}$ is the direct sum of fiber bundles $\\mathscr{C} = UTS \\oplus UTS \\oplus \\underline{\\mathbb{R}_+}$ with fiber $\\mathscr{C}_p = UT_p S \\times UT_p S \\times \\mathbb{R}_+$, where $UT S$ denotes the unit tangent bundle.   ","The geometric structure associated to a $G_2'$-Hitchin representation $\\rho$ is explicitly constructed from the unique associated $\\rho$-equivariant alternating almost-complex curve $\\hat{\\nu}: \\tilde{S} \\rightarrow \\hat{\\mathbb{S}}^{2,4}$; we critically use recent work of Collier-Toulisse on the moduli space of such curves.","Our explicit geometric structures are examined in the $G_2'$-Fuchsian case and shown to be unrelated to the $(G_2', Ein^{2,3})$-structures of Guichard-Wienhard."],"url":"http://arxiv.org/abs/2405.04492v1","category":"math.DG"}
{"created":"2024-05-07 17:00:19","title":"Resource-Efficient and Self-Adaptive Quantum Search in a Quantum-Classical Hybrid System","abstract":"Over the past decade, the rapid advancement of deep learning and big data applications has been driven by vast datasets and high-performance computing systems. However, as we approach the physical limits of semiconductor fabrication in the post-Moore's Law era, questions arise about the future of these applications. In parallel, quantum computing has made significant progress with the potential to break limits. Major companies like IBM, Google, and Microsoft provide access to noisy intermediate-scale quantum (NISQ) computers. Despite the theoretical promise of Shor's and Grover's algorithms, practical implementation on current quantum devices faces challenges, such as demanding additional resources and a high number of controlled operations. To tackle these challenges and optimize the utilization of limited onboard qubits, we introduce ReSaQuS, a resource-efficient index-value searching system within a quantum-classical hybrid framework. Building on Grover's algorithm, ReSaQuS employs an automatically managed iterative search approach. This method analyzes problem size, filters fewer probable data points, and progressively reduces the dataset with decreasing qubit requirements. Implemented using Qiskit and evaluated through extensive experiments, ReSaQuS has demonstrated a substantial reduction, up to 86.36\\% in cumulative qubit consumption and 72.72\\% in active periods, reinforcing its potential in optimizing quantum computing application deployment.","sentences":["Over the past decade, the rapid advancement of deep learning and big data applications has been driven by vast datasets and high-performance computing systems.","However, as we approach the physical limits of semiconductor fabrication in the post-Moore's Law era, questions arise about the future of these applications.","In parallel, quantum computing has made significant progress with the potential to break limits.","Major companies like IBM, Google, and Microsoft provide access to noisy intermediate-scale quantum (NISQ) computers.","Despite the theoretical promise of Shor's and Grover's algorithms, practical implementation on current quantum devices faces challenges, such as demanding additional resources and a high number of controlled operations.","To tackle these challenges and optimize the utilization of limited onboard qubits, we introduce ReSaQuS, a resource-efficient index-value searching system within a quantum-classical hybrid framework.","Building on Grover's algorithm, ReSaQuS employs an automatically managed iterative search approach.","This method analyzes problem size, filters fewer probable data points, and progressively reduces the dataset with decreasing qubit requirements.","Implemented using Qiskit and evaluated through extensive experiments, ReSaQuS has demonstrated a substantial reduction, up to 86.36\\% in cumulative qubit consumption and 72.72\\% in active periods, reinforcing its potential in optimizing quantum computing application deployment."],"url":"http://arxiv.org/abs/2405.04490v1","category":"cs.DC"}
{"created":"2024-05-07 16:56:21","title":"S3Former: Self-supervised High-resolution Transformer for Solar PV Profiling","abstract":"As the impact of climate change escalates, the global necessity to transition to sustainable energy sources becomes increasingly evident. Renewable energies have emerged as a viable solution for users, with Photovoltaic energy being a favored choice for small installations due to its reliability and efficiency. Accurate mapping of PV installations is crucial for understanding the extension of its adoption and informing energy policy. To meet this need, we introduce S3Former, designed to segment solar panels from aerial imagery and provide size and location information critical for analyzing the impact of such installations on the grid. Solar panel identification is challenging due to factors such as varying weather conditions, roof characteristics, Ground Sampling Distance variations and lack of appropriate initialization weights for optimized training. To tackle these complexities, S3Former features a Masked Attention Mask Transformer incorporating a self-supervised learning pretrained backbone. Specifically, our model leverages low-level and high-level features extracted from the backbone and incorporates an instance query mechanism incorporated on the Transformer architecture to enhance the localization of solar PV installations. We introduce a self-supervised learning phase (pretext task) to improve the initialization weights on the backbone of S3Former. We evaluated S3Former using diverse datasets, demonstrate improvement state-of-the-art models.","sentences":["As the impact of climate change escalates, the global necessity to transition to sustainable energy sources becomes increasingly evident.","Renewable energies have emerged as a viable solution for users, with Photovoltaic energy being a favored choice for small installations due to its reliability and efficiency.","Accurate mapping of PV installations is crucial for understanding the extension of its adoption and informing energy policy.","To meet this need, we introduce S3Former, designed to segment solar panels from aerial imagery and provide size and location information critical for analyzing the impact of such installations on the grid.","Solar panel identification is challenging due to factors such as varying weather conditions, roof characteristics, Ground Sampling Distance variations and lack of appropriate initialization weights for optimized training.","To tackle these complexities, S3Former features a Masked Attention Mask Transformer incorporating a self-supervised learning pretrained backbone.","Specifically, our model leverages low-level and high-level features extracted from the backbone and incorporates an instance query mechanism incorporated on the Transformer architecture to enhance the localization of solar PV installations.","We introduce a self-supervised learning phase (pretext task) to improve the initialization weights on the backbone of S3Former.","We evaluated S3Former using diverse datasets, demonstrate improvement state-of-the-art models."],"url":"http://arxiv.org/abs/2405.04489v1","category":"cs.CV"}
{"created":"2024-05-07 16:45:01","title":"Tunable superconductivity in electron- and hole-doped Bernal bilayer graphene","abstract":"Graphene-based, high quality two-dimensional electronic systems have emerged as a highly tunable platform for studying superconductivity. Specifically, superconductivity has been observed in both electron-doped and hole-doped twisted graphene moire systems, whereas in crystalline graphene systems, superconductivity has so far only been observed in hole-doped rhombohedral trilayer and hole-doped Bernal bilayer graphene (BBG). Recently, enhanced superconductivity has been demonstrated in BBG due to the proximity with a monolayer WSe2. Here, we report the observation of superconductivity and a series of flavor-symmetry-breaking phases in both electron- and hole-doped BBG/WSe2 device by electrostatic doping. The strength of the observed superconductivity is tunable by applied vertical electric fields. The maximum Berezinskii-Kosterlitz-Thouless (BKT) transition temperature for the electron- and hole-doped superconductivity is about 210 mK and 400 mK, respectively. Superconductivities emerge only when applied electric fields drive BBG electron or hole wavefunctions toward the WSe2 layer, underscoring the importance of the WSe2 layer in the observed superconductivity. We find the hole-doped superconductivity violates the Pauli paramagnetic limit, consistent with an Ising-like superconductor. In contrast, the electron-doped superconductivity obeys the Pauli limit, even though the proximity induced Ising spin-orbit coupling is also notable in the conduction band. Our findings highlight the rich physics associated with the conduction band in BBG, paving the way for further studies into the superconducting mechanisms of crystalline graphene and the development of novel superconductor devices based on BBG.","sentences":["Graphene-based, high quality two-dimensional electronic systems have emerged as a highly tunable platform for studying superconductivity.","Specifically, superconductivity has been observed in both electron-doped and hole-doped twisted graphene moire systems, whereas in crystalline graphene systems, superconductivity has so far only been observed in hole-doped rhombohedral trilayer and hole-doped Bernal bilayer graphene (BBG).","Recently, enhanced superconductivity has been demonstrated in BBG due to the proximity with a monolayer WSe2.","Here, we report the observation of superconductivity and a series of flavor-symmetry-breaking phases in both electron- and hole-doped BBG/WSe2 device by electrostatic doping.","The strength of the observed superconductivity is tunable by applied vertical electric fields.","The maximum Berezinskii-Kosterlitz-Thouless (BKT) transition temperature for the electron- and hole-doped superconductivity is about 210 mK and 400 mK, respectively.","Superconductivities emerge only when applied electric fields drive BBG electron or hole wavefunctions toward the WSe2 layer, underscoring the importance of the WSe2 layer in the observed superconductivity.","We find the hole-doped superconductivity violates the Pauli paramagnetic limit, consistent with an Ising-like superconductor.","In contrast, the electron-doped superconductivity obeys the Pauli limit, even though the proximity induced Ising spin-orbit coupling is also notable in the conduction band.","Our findings highlight the rich physics associated with the conduction band in BBG, paving the way for further studies into the superconducting mechanisms of crystalline graphene and the development of novel superconductor devices based on BBG."],"url":"http://arxiv.org/abs/2405.04479v1","category":"cond-mat.supr-con"}
{"created":"2024-05-07 16:44:24","title":"Exploration of Novel Neuromorphic Methodologies for Materials Applications","abstract":"Many of today's most interesting questions involve understanding and interpreting complex relationships within graph-based structures. For instance, in materials science, predicting material properties often relies on analyzing the intricate network of atomic interactions. Graph neural networks (GNNs) have emerged as a popular approach for these tasks; however, they suffer from limitations such as inefficient hardware utilization and over-smoothing. Recent advancements in neuromorphic computing offer promising solutions to these challenges. In this work, we evaluate two such neuromorphic strategies known as reservoir computing and hyperdimensional computing. We compare the performance of both approaches for bandgap classification and regression using a subset of the Materials Project dataset. Our results indicate recent advances in hyperdimensional computing can be applied effectively to better represent molecular graphs.","sentences":["Many of today's most interesting questions involve understanding and interpreting complex relationships within graph-based structures.","For instance, in materials science, predicting material properties often relies on analyzing the intricate network of atomic interactions.","Graph neural networks (GNNs) have emerged as a popular approach for these tasks; however, they suffer from limitations such as inefficient hardware utilization and over-smoothing.","Recent advancements in neuromorphic computing offer promising solutions to these challenges.","In this work, we evaluate two such neuromorphic strategies known as reservoir computing and hyperdimensional computing.","We compare the performance of both approaches for bandgap classification and regression using a subset of the Materials Project dataset.","Our results indicate recent advances in hyperdimensional computing can be applied effectively to better represent molecular graphs."],"url":"http://arxiv.org/abs/2405.04478v1","category":"cs.ET"}
{"created":"2024-05-07 16:38:02","title":"Nonlinear Landau damping and wave operators in sharp Gevrey spaces","abstract":"We prove nonlinear Landau damping in optimal weighted Gevrey-3 spaces for solutions of the confined Vlasov-Poisson system on $\\T^d\\times\\R^d$ which are small perturbations of homogeneous Penrose-stable equilibria.   We also prove the existence of nonlinear scattering operators associated to the confined Vlasov-Poisson evolution, as well as suitable injectivity properties and Lipschitz estimates (also in weighted Gevrey-3 spaces) on these operators.   Our results give definitive answers to two well-known open problems in the field, both of them stated in the recent review of Bedrossian [4, Section 6].","sentences":["We prove nonlinear Landau damping in optimal weighted Gevrey-3 spaces for solutions of the confined Vlasov-Poisson system on $\\T^d\\times\\R^d$ which are small perturbations of homogeneous Penrose-stable equilibria.   ","We also prove the existence of nonlinear scattering operators associated to the confined Vlasov-Poisson evolution, as well as suitable injectivity properties and Lipschitz estimates (also in weighted Gevrey-3 spaces) on these operators.   ","Our results give definitive answers to two well-known open problems in the field, both of them stated in the recent review of Bedrossian [4, Section 6]."],"url":"http://arxiv.org/abs/2405.04473v1","category":"math.AP"}
{"created":"2024-05-07 16:36:28","title":"Neural Network Quantum States for the Interacting Hofstadter Model with Higher Local Occupations and Long-Range Interactions","abstract":"Due to their immense representative power, neural network quantum states (NQS) have gained significant interest in current research. In recent advances in the field of NQS, it has been demonstrated that this approach can compete with state-of-the-art numerical techniques, making NQS a compelling alternative, in particular for the simulation of large, two-dimensional quantum systems. In this study, we show that recurrent neural network (RNN) wave functions can be employed to study systems relevant to current research in quantum many-body physics. Specifically, we employ a 2D tensorized gated RNN to explore the bosonic Hofstadter model with a variable local Hilbert space cut-off and long-range interactions. At first, we benchmark the RNN-NQS for the Hofstadter-Bose-Hubbard (HBH) Hamiltonian on a square lattice. We find that this method is, despite the complexity of the wave function, capable of efficiently identifying and representing most ground state properties. Afterwards, we apply the method to an even more challenging model for current methods, namely the Hofstadter model with long-range interactions. This model describes Rydberg-dressed atoms on a lattice subject to a synthetic magnetic field. We study systems of size up to $12 \\times 12$ sites and identify three different regimes by tuning the interaction range and the filling fraction $\\nu$. In addition to phases known from the HBH model at short-ranged interaction, we observe bubble crystals and Wigner crystals for long-ranged interactions. Especially interesting is the evidence of a bubble crystal phase on a lattice, as this gives experiments a starting point for the search of clustered liquid phases, possibly hosting non-Abelian anyon excitations. In our work we show that NQS are an efficient and reliable simulation method for quantum systems, which are the subject of current research.","sentences":["Due to their immense representative power, neural network quantum states (NQS) have gained significant interest in current research.","In recent advances in the field of NQS, it has been demonstrated that this approach can compete with state-of-the-art numerical techniques, making NQS a compelling alternative, in particular for the simulation of large, two-dimensional quantum systems.","In this study, we show that recurrent neural network (RNN) wave functions can be employed to study systems relevant to current research in quantum many-body physics.","Specifically, we employ a 2D tensorized gated RNN to explore the bosonic Hofstadter model with a variable local Hilbert space cut-off and long-range interactions.","At first, we benchmark the RNN-NQS for the Hofstadter-Bose-Hubbard (HBH) Hamiltonian on a square lattice.","We find that this method is, despite the complexity of the wave function, capable of efficiently identifying and representing most ground state properties.","Afterwards, we apply the method to an even more challenging model for current methods, namely the Hofstadter model with long-range interactions.","This model describes Rydberg-dressed atoms on a lattice subject to a synthetic magnetic field.","We study systems of size up to $12 \\times 12$ sites and identify three different regimes by tuning the interaction range and the filling fraction $\\nu$. In addition to phases known from the HBH model at short-ranged interaction, we observe bubble crystals and Wigner crystals for long-ranged interactions.","Especially interesting is the evidence of a bubble crystal phase on a lattice, as this gives experiments a starting point for the search of clustered liquid phases, possibly hosting non-Abelian anyon excitations.","In our work we show that NQS are an efficient and reliable simulation method for quantum systems, which are the subject of current research."],"url":"http://arxiv.org/abs/2405.04472v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-07 16:29:11","title":"Large-Scale MPC: Scaling Private Iris Code Uniqueness Checks to Millions of Users","abstract":"In this work we tackle privacy concerns in biometric verification systems that typically require server-side processing of sensitive data (e.g., fingerprints and Iris Codes). Concretely, we design a solution that allows us to query whether a given Iris Code is similar to one contained in a given database, while all queries and datasets are being protected using secure multiparty computation (MPC). Addressing the substantial performance demands of operational systems like World ID and aid distributions by the Red Cross, we propose new protocols to improve performance by more than three orders of magnitude compared to the recent state-of-the-art system Janus (S&P 24). Our final protocol can achieve a throughput of over a million Iris Code comparisons per second on a single CPU core, while protecting the privacy of both the query and database Iris Codes. We additionally investigate GPU acceleration for some building blocks of our protocol, which results in further speedups of over 38x compared to the respective multi-threaded CPU implementation.","sentences":["In this work we tackle privacy concerns in biometric verification systems that typically require server-side processing of sensitive data (e.g., fingerprints and Iris Codes).","Concretely, we design a solution that allows us to query whether a given Iris Code is similar to one contained in a given database, while all queries and datasets are being protected using secure multiparty computation (MPC).","Addressing the substantial performance demands of operational systems like World ID and aid distributions by the Red Cross, we propose new protocols to improve performance by more than three orders of magnitude compared to the recent state-of-the-art system Janus (S&P 24).","Our final protocol can achieve a throughput of over a million Iris Code comparisons per second on a single CPU core, while protecting the privacy of both the query and database Iris Codes.","We additionally investigate GPU acceleration for some building blocks of our protocol, which results in further speedups of over 38x compared to the respective multi-threaded CPU implementation."],"url":"http://arxiv.org/abs/2405.04463v1","category":"cs.CR"}
{"created":"2024-05-07 16:24:46","title":"Transition of dimuonium through foil","abstract":"This article presents a study of the passage of dimuonium through the foil of ordinary matter. First, we provide an overview of how dimuonium is planned to be produced for such a type of experiment and how it is expected to interact with the ordinary atoms -- predominantly electromagnetically via the screened coulomb potential of the atomic nuclei. Then, we describe the transport equations that represent the evolution of dimuonium states during the passage and their solution methods. Finally, for three different foils (Beryllium, Aluminium and Lead), we present the results of this study. To estimate impact of uncertainties in the potential of a target atom, we study 15 different approximations of the atomic potential and show that the corresponding atomic-potential-model-dependent error in the yields of the low lying states of dimuonium is quite small within the framework of the applied Born approximation. The convergence of the results after truncation of the infinite system of transport equations to the finite number of quantum states of dimuonium is also studied, and good convergence for the yields of low-lying states is demonstrated.","sentences":["This article presents a study of the passage of dimuonium through the foil of ordinary matter.","First, we provide an overview of how dimuonium is planned to be produced for such a type of experiment and how it is expected to interact with the ordinary atoms -- predominantly electromagnetically via the screened coulomb potential of the atomic nuclei.","Then, we describe the transport equations that represent the evolution of dimuonium states during the passage and their solution methods.","Finally, for three different foils (Beryllium, Aluminium and Lead), we present the results of this study.","To estimate impact of uncertainties in the potential of a target atom, we study 15 different approximations of the atomic potential and show that the corresponding atomic-potential-model-dependent error in the yields of the low lying states of dimuonium is quite small within the framework of the applied Born approximation.","The convergence of the results after truncation of the infinite system of transport equations to the finite number of quantum states of dimuonium is also studied, and good convergence for the yields of low-lying states is demonstrated."],"url":"http://arxiv.org/abs/2405.04460v1","category":"hep-ph"}
{"created":"2024-05-07 16:15:51","title":"Piecewise continuous maps on the interval","abstract":"Let $f$ be a piecewise continuous map on the interval with at most a finite number of turning points. In this paper we study some basic properties about this class of functions and show its main difference from the continuous case. We define and study the notion of closed structure, which can be seen as an extension of the notion of periodic orbit. Moreover, we also study the periodic orbits which are away from the discontinuities of $f$, extending the notion of trapped and free periodic orbits.","sentences":["Let $f$ be a piecewise continuous map on the interval with at most a finite number of turning points.","In this paper we study some basic properties about this class of functions and show its main difference from the continuous case.","We define and study the notion of closed structure, which can be seen as an extension of the notion of periodic orbit.","Moreover, we also study the periodic orbits which are away from the discontinuities of $f$, extending the notion of trapped and free periodic orbits."],"url":"http://arxiv.org/abs/2405.04452v1","category":"math.DS"}
{"created":"2024-05-07 16:15:38","title":"Analyticity for classical hard-core gases via recursion","abstract":"In the recent work of [Michelen, Perkins, Comm. Math. Phys. 399:1 (2023)], a new lower bound of $eC_{\\phi}(\\beta)^{-1}$ is obtained for the positive activity up to which the pressure of a classical system of particles with repulsive pair interactions is analytic. In this paper, we extend their method to the class of radially symmetric, locally stable, and tempered pair potentials. Our main result is that the pressure of such systems is analytic for positive activities up to $e^{2(1-W(eA_{\\phi}(\\beta)/C_{\\phi}(\\beta)))}C_{\\phi}(\\beta)^{-1}e^{-(\\beta C+1)}$, where $C>0$ is the local stability constant, $W(\\cdot)$ the Lambert $W$-function, and $A_{\\phi}(\\beta)$ the contribution from the attractive part of the pair potential to the temperedness constant. In the high-temperature limit, our result improves the classical Penrose-Ruelle bound of $C_{\\phi}(\\beta)^{-1}e^{-(\\beta C+1)}$ by a factor of $e^{2}$. This proves the absence of phase transitions in these systems in the Lee-Yang sense for activities up to the above threshold.","sentences":["In the recent work of [Michelen, Perkins, Comm.","Math.","Phys.","399:1 (2023)], a new lower bound of $eC_{\\phi}(\\beta)^{-1}$ is obtained for the positive activity up to which the pressure of a classical system of particles with repulsive pair interactions is analytic.","In this paper, we extend their method to the class of radially symmetric, locally stable, and tempered pair potentials.","Our main result is that the pressure of such systems is analytic for positive activities up to $e^{2(1-W(eA_{\\phi}(\\beta)/C_{\\phi}(\\beta)))}C_{\\phi}(\\beta)^{-1}e^{-(\\beta C+1)}$, where $C>0$ is the local stability constant, $W(\\cdot)$ the Lambert $W$-function, and $A_{\\phi}(\\beta)$ the contribution from the attractive part of the pair potential to the temperedness constant.","In the high-temperature limit, our result improves the classical Penrose-Ruelle bound of $C_{\\phi}(\\beta)^{-1}e^{-(\\beta C+1)}$ by a factor of $e^{2}$.","This proves the absence of phase transitions in these systems in the Lee-Yang sense for activities up to the above threshold."],"url":"http://arxiv.org/abs/2405.04451v1","category":"math-ph"}
{"created":"2024-05-07 16:14:28","title":"Derivation of kinetic and diffusion equations from a hard-sphere Rayleigh gas using collision trees and semigroups","abstract":"We will revisit the classical questions of understanding the statistics of various deterministic dynamics of $N$ hard spheres of diameter $\\varepsilon$ with random initial data in the Boltzmann-Grad scaling as $\\varepsilon$ tends to zero and $N$ tends to infinity. The convergence of the empiric particle dynamics to the Boltzmann-type dynamics is shown using semigroup methods to describe probability measures on collision trees associated to physical trajectories in the case of a Rayleigh gas. As an application we derive the diffusion equation by a further rescaling.","sentences":["We will revisit the classical questions of understanding the statistics of various deterministic dynamics of $N$ hard spheres of diameter $\\varepsilon$ with random initial data in the Boltzmann-Grad scaling as $\\varepsilon$ tends to zero and $N$ tends to infinity.","The convergence of the empiric particle dynamics to the Boltzmann-type dynamics is shown using semigroup methods to describe probability measures on collision trees associated to physical trajectories in the case of a Rayleigh gas.","As an application we derive the diffusion equation by a further rescaling."],"url":"http://arxiv.org/abs/2405.04449v1","category":"math.AP"}
{"created":"2024-05-07 16:07:44","title":"Immortal solutions of the K\u00e4hler-Ricci flow","abstract":"We survey some recent developments on solutions of the K\\\"ahler-Ricci flow on compact K\\\"ahler manifolds which exist for all positive times.","sentences":["We survey some recent developments on solutions of the K\\\"ahler-Ricci flow on compact K\\\"ahler manifolds which exist for all positive times."],"url":"http://arxiv.org/abs/2405.04444v1","category":"math.DG"}
{"created":"2024-05-07 15:57:45","title":"Multiple crossing during dynamical symmetry restoration and implications for the quantum Mpemba effect","abstract":"Local relaxation after a quench in 1-D quantum many-body systems is a well known and very active problem with rich phenomenology. Except for pathological cases, the local relaxation is accompanied by the local restoration of the symmetries broken by the initial state that are preserved by the unitary evolution. Recently, the entanglement asymmetry has been introduced as a probe to study the interplay between symmetry breaking and relaxation in an extended quantum system. In particular, using the asymmetry, it has been shown that the more a symmetry is initially broken, the faster it may be restored. This surprising effect, which has been also observed in trapped-ion experiments, can be seen as a quantum version of the Mpemba effect and is manifested by the crossing at a finite time of the entanglement asymmetry curves of two different initial symmetry breaking configurations. In this paper we show, how, by tuning the initial state, the symmetry dynamics in free fermionic systems can display much richer behaviour than seen previously. In particular, for certain classes of initial states, including ground states of free fermionic models with long-range couplings, the entanglement asymmetry can exhibit multiple crossings. This illustrates that the existence of the quantum Mpemba effect can only be inferred by examining the late time behaviour of the entanglement asymmetry.","sentences":["Local relaxation after a quench in 1-D quantum many-body systems is a well known and very active problem with rich phenomenology.","Except for pathological cases, the local relaxation is accompanied by the local restoration of the symmetries broken by the initial state that are preserved by the unitary evolution.","Recently, the entanglement asymmetry has been introduced as a probe to study the interplay between symmetry breaking and relaxation in an extended quantum system.","In particular, using the asymmetry, it has been shown that the more a symmetry is initially broken, the faster it may be restored.","This surprising effect, which has been also observed in trapped-ion experiments, can be seen as a quantum version of the Mpemba effect and is manifested by the crossing at a finite time of the entanglement asymmetry curves of two different initial symmetry breaking configurations.","In this paper we show, how, by tuning the initial state, the symmetry dynamics in free fermionic systems can display much richer behaviour than seen previously.","In particular, for certain classes of initial states, including ground states of free fermionic models with long-range couplings, the entanglement asymmetry can exhibit multiple crossings.","This illustrates that the existence of the quantum Mpemba effect can only be inferred by examining the late time behaviour of the entanglement asymmetry."],"url":"http://arxiv.org/abs/2405.04436v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-07 15:51:13","title":"Optimizing Information Freshness in IoT Systems with Update Rate Constraints: A Token-Based Approach","abstract":"In Internet of Things (IoT) status update systems, where information is sampled and subsequently transmitted from a source to a destination node, the imperative necessity lies in maintaining the timeliness of information and updating the system with optimal frequency. Optimizing information freshness in resource-limited status update systems often involves Constrained Markov Decision Process (CMDP) problems with update rate constraints. Solving CMDP problems, especially with multiple constraints, is a challenging task. To address this, we present a token-based approach that transforms CMDP into an unconstrained MDP, simplifying the solution process. We apply this approach to systems with one and two update rate constraints for optimizing Age of Incorrect Information (AoII) and Age of Information (AoI) metrics, respectively, and explore the analytical and numerical aspects. Additionally, we introduce an iterative triangle bisection method for solving the CMDP problems with two constraints, comparing its results with the token-based MDP approach. Our findings show that the token-based approach yields superior performance over baseline policies, converging to the optimal policy as the maximum number of tokens increases.","sentences":["In Internet of Things (IoT) status update systems, where information is sampled and subsequently transmitted from a source to a destination node, the imperative necessity lies in maintaining the timeliness of information and updating the system with optimal frequency.","Optimizing information freshness in resource-limited status update systems often involves Constrained Markov Decision Process (CMDP) problems with update rate constraints.","Solving CMDP problems, especially with multiple constraints, is a challenging task.","To address this, we present a token-based approach that transforms CMDP into an unconstrained MDP, simplifying the solution process.","We apply this approach to systems with one and two update rate constraints for optimizing Age of Incorrect Information (AoII) and Age of Information (AoI) metrics, respectively, and explore the analytical and numerical aspects.","Additionally, we introduce an iterative triangle bisection method for solving the CMDP problems with two constraints, comparing its results with the token-based MDP approach.","Our findings show that the token-based approach yields superior performance over baseline policies, converging to the optimal policy as the maximum number of tokens increases."],"url":"http://arxiv.org/abs/2405.04431v1","category":"cs.IT"}
{"created":"2024-05-07 15:49:52","title":"Intrinsic Current Concentration of Buffer Layer Material for Cable Ablation Failure: Role of Random Fiber Networks","abstract":"In recent years, the buffer layer ablation failures of high voltage cables are frequently reported by the power systems. Previous studies have dominantly regarded the buffer layer as the continuous homogeneous medium, whereas neglects its microstructures. In this paper, the current distribution within the random fiber networks of buffer layer are investigated. Experiment results of our self-designed platform revealed an uneven current distribution in buffer layer at the moment of bearing current. This phenomenon is named as the intrinsic current concentration where the current density concentrates at certain sites inner the buffer layer. And the degree of current concentration will be suppressed by compressing the sample. Then, a 2D simulation model of the random fiber networks was constructed based on the Mikado model. The simulation results also presented an uneven current distribution in the networks whose every fiber can be viewed as a micro-resistor. Two types of dimensionless current concentration factors were defined to describe the degree of current concentration, finding their values decreasing with the rise of fiber density. Meanwhile, it is equivalent of compressing the buffer layer and increasing the fiber density of model. We believe that the intrinsic current concentration phenomenon is mainly related with the inhomogeneity of geometry structure of buffer layer. The ablation traces and fracture fibers observed by the X-ray micro-computed tomography test supported this point. In addition, the non-ideal surface of sample can also induce this phenomenon. The intrinsic current concentration can aggravate the degree of originally existed macroscopic current concentration in cables, thus causing the ablation failure. Our work may unveil a deeper understanding on the cable ablation failure and the electrical response of the similar fibrous materials.","sentences":["In recent years, the buffer layer ablation failures of high voltage cables are frequently reported by the power systems.","Previous studies have dominantly regarded the buffer layer as the continuous homogeneous medium, whereas neglects its microstructures.","In this paper, the current distribution within the random fiber networks of buffer layer are investigated.","Experiment results of our self-designed platform revealed an uneven current distribution in buffer layer at the moment of bearing current.","This phenomenon is named as the intrinsic current concentration where the current density concentrates at certain sites inner the buffer layer.","And the degree of current concentration will be suppressed by compressing the sample.","Then, a 2D simulation model of the random fiber networks was constructed based on the Mikado model.","The simulation results also presented an uneven current distribution in the networks whose every fiber can be viewed as a micro-resistor.","Two types of dimensionless current concentration factors were defined to describe the degree of current concentration, finding their values decreasing with the rise of fiber density.","Meanwhile, it is equivalent of compressing the buffer layer and increasing the fiber density of model.","We believe that the intrinsic current concentration phenomenon is mainly related with the inhomogeneity of geometry structure of buffer layer.","The ablation traces and fracture fibers observed by the X-ray micro-computed tomography test supported this point.","In addition, the non-ideal surface of sample can also induce this phenomenon.","The intrinsic current concentration can aggravate the degree of originally existed macroscopic current concentration in cables, thus causing the ablation failure.","Our work may unveil a deeper understanding on the cable ablation failure and the electrical response of the similar fibrous materials."],"url":"http://arxiv.org/abs/2405.04430v1","category":"physics.app-ph"}
{"created":"2024-05-07 15:49:34","title":"BBK: a simpler, faster algorithm for enumerating maximal bicliques in large sparse bipartite graphs","abstract":"Bipartite graphs are a prevalent modeling tool for real-world networks, capturing interactions between vertices of two different types. Within this framework, bicliques emerge as crucial structures when studying dense subgraphs: they are sets of vertices such that all vertices of the first type interact with all vertices of the second type. Therefore, they allow identifying groups of closely related vertices of the network, such as individuals with similar interests or webpages with similar contents. This article introduces a new algorithm designed for the exhaustive enumeration of maximal bicliques within a bipartite graph. This algorithm, called BBK for Bipartite Bron-Kerbosch, is a new extension to the bipartite case of the Bron-Kerbosch algorithm, which enumerates the maximal cliques in standard (non-bipartite) graphs. It is faster than the state-of-the-art algorithms and allows the enumeration on massive bipartite graphs that are not manageable with existing implementations. We analyze it theoretically to establish two complexity formulas: one as a function of the input and one as a function of the output characteristics of the algorithm. We also provide an open-access implementation of BBK in C++, which we use to experiment and validate its efficiency on massive real-world datasets and show that its execution time is shorter in practice than state-of-the art algorithms. These experiments also show that the order in which the vertices are processed, as well as the choice of one of the two types of vertices on which to initiate the enumeration have an impact on the computation time.","sentences":["Bipartite graphs are a prevalent modeling tool for real-world networks, capturing interactions between vertices of two different types.","Within this framework, bicliques emerge as crucial structures when studying dense subgraphs: they are sets of vertices such that all vertices of the first type interact with all vertices of the second type.","Therefore, they allow identifying groups of closely related vertices of the network, such as individuals with similar interests or webpages with similar contents.","This article introduces a new algorithm designed for the exhaustive enumeration of maximal bicliques within a bipartite graph.","This algorithm, called BBK for Bipartite Bron-Kerbosch, is a new extension to the bipartite case of the Bron-Kerbosch algorithm, which enumerates the maximal cliques in standard (non-bipartite) graphs.","It is faster than the state-of-the-art algorithms and allows the enumeration on massive bipartite graphs that are not manageable with existing implementations.","We analyze it theoretically to establish two complexity formulas: one as a function of the input and one as a function of the output characteristics of the algorithm.","We also provide an open-access implementation of BBK in C++, which we use to experiment and validate its efficiency on massive real-world datasets and show that its execution time is shorter in practice than state-of-the art algorithms.","These experiments also show that the order in which the vertices are processed, as well as the choice of one of the two types of vertices on which to initiate the enumeration have an impact on the computation time."],"url":"http://arxiv.org/abs/2405.04428v1","category":"cs.DS"}
{"created":"2024-05-07 15:49:21","title":"Josephson threshold detector in the phase diffusion regime","abstract":"We demonstrate that the performance of threshold detectors based on Al Josephson junctions can be significantly improved by exploiting the phase diffusion regime. When the escape dynamics of the detector switches to this regime, a decrease in both - dark count rate and the standard deviation of switching current is simultaneously observed. However, this effect is essential for (i) critical currents below 100 nA, and (ii) temperatures of the order of several hundreds millikelvin. Importantly that for such detectors optimal performance occurs at finite temperatures, making the microwave single photon detection feasible even in the sub-K range. Possible explanation of these findings is discussed.","sentences":["We demonstrate that the performance of threshold detectors based on Al Josephson junctions can be significantly improved by exploiting the phase diffusion regime.","When the escape dynamics of the detector switches to this regime, a decrease in both - dark count rate and the standard deviation of switching current is simultaneously observed.","However, this effect is essential for (i) critical currents below 100 nA, and (ii) temperatures of the order of several hundreds millikelvin.","Importantly that for such detectors optimal performance occurs at finite temperatures, making the microwave single photon detection feasible even in the sub-K range.","Possible explanation of these findings is discussed."],"url":"http://arxiv.org/abs/2405.04426v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-07 15:44:39","title":"Fully Automated Selfish Mining Analysis in Efficient Proof Systems Blockchains","abstract":"We study selfish mining attacks in longest-chain blockchains like Bitcoin, but where the proof of work is replaced with efficient proof systems -- like proofs of stake or proofs of space -- and consider the problem of computing an optimal selfish mining attack which maximizes expected relative revenue of the adversary, thus minimizing the chain quality. To this end, we propose a novel selfish mining attack that aims to maximize this objective and formally model the attack as a Markov decision process (MDP). We then present a formal analysis procedure which computes an $\\epsilon$-tight lower bound on the optimal expected relative revenue in the MDP and a strategy that achieves this $\\epsilon$-tight lower bound, where $\\epsilon>0$ may be any specified precision. Our analysis is fully automated and provides formal guarantees on the correctness. We evaluate our selfish mining attack and observe that it achieves superior expected relative revenue compared to two considered baselines.   In concurrent work [Sarenche FC'24] does an automated analysis on selfish mining in predictable longest-chain blockchains based on efficient proof systems. Predictable means the randomness for the challenges is fixed for many blocks (as used e.g., in Ouroboros), while we consider unpredictable (Bitcoin-like) chains where the challenge is derived from the previous block.","sentences":["We study selfish mining attacks in longest-chain blockchains like Bitcoin, but where the proof of work is replaced with efficient proof systems -- like proofs of stake or proofs of space -- and consider the problem of computing an optimal selfish mining attack which maximizes expected relative revenue of the adversary, thus minimizing the chain quality.","To this end, we propose a novel selfish mining attack that aims to maximize this objective and formally model the attack as a Markov decision process (MDP).","We then present a formal analysis procedure which computes an $\\epsilon$-tight lower bound on the optimal expected relative revenue in the MDP and a strategy that achieves this $\\epsilon$-tight lower bound, where $\\epsilon>0$ may be any specified precision.","Our analysis is fully automated and provides formal guarantees on the correctness.","We evaluate our selfish mining attack and observe that it achieves superior expected relative revenue compared to two considered baselines.   ","In concurrent work [Sarenche FC'24] does an automated analysis on selfish mining in predictable longest-chain blockchains based on efficient proof systems.","Predictable means the randomness for the challenges is fixed for many blocks (as used e.g., in Ouroboros), while we consider unpredictable (Bitcoin-like) chains where the challenge is derived from the previous block."],"url":"http://arxiv.org/abs/2405.04420v1","category":"cs.CR"}
{"created":"2024-05-07 15:42:54","title":"The Large Deviation Principle for $W$-random spectral measures","abstract":"The $W$-random graphs provide a flexible framework for modeling large random networks. Using the Large Deviation Principle (LDP) for $W$-random graphs from [9], we prove the LDP for the corresponding class of random symmetric Hilbert-Schmidt integral operators. Our main result describes how the eigenvalues and the eigenspaces of the integral operator are affected by the large deviations in the underlying random graphon. To prove the LDP, we demonstrate continuous dependence of the spectral measures associated with integral operators on the underlying graphons and use the Contraction Principle. To illustrate our results, we obtain leading order asymptotics of the eigenvalues of the integral operators corresponding to certain random graph sequences. These examples suggest several representative scenarios of how the eigenvalues and the eigenspaces of the integral operators are affected by large deviations. Potential implications of these observations for bifurcation analysis of Dynamical Systems and Graph Signal Processing are indicated.","sentences":["The $W$-random graphs provide a flexible framework for modeling large random networks.","Using the Large Deviation Principle (LDP) for $W$-random graphs from [9], we prove the LDP for the corresponding class of random symmetric Hilbert-Schmidt integral operators.","Our main result describes how the eigenvalues and the eigenspaces of the integral operator are affected by the large deviations in the underlying random graphon.","To prove the LDP, we demonstrate continuous dependence of the spectral measures associated with integral operators on the underlying graphons and use the Contraction Principle.","To illustrate our results, we obtain leading order asymptotics of the eigenvalues of the integral operators corresponding to certain random graph sequences.","These examples suggest several representative scenarios of how the eigenvalues and the eigenspaces of the integral operators are affected by large deviations.","Potential implications of these observations for bifurcation analysis of Dynamical Systems and Graph Signal Processing are indicated."],"url":"http://arxiv.org/abs/2405.04417v1","category":"math.PR"}
{"created":"2024-05-07 15:35:43","title":"DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks","abstract":"Document image restoration is a crucial aspect of Document AI systems, as the quality of document images significantly influences the overall performance. Prevailing methods address distinct restoration tasks independently, leading to intricate systems and the incapability to harness the potential synergies of multi-task learning. To overcome this challenge, we propose DocRes, a generalist model that unifies five document image restoration tasks including dewarping, deshadowing, appearance enhancement, deblurring, and binarization. To instruct DocRes to perform various restoration tasks, we propose a novel visual prompt approach called Dynamic Task-Specific Prompt (DTSPrompt). The DTSPrompt for different tasks comprises distinct prior features, which are additional characteristics extracted from the input image. Beyond its role as a cue for task-specific execution, DTSPrompt can also serve as supplementary information to enhance the model's performance. Moreover, DTSPrompt is more flexible than prior visual prompt approaches as it can be seamlessly applied and adapted to inputs with high and variable resolutions. Experimental results demonstrate that DocRes achieves competitive or superior performance compared to existing state-of-the-art task-specific models. This underscores the potential of DocRes across a broader spectrum of document image restoration tasks. The source code is publicly available at https://github.com/ZZZHANG-jx/DocRes","sentences":["Document image restoration is a crucial aspect of Document AI systems, as the quality of document images significantly influences the overall performance.","Prevailing methods address distinct restoration tasks independently, leading to intricate systems and the incapability to harness the potential synergies of multi-task learning.","To overcome this challenge, we propose DocRes, a generalist model that unifies five document image restoration tasks including dewarping, deshadowing, appearance enhancement, deblurring, and binarization.","To instruct DocRes to perform various restoration tasks, we propose a novel visual prompt approach called Dynamic Task-Specific Prompt (DTSPrompt).","The DTSPrompt for different tasks comprises distinct prior features, which are additional characteristics extracted from the input image.","Beyond its role as a cue for task-specific execution, DTSPrompt can also serve as supplementary information to enhance the model's performance.","Moreover, DTSPrompt is more flexible than prior visual prompt approaches as it can be seamlessly applied and adapted to inputs with high and variable resolutions.","Experimental results demonstrate that DocRes achieves competitive or superior performance compared to existing state-of-the-art task-specific models.","This underscores the potential of DocRes across a broader spectrum of document image restoration tasks.","The source code is publicly available at https://github.com/ZZZHANG-jx/DocRes"],"url":"http://arxiv.org/abs/2405.04408v1","category":"cs.CV"}
{"created":"2024-05-07 15:27:46","title":"Utility-driven Optimization of TTL Cache Hierarchies under Network Delays","abstract":"We optimize hierarchies of Time-to-Live (TTL) caches under random network delays. A TTL cache assigns individual eviction timers to cached objects that are usually refreshed upon a hit where upon a miss the object requires a random time to be fetched from a parent cache. Due to their object decoupling property, TTL caches are of particular interest since the optimization of a per-object utility enables service differentiation. However, state-of-the-art exact TTL cache optimization does not extend beyond single TTL caches, especially under network delays. In this paper, we leverage the object decoupling effect to formulate the non-linear utility maximization problem for TTL cache hierarchies in terms of the exact object hit probability under random network delays. We iteratively solve the utility maximization problem to find the optimal per-object TTLs. Further, we show that the exact model suffers from tractability issues for large hierarchies and propose a machine learning approach to estimate the optimal TTL values for large systems. Finally, we provide numerical and data center trace-based evaluations for both methods showing the significant offloading improvement due to TTL optimization considering the network delays.","sentences":["We optimize hierarchies of Time-to-Live (TTL) caches under random network delays.","A TTL cache assigns individual eviction timers to cached objects that are usually refreshed upon a hit where upon a miss the object requires a random time to be fetched from a parent cache.","Due to their object decoupling property, TTL caches are of particular interest since the optimization of a per-object utility enables service differentiation.","However, state-of-the-art exact TTL cache optimization does not extend beyond single TTL caches, especially under network delays.","In this paper, we leverage the object decoupling effect to formulate the non-linear utility maximization problem for TTL cache hierarchies in terms of the exact object hit probability under random network delays.","We iteratively solve the utility maximization problem to find the optimal per-object TTLs.","Further, we show that the exact model suffers from tractability issues for large hierarchies and propose a machine learning approach to estimate the optimal TTL values for large systems.","Finally, we provide numerical and data center trace-based evaluations for both methods showing the significant offloading improvement due to TTL optimization considering the network delays."],"url":"http://arxiv.org/abs/2405.04402v1","category":"cs.NI"}
{"created":"2024-05-07 15:26:09","title":"Decentralized Algorithms for Out-of-System Interference Suppression in Distributed MIMO","abstract":"Out-of-system (OoS) interference is a potential limitation for distributed networks that operate in unlicensed spectrum or in a spectrum sharing scenario. The OoS interference differs from the in-system interference in that OoS signals and their associated channels (or even their statistics) are completely unknown. In this paper, we propose a novel distributed algorithm that can mitigate OoS interference in the uplink and suppress the signal transmission in the OoS direction in the downlink. To estimate the OoS interference, each access point (AP), upon receiving an estimate of OoS interference from a previous AP, computes a better estimate of OoS interference by rotate-and-average using Procrustes method and forwards the estimates to the next AP. This process continues until the central processing unit (CPU) receives the final estimate. Our method has comparable performance to that of a fully centralized interference rejection combining algorithm and has much lower fronthaul load requirements.","sentences":["Out-of-system (OoS) interference is a potential limitation for distributed networks that operate in unlicensed spectrum or in a spectrum sharing scenario.","The OoS interference differs from the in-system interference in that OoS signals and their associated channels (or even their statistics) are completely unknown.","In this paper, we propose a novel distributed algorithm that can mitigate OoS interference in the uplink and suppress the signal transmission in the OoS direction in the downlink.","To estimate the OoS interference, each access point (AP), upon receiving an estimate of OoS interference from a previous AP, computes a better estimate of OoS interference by rotate-and-average using Procrustes method and forwards the estimates to the next AP.","This process continues until the central processing unit (CPU) receives the final estimate.","Our method has comparable performance to that of a fully centralized interference rejection combining algorithm and has much lower fronthaul load requirements."],"url":"http://arxiv.org/abs/2405.04400v1","category":"cs.IT"}
{"created":"2024-05-07 15:25:39","title":"Solving ill-conditioned linear algebraic systems using methods that improve conditioning","abstract":"We consider the solution of systems of linear algebraic equations (SLAEs) with an ill-conditioned or degenerate exact matrix and an approximate right-hand side. An approach to solving such a problem is proposed and justified, which makes it possible to improve the conditionality of the SLAE matrix and, as a result, obtain an approximate solution that is stable to perturbations of the right hand side with higher accuracy than using other methods. The approach is implemented by an algorithm that uses so-called minimal pseudoinverse matrices. The results of numerical experiments are presented that confirm the theoretical provisions of the article.","sentences":["We consider the solution of systems of linear algebraic equations (SLAEs) with an ill-conditioned or degenerate exact matrix and an approximate right-hand side.","An approach to solving such a problem is proposed and justified, which makes it possible to improve the conditionality of the SLAE matrix and, as a result, obtain an approximate solution that is stable to perturbations of the right hand side with higher accuracy than using other methods.","The approach is implemented by an algorithm that uses so-called minimal pseudoinverse matrices.","The results of numerical experiments are presented that confirm the theoretical provisions of the article."],"url":"http://arxiv.org/abs/2405.04399v1","category":"math.NA"}
{"created":"2024-05-07 15:13:18","title":"Parallelized Multi-Agent Bayesian Optimization in Lava","abstract":"In parallel with the continuously increasing parameter space dimensionality, search and optimization algorithms should support distributed parameter evaluations to reduce cumulative runtime. Intel's neuromorphic optimization library, Lava-Optimization, was introduced as an abstract optimization system compatible with neuromorphic systems developed in the broader Lava software framework. In this work, we introduce Lava Multi-Agent Optimization (LMAO) with native support for distributed parameter evaluations communicating with a central Bayesian optimization system. LMAO provides an abstract framework for deploying distributed optimization and search algorithms within the Lava software framework. Moreover, LMAO introduces support for random and grid search along with process connections across multiple levels of mathematical precision. We evaluate the algorithmic performance of LMAO with a traditional non-convex optimization problem, a fixed-precision transductive spiking graph neural network for citation graph classification, and a neuromorphic satellite scheduling problem. Our results highlight LMAO's efficient scaling to multiple processes, reducing cumulative runtime and minimizing the likelihood of converging to local optima.","sentences":["In parallel with the continuously increasing parameter space dimensionality, search and optimization algorithms should support distributed parameter evaluations to reduce cumulative runtime.","Intel's neuromorphic optimization library, Lava-Optimization, was introduced as an abstract optimization system compatible with neuromorphic systems developed in the broader Lava software framework.","In this work, we introduce Lava Multi-Agent Optimization (LMAO) with native support for distributed parameter evaluations communicating with a central Bayesian optimization system.","LMAO provides an abstract framework for deploying distributed optimization and search algorithms within the Lava software framework.","Moreover, LMAO introduces support for random and grid search along with process connections across multiple levels of mathematical precision.","We evaluate the algorithmic performance of LMAO with a traditional non-convex optimization problem, a fixed-precision transductive spiking graph neural network for citation graph classification, and a neuromorphic satellite scheduling problem.","Our results highlight LMAO's efficient scaling to multiple processes, reducing cumulative runtime and minimizing the likelihood of converging to local optima."],"url":"http://arxiv.org/abs/2405.04387v1","category":"cs.DC"}
{"created":"2024-05-07 14:45:32","title":"Interplay between short-range and critical long-range fluctuations in the out-of-equilibrium behavior of the particle density at quantum transitions","abstract":"We address the equilibrium and out-of-equilibrium behavior of the particle density in many-body systems undergoing quantum transitions driven by the chemical potential $\\mu$. They originate from a nontrivial interplay between noncritical short-range and critical long-range quantum fluctuations. As a paradigmatic model we consider the one-dimensional fermionic Kitaev chain, for which very accurate numerical checks of our scaling ansatz can be performed up to $O(10^4)$ sites. The search for dynamic scaling behaviors of the particle density is complicated by the fact that its equilibrium (ground state) behavior is dominated by short-range fluctuations, giving rise to regular background terms and peculiar logarithmic terms from resonances between renormalization-group perturbations associated with the energy and identity operator families within the conformal field theory. To study these issues, we focus on two dynamic protocols, either instantaneous quenches or quasi-adiabatic changes of $\\mu$ to the critical value $\\mu_c$, unveiling out-of-equilibrium scaling behaviors of the particle density, which arise from the critical modes, within a dynamic finite-size scaling framework.","sentences":["We address the equilibrium and out-of-equilibrium behavior of the particle density in many-body systems undergoing quantum transitions driven by the chemical potential $\\mu$. They originate from a nontrivial interplay between noncritical short-range and critical long-range quantum fluctuations.","As a paradigmatic model we consider the one-dimensional fermionic Kitaev chain, for which very accurate numerical checks of our scaling ansatz can be performed up to $O(10^4)$ sites.","The search for dynamic scaling behaviors of the particle density is complicated by the fact that its equilibrium (ground state) behavior is dominated by short-range fluctuations, giving rise to regular background terms and peculiar logarithmic terms from resonances between renormalization-group perturbations associated with the energy and identity operator families within the conformal field theory.","To study these issues, we focus on two dynamic protocols, either instantaneous quenches or quasi-adiabatic changes of $\\mu$ to the critical value $\\mu_c$, unveiling out-of-equilibrium scaling behaviors of the particle density, which arise from the critical modes, within a dynamic finite-size scaling framework."],"url":"http://arxiv.org/abs/2405.04364v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-07 14:44:41","title":"Some Notes on the Sample Complexity of Approximate Channel Simulation","abstract":"Channel simulation algorithms can efficiently encode random samples from a prescribed target distribution $Q$ and find applications in machine learning-based lossy data compression. However, algorithms that encode exact samples usually have random runtime, limiting their applicability when a consistent encoding time is desirable. Thus, this paper considers approximate schemes with a fixed runtime instead. First, we strengthen a result of Agustsson and Theis and show that there is a class of pairs of target distribution $Q$ and coding distribution $P$, for which the runtime of any approximate scheme scales at least super-polynomially in $D_\\infty[Q \\Vert P]$. We then show, by contrast, that if we have access to an unnormalised Radon-Nikodym derivative $r \\propto dQ/dP$ and knowledge of $D_{KL}[Q \\Vert P]$, we can exploit global-bound, depth-limited A* coding to ensure $\\mathrm{TV}[Q \\Vert P] \\leq \\epsilon$ and maintain optimal coding performance with a sample complexity of only $\\exp_2\\big((D_{KL}[Q \\Vert P] + o(1)) \\big/ \\epsilon\\big)$.","sentences":["Channel simulation algorithms can efficiently encode random samples from a prescribed target distribution $Q$ and find applications in machine learning-based lossy data compression.","However, algorithms that encode exact samples usually have random runtime, limiting their applicability when a consistent encoding time is desirable.","Thus, this paper considers approximate schemes with a fixed runtime instead.","First, we strengthen a result of Agustsson and Theis and show that there is a class of pairs of target distribution $Q$ and coding distribution $P$, for which the runtime of any approximate scheme scales at least super-polynomially in $D_\\infty[Q \\Vert P]$.","We then show, by contrast, that if we have access to an unnormalised Radon-Nikodym derivative $r \\propto dQ/dP$ and knowledge of $D_{KL}[Q \\Vert P]$, we can exploit global-bound, depth-limited A* coding to ensure $\\mathrm{TV}[Q \\Vert P] \\leq \\epsilon$ and maintain optimal coding performance with a sample complexity of only $\\exp_2\\big((D_{KL}[Q \\Vert P] + o(1))","\\big/ \\epsilon\\big)$."],"url":"http://arxiv.org/abs/2405.04363v1","category":"cs.IT"}
{"created":"2024-05-07 14:33:05","title":"SmmPack: Obfuscation for SMM Modules with TPM Sealed Key","abstract":"System Management Mode (SMM) is the highest-privileged operating mode of x86 and x86-64 processors. Through SMM exploitation, attackers can tamper with the Unified Extensible Firmware Interface (UEFI) firmware, disabling the security mechanisms implemented by the operating system and hypervisor. Vulnerabilities enabling SMM code execution are often reported as Common Vulnerabilities and Exposures (CVEs); however, no security mechanisms currently exist to prevent attackers from analyzing those vulnerabilities. To increase the cost of vulnerability analysis of SMM modules, we introduced SmmPack. The core concept of SmmPack involves encrypting an SMM module with the key securely stored in a Trusted Platform Module (TPM). We assessed the effectiveness of SmmPack in preventing attackers from obtaining and analyzing SMM modules using various acquisition methods. Our results show that SmmPack significantly increases the cost by narrowing down the means of module acquisition. Furthermore, we demonstrated that SmmPack operates without compromising the performance of the original SMM modules. We also clarified the management and adoption methods of SmmPack, as well as the procedure for applying BIOS updates, and demonstrated that the implementation of SmmPack is realistic.","sentences":["System Management Mode (SMM) is the highest-privileged operating mode of x86 and x86-64 processors.","Through SMM exploitation, attackers can tamper with the Unified Extensible Firmware Interface (UEFI) firmware, disabling the security mechanisms implemented by the operating system and hypervisor.","Vulnerabilities enabling SMM code execution are often reported as Common Vulnerabilities and Exposures (CVEs); however, no security mechanisms currently exist to prevent attackers from analyzing those vulnerabilities.","To increase the cost of vulnerability analysis of SMM modules, we introduced SmmPack.","The core concept of SmmPack involves encrypting an SMM module with the key securely stored in a Trusted Platform Module (TPM).","We assessed the effectiveness of SmmPack in preventing attackers from obtaining and analyzing SMM modules using various acquisition methods.","Our results show that SmmPack significantly increases the cost by narrowing down the means of module acquisition.","Furthermore, we demonstrated that SmmPack operates without compromising the performance of the original SMM modules.","We also clarified the management and adoption methods of SmmPack, as well as the procedure for applying BIOS updates, and demonstrated that the implementation of SmmPack is realistic."],"url":"http://arxiv.org/abs/2405.04355v1","category":"cs.CR"}
{"created":"2024-05-07 14:28:04","title":"Decision-Dependent Uncertainty-Aware Distribution System Planning Under Wildfire Risk","abstract":"The interaction between power systems and wildfires can be dangerous and costly. Damaged structures, load shedding, and high operational costs are potential consequences when the grid is unprepared. In fact, the operation of distribution grids can be liable for the outbreak of wildfires when extreme weather conditions arise. Within this context, investment planning should consider the impact of operational actions on the uncertainty related to wildfires that can directly affect line failure likelihood. Neglecting this can compromise the cost-benefit evaluation in planning system investments for wildfire risk. In this paper, we propose a decision-dependent uncertainty (DDU) aware methodology that provides the optimal portfolio of investments for distribution systems while considering that high power-flow levels through line segments in high-threat areas can ignite wildfires and, therefore, increase the probability of line failures. The methodology identifies the best combination of system upgrades (installation of new lines, hardening existing lines, and placement of switching devices) to provide the necessary leeway to operate the distribution system under wildfire-prone conditions. Our case study demonstrates that by modeling the DDU relationship between power flow prescriptions and line failures, investment decisions are more accurate and better prepare the grid infrastructure to deal with wildfire risk.","sentences":["The interaction between power systems and wildfires can be dangerous and costly.","Damaged structures, load shedding, and high operational costs are potential consequences when the grid is unprepared.","In fact, the operation of distribution grids can be liable for the outbreak of wildfires when extreme weather conditions arise.","Within this context, investment planning should consider the impact of operational actions on the uncertainty related to wildfires that can directly affect line failure likelihood.","Neglecting this can compromise the cost-benefit evaluation in planning system investments for wildfire risk.","In this paper, we propose a decision-dependent uncertainty (DDU) aware methodology that provides the optimal portfolio of investments for distribution systems while considering that high power-flow levels through line segments in high-threat areas can ignite wildfires and, therefore, increase the probability of line failures.","The methodology identifies the best combination of system upgrades (installation of new lines, hardening existing lines, and placement of switching devices) to provide the necessary leeway to operate the distribution system under wildfire-prone conditions.","Our case study demonstrates that by modeling the DDU relationship between power flow prescriptions and line failures, investment decisions are more accurate and better prepare the grid infrastructure to deal with wildfire risk."],"url":"http://arxiv.org/abs/2405.04350v1","category":"math.OC"}
{"created":"2024-05-07 14:23:28","title":"Development of discontinuous Galerkin methods for hyperbolic systems that preserve a curl or a divergence constraint","abstract":"Some hyperbolic systems are known to include implicit preservation of differential constraints: these are for example the time conservation of the curl or the divergence of a vector that appear as an implicit constraint. In this article, we show that this kind of constraint can be easily conserved at the discrete level with the classical discontinuous Galerkin method, provided the right approximation space is used for the vectorial space, and under some mild assumption on the numerical flux. For this, we develop a discrete differential geometry framework for some well chosen piece-wise polynomial vector approximation space. More precisely, we define the discrete Hodge star operator, the exterior derivative, and their adjoints. The discrete adjoint divergence and curl are proven to be exactly preserved by the discontinuous Galerkin method under a small assumption on the numerical flux. Numerical tests are performed on the wave system, the two dimensional Maxwell system and the induction equation, and confirm that the differential constraints are preserved at machine precision while keeping the high order of accuracy.","sentences":["Some hyperbolic systems are known to include implicit preservation of differential constraints: these are for example the time conservation of the curl or the divergence of a vector that appear as an implicit constraint.","In this article, we show that this kind of constraint can be easily conserved at the discrete level with the classical discontinuous Galerkin method, provided the right approximation space is used for the vectorial space, and under some mild assumption on the numerical flux.","For this, we develop a discrete differential geometry framework for some well chosen piece-wise polynomial vector approximation space.","More precisely, we define the discrete Hodge star operator, the exterior derivative, and their adjoints.","The discrete adjoint divergence and curl are proven to be exactly preserved by the discontinuous Galerkin method under a small assumption on the numerical flux.","Numerical tests are performed on the wave system, the two dimensional Maxwell system and the induction equation, and confirm that the differential constraints are preserved at machine precision while keeping the high order of accuracy."],"url":"http://arxiv.org/abs/2405.04347v1","category":"math.NA"}
{"created":"2024-05-07 14:13:25","title":"High velocity solid dust impacts on tiles of tokamak-relevant temperature","abstract":"Runaway electron incidence on plasma facing components triggers explosive events that are accompanied by the expulsion of fast solid debris. Subsequent dust-wall high speed impacts constitute a mechanism of wall damage and dust destruction. Empirical damage laws that can be employed for erosion estimates are based on room-temperature impact experiments. We use light-gas gun shooting systems to accelerate solid tungsten dust to near-supersonic speeds towards bulk tungsten targets that are maintained at different temperatures. This concerns targets cooled down to $-100^{\\circ}$C with liquid nitrogen and targets resistively heated up to $400^{\\circ}$C. Post-mortem surface analysis reveals that the three erosion regimes (plastic deformation, bonding, partial disintegration) weakly depend on the target temperature within the investigated range. It is concluded that empirical damage laws based on room-temperature measurements can be safely employed for predictions.","sentences":["Runaway electron incidence on plasma facing components triggers explosive events that are accompanied by the expulsion of fast solid debris.","Subsequent dust-wall high speed impacts constitute a mechanism of wall damage and dust destruction.","Empirical damage laws that can be employed for erosion estimates are based on room-temperature impact experiments.","We use light-gas gun shooting systems to accelerate solid tungsten dust to near-supersonic speeds towards bulk tungsten targets that are maintained at different temperatures.","This concerns targets cooled down to $-100^{\\circ}$C with liquid nitrogen and targets resistively heated up to $400^{\\circ}$C. Post-mortem surface analysis reveals that the three erosion regimes (plastic deformation, bonding, partial disintegration) weakly depend on the target temperature within the investigated range.","It is concluded that empirical damage laws based on room-temperature measurements can be safely employed for predictions."],"url":"http://arxiv.org/abs/2405.04339v1","category":"physics.plasm-ph"}
{"created":"2024-05-07 14:00:51","title":"Packings of Smoothed Polygons","abstract":"This book uses optimal control theory to prove that the most unpackable centrally symmetric convex disk in the plane is a smoothed polygon. A smoothed polygon is a polygon whose corners have been rounded in a special way by arcs of hyperbolas. To be highly unpackable means that even densest packing of that disk has low density.   Motivated by Minkowski's geometry of numbers, researchers began to search for the most unpackable centrally symmetric convex disk (in brief, the most unpackable disk) starting in the early 1920s. In 1934, Reinhardt conjectured that the most unpackable disk is a smoothed octagon. Working independently of Reinhardt, Mahler attempted without success in 1947 to prove that the most unpackable disk must be a smoothed polygon. This book proves what Mahler set out to prove: Mahler's First conjecture on smoothed polygons. His second conjecture is identical to the Reinhardt conjecture, which remains open.   This book explores the many remarkable structures of this packing problem, formulated as a problem in optimal control theory on a Lie group, with connections to hyperbolic geometry and Hamiltonian mechanics. Bang-bang Pontryagin extremals to the optimal control problem are smoothed polygons. Extreme difficulties arise in the proof because of chattering behavior in the optimal control problem, corresponding to possible smoothed polygons with infinitely many sides that need to be ruled out. To analyze and eliminate the possibility of chattering solutions, the book introduces a discrete dynamical system (the Poincare first recurrence map) and gives a full description of its fixed points, stable and unstable manifolds, and basin of attraction on a blowup centered at a singular set. Some proofs in this book are computer-assisted using a computer algebra system.","sentences":["This book uses optimal control theory to prove that the most unpackable centrally symmetric convex disk in the plane is a smoothed polygon.","A smoothed polygon is a polygon whose corners have been rounded in a special way by arcs of hyperbolas.","To be highly unpackable means that even densest packing of that disk has low density.   ","Motivated by Minkowski's geometry of numbers, researchers began to search for the most unpackable centrally symmetric convex disk (in brief, the most unpackable disk) starting in the early 1920s.","In 1934, Reinhardt conjectured that the most unpackable disk is a smoothed octagon.","Working independently of Reinhardt, Mahler attempted without success in 1947 to prove that the most unpackable disk must be a smoothed polygon.","This book proves what Mahler set out to prove: Mahler's First conjecture on smoothed polygons.","His second conjecture is identical to the Reinhardt conjecture, which remains open.   ","This book explores the many remarkable structures of this packing problem, formulated as a problem in optimal control theory on a Lie group, with connections to hyperbolic geometry and Hamiltonian mechanics.","Bang-bang Pontryagin extremals to the optimal control problem are smoothed polygons.","Extreme difficulties arise in the proof because of chattering behavior in the optimal control problem, corresponding to possible smoothed polygons with infinitely many sides that need to be ruled out.","To analyze and eliminate the possibility of chattering solutions, the book introduces a discrete dynamical system (the Poincare first recurrence map) and gives a full description of its fixed points, stable and unstable manifolds, and basin of attraction on a blowup centered at a singular set.","Some proofs in this book are computer-assisted using a computer algebra system."],"url":"http://arxiv.org/abs/2405.04331v1","category":"math.OC"}
{"created":"2024-05-07 13:58:57","title":"How to reveal the rank of a matrix?","abstract":"We study algorithms called rank-revealers that reveal a matrix's rank structure. Such algorithms form a fundamental component in matrix compression, singular value estimation, and column subset selection problems. While column-pivoted QR has been widely adopted due to its practicality, it is not always a rank-revealer. Conversely, Gaussian elimination (GE) with a pivoting strategy known as global maximum volume pivoting is guaranteed to estimate a matrix's singular values but its exponential algorithmic complexity limits its interest to theory. We show that the concept of local maximum volume pivoting is a crucial and practical pivoting strategy for rank-revealers based on GE and QR, showing that it is both necessary and sufficient. This insight elevates Gu and Eisenstat's rank-revealing QR as an archetypal rank-revealer, and devise a version that is less than $2\\times$ more computationally expensive than CPQR. We unify the landscape of rank-revealers by considering GE and QR together and prove that the success of any pivoting strategy can be assessed by benchmarking it against a local maximum volume pivot.","sentences":["We study algorithms called rank-revealers that reveal a matrix's rank structure.","Such algorithms form a fundamental component in matrix compression, singular value estimation, and column subset selection problems.","While column-pivoted QR has been widely adopted due to its practicality, it is not always a rank-revealer.","Conversely, Gaussian elimination (GE) with a pivoting strategy known as global maximum volume pivoting is guaranteed to estimate a matrix's singular values but its exponential algorithmic complexity limits its interest to theory.","We show that the concept of local maximum volume pivoting is a crucial and practical pivoting strategy for rank-revealers based on GE and QR, showing that it is both necessary and sufficient.","This insight elevates Gu and Eisenstat's rank-revealing QR as an archetypal rank-revealer, and devise a version that is less than $2\\times$ more computationally expensive than CPQR.","We unify the landscape of rank-revealers by considering GE and QR together and prove that the success of any pivoting strategy can be assessed by benchmarking it against a local maximum volume pivot."],"url":"http://arxiv.org/abs/2405.04330v1","category":"math.NA"}
{"created":"2024-05-07 13:40:24","title":"GLIDS: A Global Latency Information Dissemination System","abstract":"A recent advance in networking is the deployment of path-aware multipath network architectures, where network endpoints are given multiple network paths to send their data on. In this work, we tackle the challenge of selecting paths for latency-sensitive applications. Even today's path-aware networks, which are much smaller than the current Internet, already offer dozens and in several cases over a hundred paths to a given destination, making it impractical to measure all path latencies to find the lowest latency path. Furthermore, for short flows, performing latency measurements may not provide benefits as the flow may finish before completing the measurements. To overcome these issues, we argue that endpoints should be provided with a latency estimate before sending any packets, enabling latency-aware path choice for the first packet sent. As we cannot predict the end-to-end latency due to dynamically changing queuing delays, we measure and disseminate the propagation latency, enabling novel use cases and solving concrete problems in current network protocols. We present the Global Latency Information Dissemination System (GLIDS), which is a step toward global latency transparency through the dissemination of propagation latency information.","sentences":["A recent advance in networking is the deployment of path-aware multipath network architectures, where network endpoints are given multiple network paths to send their data on.","In this work, we tackle the challenge of selecting paths for latency-sensitive applications.","Even today's path-aware networks, which are much smaller than the current Internet, already offer dozens and in several cases over a hundred paths to a given destination, making it impractical to measure all path latencies to find the lowest latency path.","Furthermore, for short flows, performing latency measurements may not provide benefits as the flow may finish before completing the measurements.","To overcome these issues, we argue that endpoints should be provided with a latency estimate before sending any packets, enabling latency-aware path choice for the first packet sent.","As we cannot predict the end-to-end latency due to dynamically changing queuing delays, we measure and disseminate the propagation latency, enabling novel use cases and solving concrete problems in current network protocols.","We present the Global Latency Information Dissemination System (GLIDS), which is a step toward global latency transparency through the dissemination of propagation latency information."],"url":"http://arxiv.org/abs/2405.04319v1","category":"cs.NI"}
{"created":"2024-05-07 13:37:43","title":"Enhancement of Chirality-Induced Spin Selectivity by Strong Electron Correlations","abstract":"Chirality-induced spin selectivity is a spin-splitting phenomenon from a helical structure with a considerably effective spin-orbit coupling. This unexpectedly large spin-splitting phenomenon has been experimentally observed in chiral organic molecules, which typically show a weak spin-orbit coupling. To understand this, we use the renormalized mean-field theory and Landauer-B\\\"{u}ttiker formulas to study the transport properties of single-stranded DNA in the presence of strong electron correlation. It shows a significant spin polarization of 46.5% near the Coulomb repulsion limit, which explains the extremely high spin polarization observed in experiments. Compared to systems without electron correlation, the averaged spin polarization in this case is 2 to 4 times greater across various system sizes. Furthermore, the parameter dependence of the spin polarization and the underlying Metal-Insulator transition are studied.","sentences":["Chirality-induced spin selectivity is a spin-splitting phenomenon from a helical structure with a considerably effective spin-orbit coupling.","This unexpectedly large spin-splitting phenomenon has been experimentally observed in chiral organic molecules, which typically show a weak spin-orbit coupling.","To understand this, we use the renormalized mean-field theory and Landauer-B\\\"{u}ttiker formulas to study the transport properties of single-stranded DNA in the presence of strong electron correlation.","It shows a significant spin polarization of 46.5% near the Coulomb repulsion limit, which explains the extremely high spin polarization observed in experiments.","Compared to systems without electron correlation, the averaged spin polarization in this case is 2 to 4 times greater across various system sizes.","Furthermore, the parameter dependence of the spin polarization and the underlying Metal-Insulator transition are studied."],"url":"http://arxiv.org/abs/2405.04316v1","category":"cond-mat.str-el"}
{"created":"2024-05-07 13:33:52","title":"Time-asymptotics of a heated string","abstract":"In the present paper, we study a model of a thermoelastic string that is initially heated. We classify all the possible asymptotic states when time tends to infinity of such a model. Actually, we show that whatever the initial data is, a heated string must converge to a flat, steady string with uniformly distributed heat. The latter distribution is calculated from the energy conservation. In order to obtain the result, we need to take a few steps. In the first two steps, time-independent bounds from above and from below (by a positive constant) of the temperature are obtained. This is done via the Moser-like iteration. The lower bound is obtained via the Moser iteration on the negative part of the logarithm of temperature. In the third step, we obtain a time-independent higher-order estimate, which yields compactness of a sequence of the values of the solution when time tends to infinity. Here, an estimate involving the Fisher information of temperature, together with a recent functional inequality from \\cite{CFHS} and an $L^2(L^2)$ estimate of the gradient of entropy, enable us to arrive at a tricky Gr\\\"{o}nwall type inequality. Finally, in the last steps, we define the dynamical system on a proper functional phase space and study its $\\omega$-limit set. To this end, we use, in particular, the quantitative version of the second principle of thermodynamics. Also, the entropy dissipation term and the bound of the entropy from below are useful when identifying the structure of the $\\omega$-limit set.","sentences":["In the present paper, we study a model of a thermoelastic string that is initially heated.","We classify all the possible asymptotic states when time tends to infinity of such a model.","Actually, we show that whatever the initial data is, a heated string must converge to a flat, steady string with uniformly distributed heat.","The latter distribution is calculated from the energy conservation.","In order to obtain the result, we need to take a few steps.","In the first two steps, time-independent bounds from above and from below (by a positive constant) of the temperature are obtained.","This is done via the Moser-like iteration.","The lower bound is obtained via the Moser iteration on the negative part of the logarithm of temperature.","In the third step, we obtain a time-independent higher-order estimate, which yields compactness of a sequence of the values of the solution when time tends to infinity.","Here, an estimate involving the Fisher information of temperature, together with a recent functional inequality from \\cite{CFHS} and an $L^2(L^2)$ estimate of the gradient of entropy, enable us to arrive at a tricky Gr\\\"{o}nwall type inequality.","Finally, in the last steps, we define the dynamical system on a proper functional phase space and study its $\\omega$-limit set.","To this end, we use, in particular, the quantitative version of the second principle of thermodynamics.","Also, the entropy dissipation term and the bound of the entropy from below are useful when identifying the structure of the $\\omega$-limit set."],"url":"http://arxiv.org/abs/2405.04310v1","category":"math.AP"}
{"created":"2024-05-07 13:33:50","title":"Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment and Spatially-variant Deformation Modeling","abstract":"Even though Non-rigid Structure-from-Motion (NRSfM) has been extensively studied and great progress has been made, there are still key challenges that hinder their broad real-world applications: 1) the inherent motion/rotation ambiguity requires either explicit camera motion recovery with extra constraint or complex Procrustean Alignment; 2) existing low-rank modeling of the global shape can over-penalize drastic deformations in the 3D shape sequence. This paper proposes to resolve the above issues from a spatial-temporal modeling perspective. First, we propose a novel Temporally-smooth Procrustean Alignment module that estimates 3D deforming shapes and adjusts the camera motion by aligning the 3D shape sequence consecutively. Our new alignment module remedies the requirement of complex reference 3D shape during alignment, which is more conductive to non-isotropic deformation modeling. Second, we propose a spatial-weighted approach to enforce the low-rank constraint adaptively at different locations to accommodate drastic spatially-variant deformation reconstruction better. Our modeling outperform existing low-rank based methods, and extensive experiments across different datasets validate the effectiveness of our method.","sentences":["Even though Non-rigid Structure-from-Motion (NRSfM) has been extensively studied and great progress has been made, there are still key challenges that hinder their broad real-world applications: 1) the inherent motion/rotation ambiguity requires either explicit camera motion recovery with extra constraint or complex Procrustean Alignment; 2) existing low-rank modeling of the global shape can over-penalize drastic deformations in the 3D shape sequence.","This paper proposes to resolve the above issues from a spatial-temporal modeling perspective.","First, we propose a novel Temporally-smooth Procrustean Alignment module that estimates 3D deforming shapes and adjusts the camera motion by aligning the 3D shape sequence consecutively.","Our new alignment module remedies the requirement of complex reference 3D shape during alignment, which is more conductive to non-isotropic deformation modeling.","Second, we propose a spatial-weighted approach to enforce the low-rank constraint adaptively at different locations to accommodate drastic spatially-variant deformation reconstruction better.","Our modeling outperform existing low-rank based methods, and extensive experiments across different datasets validate the effectiveness of our method."],"url":"http://arxiv.org/abs/2405.04309v1","category":"cs.CV"}
{"created":"2024-05-07 13:33:36","title":"Quality with Just Enough Diversity in Evolutionary Policy Search","abstract":"Evolution Strategies (ES) are effective gradient-free optimization methods that can be competitive with gradient-based approaches for policy search. ES only rely on the total episodic scores of solutions in their population, from which they estimate fitness gradients for their update with no access to true gradient information. However this makes them sensitive to deceptive fitness landscapes, and they tend to only explore one way to solve a problem. Quality-Diversity methods such as MAP-Elites introduced additional information with behavior descriptors (BD) to return a population of diverse solutions, which helps exploration but leads to a large part of the evaluation budget not being focused on finding the best performing solution. Here we show that behavior information can also be leveraged to find the best policy by identifying promising search areas which can then be efficiently explored with ES. We introduce the framework of Quality with Just Enough Diversity (JEDi) which learns the relationship between behavior and fitness to focus evaluations on solutions that matter. When trying to reach higher fitness values, JEDi outperforms both QD and ES methods on hard exploration tasks like mazes and on complex control problems with large policies.","sentences":["Evolution Strategies (ES) are effective gradient-free optimization methods that can be competitive with gradient-based approaches for policy search.","ES only rely on the total episodic scores of solutions in their population, from which they estimate fitness gradients for their update with no access to true gradient information.","However this makes them sensitive to deceptive fitness landscapes, and they tend to only explore one way to solve a problem.","Quality-Diversity methods such as MAP-Elites introduced additional information with behavior descriptors (BD) to return a population of diverse solutions, which helps exploration but leads to a large part of the evaluation budget not being focused on finding the best performing solution.","Here we show that behavior information can also be leveraged to find the best policy by identifying promising search areas which can then be efficiently explored with ES.","We introduce the framework of Quality with Just Enough Diversity (JEDi) which learns the relationship between behavior and fitness to focus evaluations on solutions that matter.","When trying to reach higher fitness values, JEDi outperforms both QD and ES methods on hard exploration tasks like mazes and on complex control problems with large policies."],"url":"http://arxiv.org/abs/2405.04308v1","category":"cs.NE"}
{"created":"2024-05-07 13:13:24","title":"Multiplicity results for critical fractional Ambrosetti-Prodi type system with nonlinearities interacting with the spectrum","abstract":"We investigated the existence of solutions for a class of Ambrosetti-Prodi type systems involving the fractional Laplacian operator and with nonlinearities reaching critical growth and interacting, in some sense, with the spectrum of the operator. The resonant case in $\\lambda_{k,s}$ for $k>1$ is also investigated.","sentences":["We investigated the existence of solutions for a class of Ambrosetti-Prodi type systems involving the fractional Laplacian operator and with nonlinearities reaching critical growth and interacting, in some sense, with the spectrum of the operator.","The resonant case in $\\lambda_{k,s}$ for $k>1$ is also investigated."],"url":"http://arxiv.org/abs/2405.04298v1","category":"math.AP"}
{"created":"2024-05-07 12:51:44","title":"PDCCH Scheduling via Maximum Independent Set","abstract":"In 5G, the Physical Downlink Control CHannel (PDCCH) carries crucial information enabling the User Equipment (UE) to connect in UL and DL. UEs are unaware of the frequency location at which PDCCH is encoded, hence they need to perform blind decoding over a limited set of possible candidates. We address the problem faced by the gNodeB of selecting PDCCH candidates for each UE to optimize data transmission. We formulate it as a Maximum Weighted Independent Set (MWIS) problem, that is known to be an NP-hard problem and cannot even be approximated. A solution method called Weight-to-Degree Ratio (WDR) Greedy emerges as a strong contender for practical implementations due to its favorable performance-to-complexity trade-off and theoretical performance guarantees.","sentences":["In 5G, the Physical Downlink Control CHannel (PDCCH) carries crucial information enabling the User Equipment (UE) to connect in UL and DL.","UEs are unaware of the frequency location at which PDCCH is encoded, hence they need to perform blind decoding over a limited set of possible candidates.","We address the problem faced by the gNodeB of selecting PDCCH candidates for each UE to optimize data transmission.","We formulate it as a Maximum Weighted Independent Set (MWIS) problem, that is known to be an NP-hard problem and cannot even be approximated.","A solution method called Weight-to-Degree Ratio (WDR) Greedy emerges as a strong contender for practical implementations due to its favorable performance-to-complexity trade-off and theoretical performance guarantees."],"url":"http://arxiv.org/abs/2405.04283v1","category":"cs.IT"}
{"created":"2024-05-07 12:49:25","title":"On a variant of Hilbert's 16th problem","abstract":"We study the number of limit cycles that a planar polynomial vector field can have as a function of its number $m$ of monomials. We prove that the number of limit cycles increases at least quadratically with $m$ and we provide good lower bounds for $m\\le10.$","sentences":["We study the number of limit cycles that a planar polynomial vector field can have as a function of its number $m$ of monomials.","We prove that the number of limit cycles increases at least quadratically with $m$ and we provide good lower bounds for $m\\le10.$"],"url":"http://arxiv.org/abs/2405.04281v1","category":"math.DS"}
{"created":"2024-05-07 12:46:50","title":"Task Presentation and Human Perception in Interactive Video Retrieval","abstract":"Interactive video retrieval is a cooperative process between humans and retrieval systems. Large-scale evaluation campaigns, however, often overlook human factors, such as the effects of perception, attention, and memory, when assessing media retrieval systems. Consequently, their setups fall short of emulating realistic retrieval scenarios. In this paper, we design novel task presentation modes based on concepts in media memorability, implement the pipelines necessary for processing target video segments, and build a custom experimental platform for the final evaluation. In order to study the effects of different task representation schemes, we conduct a large crowdsourced experiment. Our findings demonstrate that the way in which the target of a video retrieval task is presented has a substantial influence on the difficulty of the retrieval task and that individuals can successfully retrieve a target video segment despite reducing or even altering the provided hints, opening up a discussion around future evaluation protocols in the domain of interactive media retrieval.","sentences":["Interactive video retrieval is a cooperative process between humans and retrieval systems.","Large-scale evaluation campaigns, however, often overlook human factors, such as the effects of perception, attention, and memory, when assessing media retrieval systems.","Consequently, their setups fall short of emulating realistic retrieval scenarios.","In this paper, we design novel task presentation modes based on concepts in media memorability, implement the pipelines necessary for processing target video segments, and build a custom experimental platform for the final evaluation.","In order to study the effects of different task representation schemes, we conduct a large crowdsourced experiment.","Our findings demonstrate that the way in which the target of a video retrieval task is presented has a substantial influence on the difficulty of the retrieval task and that individuals can successfully retrieve a target video segment despite reducing or even altering the provided hints, opening up a discussion around future evaluation protocols in the domain of interactive media retrieval."],"url":"http://arxiv.org/abs/2405.04279v1","category":"cs.MM"}
{"created":"2024-05-07 12:46:45","title":"Uncertainty Quantification Metrics for Deep Regression","abstract":"When deploying deep neural networks on robots or other physical systems, the learned model should reliably quantify predictive uncertainty. A reliable uncertainty allows downstream modules to reason about the safety of its actions. In this work, we address metrics for evaluating such an uncertainty. Specifically, we focus on regression tasks, and investigate Area Under Sparsification Error (AUSE), Calibration Error, Spearman's Rank Correlation, and Negative Log-Likelihood (NLL). Using synthetic regression datasets, we look into how those metrics behave under four typical types of uncertainty, their stability regarding the size of the test set, and reveal their strengths and weaknesses. Our results indicate that Calibration Error is the most stable and interpretable metric, but AUSE and NLL also have their respective use cases. We discourage the usage of Spearman's Rank Correlation for evaluating uncertainties and recommend replacing it with AUSE.","sentences":["When deploying deep neural networks on robots or other physical systems, the learned model should reliably quantify predictive uncertainty.","A reliable uncertainty allows downstream modules to reason about the safety of its actions.","In this work, we address metrics for evaluating such an uncertainty.","Specifically, we focus on regression tasks, and investigate Area Under Sparsification Error (AUSE), Calibration Error, Spearman's Rank Correlation, and Negative Log-Likelihood (NLL).","Using synthetic regression datasets, we look into how those metrics behave under four typical types of uncertainty, their stability regarding the size of the test set, and reveal their strengths and weaknesses.","Our results indicate that Calibration Error is the most stable and interpretable metric, but AUSE and NLL also have their respective use cases.","We discourage the usage of Spearman's Rank Correlation for evaluating uncertainties and recommend replacing it with AUSE."],"url":"http://arxiv.org/abs/2405.04278v1","category":"cs.LG"}
{"created":"2024-05-07 12:44:14","title":"Alfv\u00e9n waves at low Magnetic Reynolds number","abstract":"This paper seeks whether Alfv\\'en waves (AW) can be produced in laboratory-scale liquid metal experiments, \\emph{i.e.) at low-magnetic Reynolds Number ($R\\!m$). AW are incompressible waves propagating along magnetic fields typically found geo and astrophysical systems. Until now, only faint linear waves have been experimentally produced in liquid metals because of the large magnetic dissipation they undergo when $R\\!m\\ll1$. Yet, controlling laboratory AW could emulate such far remote processes as anomalous heating in the solar corona, oscillations of the Earth inner core or turbulence in the solar wind. To answer this question, we force AW with an AC electric current in a liquid metal channel in a transverse magnetic field. We derive a wave-bearing extension of the usual low$-R\\!m$ MHD approximation to identify two linear regimes: The purely diffusive regime exists when $N_\\omega$, the ratio of the oscillation period to the timescale of diffusive two-dimensionalisation by the Lorentz force, is small. The propagative regime is governed by the ratio of the forcing period to the AW propagation timescale which, we call the Jameson number $J\\!a$ after Jameson (1964), JFM. In this regime, AW are dissipative and dispersive as they propagate more slowly where velocity gradients are higher. Both regimes are recovered in the FLOWCUBE experiment, in excellent agreement with the model up to $J\\!a \\lesssim 0.85$ but near the $J\\!a=1$ resonance, high amplitude waves become clearly nonlinear. Hence, in electrically driving AW, we were able to produce some of the propagative, diffusive and nonlinear processes of astro and geophysical AW.","sentences":["This paper seeks whether Alfv\\'en waves (AW) can be produced in laboratory-scale liquid metal experiments, \\emph{i.e.)","at low-magnetic Reynolds Number ($R\\!m$).","AW are incompressible waves propagating along magnetic fields typically found geo and astrophysical systems.","Until now, only faint linear waves have been experimentally produced in liquid metals because of the large magnetic dissipation they undergo when $R\\!m\\ll1$. Yet, controlling laboratory AW could emulate such far remote processes as anomalous heating in the solar corona, oscillations of the Earth inner core or turbulence in the solar wind.","To answer this question, we force AW with an AC electric current in a liquid metal channel in a transverse magnetic field.","We derive a wave-bearing extension of the usual low$-R\\!m$ MHD approximation to identify two linear regimes: The purely diffusive regime exists when $N_\\omega$, the ratio of the oscillation period to the timescale of diffusive two-dimensionalisation by the Lorentz force, is small.","The propagative regime is governed by the ratio of the forcing period to the AW propagation timescale which, we call the Jameson number $J\\!a$ after Jameson (1964), JFM.","In this regime, AW are dissipative and dispersive as they propagate more slowly where velocity gradients are higher.","Both regimes are recovered in the FLOWCUBE experiment, in excellent agreement with the model up to $J\\!a \\lesssim 0.85$ but near the $J\\!a=1$ resonance, high amplitude waves become clearly nonlinear.","Hence, in electrically driving AW, we were able to produce some of the propagative, diffusive and nonlinear processes of astro and geophysical AW."],"url":"http://arxiv.org/abs/2405.04276v1","category":"physics.flu-dyn"}
{"created":"2024-05-07 12:43:10","title":"Grey-box Recursive Parameter Identification of a Nonlinear Dynamic Model for Mineral Flotation","abstract":"This study presents a grey-box recursive identification technique to estimate key parameters in a mineral flotation process across two scenarios. The method is applied to a nonlinear physics-based dynamic model validated at a laboratory scale, allowing real-time updates of two model parameters, n and C, in response to changing conditions. The proposed approach effectively adapts to process variability and allows for continuous adjustments based on operational fluctuations, resulting in a significantly improved estimation of concentrate grade - one key performance indicator. In Scenario 1, parameters n and C achieved fit metrics of 97.99 and 96.86, respectively, with concentrate grade estimations improving from 75.1 to 98.69 using recursive identification. In Scenario 2, the fit metrics for n and C were 96.27 and 95.48, respectively, with the concentrate grade estimations increasing from 96.27 to 99.45 with recursive identification. The results demonstrate the effectiveness of the proposed grey-box recursive identification method in accurately estimating parameters and predicting concentrate grade in a mineral flotation process.","sentences":["This study presents a grey-box recursive identification technique to estimate key parameters in a mineral flotation process across two scenarios.","The method is applied to a nonlinear physics-based dynamic model validated at a laboratory scale, allowing real-time updates of two model parameters, n and C, in response to changing conditions.","The proposed approach effectively adapts to process variability and allows for continuous adjustments based on operational fluctuations, resulting in a significantly improved estimation of concentrate grade - one key performance indicator.","In Scenario 1, parameters n and C achieved fit metrics of 97.99 and 96.86, respectively, with concentrate grade estimations improving from 75.1 to 98.69 using recursive identification.","In Scenario 2, the fit metrics for n and C were 96.27 and 95.48, respectively, with the concentrate grade estimations increasing from 96.27 to 99.45 with recursive identification.","The results demonstrate the effectiveness of the proposed grey-box recursive identification method in accurately estimating parameters and predicting concentrate grade in a mineral flotation process."],"url":"http://arxiv.org/abs/2405.04275v1","category":"eess.SY"}
{"created":"2024-05-07 12:42:17","title":"The forest at EndEoR: The effect of Lyman Limit Systems on the End of Reionisation","abstract":"The final stages of cosmic reionisation (EndEoR) are expected to be strongly regulated by the residual neutral hydrogen in the already ionised regions of the Universe. Its presence limits the mean distance that ionising photons can travel and hence, the extent of the regions that sources of ionising photons can affect. The structures containing most of this residual neutral hydrogen are typically unresolved in large-scale simulations of reionisation. Here, we investigate and compare a range of approaches for including the effect of these small-scale absorbers, also known as Lyman limit systems (LLS), in such simulations. We evaluate the impact of these different approaches on the reionisation history, the evolution of the ultraviolet background, and its fluctuations. We also compare to observational results on the distribution of Lyman-$\\alpha$ opacity towards the EndEoR and the measured mean free path of ionising photons. We further consider their effect on the 21-cm power spectrum. We find that although each of the different approaches can match some of the observed probes of the final stages of reionisation, only the use of a redshift-dependent and position-dependent LLS model is able to reproduce all of them. We therefore recommend that large-scale reionisation simulations, which aim to describe both the state of the ionised and neutral intergalactic medium use such an approach, although the other, simpler approaches are applicable depending on the science goal of the simulation.","sentences":["The final stages of cosmic reionisation (EndEoR) are expected to be strongly regulated by the residual neutral hydrogen in the already ionised regions of the Universe.","Its presence limits the mean distance that ionising photons can travel and hence, the extent of the regions that sources of ionising photons can affect.","The structures containing most of this residual neutral hydrogen are typically unresolved in large-scale simulations of reionisation.","Here, we investigate and compare a range of approaches for including the effect of these small-scale absorbers, also known as Lyman limit systems (LLS), in such simulations.","We evaluate the impact of these different approaches on the reionisation history, the evolution of the ultraviolet background, and its fluctuations.","We also compare to observational results on the distribution of Lyman-$\\alpha$ opacity towards the EndEoR and the measured mean free path of ionising photons.","We further consider their effect on the 21-cm power spectrum.","We find that although each of the different approaches can match some of the observed probes of the final stages of reionisation, only the use of a redshift-dependent and position-dependent LLS model is able to reproduce all of them.","We therefore recommend that large-scale reionisation simulations, which aim to describe both the state of the ionised and neutral intergalactic medium use such an approach, although the other, simpler approaches are applicable depending on the science goal of the simulation."],"url":"http://arxiv.org/abs/2405.04273v1","category":"astro-ph.CO"}
{"created":"2024-05-07 12:39:04","title":"Very Long Baseline Array Observations of Parsec-scale Radio Emission in Dual Active Galactic Nuclei","abstract":"It is believed that dual active galactic nuclei (dual AGN) will form during galaxies merge. Studying dual-AGN emission can provide valuable insights into galaxy merging and evolution. To investigate parsec-scale radio emission properties, we observed eight radio components of four selected dual-AGN systems using the Very Long Baseline Array (VLBA) at 5 GHz in multiple-phase-center mode. Among them, two compact radio components, labeled J0051+0020B and J2300-0005A, were detected clearly on parsec scales for the first time. However, the radio emission of the other six components was resolved out in the high-resolution images. We provided the values or upper limits of the brightness temperature and radio emission power, and analyzed the emission origins in detail for each target. Based on their physical properties reported in this work and in the literature, we suggest the radio emission in J0051+0020B and J2300-0005A originates primarily from compact jets, while the other six sources show more complex emission mechanisms. In addition, our VLBA observations suggest the systematic X-ray deficit in our dual-AGN sample is likely attributed to the tidally induced effect and possible viewing angle effect.","sentences":["It is believed that dual active galactic nuclei (dual AGN) will form during galaxies merge.","Studying dual-AGN emission can provide valuable insights into galaxy merging and evolution.","To investigate parsec-scale radio emission properties, we observed eight radio components of four selected dual-AGN systems using the Very Long Baseline Array (VLBA) at 5 GHz in multiple-phase-center mode.","Among them, two compact radio components, labeled J0051+0020B and J2300-0005A, were detected clearly on parsec scales for the first time.","However, the radio emission of the other six components was resolved out in the high-resolution images.","We provided the values or upper limits of the brightness temperature and radio emission power, and analyzed the emission origins in detail for each target.","Based on their physical properties reported in this work and in the literature, we suggest the radio emission in J0051+0020B and J2300-0005A originates primarily from compact jets, while the other six sources show more complex emission mechanisms.","In addition, our VLBA observations suggest the systematic X-ray deficit in our dual-AGN sample is likely attributed to the tidally induced effect and possible viewing angle effect."],"url":"http://arxiv.org/abs/2405.04270v1","category":"astro-ph.GA"}
{"created":"2024-05-07 12:37:49","title":"Dynamics of an epidemic model with nonlocal di?usion and a free boundary","abstract":"An epidemic model, where the dispersal is approximated by nonlocal diffusion operator and spatial domain has one ?xed boundary and one free boundary, is considered in this paper. Firstly, using some elementary analysis instead of variational characterization, we show the existence and asymptotic behaviors of the principal eigenvalue of a cooperative system which can be used to characterize more epidemic models, not just ours. Then we study the existence, uniqueness and stability of a related steady state problem. Finally, we obtain a rather complete understanding for long time behaviors, spreading-vanishing dichotomy, criteria for spreading and vanishing, and spreading speed. Particularly, we prove that the asymptotic spreading speed of solution component (u; v) is equal to the spreading speed of free boundary which is ?nite if and only if a threshold condition holds for kernel functions.","sentences":["An epidemic model, where the dispersal is approximated by nonlocal diffusion operator and spatial domain has one ?xed boundary and one free boundary, is considered in this paper.","Firstly, using some elementary analysis instead of variational characterization, we show the existence and asymptotic behaviors of the principal eigenvalue of a cooperative system which can be used to characterize more epidemic models, not just ours.","Then we study the existence, uniqueness and stability of a related steady state problem.","Finally, we obtain a rather complete understanding for long time behaviors, spreading-vanishing dichotomy, criteria for spreading and vanishing, and spreading speed.","Particularly, we prove that the asymptotic spreading speed of solution component (u; v) is equal to the spreading speed of free boundary which is ?nite if and only if a threshold condition holds for kernel functions."],"url":"http://arxiv.org/abs/2405.04268v1","category":"math.AP"}
{"created":"2024-05-07 12:22:01","title":"\"Horseshoe\" Structures in the Debris Disks of Planet-Hosting Binary Stars","abstract":"The formation of a planetary system from the protoplanetary disk leads to destruction of the latter; however, a debris disk can remain in the form of asteroids and cometary material. The motion of planets can cause the formation of coorbital structures from the debris disk matter. Previous calculations have shown that such a ring-like structure is more stable if there is a binary star in the center of the system, as opposed to a single star. To analyze the properties of the coorbital structure, we have calculated a grid of models of binary star systems with a circumbinary planet moving in a planetesimal disk. The calculations are performed considering circular orbits of the stars and the planet; the mass and position of the planet, as well as the mass ratio of the stars, are varied. The analysis of the models shows that the width of the coorbital ring and its stability significantly depend on the initial parameters of the problem. Additionally, the empirical dependences of the width of the coorbital structure on the parameters of the system have been obtained, and the parameters of the models with the most stable coorbital structures have been determined. The results of the present study can be used for the search of planets around binary stars with debris disks.","sentences":["The formation of a planetary system from the protoplanetary disk leads to destruction of the latter; however, a debris disk can remain in the form of asteroids and cometary material.","The motion of planets can cause the formation of coorbital structures from the debris disk matter.","Previous calculations have shown that such a ring-like structure is more stable if there is a binary star in the center of the system, as opposed to a single star.","To analyze the properties of the coorbital structure, we have calculated a grid of models of binary star systems with a circumbinary planet moving in a planetesimal disk.","The calculations are performed considering circular orbits of the stars and the planet; the mass and position of the planet, as well as the mass ratio of the stars, are varied.","The analysis of the models shows that the width of the coorbital ring and its stability significantly depend on the initial parameters of the problem.","Additionally, the empirical dependences of the width of the coorbital structure on the parameters of the system have been obtained, and the parameters of the models with the most stable coorbital structures have been determined.","The results of the present study can be used for the search of planets around binary stars with debris disks."],"url":"http://arxiv.org/abs/2405.04262v1","category":"astro-ph.EP"}
{"created":"2024-05-07 12:17:58","title":"Insights from Basilisk: Are Open-Source EDA Tools Ready for a Multi-Million-Gate, Linux-Booting RV64 SoC Design?","abstract":"Designing complex, multi-million-gate application-specific integrated circuits requires robust and mature electronic design automation (EDA) tools. We describe our efforts in enhancing the open-source Yosys+Openroad EDA flow to implement Basilisk, a fully open-source, Linux-booting RV64GC system-on-chip (SoC) design. We analyze the quality-of-results impact of our enhancements to synthesis tools, interfaces between EDA tools, logic optimization scripts, and a newly open-sourced library of optimized arithmetic macro-operators. We also introduce a streamlined physical design flow with an improved power grid and cell placement integration. Our Basilisk SoC design was taped out in IHP's open 130 nm technology. It achieves an operating frequency of 77 MHz (51 logic levels) under typical conditions, a 2.3x improvement compared to the baseline open-source EDA flow, while also reducing logic area by 1.6x. Furthermore, tool runtime was reduced by 2.5x, and peak RAM usage decreased by 2.9x. Through collaboration with EDA tool developers and domain experts, Basilisk establishes solid \"proof of existence\" for a fully open-source EDA flow used in designing a competitive multi-million-gate digital SoC.","sentences":["Designing complex, multi-million-gate application-specific integrated circuits requires robust and mature electronic design automation (EDA) tools.","We describe our efforts in enhancing the open-source Yosys+Openroad EDA flow to implement Basilisk, a fully open-source, Linux-booting RV64GC system-on-chip (SoC) design.","We analyze the quality-of-results impact of our enhancements to synthesis tools, interfaces between EDA tools, logic optimization scripts, and a newly open-sourced library of optimized arithmetic macro-operators.","We also introduce a streamlined physical design flow with an improved power grid and cell placement integration.","Our Basilisk SoC design was taped out in IHP's open 130 nm technology.","It achieves an operating frequency of 77 MHz (51 logic levels) under typical conditions, a 2.3x improvement compared to the baseline open-source EDA flow, while also reducing logic area by 1.6x.","Furthermore, tool runtime was reduced by 2.5x, and peak RAM usage decreased by 2.9x.","Through collaboration with EDA tool developers and domain experts, Basilisk establishes solid \"proof of existence\" for a fully open-source EDA flow used in designing a competitive multi-million-gate digital SoC."],"url":"http://arxiv.org/abs/2405.04257v1","category":"cs.AR"}
{"created":"2024-05-07 12:17:22","title":"Fermi surface of the chiral topological semimetal CoSi","abstract":"We report a study of the Fermi surface of the chiral semimetal CoSi and its relationship to a network of multifold topological crossing points,Weyl points, and topological nodal planes in the electronic band structure. Combining quantum oscillations in the Hall resistivity, magnetization, and torque magnetization with ab initio electronic structure calculations, we identify two groups of Fermi-surface sheets, one centered at the R point and the other centered at the $\\Gamma$ point. The presence of topological nodal planes at the Brillouin zone boundary enforces topological protectorates on the Fermi-surface sheets centered at the R point. In addition, Weyl points exist close to the Fermi-surface sheets centered at the R and the $\\Gamma$ points. In contrast, topological crossing points at the R point and the $\\Gamma$ point, which have been advertised to feature exceptionally large Chern numbers, are located at a larger distance to the Fermi level. Representing a unique example in which the multitude of topological band crossings has been shown to form a complex network, our observations in CoSi highlight the need for detailed numerical calculations of the Berry curvature at the Fermi level, regardless of the putative existence and the possible character of topological band crossings in the band structure.","sentences":["We report a study of the Fermi surface of the chiral semimetal CoSi and its relationship to a network of multifold topological crossing points,Weyl points, and topological nodal planes in the electronic band structure.","Combining quantum oscillations in the Hall resistivity, magnetization, and torque magnetization with ab initio electronic structure calculations, we identify two groups of Fermi-surface sheets, one centered at the R point and the other centered at the $\\Gamma$ point.","The presence of topological nodal planes at the Brillouin zone boundary enforces topological protectorates on the Fermi-surface sheets centered at the R point.","In addition, Weyl points exist close to the Fermi-surface sheets centered at the R and the $\\Gamma$ points.","In contrast, topological crossing points at the R point and the $\\Gamma$ point, which have been advertised to feature exceptionally large Chern numbers, are located at a larger distance to the Fermi level.","Representing a unique example in which the multitude of topological band crossings has been shown to form a complex network, our observations in CoSi highlight the need for detailed numerical calculations of the Berry curvature at the Fermi level, regardless of the putative existence and the possible character of topological band crossings in the band structure."],"url":"http://arxiv.org/abs/2405.04256v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 12:14:39","title":"Fermat Number Transform Based Chromatic Dispersion Compensation and Adaptive Equalization Algorithm","abstract":"By introducing the Fermat number transform into chromatic dispersion compensation and adaptive equalization, the computational complexity has been reduced by 68% compared with the con?ventional implementation. Experimental results validate its transmission performance with only 0.8 dB receiver sensitivity penalty in a 75 km-40 GBaud-PDM-16QAM system.","sentences":["By introducing the Fermat number transform into chromatic dispersion compensation and adaptive equalization, the computational complexity has been reduced by 68% compared with the con?ventional implementation.","Experimental results validate its transmission performance with only 0.8 dB receiver sensitivity penalty in a 75 km-40 GBaud-PDM-16QAM system."],"url":"http://arxiv.org/abs/2405.04253v1","category":"eess.SP"}
{"created":"2024-05-07 12:09:22","title":"Weighted Least-Squares PARSIM","abstract":"Subspace identification methods (SIMs) have proven very powerful for estimating linear state-space models. To overcome the deficiencies of classical SIMs, a significant number of algorithms has appeared over the last two decades, where most of them involve a common intermediate step, that is to estimate the range space of the extended observability matrix. In this contribution, an optimized version of the parallel and parsimonious SIM (PARSIM), PARSIM\\textsubscript{opt}, is proposed by using weighted least-squares. It not only inherits all the benefits of PARSIM but also attains the best linear unbiased estimator for the above intermediate step. Furthermore, inspired by SIMs based on the predictor form, consistent estimates of the optimal weighting matrix for weighted least-squares are derived. Essential similarities, differences and simulated comparisons of some key SIMs related to our method are also presented.","sentences":["Subspace identification methods (SIMs) have proven very powerful for estimating linear state-space models.","To overcome the deficiencies of classical SIMs, a significant number of algorithms has appeared over the last two decades, where most of them involve a common intermediate step, that is to estimate the range space of the extended observability matrix.","In this contribution, an optimized version of the parallel and parsimonious SIM (PARSIM), PARSIM\\textsubscript{opt}, is proposed by using weighted least-squares.","It not only inherits all the benefits of PARSIM but also attains the best linear unbiased estimator for the above intermediate step.","Furthermore, inspired by SIMs based on the predictor form, consistent estimates of the optimal weighting matrix for weighted least-squares are derived.","Essential similarities, differences and simulated comparisons of some key SIMs related to our method are also presented."],"url":"http://arxiv.org/abs/2405.04250v1","category":"eess.SY"}
{"created":"2024-05-07 12:04:07","title":"Quantum-enhanced Markov Chain Monte Carlo for systems larger than your Quantum Computer","abstract":"Quantum computers theoretically promise computational advantage in many tasks, but it is much less clear how such advantage can be maintained when using existing and near-term hardware that has limitations in the number and quality of its qubits. One promising application was proposed in Layden et al [Nature 619, 282-287 (2023)] where a method to reduce the thermalisation time required when sampling from hard probability distribution was introduced as a Quantum-enhanced Markov Chain Monte Carlo (QeMCMC) approach. In [Nature 619, 282-287 (2023)] the size of the required quantum computer scales linearly with the problem, putting limitations on the sizes of systems that one can consider. In this work we introduce a framework to coarse grain the algorithm in such a way that the quantum computation can be performed using, multiple times, smaller quantum computers and we term the method the Coarse Grained Quantum-enhanced Markov Chain Monte Carlo (CGQeMCMC). Example strategies within this framework are put to the test, with the quantum speedup of [Nature 619, 282-287 (2023)] persisting while using only $\\sqrt{n}$ simulated qubits where $n$ is the number of qubits required in the original QeMCMC -- a quadratic reduction in resources. The coarse graining framework has the potential to be practically applicable in the near term as it requires very few qubits to approach classically intractable problem instances, here only 6 simulated qubits suffice to gain advantage compared to standard classical approaches when investigating the magnetisation of a 36 spin system. Our method is also adjustable to quantum hardware specifications, and it appears that it can be easily combined with other techniques both classical and quantum.","sentences":["Quantum computers theoretically promise computational advantage in many tasks, but it is much less clear how such advantage can be maintained when using existing and near-term hardware that has limitations in the number and quality of its qubits.","One promising application was proposed in Layden et al","[Nature 619, 282-287 (2023)] where a method to reduce the thermalisation time required when sampling from hard probability distribution was introduced as a Quantum-enhanced Markov Chain Monte Carlo (QeMCMC) approach.","In [Nature 619, 282-287 (2023)]","the size of the required quantum computer scales linearly with the problem, putting limitations on the sizes of systems that one can consider.","In this work we introduce a framework to coarse grain the algorithm in such a way that the quantum computation can be performed using, multiple times, smaller quantum computers and we term the method the Coarse Grained Quantum-enhanced Markov Chain Monte Carlo (CGQeMCMC).","Example strategies within this framework are put to the test, with the quantum speedup of [Nature 619, 282-287 (2023)] persisting while using only $\\sqrt{n}$ simulated qubits where $n$ is the number of qubits required in the original QeMCMC -- a quadratic reduction in resources.","The coarse graining framework has the potential to be practically applicable in the near term as it requires very few qubits to approach classically intractable problem instances, here only 6 simulated qubits suffice to gain advantage compared to standard classical approaches when investigating the magnetisation of a 36 spin system.","Our method is also adjustable to quantum hardware specifications, and it appears that it can be easily combined with other techniques both classical and quantum."],"url":"http://arxiv.org/abs/2405.04247v1","category":"quant-ph"}
{"created":"2024-05-07 12:03:22","title":"Dataset and Models for Item Recommendation Using Multi-Modal User Interactions","abstract":"While recommender systems with multi-modal item representations (image, audio, and text), have been widely explored, learning recommendations from multi-modal user interactions (e.g., clicks and speech) remains an open problem. We study the case of multi-modal user interactions in a setting where users engage with a service provider through multiple channels (website and call center). In such cases, incomplete modalities naturally occur, since not all users interact through all the available channels. To address these challenges, we publish a real-world dataset that allows progress in this under-researched area. We further present and benchmark various methods for leveraging multi-modal user interactions for item recommendations, and propose a novel approach that specifically deals with missing modalities by mapping user interactions to a common feature space. Our analysis reveals important interactions between the different modalities and that a frequently occurring modality can enhance learning from a less frequent one.","sentences":["While recommender systems with multi-modal item representations (image, audio, and text), have been widely explored, learning recommendations from multi-modal user interactions (e.g., clicks and speech) remains an open problem.","We study the case of multi-modal user interactions in a setting where users engage with a service provider through multiple channels (website and call center).","In such cases, incomplete modalities naturally occur, since not all users interact through all the available channels.","To address these challenges, we publish a real-world dataset that allows progress in this under-researched area.","We further present and benchmark various methods for leveraging multi-modal user interactions for item recommendations, and propose a novel approach that specifically deals with missing modalities by mapping user interactions to a common feature space.","Our analysis reveals important interactions between the different modalities and that a frequently occurring modality can enhance learning from a less frequent one."],"url":"http://arxiv.org/abs/2405.04246v1","category":"cs.IR"}
{"created":"2024-05-07 11:54:43","title":"QR factorization of ill-conditioned tall-and-skinny matrices on distributed-memory systems","abstract":"In this paper we present a novel algorithm developed for computing the QR factorisation of extremely ill-conditioned tall-and-skinny matrices on distributed memory systems. The algorithm is based on the communication-avoiding CholeskyQR2 algorithm and its block Gram-Schmidt variant. The latter improves the numerical stability of the CholeskyQR2 algorithm and significantly reduces the loss of orthogonality even for matrices with condition numbers up to $10^{15}$. Currently, there is no distributed GPU version of this algorithm available in the literature which prevents the application of this method to very large matrices. In our work we provide a distributed implementation of this algorithm and also introduce a modified version that improves the performance, especially in the case of extremely ill-conditioned matrices. The main innovation of our approach lies in the interleaving of the CholeskyQR steps with the Gram-Schmidt orthogonalisation, which ensures that update steps are performed with fully orthogonalised panels. The obtained orthogonality and numerical stability of our modified algorithm is equivalent to CholeskyQR2 with Gram-Schmidt and other state-of-the-art methods. Weak scaling tests performed with our test matrices show significant performance improvements. In particular, our algorithm outperforms state-of-the-art Householder-based QR factorisation algorithms available in ScaLAPACK by a factor of $6$ on CPU-only systems and up to $80\\times$ on GPU-based systems with distributed memory.","sentences":["In this paper we present a novel algorithm developed for computing the QR factorisation of extremely ill-conditioned tall-and-skinny matrices on distributed memory systems.","The algorithm is based on the communication-avoiding CholeskyQR2 algorithm and its block Gram-Schmidt variant.","The latter improves the numerical stability of the CholeskyQR2 algorithm and significantly reduces the loss of orthogonality even for matrices with condition numbers up to $10^{15}$. Currently, there is no distributed GPU version of this algorithm available in the literature which prevents the application of this method to very large matrices.","In our work we provide a distributed implementation of this algorithm and also introduce a modified version that improves the performance, especially in the case of extremely ill-conditioned matrices.","The main innovation of our approach lies in the interleaving of the CholeskyQR steps with the Gram-Schmidt orthogonalisation, which ensures that update steps are performed with fully orthogonalised panels.","The obtained orthogonality and numerical stability of our modified algorithm is equivalent to CholeskyQR2 with Gram-Schmidt and other state-of-the-art methods.","Weak scaling tests performed with our test matrices show significant performance improvements.","In particular, our algorithm outperforms state-of-the-art Householder-based QR factorisation algorithms available in ScaLAPACK by a factor of $6$ on CPU-only systems and up to $80\\times$ on GPU-based systems with distributed memory."],"url":"http://arxiv.org/abs/2405.04237v1","category":"cs.DC"}
{"created":"2024-05-07 11:54:32","title":"Semantic API Alignment: Linking High-level User Goals to APIs","abstract":"Large Language Models (LLMs) are becoming key in automating and assisting various software development tasks, including text-based tasks in requirements engineering but also in coding. Typically, these models are used to automate small portions of existing tasks, but we present a broader vision to span multiple steps from requirements engineering to implementation using existing libraries. This approach, which we call Semantic API Alignment (SEAL), aims to bridge the gap between a user's high-level goals and the specific functions of one or more APIs.   In this position paper, we propose a system architecture where a set of LLM-powered ``agents'' match such high-level objectives with appropriate API calls. This system could facilitate automated programming by finding matching links or, alternatively, explaining mismatches to guide manual intervention or further development.   As an initial pilot, our paper demonstrates this concept by applying LLMs to Goal-Oriented Requirements Engineering (GORE), via sub-goal analysis, for aligning with REST API specifications, specifically through a case study involving a GitHub statistics API. We discuss the potential of our approach to enhance complex tasks in software development and requirements engineering and outline future directions for research.","sentences":["Large Language Models (LLMs) are becoming key in automating and assisting various software development tasks, including text-based tasks in requirements engineering but also in coding.","Typically, these models are used to automate small portions of existing tasks, but we present a broader vision to span multiple steps from requirements engineering to implementation using existing libraries.","This approach, which we call Semantic API Alignment (SEAL), aims to bridge the gap between a user's high-level goals and the specific functions of one or more APIs.   ","In this position paper, we propose a system architecture where a set of LLM-powered ``agents'' match such high-level objectives with appropriate API calls.","This system could facilitate automated programming by finding matching links or, alternatively, explaining mismatches to guide manual intervention or further development.   ","As an initial pilot, our paper demonstrates this concept by applying LLMs to Goal-Oriented Requirements Engineering (GORE), via sub-goal analysis, for aligning with REST API specifications, specifically through a case study involving a GitHub statistics API.","We discuss the potential of our approach to enhance complex tasks in software development and requirements engineering and outline future directions for research."],"url":"http://arxiv.org/abs/2405.04236v1","category":"cs.SE"}
{"created":"2024-05-07 11:49:15","title":"Optical Photon Emission in Extended Airshowers -- Hybrid computing in the context of CORSIKA 8","abstract":"With the motivation to improve experimental gains and precision, established astroparticle experiments are currently undergoing massive upgrades. In addition, several new experiments are being built or planned. With the resulting gain in observational quality, the amount and accuracy of simulated data required for the analysis is also rising. In order to meet the increasing requirements and complexity due to the experiments' growth and to provide a unified software ecosystem, it was decided to re-develop the de facto standard extensive air shower simulation CORSIKA completely in C++ based on the original Fortran code. Since one of the largest runtime consumers is the propagation of millions of optical Cherenkov and fluorescence photons, and many experiments are starting to use them for measurements, it was decided to develop hardware-accelerated code to speed up the simulation. Specific methods have been developed to propagate photons on deep learning acceleration hardware similar to classical GPUs to take additional advantage of the current and future growth of the deep learning sector. In particular, Nvidia accelerators were tested.","sentences":["With the motivation to improve experimental gains and precision, established astroparticle experiments are currently undergoing massive upgrades.","In addition, several new experiments are being built or planned.","With the resulting gain in observational quality, the amount and accuracy of simulated data required for the analysis is also rising.","In order to meet the increasing requirements and complexity due to the experiments' growth and to provide a unified software ecosystem, it was decided to re-develop the de facto standard extensive air shower simulation CORSIKA completely in C++ based on the original Fortran code.","Since one of the largest runtime consumers is the propagation of millions of optical Cherenkov and fluorescence photons, and many experiments are starting to use them for measurements, it was decided to develop hardware-accelerated code to speed up the simulation.","Specific methods have been developed to propagate photons on deep learning acceleration hardware similar to classical GPUs to take additional advantage of the current and future growth of the deep learning sector.","In particular, Nvidia accelerators were tested."],"url":"http://arxiv.org/abs/2405.04229v1","category":"astro-ph.IM"}
{"created":"2024-05-07 11:48:52","title":"How the presence of a giant planet affects the outcome of terrestrial planet formation simulations","abstract":"The architecture and masses of planetary systems in the habitable zone could be strongly influenced by outer giant planets, if present. We investigate here the impact of outer giants on terrestrial planet formation, under the assumption that the final assembly of the planetary system is set by a giant impact phase. Utilizing a state-of-the-art N-body simulation software, GENGA, we interpret how the late stage of terrestrial planet formation results in diversity within planetary systems. We design two global model setups: in the first we place a gas giant on the outer side of planetesimals and embryos disk, while the other only has planetesimals and embryos but no giant. For the model including the outer giant, we study the effect of different giant initial masses, in the range 1.0-3.0 Jupiter mass, and orbital radii, in the range 2.0-5.8 AU.We also study the influence of different initial positions of planetesimals and embryos on the results. Our N-body simulation time is approximately 50 Myr. The results show that the existence of outer giant will promote the interaction between planetesimals and embryos, making the orbits of the formed terrestrial planets more compact, but placing the giant planet too close to the planetesimals and embryos disk suppresses the formation of massive rocky planets. In addition, under the classical theory, where planetary embryos and planetesimals collide to form terrestrial planets, our results show that the presence of a giant planet actually decreases the gap complexity of the inner planetary system.","sentences":["The architecture and masses of planetary systems in the habitable zone could be strongly influenced by outer giant planets, if present.","We investigate here the impact of outer giants on terrestrial planet formation, under the assumption that the final assembly of the planetary system is set by a giant impact phase.","Utilizing a state-of-the-art N-body simulation software, GENGA, we interpret how the late stage of terrestrial planet formation results in diversity within planetary systems.","We design two global model setups: in the first we place a gas giant on the outer side of planetesimals and embryos disk, while the other only has planetesimals and embryos but no giant.","For the model including the outer giant, we study the effect of different giant initial masses, in the range 1.0-3.0 Jupiter mass, and orbital radii, in the range 2.0-5.8 AU.We also study the influence of different initial positions of planetesimals and embryos on the results.","Our N-body simulation time is approximately 50 Myr.","The results show that the existence of outer giant will promote the interaction between planetesimals and embryos, making the orbits of the formed terrestrial planets more compact, but placing the giant planet too close to the planetesimals and embryos disk suppresses the formation of massive rocky planets.","In addition, under the classical theory, where planetary embryos and planetesimals collide to form terrestrial planets, our results show that the presence of a giant planet actually decreases the gap complexity of the inner planetary system."],"url":"http://arxiv.org/abs/2405.04228v1","category":"astro-ph.EP"}
{"created":"2024-05-07 11:46:36","title":"NEST: Neural Estimation by Sequential Testing","abstract":"Adaptive psychophysical procedures aim to increase the efficiency and reliability of measurements. With increasing stimulus and experiment complexity in the last decade, estimating multi-dimensional psychometric functions has become a challenging task for adaptive procedures. If the experimenter has limited information about the underlying psychometric function, it is not possible to use parametric techniques developed for the multi-dimensional stimulus space. Although there are non-parametric approaches that use Gaussian process methods and specific hand-crafted acquisition functions, their performance is sensitive to proper selection of the kernel function, which is not always straightforward. In this work, we use a neural network as the psychometric function estimator and introduce a novel acquisition function for stimulus selection. We thoroughly benchmark our technique both using simulations and by conducting psychovisual experiments under realistic conditions. We show that our method outperforms the state of the art without the need to select a kernel function and significantly reduces the experiment duration.","sentences":["Adaptive psychophysical procedures aim to increase the efficiency and reliability of measurements.","With increasing stimulus and experiment complexity in the last decade, estimating multi-dimensional psychometric functions has become a challenging task for adaptive procedures.","If the experimenter has limited information about the underlying psychometric function, it is not possible to use parametric techniques developed for the multi-dimensional stimulus space.","Although there are non-parametric approaches that use Gaussian process methods and specific hand-crafted acquisition functions, their performance is sensitive to proper selection of the kernel function, which is not always straightforward.","In this work, we use a neural network as the psychometric function estimator and introduce a novel acquisition function for stimulus selection.","We thoroughly benchmark our technique both using simulations and by conducting psychovisual experiments under realistic conditions.","We show that our method outperforms the state of the art without the need to select a kernel function and significantly reduces the experiment duration."],"url":"http://arxiv.org/abs/2405.04226v1","category":"stat.ME"}
{"created":"2024-05-07 11:34:30","title":"Non-escape of mass for arithmetic quantum limits on hyperbolic $4$-manifolds","abstract":"We make progress on the quantum unique ergodicity (QUE) conjecture for Hecke-Maass forms on a congruence quotient of hyperbolic $4$-space, eliminating the possibility of \"escape of mass\" for these forms.","sentences":["We make progress on the quantum unique ergodicity (QUE) conjecture for Hecke-Maass forms on a congruence quotient of hyperbolic $4$-space, eliminating the possibility of \"escape of mass\" for these forms."],"url":"http://arxiv.org/abs/2405.04221v1","category":"math.NT"}
{"created":"2024-05-07 11:32:37","title":"Deep Reinforcement Learning for Multi-User RF Charging with Non-linear Energy Harvesters","abstract":"Radio frequency (RF) wireless power transfer (WPT) is a promising technology for sustainable support of massive Internet of Things (IoT). However, RF-WPT systems are characterized by low efficiency due to channel attenuation, which can be mitigated by precoders that adjust the transmission directivity. This work considers a multi-antenna RF-WPT system with multiple non-linear energy harvesting (EH) nodes with energy demands changing over discrete time slots. This leads to the charging scheduling problem, which involves choosing the precoders at each slot to minimize the total energy consumption and meet the EH requirements. We model the problem as a Markov decision process and propose a solution relying on a low-complexity beamforming and deep deterministic policy gradient (DDPG). The results show that the proposed beamforming achieves near-optimal performance with low computational complexity, and the DDPG-based approach converges with the number of episodes and reduces the system's power consumption, while the outage probability and the power consumption increase with the number of devices.","sentences":["Radio frequency (RF) wireless power transfer (WPT) is a promising technology for sustainable support of massive Internet of Things (IoT).","However, RF-WPT systems are characterized by low efficiency due to channel attenuation, which can be mitigated by precoders that adjust the transmission directivity.","This work considers a multi-antenna RF-WPT system with multiple non-linear energy harvesting (EH) nodes with energy demands changing over discrete time slots.","This leads to the charging scheduling problem, which involves choosing the precoders at each slot to minimize the total energy consumption and meet the EH requirements.","We model the problem as a Markov decision process and propose a solution relying on a low-complexity beamforming and deep deterministic policy gradient (DDPG).","The results show that the proposed beamforming achieves near-optimal performance with low computational complexity, and the DDPG-based approach converges with the number of episodes and reduces the system's power consumption, while the outage probability and the power consumption increase with the number of devices."],"url":"http://arxiv.org/abs/2405.04218v1","category":"eess.SP"}
{"created":"2024-05-07 11:31:40","title":"CAVITY, Calar Alto Void Integral-field Treasury surveY and project extension","abstract":"We have learnt in the last decades that the majority of galaxies belong to high density regions interconnected in a sponge-like fashion. This large-scale structure is characterised by clusters, filaments, walls, where most galaxies concentrate, but also under-dense regions, called voids. The void regions and the galaxies within represent an ideal place for the study of galaxy formation and evolution as they are largely unaffected by the complex physical processes that transform galaxies in high-density environments. These void galaxies can hold the key as well to answer current challenges to the $\\Lambda$CDM paradigm. The Calar Alto Void Integral-field Treasury surveY (CAVITY) is a Legacy project approved by the Calar Alto Observatory to obtain spatially resolved spectroscopic information of $\\sim300$ void galaxies in the Local Universe (0.005 < z < 0.050) covering from -17.0 to -21.5 in $\\rm r$ band absolute magnitude. It officially started in January 2021 and has been awarded 110 useful dark observing nights at the 3.5 m telescope using the PMAS spectrograph. Complementary follow-up projects including deep optical imaging, integrated, as well as resolved CO data, and integrated HI spectra, have joint the PMAS observations and naturally complete the scientific aim of characterising galaxies in cosmic voids. The extension data has been denominated CAVITY+. The data will be available to the whole community in different data releases, the first of which is planned for July 2024, and it will provide the community with PMAS data cubes for around 100 void galaxies through a user friendly, and well documented, database platform. We present here the survey, sample selection, data reduction, quality control schemes, science goals, and some examples of the scientific power of the CAVITY and CAVITY+ data.","sentences":["We have learnt in the last decades that the majority of galaxies belong to high density regions interconnected in a sponge-like fashion.","This large-scale structure is characterised by clusters, filaments, walls, where most galaxies concentrate, but also under-dense regions, called voids.","The void regions and the galaxies within represent an ideal place for the study of galaxy formation and evolution as they are largely unaffected by the complex physical processes that transform galaxies in high-density environments.","These void galaxies can hold the key as well to answer current challenges to the $\\Lambda$CDM paradigm.","The Calar Alto Void Integral-field Treasury surveY (CAVITY) is a Legacy project approved by the Calar Alto Observatory to obtain spatially resolved spectroscopic information of $\\sim300$ void galaxies in the Local Universe (0.005 < z < 0.050) covering from -17.0 to -21.5 in $\\rm r$ band absolute magnitude.","It officially started in January 2021 and has been awarded 110 useful dark observing nights at the 3.5 m telescope using the PMAS spectrograph.","Complementary follow-up projects including deep optical imaging, integrated, as well as resolved CO data, and integrated HI spectra, have joint the PMAS observations and naturally complete the scientific aim of characterising galaxies in cosmic voids.","The extension data has been denominated CAVITY+.","The data will be available to the whole community in different data releases, the first of which is planned for July 2024, and it will provide the community with PMAS data cubes for around 100 void galaxies through a user friendly, and well documented, database platform.","We present here the survey, sample selection, data reduction, quality control schemes, science goals, and some examples of the scientific power of the CAVITY and","CAVITY+ data."],"url":"http://arxiv.org/abs/2405.04217v1","category":"astro-ph.GA"}
{"created":"2024-05-07 11:27:04","title":"On the Gelfand Problem and Viscosity Matrices for Two-Dimensional Hyperbolic Systems of Conservation Laws","abstract":"We present counter-intuitive examples of a viscous regularizations of a two-dimensional strictly hyperbolic system of conservation laws. The regularizations are obtained using two different viscosity matrices. While for both of the constructed ``viscous'' systems waves propagating in either $x$- or $y$-directions are stable, oblique waves may be linearly unstable. Numerical simulations fully corroborate these analytical results. To the best of our knowledge, this is the first nontrivial result related to the multidimensional Gelfand problem. Our conjectures provide direct answer to Gelfand's problem both in one- and multi-dimensional cases.","sentences":["We present counter-intuitive examples of a viscous regularizations of a two-dimensional strictly hyperbolic system of conservation laws.","The regularizations are obtained using two different viscosity matrices.","While for both of the constructed ``viscous'' systems waves propagating in either $x$- or $y$-directions are stable, oblique waves may be linearly unstable.","Numerical simulations fully corroborate these analytical results.","To the best of our knowledge, this is the first nontrivial result related to the multidimensional Gelfand problem.","Our conjectures provide direct answer to Gelfand's problem both in one- and multi-dimensional cases."],"url":"http://arxiv.org/abs/2405.04214v1","category":"math.NA"}
{"created":"2024-05-07 11:24:37","title":"Breast Histopathology Image Retrieval by Attention-based Adversarially Regularized Variational Graph Autoencoder with Contrastive Learning-Based Feature Extraction","abstract":"Breast cancer is a significant global health concern, particularly for women. Early detection and appropriate treatment are crucial in mitigating its impact, with histopathology examinations playing a vital role in swift diagnosis. However, these examinations often require a substantial workforce and experienced medical experts for proper recognition and cancer grading. Automated image retrieval systems have the potential to assist pathologists in identifying cancerous tissues, thereby accelerating the diagnostic process. Nevertheless, due to considerable variability among the tissue and cell patterns in histological images, proposing an accurate image retrieval model is very challenging.   This work introduces a novel attention-based adversarially regularized variational graph autoencoder model for breast histological image retrieval. Additionally, we incorporated cluster-guided contrastive learning as the graph feature extractor to boost the retrieval performance. We evaluated the proposed model's performance on two publicly available datasets of breast cancer histological images and achieved superior or very competitive retrieval performance, with average mAP scores of 96.5% for the BreakHis dataset and 94.7% for the BACH dataset, and mVP scores of 91.9% and 91.3%, respectively.   Our proposed retrieval model has the potential to be used in clinical settings to enhance diagnostic performance and ultimately benefit patients.","sentences":["Breast cancer is a significant global health concern, particularly for women.","Early detection and appropriate treatment are crucial in mitigating its impact, with histopathology examinations playing a vital role in swift diagnosis.","However, these examinations often require a substantial workforce and experienced medical experts for proper recognition and cancer grading.","Automated image retrieval systems have the potential to assist pathologists in identifying cancerous tissues, thereby accelerating the diagnostic process.","Nevertheless, due to considerable variability among the tissue and cell patterns in histological images, proposing an accurate image retrieval model is very challenging.   ","This work introduces a novel attention-based adversarially regularized variational graph autoencoder model for breast histological image retrieval.","Additionally, we incorporated cluster-guided contrastive learning as the graph feature extractor to boost the retrieval performance.","We evaluated the proposed model's performance on two publicly available datasets of breast cancer histological images and achieved superior or very competitive retrieval performance, with average mAP scores of 96.5% for the BreakHis dataset and 94.7% for the BACH dataset, and mVP scores of 91.9% and 91.3%, respectively.   ","Our proposed retrieval model has the potential to be used in clinical settings to enhance diagnostic performance and ultimately benefit patients."],"url":"http://arxiv.org/abs/2405.04211v1","category":"cs.CV"}
{"created":"2024-05-07 11:18:37","title":"Reconciling light nuclei and nuclear matter: relativistic $ab\\ initio$ calculations","abstract":"It has been a long-standing challenge to accurately predict the properties of light nuclei and nuclear matter simultaneously in nuclear $ab\\ initio$ calculations. In this Letter, we develop the relativistic quantum Monte Carlo methods for the nuclear $ab\\ initio$ problem, and calculate the ground-state energies of $A\\leq4$ nuclei using the two-nucleon Bonn force with an unprecedented high accuracy. For $A=3,4$ nuclei, the present relativistic results significantly outperforms the nonrelativistic results with only two-nucleon forces. Combining the present results for light nuclei and the previous results for nuclear matter with the same Bonn force, a correlation between the properties of light $A\\leq4$ nuclei and the nuclear saturation is revealed, and both systems are well described simultaneously, even without introducing three-nucleon forces. This provides a quantitative understanding of the connection between the light nuclei and nuclear matter saturation properties, which has been an outstanding problem in nuclear $ab\\ initio$ calculations for decades.","sentences":["It has been a long-standing challenge to accurately predict the properties of light nuclei and nuclear matter simultaneously in nuclear $ab\\ initio$ calculations.","In this Letter, we develop the relativistic quantum Monte Carlo methods for the nuclear $ab\\ initio$ problem, and calculate the ground-state energies of $A\\leq4$ nuclei using the two-nucleon Bonn force with an unprecedented high accuracy.","For $A=3,4$ nuclei, the present relativistic results significantly outperforms the nonrelativistic results with only two-nucleon forces.","Combining the present results for light nuclei and the previous results for nuclear matter with the same Bonn force, a correlation between the properties of light $A\\leq4$ nuclei and the nuclear saturation is revealed, and both systems are well described simultaneously, even without introducing three-nucleon forces.","This provides a quantitative understanding of the connection between the light nuclei and nuclear matter saturation properties, which has been an outstanding problem in nuclear $ab\\ initio$ calculations for decades."],"url":"http://arxiv.org/abs/2405.04203v1","category":"nucl-th"}
{"created":"2024-05-07 10:52:25","title":"On the Euler characteristic of the commutative graph complex and the top weight cohomology of $\\mathcal M_g$","abstract":"We prove an asymptotic formula for the Euler characteristic of Kontsevich's commutative graph complex. This formula implies that the total amount of commutative graph homology grows super-exponentially with the rank and, via a theorem of Chan, Galatius, and Payne, that the dimension of the top weight cohomology of the moduli space of curves, $\\mathcal M_g$, grows super-exponentially with the genus $g$.","sentences":["We prove an asymptotic formula for the Euler characteristic of Kontsevich's commutative graph complex.","This formula implies that the total amount of commutative graph homology grows super-exponentially with the rank and, via a theorem of Chan, Galatius, and Payne, that the dimension of the top weight cohomology of the moduli space of curves, $\\mathcal M_g$, grows super-exponentially with the genus $g$."],"url":"http://arxiv.org/abs/2405.04190v1","category":"math.AT"}
{"created":"2024-05-07 10:33:43","title":"New degeneration phenomenon for infinite-type Riemann surfaces","abstract":"Since the Teichm\\\"uller space of a surface $R$ is a deformation space of complex structures defined on $R$, its Bers boundary describes the degeneration of complex structures in a certain sense. In this paper, constructing a concrete example, we prove that if S is a Riemann surface of infinite type, there exists a Riemann surface with the marking, which is homeomorphic to the surface $R$ in the Bers boundary. We also show that many such degenerations exist in the Bers boundary.","sentences":["Since the Teichm\\\"uller space of a surface $R$ is a deformation space of complex structures defined on $R$, its Bers boundary describes the degeneration of complex structures in a certain sense.","In this paper, constructing a concrete example, we prove that if S is a Riemann surface of infinite type, there exists a Riemann surface with the marking, which is homeomorphic to the surface $R$ in the Bers boundary.","We also show that many such degenerations exist in the Bers boundary."],"url":"http://arxiv.org/abs/2405.04178v1","category":"math.GT"}
{"created":"2024-05-07 10:29:52","title":"A 49.8mm2 Fully Integrated, 1.5m Transmission-Range, High-Data-Rate IR-UWB Transmitter for Brain Implants","abstract":"To address the challenge of extending the transmission range of implantable TXs while also minimizing their size and power consumption, this paper introduces a transcutaneous, high data-rate, fully integrated IR-UWB transmitter that employs a novel co-designed power amplifier (PA) and antenna interface for enhanced performance. With the co-designed interface, we achieved the smallest footprint of 49.8mm2 and the longest transmission range of 1.5m compared to the state-of-the-art IR-UWB TXs.","sentences":["To address the challenge of extending the transmission range of implantable TXs while also minimizing their size and power consumption, this paper introduces a transcutaneous, high data-rate, fully integrated IR-UWB transmitter that employs a novel co-designed power amplifier (PA) and antenna interface for enhanced performance.","With the co-designed interface, we achieved the smallest footprint of 49.8mm2 and the longest transmission range of 1.5m compared to the state-of-the-art IR-UWB TXs."],"url":"http://arxiv.org/abs/2405.04177v1","category":"eess.SY"}
{"created":"2024-05-07 09:59:55","title":"Vaisman's theorem and local reducibility","abstract":"As proven in a celebrated theorem due to Vaisman, pure locally conformally K\\\"ahler metrics do not exist on compact K\\\"ahler manifolds. In a previous paper, we extended this result to the singular setting, more precisely to K\\\"ahler spaces which are locally irreducible. Without the additional assumption of local irreducibility, there are counterexamples for which Vaisman's theorem does not hold. In this article, we give a much broader sufficient condition under which Vaisman's theorem still holds for compact K\\\"ahler spaces which are locally reducible.","sentences":["As proven in a celebrated theorem due to Vaisman, pure locally conformally K\\\"ahler metrics do not exist on compact K\\\"ahler manifolds.","In a previous paper, we extended this result to the singular setting, more precisely to K\\\"ahler spaces which are locally irreducible.","Without the additional assumption of local irreducibility, there are counterexamples for which Vaisman's theorem does not hold.","In this article, we give a much broader sufficient condition under which Vaisman's theorem still holds for compact K\\\"ahler spaces which are locally reducible."],"url":"http://arxiv.org/abs/2405.04162v1","category":"math.DG"}
{"created":"2024-05-07 09:54:29","title":"Maxwell relation between entropy and atom-atom pair correlation","abstract":"For many-particle systems with short range interactions the local (same point) particle-particle pair correlation function represents a thermodynamic quantity that can be calculated using the Hellmann-Feynman theorem. Here we exploit this property to derive a thermodynamic Maxwell relation between the local pair correlation and the entropy of an ultracold Bose gas in one dimension (1D). To demonstrate the utility of this Maxwell relation, we apply it to the computational formalism of the stochastic projected Gross-Pitaevski equation (SPGPE) to determine the entropy of a finite-temperature 1D Bose gas from its atom-atom pair correlation function. Such a correlation function is easy to compute numerically within the SPGPE and other formalisms, which is unlike computing the entropy itself. Our calculations can be viewed as a numerical experiment that serves as a proof-of-principle demonstration of an experimental method to deduce the entropy of a quantum gas from the measured atom-atom correlations.","sentences":["For many-particle systems with short range interactions the local (same point) particle-particle pair correlation function represents a thermodynamic quantity that can be calculated using the Hellmann-Feynman theorem.","Here we exploit this property to derive a thermodynamic Maxwell relation between the local pair correlation and the entropy of an ultracold Bose gas in one dimension (1D).","To demonstrate the utility of this Maxwell relation, we apply it to the computational formalism of the stochastic projected Gross-Pitaevski equation (SPGPE) to determine the entropy of a finite-temperature 1D Bose gas from its atom-atom pair correlation function.","Such a correlation function is easy to compute numerically within the SPGPE and other formalisms, which is unlike computing the entropy itself.","Our calculations can be viewed as a numerical experiment that serves as a proof-of-principle demonstration of an experimental method to deduce the entropy of a quantum gas from the measured atom-atom correlations."],"url":"http://arxiv.org/abs/2405.04159v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-07 09:54:09","title":"Economic Complexity in Mono-Partite Networks","abstract":"Initially designed to predict and explain the economic trajectories of countries, cities, and regions, economic complexity has been found applicable in diverse contexts such as ecology and chess openings. The success of economic complexity stems from its capacity to assess hidden capabilities within a system indirectly. The existing algorithms for economic complexity operate only when the underlying interaction topology conforms to a bipartite graph. A single link disrupting the bipartite structure renders these algorithms inapplicable, even if the weight of that link is tiny compared to others. This paper presents a novel extension of economic complexity to encompass any graph, overcoming the constraints of bipartite structures. Additionally, it introduces fitness centrality and orthofitness centrality as new centrality measures in graphs. Fitness Centrality emerges as a promising metric for assessing node vulnerability, akin to node betweenness centrality. Furthermore, we unveil the cost functions that drive the minimization procedures underlying the economic complexity index and fitness centrality algorithms. This extension broadens the scope of economic complexity analysis, enabling its application in diverse network structures beyond bipartite graphs.","sentences":["Initially designed to predict and explain the economic trajectories of countries, cities, and regions, economic complexity has been found applicable in diverse contexts such as ecology and chess openings.","The success of economic complexity stems from its capacity to assess hidden capabilities within a system indirectly.","The existing algorithms for economic complexity operate only when the underlying interaction topology conforms to a bipartite graph.","A single link disrupting the bipartite structure renders these algorithms inapplicable, even if the weight of that link is tiny compared to others.","This paper presents a novel extension of economic complexity to encompass any graph, overcoming the constraints of bipartite structures.","Additionally, it introduces fitness centrality and orthofitness centrality as new centrality measures in graphs.","Fitness Centrality emerges as a promising metric for assessing node vulnerability, akin to node betweenness centrality.","Furthermore, we unveil the cost functions that drive the minimization procedures underlying the economic complexity index and fitness centrality algorithms.","This extension broadens the scope of economic complexity analysis, enabling its application in diverse network structures beyond bipartite graphs."],"url":"http://arxiv.org/abs/2405.04158v1","category":"physics.soc-ph"}
{"created":"2024-05-07 09:44:04","title":"CAKE: Sharing Slices of Confidential Data on Blockchain","abstract":"Cooperative information systems typically involve various entities in a collaborative process within a distributed environment. Blockchain technology offers a mechanism for automating such processes, even when only partial trust exists among participants. The data stored on the blockchain is replicated across all nodes in the network, ensuring accessibility to all participants. While this aspect facilitates traceability, integrity, and persistence, it poses challenges for adopting public blockchains in enterprise settings due to confidentiality issues. In this paper, we present a software tool named Control Access via Key Encryption (CAKE), designed to ensure data confidentiality in scenarios involving public blockchains. After outlining its core components and functionalities, we showcase the application of CAKE in the context of a real-world cyber-security project within the logistics domain.","sentences":["Cooperative information systems typically involve various entities in a collaborative process within a distributed environment.","Blockchain technology offers a mechanism for automating such processes, even when only partial trust exists among participants.","The data stored on the blockchain is replicated across all nodes in the network, ensuring accessibility to all participants.","While this aspect facilitates traceability, integrity, and persistence, it poses challenges for adopting public blockchains in enterprise settings due to confidentiality issues.","In this paper, we present a software tool named Control Access via Key Encryption (CAKE), designed to ensure data confidentiality in scenarios involving public blockchains.","After outlining its core components and functionalities, we showcase the application of CAKE in the context of a real-world cyber-security project within the logistics domain."],"url":"http://arxiv.org/abs/2405.04152v1","category":"cs.CR"}
{"created":"2024-05-07 09:39:52","title":"Subdifferentially polynomially bounded functions and Gaussian smoothing-based zeroth-order optimization","abstract":"We introduce the class of subdifferentially polynomially bounded (SPB) functions, which is a rich class of locally Lipschitz functions that encompasses all Lipschitz functions, all gradient- or Hessian-Lipschitz functions, and even some non-smooth locally Lipschitz functions. We show that SPB functions are compatible with Gaussian smoothing (GS), in the sense that the GS of any SPB function is well-defined and satisfies a descent lemma akin to gradient-Lipschitz functions, with the Lipschitz constant replaced by a polynomial function. Leveraging this descent lemma, we propose GS-based zeroth-order optimization algorithms with an adaptive stepsize strategy for constrained minimization of SPB functions, and analyze their iteration complexity. An important instrument in our analysis, which could be of independent interest, is the quantification of Goldstein stationarity via the GS gradient.","sentences":["We introduce the class of subdifferentially polynomially bounded (SPB) functions, which is a rich class of locally Lipschitz functions that encompasses all Lipschitz functions, all gradient- or Hessian-Lipschitz functions, and even some non-smooth locally Lipschitz functions.","We show that SPB functions are compatible with Gaussian smoothing (GS), in the sense that the GS of any SPB function is well-defined and satisfies a descent lemma akin to gradient-Lipschitz functions, with the Lipschitz constant replaced by a polynomial function.","Leveraging this descent lemma, we propose GS-based zeroth-order optimization algorithms with an adaptive stepsize strategy for constrained minimization of SPB functions, and analyze their iteration complexity.","An important instrument in our analysis, which could be of independent interest, is the quantification of Goldstein stationarity via the GS gradient."],"url":"http://arxiv.org/abs/2405.04150v1","category":"math.OC"}
{"created":"2024-05-07 09:33:43","title":"Ground-state properties of dipolar Bose-Einstein condensates with spin-orbit coupling and quantum fluctuations","abstract":"We study the ground-state properties of dipolar spin-1/2 Bose-Einstein condensates with quantum fluctuations and Rashba spin-orbit coupling (SOC). The combined effects of dipole-dipole interaction (DDI), SOC, and Lee-Huang-Yang (LHY) correction induced by quantum fluctuations on the ground-state structures and spin textures of the system are analyzed and discussed. For the nonrotating case and fixed nonlinear interspecies contact interaction strengths, our results show that structural phase transitions can be achieved by adjusting the strengths of the DDI and LHY correction. In the absence of SOC, a ground-state phase diagram is given with respect to the DDI strength and the LHY correction strength. We find that the system exhibits rich quantum phases including square droplet lattice phase, annular phase, loop-island structure, stripe-droplet coexistence phase, toroidal stripe phase, and Thomas-Fermi (TF) phase. For the rotating case, the increase of DDI strength can lead to a quantum phase transition from superfluid phase to supersolid phase. In the presence of SOC, the quantum droplets display obvious stretching and hidden vortex-antivortex clusters are formed in each component. In particular, weak or moderate SOC favors the formation of droplets while for strong SOC the ground state of the system develops into a stripe phase with hidden vortex-antivortex clusters. Furthermore, the system sustains exotic spin textures and topological excitations, such as composite skyrmion-antiskyrmion-meron-antimeron cluster, meron-antimeron string cluster, antimeron-meron-antimeron chain cluster, and peculiar skyrmion-antiskyrmion-meron-antimeron necklace with a meron-antimeron necklace embedded inside and a central spin Neel domain wall.","sentences":["We study the ground-state properties of dipolar spin-1/2 Bose-Einstein condensates with quantum fluctuations and Rashba spin-orbit coupling (SOC).","The combined effects of dipole-dipole interaction (DDI), SOC, and Lee-Huang-Yang (LHY) correction induced by quantum fluctuations on the ground-state structures and spin textures of the system are analyzed and discussed.","For the nonrotating case and fixed nonlinear interspecies contact interaction strengths, our results show that structural phase transitions can be achieved by adjusting the strengths of the DDI and LHY correction.","In the absence of SOC, a ground-state phase diagram is given with respect to the DDI strength and the LHY correction strength.","We find that the system exhibits rich quantum phases including square droplet lattice phase, annular phase, loop-island structure, stripe-droplet coexistence phase, toroidal stripe phase, and Thomas-Fermi (TF) phase.","For the rotating case, the increase of DDI strength can lead to a quantum phase transition from superfluid phase to supersolid phase.","In the presence of SOC, the quantum droplets display obvious stretching and hidden vortex-antivortex clusters are formed in each component.","In particular, weak or moderate SOC favors the formation of droplets while for strong SOC the ground state of the system develops into a stripe phase with hidden vortex-antivortex clusters.","Furthermore, the system sustains exotic spin textures and topological excitations, such as composite skyrmion-antiskyrmion-meron-antimeron cluster, meron-antimeron string cluster, antimeron-meron-antimeron chain cluster, and peculiar skyrmion-antiskyrmion-meron-antimeron necklace with a meron-antimeron necklace embedded inside and a central spin Neel domain wall."],"url":"http://arxiv.org/abs/2405.04149v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-07 09:09:28","title":"Stationary and non-stationary energy cascades in homogeneous ferrofluid turbulence","abstract":"The nonlinear transfer rate of the total energy (transfer rate of kinetic energy + transfer rate due to the work done by the magnetization) for an incompressible turbulent ferrofluid system is studied under the assumption of statistical homogeneity. Using the formalism of the two-point correlators, an exact relation connecting the second-order statistical moments to the average energy injection rate is derived for the scale-to scale transfer of the total energy. We validate the universality of the exact relation through direct numerical simulations for stationary and non-stationary cascade regimes. For a weak external magnetic field, both kinetic and the total energy cascade with nearly the same cascade rate. A stationary cascade regime is achieved and hence a good agreement between the exact energy transfer rate and the average energy injection is found. Due to the rapid alignment of the ferrofluid particles in the presence of strong external fields, the turbulence dynamics becomes non-stationary. Interestingly, there too, both kinetic and the total energy exhibit inertial range cascades but with different cascade rates which can be explained using the non-stationary form of our derived exact relation.","sentences":["The nonlinear transfer rate of the total energy (transfer rate of kinetic energy + transfer rate due to the work done by the magnetization) for an incompressible turbulent ferrofluid system is studied under the assumption of statistical homogeneity.","Using the formalism of the two-point correlators, an exact relation connecting the second-order statistical moments to the average energy injection rate is derived for the scale-to scale transfer of the total energy.","We validate the universality of the exact relation through direct numerical simulations for stationary and non-stationary cascade regimes.","For a weak external magnetic field, both kinetic and the total energy cascade with nearly the same cascade rate.","A stationary cascade regime is achieved and hence a good agreement between the exact energy transfer rate and the average energy injection is found.","Due to the rapid alignment of the ferrofluid particles in the presence of strong external fields, the turbulence dynamics becomes non-stationary.","Interestingly, there too, both kinetic and the total energy exhibit inertial range cascades but with different cascade rates which can be explained using the non-stationary form of our derived exact relation."],"url":"http://arxiv.org/abs/2405.04139v1","category":"physics.flu-dyn"}
{"created":"2024-05-07 08:51:45","title":"The utility of infrasound in global monitoring of extraterrestrial impacts: A case study of the 23 July 2008 Tajikistan bolide","abstract":"Among various observational techniques used for detection of large bolides on a global scale is a low frequency sound known as infrasound. Infrasound, which is also one of the four sensing modalities used by the International Monitoring System (IMS), offers continuous global monitoring, and can be leveraged towards planetary defense. Infrasonic records can provide an additional dimension for event characterization and a distinct perspective that might not be available through any other observational method. This paper describes infrasonic detection and characterization of the bolide that disintegrated over Tajikistan on 23 July 2008. This event was detected by two infrasound stations at distances of 1530 and 2130 km. Propagation paths to one of the stations were not predicted by the model despite being clearly detected. The presence of the signal is attributed to the acoustic energy being trapped in a weak but leaky stratospheric AtmoSOFAR channel. The infrasound signal analysis indicates that the shock originated at the point of the main breakup at an altitude of 35 km. The primary mode of shock production of the signal detected at the two stations was a spherical blast resulting from the main gross fragmentation episode. The energy estimate, based on the signal period, is 0.17-0.51 kt of TNT equivalent, suggesting a mass of 6.6-23.5 tons. The corresponding object radius, assuming the chondritic origin, was 0.78-1.18 m.","sentences":["Among various observational techniques used for detection of large bolides on a global scale is a low frequency sound known as infrasound.","Infrasound, which is also one of the four sensing modalities used by the International Monitoring System (IMS), offers continuous global monitoring, and can be leveraged towards planetary defense.","Infrasonic records can provide an additional dimension for event characterization and a distinct perspective that might not be available through any other observational method.","This paper describes infrasonic detection and characterization of the bolide that disintegrated over Tajikistan on 23 July 2008.","This event was detected by two infrasound stations at distances of 1530 and 2130 km.","Propagation paths to one of the stations were not predicted by the model despite being clearly detected.","The presence of the signal is attributed to the acoustic energy being trapped in a weak but leaky stratospheric AtmoSOFAR channel.","The infrasound signal analysis indicates that the shock originated at the point of the main breakup at an altitude of 35 km.","The primary mode of shock production of the signal detected at the two stations was a spherical blast resulting from the main gross fragmentation episode.","The energy estimate, based on the signal period, is 0.17-0.51 kt of TNT equivalent, suggesting a mass of 6.6-23.5 tons.","The corresponding object radius, assuming the chondritic origin, was 0.78-1.18 m."],"url":"http://arxiv.org/abs/2405.04127v1","category":"astro-ph.EP"}
{"created":"2024-05-07 08:49:01","title":"Optimizing Prosumer Policies in Periodic Double Auctions Inspired by Equilibrium Analysis (Extended Version)","abstract":"We consider a periodic double auction (PDA) wherein the main participants are wholesale suppliers and brokers representing retailers. The suppliers are represented by a composite supply curve and the brokers are represented by individual bids. Additionally, the brokers can participate in small-scale selling by placing individual asks; hence, they act as prosumers. Specifically, in a PDA, the prosumers who are net buyers have multiple opportunities to buy or sell multiple units of a commodity with the aim of minimizing the cost of buying across multiple rounds of the PDA. Formulating optimal bidding strategies for such a PDA setting involves planning across current and future rounds while considering the bidding strategies of other agents. In this work, we propose Markov perfect Nash equilibrium (MPNE) policies for a setup where multiple prosumers with knowledge of the composite supply curve compete to procure commodities. Thereafter, the MPNE policies are used to develop an algorithm called MPNE-BBS for the case wherein the prosumers need to re-construct an approximate composite supply curve using past auction information. The efficacy of the proposed algorithm is demonstrated on the PowerTAC wholesale market simulator against several baselines and state-of-the-art bidding policies.","sentences":["We consider a periodic double auction (PDA) wherein the main participants are wholesale suppliers and brokers representing retailers.","The suppliers are represented by a composite supply curve and the brokers are represented by individual bids.","Additionally, the brokers can participate in small-scale selling by placing individual asks; hence, they act as prosumers.","Specifically, in a PDA, the prosumers who are net buyers have multiple opportunities to buy or sell multiple units of a commodity with the aim of minimizing the cost of buying across multiple rounds of the PDA.","Formulating optimal bidding strategies for such a PDA setting involves planning across current and future rounds while considering the bidding strategies of other agents.","In this work, we propose Markov perfect Nash equilibrium (MPNE) policies for a setup where multiple prosumers with knowledge of the composite supply curve compete to procure commodities.","Thereafter, the MPNE policies are used to develop an algorithm called MPNE-BBS for the case wherein the prosumers need to re-construct an approximate composite supply curve using past auction information.","The efficacy of the proposed algorithm is demonstrated on the PowerTAC wholesale market simulator against several baselines and state-of-the-art bidding policies."],"url":"http://arxiv.org/abs/2405.04125v1","category":"eess.SY"}
{"created":"2024-05-07 08:44:29","title":"Ranking-based Client Selection with Imitation Learning for Efficient Federated Learning","abstract":"Federated Learning (FL) enables multiple devices to collaboratively train a shared model while ensuring data privacy. The selection of participating devices in each training round critically affects both the model performance and training efficiency, especially given the vast heterogeneity in training capabilities and data distribution across devices. To address these challenges, we introduce a novel device selection solution called FedRank, which is an end-to-end, ranking-based approach that is pre-trained by imitation learning against state-of-the-art analytical approaches. It not only considers data and system heterogeneity at runtime but also adaptively and efficiently chooses the most suitable clients for model training. Specifically, FedRank views client selection in FL as a ranking problem and employs a pairwise training strategy for the smart selection process. Additionally, an imitation learning-based approach is designed to counteract the cold-start issues often seen in state-of-the-art learning-based approaches. Experimental results reveal that \\model~ boosts model accuracy by 5.2\\% to 56.9\\%, accelerates the training convergence up to $2.01 \\times$ and saves the energy consumption up to $40.1\\%$.","sentences":["Federated Learning (FL) enables multiple devices to collaboratively train a shared model while ensuring data privacy.","The selection of participating devices in each training round critically affects both the model performance and training efficiency, especially given the vast heterogeneity in training capabilities and data distribution across devices.","To address these challenges, we introduce a novel device selection solution called FedRank, which is an end-to-end, ranking-based approach that is pre-trained by imitation learning against state-of-the-art analytical approaches.","It not only considers data and system heterogeneity at runtime but also adaptively and efficiently chooses the most suitable clients for model training.","Specifically, FedRank views client selection in FL as a ranking problem and employs a pairwise training strategy for the smart selection process.","Additionally, an imitation learning-based approach is designed to counteract the cold-start issues often seen in state-of-the-art learning-based approaches.","Experimental results reveal that \\model~ boosts model accuracy by 5.2\\% to 56.9\\%, accelerates the training convergence up to $2.01 \\times$ and saves the energy consumption up to $40.1\\%$."],"url":"http://arxiv.org/abs/2405.04122v1","category":"cs.LG"}
{"created":"2024-05-07 08:43:25","title":"Movable Antennas-Enabled Two-User Multicasting: Do We Really Need Alternating Optimization for Minimum Rate Maximization?","abstract":"Movable antenna (MA) technology, which can reconfigure wireless channels by flexibly moving antenna positions in a specified region, has great potential for improving communication performance. In this paper, we consider a new setup of MAs-enabled multicasting, where we adopt a simple setting in which a linear MA array-enabled source (${\\rm{S}}$) transmits a common message to two single-antenna users ${\\rm{U}}_1$ and ${\\rm{U}}_2$. We aim to maximize the minimum rate among these two users, by jointly optimizing the transmit beamforming and antenna positions at ${\\rm{S}}$. Instead of utilizing the widely-used alternating optimization (AO) approach, we reveal, with rigorous proof, that the above two variables can be optimized separately: i) the optimal antenna positions can be firstly determined via the successive convex approximation technique, based on the rule of maximizing the correlation between ${\\rm{S}}$-${\\rm{U}}_1$ and ${\\rm{S}}$-${\\rm{U}}_2$ channels; ii) afterwards, the optimal closed-form transmit beamforming can be derived via simple arguments. Compared to AO, this new approach yields the same performance but reduces the computational complexities significantly. Moreover, it can provide insightful conclusions which are not possible with AO.","sentences":["Movable antenna (MA) technology, which can reconfigure wireless channels by flexibly moving antenna positions in a specified region, has great potential for improving communication performance.","In this paper, we consider a new setup of MAs-enabled multicasting, where we adopt a simple setting in which a linear MA array-enabled source (${\\rm{S}}$) transmits a common message to two single-antenna users ${\\rm{U}}_1$ and ${\\rm{U}}_2$. We aim to maximize the minimum rate among these two users, by jointly optimizing the transmit beamforming and antenna positions at ${\\rm{S}}$. Instead of utilizing the widely-used alternating optimization (AO) approach, we reveal, with rigorous proof, that the above two variables can be optimized separately: i) the optimal antenna positions can be firstly determined via the successive convex approximation technique, based on the rule of maximizing the correlation between ${\\rm{S}}$-${\\rm{U}}_1$ and ${\\rm{S}}$-${\\rm{U}}_2$ channels; ii) afterwards, the optimal closed-form transmit beamforming can be derived via simple arguments.","Compared to AO, this new approach yields the same performance but reduces the computational complexities significantly.","Moreover, it can provide insightful conclusions which are not possible with AO."],"url":"http://arxiv.org/abs/2405.04120v1","category":"cs.IT"}
{"created":"2024-05-07 08:38:35","title":"A Stealthy Wrongdoer: Feature-Oriented Reconstruction Attack against Split Learning","abstract":"Split Learning (SL) is a distributed learning framework renowned for its privacy-preserving features and minimal computational requirements. Previous research consistently highlights the potential privacy breaches in SL systems by server adversaries reconstructing training data. However, these studies often rely on strong assumptions or compromise system utility to enhance attack performance. This paper introduces a new semi-honest Data Reconstruction Attack on SL, named Feature-Oriented Reconstruction Attack (FORA). In contrast to prior works, FORA relies on limited prior knowledge, specifically that the server utilizes auxiliary samples from the public without knowing any client's private information. This allows FORA to conduct the attack stealthily and achieve robust performance. The key vulnerability exploited by FORA is the revelation of the model representation preference in the smashed data output by victim client. FORA constructs a substitute client through feature-level transfer learning, aiming to closely mimic the victim client's representation preference. Leveraging this substitute client, the server trains the attack model to effectively reconstruct private data. Extensive experiments showcase FORA's superior performance compared to state-of-the-art methods. Furthermore, the paper systematically evaluates the proposed method's applicability across diverse settings and advanced defense strategies.","sentences":["Split Learning (SL) is a distributed learning framework renowned for its privacy-preserving features and minimal computational requirements.","Previous research consistently highlights the potential privacy breaches in SL systems by server adversaries reconstructing training data.","However, these studies often rely on strong assumptions or compromise system utility to enhance attack performance.","This paper introduces a new semi-honest Data Reconstruction Attack on SL, named Feature-Oriented Reconstruction Attack (FORA).","In contrast to prior works, FORA relies on limited prior knowledge, specifically that the server utilizes auxiliary samples from the public without knowing any client's private information.","This allows FORA to conduct the attack stealthily and achieve robust performance.","The key vulnerability exploited by FORA is the revelation of the model representation preference in the smashed data output by victim client.","FORA constructs a substitute client through feature-level transfer learning, aiming to closely mimic the victim client's representation preference.","Leveraging this substitute client, the server trains the attack model to effectively reconstruct private data.","Extensive experiments showcase FORA's superior performance compared to state-of-the-art methods.","Furthermore, the paper systematically evaluates the proposed method's applicability across diverse settings and advanced defense strategies."],"url":"http://arxiv.org/abs/2405.04115v1","category":"cs.CR"}
{"created":"2024-05-07 08:19:01","title":"Quantum dots and cryogenic radio-frequency readout electronics implemented on a fully-depleted silicon-on-insulator multi-chip assembly","abstract":"Quantum processing units will be modules of larger information processing systems containing also digital and analog electronics modules. Silicon-based quantum computing offers the enticing opportunity to manufacture all the modules using the same technology platform. Here, we present a cryogenic multi-module assembly for multiplexed readout of silicon quantum devices where all modules have been fabricated using the same fully-depleted silicon-on-insulator (FDSOI) CMOS process. The assembly is constituted by three chiplets: (i) a low-noise amplifier (LNA), (ii) a single-pole eight-throw switch (SP8T), and (iii) a silicon quantum dot (QD) array. We characterise each module individually and show (i) a gain over 35 dB, a bandwidth of 118 MHz, a minimum noise temperature of 4.2 K, (ii) an insertion loss smaller than 1.1 dB, a noise temperature smaller than 1.1 K across 0-2 GHz, and (iii) single-electron box (SEB) charge sensors. Finally, we combine all elements into a single demonstration showing time-domain radio-frequency multiplexing of two SEBs paving the way to an all-silicon quantum computing system.","sentences":["Quantum processing units will be modules of larger information processing systems containing also digital and analog electronics modules.","Silicon-based quantum computing offers the enticing opportunity to manufacture all the modules using the same technology platform.","Here, we present a cryogenic multi-module assembly for multiplexed readout of silicon quantum devices where all modules have been fabricated using the same fully-depleted silicon-on-insulator (FDSOI) CMOS process.","The assembly is constituted by three chiplets: (i) a low-noise amplifier (LNA), (ii) a single-pole eight-throw switch (SP8T), and (iii) a silicon quantum dot (QD) array.","We characterise each module individually and show (i) a gain over 35 dB, a bandwidth of 118 MHz, a minimum noise temperature of 4.2 K, (ii) an insertion loss smaller than 1.1 dB, a noise temperature smaller than 1.1 K across 0-2 GHz, and (iii) single-electron box (SEB) charge sensors.","Finally, we combine all elements into a single demonstration showing time-domain radio-frequency multiplexing of two SEBs paving the way to an all-silicon quantum computing system."],"url":"http://arxiv.org/abs/2405.04104v1","category":"quant-ph"}
{"created":"2024-05-07 08:16:00","title":"Analysis of Markovian Arrivals and Service with Applications to Intermittent Overload","abstract":"Almost all queueing analysis assumes i.i.d. arrivals and service. In reality, arrival and service rates fluctuate over time. In particular, it is common for real systems to intermittently experience overload, where the arrival rate temporarily exceeds the service rate, which an i.i.d. model cannot capture. We consider the MAMS system, where the arrival and service rates each vary according to an arbitrary finite-state Markov chain, allowing intermittent overload to be modeled.   We derive the first explicit characterization of mean queue length in the MAMS system, with explicit bounds for all arrival and service chains at all loads. Our bounds are tight in heavy traffic. We prove even stronger bounds for the important special case of two-level arrivals with intermittent overload.   Our key contribution is an extension to the drift method, based on the novel concepts of relative arrivals and relative completions. These quantities allow us to tractably capture the transient correlational effect of the arrival and service processes on the mean queue length.","sentences":["Almost all queueing analysis assumes i.i.d. arrivals and service.","In reality, arrival and service rates fluctuate over time.","In particular, it is common for real systems to intermittently experience overload, where the arrival rate temporarily exceeds the service rate, which an i.i.d. model cannot capture.","We consider the MAMS system, where the arrival and service rates each vary according to an arbitrary finite-state Markov chain, allowing intermittent overload to be modeled.   ","We derive the first explicit characterization of mean queue length in the MAMS system, with explicit bounds for all arrival and service chains at all loads.","Our bounds are tight in heavy traffic.","We prove even stronger bounds for the important special case of two-level arrivals with intermittent overload.   ","Our key contribution is an extension to the drift method, based on the novel concepts of relative arrivals and relative completions.","These quantities allow us to tractably capture the transient correlational effect of the arrival and service processes on the mean queue length."],"url":"http://arxiv.org/abs/2405.04102v1","category":"cs.PF"}
{"created":"2024-05-07 08:05:20","title":"Binarized Simplicial Convolutional Neural Networks","abstract":"Graph Neural Networks have a limitation of solely processing features on graph nodes, neglecting data on high-dimensional structures such as edges and triangles. Simplicial Convolutional Neural Networks (SCNN) represent higher-order structures using simplicial complexes to break this limitation albeit still lacking time efficiency. In this paper, we propose a novel neural network architecture on simplicial complexes named Binarized Simplicial Convolutional Neural Networks (Bi-SCNN) based on the combination of simplicial convolution with a binary-sign forward propagation strategy. The usage of the Hodge Laplacian on a binary-sign forward propagation enables Bi-SCNN to efficiently and effectively represent simplicial features that have higher-order structures than traditional graph node representations. Compared to the previous Simplicial Convolutional Neural Networks, the reduced model complexity of Bi-SCNN shortens the execution time without sacrificing the prediction performance and is less prone to the over-smoothing effect. Experimenting with real-world citation and ocean-drifter data confirmed that our proposed Bi-SCNN is efficient and accurate.","sentences":["Graph Neural Networks have a limitation of solely processing features on graph nodes, neglecting data on high-dimensional structures such as edges and triangles.","Simplicial Convolutional Neural Networks (SCNN) represent higher-order structures using simplicial complexes to break this limitation albeit still lacking time efficiency.","In this paper, we propose a novel neural network architecture on simplicial complexes named Binarized Simplicial Convolutional Neural Networks (Bi-SCNN) based on the combination of simplicial convolution with a binary-sign forward propagation strategy.","The usage of the Hodge Laplacian on a binary-sign forward propagation enables Bi-SCNN to efficiently and effectively represent simplicial features that have higher-order structures than traditional graph node representations.","Compared to the previous Simplicial Convolutional Neural Networks, the reduced model complexity of Bi-SCNN shortens the execution time without sacrificing the prediction performance and is less prone to the over-smoothing effect.","Experimenting with real-world citation and ocean-drifter data confirmed that our proposed Bi-SCNN is efficient and accurate."],"url":"http://arxiv.org/abs/2405.04098v1","category":"cs.LG"}
{"created":"2024-05-07 07:56:30","title":"Speaker Characterization by means of Attention Pooling","abstract":"State-of-the-art Deep Learning systems for speaker verification are commonly based on speaker embedding extractors. These architectures are usually composed of a feature extractor front-end together with a pooling layer to encode variable-length utterances into fixed-length speaker vectors. The authors have recently proposed the use of a Double Multi-Head Self-Attention pooling for speaker recognition, placed between a CNN-based front-end and a set of fully connected layers. This has shown to be an excellent approach to efficiently select the most relevant features captured by the front-end from the speech signal. In this paper we show excellent experimental results by adapting this architecture to other different speaker characterization tasks, such as emotion recognition, sex classification and COVID-19 detection.","sentences":["State-of-the-art Deep Learning systems for speaker verification are commonly based on speaker embedding extractors.","These architectures are usually composed of a feature extractor front-end together with a pooling layer to encode variable-length utterances into fixed-length speaker vectors.","The authors have recently proposed the use of a Double Multi-Head Self-Attention pooling for speaker recognition, placed between a CNN-based front-end and a set of fully connected layers.","This has shown to be an excellent approach to efficiently select the most relevant features captured by the front-end from the speech signal.","In this paper we show excellent experimental results by adapting this architecture to other different speaker characterization tasks, such as emotion recognition, sex classification and COVID-19 detection."],"url":"http://arxiv.org/abs/2405.04096v1","category":"eess.AS"}
{"created":"2024-05-07 07:44:04","title":"Protecting quantum gates from arbitrary single- and two-qubit errors","abstract":"We explore the protection of quantum gates from arbitrary single- and two-qubit noises with properly designed dynamical decoupling pulses. The proposed dynamical decoupling method is a concatenation of a sequence of pulses formed by $\\sigma_x$, $\\sigma_x\\sigma_x$ with another sequence constructed by $\\sigma_z$, $\\sigma_z\\sigma_z$. The concatenation of the two sequences results in desired pulses to fight agianst any single- and two-qubit errors. The success of our method relies on the ability to adjust system parameters or interaction terms, which can be achieved in different physical systems, including trapped ions and superconducting qubits. We finally explore the performance of our method numerically with the above-mentioned errors that are changing at any moment and show the preferred protection offered by the method. Therefore, our method is a timely step forward in preserving quantum gates at the level of physical qubits.","sentences":["We explore the protection of quantum gates from arbitrary single- and two-qubit noises with properly designed dynamical decoupling pulses.","The proposed dynamical decoupling method is a concatenation of a sequence of pulses formed by $\\sigma_x$, $\\sigma_x\\sigma_x$ with another sequence constructed by $\\sigma_z$, $\\sigma_z\\sigma_z$. The concatenation of the two sequences results in desired pulses to fight agianst any single- and two-qubit errors.","The success of our method relies on the ability to adjust system parameters or interaction terms, which can be achieved in different physical systems, including trapped ions and superconducting qubits.","We finally explore the performance of our method numerically with the above-mentioned errors that are changing at any moment and show the preferred protection offered by the method.","Therefore, our method is a timely step forward in preserving quantum gates at the level of physical qubits."],"url":"http://arxiv.org/abs/2405.04090v1","category":"quant-ph"}
{"created":"2024-05-07 07:32:02","title":"Existence and dynamical behaviour of vectorial standing waves with prescribed mass for Hartree-Fock type systems","abstract":"In this paper, we investigate vectorial standing waves with prescribed mass for the Hartree-Fock type system (HF system) with the double coupled feature. Such system is viewed as an approximation of the Coulomb system with two particles appeared in quantum mechanics. By exploring the interaction of the double coupled terms, we prove the exis?tence/nonexistence and symmetry of vectorial energy ground states for the corresponding stationary problem. Furthermore, we obtain the relation between vectorial energy ground states and vectorial action ground states in some cases. Finally, we establish conditions for global well-posedness and finite time blow-up to HF system with the initial data, and prove orbital stability/strong instability of standing waves.","sentences":["In this paper, we investigate vectorial standing waves with prescribed mass for the Hartree-Fock type system (HF system) with the double coupled feature.","Such system is viewed as an approximation of the Coulomb system with two particles appeared in quantum mechanics.","By exploring the interaction of the double coupled terms, we prove the exis?tence/nonexistence and symmetry of vectorial energy ground states for the corresponding stationary problem.","Furthermore, we obtain the relation between vectorial energy ground states and vectorial action ground states in some cases.","Finally, we establish conditions for global well-posedness and finite time blow-up to HF system with the initial data, and prove orbital stability/strong instability of standing waves."],"url":"http://arxiv.org/abs/2405.04084v1","category":"math.AP"}
{"created":"2024-05-07 07:27:28","title":"Logic-Skill Programming: An Optimization-based Approach to Sequential Skill Planning","abstract":"Recent advances in robot skill learning have unlocked the potential to construct task-agnostic skill libraries, facilitating the seamless sequencing of multiple simple manipulation primitives (aka. skills) to tackle significantly more complex tasks. Nevertheless, determining the optimal sequence for independently learned skills remains an open problem, particularly when the objective is given solely in terms of the final geometric configuration rather than a symbolic goal. To address this challenge, we propose Logic-Skill Programming (LSP), an optimization-based approach that sequences independently learned skills to solve long-horizon tasks. We formulate a first-order extension of a mathematical program to optimize the overall cumulative reward of all skills within a plan, abstracted by the sum of value functions. To solve such programs, we leverage the use of Tensor Train to construct the value function space, and rely on alternations between symbolic search and skill value optimization to find the appropriate skill skeleton and optimal subgoal sequence. Experimental results indicate that the obtained value functions provide a superior approximation of cumulative rewards compared to state-of-the-art Reinforcement Learning methods. Furthermore, we validate LSP in three manipulation domains, encompassing both prehensile and non-prehensile primitives. The results demonstrate its capability to identify the optimal solution over the full logic and geometric path. The real-robot experiments showcase the effectiveness of our approach to cope with contact uncertainty and external disturbances in the real world.","sentences":["Recent advances in robot skill learning have unlocked the potential to construct task-agnostic skill libraries, facilitating the seamless sequencing of multiple simple manipulation primitives (aka. skills) to tackle significantly more complex tasks.","Nevertheless, determining the optimal sequence for independently learned skills remains an open problem, particularly when the objective is given solely in terms of the final geometric configuration rather than a symbolic goal.","To address this challenge, we propose Logic-Skill Programming (LSP), an optimization-based approach that sequences independently learned skills to solve long-horizon tasks.","We formulate a first-order extension of a mathematical program to optimize the overall cumulative reward of all skills within a plan, abstracted by the sum of value functions.","To solve such programs, we leverage the use of Tensor Train to construct the value function space, and rely on alternations between symbolic search and skill value optimization to find the appropriate skill skeleton and optimal subgoal sequence.","Experimental results indicate that the obtained value functions provide a superior approximation of cumulative rewards compared to state-of-the-art Reinforcement Learning methods.","Furthermore, we validate LSP in three manipulation domains, encompassing both prehensile and non-prehensile primitives.","The results demonstrate its capability to identify the optimal solution over the full logic and geometric path.","The real-robot experiments showcase the effectiveness of our approach to cope with contact uncertainty and external disturbances in the real world."],"url":"http://arxiv.org/abs/2405.04082v1","category":"cs.RO"}
{"created":"2024-05-07 07:26:39","title":"Derisking of subsynchronous torsional oscillations in power systems with conventional and inverter-based generation","abstract":"This article proposes an application of a derisking methodology of subsynchronous torsional oscillations considering a realistic use case. The main objective is to summarize and draft a synthetic paper clarifying the complete methodology highlighting the main information needed step-by-step. For exemplification, a real model from a decommissioned oil power plant is adopted, where a fictitious high voltage direct current power link is connected. In this article, stress is laid on details of the application of the derisking methods: the unit interaction factor and the complex torque coefficients method. Then, the different steps to obtain results are explicitly explained. Moreover, the design and tuning process of supplementary subsynchronous damping controller is discussed. This mitigation section uses minimal information to correctly damp the unstable oscillations, as one would expect from industrial projects where the data sharing may be limited. Finally, the resources needed to perform each step of the study were summarized.","sentences":["This article proposes an application of a derisking methodology of subsynchronous torsional oscillations considering a realistic use case.","The main objective is to summarize and draft a synthetic paper clarifying the complete methodology highlighting the main information needed step-by-step.","For exemplification, a real model from a decommissioned oil power plant is adopted, where a fictitious high voltage direct current power link is connected.","In this article, stress is laid on details of the application of the derisking methods: the unit interaction factor and the complex torque coefficients method.","Then, the different steps to obtain results are explicitly explained.","Moreover, the design and tuning process of supplementary subsynchronous damping controller is discussed.","This mitigation section uses minimal information to correctly damp the unstable oscillations, as one would expect from industrial projects where the data sharing may be limited.","Finally, the resources needed to perform each step of the study were summarized."],"url":"http://arxiv.org/abs/2405.04080v1","category":"eess.SY"}
{"created":"2024-05-07 07:25:52","title":"Leveraging swarm capabilities to assist other systems","abstract":"Most studies in swarm robotics treat the swarm as an isolated system of interest. We argue that the prevailing view of swarms as self-sufficient, independent systems limits the scope of potential applications for swarm robotics. A robot swarm could act as a support in an heterogeneous system comprising other robots and/or human operators, in particular by quickly providing access to a large amount of data acquired in large unknown environments. Tasks such as target identification & tracking, scouting, or monitoring/surveillance could benefit from this approach.","sentences":["Most studies in swarm robotics treat the swarm as an isolated system of interest.","We argue that the prevailing view of swarms as self-sufficient, independent systems limits the scope of potential applications for swarm robotics.","A robot swarm could act as a support in an heterogeneous system comprising other robots and/or human operators, in particular by quickly providing access to a large amount of data acquired in large unknown environments.","Tasks such as target identification & tracking, scouting, or monitoring/surveillance could benefit from this approach."],"url":"http://arxiv.org/abs/2405.04079v1","category":"cs.RO"}
{"created":"2024-05-07 07:21:18","title":"Zone-selection effect of photoelectron intensity distributions in a nonsymmorphic system RAlSi (R : Ce and Nd)","abstract":"We investigate the electronic structures of noncentrosymmetric Weyl semimetals RAlSi (R: Ce and Nd) using soft x-ray angle-resolved photoemission spectroscopy. We find that the photoelectron intensity distribution observed in the momentum-resolved electronic bands is highly sensitive to the covered Brillouin zone (BZ) due to the zone-selection effect arising from the nonsymmorphic crystal structure of RAlSi. Our data reconstruct the photoelectron distributions varied according to the zone-selection effect, and reveal comprehensive information about the electronic band structures reproduced by band calculations. This detailed information enables us to experimentally trace the Weyl-cone dispersion throughout three-dimensional momentum space, providing valuable insights into the unique properties of RAlSi.","sentences":["We investigate the electronic structures of noncentrosymmetric Weyl semimetals RAlSi (R: Ce and Nd) using soft x-ray angle-resolved photoemission spectroscopy.","We find that the photoelectron intensity distribution observed in the momentum-resolved electronic bands is highly sensitive to the covered Brillouin zone (BZ) due to the zone-selection effect arising from the nonsymmorphic crystal structure of RAlSi.","Our data reconstruct the photoelectron distributions varied according to the zone-selection effect, and reveal comprehensive information about the electronic band structures reproduced by band calculations.","This detailed information enables us to experimentally trace the Weyl-cone dispersion throughout three-dimensional momentum space, providing valuable insights into the unique properties of RAlSi."],"url":"http://arxiv.org/abs/2405.04077v1","category":"cond-mat.str-el"}
{"created":"2024-05-07 07:20:40","title":"Adsorption-controlled epitaxy of perovskites","abstract":"I propose to use laser heating both for the substrate and the thermal evaporation sources in a vacuum chamber operating at pressures from XHV to values where the mean free path of the particles approaches or slightly exceeds the source-substrate distance. The concept combines the advantages of the molecular beam epitaxy (MBE) and pulsed laser deposition (PLD) methods to allow ultrapure deposition with continuous stoichiometry variation at high background pressures of arbitrary gases or molecular beams. Theory and preliminary experiments suggest that this setup is capable of growing complex oxides such as SrTiO$_3$ in the adsorption-controlled regime, similar to GaAs, in a background of molecular oxygen. This regime is neither accessible to MBE nor to PLD, making this laser epitaxy approach a unique tool to explore new growth regimes with the potential to fabricate structures such as modulation-doped heterostructures with low levels of background impurities that are impossible to synthesize with the current techniques. The technological simplicity and exceedingly compact size of the deposition chamber enable easy and rapid switching between different materials systems and the efficient synthesis of new materials that involve corrosive constituents. In contrast to PLD, the method may be scaled in a straightforward manner to large substrate sizes, providing a direct path from research to mass production.","sentences":["I propose to use laser heating both for the substrate and the thermal evaporation sources in a vacuum chamber operating at pressures from XHV to values where the mean free path of the particles approaches or slightly exceeds the source-substrate distance.","The concept combines the advantages of the molecular beam epitaxy (MBE) and pulsed laser deposition (PLD) methods to allow ultrapure deposition with continuous stoichiometry variation at high background pressures of arbitrary gases or molecular beams.","Theory and preliminary experiments suggest that this setup is capable of growing complex oxides such as SrTiO$_3$ in the adsorption-controlled regime, similar to GaAs, in a background of molecular oxygen.","This regime is neither accessible to MBE nor to PLD, making this laser epitaxy approach a unique tool to explore new growth regimes with the potential to fabricate structures such as modulation-doped heterostructures with low levels of background impurities that are impossible to synthesize with the current techniques.","The technological simplicity and exceedingly compact size of the deposition chamber enable easy and rapid switching between different materials systems and the efficient synthesis of new materials that involve corrosive constituents.","In contrast to PLD, the method may be scaled in a straightforward manner to large substrate sizes, providing a direct path from research to mass production."],"url":"http://arxiv.org/abs/2405.04075v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 07:19:25","title":"IMU-Aided Event-based Stereo Visual Odometry","abstract":"Direct methods for event-based visual odometry solve the mapping and camera pose tracking sub-problems by establishing implicit data association in a way that the generative model of events is exploited. The main bottlenecks faced by state-of-the-art work in this field include the high computational complexity of mapping and the limited accuracy of tracking. In this paper, we improve our previous direct pipeline \\textit{Event-based Stereo Visual Odometry} in terms of accuracy and efficiency. To speed up the mapping operation, we propose an efficient strategy of edge-pixel sampling according to the local dynamics of events. The mapping performance in terms of completeness and local smoothness is also improved by combining the temporal stereo results and the static stereo results. To circumvent the degeneracy issue of camera pose tracking in recovering the yaw component of general 6-DoF motion, we introduce as a prior the gyroscope measurements via pre-integration. Experiments on publicly available datasets justify our improvement. We release our pipeline as an open-source software for future research in this field.","sentences":["Direct methods for event-based visual odometry solve the mapping and camera pose tracking sub-problems by establishing implicit data association in a way that the generative model of events is exploited.","The main bottlenecks faced by state-of-the-art work in this field include the high computational complexity of mapping and the limited accuracy of tracking.","In this paper, we improve our previous direct pipeline \\textit{Event-based Stereo Visual Odometry} in terms of accuracy and efficiency.","To speed up the mapping operation, we propose an efficient strategy of edge-pixel sampling according to the local dynamics of events.","The mapping performance in terms of completeness and local smoothness is also improved by combining the temporal stereo results and the static stereo results.","To circumvent the degeneracy issue of camera pose tracking in recovering the yaw component of general 6-DoF motion, we introduce as a prior the gyroscope measurements via pre-integration.","Experiments on publicly available datasets justify our improvement.","We release our pipeline as an open-source software for future research in this field."],"url":"http://arxiv.org/abs/2405.04071v1","category":"cs.RO"}
{"created":"2024-05-07 07:15:25","title":"Characterizing Regional Importance in Cities with Human Mobility Motifs in Metro Networks","abstract":"Uncovering higher-order spatiotemporal dependencies within human mobility networks offers valuable insights into the analysis of urban structures. In most existing studies, human mobility networks are typically constructed by aggregating all trips without distinguishing who takes which specific trip. Instead, we claim individual mobility motifs, higher-order structures generated by daily trips of people, as fundamental units of human mobility networks. In this paper, we propose two network construction frameworks at the level of mobility motifs in characterizing regional importance in cities. Firstly, we enhance the structural dependencies within mobility motifs and proceed to construct mobility networks based on the enhanced mobility motifs. Secondly, taking inspiration from PageRank, we speculate that people would allocate values of importance to destinations according to their trip intentions. A motif-wise network construction framework is proposed based on the established mechanism. Leveraging large-scale metro data across cities, we construct three types of human mobility networks and characterize the regional importance by node importance indicators. Our comparison results suggest that the motif-based mobility network outperforms the classic mobility network, thus highlighting the efficacy of the introduced human mobility motifs. Finally, we demonstrate that the performance in characterizing the regional importance is significantly improved by our motif-wise framework.","sentences":["Uncovering higher-order spatiotemporal dependencies within human mobility networks offers valuable insights into the analysis of urban structures.","In most existing studies, human mobility networks are typically constructed by aggregating all trips without distinguishing who takes which specific trip.","Instead, we claim individual mobility motifs, higher-order structures generated by daily trips of people, as fundamental units of human mobility networks.","In this paper, we propose two network construction frameworks at the level of mobility motifs in characterizing regional importance in cities.","Firstly, we enhance the structural dependencies within mobility motifs and proceed to construct mobility networks based on the enhanced mobility motifs.","Secondly, taking inspiration from PageRank, we speculate that people would allocate values of importance to destinations according to their trip intentions.","A motif-wise network construction framework is proposed based on the established mechanism.","Leveraging large-scale metro data across cities, we construct three types of human mobility networks and characterize the regional importance by node importance indicators.","Our comparison results suggest that the motif-based mobility network outperforms the classic mobility network, thus highlighting the efficacy of the introduced human mobility motifs.","Finally, we demonstrate that the performance in characterizing the regional importance is significantly improved by our motif-wise framework."],"url":"http://arxiv.org/abs/2405.04066v1","category":"cs.SI"}
{"created":"2024-05-07 07:03:29","title":"Three-dimensional hidden phase probed by in-plane magnetotransport in kagome metal CsV3Sb5 thin flakes","abstract":"Transition metal compounds with kagome structure have been found to exhibit a variety of exotic structural, electronic, and magnetic orders. These orders are competing with energies very close to each other, resulting in complex phase transitions. Some of the phases are easily observable, such as the charge density wave (CDW) and the superconducting phase, while others are more challenging to identify and characterize. Here we present magneto-transport evidence of a new phase below ~35 K in the kagome topological metal CsV3Sb5 (CVS) thin flakes between the CDW and the superconducting transition temperatures. This phase is characterized by six-fold rotational symmetry in the in-plane magnetoresistance (MR) and is connected to the orbital current order in CVS. Furthermore, the phase is characterized by a large in-plane negative magnetoresistance, which suggests the existence of a three-dimensional, magnetic field-tunable orbital current ordered phase. Our results highlight the potential of magneto-transport to reveal the interactions between exotic quantum states of matter and to uncover the symmetry of such hidden phases.","sentences":["Transition metal compounds with kagome structure have been found to exhibit a variety of exotic structural, electronic, and magnetic orders.","These orders are competing with energies very close to each other, resulting in complex phase transitions.","Some of the phases are easily observable, such as the charge density wave (CDW) and the superconducting phase, while others are more challenging to identify and characterize.","Here we present magneto-transport evidence of a new phase below ~35 K in the kagome topological metal CsV3Sb5 (CVS) thin flakes between the CDW and the superconducting transition temperatures.","This phase is characterized by six-fold rotational symmetry in the in-plane magnetoresistance (MR) and is connected to the orbital current order in CVS.","Furthermore, the phase is characterized by a large in-plane negative magnetoresistance, which suggests the existence of a three-dimensional, magnetic field-tunable orbital current ordered phase.","Our results highlight the potential of magneto-transport to reveal the interactions between exotic quantum states of matter and to uncover the symmetry of such hidden phases."],"url":"http://arxiv.org/abs/2405.04059v1","category":"cond-mat.str-el"}
{"created":"2024-05-07 06:59:48","title":"Super-Earths and Earth-like Exoplanets","abstract":"In the last few years astronomical surveys have expanded the reach of planetary science into the realm of small and dense extrasolar worlds. These share a number of characteristics with the terrestrial and icy planetary objects of the Solar System, but keep stretching previous understanding of the known limits of planetary thermodynamics, material properties, and climate regimes. Improved compositional and thermal constraints on exoplanets below $\\sim$2 Earth radii suggest efficient accretion of atmosphere-forming volatile elements in a fraction of planetary systems, pointing to rapid formation, planet-scale melting, and chemical equilibration between the core, mantle, and atmosphere of rocky and volatile-rich exoplanets. Meaningful interpretation of novel observational data from these worlds necessitates cross-disciplinary expansion of known material properties under extreme thermodynamic, non-solar conditions, and accounting for dynamic feedbacks between interior and atmospheric processes. Exploration of the atmosphere and surface composition of individual, short-period super-Earths in the next few years will enable key inferences on magma ocean dynamics, the redox state of rocky planetary mantles, and mixing between volatile and refractory phases in planetary regimes that are absent from the present-day Solar System, and reminiscent of the conditions of the prebiotic Earth. The atmospheric characterization of climate diversity and the statistical search for biosignatures on terrestrial exoplanets on temperate orbits will require space-based direct imaging surveys, capable of resolving emission features of major and trace gases in both shortwave and mid-infrared wavelengths.","sentences":["In the last few years astronomical surveys have expanded the reach of planetary science into the realm of small and dense extrasolar worlds.","These share a number of characteristics with the terrestrial and icy planetary objects of the Solar System, but keep stretching previous understanding of the known limits of planetary thermodynamics, material properties, and climate regimes.","Improved compositional and thermal constraints on exoplanets below $\\sim$2 Earth radii suggest efficient accretion of atmosphere-forming volatile elements in a fraction of planetary systems, pointing to rapid formation, planet-scale melting, and chemical equilibration between the core, mantle, and atmosphere of rocky and volatile-rich exoplanets.","Meaningful interpretation of novel observational data from these worlds necessitates cross-disciplinary expansion of known material properties under extreme thermodynamic, non-solar conditions, and accounting for dynamic feedbacks between interior and atmospheric processes.","Exploration of the atmosphere and surface composition of individual, short-period super-Earths in the next few years will enable key inferences on magma ocean dynamics, the redox state of rocky planetary mantles, and mixing between volatile and refractory phases in planetary regimes that are absent from the present-day Solar System, and reminiscent of the conditions of the prebiotic Earth.","The atmospheric characterization of climate diversity and the statistical search for biosignatures on terrestrial exoplanets on temperate orbits will require space-based direct imaging surveys, capable of resolving emission features of major and trace gases in both shortwave and mid-infrared wavelengths."],"url":"http://arxiv.org/abs/2405.04057v1","category":"astro-ph.EP"}
{"created":"2024-05-07 06:57:42","title":"Bidirectional Adversarial Autoencoders for the design of Plasmonic Metasurfaces","abstract":"Deep Learning has been a critical part of designing inverse design methods that are computationally efficient and accurate. An example of this is the design of photonic metasurfaces by using their photoluminescent spectrum as the input data to predict their topology. One fundamental challenge of these systems is their ability to represent nonlinear relationships between sets of data that have different dimensionalities. Existing design methods often implement a conditional Generative Adversarial Network in order to solve this problem, but in many cases the solution is unable to generate structures that provide multiple peaks when validated. It is demonstrated that in response to the target spectrum, the Bidirectional Adversarial Autoencoder is able to generate structures that provide multiple peaks on several occasions. As a result the proposed model represents an important advance towards the generation of nonlinear photonic metasurfaces that can be used in advanced metasurface design.","sentences":["Deep Learning has been a critical part of designing inverse design methods that are computationally efficient and accurate.","An example of this is the design of photonic metasurfaces by using their photoluminescent spectrum as the input data to predict their topology.","One fundamental challenge of these systems is their ability to represent nonlinear relationships between sets of data that have different dimensionalities.","Existing design methods often implement a conditional Generative Adversarial Network in order to solve this problem, but in many cases the solution is unable to generate structures that provide multiple peaks when validated.","It is demonstrated that in response to the target spectrum, the Bidirectional Adversarial Autoencoder is able to generate structures that provide multiple peaks on several occasions.","As a result the proposed model represents an important advance towards the generation of nonlinear photonic metasurfaces that can be used in advanced metasurface design."],"url":"http://arxiv.org/abs/2405.04056v1","category":"physics.optics"}
{"created":"2024-05-07 06:49:06","title":"On the quantization goodness of polar lattices","abstract":"In this work, we prove that polar lattices, when tailored for lossy compression, are quantization-good in the sense that their normalized second moments approach $\\frac{1}{2\\pi e}$ as the dimension of lattices increases. It has been predicted by Zamir et al. \\cite{ZamirQZ96} that the Entropy Coded Dithered Quantization (ECDQ) system using quantization-good lattices can achieve the rate-distortion bound of i.i.d. Gaussian sources. In our previous work \\cite{LingQZ}, we established that polar lattices are indeed capable of attaining the same objective. It is reasonable to conjecture that polar lattices also demonstrate quantization goodness in the context of lossy compression. This study confirms this hypothesis.","sentences":["In this work, we prove that polar lattices, when tailored for lossy compression, are quantization-good in the sense that their normalized second moments approach $\\frac{1}{2\\pi e}$ as the dimension of lattices increases.","It has been predicted by Zamir et al. \\cite{ZamirQZ96} that the Entropy Coded Dithered Quantization (ECDQ) system using quantization-good lattices can achieve the rate-distortion bound of i.i.d.","Gaussian sources.","In our previous work \\cite{LingQZ}, we established that polar lattices are indeed capable of attaining the same objective.","It is reasonable to conjecture that polar lattices also demonstrate quantization goodness in the context of lossy compression.","This study confirms this hypothesis."],"url":"http://arxiv.org/abs/2405.04051v1","category":"cs.IT"}
{"created":"2024-05-07 06:32:03","title":"Scattering of Giant Planets and Implications for the Origin of the Hierarchical and Eccentric Two-planet System GJ 1148","abstract":"The GJ 1148 system has two Saturn-mass planets orbiting around an M dwarf star on hierarchical and eccentric orbits, with orbital period ratio of 13 and eccentricities of both planets of 0.375. The inner planet is in the regime of eccentric warm Jupiters. We perform numerical experiments to study the planet-planet scattering scenario for the origin of this orbital architecture. We consider a third planet of $0.1 M_J$ (Jupiter's mass) in the initial GJ 1148 system with initial orbital separations of 3.5, 4, and 4.5 mutual Hill radii and initial semimajor axis of the innermost planet in the range of 0.10-0.50 au. The majority of scattering results in planet-planet collisions, followed by planet ejections, and planet-star close approaches. Among them, only planet ejections produce eccentric and widely separated two-planet systems, with some having similar orbital properties to the GJ 1148 system. We also examine the effects of general relativistic apsidal precession and a higher mass of $0.227 M_J$ for the third planet. The simulation results suggest that the GJ 1148 system may have lost a giant planet. We also perform simulations of the general problem of the origin of warm Jupiters by planet-planet scattering. As in the GJ 1148 simulations, a nontrivial number of stable two-planet systems are produced by ejection, which disagrees with the result from a previous study showing that two-planet systems arise exclusively through planet-planet collisions.","sentences":["The GJ 1148 system has two Saturn-mass planets orbiting around an M dwarf star on hierarchical and eccentric orbits, with orbital period ratio of 13 and eccentricities of both planets of 0.375.","The inner planet is in the regime of eccentric warm Jupiters.","We perform numerical experiments to study the planet-planet scattering scenario for the origin of this orbital architecture.","We consider a third planet of $0.1 M_J$ (Jupiter's mass) in the initial GJ 1148 system with initial orbital separations of 3.5, 4, and 4.5 mutual Hill radii and initial semimajor axis of the innermost planet in the range of 0.10-0.50 au.","The majority of scattering results in planet-planet collisions, followed by planet ejections, and planet-star close approaches.","Among them, only planet ejections produce eccentric and widely separated two-planet systems, with some having similar orbital properties to the GJ 1148 system.","We also examine the effects of general relativistic apsidal precession and a higher mass of $0.227 M_J$ for the third planet.","The simulation results suggest that the GJ 1148 system may have lost a giant planet.","We also perform simulations of the general problem of the origin of warm Jupiters by planet-planet scattering.","As in the GJ 1148 simulations, a nontrivial number of stable two-planet systems are produced by ejection, which disagrees with the result from a previous study showing that two-planet systems arise exclusively through planet-planet collisions."],"url":"http://arxiv.org/abs/2405.04045v1","category":"astro-ph.EP"}
{"created":"2024-05-07 06:23:03","title":"Bohr radius for invariant families of bounded analytic functions and certain Integral transforms","abstract":"In this paper, we first obtain a refined Bohr radius for invariant families of bounded analytic functions on unit disk $ \\mathbb{D} $. Then, we obtain Bohr inequality for certain integral transforms, namely Fourier (discrete) and Laplace (discrete) transforms of bounded analytic functions $ f(z)=\\sum_{n=0}^{\\infty}a_nz^n $, in a simply connected domain \\begin{align*}   \\Omega_\\gamma=\\biggl\\{z\\in\\mathbb{C}: \\bigg|z+\\dfrac{\\gamma}{1-\\gamma}\\bigg|<\\dfrac{1}{1-\\gamma}\\;\\mbox{for}\\; 0\\leq \\gamma<1\\biggr\\},   \\end{align*} where $ \\Omega_0=\\mathbb{D} $. These results generalize some existing results. We also show that a better estimate can be obtained in radius and inequality can be shown sharp for Laplace transform of $ f $.","sentences":["In this paper, we first obtain a refined Bohr radius for invariant families of bounded analytic functions on unit disk $ \\mathbb{D} $.","Then, we obtain Bohr inequality for certain integral transforms, namely Fourier (discrete) and Laplace (discrete) transforms of bounded analytic functions $ f(z)=\\sum_{n=0}^{\\infty}a_nz^n $, in a simply connected domain \\begin{align*}   \\Omega_\\gamma=\\biggl\\{z\\in\\mathbb{C}: \\bigg|z+\\dfrac{\\gamma}{1-\\gamma}\\bigg|<\\dfrac{1}{1-\\gamma}\\;\\mbox{for}\\; 0\\leq \\gamma<1\\biggr\\},   \\end{align*} where $ \\Omega_0=\\mathbb{D} $.","These results generalize some existing results.","We also show that a better estimate can be obtained in radius and inequality can be shown sharp for Laplace transform of $ f $."],"url":"http://arxiv.org/abs/2405.04040v1","category":"math.CV"}
{"created":"2024-05-07 06:12:12","title":"FeZnSb$_2$: A possible NiAs-type hexagonal superconductor","abstract":"We present a first-principles investigation of electronic structure, lattice dynamics, and electron-phonon coupling of NiAs-type structure FeZnSb$_2$ and the isostructural parent compound FeSb within the framework of density functional theory. The calculation on partial disordered system FeZnSb$_2$ was performed in a fixed configuration. This hypothetical ordered structure is predicted to be superconducting.","sentences":["We present a first-principles investigation of electronic structure, lattice dynamics, and electron-phonon coupling of NiAs-type structure FeZnSb$_2$ and the isostructural parent compound FeSb within the framework of density functional theory.","The calculation on partial disordered system FeZnSb$_2$ was performed in a fixed configuration.","This hypothetical ordered structure is predicted to be superconducting."],"url":"http://arxiv.org/abs/2405.04037v1","category":"cond-mat.supr-con"}
{"created":"2024-05-07 06:09:53","title":"uTNT: Unikernels for Efficient and Flexible Internet Probing","abstract":"The last twenty years have seen the development and popularity of network measurement infrastructures. Internet measurement platforms have become common and have demonstrated their relevance in Internet understanding and security observation. However, despite their popularity, those platforms lack of flexibility and reactivity, as they are usually used for longitudinal measurements. As a consequence, they may miss detecting events that are security or Internet-related. During the same period, operating systems have evolved to virtual machines (VMs) as self-contained units for running applications, with the recent rise of unikernels, ultra-lightweight VMs tailored for specific applications, eliminating the need for a host OS. In this paper, we advocate that measurement infrastructures could take advantage of unikernels to become more flexible and efficient. We propose uTNT, a proof-of-concept unikernel-based implementation of TNT, a traceroute extension able to reveal MPLS tunnels. This paper documents the full toolchain for porting TNT into a unikernel and evaluates uTNT performance with respect to more traditional approaches. The paper also discusses a use case in which uTNT could find a suitable usage. uTNT source code is publicly available on Gitlab.","sentences":["The last twenty years have seen the development and popularity of network measurement infrastructures.","Internet measurement platforms have become common and have demonstrated their relevance in Internet understanding and security observation.","However, despite their popularity, those platforms lack of flexibility and reactivity, as they are usually used for longitudinal measurements.","As a consequence, they may miss detecting events that are security or Internet-related.","During the same period, operating systems have evolved to virtual machines (VMs) as self-contained units for running applications, with the recent rise of unikernels, ultra-lightweight VMs tailored for specific applications, eliminating the need for a host OS.","In this paper, we advocate that measurement infrastructures could take advantage of unikernels to become more flexible and efficient.","We propose uTNT, a proof-of-concept unikernel-based implementation of TNT, a traceroute extension able to reveal MPLS tunnels.","This paper documents the full toolchain for porting TNT into a unikernel and evaluates uTNT performance with respect to more traditional approaches.","The paper also discusses a use case in which uTNT could find a suitable usage.","uTNT source code is publicly available on Gitlab."],"url":"http://arxiv.org/abs/2405.04036v1","category":"cs.OS"}
{"created":"2024-05-07 06:09:40","title":"Absence of binding of heavy fermions by two light atoms in two dimensions","abstract":"By developing the mean-field theory valid for large $N$, we investigate the problem of two light fermions interacting via a zero-range potential with $N$ heavy fermions in two dimensions. We obtain numerical evidence that this system is never fully bound. It always splits into droplets containing a single light atom. This is in contrast to the one-dimensional case where any number of heavy and light fermions can be bound together.","sentences":["By developing the mean-field theory valid for large $N$, we investigate the problem of two light fermions interacting via a zero-range potential with $N$ heavy fermions in two dimensions.","We obtain numerical evidence that this system is never fully bound.","It always splits into droplets containing a single light atom.","This is in contrast to the one-dimensional case where any number of heavy and light fermions can be bound together."],"url":"http://arxiv.org/abs/2405.04035v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-07 06:09:37","title":"Differentially Private Post-Processing for Fair Regression","abstract":"This paper describes a differentially private post-processing algorithm for learning fair regressors satisfying statistical parity, addressing privacy concerns of machine learning models trained on sensitive data, as well as fairness concerns of their potential to propagate historical biases. Our algorithm can be applied to post-process any given regressor to improve fairness by remapping its outputs. It consists of three steps: first, the output distributions are estimated privately via histogram density estimation and the Laplace mechanism, then their Wasserstein barycenter is computed, and the optimal transports to the barycenter are used for post-processing to satisfy fairness. We analyze the sample complexity of our algorithm and provide fairness guarantee, revealing a trade-off between the statistical bias and variance induced from the choice of the number of bins in the histogram, in which using less bins always favors fairness at the expense of error.","sentences":["This paper describes a differentially private post-processing algorithm for learning fair regressors satisfying statistical parity, addressing privacy concerns of machine learning models trained on sensitive data, as well as fairness concerns of their potential to propagate historical biases.","Our algorithm can be applied to post-process any given regressor to improve fairness by remapping its outputs.","It consists of three steps: first, the output distributions are estimated privately via histogram density estimation and the Laplace mechanism, then their Wasserstein barycenter is computed, and the optimal transports to the barycenter are used for post-processing to satisfy fairness.","We analyze the sample complexity of our algorithm and provide fairness guarantee, revealing a trade-off between the statistical bias and variance induced from the choice of the number of bins in the histogram, in which using less bins always favors fairness at the expense of error."],"url":"http://arxiv.org/abs/2405.04034v1","category":"cs.LG"}
{"created":"2024-05-07 06:03:13","title":"Uncovering implementable dormant pruning decisions from three different stakeholder perspectives","abstract":"Dormant pruning, or the removal of unproductive portions of a tree while a tree is not actively growing, is an important orchard task to help maintain yield, requiring years to build expertise. Because of long training periods and an increasing labor shortage in agricultural jobs, pruning could benefit from robotic automation. However, to program robots to prune branches, we first need to understand how pruning decisions are made, and what variables in the environment (e.g., branch size and thickness) we need to capture. Working directly with three pruning stakeholders -- horticulturists, growers, and pruners -- we find that each group of human experts approaches pruning decision-making differently. To capture this knowledge, we present three studies and two extracted pruning protocols from field work conducted in Prosser, Washington in January 2022 and 2023. We interviewed six stakeholders (two in each group) and observed pruning across three cultivars -- Bing Cherries, Envy Apples, and Jazz Apples -- and two tree architectures -- Upright Fruiting Offshoot and V-Trellis. Leveraging participant interviews and video data, this analysis uses grounded coding to extract pruning terminology, discover horticultural contexts that influence pruning decisions, and find implementable pruning heuristics for autonomous systems. The results include a validated terminology set, which we offer for use by both pruning stakeholders and roboticists, to communicate general pruning concepts and heuristics. The results also highlight seven pruning heuristics utilizing this terminology set that would be relevant for use by future autonomous robot pruning systems, and characterize three discovered horticultural contexts (i.e., environmental management, crop-load management, and replacement wood) across all three cultivars.","sentences":["Dormant pruning, or the removal of unproductive portions of a tree while a tree is not actively growing, is an important orchard task to help maintain yield, requiring years to build expertise.","Because of long training periods and an increasing labor shortage in agricultural jobs, pruning could benefit from robotic automation.","However, to program robots to prune branches, we first need to understand how pruning decisions are made, and what variables in the environment (e.g., branch size and thickness) we need to capture.","Working directly with three pruning stakeholders -- horticulturists, growers, and pruners -- we find that each group of human experts approaches pruning decision-making differently.","To capture this knowledge, we present three studies and two extracted pruning protocols from field work conducted in Prosser, Washington in January 2022 and 2023.","We interviewed six stakeholders (two in each group) and observed pruning across three cultivars -- Bing Cherries, Envy Apples, and Jazz Apples -- and two tree architectures --","Upright Fruiting Offshoot and V-Trellis.","Leveraging participant interviews and video data, this analysis uses grounded coding to extract pruning terminology, discover horticultural contexts that influence pruning decisions, and find implementable pruning heuristics for autonomous systems.","The results include a validated terminology set, which we offer for use by both pruning stakeholders and roboticists, to communicate general pruning concepts and heuristics.","The results also highlight seven pruning heuristics utilizing this terminology set that would be relevant for use by future autonomous robot pruning systems, and characterize three discovered horticultural contexts (i.e., environmental management, crop-load management, and replacement wood) across all three cultivars."],"url":"http://arxiv.org/abs/2405.04030v1","category":"cs.RO"}
{"created":"2024-05-07 06:00:47","title":"Masked Graph Transformer for Large-Scale Recommendation","abstract":"Graph Transformers have garnered significant attention for learning graph-structured data, thanks to their superb ability to capture long-range dependencies among nodes. However, the quadratic space and time complexity hinders the scalability of Graph Transformers, particularly for large-scale recommendation. Here we propose an efficient Masked Graph Transformer, named MGFormer, capable of capturing all-pair interactions among nodes with a linear complexity. To achieve this, we treat all user/item nodes as independent tokens, enhance them with positional embeddings, and feed them into a kernelized attention module. Additionally, we incorporate learnable relative degree information to appropriately reweigh the attentions. Experimental results show the superior performance of our MGFormer, even with a single attention layer.","sentences":["Graph Transformers have garnered significant attention for learning graph-structured data, thanks to their superb ability to capture long-range dependencies among nodes.","However, the quadratic space and time complexity hinders the scalability of Graph Transformers, particularly for large-scale recommendation.","Here we propose an efficient Masked Graph Transformer, named MGFormer, capable of capturing all-pair interactions among nodes with a linear complexity.","To achieve this, we treat all user/item nodes as independent tokens, enhance them with positional embeddings, and feed them into a kernelized attention module.","Additionally, we incorporate learnable relative degree information to appropriately reweigh the attentions.","Experimental results show the superior performance of our MGFormer, even with a single attention layer."],"url":"http://arxiv.org/abs/2405.04028v1","category":"cs.IR"}
{"created":"2024-05-07 05:59:25","title":"Joint Visibility Region Detection and Channel Estimation for XL-MIMO Systems via Alternating MAP","abstract":"We investigate a joint visibility region (VR) detection and channel estimation problem in extremely large-scale multiple-input-multiple-output (XL-MIMO) systems, where near-field propagation and spatial non-stationary effects exist. In this case, each scatterer can only see a subset of antennas, i.e., it has a certain VR over the antennas. Because of the spatial correlation among adjacent sub-arrays, VR of scatterers exhibits a two-dimensional (2D) clustered sparsity. We design a 2D Markov prior model to capture such a structured sparsity. Based on this, a novel alternating maximum a posteriori (MAP) framework is developed for high-accuracy VR detection and channel estimation. The alternating MAP framework consists of three basic modules: a channel estimation module, a VR detection module, and a grid update module. Specifically, the first module is a low-complexity inverse-free variational Bayesian inference (IF-VBI) algorithm that avoids the matrix inverse via minimizing a relaxed Kullback-Leibler (KL) divergence. The second module is a structured expectation propagation (EP) algorithm which has the ability to deal with complicated prior information. And the third module refines polar-domain grid parameters via gradient ascent. Simulations demonstrate the superiority of the proposed algorithm in both VR detection and channel estimation.","sentences":["We investigate a joint visibility region (VR) detection and channel estimation problem in extremely large-scale multiple-input-multiple-output (XL-MIMO) systems, where near-field propagation and spatial non-stationary effects exist.","In this case, each scatterer can only see a subset of antennas, i.e., it has a certain VR over the antennas.","Because of the spatial correlation among adjacent sub-arrays, VR of scatterers exhibits a two-dimensional (2D) clustered sparsity.","We design a 2D Markov prior model to capture such a structured sparsity.","Based on this, a novel alternating maximum a posteriori (MAP) framework is developed for high-accuracy VR detection and channel estimation.","The alternating MAP framework consists of three basic modules: a channel estimation module, a VR detection module, and a grid update module.","Specifically, the first module is a low-complexity inverse-free variational Bayesian inference (IF-VBI) algorithm that avoids the matrix inverse via minimizing a relaxed Kullback-Leibler (KL) divergence.","The second module is a structured expectation propagation (EP) algorithm which has the ability to deal with complicated prior information.","And the third module refines polar-domain grid parameters via gradient ascent.","Simulations demonstrate the superiority of the proposed algorithm in both VR detection and channel estimation."],"url":"http://arxiv.org/abs/2405.04027v1","category":"eess.SP"}
{"created":"2024-05-07 17:56:06","title":"The ramification tree and almost Dedekind domains of prescribed SP-rank","abstract":"Given a valuation $v$ with quotient field $K$ and a sequence $\\mathcal{K} :K_0\\subseteq K_1\\subseteq\\cdots$ of finite extensions of $K$, we construct a weighted tree $\\mathcal{T}(v,\\mathcal{K})$ encoding information about the ramification of $v$ in the extensions $K_i$; conversely, we show that a weighted tree $\\mathcal{T}$ can be expressed as $\\mathcal{T}(v,\\mathcal{K})$ under some mild hypothesis on $v$ or on $\\mathcal{T}$. We use this construction to construct, for every countable successor ordinal number $\\alpha$, an almost Dedekind domain $D$, integral over $V$ (the valuation domain of $v$) whose SP-rank is $\\alpha$. Subsequently, we extend this result to countable limit ordinal numbers by considering integral extensions of Dedekind domains with countably many maximal ideals.","sentences":["Given a valuation $v$ with quotient field $K$ and a sequence $\\mathcal{K} :K_0\\subseteq K_1\\subseteq\\cdots$ of finite extensions of $K$, we construct a weighted tree $\\mathcal{T}(v,\\mathcal{K})$ encoding information about the ramification of $v$ in the extensions $K_i$; conversely, we show that a weighted tree $\\mathcal{T}$ can be expressed as $\\mathcal{T}(v,\\mathcal{K})$ under some mild hypothesis on $v$ or on $\\mathcal{T}$. We use this construction to construct, for every countable successor ordinal number $\\alpha$, an almost Dedekind domain $D$, integral over $V$ (the valuation domain of $v$) whose SP-rank is $\\alpha$. Subsequently, we extend this result to countable limit ordinal numbers by considering integral extensions of Dedekind domains with countably many maximal ideals."],"url":"http://arxiv.org/abs/2405.04523v1","category":"math.AC"}
{"created":"2024-05-07 17:56:04","title":"Astrometric Redshifts of Supernovae","abstract":"Differential Chromatic Refraction (DCR) is caused by the wavelength dependence of our atmosphere's refractive index, which shifts the apparent positions of stars and galaxies and distorts their shapes depending on their spectral energy distributions (SEDs). While this effect is typically mitigated and corrected for in imaging observations, we investigate how DCR can instead be used to our advantage to infer the redshifts of supernovae from multi-band, time-series imaging data. We simulate Type Ia supernovae (SNe Ia) in the proposed Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) Deep Drilling Field (DDF), and evaluate astrometric redshifts. We find that the redshift accuracy improves dramatically with the statistical quality of the astrometric measurements as well as with the accuracy of the astrometric solution. For a conservative choice of a 5-mas systematic uncertainty floor, we find that our redshift estimation is accurate at $z < 0.6$. We then combine our astrometric redshifts with both host galaxy photometric redshifts and supernovae photometric (light-curve) redshifts and show that this considerably improves the overall redshift estimates. These astrometric redshifts will be valuable especially since Rubin will discover a vast number of supernovae for which we will not be able to obtain spectroscopic redshifts.","sentences":["Differential Chromatic Refraction (DCR) is caused by the wavelength dependence of our atmosphere's refractive index, which shifts the apparent positions of stars and galaxies and distorts their shapes depending on their spectral energy distributions (SEDs).","While this effect is typically mitigated and corrected for in imaging observations, we investigate how DCR can instead be used to our advantage to infer the redshifts of supernovae from multi-band, time-series imaging data.","We simulate Type Ia supernovae (SNe Ia) in the proposed Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST)","Deep Drilling Field (DDF), and evaluate astrometric redshifts.","We find that the redshift accuracy improves dramatically with the statistical quality of the astrometric measurements as well as with the accuracy of the astrometric solution.","For a conservative choice of a 5-mas systematic uncertainty floor, we find that our redshift estimation is accurate at $z < 0.6$.","We then combine our astrometric redshifts with both host galaxy photometric redshifts and supernovae photometric (light-curve) redshifts and show that this considerably improves the overall redshift estimates.","These astrometric redshifts will be valuable especially since Rubin will discover a vast number of supernovae for which we will not be able to obtain spectroscopic redshifts."],"url":"http://arxiv.org/abs/2405.04522v1","category":"astro-ph.CO"}
{"created":"2024-05-07 16:41:36","title":"Bayesian Copula Density Estimation Using Bernstein Yett-Uniform Priors","abstract":"Probability density estimation is a central task in statistics. Copula-based models provide a great deal of flexibility in modelling multivariate distributions, allowing for the specifications of models for the marginal distributions separately from the dependence structure (copula) that links them to form a joint distribution. Choosing a class of copula models is not a trivial task and its misspecification can lead to wrong conclusions. We introduce a novel class of random Bernstein copula functions, and studied its support and the behavior of its posterior distribution. The proposal is based on a particular class of random grid-uniform copulas, referred to as yett-uniform copulas. Alternative Markov chain Monte Carlo algorithms for exploring the posterior distribution under the proposed model are also studied. The methodology is illustrated by means of simulated and real data.","sentences":["Probability density estimation is a central task in statistics.","Copula-based models provide a great deal of flexibility in modelling multivariate distributions, allowing for the specifications of models for the marginal distributions separately from the dependence structure (copula) that links them to form a joint distribution.","Choosing a class of copula models is not a trivial task and its misspecification can lead to wrong conclusions.","We introduce a novel class of random Bernstein copula functions, and studied its support and the behavior of its posterior distribution.","The proposal is based on a particular class of random grid-uniform copulas, referred to as yett-uniform copulas.","Alternative Markov chain Monte Carlo algorithms for exploring the posterior distribution under the proposed model are also studied.","The methodology is illustrated by means of simulated and real data."],"url":"http://arxiv.org/abs/2405.04475v1","category":"stat.ME"}
{"created":"2024-05-07 16:31:18","title":"Turning the Ratchet: Dynamic Screening with Multiple Agents","abstract":"We study a dynamic contracting problem with multiple agents and a lack of commitment. A principal who can only commit to one-period contracts wants to screen efficient agents over time. Once an agent reveals his type, the principal becomes tempted to revise contract terms, causing a \"ratchet effect.\" Alterations of contracts are observable and, hence, whenever past promises are not honored future information revelation stops. We provide a necessary and sufficient condition under which the principal is able to foster information revelation. When players are sufficiently patient, the agents' private information is either never revealed or fully revealed in a sequential manner. Optimal contracts entail high-powered incentives after an agent's type is initially disclosed, and rewards for information revelation disappear in the long run.","sentences":["We study a dynamic contracting problem with multiple agents and a lack of commitment.","A principal who can only commit to one-period contracts wants to screen efficient agents over time.","Once an agent reveals his type, the principal becomes tempted to revise contract terms, causing a \"ratchet effect.\"","Alterations of contracts are observable and, hence, whenever past promises are not honored future information revelation stops.","We provide a necessary and sufficient condition under which the principal is able to foster information revelation.","When players are sufficiently patient, the agents' private information is either never revealed or fully revealed in a sequential manner.","Optimal contracts entail high-powered incentives after an agent's type is initially disclosed, and rewards for information revelation disappear in the long run."],"url":"http://arxiv.org/abs/2405.04468v1","category":"econ.TH"}
{"created":"2024-05-07 16:29:37","title":"Two-way Fixed Effects and Differences-in-Differences Estimators in Heterogeneous Adoption Designs","abstract":"We consider treatment-effect estimation under a parallel trends assumption, in heterogeneous adoption designs where no unit is treated at period one, and units receive a weakly positive dose at period two. First, we develop a test of the assumption that the treatment effect is mean independent of the treatment, under which the commonly-used two-way-fixed-effects estimator is consistent. When this test is rejected, we propose alternative, robust estimators. If there are stayers with a period-two treatment equal to 0, the robust estimator is a difference-in-differences (DID) estimator using stayers as the control group. If there are quasi-stayers with a period-two treatment arbitrarily close to zero, the robust estimator is a DID using units with a period-two treatment below a bandwidth as controls. Finally, without stayers or quasi-stayers, we propose non-parametric bounds, and an estimator relying on a parametric specification of treatment-effect heterogeneity. We use our results to revisit Pierce and Schott (2016) and Enikolopov et al. (2011).","sentences":["We consider treatment-effect estimation under a parallel trends assumption, in heterogeneous adoption designs where no unit is treated at period one, and units receive a weakly positive dose at period two.","First, we develop a test of the assumption that the treatment effect is mean independent of the treatment, under which the commonly-used two-way-fixed-effects estimator is consistent.","When this test is rejected, we propose alternative, robust estimators.","If there are stayers with a period-two treatment equal to 0, the robust estimator is a difference-in-differences (DID) estimator using stayers as the control group.","If there are quasi-stayers with a period-two treatment arbitrarily close to zero, the robust estimator is a DID using units with a period-two treatment below a bandwidth as controls.","Finally, without stayers or quasi-stayers, we propose non-parametric bounds, and an estimator relying on a parametric specification of treatment-effect heterogeneity.","We use our results to revisit Pierce and Schott (2016) and Enikolopov et al. (2011)."],"url":"http://arxiv.org/abs/2405.04465v1","category":"econ.EM"}
{"created":"2024-05-07 15:31:58","title":"Weakly-Supervised Residual Evidential Learning for Multi-Instance Uncertainty Estimation","abstract":"Uncertainty estimation (UE), as an effective means of quantifying predictive uncertainty, is crucial for safe and reliable decision-making, especially in high-risk scenarios. Existing UE schemes usually assume that there are completely-labeled samples to support fully-supervised learning. In practice, however, many UE tasks often have no sufficiently-labeled data to use, such as the Multiple Instance Learning (MIL) with only weak instance annotations. To bridge this gap, this paper, for the first time, addresses the weakly-supervised issue of Multi-Instance UE (MIUE) and proposes a new baseline scheme, Multi-Instance Residual Evidential Learning (MIREL). Particularly, at the fine-grained instance UE with only weak supervision, we derive a multi-instance residual operator through the Fundamental Theorem of Symmetric Functions. On this operator derivation, we further propose MIREL to jointly model the high-order predictive distribution at bag and instance levels for MIUE. Extensive experiments empirically demonstrate that our MIREL not only could often make existing MIL networks perform better in MIUE, but also could surpass representative UE methods by large margins, especially in instance-level UE tasks.","sentences":["Uncertainty estimation (UE), as an effective means of quantifying predictive uncertainty, is crucial for safe and reliable decision-making, especially in high-risk scenarios.","Existing UE schemes usually assume that there are completely-labeled samples to support fully-supervised learning.","In practice, however, many UE tasks often have no sufficiently-labeled data to use, such as the Multiple Instance Learning (MIL) with only weak instance annotations.","To bridge this gap, this paper, for the first time, addresses the weakly-supervised issue of Multi-Instance UE (MIUE) and proposes a new baseline scheme, Multi-Instance Residual Evidential Learning (MIREL).","Particularly, at the fine-grained instance UE with only weak supervision, we derive a multi-instance residual operator through the Fundamental Theorem of Symmetric Functions.","On this operator derivation, we further propose MIREL to jointly model the high-order predictive distribution at bag and instance levels for MIUE.","Extensive experiments empirically demonstrate that our MIREL not only could often make existing MIL networks perform better in MIUE, but also could surpass representative UE methods by large margins, especially in instance-level UE tasks."],"url":"http://arxiv.org/abs/2405.04405v1","category":"cs.LG"}
{"created":"2024-05-07 15:29:48","title":"Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks","abstract":"Augmenting Large Language Models (LLMs) with image-understanding capabilities has resulted in a boom of high-performing Vision-Language models (VLMs). While studying the alignment of LLMs to human values has received widespread attention, the safety of VLMs has not received the same attention. In this paper, we explore the impact of jailbreaking on three state-of-the-art VLMs, each using a distinct modeling approach. By comparing each VLM to their respective LLM backbone, we find that each VLM is more susceptible to jailbreaking. We consider this as an undesirable outcome from visual instruction-tuning, which imposes a forgetting effect on an LLM's safety guardrails. Therefore, we provide recommendations for future work based on evaluation strategies that aim to highlight the weaknesses of a VLM, as well as take safety measures into account during visual instruction tuning.","sentences":["Augmenting Large Language Models (LLMs) with image-understanding capabilities has resulted in a boom of high-performing Vision-Language models (VLMs).","While studying the alignment of LLMs to human values has received widespread attention, the safety of VLMs has not received the same attention.","In this paper, we explore the impact of jailbreaking on three state-of-the-art VLMs, each using a distinct modeling approach.","By comparing each VLM to their respective LLM backbone, we find that each VLM is more susceptible to jailbreaking.","We consider this as an undesirable outcome from visual instruction-tuning, which imposes a forgetting effect on an LLM's safety guardrails.","Therefore, we provide recommendations for future work based on evaluation strategies that aim to highlight the weaknesses of a VLM, as well as take safety measures into account during visual instruction tuning."],"url":"http://arxiv.org/abs/2405.04403v1","category":"cs.CV"}
{"created":"2024-05-07 15:25:33","title":"A first-principles study of structural, elastic, electronic, and transport properties of Cs2Te","abstract":"The pursuit to operate photocathodes at high accelerating gradients to increase brightness of electron beams is gaining interests within the accelerator community. Cesium telluride (Cs2Te) is a widely used photocathode material and it is presumed to offer resilience to higher gradients because of its wider band gap compared to other semiconductors. Despite its advantages, crucial material properties of Cs2Te remain largely unknown both in theory and experiments. In this study, we employ first-principles calculations to provide detailed structural, elastic, electronic and transport properties of Cs2Te. It is found that Cs2Te has an intrinsic mobility of 20 cm2/Vs for electrons and 2.0 cm2/Vs for holes at room temperature. The low mobility is primarily limited by the strong polar optical phonon scattering. Cs2Te also exhibits ultralow lattice thermal conductivity of 0.2 W/(m*K) at room temperature. Based on the energy gain/loss balance under external field and electron-phonon scattering, we predict that Cs2Te has a dielectric breakdown field in the range from ~60 MV/m to ~132 MV/m at room temperature dependent on the doping level of Cs2Te. Our results are crucial to advance the understanding of applicability of Cs2Te photocathodes for high-gradient operation.","sentences":["The pursuit to operate photocathodes at high accelerating gradients to increase brightness of electron beams is gaining interests within the accelerator community.","Cesium telluride (Cs2Te) is a widely used photocathode material and it is presumed to offer resilience to higher gradients because of its wider band gap compared to other semiconductors.","Despite its advantages, crucial material properties of Cs2Te remain largely unknown both in theory and experiments.","In this study, we employ first-principles calculations to provide detailed structural, elastic, electronic and transport properties of Cs2Te.","It is found that Cs2Te has an intrinsic mobility of 20 cm2/Vs for electrons and 2.0 cm2/Vs for holes at room temperature.","The low mobility is primarily limited by the strong polar optical phonon scattering.","Cs2Te also exhibits ultralow lattice thermal conductivity of 0.2 W/(m*K) at room temperature.","Based on the energy gain/loss balance under external field and electron-phonon scattering, we predict that Cs2Te has a dielectric breakdown field in the range from ~60 MV/m to ~132 MV/m at room temperature dependent on the doping level of Cs2Te.","Our results are crucial to advance the understanding of applicability of Cs2Te photocathodes for high-gradient operation."],"url":"http://arxiv.org/abs/2405.04398v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 14:40:42","title":"On the Iwasawa theory of Cayley graphs","abstract":"This paper explores Iwasawa theory from a graph theoretic perspective, focusing on the algebraic and combinatorial properties of Cayley graphs. Using representation theory, we analyze Iwasawa-theoretic invariants within $\\mathbb{Z}_\\ell$-towers of Cayley graphs, revealing connections between graph theory, number theory, and group theory. Key results include the factorization of associated Iwasawa polynomials and the decomposition of $\\mu$- and $\\lambda$-invariants. Additionally, we apply these insights to complete graphs, establishing conditions under which these invariants vanish.","sentences":["This paper explores Iwasawa theory from a graph theoretic perspective, focusing on the algebraic and combinatorial properties of Cayley graphs.","Using representation theory, we analyze Iwasawa-theoretic invariants within $\\mathbb{Z}_\\ell$-towers of Cayley graphs, revealing connections between graph theory, number theory, and group theory.","Key results include the factorization of associated Iwasawa polynomials and the decomposition of $\\mu$- and $\\lambda$-invariants.","Additionally, we apply these insights to complete graphs, establishing conditions under which these invariants vanish."],"url":"http://arxiv.org/abs/2405.04361v1","category":"math.NT"}
{"created":"2024-05-07 14:31:21","title":"Third density and acoustic virial coefficients of helium isotopologues from ab initio calculations","abstract":"Improved two-body and three-body potentials for helium have been used to calculate from first principles the third density and acoustic virial coefficients for both $^4$He and $^3$He. For the third density virial coefficient $C(T)$, uncertainties have been reduced by a factor of 4--5 compared to the previous state of the art; the accuracy of first-principles $C(T)$ now exceeds that of the best experiments by more than two orders of magnitude. The range of calculations has been extended to temperatures as low as 0.5~K. For the third acoustic virial coefficient $\\gamma_a(T)$, we applied the Schlessinger Point Method, which can calculate $\\gamma_a$ and its uncertainty based on the $C(T)$ data, overcoming some limitations of direct path-integral calculation. The resulting $\\gamma_a$ are calculated at temperatures down to 0.5~K; they are consistent with available experimental data but have much smaller uncertainties. The first-principles data presented here will enable improvement of primary temperature and pressure metrology based on gas properties.","sentences":["Improved two-body and three-body potentials for helium have been used to calculate from first principles the third density and acoustic virial coefficients for both $^4$He and $^3$He.","For the third density virial coefficient $C(T)$, uncertainties have been reduced by a factor of 4--5 compared to the previous state of the art; the accuracy of first-principles $C(T)$ now exceeds that of the best experiments by more than two orders of magnitude.","The range of calculations has been extended to temperatures as low as 0.5~K. For the third acoustic virial coefficient $\\gamma_a(T)$, we applied the Schlessinger Point Method, which can calculate $\\gamma_a$ and its uncertainty based on the $C(T)$ data, overcoming some limitations of direct path-integral calculation.","The resulting $\\gamma_a$ are calculated at temperatures down to 0.5~K; they are consistent with available experimental data but have much smaller uncertainties.","The first-principles data presented here will enable improvement of primary temperature and pressure metrology based on gas properties."],"url":"http://arxiv.org/abs/2405.04353v1","category":"physics.chem-ph"}
{"created":"2024-05-07 14:26:05","title":"Overdetermined elliptic problems in nontrivial exterior domains of the hyperbolic space","abstract":"We construct nontrivial unbounded domains $\\Omega$ in the hyperbolic space $\\mathbb{H}^N$, $N \\in \\{2,3,4\\}$, bifurcating from the complement of a ball, such that the overdetermined elliptic problem \\begin{equation} -\\Delta_{\\mathbb{H}^N} u+u-u^p=0\\,\\, \\text{in}\\,\\,\\Omega, \\,\\, u=0,\\,\\,\\partial_\\nu u=\\text{const}\\,\\,\\text{on}\\,\\,\\partial\\Omega\\nonumber \\end{equation} has a positive bounded solution in $C^{2,\\alpha}\\left(\\Omega\\right) \\cap H^1\\left(\\Omega\\right)$. We also give a condition under which this construction holds for larger dimensions $N$. This is linked to the Berestycki-Caffarelli-Nirenberg conjecture on overdetermined elliptic problems, and, as far as we know, is the first nontrivial example of solution to an overdetermined elliptic problem in the hyperbolic space.","sentences":["We construct nontrivial unbounded domains $\\Omega$ in the hyperbolic space $\\mathbb{H}^N$, $N \\in \\{2,3,4\\}$, bifurcating from the complement of a ball, such that the overdetermined elliptic problem \\begin{equation} -\\Delta_{\\mathbb{H}^N} u+u-u^p=0\\,\\, \\text{in}\\,\\,\\Omega, \\,\\, u=0,\\,\\,\\partial_\\nu u=\\text{const}\\,\\,\\text{on}\\,\\,\\partial\\Omega\\nonumber \\end{equation} has a positive bounded solution in $C^{2,\\alpha}\\left(\\Omega\\right) \\cap H^1\\left(\\Omega\\right)$.","We also give a condition under which this construction holds for larger dimensions $N$. This is linked to the Berestycki-Caffarelli-Nirenberg conjecture on overdetermined elliptic problems, and, as far as we know, is the first nontrivial example of solution to an overdetermined elliptic problem in the hyperbolic space."],"url":"http://arxiv.org/abs/2405.04348v1","category":"math.AP"}
{"created":"2024-05-07 14:14:29","title":"Frustrated Superconductivity in the Trilayer Nickelate La$_4$Ni$_3$O$_{10}$","abstract":"Motivated by the opposite trend of maximum $T_c$ in multilayer nickelate and cuprate superconductors, we propose an interlayer pairing scenario for the superconductivity of the trilayer nickelate La$_4$Ni$_3$O$_{10}$ discovered recently under high pressure. Our theory reveals intrinsic frustration of the spin-singlet pairing formed between two outer layers and the same inner layer, which causes strong superconducting fluctuations between layers and thus explains the reduction of its maximum $T_c$ compared to that of the bilayer La$_3$Ni$_2$O$_{7}$. This supports a fundamental distinction between multilayer nickelate and cuprate superconductors associated with their potentially different (interlayer vs intralayer) pairing mechanisms. Our theory predicts extended $s^\\pm$-wave gap structures in La$_4$Ni$_3$O$_{10}$, with varying signs and possible nodes on different Fermi pockets. We also find an intrinsic Josephson coupling with potentially interesting consequences to be examined in future experiments.","sentences":["Motivated by the opposite trend of maximum $T_c$ in multilayer nickelate and cuprate superconductors, we propose an interlayer pairing scenario for the superconductivity of the trilayer nickelate La$_4$Ni$_3$O$_{10}$ discovered recently under high pressure.","Our theory reveals intrinsic frustration of the spin-singlet pairing formed between two outer layers and the same inner layer, which causes strong superconducting fluctuations between layers and thus explains the reduction of its maximum $T_c$ compared to that of the bilayer La$_3$Ni$_2$O$_{7}$. This supports a fundamental distinction between multilayer nickelate and cuprate superconductors associated with their potentially different (interlayer vs intralayer) pairing mechanisms.","Our theory predicts extended $s^\\pm$-wave gap structures in La$_4$Ni$_3$O$_{10}$, with varying signs and possible nodes on different Fermi pockets.","We also find an intrinsic Josephson coupling with potentially interesting consequences to be examined in future experiments."],"url":"http://arxiv.org/abs/2405.04340v1","category":"cond-mat.supr-con"}
{"created":"2024-05-07 14:13:16","title":"The computational content of multidimensional discontinuity","abstract":"The Weihrauch degrees are a tool to gauge the computational difficulty of mathematical problems. Often, what makes these problems hard is their discontinuity. We look at discontinuity in its purest form, that is, at otherwise constant functions that make a single discontinuous step along each dimension of their underlying space. This is an extension of previous work of Kihara, Pauly, Westrick from a single dimension to multiple dimensions. Among other results, we obtain strict hierarchies in the Weihrauch degrees, one of which orders mathematical problems by the richness of the truth-tables determining how discontinuous steps influence the output.","sentences":["The Weihrauch degrees are a tool to gauge the computational difficulty of mathematical problems.","Often, what makes these problems hard is their discontinuity.","We look at discontinuity in its purest form, that is, at otherwise constant functions that make a single discontinuous step along each dimension of their underlying space.","This is an extension of previous work of Kihara, Pauly, Westrick from a single dimension to multiple dimensions.","Among other results, we obtain strict hierarchies in the Weihrauch degrees, one of which orders mathematical problems by the richness of the truth-tables determining how discontinuous steps influence the output."],"url":"http://arxiv.org/abs/2405.04338v1","category":"math.LO"}
{"created":"2024-05-07 14:04:32","title":"The tail distribution function of the partition function for directed polymer in the weak disorder phase","abstract":"We investigate the upper tail distribution of the partition function of the directed polymer in a random environment on $\\mathbb Z^d$ in the weak disorder phase. We show that the distribution of the infinite volume partition function $W^{\\beta}_{\\infty}$ displays a power-law decay, with an exponent $p^*(\\beta)\\in [1+\\frac{2}{d},\\infty)$. We also prove that the distribution of the suprema of the point-to-point and point-to-line partition functions display the same behavior. On the way to these results, we prove a technical estimate of independent interest: the $L^p$-norm of the partition function at the time when it overshoots a high value $A$ is comparable to $A$. We use this estimate to extend the validity of many recent results that were proved under the assumption that the environment is upper bounded.","sentences":["We investigate the upper tail distribution of the partition function of the directed polymer in a random environment on $\\mathbb Z^d$ in the weak disorder phase.","We show that the distribution of the infinite volume partition function $W^{\\beta}_{\\infty}$ displays a power-law decay, with an exponent $p^*(\\beta)\\in","[1+\\frac{2}{d},\\infty)$. We also prove that the distribution of the suprema of the point-to-point and point-to-line partition functions display the same behavior.","On the way to these results, we prove a technical estimate of independent interest: the $L^p$-norm of the partition function at the time when it overshoots a high value $A$ is comparable to $A$.","We use this estimate to extend the validity of many recent results that were proved under the assumption that the environment is upper bounded."],"url":"http://arxiv.org/abs/2405.04335v1","category":"math.PR"}
{"created":"2024-05-07 14:01:27","title":"WALLETRADAR: Towards Automating the Detection of Vulnerabilities in Browser-based Cryptocurrency Wallets","abstract":"Cryptocurrency wallets, acting as fundamental infrastructure to the blockchain ecosystem, have seen significant user growth, particularly among browser-based wallets (i.e., browser extensions). However, this expansion accompanies security challenges, making these wallets prime targets for malicious activities. Despite a substantial user base, there is not only a significant gap in comprehensive security analysis but also a pressing need for specialized tools that can aid developers in reducing vulnerabilities during the development process. To fill the void, we present a comprehensive security analysis of browser-based wallets in this paper, along with the development of an automated tool designed for this purpose. We first compile a taxonomy of security vulnerabilities resident in cryptocurrency wallets by harvesting historical security reports. Based on this, we design WALLETRADAR, an automated detection framework that can accurately identify security issues based on static and dynamic analysis. Evaluation of 96 popular browser-based wallets shows WALLETRADAR's effectiveness, by successfully automating the detection process in 90% of these wallets with high precision. This evaluation has led to the discovery of 116 security vulnerabilities corresponding to 70 wallets. By the time of this paper, we have received confirmations of 10 vulnerabilities from 8 wallet developers, with over $2,000 bug bounties. Further, we observed that 12 wallet developers have silently fixed 16 vulnerabilities after our disclosure. WALLETRADAR can effectively automate the identification of security risks in cryptocurrency wallets, thereby enhancing software development quality and safety in the blockchain ecosystem.","sentences":["Cryptocurrency wallets, acting as fundamental infrastructure to the blockchain ecosystem, have seen significant user growth, particularly among browser-based wallets (i.e., browser extensions).","However, this expansion accompanies security challenges, making these wallets prime targets for malicious activities.","Despite a substantial user base, there is not only a significant gap in comprehensive security analysis but also a pressing need for specialized tools that can aid developers in reducing vulnerabilities during the development process.","To fill the void, we present a comprehensive security analysis of browser-based wallets in this paper, along with the development of an automated tool designed for this purpose.","We first compile a taxonomy of security vulnerabilities resident in cryptocurrency wallets by harvesting historical security reports.","Based on this, we design WALLETRADAR, an automated detection framework that can accurately identify security issues based on static and dynamic analysis.","Evaluation of 96 popular browser-based wallets shows WALLETRADAR's effectiveness, by successfully automating the detection process in 90% of these wallets with high precision.","This evaluation has led to the discovery of 116 security vulnerabilities corresponding to 70 wallets.","By the time of this paper, we have received confirmations of 10 vulnerabilities from 8 wallet developers, with over $2,000 bug bounties.","Further, we observed that 12 wallet developers have silently fixed 16 vulnerabilities after our disclosure.","WALLETRADAR can effectively automate the identification of security risks in cryptocurrency wallets, thereby enhancing software development quality and safety in the blockchain ecosystem."],"url":"http://arxiv.org/abs/2405.04332v1","category":"cs.CR"}
{"created":"2024-05-07 13:47:35","title":"Molecular Identification via Molecular Fingerprint extraction from Atomic Force Microscopy images","abstract":"Non--Contact Atomic Force Microscopy with CO--functionalized metal tips (referred to as HR-AFM) provides access to the internal structure of individual molecules adsorbed on a surface with totally unprecedented resolution. Previous works have shown that deep learning (DL) models can retrieve the chemical and structural information encoded in a 3D stack of constant-height HR--AFM images, leading to molecular identification. In this work, we overcome their limitations by using a well-established description of the molecular structure in terms of topological fingerprints, the 1024--bit Extended Connectivity Chemical Fingerprints of radius 2 (ECFP4), that were developed for substructure and similarity searching. ECFPs provide local structural information of the molecule, each bit correlating with a particular substructure within the molecule. Our DL model is able to extract this optimized structural descriptor from the 3D HR--AFM stacks and use it, through virtual screening, to identify molecules from their predicted ECFP4 with a retrieval accuracy on theoretical images of 95.4\\%. Furthermore, this approach, unlike previous DL models, assigns a confidence score, the Tanimoto similarity, to each of the candidate molecules, thus providing information on the reliability of the identification.   By construction, the number of times a certain substructure is present in the molecule is lost during the hashing process, necessary to make them useful for machine learning applications. We show that it is possible to complement the fingerprint-based virtual screening with global information provided by another DL model that predicts from the same HR--AFM stacks the chemical formula, boosting the identification accuracy up to a 97.6\\%. Finally, we perform a limited test with experimental images, obtaining promising results towards the application of this pipeline under real conditions","sentences":["Non--Contact Atomic Force Microscopy with CO--functionalized metal tips (referred to as HR-AFM) provides access to the internal structure of individual molecules adsorbed on a surface with totally unprecedented resolution.","Previous works have shown that deep learning (DL) models can retrieve the chemical and structural information encoded in a 3D stack of constant-height HR--AFM images, leading to molecular identification.","In this work, we overcome their limitations by using a well-established description of the molecular structure in terms of topological fingerprints, the 1024--bit Extended Connectivity Chemical Fingerprints of radius 2 (ECFP4), that were developed for substructure and similarity searching.","ECFPs provide local structural information of the molecule, each bit correlating with a particular substructure within the molecule.","Our DL model is able to extract this optimized structural descriptor from the 3D HR--AFM stacks and use it, through virtual screening, to identify molecules from their predicted ECFP4 with a retrieval accuracy on theoretical images of 95.4\\%.","Furthermore, this approach, unlike previous DL models, assigns a confidence score, the Tanimoto similarity, to each of the candidate molecules, thus providing information on the reliability of the identification.   ","By construction, the number of times a certain substructure is present in the molecule is lost during the hashing process, necessary to make them useful for machine learning applications.","We show that it is possible to complement the fingerprint-based virtual screening with global information provided by another DL model that predicts from the same HR--AFM stacks the chemical formula, boosting the identification accuracy up to a 97.6\\%.","Finally, we perform a limited test with experimental images, obtaining promising results towards the application of this pipeline under real conditions"],"url":"http://arxiv.org/abs/2405.04321v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 13:38:47","title":"The Injective category number on continuous maps","abstract":"We present the notion of injective category number of a map $f\\colon X\\to Y$ with basic results about this numerical invariant. For instance, we explore the injective category number under pullbacks and compositions. In the case that $f$ is the quotient map $q:X\\to X/\\tau$, where $X$ is a normal space and $\\tau:X\\to X$ is a fixed-point free involution, we show that the injective category of $q$ is equal to the index of $(X,\\tau)$ plus $2$.","sentences":["We present the notion of injective category number of a map $f\\colon X\\to Y$ with basic results about this numerical invariant.","For instance, we explore the injective category number under pullbacks and compositions.","In the case that $f$ is the quotient map $q:X\\to X/\\tau$, where $X$ is a normal space and $\\tau:X\\to X$ is a fixed-point free involution, we show that the injective category of $q$ is equal to the index of $(X,\\tau)$ plus $2$."],"url":"http://arxiv.org/abs/2405.04317v1","category":"math.AT"}
{"created":"2024-05-07 13:26:09","title":"Progressive Quantum Algorithm for Quantum Alternating Operator Ansatz","abstract":"Recently, Hadfield has proposed a novel Quantum Alternating Operator Ansatz (QAOA+) to tackle Constrained Combinatorial Optimization Problems (CCOPs), and it has wide applications. However, the large requirement of multi-qubit controlled gates in QAOA+ limits its applications in solving larger-scale CCOPs. To mitigate the resources overhead of QAOA+, we introduce an approach termed Progressive Quantum Algorithm (PQA). In this paper, the concept and performance of PQA are introduced focusing on the Maximal Independent Set (MIS) problem. PQA aims to yield the solution of the target graph $G$ with fewer resources by solving the MIS problem on a desired derived subgraph that has the same MIS solution as $G$ but has a much smaller graph size. To construct such a desired subgraph, PQA gradually and regularly expands the graph size starting from a well-designed initial subgraph. After each expansion, PQA solves the MIS problem on the current subgraph using QAOA+ and estimates whether the current graph has the same MIS solution as the target graph. PQA repeats the graph expansion and solving process until reaching the stop condition. In our simulations, the performance of PQA is benchmarked on Erd\\H{o}s-R\\'enyi (ER) and regular graphs. The simulation results suggest that PQA showcases higher average approximation ratio (AAR) and significant quantum resource savings compared with directly solves the original problem using QAOA+ (DS-QAOA+) at the same level depth $p$. Remarkably, the AAR obtained by PQA is $12.9305\\%$ ($4.8645\\%$) higher than DS-QAOA+ on ER (regular) graphs, and the average number of multi-qubit gates (qubits) consumed by PQA is 1/3 (1/2) of that of DS-QAOA+. The remarkable efficiency of PQA makes it possible to solve larger-scale CCOPs on the current quantum devices.","sentences":["Recently, Hadfield has proposed a novel Quantum Alternating Operator Ansatz (QAOA+) to tackle Constrained Combinatorial Optimization Problems (CCOPs), and it has wide applications.","However, the large requirement of multi-qubit controlled gates in QAOA+ limits its applications in solving larger-scale CCOPs.","To mitigate the resources overhead of QAOA+, we introduce an approach termed Progressive Quantum Algorithm (PQA).","In this paper, the concept and performance of PQA are introduced focusing on the Maximal Independent Set (MIS) problem.","PQA aims to yield the solution of the target graph $G$ with fewer resources by solving the MIS problem on a desired derived subgraph that has the same MIS solution as $G$ but has a much smaller graph size.","To construct such a desired subgraph, PQA gradually and regularly expands the graph size starting from a well-designed initial subgraph.","After each expansion, PQA solves the MIS problem on the current subgraph using QAOA+ and estimates whether the current graph has the same MIS solution as the target graph.","PQA repeats the graph expansion and solving process until reaching the stop condition.","In our simulations, the performance of PQA is benchmarked on Erd\\H{o}s-R\\'enyi (ER) and regular graphs.","The simulation results suggest that PQA showcases higher average approximation ratio (AAR) and significant quantum resource savings compared with directly solves the original problem using QAOA+ (DS-QAOA+) at the same level depth $p$. Remarkably, the AAR obtained by PQA is $12.9305\\%$ ($4.8645\\%$) higher than DS-QAOA+ on ER (regular) graphs, and the average number of multi-qubit gates (qubits) consumed by PQA is 1/3 (1/2) of that of DS-QAOA+.","The remarkable efficiency of PQA makes it possible to solve larger-scale CCOPs on the current quantum devices."],"url":"http://arxiv.org/abs/2405.04303v1","category":"quant-ph"}
{"created":"2024-05-07 12:00:22","title":"Improving semi-device-independent randomness certification by entropy accumulation","abstract":"Certified randomness guaranteed to be unpredictable by adversaries is central to information security. The fundamental randomness inherent in quantum physics makes certification possible from devices that are only weakly characterised, i.e. requiring little trust in their implementation. It was recently shown that the amount of certifiable randomness can be greatly improved using the so-called Entropy Accumulation Theorem generalised to prepare-and-measure settings. Furthermore, this approach allows a finite-size analysis which avoids assuming that all rounds are independent and identically distributed. Here, we demonstrate this improvement in semi-device-independent randomness certification from untrusted measurements.","sentences":["Certified randomness guaranteed to be unpredictable by adversaries is central to information security.","The fundamental randomness inherent in quantum physics makes certification possible from devices that are only weakly characterised, i.e. requiring little trust in their implementation.","It was recently shown that the amount of certifiable randomness can be greatly improved using the so-called Entropy Accumulation Theorem generalised to prepare-and-measure settings.","Furthermore, this approach allows a finite-size analysis which avoids assuming that all rounds are independent and identically distributed.","Here, we demonstrate this improvement in semi-device-independent randomness certification from untrusted measurements."],"url":"http://arxiv.org/abs/2405.04244v1","category":"quant-ph"}
{"created":"2024-05-07 11:22:01","title":"Local derivations and automorphisms of nilpotent Lie algebra","abstract":"The paper is devoted to the study of local derivations and automorphisms of nilpotent Lie algebras. Namely, we proved that nilpotent Lie algebras with indices of nilpotency $3$ and $4$ admit local derivation (local automorphisms) which is not a derivation (automorphisms). Further, it is presented a sufficient condition under which a nilpotent Lie algebra admits a local derivation which is not a derivation. With the same condition, it is proved the existence of pure local automorphism on a nilpotent Lie algebra. Finally, we present an $n$-dimensional non-associative algebra for which the space of local derivations coincides with the space of derivations.","sentences":["The paper is devoted to the study of local derivations and automorphisms of nilpotent Lie algebras.","Namely, we proved that nilpotent Lie algebras with indices of nilpotency $3$ and $4$ admit local derivation (local automorphisms) which is not a derivation (automorphisms).","Further, it is presented a sufficient condition under which a nilpotent Lie algebra admits a local derivation which is not a derivation.","With the same condition, it is proved the existence of pure local automorphism on a nilpotent Lie algebra.","Finally, we present an $n$-dimensional non-associative algebra for which the space of local derivations coincides with the space of derivations."],"url":"http://arxiv.org/abs/2405.04209v1","category":"math.RA"}
{"created":"2024-05-07 11:13:01","title":"Quaternionic representation and design of depolarizers","abstract":"Quaternions have been used to represent polarization states and polarization operators. But so far, only polarizers, dichroic or non-depolarizing devices have been represented in that way. We propose a quaternionic representation of perfect as well as partial depolarizers. It leads us to design actual setups for producing natural (unpolarized) light from an arbitrary partially or completely polarized wave or for reducing the degree of polarization of a wave. We conclude by making the link with Azzam's polarization orthogonalization problem.","sentences":["Quaternions have been used to represent polarization states and polarization operators.","But so far, only polarizers, dichroic or non-depolarizing devices have been represented in that way.","We propose a quaternionic representation of perfect as well as partial depolarizers.","It leads us to design actual setups for producing natural (unpolarized) light from an arbitrary partially or completely polarized wave or for reducing the degree of polarization of a wave.","We conclude by making the link with Azzam's polarization orthogonalization problem."],"url":"http://arxiv.org/abs/2405.04196v1","category":"physics.optics"}
{"created":"2024-05-07 10:08:15","title":"Three variations of Heads or Tails Game for Bitcoin","abstract":"We present three very simple variants of the classic Heads or Tails game using chips, each of which contributes to our understanding of the Bitcoin protocol. The first variant addresses the issue of temporary Bitcoin forks, which occur when two miners discover blocks simultaneously. We determine the threshold at which an honest but temporarily ``Byzantine'' miner persists in mining on their fork to save his orphaned blocks. The second variant of Heads or Tails game is biased in favor of the player and helps to explain why the difficulty adjustment formula is vulnerable to attacks of Nakamoto's consensus. We derive directly and in a simple way, without relying on a Markov decision solver as was the case until now, the threshold beyond which a miner without connectivity finds it advantageous to adopt a deviant mining strategy on Bitcoin. The third variant of Heads or Tails game is unbiased and demonstrates that this issue in the Difficulty Adjustment formula can be fully rectified. Our results are in agreement with the existing literature that we clarify both qualitatively and quantitatively using very simple models and scripts that are easy to implement.","sentences":["We present three very simple variants of the classic Heads or Tails game using chips, each of which contributes to our understanding of the Bitcoin protocol.","The first variant addresses the issue of temporary Bitcoin forks, which occur when two miners discover blocks simultaneously.","We determine the threshold at which an honest but temporarily ``Byzantine'' miner persists in mining on their fork to save his orphaned blocks.","The second variant of Heads or Tails game is biased in favor of the player and helps to explain why the difficulty adjustment formula is vulnerable to attacks of Nakamoto's consensus.","We derive directly and in a simple way, without relying on a Markov decision solver as was the case until now, the threshold beyond which a miner without connectivity finds it advantageous to adopt a deviant mining strategy on Bitcoin.","The third variant of Heads or Tails game is unbiased and demonstrates that this issue in the Difficulty Adjustment formula can be fully rectified.","Our results are in agreement with the existing literature that we clarify both qualitatively and quantitatively using very simple models and scripts that are easy to implement."],"url":"http://arxiv.org/abs/2405.04168v1","category":"cs.CR"}
{"created":"2024-05-07 09:22:21","title":"The ground states and first radial excitations of the vector tetraquark states with explicit P-waves via the QCD sum rules","abstract":"In this work, we choose diquark-antidiquark type four-quark currents with an explicit P-wave to study the ground states and first radial excitations of the hidden-charm tetraquark states with the quantum numbers $J^{PC}=1^{--}$. And we obtain the lowest vector tetraquark masses and make possible assignments of the existing $Y$ states. There indeed exists a hidden-charm tetraquark state with the $J^{PC}=1^{--}$ at the energy about $4.75\\,\\rm{GeV}$ as the first radial excitation to account for the BESIII data.","sentences":["In this work, we choose diquark-antidiquark type four-quark currents with an explicit P-wave to study the ground states and first radial excitations of the hidden-charm tetraquark states with the quantum numbers $J^{PC}=1^{--}$.","And we obtain the lowest vector tetraquark masses and make possible assignments of the existing $Y$ states.","There indeed exists a hidden-charm tetraquark state with the $J^{PC}=1^{--}$ at the energy about $4.75\\,\\rm{GeV}$ as the first radial excitation to account for the BESIII data."],"url":"http://arxiv.org/abs/2405.04145v1","category":"hep-ph"}
{"created":"2024-05-07 09:19:53","title":"Lossy Compression with Data, Perception, and Classification Constraints","abstract":"Balancing diverse task objectives under limited rate is crucial for developing robust multi-task deep learning (DL) models and improving performance across various domains. In this paper, we consider the lossy compression problem with human-centric and task-oriented metrics, such as perceptual quality and classification accuracy. We investigate two ternary relationships, namely, the rate-distortion-classification (RDC) and rate-perception-classification (RPC). For both RDC and RPC functions, we derive the closed-form expressions of the optimal rate for both binary and Gaussian sources. Notably, both RDC and RPC relationships exhibit distinct characteristics compared to the previous RDP tradeoff proposed by Blau et al. Then, we conduct experiments by implementing a DL-based image compression framework, incorporating rate, distortion, perception, and classification constraints. The experimental results verify the theoretical characteristics of RDC and RPC tradeoffs, providing information-theoretical insights into the design of loss functions to balance diverse task objectives in deep learning.","sentences":["Balancing diverse task objectives under limited rate is crucial for developing robust multi-task deep learning (DL) models and improving performance across various domains.","In this paper, we consider the lossy compression problem with human-centric and task-oriented metrics, such as perceptual quality and classification accuracy.","We investigate two ternary relationships, namely, the rate-distortion-classification (RDC) and rate-perception-classification (RPC).","For both RDC and RPC functions, we derive the closed-form expressions of the optimal rate for both binary and Gaussian sources.","Notably, both RDC and RPC relationships exhibit distinct characteristics compared to the previous RDP tradeoff proposed by Blau et al.","Then, we conduct experiments by implementing a DL-based image compression framework, incorporating rate, distortion, perception, and classification constraints.","The experimental results verify the theoretical characteristics of RDC and RPC tradeoffs, providing information-theoretical insights into the design of loss functions to balance diverse task objectives in deep learning."],"url":"http://arxiv.org/abs/2405.04144v1","category":"cs.IT"}
{"created":"2024-05-07 09:01:02","title":"Geometry and Dynamics of LayerNorm","abstract":"A technical note aiming to offer deeper intuition for the LayerNorm function common in deep neural networks. LayerNorm is defined relative to a distinguished 'neural' basis, but it does more than just normalize the corresponding vector elements. Rather, it implements a composition -- of linear projection, nonlinear scaling, and then affine transformation -- on input activation vectors. We develop both a new mathematical expression and geometric intuition, to make the net effect more transparent. We emphasize that, when LayerNorm acts on an N-dimensional vector space, all outcomes of LayerNorm lie within the intersection of an (N-1)-dimensional hyperplane and the interior of an N-dimensional hyperellipsoid. This intersection is the interior of an (N-1)-dimensional hyperellipsoid, and typical inputs are mapped near its surface. We find the direction and length of the principal axes of this (N-1)-dimensional hyperellipsoid via the eigen-decomposition of a simply constructed matrix.","sentences":["A technical note aiming to offer deeper intuition for the LayerNorm function common in deep neural networks.","LayerNorm is defined relative to a distinguished 'neural' basis, but it does more than just normalize the corresponding vector elements.","Rather, it implements a composition -- of linear projection, nonlinear scaling, and then affine transformation -- on input activation vectors.","We develop both a new mathematical expression and geometric intuition, to make the net effect more transparent.","We emphasize that, when LayerNorm acts on an N-dimensional vector space, all outcomes of LayerNorm lie within the intersection of an (N-1)-dimensional hyperplane and the interior of an N-dimensional hyperellipsoid.","This intersection is the interior of an (N-1)-dimensional hyperellipsoid, and typical inputs are mapped near its surface.","We find the direction and length of the principal axes of this (N-1)-dimensional hyperellipsoid via the eigen-decomposition of a simply constructed matrix."],"url":"http://arxiv.org/abs/2405.04134v1","category":"cs.LG"}
{"created":"2024-05-07 08:28:51","title":"Adaptive Least Mean pth Power Graph Neural Networks","abstract":"In the presence of impulsive noise, and missing observations, accurate online prediction of time-varying graph signals poses a crucial challenge in numerous application domains. We propose the Adaptive Least Mean $p^{th}$ Power Graph Neural Networks (LMP-GNN), a universal framework combining adaptive filter and graph neural network for online graph signal estimation. LMP-GNN retains the advantage of adaptive filtering in handling noise and missing observations as well as the online update capability. The incorporated graph neural network within the LMP-GNN can train and update filter parameters online instead of predefined filter parameters in previous methods, outputting more accurate prediction results. The adaptive update scheme of the LMP-GNN follows the solution of a $l_p$-norm optimization, rooting to the minimum dispersion criterion, and yields robust estimation results for time-varying graph signals under impulsive noise. A special case of LMP-GNN named the Sign-GNN is also provided and analyzed, Experiment results on two real-world datasets of temperature graph and traffic graph under four different noise distributions prove the effectiveness and robustness of our proposed LMP-GNN.","sentences":["In the presence of impulsive noise, and missing observations, accurate online prediction of time-varying graph signals poses a crucial challenge in numerous application domains.","We propose the Adaptive Least Mean $p^{th}$ Power Graph Neural Networks (LMP-GNN), a universal framework combining adaptive filter and graph neural network for online graph signal estimation.","LMP-GNN retains the advantage of adaptive filtering in handling noise and missing observations as well as the online update capability.","The incorporated graph neural network within the LMP-GNN can train and update filter parameters online instead of predefined filter parameters in previous methods, outputting more accurate prediction results.","The adaptive update scheme of the LMP-GNN follows the solution of a $l_p$-norm optimization, rooting to the minimum dispersion criterion, and yields robust estimation results for time-varying graph signals under impulsive noise.","A special case of LMP-GNN named the Sign-GNN is also provided and analyzed, Experiment results on two real-world datasets of temperature graph and traffic graph under four different noise distributions prove the effectiveness and robustness of our proposed LMP-GNN."],"url":"http://arxiv.org/abs/2405.04111v1","category":"cs.LG"}
{"created":"2024-05-07 08:24:22","title":"Adaptive Graph Normalized Sign Algorithm","abstract":"Efficient and robust prediction of graph signals is challenging when the signals are under impulsive noise and have missing data. Exploiting graph signal processing (GSP) and leveraging the simplicity of the classical adaptive sign algorithm, we propose an adaptive algorithm on graphs named the Graph Normalized Sign (GNS). GNS approximated a normalization term into the update, therefore achieving faster convergence and lower error compared to previous adaptive GSP algorithms. In the task of the online prediction of multivariate temperature data under impulsive noise, GNS outputs fast and robust predictions.","sentences":["Efficient and robust prediction of graph signals is challenging when the signals are under impulsive noise and have missing data.","Exploiting graph signal processing (GSP) and leveraging the simplicity of the classical adaptive sign algorithm, we propose an adaptive algorithm on graphs named the Graph Normalized Sign (GNS).","GNS approximated a normalization term into the update, therefore achieving faster convergence and lower error compared to previous adaptive GSP algorithms.","In the task of the online prediction of multivariate temperature data under impulsive noise, GNS outputs fast and robust predictions."],"url":"http://arxiv.org/abs/2405.04107v1","category":"eess.SP"}
{"created":"2024-05-07 08:22:36","title":"On self-dual Carrollian conformal nonlinear electrodynamics","abstract":"In this work, we study the duality symmetry group of Carrollian (nonlinear) electrodynamics and propose a family of Carrollian ModMax theories, which are invariant under Carrollian $\\text{SO}(2)$ electromagnetic (EM) duality transformations and conformal transformation. We define the Carrollian $\\text{SO}(2)$ EM transformations, with the help of Hodge duality in Carrollian geometry, then we rederive the Gaillard-Zumino consistency condition for EM duality of Carrollian nonlinear electrodynamics. Together with the traceless condition for the energy-momentum tensor, we are able to determine the Lagrangian of the Carrollian ModMax theories among pure electrodynamics. We furthermore study their behaviors under the $\\sqrt{T\\bar{T}}$ deformation flow, and show that these theories deform to each other and may reach two endpoints under the flow, with one of the endpoint being the Carrollian Maxwell theory. As a byproduct, we construct a family of two-dimensional Carrollian ModMax-like multiple scalar theories, which are closed under the $\\sqrt{T\\bar{T}}$ flow and may flow to a BMS free multi-scalar model.","sentences":["In this work, we study the duality symmetry group of Carrollian (nonlinear) electrodynamics and propose a family of Carrollian ModMax theories, which are invariant under Carrollian $\\text{SO}(2)$ electromagnetic (EM) duality transformations and conformal transformation.","We define the Carrollian $\\text{SO}(2)$ EM transformations, with the help of Hodge duality in Carrollian geometry, then we rederive the Gaillard-Zumino consistency condition for EM duality of Carrollian nonlinear electrodynamics.","Together with the traceless condition for the energy-momentum tensor, we are able to determine the Lagrangian of the Carrollian ModMax theories among pure electrodynamics.","We furthermore study their behaviors under the $\\sqrt{T\\bar{T}}$ deformation flow, and show that these theories deform to each other and may reach two endpoints under the flow, with one of the endpoint being the Carrollian Maxwell theory.","As a byproduct, we construct a family of two-dimensional Carrollian ModMax-like multiple scalar theories, which are closed under the $\\sqrt{T\\bar{T}}$ flow and may flow to a BMS free multi-scalar model."],"url":"http://arxiv.org/abs/2405.04105v1","category":"hep-th"}
{"created":"2024-05-07 07:46:15","title":"RTModel: a platform for real-time modeling and massive analysis of microlensing events","abstract":"Microlensing of stars in our Galaxy has long been used to detect and characterize stellar populations, exoplanets, brown dwarfs, stellar remnants and whatever objects may magnify the source stars with their gravitational fields. The interpretation of microlensing light curves is relatively simple for single lenses and single sources but becomes more and more complicated if we add more objects and take their relative motion into account. RTModel is a modeling platform that has been very active in the real-time investigation of microlensing events, providing preliminary models that have proven very useful for driving follow-up resources towards the most interesting events. The success of RTModel is due to the ability to make a thorough and aimed exploration of the parameter space in a relatively short time. This is obtained by three key ideas: the initial conditions are chosen from a template library including all possible caustic crossing and approaches; the fits are performed by the Levenberg-Marquardt algorithm using a bumper mechanism to explore multiple minima; the basic computations of microlensing magnification are performed by the fast and robust VBBinaryLensing package. In this paper we will illustrate all algorithms in RTModel in detail, with the purpose of fostering new ideas in view of future microlensing pipelines aimed at massive microlensing analysis.","sentences":["Microlensing of stars in our Galaxy has long been used to detect and characterize stellar populations, exoplanets, brown dwarfs, stellar remnants and whatever objects may magnify the source stars with their gravitational fields.","The interpretation of microlensing light curves is relatively simple for single lenses and single sources but becomes more and more complicated if we add more objects and take their relative motion into account.","RTModel is a modeling platform that has been very active in the real-time investigation of microlensing events, providing preliminary models that have proven very useful for driving follow-up resources towards the most interesting events.","The success of RTModel is due to the ability to make a thorough and aimed exploration of the parameter space in a relatively short time.","This is obtained by three key ideas: the initial conditions are chosen from a template library including all possible caustic crossing and approaches; the fits are performed by the Levenberg-Marquardt algorithm using a bumper mechanism to explore multiple minima; the basic computations of microlensing magnification are performed by the fast and robust VBBinaryLensing package.","In this paper we will illustrate all algorithms in RTModel in detail, with the purpose of fostering new ideas in view of future microlensing pipelines aimed at massive microlensing analysis."],"url":"http://arxiv.org/abs/2405.04092v1","category":"astro-ph.IM"}
{"created":"2024-05-07 07:43:17","title":"Bidirectional cascaded superfluorescent lasing in air enabled by resonant third harmonic photon exchange from nitrogen to argon","abstract":"Cavity-free lasing in atmospheric air has stimulated intense research towards fundamental understanding of underlying physical mechanisms. In this Letter, we identify a new mechanism -- third harmonic photon mediated resonant energy transfer pathway leading to population inversion in argon via initial three-photon excitation of nitrogen molecules irradiated by intense 261 nm pulses -- that enables bidirectional two-color cascaded lasing in atmospheric air. By making pump-probe measurements, we conclusively show that such cascaded lasing results from superfluorescence (SF) rather than amplified spontaneous emission (ASE). Such cascaded lasing with the capability of producing bidirectional multicolor coherent pulses opens additional possibilities for remote sensing applications.","sentences":["Cavity-free lasing in atmospheric air has stimulated intense research towards fundamental understanding of underlying physical mechanisms.","In this Letter, we identify a new mechanism -- third harmonic photon mediated resonant energy transfer pathway leading to population inversion in argon via initial three-photon excitation of nitrogen molecules irradiated by intense 261 nm pulses -- that enables bidirectional two-color cascaded lasing in atmospheric air.","By making pump-probe measurements, we conclusively show that such cascaded lasing results from superfluorescence (SF) rather than amplified spontaneous emission (ASE).","Such cascaded lasing with the capability of producing bidirectional multicolor coherent pulses opens additional possibilities for remote sensing applications."],"url":"http://arxiv.org/abs/2405.04089v1","category":"physics.optics"}
{"created":"2024-05-07 07:19:49","title":"Precise Large Deviations For The Total Population Of Heavy-Tailed Subcritical Branching Process With Immigration","abstract":"In this article we focus on the partial sum $S_{n}=X_{1}+\\cdots+X_{n}$ of the subcritical branching process with immigration $\\{X_{n}\\}_{n\\in\\mathbb{N_{+}}}$, under the condition that one of the offspring $\\xi$ or immigration $\\eta$ is regularly varying. The tail distribution of $S_n$ is heavily dependent on that of $\\xi$ and $\\eta$, and a precise large deviation probability for $S_{n}$ is specified. (i)When the tail of offspring $\\xi$ is lighter than immigration $\\eta$, uniformly for $x\\geq x_{n}$, $P(S_{n}-ES_{n}>x)\\sim c_{1}nP(\\eta>x)$ with some constant $c_{1}$ and sequence $\\{x_{n}\\}$, where $c_{1}$ is only related to the mean of offspring; (ii) When the tail of immigration $\\eta$ is not heavier than offspring $\\xi$, uniformly for $x\\geq x_{n}$,$P(S_{n} ES_{n}>x)\\sim c_{2}nP(\\xi>x)$ with some constant $c_{2}$ and sequence $\\{x_{n}\\}$, where $c_{2}$ is related to both the mean of offspring and the mean of immigration.","sentences":["In this article we focus on the partial sum $S_{n}=X_{1}+\\cdots+X_{n}$ of the subcritical branching process with immigration $\\{X_{n}\\}_{n\\in\\mathbb{N_{+}}}$, under the condition that one of the offspring $\\xi$ or immigration $\\eta$ is regularly varying.","The tail distribution of $S_n$ is heavily dependent on that of $\\xi$ and $\\eta$, and a precise large deviation probability for $S_{n}$ is specified.","(i)When the tail of offspring $\\xi$ is lighter than immigration $\\eta$, uniformly for $x\\geq x_{n}$, $P(S_{n}-ES_{n}>x)\\sim c_{1}nP(\\eta>x)$ with some constant $c_{1}$ and sequence $\\{x_{n}\\}$, where $c_{1}$ is only related to the mean of offspring; (ii) When the tail of immigration $\\eta$ is not heavier than offspring $\\xi$, uniformly for $x\\geq x_{n}$,$P(S_{n} ES_{n}>x)\\sim c_{2}nP(\\xi>x)$","with some constant $c_{2}$ and sequence $\\{x_{n}\\}$, where $c_{2}$ is related to both the mean of offspring and the mean of immigration."],"url":"http://arxiv.org/abs/2405.04073v1","category":"math.PR"}
{"created":"2024-05-07 07:00:19","title":"On the irreducibility of $f(2^n,3^m,X)$ and other such polynomials","abstract":"Let $f(t_1, \\ldots, t_r, X)\\in \\mathbb{Z}[t_1, \\ldots, t_r,X]$ be irreducible and let $a_1, \\ldots, a_r\\in \\mathbb{Z} \\smallsetminus \\{0,\\pm 1\\}$. Under a necessary ramification assumption on $f$, and conditionally on the Generalized Riemann Hypothesis, we show that for almost all integers $n_1, \\ldots, n_r$, the polynomial $f(a_1^{n_1}, \\ldots, a_r^{n_r}, X)$ is irreducible in $\\mathbb{Q}[X]$.","sentences":["Let $f(t_1, \\ldots, t_r, X)\\in \\mathbb{Z}[t_1, \\ldots, t_r,X]$ be irreducible and let $a_1, \\ldots, a_r\\in \\mathbb{Z} \\smallsetminus \\{0,\\pm 1\\}$. Under a necessary ramification assumption on $f$, and conditionally on the Generalized Riemann Hypothesis, we show that for almost all integers $n_1, \\ldots, n_r$, the polynomial $f(a_1^{n_1}, \\ldots, a_r^{n_r}, X)$ is irreducible in $\\mathbb{Q}[X]$."],"url":"http://arxiv.org/abs/2405.04058v1","category":"math.NT"}
{"created":"2024-05-07 06:55:10","title":"What Impacts the Quality of the User Answers when Asked about the Current Context?","abstract":"Sensor data provide an objective view of reality but fail to capture the subjective motivations behind an individual's behavior. This latter information is crucial for learning about the various dimensions of the personal context, thus increasing predictability. The main limitation is the human input, which is often not of the quality that is needed. The work so far has focused on the usually high number of missing answers. The focus of this paper is on \\textit{the number of mistakes} made when answering questions. Three are the main contributions of this paper. First, we show that the user's reaction time, i.e., the time before starting to respond, is the main cause of a low answer quality, where its effects are both direct and indirect, the latter relating to its impact on the completion time, i.e., the time taken to compile the response. Second, we identify the specific exogenous (e.g., the situational or temporal context) and endogenous (e.g., mood, personality traits) factors which have an influence on the reaction time, as well as on the completion time. Third, we show how reaction and completion time compose their effects on the answer quality. The paper concludes with a set of actionable recommendations.","sentences":["Sensor data provide an objective view of reality but fail to capture the subjective motivations behind an individual's behavior.","This latter information is crucial for learning about the various dimensions of the personal context, thus increasing predictability.","The main limitation is the human input, which is often not of the quality that is needed.","The work so far has focused on the usually high number of missing answers.","The focus of this paper is on \\textit{the number of mistakes} made when answering questions.","Three are the main contributions of this paper.","First, we show that the user's reaction time, i.e., the time before starting to respond, is the main cause of a low answer quality, where its effects are both direct and indirect, the latter relating to its impact on the completion time, i.e., the time taken to compile the response.","Second, we identify the specific exogenous (e.g., the situational or temporal context) and endogenous (e.g., mood, personality traits) factors which have an influence on the reaction time, as well as on the completion time.","Third, we show how reaction and completion time compose their effects on the answer quality.","The paper concludes with a set of actionable recommendations."],"url":"http://arxiv.org/abs/2405.04054v1","category":"cs.HC"}
{"created":"2024-05-07 06:33:20","title":"Uniform-in-time estimates for mean-field type SDEs and applications","abstract":"Via constructing an asymptotic coupling by reflection, in this paper we establish uniform-in-time estimates on probability distances for mean-field type SDEs, where the drift terms under consideration are dissipative merely in the long distance. As applications, we (i) explore the long time probability distance estimate between an SDE and its delay version; (ii) investigate the issue on uniform-in-time propagation of chaos for McKean-Vlasov SDEs, where the drifts might be singular with respect to the spatial variables and need not to be of convolution type; (iii) tackle the discretization error bounds in an infinite-time horizon for stochastic algorithms (e.g. backward/tamed/adaptive Euler-Maruyama schemes as three typical candidates) associated with McKean-Vlasov SDEs.","sentences":["Via constructing an asymptotic coupling by reflection, in this paper we establish uniform-in-time estimates on probability distances for mean-field type SDEs, where the drift terms under consideration are dissipative merely in the long distance.","As applications, we (i) explore the long time probability distance estimate between an SDE and its delay version; (ii) investigate the issue on uniform-in-time propagation of chaos for McKean-Vlasov SDEs, where the drifts might be singular with respect to the spatial variables and need not to be of convolution type; (iii) tackle the discretization error bounds in an infinite-time horizon for stochastic algorithms (e.g. backward/tamed/adaptive Euler-Maruyama schemes as three typical candidates) associated with McKean-Vlasov SDEs."],"url":"http://arxiv.org/abs/2405.04047v1","category":"math.PR"}
{"created":"2024-05-07 06:17:06","title":"Self-Replicating and Self-Employed Smart Contract on Ethereum Blockchain","abstract":"Blockchain is the underlying technology for cryptocurrencies such as Bitcoin. Blockchain is a robust distributed ledger that uses consensus algorithms to approve transactions in a decentralized manner, making malicious tampering extremely difficult. Ethereum, one of the blockchains, can be seen as an unstoppable computer which shared by users around the world that can run Turing-complete programs. In order to run any program on Ethereum, Ether (currency on Ethereum) is required. In other words, Ether can be seen as a kind of energy in the Ethereum world. We developed self-replicating and self-employed agents who earn the energy by themselves to replicate them, on the Ethereum blockchain. The agents can issued their token and gain Ether each time the tokens are sold. When a certain amount of Ether is accumulated, the agent replicates itself and leaves offspring. The goal of this project is to implement artificial agents that lives for itself, not as a tool for humans, in the open cyber space connected to the real world.","sentences":["Blockchain is the underlying technology for cryptocurrencies such as Bitcoin.","Blockchain is a robust distributed ledger that uses consensus algorithms to approve transactions in a decentralized manner, making malicious tampering extremely difficult.","Ethereum, one of the blockchains, can be seen as an unstoppable computer which shared by users around the world that can run Turing-complete programs.","In order to run any program on Ethereum, Ether (currency on Ethereum) is required.","In other words, Ether can be seen as a kind of energy in the Ethereum world.","We developed self-replicating and self-employed agents who earn the energy by themselves to replicate them, on the Ethereum blockchain.","The agents can issued their token and gain Ether each time the tokens are sold.","When a certain amount of Ether is accumulated, the agent replicates itself and leaves offspring.","The goal of this project is to implement artificial agents that lives for itself, not as a tool for humans, in the open cyber space connected to the real world."],"url":"http://arxiv.org/abs/2405.04038v1","category":"cs.NE"}
{"created":"2024-05-07 06:03:10","title":"Enabling Privacy-Preserving and Publicly Auditable Federated Learning","abstract":"Federated learning (FL) has attracted widespread attention because it supports the joint training of models by multiple participants without moving private dataset. However, there are still many security issues in FL that deserve discussion. In this paper, we consider three major issues: 1) how to ensure that the training process can be publicly audited by any third party; 2) how to avoid the influence of malicious participants on training; 3) how to ensure that private gradients and models are not leaked to third parties. Many solutions have been proposed to address these issues, while solving the above three problems simultaneously is seldom considered. In this paper, we propose a publicly auditable and privacy-preserving federated learning scheme that is resistant to malicious participants uploading gradients with wrong directions and enables anyone to audit and verify the correctness of the training process. In particular, we design a robust aggregation algorithm capable of detecting gradients with wrong directions from malicious participants. Then, we design a random vector generation algorithm and combine it with zero sharing and blockchain technologies to make the joint training process publicly auditable, meaning anyone can verify the correctness of the training. Finally, we conduct a series of experiments, and the experimental results show that the model generated by the protocol is comparable in accuracy to the original FL approach while keeping security advantages.","sentences":["Federated learning (FL) has attracted widespread attention because it supports the joint training of models by multiple participants without moving private dataset.","However, there are still many security issues in FL that deserve discussion.","In this paper, we consider three major issues: 1) how to ensure that the training process can be publicly audited by any third party; 2) how to avoid the influence of malicious participants on training; 3) how to ensure that private gradients and models are not leaked to third parties.","Many solutions have been proposed to address these issues, while solving the above three problems simultaneously is seldom considered.","In this paper, we propose a publicly auditable and privacy-preserving federated learning scheme that is resistant to malicious participants uploading gradients with wrong directions and enables anyone to audit and verify the correctness of the training process.","In particular, we design a robust aggregation algorithm capable of detecting gradients with wrong directions from malicious participants.","Then, we design a random vector generation algorithm and combine it with zero sharing and blockchain technologies to make the joint training process publicly auditable, meaning anyone can verify the correctness of the training.","Finally, we conduct a series of experiments, and the experimental results show that the model generated by the protocol is comparable in accuracy to the original FL approach while keeping security advantages."],"url":"http://arxiv.org/abs/2405.04029v1","category":"cs.CR"}
{"created":"2024-05-07 05:58:44","title":"Optimal Group Fair Classifiers from Linear Post-Processing","abstract":"We propose a post-processing algorithm for fair classification that mitigates model bias under a unified family of group fairness criteria covering statistical parity, equal opportunity, and equalized odds, applicable to multi-class problems and both attribute-aware and attribute-blind settings. It achieves fairness by re-calibrating the output score of the given base model with a \"fairness cost\" -- a linear combination of the (predicted) group memberships. Our algorithm is based on a representation result showing that the optimal fair classifier can be expressed as a linear post-processing of the loss function and the group predictor, derived via using these as sufficient statistics to reformulate the fair classification problem as a linear program. The parameters of the post-processor are estimated by solving the empirical LP. Experiments on benchmark datasets show the efficiency and effectiveness of our algorithm at reducing disparity compared to existing algorithms, including in-processing, especially on larger problems.","sentences":["We propose a post-processing algorithm for fair classification that mitigates model bias under a unified family of group fairness criteria covering statistical parity, equal opportunity, and equalized odds, applicable to multi-class problems and both attribute-aware and attribute-blind settings.","It achieves fairness by re-calibrating the output score of the given base model with a \"fairness cost\" -- a linear combination of the (predicted) group memberships.","Our algorithm is based on a representation result showing that the optimal fair classifier can be expressed as a linear post-processing of the loss function and the group predictor, derived via using these as sufficient statistics to reformulate the fair classification problem as a linear program.","The parameters of the post-processor are estimated by solving the empirical LP.","Experiments on benchmark datasets show the efficiency and effectiveness of our algorithm at reducing disparity compared to existing algorithms, including in-processing, especially on larger problems."],"url":"http://arxiv.org/abs/2405.04025v1","category":"cs.LG"}
{"created":"2024-05-07 05:55:50","title":"Lumbar Spine Tumor Segmentation and Localization in T2 MRI Images Using AI","abstract":"In medical imaging, segmentation and localization of spinal tumors in three-dimensional (3D) space pose significant computational challenges, primarily stemming from limited data availability. In response, this study introduces a novel data augmentation technique, aimed at automating spine tumor segmentation and localization through AI approaches. Leveraging a fusion of fuzzy c-means clustering and Random Forest algorithms, the proposed method achieves successful spine tumor segmentation based on predefined masks initially delineated by domain experts in medical imaging. Subsequently, a Convolutional Neural Network (CNN) architecture is employed for tumor classification. Moreover, 3D vertebral segmentation and labeling techniques are used to help pinpoint the exact location of the tumors in the lumbar spine. Results indicate a remarkable performance, with 99% accuracy for tumor segmentation, 98% accuracy for tumor classification, and 99% accuracy for tumor localization achieved with the proposed approach. These metrics surpass the efficacy of existing state-of-the-art techniques, as evidenced by superior Dice Score, Class Accuracy, and Intersection over Union (IOU) on class accuracy metrics. This innovative methodology holds promise for enhancing the diagnostic capabilities in detecting and characterizing spinal tumors, thereby facilitating more effective clinical decision-making.","sentences":["In medical imaging, segmentation and localization of spinal tumors in three-dimensional (3D) space pose significant computational challenges, primarily stemming from limited data availability.","In response, this study introduces a novel data augmentation technique, aimed at automating spine tumor segmentation and localization through AI approaches.","Leveraging a fusion of fuzzy c-means clustering and Random Forest algorithms, the proposed method achieves successful spine tumor segmentation based on predefined masks initially delineated by domain experts in medical imaging.","Subsequently, a Convolutional Neural Network (CNN) architecture is employed for tumor classification.","Moreover, 3D vertebral segmentation and labeling techniques are used to help pinpoint the exact location of the tumors in the lumbar spine.","Results indicate a remarkable performance, with 99% accuracy for tumor segmentation, 98% accuracy for tumor classification, and 99% accuracy for tumor localization achieved with the proposed approach.","These metrics surpass the efficacy of existing state-of-the-art techniques, as evidenced by superior Dice Score, Class Accuracy, and Intersection over Union (IOU) on class accuracy metrics.","This innovative methodology holds promise for enhancing the diagnostic capabilities in detecting and characterizing spinal tumors, thereby facilitating more effective clinical decision-making."],"url":"http://arxiv.org/abs/2405.04023v1","category":"eess.IV"}
{"created":"2024-05-07 05:29:55","title":"An Improved Finite-time Analysis of Temporal Difference Learning with Deep Neural Networks","abstract":"Temporal difference (TD) learning algorithms with neural network function parameterization have well-established empirical success in many practical large-scale reinforcement learning tasks. However, theoretical understanding of these algorithms remains challenging due to the nonlinearity of the action-value approximation. In this paper, we develop an improved non-asymptotic analysis of the neural TD method with a general $L$-layer neural network. New proof techniques are developed and an improved new $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ sample complexity is derived. To our best knowledge, this is the first finite-time analysis of neural TD that achieves an $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ complexity under the Markovian sampling, as opposed to the best known $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ complexity in the existing literature.","sentences":["Temporal difference (TD) learning algorithms with neural network function parameterization have well-established empirical success in many practical large-scale reinforcement learning tasks.","However, theoretical understanding of these algorithms remains challenging due to the nonlinearity of the action-value approximation.","In this paper, we develop an improved non-asymptotic analysis of the neural TD method with a general $L$-layer neural network.","New proof techniques are developed and an improved new $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ sample complexity is derived.","To our best knowledge, this is the first finite-time analysis of neural TD that achieves an $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ complexity under the Markovian sampling, as opposed to the best known $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ complexity in the existing literature."],"url":"http://arxiv.org/abs/2405.04017v1","category":"cs.LG"}
{"created":"2024-05-07 05:12:35","title":"Latency and Energy Minimization in NOMA-Assisted MEC Network: A Federated Deep Reinforcement Learning Approach","abstract":"Multi-access edge computing (MEC) is seen as a vital component of forthcoming 6G wireless networks, aiming to support emerging applications that demand high service reliability and low latency. However, ensuring the ultra-reliable and low-latency performance of MEC networks poses a significant challenge due to uncertainties associated with wireless links, constraints imposed by communication and computing resources, and the dynamic nature of network traffic. Enabling ultra-reliable and low-latency MEC mandates efficient load balancing jointly with resource allocation. In this paper, we investigate the joint optimization problem of offloading decisions, computation and communication resource allocation to minimize the expected weighted sum of delivery latency and energy consumption in a non-orthogonal multiple access (NOMA)-assisted MEC network. Given the formulated problem is a mixed-integer non-linear programming (MINLP), a new multi-agent federated deep reinforcement learning (FDRL) solution based on double deep Q-network (DDQN) is developed to efficiently optimize the offloading strategies across the MEC network while accelerating the learning process of the Internet-of-Thing (IoT) devices. Simulation results show that the proposed FDRL scheme can effectively reduce the weighted sum of delivery latency and energy consumption of IoT devices in the MEC network and outperform the baseline approaches.","sentences":["Multi-access edge computing (MEC) is seen as a vital component of forthcoming 6G wireless networks, aiming to support emerging applications that demand high service reliability and low latency.","However, ensuring the ultra-reliable and low-latency performance of MEC networks poses a significant challenge due to uncertainties associated with wireless links, constraints imposed by communication and computing resources, and the dynamic nature of network traffic.","Enabling ultra-reliable and low-latency MEC mandates efficient load balancing jointly with resource allocation.","In this paper, we investigate the joint optimization problem of offloading decisions, computation and communication resource allocation to minimize the expected weighted sum of delivery latency and energy consumption in a non-orthogonal multiple access (NOMA)-assisted MEC network.","Given the formulated problem is a mixed-integer non-linear programming (MINLP), a new multi-agent federated deep reinforcement learning (FDRL) solution based on double deep Q-network (DDQN) is developed to efficiently optimize the offloading strategies across the MEC network while accelerating the learning process of the Internet-of-Thing (IoT) devices.","Simulation results show that the proposed FDRL scheme can effectively reduce the weighted sum of delivery latency and energy consumption of IoT devices in the MEC network and outperform the baseline approaches."],"url":"http://arxiv.org/abs/2405.04012v1","category":"eess.SY"}
{"created":"2024-05-07 04:59:19","title":"Explainability-Informed Targeted Malware Misclassification","abstract":"In recent years, there has been a surge in malware attacks across critical infrastructures, requiring further research and development of appropriate response and remediation strategies in malware detection and classification. Several works have used machine learning models for malware classification into categories, and deep neural networks have shown promising results. However, these models have shown its vulnerabilities against intentionally crafted adversarial attacks, which yields misclassification of a malicious file. Our paper explores such adversarial vulnerabilities of neural network based malware classification system in the dynamic and online analysis environments. To evaluate our approach, we trained Feed Forward Neural Networks (FFNN) to classify malware categories based on features obtained from dynamic and online analysis environments. We use the state-of-the-art method, SHapley Additive exPlanations (SHAP), for the feature attribution for malware classification, to inform the adversarial attackers about the features with significant importance on classification decision. Using the explainability-informed features, we perform targeted misclassification adversarial white-box evasion attacks using the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks against the trained classifier. Our results demonstrated high evasion rate for some instances of attacks, showing a clear vulnerability of a malware classifier for such attacks. We offer recommendations for a balanced approach and a benchmark for much-needed future research into evasion attacks against malware classifiers, and develop more robust and trustworthy solutions.","sentences":["In recent years, there has been a surge in malware attacks across critical infrastructures, requiring further research and development of appropriate response and remediation strategies in malware detection and classification.","Several works have used machine learning models for malware classification into categories, and deep neural networks have shown promising results.","However, these models have shown its vulnerabilities against intentionally crafted adversarial attacks, which yields misclassification of a malicious file.","Our paper explores such adversarial vulnerabilities of neural network based malware classification system in the dynamic and online analysis environments.","To evaluate our approach, we trained Feed Forward Neural Networks (FFNN) to classify malware categories based on features obtained from dynamic and online analysis environments.","We use the state-of-the-art method, SHapley Additive exPlanations (SHAP), for the feature attribution for malware classification, to inform the adversarial attackers about the features with significant importance on classification decision.","Using the explainability-informed features, we perform targeted misclassification adversarial white-box evasion attacks using the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks against the trained classifier.","Our results demonstrated high evasion rate for some instances of attacks, showing a clear vulnerability of a malware classifier for such attacks.","We offer recommendations for a balanced approach and a benchmark for much-needed future research into evasion attacks against malware classifiers, and develop more robust and trustworthy solutions."],"url":"http://arxiv.org/abs/2405.04010v1","category":"cs.CR"}
{"created":"2024-05-07 04:55:25","title":"Accreting Schwarzschild-like compact object: Plasma-photon interaction and stability","abstract":"Accretion is a common phenomenon associated with any astrophysical compact object, which is best described by plasma, a state of matter composed of electrons and heavy ions. In this paper, we analyze the linear dynamics of electromagnetic (EM) fields propagating through the accreting plasma around a static and spherically symmetric horizon-less, exotic compact object (ECO). The general equations governing the propagation of EM waves in such a background exhibits quasi-bound states, with a characteristic oscillation around the BH values, for both the axial and the polar modes, as well as for homogeneous and inhomogeneous plasma distributions. The amplitude of these oscillations depend on the non-zero reflectivity of the surface of the compact object, while the oscillation length depends on its compactness. This results into slower decay of the quasi-bound states with time for certain parameter space of the plasma frequency, compared to BHs, making these ECOs more prone to instabilities.","sentences":["Accretion is a common phenomenon associated with any astrophysical compact object, which is best described by plasma, a state of matter composed of electrons and heavy ions.","In this paper, we analyze the linear dynamics of electromagnetic (EM) fields propagating through the accreting plasma around a static and spherically symmetric horizon-less, exotic compact object (ECO).","The general equations governing the propagation of EM waves in such a background exhibits quasi-bound states, with a characteristic oscillation around the BH values, for both the axial and the polar modes, as well as for homogeneous and inhomogeneous plasma distributions.","The amplitude of these oscillations depend on the non-zero reflectivity of the surface of the compact object, while the oscillation length depends on its compactness.","This results into slower decay of the quasi-bound states with time for certain parameter space of the plasma frequency, compared to BHs, making these ECOs more prone to instabilities."],"url":"http://arxiv.org/abs/2405.04006v1","category":"gr-qc"}
{"created":"2024-05-07 04:19:15","title":"Revisiting Kinetic Monte Carlo Algorithms for Time-dependent Processes: from open-loop control to feedback control","abstract":"Simulating stochastic systems with feedback control is challenging due to the complex interplay between the system's dynamics and the feedback-dependent control protocols. We present a single-step-trajectory probability analysis to time-dependent stochastic systems. Based on this analysis, we revisit several time-dependent kinetic Monte Carlo (KMC) algorithms designed for systems under open-loop-control protocols. Our analysis provides an unified alternative proof to these algorithms, summarized into a pedagogical tutorial. Moreover, with the trajectory probability analysis, we present a novel feedback-controlled KMC algorithm that accurately captures the dynamics systems controlled by external signal based on measurements of the system's state. Our method correctly captures the system dynamics and avoids the artificial Zeno effect that arises from incorrectly applying the direct Gillespie algorithm to feedback-controlled systems. This work provides a unified perspective on existing open-loop-control KMC algorithms and also offers a powerful and accurate tool for simulating stochastic systems with feedback control.","sentences":["Simulating stochastic systems with feedback control is challenging due to the complex interplay between the system's dynamics and the feedback-dependent control protocols.","We present a single-step-trajectory probability analysis to time-dependent stochastic systems.","Based on this analysis, we revisit several time-dependent kinetic Monte Carlo (KMC) algorithms designed for systems under open-loop-control protocols.","Our analysis provides an unified alternative proof to these algorithms, summarized into a pedagogical tutorial.","Moreover, with the trajectory probability analysis, we present a novel feedback-controlled KMC algorithm that accurately captures the dynamics systems controlled by external signal based on measurements of the system's state.","Our method correctly captures the system dynamics and avoids the artificial Zeno effect that arises from incorrectly applying the direct Gillespie algorithm to feedback-controlled systems.","This work provides a unified perspective on existing open-loop-control KMC algorithms and also offers a powerful and accurate tool for simulating stochastic systems with feedback control."],"url":"http://arxiv.org/abs/2405.03997v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-07 04:19:07","title":"Finite modular majoron","abstract":"We point out that the accidental $U(1)_{B-L}$ symmetry can arise from a finite modular symmetry $\\Gamma_N$ in the type-I seesaw. The finite modular symmetry is spontaneously broken in such a way that the residual $\\mathbb{Z}^T_N$ discrete symmetry, associated with the $T$-transformation which shifts the modulus $\\tau \\to \\tau+ 1$, remains unbroken. This discrete $\\mathbb{Z}^T_N$ symmetry mimics $U(1)_{B-L}$, and hence the majoron appears as a pseudo Nambu-Goldstone boson of $U(1)_{B-L}$. Without introducing additional interactions, the modulus $\\tau$ can be stabilized by the Coleman-Weinberg (CW) potential given by the Majorana mass terms of the right-handed neutrinos. We study cosmological implications of the majoron, with particular interests in the dark matter and dark radiation, where the latter may alleviate the Hubble tension. We also find that the CW potential can have a wide range of nearly exponential shape which prevents $\\tau$ from overshooting, and makes the amount of dark radiation not too large.","sentences":["We point out that the accidental $U(1)_{B-L}$ symmetry can arise from a finite modular symmetry $\\Gamma_N$ in the type-I seesaw.","The finite modular symmetry is spontaneously broken in such a way that the residual $\\mathbb{Z}^T_N$ discrete symmetry, associated with the $T$-transformation which shifts the modulus $\\tau \\to \\tau+ 1$, remains unbroken.","This discrete $\\mathbb{Z}^T_N$ symmetry mimics $U(1)_{B-L}$, and hence the majoron appears as a pseudo Nambu-Goldstone boson of $U(1)_{B-L}$. Without introducing additional interactions, the modulus $\\tau$ can be stabilized by the Coleman-Weinberg (CW) potential given by the Majorana mass terms of the right-handed neutrinos.","We study cosmological implications of the majoron, with particular interests in the dark matter and dark radiation, where the latter may alleviate the Hubble tension.","We also find that the CW potential can have a wide range of nearly exponential shape which prevents $\\tau$ from overshooting, and makes the amount of dark radiation not too large."],"url":"http://arxiv.org/abs/2405.03996v1","category":"hep-ph"}
{"created":"2024-05-07 04:11:03","title":"Research on financial fraud algorithm based on federal learning and big data technology","abstract":"With the deepening of the digitization degree of financial business, financial fraud presents more complex and hidden characteristics, which poses a severe challenge to the risk prevention and control ability of financial institutions. At the same time, the vigorous development of big data technology provides massive potential information resources, and federated learning, as an emerging distributed machine learning paradigm, can realize multi-party data collaborative modeling under the premise of protecting data privacy. This paper firstly elaborates the basic principle, advantages and unique value of federated learning in solving data silos and protecting user privacy. Aiming at the needs of financial fraud detection, this paper discusses the design of federal learning architecture suitable for this scenario, including selecting suitable model type (such as neural network), setting reasonable data partitioning and updating rules. The central theme of the dissertation revolves around the exploration and execution of an algorithm for detecting financial fraud, which is grounded in federated learning methodologies. With a federated learning framework, each participant trains the model locally and exchanges only model parameters rather than raw data, enabling iterative optimization of the global model while protecting data privacy. To ascertain the efficacy and superiority of the suggested algorithm, a meticulous experimental investigation is both devised and executed. A real-world financial fraud dataset is selected to compare the fraud detection performance using traditional centralized learning and federated learning. The findings from the experiments reveal that the federated learning-based financial fraud algorithm achieves a substantial reduction in the likelihood of data privacy breaches without compromising on high detection accuracies.","sentences":["With the deepening of the digitization degree of financial business, financial fraud presents more complex and hidden characteristics, which poses a severe challenge to the risk prevention and control ability of financial institutions.","At the same time, the vigorous development of big data technology provides massive potential information resources, and federated learning, as an emerging distributed machine learning paradigm, can realize multi-party data collaborative modeling under the premise of protecting data privacy.","This paper firstly elaborates the basic principle, advantages and unique value of federated learning in solving data silos and protecting user privacy.","Aiming at the needs of financial fraud detection, this paper discusses the design of federal learning architecture suitable for this scenario, including selecting suitable model type (such as neural network), setting reasonable data partitioning and updating rules.","The central theme of the dissertation revolves around the exploration and execution of an algorithm for detecting financial fraud, which is grounded in federated learning methodologies.","With a federated learning framework, each participant trains the model locally and exchanges only model parameters rather than raw data, enabling iterative optimization of the global model while protecting data privacy.","To ascertain the efficacy and superiority of the suggested algorithm, a meticulous experimental investigation is both devised and executed.","A real-world financial fraud dataset is selected to compare the fraud detection performance using traditional centralized learning and federated learning.","The findings from the experiments reveal that the federated learning-based financial fraud algorithm achieves a substantial reduction in the likelihood of data privacy breaches without compromising on high detection accuracies."],"url":"http://arxiv.org/abs/2405.03992v1","category":"cs.CE"}
{"created":"2024-05-07 03:55:57","title":"Navigating Chemical Space with Latent Flows","abstract":"Recent progress of deep generative models in the vision and language domain has stimulated significant interest in more structured data generation such as molecules. However, beyond generating new random molecules, efficient exploration and a comprehensive understanding of the vast chemical space are of great importance to molecular science and applications in drug design and materials discovery. In this paper, we propose a new framework, ChemFlow, to traverse chemical space through navigating the latent space learned by molecule generative models through flows. We introduce a dynamical system perspective that formulates the problem as learning a vector field that transports the mass of the molecular distribution to the region with desired molecular properties or structure diversity. Under this framework, we unify previous approaches on molecule latent space traversal and optimization and propose alternative competing methods incorporating different physical priors. We validate the efficacy of ChemFlow on molecule manipulation and single- and multi-objective molecule optimization tasks under both supervised and unsupervised molecular discovery settings. Codes and demos are publicly available on GitHub at https://github.com/garywei944/ChemFlow.","sentences":["Recent progress of deep generative models in the vision and language domain has stimulated significant interest in more structured data generation such as molecules.","However, beyond generating new random molecules, efficient exploration and a comprehensive understanding of the vast chemical space are of great importance to molecular science and applications in drug design and materials discovery.","In this paper, we propose a new framework, ChemFlow, to traverse chemical space through navigating the latent space learned by molecule generative models through flows.","We introduce a dynamical system perspective that formulates the problem as learning a vector field that transports the mass of the molecular distribution to the region with desired molecular properties or structure diversity.","Under this framework, we unify previous approaches on molecule latent space traversal and optimization and propose alternative competing methods incorporating different physical priors.","We validate the efficacy of ChemFlow on molecule manipulation and single- and multi-objective molecule optimization tasks under both supervised and unsupervised molecular discovery settings.","Codes and demos are publicly available on GitHub at https://github.com/garywei944/ChemFlow."],"url":"http://arxiv.org/abs/2405.03987v1","category":"cs.LG"}
{"created":"2024-05-07 03:25:24","title":"Anomalous Gate-tunable Capacitance in Graphene Moir\u00e9 Heterostructures","abstract":"Interface engineered ferroelectricity in van der Waals heterostructures is of broad interest both fundamentally and technologically for the applications in neuromorphic computing and so on. In particular, the moir\\'e ferroelectricity in graphene/hexagonal boron nitride (hBN) heterostructures driven by charge ordering instead of traditional lattice displacement has drawn considerable attention because of its fascinating properties and promising high-frequency programmable electrical polarization switching. Yet, the underlying mechanism of the electronic ferroelectricity is still under debate. On the other hand, combining the interface engineered ferroelectricity and strong correlations in moir\\'e heterostructures could enable the realization of novel quantum states such as ferroelectric superconductivity and multiferroicity. Here we study the electronic transport properties of twisted double bilayer graphene (TDBLG), aligned with one of the neighbouring hBN. We observe a strong gating hysteresis and ferroelectric-like behaviour, as well as the electronic ratchet effect. We find that the top gate is anomalously screened. On the contrary, the back gate is anomalously doubly efficient in injecting charges into graphene, that is, the effective back gate capacitance is two times larger than its geometry capacitance. This unexpected gate-tunable capacitance causes a dramatic change of electric fields between forward and backward scans. The asymmetric gating behaviours and anomalous change in capacitance could be explained with a simple model involved with a spontaneous electric polarization between top hBN and graphene. Our work provides more insights into the mysterious ferroelectricity in graphene/hBN moir\\'e heterostructures and paves the way to the understanding of the underlying mechanism.","sentences":["Interface engineered ferroelectricity in van der Waals heterostructures is of broad interest both fundamentally and technologically for the applications in neuromorphic computing and so on.","In particular, the moir\\'e ferroelectricity in graphene/hexagonal boron nitride (hBN) heterostructures driven by charge ordering instead of traditional lattice displacement has drawn considerable attention because of its fascinating properties and promising high-frequency programmable electrical polarization switching.","Yet, the underlying mechanism of the electronic ferroelectricity is still under debate.","On the other hand, combining the interface engineered ferroelectricity and strong correlations in moir\\'e heterostructures could enable the realization of novel quantum states such as ferroelectric superconductivity and multiferroicity.","Here we study the electronic transport properties of twisted double bilayer graphene (TDBLG), aligned with one of the neighbouring hBN.","We observe a strong gating hysteresis and ferroelectric-like behaviour, as well as the electronic ratchet effect.","We find that the top gate is anomalously screened.","On the contrary, the back gate is anomalously doubly efficient in injecting charges into graphene, that is, the effective back gate capacitance is two times larger than its geometry capacitance.","This unexpected gate-tunable capacitance causes a dramatic change of electric fields between forward and backward scans.","The asymmetric gating behaviours and anomalous change in capacitance could be explained with a simple model involved with a spontaneous electric polarization between top hBN and graphene.","Our work provides more insights into the mysterious ferroelectricity in graphene/hBN moir\\'e heterostructures and paves the way to the understanding of the underlying mechanism."],"url":"http://arxiv.org/abs/2405.03976v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-07 02:45:50","title":"Joint Estimation of Identity Verification and Relative Pose for Partial Fingerprints","abstract":"Currently, portable electronic devices are becoming more and more popular. For lightweight considerations, their fingerprint recognition modules usually use limited-size sensors. However, partial fingerprints have few matchable features, especially when there are differences in finger pressing posture or image quality, which makes partial fingerprint verification challenging. Most existing methods regard fingerprint position rectification and identity verification as independent tasks, ignoring the coupling relationship between them -- relative pose estimation typically relies on paired features as anchors, and authentication accuracy tends to improve with more precise pose alignment. Consequently, in this paper we propose a method that jointly estimates identity verification and relative pose for partial fingerprints, aiming to leverage their inherent correlation to improve each other. To achieve this, we propose a multi-task CNN (Convolutional Neural Network)-Transformer hybrid network, and design a pre-training task to enhance the feature extraction capability. Experiments on multiple public datasets (NIST SD14, FVC2002 DB1A & DB3A, FVC2004 DB1A & DB2A, FVC2006 DB1A) and an in-house dataset show that our method achieves state-of-the-art performance in both partial fingerprint verification and relative pose estimation, while being more efficient than previous methods.","sentences":["Currently, portable electronic devices are becoming more and more popular.","For lightweight considerations, their fingerprint recognition modules usually use limited-size sensors.","However, partial fingerprints have few matchable features, especially when there are differences in finger pressing posture or image quality, which makes partial fingerprint verification challenging.","Most existing methods regard fingerprint position rectification and identity verification as independent tasks, ignoring the coupling relationship between them -- relative pose estimation typically relies on paired features as anchors, and authentication accuracy tends to improve with more precise pose alignment.","Consequently, in this paper we propose a method that jointly estimates identity verification and relative pose for partial fingerprints, aiming to leverage their inherent correlation to improve each other.","To achieve this, we propose a multi-task CNN (Convolutional Neural Network)-Transformer hybrid network, and design a pre-training task to enhance the feature extraction capability.","Experiments on multiple public datasets (NIST SD14, FVC2002 DB1A & DB3A, FVC2004 DB1A & DB2A, FVC2006 DB1A) and an in-house dataset show that our method achieves state-of-the-art performance in both partial fingerprint verification and relative pose estimation, while being more efficient than previous methods."],"url":"http://arxiv.org/abs/2405.03959v1","category":"cs.CV"}
{"created":"2024-05-07 02:27:44","title":"Collapse of Neutrino Wave Functions under Penrose Gravitational Reduction","abstract":"Models of spontaneous wave function collapse have been postulated to address the measurement problem in quantum mechanics. Their primary function is to convert coherent quantum superpositions into incoherent ones, with the result that macroscopic objects cannot be placed into widely separated superpositions for observably prolonged times. Many of these processes will also lead to loss of coherence in neutrino oscillations, producing observable signatures in the flavor profile of neutrinos at long travel distances. The majority of studies of neutrino oscillation coherence to date have focused on variants of the continuous state localization model, whereby an effective decoherence strength parameter is used to model the rate of coherence loss with an assumed energy dependence. Another class of collapse models that have been proposed posit connections to the configuration of gravitational field accompanying the mass distribution associated with each wave function that is in the superposition. A particularly interesting and prescriptive model is Penrose's description of gravitational collapse which proposes a decoherence time $\\tau$ determined through $E_{g}\\tau\\sim\\hbar$, where $E_{g}$ is a calculable function of the Newtonian gravitational potential. Here we explore application of the Penrose collapse model to neutrino oscillations, reinterpreting previous experimental limits on neutrino decoherence in terms of this model. We identify effects associated with both spatial collapse and momentum diffusion, finding that the latter is ruled out in data from the IceCube South Pole Neutrino Observatory so long as the neutrino wave packet width at production is $\\sigma_{\\nu,x}\\leq2\\times10^{-12}$ m.","sentences":["Models of spontaneous wave function collapse have been postulated to address the measurement problem in quantum mechanics.","Their primary function is to convert coherent quantum superpositions into incoherent ones, with the result that macroscopic objects cannot be placed into widely separated superpositions for observably prolonged times.","Many of these processes will also lead to loss of coherence in neutrino oscillations, producing observable signatures in the flavor profile of neutrinos at long travel distances.","The majority of studies of neutrino oscillation coherence to date have focused on variants of the continuous state localization model, whereby an effective decoherence strength parameter is used to model the rate of coherence loss with an assumed energy dependence.","Another class of collapse models that have been proposed posit connections to the configuration of gravitational field accompanying the mass distribution associated with each wave function that is in the superposition.","A particularly interesting and prescriptive model is Penrose's description of gravitational collapse which proposes a decoherence time $\\tau$ determined through $E_{g}\\tau\\sim\\hbar$, where $E_{g}$ is a calculable function of the Newtonian gravitational potential.","Here we explore application of the Penrose collapse model to neutrino oscillations, reinterpreting previous experimental limits on neutrino decoherence in terms of this model.","We identify effects associated with both spatial collapse and momentum diffusion, finding that the latter is ruled out in data from the IceCube South Pole Neutrino Observatory so long as the neutrino wave packet width at production is $\\sigma_{\\nu,x}\\leq2\\times10^{-12}$ m."],"url":"http://arxiv.org/abs/2405.03954v1","category":"hep-ph"}
{"created":"2024-05-07 02:02:05","title":"Remark on neutrino oscillations","abstract":"The oscillations of ultra-relativistic neutrinos are realized by the propagation of assumed zero-mass on-shell neutrinos with the speed of light in vacuum combined with the phase modulation by the small mass term $\\exp[-i(m^{2}_{\\nu_{k}}/2|\\vec{p}|)\\tau]$ with a time parameter $\\tau$. This picture is realized in the first quantization by the mass expansion and in field theory by the use of $\\delta(x^{0}-y^{0}-\\tau) \\langle 0|T^{\\star}\\nu_{L k}(x)\\overline{\\nu_{L k}(y)}|0\\rangle$ with the neutrino mass eigenstates $\\nu_{L k}$ and a finite positive $\\tau$ after the contour integral of the propagating neutrino energies.   By noting that the conventional detectors are insensitive to neutrino masses, the measured energy-momenta of the initial and final states with assumed zero-mass neutrinos are conserved. The propagating neutrinos preserve the three-momentum in this sense but the energies of the massive neutrinos are conserved up to uncertainty relations and thus leading to oscillations. Conceptual complications in the case of Majorana neutrinos due to the charge conjugation in $d=4$ are also discussed.","sentences":["The oscillations of ultra-relativistic neutrinos are realized by the propagation of assumed zero-mass on-shell neutrinos with the speed of light in vacuum combined with the phase modulation by the small mass term $\\exp[-i(m^{2}_{\\nu_{k}}/2|\\vec{p}|)\\tau]$ with a time parameter $\\tau$. This picture is realized in the first quantization by the mass expansion and in field theory by the use of $\\delta(x^{0}-y^{0}-\\tau)","\\langle 0|T^{\\star}\\nu_{L k}(x)\\overline{\\nu_{L k}(y)}|0\\rangle$ with the neutrino mass eigenstates $\\nu_{L k}$ and a finite positive $\\tau$ after the contour integral of the propagating neutrino energies.   ","By noting that the conventional detectors are insensitive to neutrino masses, the measured energy-momenta of the initial and final states with assumed zero-mass neutrinos are conserved.","The propagating neutrinos preserve the three-momentum in this sense but the energies of the massive neutrinos are conserved up to uncertainty relations and thus leading to oscillations.","Conceptual complications in the case of Majorana neutrinos due to the charge conjugation in $d=4$ are also discussed."],"url":"http://arxiv.org/abs/2405.03940v1","category":"hep-ph"}
{"created":"2024-05-07 01:17:30","title":"Shape optimization for high efficiency metasurfaces: theory and implementation","abstract":"Complex non-local behavior makes designing high efficiency and multifunctional metasurfaces a significant challenge. While using libraries of meta-atoms provide a simple and fast implementation methodology, pillar to pillar interaction often imposes performance limitations. On the other extreme, inverse design based on topology optimization leverages non-local coupling to achieve high efficiency, but leads to complex and difficult to fabricate structures. In this paper, we demonstrate numerically and experimentally a shape optimization method that enables high efficiency metasurfaces while providing direct control of the structure complexity. The proposed method provides a path towards manufacturability of inverse-designed high efficiency metasurfaces.","sentences":["Complex non-local behavior makes designing high efficiency and multifunctional metasurfaces a significant challenge.","While using libraries of meta-atoms provide a simple and fast implementation methodology, pillar to pillar interaction often imposes performance limitations.","On the other extreme, inverse design based on topology optimization leverages non-local coupling to achieve high efficiency, but leads to complex and difficult to fabricate structures.","In this paper, we demonstrate numerically and experimentally a shape optimization method that enables high efficiency metasurfaces while providing direct control of the structure complexity.","The proposed method provides a path towards manufacturability of inverse-designed high efficiency metasurfaces."],"url":"http://arxiv.org/abs/2405.03930v1","category":"physics.optics"}
{"created":"2024-05-07 00:44:42","title":"Stochastic simulation of binary annihilation reactions within q-analysis formalism","abstract":"Although Gillespie's algorithm is justified under a set of axioms based on the assumption of homogeneity of the system, many chemical systems deviate from this assumption, as is the case for reactions taking place in low-mobility media. Using instead the generalized q formalism, we propose a new stochastic simulation algorithm by redefining the probability with which a mu reaction occurs between t + tao and t + tao + delta tao as P(tao q, mu) = a mu exp q(-a0 tao q), taking into account the separation of the natural exponential by the q parameter. Our algorithm has been implemented in the study of binary annihilation reactions, demonstrating a wider amplitude within the range of established physicochemical reactions, being the stochastic Gillespie scheme and the classical deterministic approach, particular cases of this new proposal. The effect of the nonextensivity parameter, q, on the reaction rate is analyzed and its relationship with the reaction order, n, and the heterogeneity parameter, h, is determined for two different reactant concentrations in the annihilation reaction. Different behaviors of these parameters are observed for the two types of samples, especially as q moves away from 1, confirming that quasi-second order reactions occur when reactant concentrations are similar and quasi-first order reactions when they are different. Empirical expressions between the classical reaction order and the degree of heterogeneity are proposed. The results obtained allow us to associate the behavior of sub and supra Arrhenius kinetics reported in other different reactions with the degree of non-extensiveness of the reaction.","sentences":["Although Gillespie's algorithm is justified under a set of axioms based on the assumption of homogeneity of the system, many chemical systems deviate from this assumption, as is the case for reactions taking place in low-mobility media.","Using instead the generalized q formalism, we propose a new stochastic simulation algorithm by redefining the probability with which a mu reaction occurs between t + tao and t + tao + delta tao as P(tao q, mu) = a mu exp q(-a0 tao q), taking into account the separation of the natural exponential by the q parameter.","Our algorithm has been implemented in the study of binary annihilation reactions, demonstrating a wider amplitude within the range of established physicochemical reactions, being the stochastic Gillespie scheme and the classical deterministic approach, particular cases of this new proposal.","The effect of the nonextensivity parameter, q, on the reaction rate is analyzed and its relationship with the reaction order, n, and the heterogeneity parameter, h, is determined for two different reactant concentrations in the annihilation reaction.","Different behaviors of these parameters are observed for the two types of samples, especially as q moves away from 1, confirming that quasi-second order reactions occur when reactant concentrations are similar and quasi-first order reactions when they are different.","Empirical expressions between the classical reaction order and the degree of heterogeneity are proposed.","The results obtained allow us to associate the behavior of sub and supra Arrhenius kinetics reported in other different reactions with the degree of non-extensiveness of the reaction."],"url":"http://arxiv.org/abs/2405.03922v1","category":"physics.chem-ph"}
{"created":"2024-05-06 23:06:34","title":"Modeling and performance analysis of Implicit Electric Field Conjugation with two deformable mirrors applied to the Roman Coronagraph","abstract":"High-order wavefront sensing and control (HOWFSC) is key to create a dark hole region within the coronagraphic image plane where high contrasts are achieved. The Roman Coronagraph is expected to perform its HOWFSC with a ground-in-the-loop scheme due to the computational complexity of the Electric Field Conjugation (EFC) algorithm. This scheme provides the flexibility to alter the HOWFSC algorithm for given science objectives. The baseline HOWFSC scheme involves running EFC while observing a bright star such as {\\zeta} Puppis to create the initial dark hole followed by a slew to the science target. The new implicit EFC (iEFC) algorithm removes the optical diffraction model from the controller, making the final contrast independent of model accuracy. While previously demonstrated with a single DM, iEFC is extended to two deformable mirror systems in order to create annular dark holes. The algorithm is then applied to the Wide-Field-of-View Shaped Pupil Coronagraph (SPC-WFOV) mode designed for the Roman Space Telescope using end-to-end physical optics models. Initial monochromatic simulations demonstrate the efficacy of iEFC as well as the optimal choice of modes for the SPC-WFOV instrument. Further simulations with a 3.6% wavefront control bandpass and a broader 10% bandpass then demonstrate that iEFC can be used in broadband scenarios to achieve contrasts below 1E-8 with Roman. Finally, an EMCCD model is implemented to estimate calibration times and predict the controller's performance. Here, 1E-8 contrasts are achieved with a calibration time of about 6.8 hours assuming the reference star is {\\zeta} Puppis. The results here indicate that iEFC can be a valid HOWFSC method that can mitigate the risk of model errors associated with space-borne coronagraphs, but to maximize iEFC performance, lengthy calibration times will be required to mitigate the noise accumulated during calibration.","sentences":["High-order wavefront sensing and control (HOWFSC) is key to create a dark hole region within the coronagraphic image plane where high contrasts are achieved.","The Roman Coronagraph is expected to perform its HOWFSC with a ground-in-the-loop scheme due to the computational complexity of the Electric Field Conjugation (EFC) algorithm.","This scheme provides the flexibility to alter the HOWFSC algorithm for given science objectives.","The baseline HOWFSC scheme involves running EFC while observing a bright star such as {\\zeta} Puppis to create the initial dark hole followed by a slew to the science target.","The new implicit EFC (iEFC) algorithm removes the optical diffraction model from the controller, making the final contrast independent of model accuracy.","While previously demonstrated with a single DM, iEFC is extended to two deformable mirror systems in order to create annular dark holes.","The algorithm is then applied to the Wide-Field-of-View Shaped Pupil Coronagraph (SPC-WFOV) mode designed for the Roman Space Telescope using end-to-end physical optics models.","Initial monochromatic simulations demonstrate the efficacy of iEFC as well as the optimal choice of modes for the SPC-WFOV instrument.","Further simulations with a 3.6% wavefront control bandpass and a broader 10% bandpass then demonstrate that iEFC can be used in broadband scenarios to achieve contrasts below 1E-8 with Roman.","Finally, an EMCCD model is implemented to estimate calibration times and predict the controller's performance.","Here, 1E-8 contrasts are achieved with a calibration time of about 6.8 hours assuming the reference star is {\\zeta} Puppis.","The results here indicate that iEFC can be a valid HOWFSC method that can mitigate the risk of model errors associated with space-borne coronagraphs, but to maximize iEFC performance, lengthy calibration times will be required to mitigate the noise accumulated during calibration."],"url":"http://arxiv.org/abs/2405.03899v1","category":"astro-ph.IM"}
{"created":"2024-05-06 22:51:48","title":"Large Effects of Small Cues: Priming Selfish Economic Decisions","abstract":"Many experimental studies report that economics students tend to act more selfishly than students of other disciplines, a finding that received widespread public and professional attention. Two main explanations that the existing literature offers for the differences found in the behavior between economists and noneconomists are the selection effect, and the indoctrination effect. We offer an alternative, novel explanation. We argue that these differences can be explained by differences in the interpretation of the context. We test this hypothesis by conducting two social dilemma experiments in the US and Israel with participants from both economics and non-economics majors. In the experiments, participants face a tradeoff between profit maximization, that is the market norm and workers welfare, that is the social norm. We use priming to manipulate the cues that the participants receive before they make their decision. We find that when participants receive cues signaling that the decision has an economic context, both economics and non-economics students tend to maximize profits. When the participants receive cues emphasizing social norms, on the other hand, both economics and non-economics students are less likely to maximize profits. We conclude that some of the differences found between the decisions of economics and non-economics students can be explained by contextual cues.","sentences":["Many experimental studies report that economics students tend to act more selfishly than students of other disciplines, a finding that received widespread public and professional attention.","Two main explanations that the existing literature offers for the differences found in the behavior between economists and noneconomists are the selection effect, and the indoctrination effect.","We offer an alternative, novel explanation.","We argue that these differences can be explained by differences in the interpretation of the context.","We test this hypothesis by conducting two social dilemma experiments in the US and Israel with participants from both economics and non-economics majors.","In the experiments, participants face a tradeoff between profit maximization, that is the market norm and workers welfare, that is the social norm.","We use priming to manipulate the cues that the participants receive before they make their decision.","We find that when participants receive cues signaling that the decision has an economic context, both economics and non-economics students tend to maximize profits.","When the participants receive cues emphasizing social norms, on the other hand, both economics and non-economics students are less likely to maximize profits.","We conclude that some of the differences found between the decisions of economics and non-economics students can be explained by contextual cues."],"url":"http://arxiv.org/abs/2405.03893v1","category":"econ.GN"}
{"created":"2024-05-06 22:23:31","title":"Measurized Discounted Markov Decision Processes","abstract":"In this paper, we build a framework that facilitates the analysis of discounted infinite horizon Markov Decision Processes (MDPs) by visualizing them as deterministic processes where the states are probability measures on the original state space and the actions are stochastic kernels on the original action space. We provide a simple general algebraic approach to lifting any MDP to this space of measures; we call this to measurize the original stochastic MDP. We show that measurized MDPs are in fact a generalization of stochastic MDPs, thus the measurized framework can be deployed without loss of fidelity. Lifting an MDP can be convenient because the measurized framework enables constraints and value function approximations that are not easily available from the standard MDP setting. For instance, one can add restrictions or build approximations based on moments, quantiles, risk measures, etc. Moreover, since the measurized counterpart to any MDP is deterministic, the measurized optimality equations trade the complexity of dealing with the expected value function that appears in the stochastic optimality equations with a more complex state space.","sentences":["In this paper, we build a framework that facilitates the analysis of discounted infinite horizon Markov Decision Processes (MDPs) by visualizing them as deterministic processes where the states are probability measures on the original state space and the actions are stochastic kernels on the original action space.","We provide a simple general algebraic approach to lifting any MDP to this space of measures; we call this to measurize the original stochastic MDP.","We show that measurized MDPs are in fact a generalization of stochastic MDPs, thus the measurized framework can be deployed without loss of fidelity.","Lifting an MDP can be convenient because the measurized framework enables constraints and value function approximations that are not easily available from the standard MDP setting.","For instance, one can add restrictions or build approximations based on moments, quantiles, risk measures, etc.","Moreover, since the measurized counterpart to any MDP is deterministic, the measurized optimality equations trade the complexity of dealing with the expected value function that appears in the stochastic optimality equations with a more complex state space."],"url":"http://arxiv.org/abs/2405.03888v1","category":"math.OC"}
{"created":"2024-05-06 21:48:24","title":"Bounds and Dualities of Type II Little String Theories","abstract":"We explore the symmetry structure of Type II Little String Theories and their T-dualities. We construct these theories both from the bottom-up perspective starting with seed Superconformal Field Theories, and from the top-down using F-/M-theory. By exploiting anomaly inflow and unitarity of the LST worldsheet theory, we derive strong conditions on the possible 6D bulk theories and their flavor algebras. These constraints continue to apply if gravity is coupled to the theory. We also study the higher form symmetry structure of these theories and show how they get exchanged under T-duality. Finally, we comment on seemingly consistent bottom-up Little String Theories that cannot be constructed from the top-down approach.","sentences":["We explore the symmetry structure of Type II Little String Theories and their T-dualities.","We construct these theories both from the bottom-up perspective starting with seed Superconformal Field Theories, and from the top-down using F-/M-theory.","By exploiting anomaly inflow and unitarity of the LST worldsheet theory, we derive strong conditions on the possible 6D bulk theories and their flavor algebras.","These constraints continue to apply if gravity is coupled to the theory.","We also study the higher form symmetry structure of these theories and show how they get exchanged under T-duality.","Finally, we comment on seemingly consistent bottom-up Little String Theories that cannot be constructed from the top-down approach."],"url":"http://arxiv.org/abs/2405.03877v1","category":"hep-th"}
{"created":"2024-05-06 21:37:09","title":"Nonlinear Schr\u00f6dinger-Poisson systems in dimension two: the zero mass case","abstract":"We provide an existence result for a Schr\\\"odinger-Poisson system in gradient form, set in the whole plane, in the case of zero mass. Since the setting is limiting for the Sobolev embedding, we admit nonlinearities with subcritical or critical growth in the sense of Trudinger-Moser. In particular, the absence of the mass term requires a nonstandard functional framework, based on homogeneous Sobolev spaces. These features, combined with the logarithmic behaviour of the kernel of the Poisson equation, make the analysis delicate, since standard variational tools cannot be applied. The system is solved by considering the corresponding logarithmic Choquard equation. We prove the existence of a mountain pass-type solution via a careful analysis on specific Cerami sequences, whose boundedness is achieved by exploiting an appropriate functional, obtained by evaluating the energy functional on particular paths.","sentences":["We provide an existence result for a Schr\\\"odinger-Poisson system in gradient form, set in the whole plane, in the case of zero mass.","Since the setting is limiting for the Sobolev embedding, we admit nonlinearities with subcritical or critical growth in the sense of Trudinger-Moser.","In particular, the absence of the mass term requires a nonstandard functional framework, based on homogeneous Sobolev spaces.","These features, combined with the logarithmic behaviour of the kernel of the Poisson equation, make the analysis delicate, since standard variational tools cannot be applied.","The system is solved by considering the corresponding logarithmic Choquard equation.","We prove the existence of a mountain pass-type solution via a careful analysis on specific Cerami sequences, whose boundedness is achieved by exploiting an appropriate functional, obtained by evaluating the energy functional on particular paths."],"url":"http://arxiv.org/abs/2405.03871v1","category":"math.AP"}
{"created":"2024-05-06 21:18:27","title":"Observation uncertainty effects on the precision of interior planetary parameters","abstract":"Determining compositions of low-mass exoplanets is essential in understanding their origins. The certainty by which masses and radius are measured affects our ability to discern planets that are rocky or volatile rich. In this study, we aim to determine sound observational strategies to avoid diminishing returns. We quantify how uncertainties in mass, radius and model assumptions propagate into errors in inferred compositions of rocky and water planets. For a target error in a planet's iron-mass fraction or water content, we calculate the corresponding required accuracies in radius and mass. For instance, a rocky planet with a known radius error of 2% (corresponding to TESS detection best errors) demands mass precision to be at 5-11% to attain a 8 wt% precision in iron-mass fraction, regardless of mass. Similarly, a water world of equal radius precision requires 9-20% mass precision to confine the water content within a 10 wt% margin. Lighter planets are more difficult to constrain, especially water-rich versus water-poor worlds. Studying Earth as an exoplanet, we find a $\\sim \\pm5$ point \"error floor\" in iron-mass fraction and $\\sim \\pm7$ in core-mass fraction from our lack of knowledge on mineralogy. The results presented here can quickly guide observing strategies to maximize insights into small exoplanet compositions while avoiding over-observing.","sentences":["Determining compositions of low-mass exoplanets is essential in understanding their origins.","The certainty by which masses and radius are measured affects our ability to discern planets that are rocky or volatile rich.","In this study, we aim to determine sound observational strategies to avoid diminishing returns.","We quantify how uncertainties in mass, radius and model assumptions propagate into errors in inferred compositions of rocky and water planets.","For a target error in a planet's iron-mass fraction or water content, we calculate the corresponding required accuracies in radius and mass.","For instance, a rocky planet with a known radius error of 2% (corresponding to TESS detection best errors) demands mass precision to be at 5-11% to attain a 8 wt% precision in iron-mass fraction, regardless of mass.","Similarly, a water world of equal radius precision requires 9-20% mass precision to confine the water content within a 10 wt% margin.","Lighter planets are more difficult to constrain, especially water-rich versus water-poor worlds.","Studying Earth as an exoplanet, we find a $\\sim \\pm5$ point \"error floor\" in iron-mass fraction and $\\sim \\pm7$ in core-mass fraction from our lack of knowledge on mineralogy.","The results presented here can quickly guide observing strategies to maximize insights into small exoplanet compositions while avoiding over-observing."],"url":"http://arxiv.org/abs/2405.03860v1","category":"astro-ph.EP"}
{"created":"2024-05-06 21:16:46","title":"Quantitative Contraction Rates for McKean-Vlasov Stochastic Differential Equations with Multiplicative Noise","abstract":"This work focuses on the quantitative contraction rates for McKean-Vlasov stochastic differential equations (SDEs) with multiplicative noise. Under suitable conditions on the coefficients of the SDE, this paper derives explicit quantitative contraction rates for the convergence in Wasserstein distances of McKean-Vlasov SDEs using the coupling method.The contraction results are then used to prove a propagation of chaos uniformly in time, which provides quantitative bounds on convergence rate of interacting particle systems, and establishes exponential ergodicity for McKean-Vlasov SDEs.","sentences":["This work focuses on the quantitative contraction rates for McKean-Vlasov stochastic differential equations (SDEs) with multiplicative noise.","Under suitable conditions on the coefficients of the SDE, this paper derives explicit quantitative contraction rates for the convergence in Wasserstein distances of McKean-Vlasov SDEs using the coupling method.","The contraction results are then used to prove a propagation of chaos uniformly in time, which provides quantitative bounds on convergence rate of interacting particle systems, and establishes exponential ergodicity for McKean-Vlasov SDEs."],"url":"http://arxiv.org/abs/2405.03859v1","category":"math.PR"}
{"created":"2024-05-06 20:58:36","title":"Upper Bounds for Complexity of Asymptotically Optimal Learned Indexes","abstract":"Learned indexes leverage machine learning models to accelerate query answering in databases, showing impressive practical performance. However, theoretical understanding of these methods remains incomplete. Existing research suggests that learned indexes have superior asymptotic complexity compared to their non-learned counterparts, but these findings have been established under restrictive probabilistic assumptions. Specifically, for a sorted array with $n$ elements, it has been shown that learned indexes can find a key in $O(\\log(\\log n))$ expected time using at most linear space, compared with $O(\\log n)$ for non-learned methods.   In this work, we prove $O(1)$ expected time can be achieved with at most linear space, thereby establishing the tightest upper bound so far for the time complexity of an asymptotically optimal learned index. Notably, we use weaker probabilistic assumptions than prior work, meaning our results generalize previous efforts. Furthermore, we introduce a new measure of statistical complexity for data. This metric exhibits an information-theoretical interpretation and can be estimated in practice. This characterization provides further theoretical understanding of learned indexes, by helping to explain why some datasets seem to be particularly challenging for these methods.","sentences":["Learned indexes leverage machine learning models to accelerate query answering in databases, showing impressive practical performance.","However, theoretical understanding of these methods remains incomplete.","Existing research suggests that learned indexes have superior asymptotic complexity compared to their non-learned counterparts, but these findings have been established under restrictive probabilistic assumptions.","Specifically, for a sorted array with $n$ elements, it has been shown that learned indexes can find a key in $O(\\log(\\log n))$ expected time using at most linear space, compared with $O(\\log n)$ for non-learned methods.   ","In this work, we prove $O(1)$ expected time can be achieved with at most linear space, thereby establishing the tightest upper bound so far for the time complexity of an asymptotically optimal learned index.","Notably, we use weaker probabilistic assumptions than prior work, meaning our results generalize previous efforts.","Furthermore, we introduce a new measure of statistical complexity for data.","This metric exhibits an information-theoretical interpretation and can be estimated in practice.","This characterization provides further theoretical understanding of learned indexes, by helping to explain why some datasets seem to be particularly challenging for these methods."],"url":"http://arxiv.org/abs/2405.03851v1","category":"cs.DB"}
{"created":"2024-05-06 20:51:59","title":"On the Convergence of Malleability and the HPC PowerStack: Exploiting Dynamism in Over-Provisioned and Power-Constrained HPC Systems","abstract":"Recent High-Performance Computing (HPC) systems are facing important challenges, such as massive power consumption, while at the same time significantly under-utilized system resources. Given the power consumption trends, future systems will be deployed in an over-provisioned manner where more resources are installed than they can afford to power simultaneously. In such a scenario, maximizing resource utilization and energy efficiency, while keeping a given power constraint, is pivotal. Driven by this observation, in this position paper we first highlight the recent trends of resource management techniques, with a particular focus on malleability support (i.e., dynamically scaling resource allocations/requirements for a job), co-scheduling (i.e., co-locating multiple jobs within a node), and power management. Second, we consider putting them together, assess their relationships/synergies, and discuss the functionality requirements in each software component for future over-provisioned and power-constrained HPC systems. Third, we briefly introduce our ongoing efforts on the integration of software tools, which will ultimately lead to the convergence of malleability and power management, as it is designed in the HPC PowerStack initiative.","sentences":["Recent High-Performance Computing (HPC) systems are facing important challenges, such as massive power consumption, while at the same time significantly under-utilized system resources.","Given the power consumption trends, future systems will be deployed in an over-provisioned manner where more resources are installed than they can afford to power simultaneously.","In such a scenario, maximizing resource utilization and energy efficiency, while keeping a given power constraint, is pivotal.","Driven by this observation, in this position paper we first highlight the recent trends of resource management techniques, with a particular focus on malleability support (i.e., dynamically scaling resource allocations/requirements for a job), co-scheduling (i.e., co-locating multiple jobs within a node), and power management.","Second, we consider putting them together, assess their relationships/synergies, and discuss the functionality requirements in each software component for future over-provisioned and power-constrained HPC systems.","Third, we briefly introduce our ongoing efforts on the integration of software tools, which will ultimately lead to the convergence of malleability and power management, as it is designed in the HPC PowerStack initiative."],"url":"http://arxiv.org/abs/2405.03847v1","category":"cs.DC"}
{"created":"2024-05-06 20:51:28","title":"Enhancing Apparent Personality Trait Analysis with Cross-Modal Embeddings","abstract":"Automatic personality trait assessment is essential for high-quality human-machine interactions. Systems capable of human behavior analysis could be used for self-driving cars, medical research, and surveillance, among many others. We present a multimodal deep neural network with a Siamese extension for apparent personality trait prediction trained on short video recordings and exploiting modality invariant embeddings. Acoustic, visual, and textual information are utilized to reach high-performance solutions in this task. Due to the highly centralized target distribution of the analyzed dataset, the changes in the third digit are relevant. Our proposed method addresses the challenge of under-represented extreme values, achieves 0.0033 MAE average improvement, and shows a clear advantage over the baseline multimodal DNN without the introduced module.","sentences":["Automatic personality trait assessment is essential for high-quality human-machine interactions.","Systems capable of human behavior analysis could be used for self-driving cars, medical research, and surveillance, among many others.","We present a multimodal deep neural network with a Siamese extension for apparent personality trait prediction trained on short video recordings and exploiting modality invariant embeddings.","Acoustic, visual, and textual information are utilized to reach high-performance solutions in this task.","Due to the highly centralized target distribution of the analyzed dataset, the changes in the third digit are relevant.","Our proposed method addresses the challenge of under-represented extreme values, achieves 0.0033 MAE average improvement, and shows a clear advantage over the baseline multimodal DNN without the introduced module."],"url":"http://arxiv.org/abs/2405.03846v1","category":"cs.CV"}
{"created":"2024-05-06 20:48:37","title":"Perception in Pixels: Understanding Avatar Representation in Video-Mediated Collaborative Interactions","abstract":"Despite the abundance of research concerning virtual reality (VR) avatars, the impact of screen-based or augmented reality (AR) avatars for real-world applications remain relatively unexplored. Notably, there is a lack of research examining video-mediated collaborative interaction experiences using AR avatars for goal-directed group activities. This study bridges this gap with a mixed-methods, quasi-experimental user study that investigates video-based small-group interactions when employing AR avatars as opposed to traditional video for user representation. We found that the use of avatars positively influenced self-esteem and video-based collaboration satisfaction. In addition, our group interview findings highlight experiences and perceptions regarding the dynamic use of avatars in video-mediated collaborative interactions, including benefits, challenges, and factors that would influence a decision to use avatars. This study contributes an empirical understanding of avatar representation in mediating video-based collaborative interactions, implications and perceptions surrounding the adoption of AR avatars, and a comprehensive comparison of key characteristics between user representations.","sentences":["Despite the abundance of research concerning virtual reality (VR) avatars, the impact of screen-based or augmented reality (AR) avatars for real-world applications remain relatively unexplored.","Notably, there is a lack of research examining video-mediated collaborative interaction experiences using AR avatars for goal-directed group activities.","This study bridges this gap with a mixed-methods, quasi-experimental user study that investigates video-based small-group interactions when employing AR avatars as opposed to traditional video for user representation.","We found that the use of avatars positively influenced self-esteem and video-based collaboration satisfaction.","In addition, our group interview findings highlight experiences and perceptions regarding the dynamic use of avatars in video-mediated collaborative interactions, including benefits, challenges, and factors that would influence a decision to use avatars.","This study contributes an empirical understanding of avatar representation in mediating video-based collaborative interactions, implications and perceptions surrounding the adoption of AR avatars, and a comprehensive comparison of key characteristics between user representations."],"url":"http://arxiv.org/abs/2405.03844v1","category":"cs.HC"}
{"created":"2024-05-06 20:40:38","title":"Optimizing Hardware Resource Partitioning and Job Allocations on Modern GPUs under Power Caps","abstract":"CPU-GPU heterogeneous systems are now commonly used in HPC (High-Performance Computing). However, improving the utilization and energy-efficiency of such systems is still one of the most critical issues. As one single program typically cannot fully utilize all resources within a node/chip, co-scheduling (or co-locating) multiple programs with complementary resource requirements is a promising solution. Meanwhile, as power consumption has become the first-class design constraint for HPC systems, such co-scheduling techniques should be well-tailored for power-constrained environments. To this end, the industry recently started supporting hardware-level resource partitioning features on modern GPUs for realizing efficient co-scheduling, which can operate with existing power capping features. For example, NVidia's MIG (Multi-Instance GPU) partitions one single GPU into multiple instances at the granularity of a GPC (Graphics Processing Cluster). In this paper, we explicitly target the combination of hardware-level GPU partitioning features and power capping for power-constrained HPC systems. We provide a systematic methodology to optimize the combination of chip partitioning, job allocations, as well as power capping based on our scalability/interference modeling while taking a variety of aspects into account, such as compute/memory intensity and utilization in heterogeneous computational resources (e.g., Tensor Cores). The experimental result indicates that our approach is successful in selecting a near optimal combination across multiple different workloads.","sentences":["CPU-GPU heterogeneous systems are now commonly used in HPC (High-Performance Computing).","However, improving the utilization and energy-efficiency of such systems is still one of the most critical issues.","As one single program typically cannot fully utilize all resources within a node/chip, co-scheduling (or co-locating) multiple programs with complementary resource requirements is a promising solution.","Meanwhile, as power consumption has become the first-class design constraint for HPC systems, such co-scheduling techniques should be well-tailored for power-constrained environments.","To this end, the industry recently started supporting hardware-level resource partitioning features on modern GPUs for realizing efficient co-scheduling, which can operate with existing power capping features.","For example, NVidia's MIG (Multi-Instance GPU) partitions one single GPU into multiple instances at the granularity of a GPC (Graphics Processing Cluster).","In this paper, we explicitly target the combination of hardware-level GPU partitioning features and power capping for power-constrained HPC systems.","We provide a systematic methodology to optimize the combination of chip partitioning, job allocations, as well as power capping based on our scalability/interference modeling while taking a variety of aspects into account, such as compute/memory intensity and utilization in heterogeneous computational resources (e.g., Tensor Cores).","The experimental result indicates that our approach is successful in selecting a near optimal combination across multiple different workloads."],"url":"http://arxiv.org/abs/2405.03838v1","category":"cs.DC"}
{"created":"2024-05-06 20:35:38","title":"Covariance-free Multifidelity Control Variates Importance Sampling for Reliability Analysis of Rare Events","abstract":"Multifidelity modeling has been steadily gaining attention as a tool to address the problem of exorbitant model evaluation costs that makes the estimation of failure probabilities a significant computational challenge for complex real-world problems, particularly when failure is a rare event. To implement multifidelity modeling, estimators that efficiently combine information from multiple models/sources are necessary. In past works, the variance reduction techniques of Control Variates (CV) and Importance Sampling (IS) have been leveraged for this task. In this paper, we present the CVIS framework; a creative take on a coupled Control Variates and Importance Sampling estimator for bifidelity reliability analysis. The framework addresses some of the practical challenges of the CV method by using an estimator for the control variate mean and side-stepping the need to estimate the covariance between the original estimator and the control variate through a clever choice for the tuning constant. The task of selecting an efficient IS distribution is also considered, with a view towards maximally leveraging the bifidelity structure and maintaining expressivity. Additionally, a diagnostic is provided that indicates both the efficiency of the algorithm as well as the relative predictive quality of the models utilized. Finally, the behavior and performance of the framework is explored through analytical and numerical examples.","sentences":["Multifidelity modeling has been steadily gaining attention as a tool to address the problem of exorbitant model evaluation costs that makes the estimation of failure probabilities a significant computational challenge for complex real-world problems, particularly when failure is a rare event.","To implement multifidelity modeling, estimators that efficiently combine information from multiple models/sources are necessary.","In past works, the variance reduction techniques of Control Variates (CV) and Importance Sampling (IS) have been leveraged for this task.","In this paper, we present the CVIS framework; a creative take on a coupled Control Variates and Importance Sampling estimator for bifidelity reliability analysis.","The framework addresses some of the practical challenges of the CV method by using an estimator for the control variate mean and side-stepping the need to estimate the covariance between the original estimator and the control variate through a clever choice for the tuning constant.","The task of selecting an efficient IS distribution is also considered, with a view towards maximally leveraging the bifidelity structure and maintaining expressivity.","Additionally, a diagnostic is provided that indicates both the efficiency of the algorithm as well as the relative predictive quality of the models utilized.","Finally, the behavior and performance of the framework is explored through analytical and numerical examples."],"url":"http://arxiv.org/abs/2405.03834v1","category":"stat.ME"}
{"created":"2024-05-06 20:24:20","title":"Orchestrated Co-scheduling, Resource Partitioning, and Power Capping on CPU-GPU Heterogeneous Systems via Machine Learning","abstract":"CPU-GPU heterogeneous architectures are now commonly used in a wide variety of computing systems from mobile devices to supercomputers. Maximizing the throughput for multi-programmed workloads on such systems is indispensable as one single program typically cannot fully exploit all available resources. At the same time, power consumption is a key issue and often requires optimizing power allocations to the CPU and GPU while enforcing a total power constraint, in particular when the power/thermal requirements are strict. The result is a system-wide optimization problem with several knobs. In particular we focus on (1) co-scheduling decisions, i.e., selecting programs to co-locate in a space sharing manner; (2) resource partitioning on both CPUs and GPUs; and (3) power capping on both CPUs and GPUs. We solve this problem using predictive performance modeling using machine learning in order to coordinately optimize the above knob setups. Our experiential results using a real system show that our approach achieves up to 67% of speedup compared to a time-sharing-based scheduling with a naive power capping that evenly distributes power budgets across components.","sentences":["CPU-GPU heterogeneous architectures are now commonly used in a wide variety of computing systems from mobile devices to supercomputers.","Maximizing the throughput for multi-programmed workloads on such systems is indispensable as one single program typically cannot fully exploit all available resources.","At the same time, power consumption is a key issue and often requires optimizing power allocations to the CPU and GPU while enforcing a total power constraint, in particular when the power/thermal requirements are strict.","The result is a system-wide optimization problem with several knobs.","In particular we focus on (1) co-scheduling decisions, i.e., selecting programs to co-locate in a space sharing manner; (2) resource partitioning on both CPUs and GPUs; and (3) power capping on both CPUs and GPUs.","We solve this problem using predictive performance modeling using machine learning in order to coordinately optimize the above knob setups.","Our experiential results using a real system show that our approach achieves up to 67% of speedup compared to a time-sharing-based scheduling with a naive power capping that evenly distributes power budgets across components."],"url":"http://arxiv.org/abs/2405.03831v1","category":"cs.DC"}
{"created":"2024-05-06 20:18:33","title":"The Trajectory of Romance Scams in the U.S","abstract":"Romance scams (RS) inflict financial and emotional damage by defrauding victims under the guise of meaningful relationships. This research study examines RS trends in the U.S. through a quantitative analysis of web searches, news articles, research publications, and government reports from 2004 to 2023. This is the first study to use multiple sources for RS trend analysis. Results reveal increasing public interest and media coverage contrasted by a recent decrease in incidents reported to authorities. The frequency of research dedicated to RS has steadily grown but focuses predominantly on documenting the problem rather than developing solutions. Overall, findings suggest RS escalation despite declining official reports, which are likely obscured by low victim reporting rates. This highlights the need for greater awareness to encourage reporting enabling accurate data-driven policy responses. Additionally, more research must focus on techniques to counter these crimes. With improved awareness and prevention, along with responses informed by more accurate data, the rising RS threat can perhaps be mitigated.","sentences":["Romance scams (RS) inflict financial and emotional damage by defrauding victims under the guise of meaningful relationships.","This research study examines RS trends in the U.S. through a quantitative analysis of web searches, news articles, research publications, and government reports from 2004 to 2023.","This is the first study to use multiple sources for RS trend analysis.","Results reveal increasing public interest and media coverage contrasted by a recent decrease in incidents reported to authorities.","The frequency of research dedicated to RS has steadily grown but focuses predominantly on documenting the problem rather than developing solutions.","Overall, findings suggest RS escalation despite declining official reports, which are likely obscured by low victim reporting rates.","This highlights the need for greater awareness to encourage reporting enabling accurate data-driven policy responses.","Additionally, more research must focus on techniques to counter these crimes.","With improved awareness and prevention, along with responses informed by more accurate data, the rising RS threat can perhaps be mitigated."],"url":"http://arxiv.org/abs/2405.03828v1","category":"cs.CR"}
{"created":"2024-05-06 19:53:36","title":"Stochastic behavior of an n-node blockchain under cyber attacks from multiple hackers with random re-setting times","abstract":"This paper investigates the stochastic behavior of an n-node blockchain which is continuously monitored and faces non-stop cyber attacks from multiple hackers. The blockchain will start being re-set once hacking is detected, forfeiting previous efforts of all hackers. It is assumed the re-setting process takes a random amount of time. Multiple independent hackers will keep attempting to hack into the blockchain until one of them succeeds. For arbitrary distributions of the hacking times, detecting times, and re-setting times, we derive the instantaneous functional probability, the limiting functional probability, and the mean functional time of the blockchain. Moreover, we establish that these quantities are increasing functions of the number of nodes, formalizing the intuition that the more nodes a blockchain has the more secure it is.","sentences":["This paper investigates the stochastic behavior of an n-node blockchain which is continuously monitored and faces non-stop cyber attacks from multiple hackers.","The blockchain will start being re-set once hacking is detected, forfeiting previous efforts of all hackers.","It is assumed the re-setting process takes a random amount of time.","Multiple independent hackers will keep attempting to hack into the blockchain until one of them succeeds.","For arbitrary distributions of the hacking times, detecting times, and re-setting times, we derive the instantaneous functional probability, the limiting functional probability, and the mean functional time of the blockchain.","Moreover, we establish that these quantities are increasing functions of the number of nodes, formalizing the intuition that the more nodes a blockchain has the more secure it is."],"url":"http://arxiv.org/abs/2405.03814v1","category":"stat.AP"}
{"created":"2024-05-06 19:52:57","title":"Large Language Models as Instruments of Power: New Regimes of Autonomous Manipulation and Control","abstract":"Large language models (LLMs) can reproduce a wide variety of rhetorical styles and generate text that expresses a broad spectrum of sentiments. This capacity, now available at low cost, makes them powerful tools for manipulation and control. In this paper, we consider a set of underestimated societal harms made possible by the rapid and largely unregulated adoption of LLMs. Rather than consider LLMs as isolated digital artefacts used to displace this or that area of work, we focus on the large-scale computational infrastructure upon which they are instrumentalised across domains. We begin with discussion on how LLMs may be used to both pollute and uniformize information environments and how these modalities may be leveraged as mechanisms of control. We then draw attention to several areas of emerging research, each of which compounds the capabilities of LLMs as instruments of power. These include (i) persuasion through the real-time design of choice architectures in conversational interfaces (e.g., via \"AI personas\"), (ii) the use of LLM-agents as computational models of human agents (e.g., \"silicon subjects\"), (iii) the use of LLM-agents as computational models of human agent populations (e.g., \"silicon societies\") and finally, (iv) the combination of LLMs with reinforcement learning to produce controllable and steerable strategic dialogue models. We draw these strands together to discuss how these areas may be combined to build LLM-based systems that serve as powerful instruments of individual, social and political control via the simulation and disingenuous \"prediction\" of human behaviour, intent, and action.","sentences":["Large language models (LLMs) can reproduce a wide variety of rhetorical styles and generate text that expresses a broad spectrum of sentiments.","This capacity, now available at low cost, makes them powerful tools for manipulation and control.","In this paper, we consider a set of underestimated societal harms made possible by the rapid and largely unregulated adoption of LLMs.","Rather than consider LLMs as isolated digital artefacts used to displace this or that area of work, we focus on the large-scale computational infrastructure upon which they are instrumentalised across domains.","We begin with discussion on how LLMs may be used to both pollute and uniformize information environments and how these modalities may be leveraged as mechanisms of control.","We then draw attention to several areas of emerging research, each of which compounds the capabilities of LLMs as instruments of power.","These include (i) persuasion through the real-time design of choice architectures in conversational interfaces (e.g., via \"AI personas\"), (ii) the use of LLM-agents as computational models of human agents (e.g., \"silicon subjects\"), (iii) the use of LLM-agents as computational models of human agent populations (e.g., \"silicon societies\") and finally, (iv) the combination of LLMs with reinforcement learning to produce controllable and steerable strategic dialogue models.","We draw these strands together to discuss how these areas may be combined to build LLM-based systems that serve as powerful instruments of individual, social and political control via the simulation and disingenuous \"prediction\" of human behaviour, intent, and action."],"url":"http://arxiv.org/abs/2405.03813v1","category":"cs.SI"}
{"created":"2024-05-06 19:48:00","title":"Bipartite OTOC in open quantum systems: information scrambling and irreversibility","abstract":"The field of information scrambling has seen significant growth over the last decade, where the out-of-time-ordered correlator (OTOC) has emerged as a prominent tool to probe it. In this work, we use bipartite OTOC, a particular form of OTOC, to study information scrambling in the atom-field interaction models and the model of the Ising spin chain interacting with a tilted magnetic field. This is done considering the effects of open quantum systems. A relationship between information scrambling, using bipartite OTOC, and irreversibility, using entropy production, is probed under unitary dynamics. The equivalence of bipartite OTOC with operator entanglement is explicitly shown for the Ising model.","sentences":["The field of information scrambling has seen significant growth over the last decade, where the out-of-time-ordered correlator (OTOC) has emerged as a prominent tool to probe it.","In this work, we use bipartite OTOC, a particular form of OTOC, to study information scrambling in the atom-field interaction models and the model of the Ising spin chain interacting with a tilted magnetic field.","This is done considering the effects of open quantum systems.","A relationship between information scrambling, using bipartite OTOC, and irreversibility, using entropy production, is probed under unitary dynamics.","The equivalence of bipartite OTOC with operator entanglement is explicitly shown for the Ising model."],"url":"http://arxiv.org/abs/2405.03810v1","category":"quant-ph"}
{"created":"2024-05-06 19:30:57","title":"In Situ AI Prototyping: Infusing Multimodal Prompts into Mobile Settings with MobileMaker","abstract":"Recent advances in multimodal large language models (LLMs) have lowered the barriers to rapidly prototyping AI-powered features via prompting, especially for mobile-intended use cases. Despite the value of situated user feedback, the process of soliciting early, mobile-situated user feedback on AI prototypes remains challenging. The broad scope and flexibility of LLMs means that, for a given use-case-specific prototype, there is a crucial need to understand the wide range of in-the-wild input likely to be provided by the user, as well as their in-context expectations of the AI's behavior. To explore the concept of in situ AI prototyping and testing, we created MobileMaker: an AI prototyping tool that enables designers to rapidly create mobile AI prototypes that can be tested on-device, and enables testers to make on-device, in-the-field revisions of the prototype through natural language. In an exploratory study with 16 users, we explored how user feedback on prototypes created with MobileMaker compares to that of existing prototyping tools (e.g., Figma, prompt editors). We found that MobileMaker prototypes enabled more serendipitous discovery of: model input edge cases, discrepancies between AI's and user's in-context interpretation of the task, and contextual signals missed by the AI. Furthermore, we learned that while the ability to make in-the-wild revisions led users to feel more fulfilled as active participants in the design process, it might also constrain their feedback to the subset of changes perceived as more actionable or implementable by the prototyping tool.","sentences":["Recent advances in multimodal large language models (LLMs) have lowered the barriers to rapidly prototyping AI-powered features via prompting, especially for mobile-intended use cases.","Despite the value of situated user feedback, the process of soliciting early, mobile-situated user feedback on AI prototypes remains challenging.","The broad scope and flexibility of LLMs means that, for a given use-case-specific prototype, there is a crucial need to understand the wide range of in-the-wild input likely to be provided by the user, as well as their in-context expectations of the AI's behavior.","To explore the concept of in situ AI prototyping and testing, we created MobileMaker: an AI prototyping tool that enables designers to rapidly create mobile AI prototypes that can be tested on-device, and enables testers to make on-device, in-the-field revisions of the prototype through natural language.","In an exploratory study with 16 users, we explored how user feedback on prototypes created with MobileMaker compares to that of existing prototyping tools (e.g., Figma, prompt editors).","We found that MobileMaker prototypes enabled more serendipitous discovery of: model input edge cases, discrepancies between AI's and user's in-context interpretation of the task, and contextual signals missed by the AI.","Furthermore, we learned that while the ability to make in-the-wild revisions led users to feel more fulfilled as active participants in the design process, it might also constrain their feedback to the subset of changes perceived as more actionable or implementable by the prototyping tool."],"url":"http://arxiv.org/abs/2405.03806v1","category":"cs.HC"}
{"created":"2024-05-06 19:18:56","title":"A note on H\u00f6lder regularity of weak solutions to linear elliptic equations","abstract":"In this paper, we show that weak solutions of $$-\\text{div} \\mathbb{A}(x)\\nabla u = 0 \\qquad \\text{where}\\quad \\mathbb{A}(x)= \\mathbb{A}(x)^T \\,\\, \\text{and} \\,\\, \\lambda |\\zeta|^2 \\leq \\langle \\mathbb{A}(x)\\zeta,\\zeta\\rangle \\leq \\Lambda |\\zeta|^2,$$   and $\\mathbb{A}(x) \\equiv \\mathbb{A}$ is a constant matrix are H\\\"older continuous $u \\in C^{\\alpha}_{\\text{loc}}$ with $\\alpha \\geq \\frac12 \\left(-(n-2) + \\sqrt{(n-2)^2 + \\frac{4(n-1)\\lambda}{\\Lambda}} \\right)$. This implies that the example constructed by Piccinini - Spagnolo is sharp in the class of constant matrices $\\mathbb{A}(x) \\equiv \\mathbb{A}$. The proof of H\\\"older regularity does not go through a reduction of oscillation type argument and instead is achieved through a monotonicity formula.   In the case of general matrices $\\mathbb{A}(x)$, we obtain the same regularity under some additional hypothesis.","sentences":["In this paper, we show that weak solutions of $$-\\text{div} \\mathbb{A}(x)\\nabla u = 0","\\qquad \\text{where}\\quad \\mathbb{A}(x)= \\mathbb{A}(x)^T \\,\\, \\text{and} \\,\\, \\lambda |\\zeta|^2 \\leq \\langle \\mathbb{A}(x)\\zeta,\\zeta\\rangle \\leq \\Lambda |\\zeta|^2,$$   and $\\mathbb{A}(x) \\equiv \\mathbb{A}$ is a constant matrix are H\\\"older continuous $u \\in C^{\\alpha}_{\\text{loc}}$ with $\\alpha \\geq \\frac12 \\left(-(n-2) + \\sqrt{(n-2)^2 + \\frac{4(n-1)\\lambda}{\\Lambda}} \\right)$. This implies that the example constructed by Piccinini - Spagnolo is sharp in the class of constant matrices $\\mathbb{A}(x) \\equiv \\mathbb{A}$.","The proof of H\\\"older regularity does not go through a reduction of oscillation type argument and instead is achieved through a monotonicity formula.   ","In the case of general matrices $\\mathbb{A}(x)$, we obtain the same regularity under some additional hypothesis."],"url":"http://arxiv.org/abs/2405.03802v1","category":"math.AP"}
{"created":"2024-05-06 19:05:31","title":"Update Rate, Accuracy, and Age of Information in a Wireless Sensor Network","abstract":"Age of Information (AoI), namely the time that has elapsed since the most recently delivered packet was generated, is receiving increasing attention with the emergence of many real-time applications that rely on the exchange of time-sensitive information. AoI captures the freshness of the information from the perspective of the destination. The term \"accuracy of information\" is used to assess how close the estimate at the destination is to the parameter value measured by the sensor. In this paper, the mean square error (MSE) is used to evaluate the accuracy of information. We focus on a single sensor that monitors a time-sensitive physical process, which is modelled as a random walk. Whenever the state of the random walk changes by more than a specified threshold, the sensor generates a status update packet and transmits it to the destination. When no update packet is received, the destination assumes that the state of the process has not changed. We study the problem of finding the minimum update rate under AoI and accuracy of information constraints. More specifically, we derive analytical expressions for the update rate, the AoI, and the MSE.","sentences":["Age of Information (AoI), namely the time that has elapsed since the most recently delivered packet was generated, is receiving increasing attention with the emergence of many real-time applications that rely on the exchange of time-sensitive information.","AoI captures the freshness of the information from the perspective of the destination.","The term \"accuracy of information\" is used to assess how close the estimate at the destination is to the parameter value measured by the sensor.","In this paper, the mean square error (MSE) is used to evaluate the accuracy of information.","We focus on a single sensor that monitors a time-sensitive physical process, which is modelled as a random walk.","Whenever the state of the random walk changes by more than a specified threshold, the sensor generates a status update packet and transmits it to the destination.","When no update packet is received, the destination assumes that the state of the process has not changed.","We study the problem of finding the minimum update rate under AoI and accuracy of information constraints.","More specifically, we derive analytical expressions for the update rate, the AoI, and the MSE."],"url":"http://arxiv.org/abs/2405.03798v1","category":"cs.IT"}
{"created":"2024-05-06 18:39:00","title":"Exploring Self-Interacting Dark Matter Halos with Diverse Baryonic Distributions: A Parametric Approach","abstract":"Galaxies residing in dark matter halos exert significant gravitational effects that alter halo structure and dynamics. The complexity of these interactions escalates with the diversity of galactic structures and the variability in dark matter halo profiles under self-interacting dark matter (SIDM) models. This work extends the parametric model for dark matter-only halos presented in arXiv:2305.16176 to incorporate baryons. We adapt this model to consistently represent the SIDM halo density profile over time, highlighting the role of a gravothermal phase in characterizing the state of an SIDM halo. Given this phase, the density profile in SIDM is determined by a fictitious progenitor -- consisting of an NFW halo influenced by a baryonic potential -- that has evolved to its present state. In the temporal dimension, the model incorporates a form factor that rescales the evolution time in the dark matter-only case, thereby enabling the introduction of a universal phase. In the radial dimension, the halo density profile is parametrized to reflect the influences of baryons. We calibrate the model through N-body simulations with baryon potentials to fit various stellar-to-halo mass ratios and size-mass relationships. Our parametric approach is numerically efficient, enabling the exploration of SIDM effects across a diverse set of halos, as exemplified by a case study using an illustrative sample that spans five orders of magnitude in the mass range. We also demonstrate that the effects of evolution history and the specific SIDM model can be separated from the current states of galaxies and halos, leaving the task of identifying consistent SIDM models to dedicated post-processing analyses.","sentences":["Galaxies residing in dark matter halos exert significant gravitational effects that alter halo structure and dynamics.","The complexity of these interactions escalates with the diversity of galactic structures and the variability in dark matter halo profiles under self-interacting dark matter (SIDM) models.","This work extends the parametric model for dark matter-only halos presented in arXiv:2305.16176 to incorporate baryons.","We adapt this model to consistently represent the SIDM halo density profile over time, highlighting the role of a gravothermal phase in characterizing the state of an SIDM halo.","Given this phase, the density profile in SIDM is determined by a fictitious progenitor -- consisting of an NFW halo influenced by a baryonic potential -- that has evolved to its present state.","In the temporal dimension, the model incorporates a form factor that rescales the evolution time in the dark matter-only case, thereby enabling the introduction of a universal phase.","In the radial dimension, the halo density profile is parametrized to reflect the influences of baryons.","We calibrate the model through N-body simulations with baryon potentials to fit various stellar-to-halo mass ratios and size-mass relationships.","Our parametric approach is numerically efficient, enabling the exploration of SIDM effects across a diverse set of halos, as exemplified by a case study using an illustrative sample that spans five orders of magnitude in the mass range.","We also demonstrate that the effects of evolution history and the specific SIDM model can be separated from the current states of galaxies and halos, leaving the task of identifying consistent SIDM models to dedicated post-processing analyses."],"url":"http://arxiv.org/abs/2405.03787v1","category":"astro-ph.CO"}
{"created":"2024-05-06 18:20:05","title":"Light-induced Hall currents in altermagnets","abstract":"Berry curvature, a momentum space property, can manifest itself in current responses. The well-known anomalous Hall effect in time-reversal-breaking systems arises from a Berry curvature monopole. In time-reversal-invariant materials, a second-order Hall conductivity emerges from a Berry curvature dipole. Recently, it has been shown that a Berry curvature quadrupole induces a third-order ac Hall response in systems that break time reversal ($K$) and a fourfold rotational ($C_{4}$) symmetry, while remaining invariant under the combination of the two ($C_{4}K$). In this letter, we demonstrate that incident light can induce a $\\rm dc$ Hall current in such systems, driven by the Berry curvature quadrupole. We consider a combination of a static $\\rm dc$ electric field and an ac light-induced electric field. We calculate the current perpendicular to both the static electric field and the fourfold axis. Remarkably, the induced current is generically spin-polarized. A net charge current appears for light that is linearly or elliptically polarized, but not for circular polarization. In contrast, the spin current remains unchanged when the polarization of light is varied. This allows for rich possibilities such as generating a spin current by shining circularly polarized light on an altermagnetic material. We demonstrate this physics using a two-dimensional toy model for altermagnets.","sentences":["Berry curvature, a momentum space property, can manifest itself in current responses.","The well-known anomalous Hall effect in time-reversal-breaking systems arises from a Berry curvature monopole.","In time-reversal-invariant materials, a second-order Hall conductivity emerges from a Berry curvature dipole.","Recently, it has been shown that a Berry curvature quadrupole induces a third-order ac Hall response in systems that break time reversal ($K$) and a fourfold rotational ($C_{4}$) symmetry, while remaining invariant under the combination of the two ($C_{4}K$).","In this letter, we demonstrate that incident light can induce a $\\rm dc$ Hall current in such systems, driven by the Berry curvature quadrupole.","We consider a combination of a static $\\rm dc$ electric field and an ac light-induced electric field.","We calculate the current perpendicular to both the static electric field and the fourfold axis.","Remarkably, the induced current is generically spin-polarized.","A net charge current appears for light that is linearly or elliptically polarized, but not for circular polarization.","In contrast, the spin current remains unchanged when the polarization of light is varied.","This allows for rich possibilities such as generating a spin current by shining circularly polarized light on an altermagnetic material.","We demonstrate this physics using a two-dimensional toy model for altermagnets."],"url":"http://arxiv.org/abs/2405.03779v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-06 18:19:22","title":"An Autoregressive Model for Time Series of Random Objects","abstract":"Random variables in metric spaces indexed by time and observed at equally spaced time points are receiving increased attention due to their broad applicability. However, the absence of inherent structure in metric spaces has resulted in a literature that is predominantly non-parametric and model-free. To address this gap in models for time series of random objects, we introduce an adaptation of the classical linear autoregressive model tailored for data lying in a Hadamard space. The parameters of interest in this model are the Fr\\'echet mean and a concentration parameter, both of which we prove can be consistently estimated from data. Additionally, we propose a test statistic and establish its asymptotic normality, thereby enabling hypothesis testing for the absence of serial dependence. Finally, we introduce a bootstrap procedure to obtain critical values for the test statistic under the null hypothesis. Theoretical results of our method, including the convergence of the estimators as well as the size and power of the test, are illustrated through simulations, and the utility of the model is demonstrated by an analysis of a time series of consumer inflation expectations.","sentences":["Random variables in metric spaces indexed by time and observed at equally spaced time points are receiving increased attention due to their broad applicability.","However, the absence of inherent structure in metric spaces has resulted in a literature that is predominantly non-parametric and model-free.","To address this gap in models for time series of random objects, we introduce an adaptation of the classical linear autoregressive model tailored for data lying in a Hadamard space.","The parameters of interest in this model are the Fr\\'echet mean and a concentration parameter, both of which we prove can be consistently estimated from data.","Additionally, we propose a test statistic and establish its asymptotic normality, thereby enabling hypothesis testing for the absence of serial dependence.","Finally, we introduce a bootstrap procedure to obtain critical values for the test statistic under the null hypothesis.","Theoretical results of our method, including the convergence of the estimators as well as the size and power of the test, are illustrated through simulations, and the utility of the model is demonstrated by an analysis of a time series of consumer inflation expectations."],"url":"http://arxiv.org/abs/2405.03778v1","category":"stat.ME"}
{"created":"2024-05-06 18:17:42","title":"Near-optimal decoding algorithm for color codes using Population Annealing","abstract":"The development and use of large-scale quantum computers relies on integrating quantum error-correcting (QEC) schemes into the quantum computing pipeline. A fundamental part of the QEC protocol is the decoding of the syndrome to identify a recovery operation with a high success rate. In this work, we implement a decoder that finds the recovery operation with the highest success probability by mapping the decoding problem to a spin system and using Population Annealing to estimate the free energy of the different error classes. We study the decoder performance on a 4.8.8 color code lattice under different noise models, including code capacity with bit-flip and depolarizing noise, and phenomenological noise, which considers noisy measurements, with performance reaching near-optimal thresholds. This decoding algorithm can be applied to a wide variety of stabilizer codes, including surface codes and quantum low-density parity-check (qLDPC) codes.","sentences":["The development and use of large-scale quantum computers relies on integrating quantum error-correcting (QEC) schemes into the quantum computing pipeline.","A fundamental part of the QEC protocol is the decoding of the syndrome to identify a recovery operation with a high success rate.","In this work, we implement a decoder that finds the recovery operation with the highest success probability by mapping the decoding problem to a spin system and using Population Annealing to estimate the free energy of the different error classes.","We study the decoder performance on a 4.8.8 color code lattice under different noise models, including code capacity with bit-flip and depolarizing noise, and phenomenological noise, which considers noisy measurements, with performance reaching near-optimal thresholds.","This decoding algorithm can be applied to a wide variety of stabilizer codes, including surface codes and quantum low-density parity-check (qLDPC) codes."],"url":"http://arxiv.org/abs/2405.03776v1","category":"quant-ph"}
{"created":"2024-05-06 18:10:03","title":"Atomically thin obstructed atomic insulators with robust edge modes and quantized spin Hall effect","abstract":"Symmetry-protected edge states serve as direct evidence of nontrivial electronic topology in atomically thin materials. Finding these states in experimentally realizable single-phase materials presents a substantial challenge for their use for both fundamental studies and developing functional nanoscale devices. Here, we show the presence of robust edge states in phosphorene and group-Va monolayers with puckered lattice structures. By carefully analyzing the symmetry of the atomic sites and edge modes properties, we demonstrate that these atomically thin two-dimensional (2D) materials realize recently introduced obstructed atomic insulator states with partially occupied edge modes. The obstructed edge modes attain a Rashba-type spin splitting with Rashba parameter ($\\alpha$) of 1.52 eV \\r{A} for arsenene. Under strain or doping effects, these obstructed insulators transition to a phase with substantial spin-Berry curvature, yielding a double quantum spin Hall state with a spin Hall conductivity of 4$\\frac{e^2}{h}$. The experimental availability of phosphorene and other group-Va materials featuring puckered lattice structures could enable verification of obstructed atomic states and enhanced spin-Berry curvature effects discussed in this study, offering the potential for applications in topological electronic and spintronic devices.","sentences":["Symmetry-protected edge states serve as direct evidence of nontrivial electronic topology in atomically thin materials.","Finding these states in experimentally realizable single-phase materials presents a substantial challenge for their use for both fundamental studies and developing functional nanoscale devices.","Here, we show the presence of robust edge states in phosphorene and group-Va monolayers with puckered lattice structures.","By carefully analyzing the symmetry of the atomic sites and edge modes properties, we demonstrate that these atomically thin two-dimensional (2D) materials realize recently introduced obstructed atomic insulator states with partially occupied edge modes.","The obstructed edge modes attain a Rashba-type spin splitting with Rashba parameter ($\\alpha$) of 1.52 eV \\r{A} for arsenene.","Under strain or doping effects, these obstructed insulators transition to a phase with substantial spin-Berry curvature, yielding a double quantum spin Hall state with a spin Hall conductivity of 4$\\frac{e^2}{h}$.","The experimental availability of phosphorene and other group-Va materials featuring puckered lattice structures could enable verification of obstructed atomic states and enhanced spin-Berry curvature effects discussed in this study, offering the potential for applications in topological electronic and spintronic devices."],"url":"http://arxiv.org/abs/2405.03771v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 18:04:41","title":"Mitigating errors in logical qubits","abstract":"Quantum error correcting codes protect quantum information, allowing for large quantum computations provided that physical error rates are sufficiently low. We combine post-selection with surface code error correction through the use of a parameterized family of exclusive decoders, which are able to abort on decoding instances that are deemed too difficult. We develop new numerical sampling methods to quantify logical failure rates with exclusive decoders as well as the trade-off in terms of the amount of post-selection required. For the most discriminating of exclusive decoders, we demonstrate a threshold of 50\\% under depolarizing noise for the surface code (or $32(1)\\%$ for the fault-tolerant case with phenomenological measurement errors), and up to a quadratic improvement in logical failure rates below threshold. Furthermore, surprisingly, with a modest exclusion criterion, we identify a regime at low error rates where the exclusion rate decays with code distance, providing a pathway for scalable and time-efficient quantum computing with post-selection. We apply our exclusive decoder to the 15-to-1 magic state distillation protocol, and report a $75\\%$ reduction in the number of physical qubits required, and a $60\\%$ reduction in the total spacetime volume required, including accounting for repetitions required for post-selection. We also consider other applications, as an error mitigation technique, and in concatenated schemes. Our work highlights the importance of post-selection as a powerful tool in quantum error correction.","sentences":["Quantum error correcting codes protect quantum information, allowing for large quantum computations provided that physical error rates are sufficiently low.","We combine post-selection with surface code error correction through the use of a parameterized family of exclusive decoders, which are able to abort on decoding instances that are deemed too difficult.","We develop new numerical sampling methods to quantify logical failure rates with exclusive decoders as well as the trade-off in terms of the amount of post-selection required.","For the most discriminating of exclusive decoders, we demonstrate a threshold of 50\\% under depolarizing noise for the surface code (or $32(1)\\%$ for the fault-tolerant case with phenomenological measurement errors), and up to a quadratic improvement in logical failure rates below threshold.","Furthermore, surprisingly, with a modest exclusion criterion, we identify a regime at low error rates where the exclusion rate decays with code distance, providing a pathway for scalable and time-efficient quantum computing with post-selection.","We apply our exclusive decoder to the 15-to-1 magic state distillation protocol, and report a $75\\%$ reduction in the number of physical qubits required, and a $60\\%$ reduction in the total spacetime volume required, including accounting for repetitions required for post-selection.","We also consider other applications, as an error mitigation technique, and in concatenated schemes.","Our work highlights the importance of post-selection as a powerful tool in quantum error correction."],"url":"http://arxiv.org/abs/2405.03766v1","category":"quant-ph"}
{"created":"2024-05-06 18:02:00","title":"GOVERN: Gradient Orientation Vote Ensemble for Multi-Teacher Reinforced Distillation","abstract":"Pre-trained language models have become an integral component of question-answering systems, achieving remarkable performance. For practical deployment, it is critical to carry out knowledge distillation to preserve high performance under computational constraints. In this paper, we address a key question: given the importance of unsupervised distillation for student performance, how does one effectively ensemble knowledge from multiple teachers at this stage without the guidance of ground-truth labels? We propose a novel algorithm, GOVERN, to tackle this issue. GOVERN has demonstrated significant improvements in both offline and online experiments. The proposed algorithm has been successfully deployed in a real-world commercial question-answering system.","sentences":["Pre-trained language models have become an integral component of question-answering systems, achieving remarkable performance.","For practical deployment, it is critical to carry out knowledge distillation to preserve high performance under computational constraints.","In this paper, we address a key question: given the importance of unsupervised distillation for student performance, how does one effectively ensemble knowledge from multiple teachers at this stage without the guidance of ground-truth labels?","We propose a novel algorithm, GOVERN, to tackle this issue.","GOVERN has demonstrated significant improvements in both offline and online experiments.","The proposed algorithm has been successfully deployed in a real-world commercial question-answering system."],"url":"http://arxiv.org/abs/2405.03764v1","category":"cs.CL"}
{"created":"2024-05-06 18:01:06","title":"Band structure engineering using a moir\u00e9 polar substrate","abstract":"Applying long wavelength periodic potentials on quantum materials has recently been demonstrated to be a promising pathway for engineering novel quantum phases of matter. Here, we utilize twisted bilayer boron nitride (BN) as a moir\\'e substrate for band structure engineering. Small-angle-twisted bilayer BN is endowed with periodically arranged up and down polar domains, which imprints a periodic electrostatic potential on a target two-dimensional (2D) material placed on top. As a proof of concept, we use Bernal bilayer graphene as the target material. The resulting modulation of the band structure appears as superlattice resistance peaks, tunable by varying the twist angle, and Hofstadter butterfly physics under a magnetic field. Additionally, we demonstrate the tunability of the moir\\'e potential by altering the dielectric thickness underneath the twisted BN. Finally, we find that near-60{\\deg}-twisted bilayer BN provides a unique platform for studying the moir\\'e structural effect without the contribution from electrostatic moir\\'e potentials. Tunable moir\\'e polar substrates may serve as versatile platforms to engineer the electronic, optical, and mechanical properties of 2D materials and van der Waals heterostructures.","sentences":["Applying long wavelength periodic potentials on quantum materials has recently been demonstrated to be a promising pathway for engineering novel quantum phases of matter.","Here, we utilize twisted bilayer boron nitride (BN) as a moir\\'e substrate for band structure engineering.","Small-angle-twisted bilayer BN is endowed with periodically arranged up and down polar domains, which imprints a periodic electrostatic potential on a target two-dimensional (2D) material placed on top.","As a proof of concept, we use Bernal bilayer graphene as the target material.","The resulting modulation of the band structure appears as superlattice resistance peaks, tunable by varying the twist angle, and Hofstadter butterfly physics under a magnetic field.","Additionally, we demonstrate the tunability of the moir\\'e potential by altering the dielectric thickness underneath the twisted BN.","Finally, we find that near-60{\\deg}-twisted bilayer BN provides a unique platform for studying the moir\\'e structural effect without the contribution from electrostatic moir\\'e potentials.","Tunable moir\\'e","polar substrates may serve as versatile platforms to engineer the electronic, optical, and mechanical properties of 2D materials and van der Waals heterostructures."],"url":"http://arxiv.org/abs/2405.03761v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-06 18:00:03","title":"Early Fault-Tolerant Quantum Algorithms in Practice: Application to Ground-State Energy Estimation","abstract":"We explore the practicality of early fault-tolerant quantum algorithms, focusing on ground-state energy estimation problems. Specifically, we address the computation of the cumulative distribution function (CDF) of the spectral measure of the Hamiltonian and the identification of its discontinuities. Scaling to bigger system sizes unveils three challenges: the smoothness of the CDF for large supports, the absence of tight lower bounds on the overlap with the actual ground state, and the complexity of preparing high-quality initial states. To tackle these challenges, we introduce a signal processing technique for identifying the inflection point of the CDF. We argue that this change of paradigm significantly simplifies the problem, making it more accessible while still being accurate. Hence, instead of trying to find the exact ground-state energy, we advocate improving on the classical estimate by aiming at the low-energy support of the initial state. Furthermore, we offer quantitative resource estimates for the maximum number of samples required to identify an increase in the CDF of a given size. Finally, we conduct numerical experiments on a 26-qubit fully-connected Heisenberg model using a truncated density-matrix renormalization group (DMRG) initial state of low bond dimension. Results show that the prediction obtained with the quantum algorithm aligns well with the DMRG-converged energy at large bond dimensions and requires several orders of magnitude fewer samples than predicted by the theory. Hence, we argue that CDF-based quantum algorithms are a viable, practical alternative to quantum phase estimation in resource-limited scenarios.","sentences":["We explore the practicality of early fault-tolerant quantum algorithms, focusing on ground-state energy estimation problems.","Specifically, we address the computation of the cumulative distribution function (CDF) of the spectral measure of the Hamiltonian and the identification of its discontinuities.","Scaling to bigger system sizes unveils three challenges: the smoothness of the CDF for large supports, the absence of tight lower bounds on the overlap with the actual ground state, and the complexity of preparing high-quality initial states.","To tackle these challenges, we introduce a signal processing technique for identifying the inflection point of the CDF.","We argue that this change of paradigm significantly simplifies the problem, making it more accessible while still being accurate.","Hence, instead of trying to find the exact ground-state energy, we advocate improving on the classical estimate by aiming at the low-energy support of the initial state.","Furthermore, we offer quantitative resource estimates for the maximum number of samples required to identify an increase in the CDF of a given size.","Finally, we conduct numerical experiments on a 26-qubit fully-connected Heisenberg model using a truncated density-matrix renormalization group (DMRG) initial state of low bond dimension.","Results show that the prediction obtained with the quantum algorithm aligns well with the DMRG-converged energy at large bond dimensions and requires several orders of magnitude fewer samples than predicted by the theory.","Hence, we argue that CDF-based quantum algorithms are a viable, practical alternative to quantum phase estimation in resource-limited scenarios."],"url":"http://arxiv.org/abs/2405.03754v1","category":"quant-ph"}
{"created":"2024-05-06 18:00:01","title":"ATClean: A Novel Method for Detecting Low-Luminosity Transients and Application to Pre-explosion Counterparts from SN 2023ixf","abstract":"In an effort to search for faint sources of emission over arbitrary timescales, we present a novel method for analyzing forced photometry light curves in difference imaging from optical surveys. Our method \"ATLAS Clean'' or ATClean, utilizes the reported fluxes, uncertainties, and fits to the point-spread function from difference images to quantify the statistical significance of individual measurements. We apply this method to control light curves across the image to determine whether any source of flux is present in the data for a range of specific timescales. From ATLAS $o$-band imaging at the site of the Type II supernova (SN) 2023ixf in M101 from 2015--2023, we show that this method accurately reproduces the 3$\\sigma$ flux limits produced from other, more computationally expensive methods. We derive limits for emission on timescales of 5~days and 80-300~days at the site of SN\\,2023ixf, which are 19.8 and 21.3~mag, respectively. The latter limits rule out variability for unextinguished red supergiants (RSG) with initial masses $>$22~$M_{\\odot}$, comparable to the most luminous predictions for the SN 2023ixf progenitor system. We also compare our limits to short timescale outbursts, similar to those expected for Type IIn SN progenitor stars or the Type II SN 2020tlf, and rule out outburst ejecta masses of $>$0.021~$M_{\\odot}$, much lower than the inferred mass of circumstellar matter around SN 2023ixf in the literature. In the future, these methods can be applied to any forced point-spread function photometry on difference imaging from other surveys, such as Rubin optical imaging.","sentences":["In an effort to search for faint sources of emission over arbitrary timescales, we present a novel method for analyzing forced photometry light curves in difference imaging from optical surveys.","Our method \"ATLAS Clean'' or ATClean, utilizes the reported fluxes, uncertainties, and fits to the point-spread function from difference images to quantify the statistical significance of individual measurements.","We apply this method to control light curves across the image to determine whether any source of flux is present in the data for a range of specific timescales.","From ATLAS $o$-band imaging at the site of the Type II supernova (SN) 2023ixf in M101 from 2015--2023, we show that this method accurately reproduces the 3$\\sigma$ flux limits produced from other, more computationally expensive methods.","We derive limits for emission on timescales of 5~days and 80-300~days at the site of SN\\,2023ixf, which are 19.8 and 21.3~mag, respectively.","The latter limits rule out variability for unextinguished red supergiants (RSG) with initial masses $>$22~$M_{\\odot}$, comparable to the most luminous predictions for the SN 2023ixf progenitor system.","We also compare our limits to short timescale outbursts, similar to those expected for Type IIn SN progenitor stars or the Type II SN 2020tlf, and rule out outburst ejecta masses of $>$0.021~$M_{\\odot}$, much lower than the inferred mass of circumstellar matter around SN 2023ixf in the literature.","In the future, these methods can be applied to any forced point-spread function photometry on difference imaging from other surveys, such as Rubin optical imaging."],"url":"http://arxiv.org/abs/2405.03747v1","category":"astro-ph.HE"}
{"created":"2024-05-06 18:00:00","title":"The Population II Extragalactic Distance Scale: A TRGB Distance to the Fornax Cluster with JWST","abstract":"Differences between the local value of the Hubble constant measured via the distance ladder versus the value inferred from the cosmic microwave background with the assumption of the standard $\\Lambda$CDM model have reached over 5$\\sigma$ significance. To determine if this discrepancy is due to new physics or more mundane systematic errors, it is essential to remove as many sources of systematic uncertainty as possible by developing high-precision distance ladders that are independent of the traditional Cepheid and Type Ia supernovae route. Here we present JWST observations of three early-type Fornax Cluster galaxies, the first of fourteen observations from a Cycle 2 JWST program. Our modest integration times allow us to measure highly precise tip of the red giant branch (TRGB) distances, and will also be used to perform measurements of Surface Brightness Fluctuations (SBF). From these three galaxies, we determine an average TRGB distance modulus to the Fornax Cluster of $\\mu$ = 31.424 $\\pm$ 0.077 mag, or D = 19.3 $\\pm$ 0.7 Mpc. With eleven more scheduled observations in nearby elliptical galaxies, our program will allow us set the zero point of the SBF scale to better than 2$\\%$ for more distant measurements, charting a path towards a high-precision measurement of $H_{0}$ that is independent of the traditional Cepheid-SN Ia distance ladder.","sentences":["Differences between the local value of the Hubble constant measured via the distance ladder versus the value inferred from the cosmic microwave background with the assumption of the standard $\\Lambda$CDM model have reached over 5$\\sigma$ significance.","To determine if this discrepancy is due to new physics or more mundane systematic errors, it is essential to remove as many sources of systematic uncertainty as possible by developing high-precision distance ladders that are independent of the traditional Cepheid and Type Ia supernovae route.","Here we present JWST observations of three early-type Fornax Cluster galaxies, the first of fourteen observations from a Cycle 2 JWST program.","Our modest integration times allow us to measure highly precise tip of the red giant branch (TRGB) distances, and will also be used to perform measurements of Surface Brightness Fluctuations (SBF).","From these three galaxies, we determine an average TRGB distance modulus to the Fornax Cluster of $\\mu$ = 31.424 $\\pm$ 0.077 mag, or D = 19.3 $\\pm$ 0.7 Mpc.","With eleven more scheduled observations in nearby elliptical galaxies, our program will allow us set the zero point of the SBF scale to better than 2$\\%$ for more distant measurements, charting a path towards a high-precision measurement of $H_{0}$ that is independent of the traditional Cepheid-SN Ia distance ladder."],"url":"http://arxiv.org/abs/2405.03743v1","category":"astro-ph.CO"}
{"created":"2024-05-06 18:00:00","title":"Probing SUSY at Gravitational Wave Observatories","abstract":"Under the assumption that the recent pulsar timing array evidence for a stochastic gravitational wave (GW) background at nanohertz frequencies is generated by metastable cosmic strings, we analyze the potential of present and future GW observatories for probing the change of particle degrees of freedom caused, e.g., by a supersymmetric (SUSY) extension of the Standard Model (SM). We find that signs of the characteristic doubling of degrees of freedom predicted by SUSY could be detected at Einstein Telescope and Cosmic Explorer even if the masses of the SUSY partner particles are as high as about $10^4$ TeV, far above the reach of any currently envisioned particle collider. We also discuss the detection prospects for the case that some entropy production, e.g. from a late decaying modulus field inducing a temporary matter domination phase in the evolution of the universe, somewhat dilutes the GW spectrum, delaying discovery of the stochastic GW background at LIGO-Virgo-KAGRA. In our analysis we focus on SUSY, but any theory beyond the SM predicting a significant increase of particle degrees of freedom could be probed this way.","sentences":["Under the assumption that the recent pulsar timing array evidence for a stochastic gravitational wave (GW) background at nanohertz frequencies is generated by metastable cosmic strings, we analyze the potential of present and future GW observatories for probing the change of particle degrees of freedom caused, e.g., by a supersymmetric (SUSY) extension of the Standard Model (SM).","We find that signs of the characteristic doubling of degrees of freedom predicted by SUSY could be detected at Einstein Telescope and Cosmic Explorer even if the masses of the SUSY partner particles are as high as about $10^4$ TeV, far above the reach of any currently envisioned particle collider.","We also discuss the detection prospects for the case that some entropy production, e.g. from a late decaying modulus field inducing a temporary matter domination phase in the evolution of the universe, somewhat dilutes the GW spectrum, delaying discovery of the stochastic GW background at LIGO-Virgo-KAGRA.","In our analysis we focus on SUSY, but any theory beyond the SM predicting a significant increase of particle degrees of freedom could be probed this way."],"url":"http://arxiv.org/abs/2405.03746v1","category":"hep-ph"}
{"created":"2024-05-06 17:52:04","title":"Why is SAM Robust to Label Noise?","abstract":"Sharpness-Aware Minimization (SAM) is most known for achieving state-of the-art performances on natural image and language tasks. However, its most pronounced improvements (of tens of percent) is rather in the presence of label noise. Understanding SAM's label noise robustness requires a departure from characterizing the robustness of minimas lying in \"flatter\" regions of the loss landscape. In particular, the peak performance under label noise occurs with early stopping, far before the loss converges. We decompose SAM's robustness into two effects: one induced by changes to the logit term and the other induced by changes to the network Jacobian. The first can be observed in linear logistic regression where SAM provably up-weights the gradient contribution from clean examples. Although this explicit up-weighting is also observable in neural networks, when we intervene and modify SAM to remove this effect, surprisingly, we see no visible degradation in performance. We infer that SAM's effect in deeper networks is instead explained entirely by the effect SAM has on the network Jacobian. We theoretically derive the implicit regularization induced by this Jacobian effect in two layer linear networks. Motivated by our analysis, we see that cheaper alternatives to SAM that explicitly induce these regularization effects largely recover the benefits in deep networks trained on real-world datasets.","sentences":["Sharpness-Aware Minimization (SAM) is most known for achieving state-of the-art performances on natural image and language tasks.","However, its most pronounced improvements (of tens of percent) is rather in the presence of label noise.","Understanding SAM's label noise robustness requires a departure from characterizing the robustness of minimas lying in \"flatter\" regions of the loss landscape.","In particular, the peak performance under label noise occurs with early stopping, far before the loss converges.","We decompose SAM's robustness into two effects: one induced by changes to the logit term and the other induced by changes to the network Jacobian.","The first can be observed in linear logistic regression where SAM provably up-weights the gradient contribution from clean examples.","Although this explicit up-weighting is also observable in neural networks, when we intervene and modify SAM to remove this effect, surprisingly, we see no visible degradation in performance.","We infer that SAM's effect in deeper networks is instead explained entirely by the effect SAM has on the network Jacobian.","We theoretically derive the implicit regularization induced by this Jacobian effect in two layer linear networks.","Motivated by our analysis, we see that cheaper alternatives to SAM that explicitly induce these regularization effects largely recover the benefits in deep networks trained on real-world datasets."],"url":"http://arxiv.org/abs/2405.03676v1","category":"cs.LG"}
{"created":"2024-05-06 17:50:35","title":"Topological Quantum Batteries","abstract":"We propose an innovative design for topological quantum batteries that involves coupling two atoms to a one-dimensional lattice with topological features. Employing the resolvent method, we analytically explore the thermodynamic performances of quantum batteries (QBs). First, we demonstrate that only coherent bound states significantly contribute to the stored energy of QBs. We observe near-perfect energy transfer from the quantum charger to the quantum battery (QB) in the topologically nontrivial phase. Conversely, in the topologically trivial phase, we reveal that under the Markov limit, the charging process of the QB is almost completely prohibited due to the emergence of degenerate zero-energy bound states. Moreover, we discover that the maximum energy storage exhibits singular behavior at the phase boundaries. Second, we find that direct coupling between the QB and quantum charger renders the ergotropy immune to sublattice dissipation, facilitated by the presence of a dark state and vacancy-like dressed bound state. Further, we show that as dissipation intensifies along with the emergence of the quantum Zeno effect, the charging power of QBs is transiently enhanced. Our findings provide insightful guidelines for practically enhancing the performance of QBs through structured reservoir engineering.","sentences":["We propose an innovative design for topological quantum batteries that involves coupling two atoms to a one-dimensional lattice with topological features.","Employing the resolvent method, we analytically explore the thermodynamic performances of quantum batteries (QBs).","First, we demonstrate that only coherent bound states significantly contribute to the stored energy of QBs.","We observe near-perfect energy transfer from the quantum charger to the quantum battery (QB) in the topologically nontrivial phase.","Conversely, in the topologically trivial phase, we reveal that under the Markov limit, the charging process of the QB is almost completely prohibited due to the emergence of degenerate zero-energy bound states.","Moreover, we discover that the maximum energy storage exhibits singular behavior at the phase boundaries.","Second, we find that direct coupling between the QB and quantum charger renders the ergotropy immune to sublattice dissipation, facilitated by the presence of a dark state and vacancy-like dressed bound state.","Further, we show that as dissipation intensifies along with the emergence of the quantum Zeno effect, the charging power of QBs is transiently enhanced.","Our findings provide insightful guidelines for practically enhancing the performance of QBs through structured reservoir engineering."],"url":"http://arxiv.org/abs/2405.03675v1","category":"quant-ph"}
{"created":"2024-05-06 17:49:32","title":"Anti-Heroes: An Ethics-focused Method for Responsible Designer Intentions","abstract":"HCI and design researchers have designed, adopted, and customized a range of ethics-focused methods to inscribe values and support ethical decision making in a design process. In this work-in-progress, we add to this body of resources, constructing a method that surfaces the designer's intentions in an action-focused way, encouraging consideration of both manipulative and value-centered roles. Anti-Heroes is a card deck that allows a designer to playfully take on pairs of manipulative (Anti-Hero) and value-centered (Hero) roles during design ideation/conceptualization, evaluation, and ethical dialogue. The card deck includes twelve cards with Anti-Hero and Hero faces, along with three action cards that include reflective questions for different play modes. Alongside the creation of the Anti-Hero card deck, we describe the evaluation and iteration of the card deck through playtesting sessions with four groups of three design students. We propose implications of Anti-Heros for technology and design education and practice.","sentences":["HCI and design researchers have designed, adopted, and customized a range of ethics-focused methods to inscribe values and support ethical decision making in a design process.","In this work-in-progress, we add to this body of resources, constructing a method that surfaces the designer's intentions in an action-focused way, encouraging consideration of both manipulative and value-centered roles.","Anti-Heroes is a card deck that allows a designer to playfully take on pairs of manipulative (Anti-Hero) and value-centered (Hero) roles during design ideation/conceptualization, evaluation, and ethical dialogue.","The card deck includes twelve cards with Anti-Hero and Hero faces, along with three action cards that include reflective questions for different play modes.","Alongside the creation of the Anti-Hero card deck, we describe the evaluation and iteration of the card deck through playtesting sessions with four groups of three design students.","We propose implications of Anti-Heros for technology and design education and practice."],"url":"http://arxiv.org/abs/2405.03674v1","category":"cs.HC"}
{"created":"2024-05-07 16:53:42","title":"Adapting WavLM for Speech Emotion Recognition","abstract":"Recently, the usage of speech self-supervised models (SSL) for downstream tasks has been drawing a lot of attention. While large pre-trained models commonly outperform smaller models trained from scratch, questions regarding the optimal fine-tuning strategies remain prevalent. In this paper, we explore the fine-tuning strategies of the WavLM Large model for the speech emotion recognition task on the MSP Podcast Corpus. More specifically, we perform a series of experiments focusing on using gender and semantic information from utterances. We then sum up our findings and describe the final model we used for submission to Speech Emotion Recognition Challenge 2024.","sentences":["Recently, the usage of speech self-supervised models (SSL) for downstream tasks has been drawing a lot of attention.","While large pre-trained models commonly outperform smaller models trained from scratch, questions regarding the optimal fine-tuning strategies remain prevalent.","In this paper, we explore the fine-tuning strategies of the WavLM Large model for the speech emotion recognition task on the MSP Podcast Corpus.","More specifically, we perform a series of experiments focusing on using gender and semantic information from utterances.","We then sum up our findings and describe the final model we used for submission to Speech Emotion Recognition Challenge 2024."],"url":"http://arxiv.org/abs/2405.04485v1","category":"cs.LG"}
{"created":"2024-05-07 15:00:11","title":"Choose What You Need: Disentangled Representation Learning for Scene Text Recognition, Removal and Editing","abstract":"Scene text images contain not only style information (font, background) but also content information (character, texture). Different scene text tasks need different information, but previous representation learning methods use tightly coupled features for all tasks, resulting in sub-optimal performance. We propose a Disentangled Representation Learning framework (DARLING) aimed at disentangling these two types of features for improved adaptability in better addressing various downstream tasks (choose what you really need). Specifically, we synthesize a dataset of image pairs with identical style but different content. Based on the dataset, we decouple the two types of features by the supervision design. Clearly, we directly split the visual representation into style and content features, the content features are supervised by a text recognition loss, while an alignment loss aligns the style features in the image pairs. Then, style features are employed in reconstructing the counterpart image via an image decoder with a prompt that indicates the counterpart's content. Such an operation effectively decouples the features based on their distinctive properties. To the best of our knowledge, this is the first time in the field of scene text that disentangles the inherent properties of the text images. Our method achieves state-of-the-art performance in Scene Text Recognition, Removal, and Editing.","sentences":["Scene text images contain not only style information (font, background) but also content information (character, texture).","Different scene text tasks need different information, but previous representation learning methods use tightly coupled features for all tasks, resulting in sub-optimal performance.","We propose a Disentangled Representation Learning framework (DARLING) aimed at disentangling these two types of features for improved adaptability in better addressing various downstream tasks (choose what you really need).","Specifically, we synthesize a dataset of image pairs with identical style but different content.","Based on the dataset, we decouple the two types of features by the supervision design.","Clearly, we directly split the visual representation into style and content features, the content features are supervised by a text recognition loss, while an alignment loss aligns the style features in the image pairs.","Then, style features are employed in reconstructing the counterpart image via an image decoder with a prompt that indicates the counterpart's content.","Such an operation effectively decouples the features based on their distinctive properties.","To the best of our knowledge, this is the first time in the field of scene text that disentangles the inherent properties of the text images.","Our method achieves state-of-the-art performance in Scene Text Recognition, Removal, and Editing."],"url":"http://arxiv.org/abs/2405.04377v1","category":"cs.CV"}
{"created":"2024-05-07 14:58:12","title":"Towards Stability of Parameter-free Optimization","abstract":"Hyperparameter tuning, particularly the selection of an appropriate learning rate in adaptive gradient training methods, remains a challenge. To tackle this challenge, in this paper, we propose a novel parameter-free optimizer, AdamG (Adam with the golden step size), designed to automatically adapt to diverse optimization problems without manual tuning. The core technique underlying AdamG is our golden step size derived for the AdaGrad-Norm algorithm, which is expected to help AdaGrad-Norm preserve the tuning-free convergence and approximate the optimal step size in expectation w.r.t. various optimization scenarios. To better evaluate tuning-free performance, we propose a novel evaluation criterion, stability, to comprehensively assess the efficacy of parameter-free optimizers in addition to classical performance criteria. Empirical results demonstrate that compared with other parameter-free baselines, AdamG achieves superior performance, which is consistently on par with Adam using a manually tuned learning rate across various optimization tasks.","sentences":["Hyperparameter tuning, particularly the selection of an appropriate learning rate in adaptive gradient training methods, remains a challenge.","To tackle this challenge, in this paper, we propose a novel parameter-free optimizer, AdamG (Adam with the golden step size), designed to automatically adapt to diverse optimization problems without manual tuning.","The core technique underlying AdamG is our golden step size derived for the AdaGrad-Norm algorithm, which is expected to help AdaGrad-Norm preserve the tuning-free convergence and approximate the optimal step size in expectation w.r.t.","various optimization scenarios.","To better evaluate tuning-free performance, we propose a novel evaluation criterion, stability, to comprehensively assess the efficacy of parameter-free optimizers in addition to classical performance criteria.","Empirical results demonstrate that compared with other parameter-free baselines, AdamG achieves superior performance, which is consistently on par with Adam using a manually tuned learning rate across various optimization tasks."],"url":"http://arxiv.org/abs/2405.04376v1","category":"cs.LG"}
{"created":"2024-05-07 14:36:33","title":"A Personalizable Controller for the Walking Assistive omNi-Directional Exo-Robot (WANDER)","abstract":"Preserving and encouraging mobility in the elderly and adults with chronic conditions is of paramount importance. However, existing walking aids are either inadequate to provide sufficient support to users' stability or too bulky and poorly maneuverable to be used outside hospital environments. In addition, they all lack adaptability to individual requirements. To address these challenges, this paper introduces WANDER, a novel Walking Assistive omNi-Directional Exo-Robot. It consists of an omnidirectional platform and a robust aluminum structure mounted on top of it, which provides partial body weight support. A comfortable and minimally restrictive coupling interface embedded with a force/torque sensor allows to detect users' intentions, which are translated into command velocities by means of a variable admittance controller. An optimization technique based on users' preferences, i.e., Preference-Based Optimization (PBO) guides the choice of the admittance parameters (i.e., virtual mass and damping) to better fit subject-specific needs and characteristics. Experiments with twelve healthy subjects exhibited a significant decrease in energy consumption and jerk when using WANDER with PBO parameters as well as improved user performance and comfort. The great interpersonal variability in the optimized parameters highlights the importance of personalized control settings when walking with an assistive device, aiming to enhance users' comfort and mobility while ensuring reliable physical support.","sentences":["Preserving and encouraging mobility in the elderly and adults with chronic conditions is of paramount importance.","However, existing walking aids are either inadequate to provide sufficient support to users' stability or too bulky and poorly maneuverable to be used outside hospital environments.","In addition, they all lack adaptability to individual requirements.","To address these challenges, this paper introduces WANDER, a novel Walking Assistive omNi-Directional Exo-Robot.","It consists of an omnidirectional platform and a robust aluminum structure mounted on top of it, which provides partial body weight support.","A comfortable and minimally restrictive coupling interface embedded with a force/torque sensor allows to detect users' intentions, which are translated into command velocities by means of a variable admittance controller.","An optimization technique based on users' preferences, i.e., Preference-Based Optimization (PBO) guides the choice of the admittance parameters (i.e., virtual mass and damping) to better fit subject-specific needs and characteristics.","Experiments with twelve healthy subjects exhibited a significant decrease in energy consumption and jerk when using WANDER with PBO parameters as well as improved user performance and comfort.","The great interpersonal variability in the optimized parameters highlights the importance of personalized control settings when walking with an assistive device, aiming to enhance users' comfort and mobility while ensuring reliable physical support."],"url":"http://arxiv.org/abs/2405.04359v1","category":"cs.RO"}
{"created":"2024-05-07 11:05:47","title":"Quantum software experiments: A reporting and laboratory package structure guidelines","abstract":"Background. In the realm of software engineering, there are widely accepted guidelines for reporting and creating laboratory packages. Unfortunately, the landscape differs considerably in the emerging field of quantum computing. To the best of our knowledge, no standardized guidelines exist for describing experiments or outlining the necessary structures for quantum software laboratory packages. Aims. This paper endeavors to enhance the replicability and verifiability of quantum software experiments. Method. This objective is pursued through the proposition of guidelines for reporting and the delineation of a structure for laboratory packages tailored to quantum computing experiments. Specifically, we advocate for an extension and adaption of established guidelines in experimental software engineering, integrating novel elements to address the specific requirements of quantum software engineering. Results. In validating the utility and effectiveness of the proposed guidelines, we conducted a review encompassing 11 works (5 focusing on reporting guidelines and 6 on laboratory packages). In particular, this review highlighted the absence of standardized guidelines and structure of laboratory packages for quantum software experiments. Conclusions. Our assessment revealed gaps in information and opportunities for enhancement within the evaluated papers and laboratory packages. Our proposal contributes to the advancement of quantum software engineering research, taking a fundamental step toward fostering rigorous and reliable scientific research in this emerging paradigm.","sentences":["Background.","In the realm of software engineering, there are widely accepted guidelines for reporting and creating laboratory packages.","Unfortunately, the landscape differs considerably in the emerging field of quantum computing.","To the best of our knowledge, no standardized guidelines exist for describing experiments or outlining the necessary structures for quantum software laboratory packages.","Aims.","This paper endeavors to enhance the replicability and verifiability of quantum software experiments.","Method.","This objective is pursued through the proposition of guidelines for reporting and the delineation of a structure for laboratory packages tailored to quantum computing experiments.","Specifically, we advocate for an extension and adaption of established guidelines in experimental software engineering, integrating novel elements to address the specific requirements of quantum software engineering.","Results.","In validating the utility and effectiveness of the proposed guidelines, we conducted a review encompassing 11 works (5 focusing on reporting guidelines and 6 on laboratory packages).","In particular, this review highlighted the absence of standardized guidelines and structure of laboratory packages for quantum software experiments.","Conclusions.","Our assessment revealed gaps in information and opportunities for enhancement within the evaluated papers and laboratory packages.","Our proposal contributes to the advancement of quantum software engineering research, taking a fundamental step toward fostering rigorous and reliable scientific research in this emerging paradigm."],"url":"http://arxiv.org/abs/2405.04192v1","category":"cs.SE"}
{"created":"2024-05-07 10:00:38","title":"Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation","abstract":"Automatic Sign Language Translation requires the integration of both computer vision and natural language processing to effectively bridge the communication gap between sign and spoken languages. However, the deficiency in large-scale training data to support sign language translation means we need to leverage resources from spoken language. We introduce, Sign2GPT, a novel framework for sign language translation that utilizes large-scale pretrained vision and language models via lightweight adapters for gloss-free sign language translation. The lightweight adapters are crucial for sign language translation, due to the constraints imposed by limited dataset sizes and the computational requirements when training with long sign videos. We also propose a novel pretraining strategy that directs our encoder to learn sign representations from automatically extracted pseudo-glosses without requiring gloss order information or annotations. We evaluate our approach on two public benchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T and CSL-Daily, and improve on state-of-the-art gloss-free translation performance with a significant margin.","sentences":["Automatic Sign Language Translation requires the integration of both computer vision and natural language processing to effectively bridge the communication gap between sign and spoken languages.","However, the deficiency in large-scale training data to support sign language translation means we need to leverage resources from spoken language.","We introduce, Sign2GPT, a novel framework for sign language translation that utilizes large-scale pretrained vision and language models via lightweight adapters for gloss-free sign language translation.","The lightweight adapters are crucial for sign language translation, due to the constraints imposed by limited dataset sizes and the computational requirements when training with long sign videos.","We also propose a novel pretraining strategy that directs our encoder to learn sign representations from automatically extracted pseudo-glosses without requiring gloss order information or annotations.","We evaluate our approach on two public benchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T and CSL-Daily, and improve on state-of-the-art gloss-free translation performance with a significant margin."],"url":"http://arxiv.org/abs/2405.04164v1","category":"cs.CV"}
{"created":"2024-05-07 08:50:25","title":"Refining Joint Text and Source Code Embeddings for Retrieval Task with Parameter-Efficient Fine-Tuning","abstract":"The latest developments in Natural Language Processing (NLP) have demonstrated remarkable progress in a code-text retrieval problem. As the Transformer-based models used in this task continue to increase in size, the computational costs and time required for end-to-end fine-tuning become substantial. This poses a significant challenge for adapting and utilizing these models when computational resources are limited. Motivated by these concerns, we propose a fine-tuning framework that leverages Parameter-Efficient Fine-Tuning (PEFT) techniques. Moreover, we adopt contrastive learning objectives to improve the quality of bimodal representations learned by transformer models. Additionally, for PEFT methods we provide extensive benchmarking, the lack of which has been highlighted as a crucial problem in the literature. Based on the thorough experimentation with the CodeT5+ model conducted on two datasets, we demonstrate that the proposed fine-tuning framework has the potential to improve code-text retrieval performance by tuning only 0.4% parameters at most.","sentences":["The latest developments in Natural Language Processing (NLP) have demonstrated remarkable progress in a code-text retrieval problem.","As the Transformer-based models used in this task continue to increase in size, the computational costs and time required for end-to-end fine-tuning become substantial.","This poses a significant challenge for adapting and utilizing these models when computational resources are limited.","Motivated by these concerns, we propose a fine-tuning framework that leverages Parameter-Efficient Fine-Tuning (PEFT) techniques.","Moreover, we adopt contrastive learning objectives to improve the quality of bimodal representations learned by transformer models.","Additionally, for PEFT methods we provide extensive benchmarking, the lack of which has been highlighted as a crucial problem in the literature.","Based on the thorough experimentation with the CodeT5+ model conducted on two datasets, we demonstrate that the proposed fine-tuning framework has the potential to improve code-text retrieval performance by tuning only 0.4% parameters at most."],"url":"http://arxiv.org/abs/2405.04126v1","category":"cs.LG"}
{"created":"2024-05-07 08:12:38","title":"Effect of realistic oscillator phase noise on the performance of cell-free networks","abstract":"To keep supporting 6G requirements, the radio access infrastructure will increasingly densify. Cell-free (CF) networks offer extreme flexibility by coherently serving users with multiple Access points (APs). This paradigm requires precise and stable phase synchronization. In this article, we adapt the standardized 5G NR setup (subcarrier spacing, OFDM symbol duration and allocation) to investigate the effect of Phase Noise (PN) on the simulated performance of scalable CF networks. In contrast to the prior literature relying on the simplified model of a free-running oscillator with the Wiener process, we deploy a realistic hardware-inspired phase noise model reproducing the Local Oscillator (LO) phase drift. Our results demonstrate that even affordable LOs offer sufficient stability to ensure negligible loss of uplink Spectral Efficiency (SE) on the time scale of the standardized 5G Transmission Time Interval of 1 ms. This study substantiates the feasibility of CF networks based on 5G standards.","sentences":["To keep supporting 6G requirements, the radio access infrastructure will increasingly densify.","Cell-free (CF) networks offer extreme flexibility by coherently serving users with multiple Access points (APs).","This paradigm requires precise and stable phase synchronization.","In this article, we adapt the standardized 5G NR setup (subcarrier spacing, OFDM symbol duration and allocation) to investigate the effect of Phase Noise (PN) on the simulated performance of scalable CF networks.","In contrast to the prior literature relying on the simplified model of a free-running oscillator with the Wiener process, we deploy a realistic hardware-inspired phase noise model reproducing the Local Oscillator (LO) phase drift.","Our results demonstrate that even affordable LOs offer sufficient stability to ensure negligible loss of uplink Spectral Efficiency (SE) on the time scale of the standardized 5G Transmission Time Interval of 1 ms.","This study substantiates the feasibility of CF networks based on 5G standards."],"url":"http://arxiv.org/abs/2405.04099v1","category":"cs.NI"}
{"created":"2024-05-07 06:42:30","title":"Watermarking Neuromorphic Brains: Intellectual Property Protection in Spiking Neural Networks","abstract":"As spiking neural networks (SNNs) gain traction in deploying neuromorphic computing solutions, protecting their intellectual property (IP) has become crucial. Without adequate safeguards, proprietary SNN architectures are at risk of theft, replication, or misuse, which could lead to significant financial losses for the owners. While IP protection techniques have been extensively explored for artificial neural networks (ANNs), their applicability and effectiveness for the unique characteristics of SNNs remain largely unexplored. In this work, we pioneer an investigation into adapting two prominent watermarking approaches, namely, fingerprint-based and backdoor-based mechanisms to secure proprietary SNN architectures. We conduct thorough experiments to evaluate the impact on fidelity, resilience against overwrite threats, and resistance to compression attacks when applying these watermarking techniques to SNNs, drawing comparisons with their ANN counterparts. This study lays the groundwork for developing neuromorphic-aware IP protection strategies tailored to the distinctive dynamics of SNNs.","sentences":["As spiking neural networks (SNNs) gain traction in deploying neuromorphic computing solutions, protecting their intellectual property (IP) has become crucial.","Without adequate safeguards, proprietary SNN architectures are at risk of theft, replication, or misuse, which could lead to significant financial losses for the owners.","While IP protection techniques have been extensively explored for artificial neural networks (ANNs), their applicability and effectiveness for the unique characteristics of SNNs remain largely unexplored.","In this work, we pioneer an investigation into adapting two prominent watermarking approaches, namely, fingerprint-based and backdoor-based mechanisms to secure proprietary SNN architectures.","We conduct thorough experiments to evaluate the impact on fidelity, resilience against overwrite threats, and resistance to compression attacks when applying these watermarking techniques to SNNs, drawing comparisons with their ANN counterparts.","This study lays the groundwork for developing neuromorphic-aware IP protection strategies tailored to the distinctive dynamics of SNNs."],"url":"http://arxiv.org/abs/2405.04049v1","category":"cs.CR"}
{"created":"2024-05-07 05:32:26","title":"Generalized Langevin dynamics for single beads in linear elastic network","abstract":"We derive generalized Langevin equations (GLEs) for single beads in linear elastic networks. In particular, the derivations of the GLEs are conducted without employing normal modes, resulting in two distinct representations in terms of resistance and mobility kernels. The fluctuation-dissipation relations are also confirmed for both GLEs. Subsequently, we demonstrate that these two representations are interconnected via Laplace transforms. Furthermore, another GLE is derived by utilizing a projection operator method, and it is shown that the equation obtained through the projection scheme is consistent with the GLE with the resistance kernel. Finally, as the simplest example, the present framework is applied to the Rouse model, and the GLEs with the resistance and mobility kernels are explicitly derived for arbitrary positions of the tagged bead in the Rouse chain.","sentences":["We derive generalized Langevin equations (GLEs) for single beads in linear elastic networks.","In particular, the derivations of the GLEs are conducted without employing normal modes, resulting in two distinct representations in terms of resistance and mobility kernels.","The fluctuation-dissipation relations are also confirmed for both GLEs.","Subsequently, we demonstrate that these two representations are interconnected via Laplace transforms.","Furthermore, another GLE is derived by utilizing a projection operator method, and it is shown that the equation obtained through the projection scheme is consistent with the GLE with the resistance kernel.","Finally, as the simplest example, the present framework is applied to the Rouse model, and the GLEs with the resistance and mobility kernels are explicitly derived for arbitrary positions of the tagged bead in the Rouse chain."],"url":"http://arxiv.org/abs/2405.04019v1","category":"cond-mat.soft"}
{"created":"2024-05-07 05:14:31","title":"On the uniqueness of the mild solution of the critical quasi-geostrophic equation","abstract":"We demonstrate that the uniqueness of the mild solution of the two-dimensional quasi-geostrophic equation with the critical dissipation holds in the scaling critical homogeneous Besov space $\\dot{B}^0_{\\infty,1}$. We consider a solustion of integral equation, and our result does not need regularity assumption.","sentences":["We demonstrate that the uniqueness of the mild solution of the two-dimensional quasi-geostrophic equation with the critical dissipation holds in the scaling critical homogeneous Besov space $\\dot{B}^0_{\\infty,1}$. We consider a solustion of integral equation, and our result does not need regularity assumption."],"url":"http://arxiv.org/abs/2405.04014v1","category":"math.AP"}
{"created":"2024-05-07 02:54:10","title":"Memristive switching in the surface of a charge-density-wave topological semimetal","abstract":"Owing to the outstanding properties provided by nontrivial band topology, topological phases of matter are considered as a promising platform towards low-dissipation electronics, efficient spin-charge conversion, and topological quantum computation. Achieving ferroelectricity in topological materials enables the non-volatile control of the quantum states, which could greatly facilitate topological electronic research. However, ferroelectricity is generally incompatible with systems featuring metallicity due to the screening effect of free carriers. In this study, we report the observation of memristive switching based on the ferroelectric surface state of a topological semimetal (TaSe4)2I. We find that the surface state of (TaSe4)2I presents out-of-plane ferroelectric polarization due to surface reconstruction. With the combination of ferroelectric surface and charge-density-wave-gapped bulk states, an electric switchable barrier height can be achieved in (TaSe4)2I-metal contact. By employing a multi-terminal grounding design, we manage to construct a prototype ferroelectric memristor based on (TaSe4)2I with on/off ratio up to 10^3, endurance over 10^3 cycles, and good retention characteristics. The origin of the ferroelectric surface state is further investigated by first-principles calculations, which reveals an interplay between ferroelectricity and band topology. The emergence of ferroelectricity in (TaSe4)2I not only demonstrates it as a rare but essential case of ferroelectric topological materials, but also opens new routes towards the implementation of topological materials in functional electronic devices.","sentences":["Owing to the outstanding properties provided by nontrivial band topology, topological phases of matter are considered as a promising platform towards low-dissipation electronics, efficient spin-charge conversion, and topological quantum computation.","Achieving ferroelectricity in topological materials enables the non-volatile control of the quantum states, which could greatly facilitate topological electronic research.","However, ferroelectricity is generally incompatible with systems featuring metallicity due to the screening effect of free carriers.","In this study, we report the observation of memristive switching based on the ferroelectric surface state of a topological semimetal (TaSe4)2I. We find that the surface state of (TaSe4)2I presents out-of-plane ferroelectric polarization due to surface reconstruction.","With the combination of ferroelectric surface and charge-density-wave-gapped bulk states, an electric switchable barrier height can be achieved in (TaSe4)2I-metal contact.","By employing a multi-terminal grounding design, we manage to construct a prototype ferroelectric memristor based on (TaSe4)2I with on/off ratio up to 10^3, endurance over 10^3 cycles, and good retention characteristics.","The origin of the ferroelectric surface state is further investigated by first-principles calculations, which reveals an interplay between ferroelectricity and band topology.","The emergence of ferroelectricity in (TaSe4)2I not only demonstrates it as a rare but essential case of ferroelectric topological materials, but also opens new routes towards the implementation of topological materials in functional electronic devices."],"url":"http://arxiv.org/abs/2405.03966v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 02:42:17","title":"Adaptive Speech Emotion Representation Learning Based On Dynamic Graph","abstract":"Graph representation learning has become a hot research topic due to its powerful nonlinear fitting capability in extracting representative node embeddings. However, for sequential data such as speech signals, most traditional methods merely focus on the static graph created within a sequence, and largely overlook the intrinsic evolving patterns of these data. This may reduce the efficiency of graph representation learning for sequential data. For this reason, we propose an adaptive graph representation learning method based on dynamically evolved graphs, which are consecutively constructed on a series of subsequences segmented by a sliding window. In doing this, it is better to capture local and global context information within a long sequence. Moreover, we introduce a weighted approach to update the node representation rather than the conventional average one, where the weights are calculated by a novel matrix computation based on the degree of neighboring nodes. Finally, we construct a learnable graph convolutional layer that combines the graph structure loss and classification loss to optimize the graph structure. To verify the effectiveness of the proposed method, we conducted experiments for speech emotion recognition on the IEMOCAP and RAVDESS datasets. Experimental results show that the proposed method outperforms the latest (non-)graph-based models.","sentences":["Graph representation learning has become a hot research topic due to its powerful nonlinear fitting capability in extracting representative node embeddings.","However, for sequential data such as speech signals, most traditional methods merely focus on the static graph created within a sequence, and largely overlook the intrinsic evolving patterns of these data.","This may reduce the efficiency of graph representation learning for sequential data.","For this reason, we propose an adaptive graph representation learning method based on dynamically evolved graphs, which are consecutively constructed on a series of subsequences segmented by a sliding window.","In doing this, it is better to capture local and global context information within a long sequence.","Moreover, we introduce a weighted approach to update the node representation rather than the conventional average one, where the weights are calculated by a novel matrix computation based on the degree of neighboring nodes.","Finally, we construct a learnable graph convolutional layer that combines the graph structure loss and classification loss to optimize the graph structure.","To verify the effectiveness of the proposed method, we conducted experiments for speech emotion recognition on the IEMOCAP and RAVDESS datasets.","Experimental results show that the proposed method outperforms the latest (non-)graph-based models."],"url":"http://arxiv.org/abs/2405.03956v1","category":"cs.SD"}
{"created":"2024-05-06 20:22:56","title":"Cloud Storage Integrity at Scale: A Case for Dynamic Hash Trees","abstract":"Merkle hash trees are the state-of-the-art method to protect the integrity of storage systems. However, using a hash tree can severely degrade performance, and prior works optimizing them have yet to yield a concrete understanding of the scalability of certain designs in the context of large-scale cloud storage systems. In this paper, we take a first-principles approach to analyzing hash tree performance for storage by introducing a definition of an optimal hash tree and a principled methodology for evaluating hash tree designs. We show that state-of-the-art designs are not scalable; they incur up to 40.1X slowdowns over an insecure baseline and deliver <50% of optimal performance across various experiments. We then exploit the characteristics of optimal hash trees to design Dynamic Hash Trees (DHTs), hash trees that can adapt to workload patterns on-the-fly, delivering >95% of optimal read and write performance and up to 4.2X speedups over the state-of-the art. Our novel methodology and DHT design provides a new foundation in the search for integrity mechanisms that can operate efficiently at scale.","sentences":["Merkle hash trees are the state-of-the-art method to protect the integrity of storage systems.","However, using a hash tree can severely degrade performance, and prior works optimizing them have yet to yield a concrete understanding of the scalability of certain designs in the context of large-scale cloud storage systems.","In this paper, we take a first-principles approach to analyzing hash tree performance for storage by introducing a definition of an optimal hash tree and a principled methodology for evaluating hash tree designs.","We show that state-of-the-art designs are not scalable; they incur up to 40.1X slowdowns over an insecure baseline and deliver <50% of optimal performance across various experiments.","We then exploit the characteristics of optimal hash trees to design Dynamic Hash Trees (DHTs), hash trees that can adapt to workload patterns on-the-fly, delivering >95% of optimal read and write performance and up to 4.2X speedups over the state-of-the art.","Our novel methodology and DHT design provides a new foundation in the search for integrity mechanisms that can operate efficiently at scale."],"url":"http://arxiv.org/abs/2405.03830v1","category":"cs.CR"}
{"created":"2024-05-06 18:13:37","title":"Novel Tour Construction Heuristic for Pick-Up and Delivery Routing Problems","abstract":"In logistic applications that require the pickup and delivery of items, route optimization problems can be modeled as precedence constrained traveling salesperson problems. The combinatorial nature of this problem restricts the application of exact algorithms to small instances, and heuristics are largely preferred for tractability. However, due to precedence constraints that restrict the order in which locations can be visited, heuristics outside of the nearest neighbor algorithm have been neglected in literature. While the convex hull cheapest insertion heuristic is known to produce good solutions in the absence of precedence constraints, i.e., when locations can be visited in any order, it has not been adapted for pick-up and delivery considerations. This paper presents an adapted convex hull cheapest insertion heuristic that accounts for precedence constraints and compares its solutions with the nearest neighbor heuristic using the TSPLIB benchmark data set. The proposed algorithm is particularly suited to cases where pickups are located in the periphery and deliveries are centrally located, outperforming the Nearest Neighbor algorithm in every examined instance.","sentences":["In logistic applications that require the pickup and delivery of items, route optimization problems can be modeled as precedence constrained traveling salesperson problems.","The combinatorial nature of this problem restricts the application of exact algorithms to small instances, and heuristics are largely preferred for tractability.","However, due to precedence constraints that restrict the order in which locations can be visited, heuristics outside of the nearest neighbor algorithm have been neglected in literature.","While the convex hull cheapest insertion heuristic is known to produce good solutions in the absence of precedence constraints, i.e., when locations can be visited in any order, it has not been adapted for pick-up and delivery considerations.","This paper presents an adapted convex hull cheapest insertion heuristic that accounts for precedence constraints and compares its solutions with the nearest neighbor heuristic using the TSPLIB benchmark data set.","The proposed algorithm is particularly suited to cases where pickups are located in the periphery and deliveries are centrally located, outperforming the Nearest Neighbor algorithm in every examined instance."],"url":"http://arxiv.org/abs/2405.03774v1","category":"math.CO"}
{"created":"2024-05-06 18:09:48","title":"Foundation Models for Video Understanding: A Survey","abstract":"Video Foundation Models (ViFMs) aim to learn a general-purpose representation for various video understanding tasks. Leveraging large-scale datasets and powerful models, ViFMs achieve this by capturing robust and generic features from video data. This survey analyzes over 200 video foundational models, offering a comprehensive overview of benchmarks and evaluation metrics across 14 distinct video tasks categorized into 3 main categories. Additionally, we offer an in-depth performance analysis of these models for the 6 most common video tasks. We categorize ViFMs into three categories: 1) Image-based ViFMs, which adapt existing image models for video tasks, 2) Video-Based ViFMs, which utilize video-specific encoding methods, and 3) Universal Foundational Models (UFMs), which combine multiple modalities (image, video, audio, and text etc.) within a single framework. By comparing the performance of various ViFMs on different tasks, this survey offers valuable insights into their strengths and weaknesses, guiding future advancements in video understanding. Our analysis surprisingly reveals that image-based foundation models consistently outperform video-based models on most video understanding tasks. Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance on video tasks. We share the comprehensive list of ViFMs studied in this work at: \\url{https://github.com/NeeluMadan/ViFM_Survey.git}","sentences":["Video Foundation Models (ViFMs) aim to learn a general-purpose representation for various video understanding tasks.","Leveraging large-scale datasets and powerful models, ViFMs achieve this by capturing robust and generic features from video data.","This survey analyzes over 200 video foundational models, offering a comprehensive overview of benchmarks and evaluation metrics across 14 distinct video tasks categorized into 3 main categories.","Additionally, we offer an in-depth performance analysis of these models for the 6 most common video tasks.","We categorize ViFMs into three categories: 1) Image-based ViFMs, which adapt existing image models for video tasks, 2) Video-Based ViFMs, which utilize video-specific encoding methods, and 3) Universal Foundational Models (UFMs), which combine multiple modalities (image, video, audio, and text etc.) within a single framework.","By comparing the performance of various ViFMs on different tasks, this survey offers valuable insights into their strengths and weaknesses, guiding future advancements in video understanding.","Our analysis surprisingly reveals that image-based foundation models consistently outperform video-based models on most video understanding tasks.","Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance on video tasks.","We share the comprehensive list of ViFMs studied in this work at: \\url{https://github.com/NeeluMadan/ViFM_Survey.git}"],"url":"http://arxiv.org/abs/2405.03770v1","category":"cs.CV"}
{"created":"2024-05-06 18:00:00","title":"White dwarf eccentricity fluctuation and dissipation by AGB convection","abstract":"Millisecond pulsars with white dwarf companions have typical eccentricities $e\\sim 10^{-6}-10^{-3}$. The eccentricities of helium white dwarfs are explained well by applying the fluctuation-dissipation theorem to convective eddies in their red giant progenitors. We extend this theory to more massive carbon-oxygen (CO) white dwarfs with asymptotic giant branch (AGB) progenitors. Due to the radiation pressure in AGB stars, the dominant factor in determining the remnant white dwarf's eccentricity is the critical residual hydrogen envelope mass $m_{\\rm env}$ required to inflate the star to giant proportions. Using a suite of MESA stellar evolution simulations with $\\Delta m_{\\rm c}=10^{-3}\\,{\\rm M}_\\odot$ core-mass intervals, we resolved the AGB thermal pulses and found that the critical $m_{\\rm env}\\propto m_{\\rm c}^{-6}$. This steep dependence causes the $e(m_{\\rm c})$ relation to turn over, such that $e\\sim 3\\times 10^{-3}$ almost independently of the remnant CO white dwarf's mass $m_{\\rm c}$. Nearly all of the measured eccentricities lie below this robust theoretical limit, indicating that the eccentricity is damped during the common-envelope inspiral that follows the unstable Roche-lobe overflow of the AGB star. Specifically, we focused on white dwarfs with median masses $m_{\\rm c}>0.6\\,{\\rm M}_\\odot$. These massive white dwarfs begin their inspiral with practically identical orbital periods and eccentricities, eliminating any dependence on the initial conditions. For this sub-sample, we find an empirical relation $e\\propto P^{3/2}$ between the final period and eccentricity that is much tighter than previous studies - motivating theoretical work on the eccentricity evolution during the common envelope phase.","sentences":["Millisecond pulsars with white dwarf companions have typical eccentricities $e\\sim 10^{-6}-10^{-3}$.","The eccentricities of helium white dwarfs are explained well by applying the fluctuation-dissipation theorem to convective eddies in their red giant progenitors.","We extend this theory to more massive carbon-oxygen (CO) white dwarfs with asymptotic giant branch (AGB) progenitors.","Due to the radiation pressure in AGB stars, the dominant factor in determining the remnant white dwarf's eccentricity is the critical residual hydrogen envelope mass $m_{\\rm env}$ required to inflate the star to giant proportions.","Using a suite of MESA stellar evolution simulations with $\\Delta m_{\\rm c}=10^{-3}\\,{\\rm M}_\\odot$ core-mass intervals, we resolved the AGB thermal pulses and found that the critical $m_{\\rm env}\\propto m_{\\rm c}^{-6}$. This steep dependence causes the $e(m_{\\rm c})$ relation to turn over, such that $e\\sim 3\\times 10^{-3}$ almost independently of the remnant CO white dwarf's mass $m_{\\rm c}$. Nearly all of the measured eccentricities lie below this robust theoretical limit, indicating that the eccentricity is damped during the common-envelope inspiral that follows the unstable Roche-lobe overflow of the AGB star.","Specifically, we focused on white dwarfs with median masses $m_{\\rm c}>0.6\\,{\\rm M}_\\odot$.","These massive white dwarfs begin their inspiral with practically identical orbital periods and eccentricities, eliminating any dependence on the initial conditions.","For this sub-sample, we find an empirical relation $e\\propto P^{3/2}$ between the final period and eccentricity that is much tighter than previous studies - motivating theoretical work on the eccentricity evolution during the common envelope phase."],"url":"http://arxiv.org/abs/2405.03745v1","category":"astro-ph.SR"}
{"created":"2024-05-06 17:38:20","title":"Competitive strategies to use \"warm start\" algorithms with predictions","abstract":"We consider the problem of learning and using predictions for warm start algorithms with predictions. In this setting, an algorithm is given an instance of a problem, and a prediction of the solution. The runtime of the algorithm is bounded by the distance from the predicted solution to the true solution of the instance. Previous work has shown that when instances are drawn iid from some distribution, it is possible to learn an approximately optimal fixed prediction (Dinitz et al, NeurIPS 2021), and in the adversarial online case, it is possible to compete with the best fixed prediction in hindsight (Khodak et al, NeurIPS 2022).   In this work we give competitive guarantees against stronger benchmarks that consider a set of $k$ predictions $\\mathbf{P}$. That is, the \"optimal offline cost\" to solve an instance with respect to $\\mathbf{P}$ is the distance from the true solution to the closest member of $\\mathbf{P}$. This is analogous to the $k$-medians objective function. In the distributional setting, we show a simple strategy that incurs cost that is at most an $O(k)$ factor worse than the optimal offline cost. We then show a way to leverage learnable coarse information, in the form of partitions of the instance space into groups of \"similar\" instances, that allows us to potentially avoid this $O(k)$ factor.   Finally, we consider an online version of the problem, where we compete against offline strategies that are allowed to maintain a moving set of $k$ predictions or \"trajectories,\" and are charged for how much the predictions move. We give an algorithm that does at most $O(k^4 \\ln^2 k)$ times as much work as any offline strategy of $k$ trajectories. This algorithm is deterministic (robust to an adaptive adversary), and oblivious to the setting of $k$. Thus the guarantee holds for all $k$ simultaneously.","sentences":["We consider the problem of learning and using predictions for warm start algorithms with predictions.","In this setting, an algorithm is given an instance of a problem, and a prediction of the solution.","The runtime of the algorithm is bounded by the distance from the predicted solution to the true solution of the instance.","Previous work has shown that when instances are drawn iid from some distribution, it is possible to learn an approximately optimal fixed prediction (Dinitz et al, NeurIPS 2021), and in the adversarial online case, it is possible to compete with the best fixed prediction in hindsight (Khodak et al, NeurIPS 2022).   ","In this work we give competitive guarantees against stronger benchmarks that consider a set of $k$ predictions $\\mathbf{P}$. That is, the \"optimal offline cost\" to solve an instance with respect to $\\mathbf{P}$ is the distance from the true solution to the closest member of $\\mathbf{P}$. This is analogous to the $k$-medians objective function.","In the distributional setting, we show a simple strategy that incurs cost that is at most an $O(k)$ factor worse than the optimal offline cost.","We then show a way to leverage learnable coarse information, in the form of partitions of the instance space into groups of \"similar\" instances, that allows us to potentially avoid this $O(k)$ factor.   ","Finally, we consider an online version of the problem, where we compete against offline strategies that are allowed to maintain a moving set of $k$ predictions or \"trajectories,\" and are charged for how much the predictions move.","We give an algorithm that does at most $O(k^4 \\ln^2 k)$ times as much work as any offline strategy of $k$ trajectories.","This algorithm is deterministic (robust to an adaptive adversary), and oblivious to the setting of $k$.","Thus the guarantee holds for all $k$ simultaneously."],"url":"http://arxiv.org/abs/2405.03661v1","category":"cs.DS"}
{"created":"2024-05-06 17:14:34","title":"Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders","abstract":"Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance. Existing approaches perform k-NN search with CE by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization. DE-based retrieve-and-rerank approaches suffer from poor recall on new domains and the retrieval with DE is decoupled from the CE. While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale. In this paper, we address these shortcomings with our proposed sparse-matrix factorization based method that efficiently computes latent query and item embeddings to approximate CE scores and performs k-NN search with the approximate CE similarity. We compute item embeddings offline by factorizing a sparse matrix containing query-item CE scores for a set of train queries. Our method produces a high-quality approximation while requiring only a fraction of CE calls as compared to CUR-based methods, and allows for leveraging DE to initialize the embedding space while avoiding compute- and resource-intensive finetuning of DE via distillation. At test time, the item embeddings remain fixed and retrieval occurs over rounds, alternating between a) estimating the test query embedding by minimizing error in approximating CE scores of items retrieved thus far, and b) using the updated test query embedding for retrieving more items. Our k-NN search method improves recall by up to 5% (k=1) and 54% (k=100) over DE-based approaches. Additionally, our indexing approach achieves a speedup of up to 100x over CUR-based and 5x over DE distillation methods, while matching or improving k-NN search recall over baselines.","sentences":["Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance.","Existing approaches perform k-NN search with CE by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization.","DE-based retrieve-and-rerank approaches suffer from poor recall on new domains and the retrieval with DE is decoupled from the CE.","While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale.","In this paper, we address these shortcomings with our proposed sparse-matrix factorization based method that efficiently computes latent query and item embeddings to approximate CE scores and performs k-NN search with the approximate CE similarity.","We compute item embeddings offline by factorizing a sparse matrix containing query-item CE scores for a set of train queries.","Our method produces a high-quality approximation while requiring only a fraction of CE calls as compared to CUR-based methods, and allows for leveraging DE to initialize the embedding space while avoiding compute- and resource-intensive finetuning of DE via distillation.","At test time, the item embeddings remain fixed and retrieval occurs over rounds, alternating between a) estimating the test query embedding by minimizing error in approximating CE scores of items retrieved thus far, and b) using the updated test query embedding for retrieving more items.","Our k-NN search method improves recall by up to 5% (k=1) and 54% (k=100) over DE-based approaches.","Additionally, our indexing approach achieves a speedup of up to 100x over CUR-based and 5x over DE distillation methods, while matching or improving k-NN search recall over baselines."],"url":"http://arxiv.org/abs/2405.03651v1","category":"cs.IR"}
{"created":"2024-05-06 16:17:33","title":"Trackable Island-model Genetic Algorithms at Wafer Scale","abstract":"Emerging ML/AI hardware accelerators, like the 850,000 processor Cerebras Wafer-Scale Engine (WSE), hold great promise to scale up the capabilities of evolutionary computation. However, challenges remain in maintaining visibility into underlying evolutionary processes while efficiently utilizing these platforms' large processor counts. Here, we focus on the problem of extracting phylogenetic information from digital evolution on the WSE platform. We present a tracking-enabled asynchronous island-based genetic algorithm (GA) framework for WSE hardware. Emulated and on-hardware GA benchmarks with a simple tracking-enabled agent model clock upwards of 1 million generations a minute for population sizes reaching 16 million. This pace enables quadrillions of evaluations a day. We validate phylogenetic reconstructions from these trials and demonstrate their suitability for inference of underlying evolutionary conditions. In particular, we demonstrate extraction of clear phylometric signals that differentiate wafer-scale runs with adaptive dynamics enabled versus disabled. Together, these benchmark and validation trials reflect strong potential for highly scalable evolutionary computation that is both efficient and observable. Kernel code implementing the island-model GA supports drop-in customization to support any fixed-length genome content and fitness criteria, allowing it to be leveraged to advance research interests across the community.","sentences":["Emerging ML/AI hardware accelerators, like the 850,000 processor Cerebras Wafer-Scale Engine (WSE), hold great promise to scale up the capabilities of evolutionary computation.","However, challenges remain in maintaining visibility into underlying evolutionary processes while efficiently utilizing these platforms' large processor counts.","Here, we focus on the problem of extracting phylogenetic information from digital evolution on the WSE platform.","We present a tracking-enabled asynchronous island-based genetic algorithm (GA) framework for WSE hardware.","Emulated and on-hardware GA benchmarks with a simple tracking-enabled agent model clock upwards of 1 million generations a minute for population sizes reaching 16 million.","This pace enables quadrillions of evaluations a day.","We validate phylogenetic reconstructions from these trials and demonstrate their suitability for inference of underlying evolutionary conditions.","In particular, we demonstrate extraction of clear phylometric signals that differentiate wafer-scale runs with adaptive dynamics enabled versus disabled.","Together, these benchmark and validation trials reflect strong potential for highly scalable evolutionary computation that is both efficient and observable.","Kernel code implementing the island-model GA supports drop-in customization to support any fixed-length genome content and fitness criteria, allowing it to be leveraged to advance research interests across the community."],"url":"http://arxiv.org/abs/2405.03605v1","category":"cs.NE"}
{"created":"2024-05-06 16:13:52","title":"Flexible terahertz metasurface absorbers empowered by bound states in the continuum","abstract":"Terahertz absorbers are crucial to the cutting-edge techniques in the next-generation wireless communications, imaging, sensing, and radar stealth, as they fundamentally determine the performance of detectors and cloaking capabilities. It has long been a pressing task to find absorbers with customizable performance that can adapt to various environments with low cost and great flexibility. Here, we demonstrate perfect absorption empowered by bound states in the continuum (BICs) allowing for the tailoring of absorption coefficient, bandwidth, and field of view. The one-port absorbers are interpreted using temporal coupled-mode theory highlighting the dominant role of BICs in the far-field radiation properties. Through a thorough investigation of BICs from the perspective of lattice symmetry, we unravel the radiation features of three BIC modes using both multipolar and topological analysis. The versatile radiation capabilities of BICs provide ample freedom to meet specific requirements of absorbers, including tunable bandwidth, stable performance in a large field of view, and multi-band absorption using a thin and flexible film without extreme geometric demands. Our findings offer a systematic approach to developing optoelectronic devices and demonstrate the significant potential of BICs for optical and photonic applications which will stimulate further studies on terahertz photonics and metasurfaces.","sentences":["Terahertz absorbers are crucial to the cutting-edge techniques in the next-generation wireless communications, imaging, sensing, and radar stealth, as they fundamentally determine the performance of detectors and cloaking capabilities.","It has long been a pressing task to find absorbers with customizable performance that can adapt to various environments with low cost and great flexibility.","Here, we demonstrate perfect absorption empowered by bound states in the continuum (BICs) allowing for the tailoring of absorption coefficient, bandwidth, and field of view.","The one-port absorbers are interpreted using temporal coupled-mode theory highlighting the dominant role of BICs in the far-field radiation properties.","Through a thorough investigation of BICs from the perspective of lattice symmetry, we unravel the radiation features of three BIC modes using both multipolar and topological analysis.","The versatile radiation capabilities of BICs provide ample freedom to meet specific requirements of absorbers, including tunable bandwidth, stable performance in a large field of view, and multi-band absorption using a thin and flexible film without extreme geometric demands.","Our findings offer a systematic approach to developing optoelectronic devices and demonstrate the significant potential of BICs for optical and photonic applications which will stimulate further studies on terahertz photonics and metasurfaces."],"url":"http://arxiv.org/abs/2405.03600v1","category":"physics.optics"}
{"created":"2024-05-06 15:59:46","title":"Effective Quadratic Error Bounds for Floating-Point Algorithms Computing the Hypotenuse Function","abstract":"We provide tools to help automate the error analysis of algorithms that evaluate simple functions over the floating-point numbers. The aim is to obtain tight relative error bounds for these algorithms, expressed as a function of the unit round-off. Due to the discrete nature of the set of floating-point numbers, the largest errors are often intrinsically \"arithmetic\" in the sense that their appearance may depend on specific bit patterns in the binary representations of intermediate variables, which may be present only for some precisions. We focus on generic (i.e., parameterized by the precision) and analytic over-estimations that still capture the correlations between the errors made at each step of the algorithms. Using methods from computer algebra, which we adapt to the particular structure of the polynomial systems that encode the errors, we obtain bounds with a linear term in the unit round-off that is sharp in manycases. An explicit quadratic bound is given, rather than the $O()$-estimate that is more common in this area. This is particularly important when using low precision formats, which are increasingly common in modern processors. Using this approach, we compare five algorithms for computing the hypotenuse function, ranging from elementary to quite challenging.","sentences":["We provide tools to help automate the error analysis of algorithms that evaluate simple functions over the floating-point numbers.","The aim is to obtain tight relative error bounds for these algorithms, expressed as a function of the unit round-off.","Due to the discrete nature of the set of floating-point numbers, the largest errors are often intrinsically \"arithmetic\" in the sense that their appearance may depend on specific bit patterns in the binary representations of intermediate variables, which may be present only for some precisions.","We focus on generic (i.e., parameterized by the precision) and analytic over-estimations that still capture the correlations between the errors made at each step of the algorithms.","Using methods from computer algebra, which we adapt to the particular structure of the polynomial systems that encode the errors, we obtain bounds with a linear term in the unit round-off that is sharp in manycases.","An explicit quadratic bound is given, rather than the $O()$-estimate that is more common in this area.","This is particularly important when using low precision formats, which are increasingly common in modern processors.","Using this approach, we compare five algorithms for computing the hypotenuse function, ranging from elementary to quite challenging."],"url":"http://arxiv.org/abs/2405.03588v1","category":"math.NA"}
{"created":"2024-05-06 15:59:29","title":"Dissipative gradient nonlinearities prevent $\u03b4$-formations in local and nonlocal attraction-repulsion chemotaxis models","abstract":"We study some attraction repulsion chemotaxis models, characterized by nonlinearities laws for the diffusion of the cell density, and for the chemosensitivities and the production rates of the chemoattractant and the chemorepellent. Additionally, a source also involving some expression of the gradient of the species is incorporated.","sentences":["We study some attraction repulsion chemotaxis models, characterized by nonlinearities laws for the diffusion of the cell density, and for the chemosensitivities and the production rates of the chemoattractant and the chemorepellent.","Additionally, a source also involving some expression of the gradient of the species is incorporated."],"url":"http://arxiv.org/abs/2405.03586v1","category":"math.AP"}
{"created":"2024-05-06 15:55:18","title":"A GPU-Accelerated Interior Point Method for Radiation Therapy Optimization","abstract":"Optimization plays a central role in modern radiation therapy, where it is used to determine optimal treatment machine parameters in order to deliver precise doses adapted to each patient case. In general, solving the optimization problems that arise can present a computational bottleneck in the treatment planning process, as they can be large in terms of both variables and constraints. In this paper, we develop a GPU accelerated optimization solver for radiation therapy applications, based on an interior point method (IPM) utilizing iterative linear algebra to find search directions. The use of iterative linear algebra makes the solver suitable for porting to GPUs, as the core computational kernels become standard matrix-vector or vector-vector operations. Our solver is implemented in C++20 and uses CUDA for GPU acceleration.   The problems we solve are from the commercial treatment planning system RayStation, developed by RaySearch Laboratories (Stockholm, Sweden), which is used clinically in hundreds of cancer clinics around the world. RayStation solves (in general) nonlinear optimization problems using a sequential quadratic programming (SQP) method, where the main computation lies in solving quadratic programming (QP) sub-problems in each iteration. GPU acceleration for the solution of such QP sub-problems is the focus of the interior point method of this work. We benchmark our solver against the existing QP-solver in RayStation and show that our GPU accelerated IPM can accelerate the aggregated time-to-solution for all QP sub-problems in one SQP solve by 1.4 and 4.4 times, respectively, for two real patient cases.","sentences":["Optimization plays a central role in modern radiation therapy, where it is used to determine optimal treatment machine parameters in order to deliver precise doses adapted to each patient case.","In general, solving the optimization problems that arise can present a computational bottleneck in the treatment planning process, as they can be large in terms of both variables and constraints.","In this paper, we develop a GPU accelerated optimization solver for radiation therapy applications, based on an interior point method (IPM) utilizing iterative linear algebra to find search directions.","The use of iterative linear algebra makes the solver suitable for porting to GPUs, as the core computational kernels become standard matrix-vector or vector-vector operations.","Our solver is implemented in C++20 and uses CUDA for GPU acceleration.   ","The problems we solve are from the commercial treatment planning system RayStation, developed by RaySearch Laboratories (Stockholm, Sweden), which is used clinically in hundreds of cancer clinics around the world.","RayStation solves (in general) nonlinear optimization problems using a sequential quadratic programming (SQP) method, where the main computation lies in solving quadratic programming (QP) sub-problems in each iteration.","GPU acceleration for the solution of such QP sub-problems is the focus of the interior point method of this work.","We benchmark our solver against the existing QP-solver in RayStation and show that our GPU accelerated IPM can accelerate the aggregated time-to-solution for all QP sub-problems in one SQP solve by 1.4 and 4.4 times, respectively, for two real patient cases."],"url":"http://arxiv.org/abs/2405.03584v1","category":"math.OC"}
{"created":"2024-05-06 14:53:32","title":"Asymptotic-preserving hybridizable discontinuous Galerkin method for the Westervelt quasilinear wave equation","abstract":"We discuss the asymptotic-preserving properties of a hybridizable discontinuous Galerkin method for the Westervelt model of ultrasound waves. More precisely, we show that the proposed method is robust with respect to small values of the sound diffusivity damping parameter~$\\delta$ by deriving low- and high-order energy stability estimates, and \\emph{a priori} error bounds that are independent of~$\\delta$. Such bounds are then used to show that, when~$\\delta \\rightarrow 0^+$, the method remains stable and the discrete acoustic velocity potential~$\\psi_h^{(\\delta)}$ converges to~$\\psi_h^{(0)}$, where the latter is the singular vanishing dissipation limit. Moreover, we prove optimal convergence for the approximation of the acoustic particle velocity variable~$\\bv = \\nabla \\psi$. The established theoretical results are illustrated with some numerical experiments.","sentences":["We discuss the asymptotic-preserving properties of a hybridizable discontinuous Galerkin method for the Westervelt model of ultrasound waves.","More precisely, we show that the proposed method is robust with respect to small values of the sound diffusivity damping parameter~$\\delta$ by deriving low- and high-order energy stability estimates, and \\emph{a priori} error bounds that are independent of~$\\delta$. Such bounds are then used to show that, when~$\\delta \\rightarrow 0^+$, the method remains stable and the discrete acoustic velocity potential~$\\psi_h^{(\\delta)}$ converges to~$\\psi_h^{(0)}$, where the latter is the singular vanishing dissipation limit.","Moreover, we prove optimal convergence for the approximation of the acoustic particle velocity variable~$\\bv","= \\nabla \\psi$.","The established theoretical results are illustrated with some numerical experiments."],"url":"http://arxiv.org/abs/2405.03535v1","category":"math.NA"}
{"created":"2024-05-06 14:41:46","title":"On anomalous dissipation induced by transport noise","abstract":"In this paper, we show that suitable transport noises produce anomalous dissipation of energy of solutions to the 2D Navier-Stokes equations and diffusion equations in all dimensions. The key ingredients are Meyers' type estimates for SPDEs with transport noise which are combined with recent scaling limits for such SPDEs. The former allow us to obtain, for the first time for such type of scaling limits, convergence in a space of positive smoothness uniformly in time. Compared to known results, one of the main novelties is that anomalous dissipation might take place even in presence of a transport noise of arbitrarily small intensity. Further, we discuss physical interpretations.","sentences":["In this paper, we show that suitable transport noises produce anomalous dissipation of energy of solutions to the 2D Navier-Stokes equations and diffusion equations in all dimensions.","The key ingredients are Meyers' type estimates for SPDEs with transport noise which are combined with recent scaling limits for such SPDEs.","The former allow us to obtain, for the first time for such type of scaling limits, convergence in a space of positive smoothness uniformly in time.","Compared to known results, one of the main novelties is that anomalous dissipation might take place even in presence of a transport noise of arbitrarily small intensity.","Further, we discuss physical interpretations."],"url":"http://arxiv.org/abs/2405.03525v1","category":"math.AP"}
{"created":"2024-05-06 14:20:49","title":"Emergence of condensation patterns in kinetic equations for opinion dynamics","abstract":"In this work, we define a class of models to understand the impact of population size on opinion formation dynamics, a phenomenon usually related to group conformity. To this end, we introduce a new kinetic model in which the interaction frequency is weighted by the kinetic density. In the quasi-invariant regime, this model reduces to a Kaniadakis-Quarati-type equation with nonlinear drift, originally introduced for the dynamics of bosons in a spatially homogeneous setting. From the obtained PDE for the evolution of the opinion density, we determine the regime of parameters for which a critical mass exists and triggers blow-up of the solution. Therefore, the model is capable of describing strong conformity phenomena in cases where the total density of individuals holding a given opinion exceeds a fixed critical size. In the final part, several numerical experiments demonstrate the features of the introduced class of models and the related consensus effects.","sentences":["In this work, we define a class of models to understand the impact of population size on opinion formation dynamics, a phenomenon usually related to group conformity.","To this end, we introduce a new kinetic model in which the interaction frequency is weighted by the kinetic density.","In the quasi-invariant regime, this model reduces to a Kaniadakis-Quarati-type equation with nonlinear drift, originally introduced for the dynamics of bosons in a spatially homogeneous setting.","From the obtained PDE for the evolution of the opinion density, we determine the regime of parameters for which a critical mass exists and triggers blow-up of the solution.","Therefore, the model is capable of describing strong conformity phenomena in cases where the total density of individuals holding a given opinion exceeds a fixed critical size.","In the final part, several numerical experiments demonstrate the features of the introduced class of models and the related consensus effects."],"url":"http://arxiv.org/abs/2405.03507v1","category":"nlin.AO"}
{"created":"2024-05-06 13:55:39","title":"Whispy: Adapting STT Whisper Models to Real-Time Environments","abstract":"Large general-purpose transformer models have recently become the mainstay in the realm of speech analysis. In particular, Whisper achieves state-of-the-art results in relevant tasks such as speech recognition, translation, language identification, and voice activity detection. However, Whisper models are not designed to be used in real-time conditions, and this limitation makes them unsuitable for a vast plethora of practical applications. In this paper, we introduce Whispy, a system intended to bring live capabilities to the Whisper pretrained models. As a result of a number of architectural optimisations, Whispy is able to consume live audio streams and generate high level, coherent voice transcriptions, while still maintaining a low computational cost. We evaluate the performance of our system on a large repository of publicly available speech datasets, investigating how the transcription mechanism introduced by Whispy impacts on the Whisper output. Experimental results show how Whispy excels in robustness, promptness, and accuracy.","sentences":["Large general-purpose transformer models have recently become the mainstay in the realm of speech analysis.","In particular, Whisper achieves state-of-the-art results in relevant tasks such as speech recognition, translation, language identification, and voice activity detection.","However, Whisper models are not designed to be used in real-time conditions, and this limitation makes them unsuitable for a vast plethora of practical applications.","In this paper, we introduce Whispy, a system intended to bring live capabilities to the Whisper pretrained models.","As a result of a number of architectural optimisations, Whispy is able to consume live audio streams and generate high level, coherent voice transcriptions, while still maintaining a low computational cost.","We evaluate the performance of our system on a large repository of publicly available speech datasets, investigating how the transcription mechanism introduced by Whispy impacts on the Whisper output.","Experimental results show how Whispy excels in robustness, promptness, and accuracy."],"url":"http://arxiv.org/abs/2405.03484v1","category":"cs.SD"}
{"created":"2024-05-06 13:45:44","title":"Motion Planning under Uncertainty: Integrating Learning-Based Multi-Modal Predictors into Branch Model Predictive Control","abstract":"In complex traffic environments, autonomous vehicles face multi-modal uncertainty about other agents' future behavior. To address this, recent advancements in learningbased motion predictors output multi-modal predictions. We present our novel framework that leverages Branch Model Predictive Control(BMPC) to account for these predictions. The framework includes an online scenario-selection process guided by topology and collision risk criteria. This efficiently selects a minimal set of predictions, rendering the BMPC realtime capable. Additionally, we introduce an adaptive decision postponing strategy that delays the planner's commitment to a single scenario until the uncertainty is resolved. Our comprehensive evaluations in traffic intersection and random highway merging scenarios demonstrate enhanced comfort and safety through our method.","sentences":["In complex traffic environments, autonomous vehicles face multi-modal uncertainty about other agents' future behavior.","To address this, recent advancements in learningbased motion predictors output multi-modal predictions.","We present our novel framework that leverages Branch Model Predictive Control(BMPC) to account for these predictions.","The framework includes an online scenario-selection process guided by topology and collision risk criteria.","This efficiently selects a minimal set of predictions, rendering the BMPC realtime capable.","Additionally, we introduce an adaptive decision postponing strategy that delays the planner's commitment to a single scenario until the uncertainty is resolved.","Our comprehensive evaluations in traffic intersection and random highway merging scenarios demonstrate enhanced comfort and safety through our method."],"url":"http://arxiv.org/abs/2405.03470v1","category":"cs.RO"}
{"created":"2024-05-06 12:58:21","title":"Annealed adaptive importance sampling method in PINNs for solving high dimensional partial differential equations","abstract":"Physics-informed neural networks (PINNs) have emerged as powerful tools for solving a wide range of partial differential equations (PDEs). However, despite their user-friendly interface and broad applicability, PINNs encounter challenges in accurately resolving PDEs, especially when dealing with singular cases that may lead to unsatisfactory local minima. To address these challenges and improve solution accuracy, we propose an innovative approach called Annealed Adaptive Importance Sampling (AAIS) for computing the discretized PDE residuals of the cost functions, inspired by the Expectation Maximization algorithm used in finite mixtures to mimic target density. Our objective is to approximate discretized PDE residuals by strategically sampling additional points in regions with elevated residuals, thus enhancing the effectiveness and accuracy of PINNs. Implemented together with a straightforward resampling strategy within PINNs, our AAIS algorithm demonstrates significant improvements in efficiency across a range of tested PDEs, even with limited training datasets. Moreover, our proposed AAIS-PINN method shows promising capabilities in solving high-dimensional singular PDEs. The adaptive sampling framework introduced here can be integrated into various PINN frameworks.","sentences":["Physics-informed neural networks (PINNs) have emerged as powerful tools for solving a wide range of partial differential equations (PDEs).","However, despite their user-friendly interface and broad applicability, PINNs encounter challenges in accurately resolving PDEs, especially when dealing with singular cases that may lead to unsatisfactory local minima.","To address these challenges and improve solution accuracy, we propose an innovative approach called Annealed Adaptive Importance Sampling (AAIS) for computing the discretized PDE residuals of the cost functions, inspired by the Expectation Maximization algorithm used in finite mixtures to mimic target density.","Our objective is to approximate discretized PDE residuals by strategically sampling additional points in regions with elevated residuals, thus enhancing the effectiveness and accuracy of PINNs.","Implemented together with a straightforward resampling strategy within PINNs, our AAIS algorithm demonstrates significant improvements in efficiency across a range of tested PDEs, even with limited training datasets.","Moreover, our proposed AAIS-PINN method shows promising capabilities in solving high-dimensional singular PDEs.","The adaptive sampling framework introduced here can be integrated into various PINN frameworks."],"url":"http://arxiv.org/abs/2405.03433v1","category":"math.NA"}
{"created":"2024-05-06 12:47:16","title":"Geometry-aware framework for deep energy method: an application to structural mechanics with hyperelastic materials","abstract":"Physics-Informed Neural Networks (PINNs) have gained considerable interest in diverse engineering domains thanks to their capacity to integrate physical laws into deep learning models. Recently, geometry-aware PINN-based approaches that employ the strong form of underlying physical system equations have been developed with the aim of integrating geometric information into PINNs. Despite ongoing research, the assessment of PINNs in problems with various geometries remains an active area of investigation. In this work, we introduce a novel physics-informed framework named the Geometry-Aware Deep Energy Method (GADEM) for solving structural mechanics problems on different geometries. As the weak form of the physical system equation (or the energy-based approach) has demonstrated clear advantages compared to the strong form for solving solid mechanics problems, GADEM employs the weak form and aims to infer the solution on multiple shapes of geometries. Integrating a geometry-aware framework into an energy-based method results in an effective physics-informed deep learning model in terms of accuracy and computational cost. Different ways to represent the geometric information and to encode the geometric latent vectors are investigated in this work. We introduce a loss function of GADEM which is minimized based on the potential energy of all considered geometries. An adaptive learning method is also employed for the sampling of collocation points to enhance the performance of GADEM. We present some applications of GADEM to solve solid mechanics problems, including a loading simulation of a toy tire involving contact mechanics and large deformation hyperelasticity. The numerical results of this work demonstrate the remarkable capability of GADEM to infer the solution on various and new shapes of geometries using only one trained model.","sentences":["Physics-Informed Neural Networks (PINNs) have gained considerable interest in diverse engineering domains thanks to their capacity to integrate physical laws into deep learning models.","Recently, geometry-aware PINN-based approaches that employ the strong form of underlying physical system equations have been developed with the aim of integrating geometric information into PINNs.","Despite ongoing research, the assessment of PINNs in problems with various geometries remains an active area of investigation.","In this work, we introduce a novel physics-informed framework named the Geometry-Aware Deep Energy Method (GADEM) for solving structural mechanics problems on different geometries.","As the weak form of the physical system equation (or the energy-based approach) has demonstrated clear advantages compared to the strong form for solving solid mechanics problems, GADEM employs the weak form and aims to infer the solution on multiple shapes of geometries.","Integrating a geometry-aware framework into an energy-based method results in an effective physics-informed deep learning model in terms of accuracy and computational cost.","Different ways to represent the geometric information and to encode the geometric latent vectors are investigated in this work.","We introduce a loss function of GADEM which is minimized based on the potential energy of all considered geometries.","An adaptive learning method is also employed for the sampling of collocation points to enhance the performance of GADEM.","We present some applications of GADEM to solve solid mechanics problems, including a loading simulation of a toy tire involving contact mechanics and large deformation hyperelasticity.","The numerical results of this work demonstrate the remarkable capability of GADEM to infer the solution on various and new shapes of geometries using only one trained model."],"url":"http://arxiv.org/abs/2405.03427v1","category":"cs.LG"}
{"created":"2024-05-06 12:44:37","title":"Gaussian Stochastic Weight Averaging for Bayesian Low-Rank Adaptation of Large Language Models","abstract":"Fine-tuned Large Language Models (LLMs) often suffer from overconfidence and poor calibration, particularly when fine-tuned on small datasets. To address these challenges, we propose a simple combination of Low-Rank Adaptation (LoRA) with Gaussian Stochastic Weight Averaging (SWAG), facilitating approximate Bayesian inference in LLMs. Through extensive testing across several Natural Language Processing (NLP) benchmarks, we demonstrate that our straightforward and computationally efficient approach improves model generalization and calibration. We further show that our method exhibits greater robustness against distribution shift, as reflected in its performance on out-of-distribution tasks.","sentences":["Fine-tuned Large Language Models (LLMs) often suffer from overconfidence and poor calibration, particularly when fine-tuned on small datasets.","To address these challenges, we propose a simple combination of Low-Rank Adaptation (LoRA) with Gaussian Stochastic Weight Averaging (SWAG), facilitating approximate Bayesian inference in LLMs.","Through extensive testing across several Natural Language Processing (NLP) benchmarks, we demonstrate that our straightforward and computationally efficient approach improves model generalization and calibration.","We further show that our method exhibits greater robustness against distribution shift, as reflected in its performance on out-of-distribution tasks."],"url":"http://arxiv.org/abs/2405.03425v1","category":"cs.CL"}
{"created":"2024-05-06 12:40:15","title":"Implantable Adaptive Cells: differentiable architecture search to improve the performance of any trained U-shaped network","abstract":"This paper introduces a novel approach to enhance the performance of pre-trained neural networks in medical image segmentation using Neural Architecture Search (NAS) methods, specifically Differentiable Architecture Search (DARTS). We present the concept of Implantable Adaptive Cell (IAC), small but powerful modules identified through Partially-Connected DARTS, designed to be injected into the skip connections of an existing and already trained U-shaped model. Our strategy allows for the seamless integration of the IAC into the pre-existing architecture, thereby enhancing its performance without necessitating a complete retraining from scratch. The empirical studies, focusing on medical image segmentation tasks, demonstrate the efficacy of this method. The integration of specialized IAC cells into various configurations of the U-Net model increases segmentation accuracy by almost 2\\% points on average for the validation dataset and over 3\\% points for the training dataset. The findings of this study not only offer a cost-effective alternative to the complete overhaul of complex models for performance upgrades but also indicate the potential applicability of our method to other architectures and problem domains.","sentences":["This paper introduces a novel approach to enhance the performance of pre-trained neural networks in medical image segmentation using Neural Architecture Search (NAS) methods, specifically Differentiable Architecture Search (DARTS).","We present the concept of Implantable Adaptive Cell (IAC), small but powerful modules identified through Partially-Connected DARTS, designed to be injected into the skip connections of an existing and already trained U-shaped model.","Our strategy allows for the seamless integration of the IAC into the pre-existing architecture, thereby enhancing its performance without necessitating a complete retraining from scratch.","The empirical studies, focusing on medical image segmentation tasks, demonstrate the efficacy of this method.","The integration of specialized IAC cells into various configurations of the U-Net model increases segmentation accuracy by almost 2\\% points on average for the validation dataset and over 3\\% points for the training dataset.","The findings of this study not only offer a cost-effective alternative to the complete overhaul of complex models for performance upgrades but also indicate the potential applicability of our method to other architectures and problem domains."],"url":"http://arxiv.org/abs/2405.03420v1","category":"cs.CV"}
{"created":"2024-05-06 12:28:42","title":"Adaptive Accelerated Composite Minimization","abstract":"The choice of the stepsize in first-order convex optimization is typically based on the smoothness constant and plays a crucial role in the performance of algorithms. Recently, there has been a resurgent interest in introducing adaptive stepsizes that do not explicitly depend on smooth constant. In this paper, we propose a novel adaptive stepsize rule based on function evaluations (i.e., zero-order information) that enjoys provable convergence guarantees for both accelerated and non-accelerated gradient descent. We further discuss the similarities and differences between the proposed stepsize regimes and the existing stepsize rules (including Polyak and Armijo). Numerically, we benchmark the performance of our proposed algorithms with the state-of-the-art literature in three different classes of smooth minimization (logistic regression, quadratic programming, log-sum-exponential, and approximate semidefinite programming), composite minimization ($\\ell_1$ constrained and regularized problems), and non-convex minimization (cubic problem).","sentences":["The choice of the stepsize in first-order convex optimization is typically based on the smoothness constant and plays a crucial role in the performance of algorithms.","Recently, there has been a resurgent interest in introducing adaptive stepsizes that do not explicitly depend on smooth constant.","In this paper, we propose a novel adaptive stepsize rule based on function evaluations (i.e., zero-order information) that enjoys provable convergence guarantees for both accelerated and non-accelerated gradient descent.","We further discuss the similarities and differences between the proposed stepsize regimes and the existing stepsize rules (including Polyak and Armijo).","Numerically, we benchmark the performance of our proposed algorithms with the state-of-the-art literature in three different classes of smooth minimization (logistic regression, quadratic programming, log-sum-exponential, and approximate semidefinite programming), composite minimization ($\\ell_1$ constrained and regularized problems), and non-convex minimization (cubic problem)."],"url":"http://arxiv.org/abs/2405.03414v1","category":"math.OC"}
{"created":"2024-05-06 12:24:49","title":"SL-SLAM: A robust visual-inertial SLAM based deep feature extraction and matching","abstract":"This paper explores how deep learning techniques can improve visual-based SLAM performance in challenging environments. By combining deep feature extraction and deep matching methods, we introduce a versatile hybrid visual SLAM system designed to enhance adaptability in challenging scenarios, such as low-light conditions, dynamic lighting, weak-texture areas, and severe jitter. Our system supports multiple modes, including monocular, stereo, monocular-inertial, and stereo-inertial configurations. We also perform analysis how to combine visual SLAM with deep learning methods to enlighten other researches. Through extensive experiments on both public datasets and self-sampled data, we demonstrate the superiority of the SL-SLAM system over traditional approaches. The experimental results show that SL-SLAM outperforms state-of-the-art SLAM algorithms in terms of localization accuracy and tracking robustness. For the benefit of community, we make public the source code at https://github.com/zzzzxxxx111/SLslam.","sentences":["This paper explores how deep learning techniques can improve visual-based SLAM performance in challenging environments.","By combining deep feature extraction and deep matching methods, we introduce a versatile hybrid visual SLAM system designed to enhance adaptability in challenging scenarios, such as low-light conditions, dynamic lighting, weak-texture areas, and severe jitter.","Our system supports multiple modes, including monocular, stereo, monocular-inertial, and stereo-inertial configurations.","We also perform analysis how to combine visual SLAM with deep learning methods to enlighten other researches.","Through extensive experiments on both public datasets and self-sampled data, we demonstrate the superiority of the SL-SLAM system over traditional approaches.","The experimental results show that SL-SLAM outperforms state-of-the-art SLAM algorithms in terms of localization accuracy and tracking robustness.","For the benefit of community, we make public the source code at https://github.com/zzzzxxxx111/SLslam."],"url":"http://arxiv.org/abs/2405.03413v1","category":"cs.RO"}
{"created":"2024-05-06 11:27:27","title":"Knowledge-aware Text-Image Retrieval for Remote Sensing Images","abstract":"Image-based retrieval in large Earth observation archives is challenging because one needs to navigate across thousands of candidate matches only with the query image as a guide. By using text as information supporting the visual query, the retrieval system gains in usability, but at the same time faces difficulties due to the diversity of visual signals that cannot be summarized by a short caption only. For this reason, as a matching-based task, cross-modal text-image retrieval often suffers from information asymmetry between texts and images. To address this challenge, we propose a Knowledge-aware Text-Image Retrieval (KTIR) method for remote sensing images. By mining relevant information from an external knowledge graph, KTIR enriches the text scope available in the search query and alleviates the information gaps between texts and images for better matching. Moreover, by integrating domain-specific knowledge, KTIR also enhances the adaptation of pre-trained vision-language models to remote sensing applications. Experimental results on three commonly used remote sensing text-image retrieval benchmarks show that the proposed knowledge-aware method leads to varied and consistent retrievals, outperforming state-of-the-art retrieval methods.","sentences":["Image-based retrieval in large Earth observation archives is challenging because one needs to navigate across thousands of candidate matches only with the query image as a guide.","By using text as information supporting the visual query, the retrieval system gains in usability, but at the same time faces difficulties due to the diversity of visual signals that cannot be summarized by a short caption only.","For this reason, as a matching-based task, cross-modal text-image retrieval often suffers from information asymmetry between texts and images.","To address this challenge, we propose a Knowledge-aware Text-Image Retrieval (KTIR) method for remote sensing images.","By mining relevant information from an external knowledge graph, KTIR enriches the text scope available in the search query and alleviates the information gaps between texts and images for better matching.","Moreover, by integrating domain-specific knowledge, KTIR also enhances the adaptation of pre-trained vision-language models to remote sensing applications.","Experimental results on three commonly used remote sensing text-image retrieval benchmarks show that the proposed knowledge-aware method leads to varied and consistent retrievals, outperforming state-of-the-art retrieval methods."],"url":"http://arxiv.org/abs/2405.03373v1","category":"cs.CV"}
{"created":"2024-05-06 11:04:32","title":"VACO: a Multi-perspective Development of a Therapeutic and Motivational Virtual Robotic Agent for Concentration for children with ADHD","abstract":"In this work, we present (i) a novel approach how artificial intelligence can support in the therapy for better concentration of children with Attention Deficit Hyperactivity Disorder (ADHD) through motivational attention training with a virtual robotic agent and (ii) a development process in which different stakeholders are included with their perspectives. Therefore, we present three participative approaches to include the perspectives of different stakeholders. An online survey (Study I) was conducted with parents in Germany with the aim of ascertaining whether they would use software to promote their children's attention, what influences their attitude towards using it, and what requirements it would have to meet. About half of the parents would be willing to use software to promote attention. To develop the software as close to practice as possible, one of the developers took part in an intensive training for ADHD with the aim of testing which of the elements are technically feasible. Afterward, a first prototype was presented to clinicians (Study II) to make further adjustments. A first feasibility test (Study III) was conducted with the end users to check if the system works and if children and adolescents can use it. Attentional performance software offers multiple opportunities in the treatment of ADHD if the system is adapted to the needs of the practitioner and end user. This development process requires a lot of time and close interdisciplinary collaboration.","sentences":["In this work, we present (i) a novel approach how artificial intelligence can support in the therapy for better concentration of children with Attention Deficit Hyperactivity Disorder (ADHD) through motivational attention training with a virtual robotic agent and (ii) a development process in which different stakeholders are included with their perspectives.","Therefore, we present three participative approaches to include the perspectives of different stakeholders.","An online survey (Study I) was conducted with parents in Germany with the aim of ascertaining whether they would use software to promote their children's attention, what influences their attitude towards using it, and what requirements it would have to meet.","About half of the parents would be willing to use software to promote attention.","To develop the software as close to practice as possible, one of the developers took part in an intensive training for ADHD with the aim of testing which of the elements are technically feasible.","Afterward, a first prototype was presented to clinicians (Study II) to make further adjustments.","A first feasibility test (Study III) was conducted with the end users to check if the system works and if children and adolescents can use it.","Attentional performance software offers multiple opportunities in the treatment of ADHD if the system is adapted to the needs of the practitioner and end user.","This development process requires a lot of time and close interdisciplinary collaboration."],"url":"http://arxiv.org/abs/2405.03354v1","category":"cs.HC"}
{"created":"2024-05-06 11:02:26","title":"Salient Object Detection From Arbitrary Modalities","abstract":"Toward desirable saliency prediction, the types and numbers of inputs for a salient object detection (SOD) algorithm may dynamically change in many real-life applications. However, existing SOD algorithms are mainly designed or trained for one particular type of inputs, failing to be generalized to other types of inputs. Consequentially, more types of SOD algorithms need to be prepared in advance for handling different types of inputs, raising huge hardware and research costs. Differently, in this paper, we propose a new type of SOD task, termed Arbitrary Modality SOD (AM SOD). The most prominent characteristics of AM SOD are that the modality types and modality numbers will be arbitrary or dynamically changed. The former means that the inputs to the AM SOD algorithm may be arbitrary modalities such as RGB, depths, or even any combination of them. While, the latter indicates that the inputs may have arbitrary modality numbers as the input type is changed, e.g. single-modality RGB image, dual-modality RGB-Depth (RGB-D) images or triple-modality RGB-Depth-Thermal (RGB-D-T) images. Accordingly, a preliminary solution to the above challenges, \\i.e. a modality switch network (MSN), is proposed in this paper. In particular, a modality switch feature extractor (MSFE) is first designed to extract discriminative features from each modality effectively by introducing some modality indicators, which will generate some weights for modality switching. Subsequently, a dynamic fusion module (DFM) is proposed to adaptively fuse features from a variable number of modalities based on a novel Transformer structure. Finally, a new dataset, named AM-XD, is constructed to facilitate research on AM SOD. Extensive experiments demonstrate that our AM SOD method can effectively cope with changes in the type and number of input modalities for robust salient object detection.","sentences":["Toward desirable saliency prediction, the types and numbers of inputs for a salient object detection (SOD) algorithm may dynamically change in many real-life applications.","However, existing SOD algorithms are mainly designed or trained for one particular type of inputs, failing to be generalized to other types of inputs.","Consequentially, more types of SOD algorithms need to be prepared in advance for handling different types of inputs, raising huge hardware and research costs.","Differently, in this paper, we propose a new type of SOD task, termed Arbitrary Modality SOD (AM SOD).","The most prominent characteristics of AM SOD are that the modality types and modality numbers will be arbitrary or dynamically changed.","The former means that the inputs to the AM SOD algorithm may be arbitrary modalities such as RGB, depths, or even any combination of them.","While, the latter indicates that the inputs may have arbitrary modality numbers as the input type is changed, e.g. single-modality RGB image, dual-modality RGB-Depth (RGB-D) images or triple-modality RGB-Depth-Thermal (RGB-D-T) images.","Accordingly, a preliminary solution to the above challenges, \\i.e.","a modality switch network (MSN), is proposed in this paper.","In particular, a modality switch feature extractor (MSFE) is first designed to extract discriminative features from each modality effectively by introducing some modality indicators, which will generate some weights for modality switching.","Subsequently, a dynamic fusion module (DFM) is proposed to adaptively fuse features from a variable number of modalities based on a novel Transformer structure.","Finally, a new dataset, named AM-XD, is constructed to facilitate research on AM SOD.","Extensive experiments demonstrate that our AM SOD method can effectively cope with changes in the type and number of input modalities for robust salient object detection."],"url":"http://arxiv.org/abs/2405.03352v1","category":"cs.CV"}
{"created":"2024-05-06 11:02:02","title":"Modality Prompts for Arbitrary Modality Salient Object Detection","abstract":"This paper delves into the task of arbitrary modality salient object detection (AM SOD), aiming to detect salient objects from arbitrary modalities, eg RGB images, RGB-D images, and RGB-D-T images. A novel modality-adaptive Transformer (MAT) will be proposed to investigate two fundamental challenges of AM SOD, ie more diverse modality discrepancies caused by varying modality types that need to be processed, and dynamic fusion design caused by an uncertain number of modalities present in the inputs of multimodal fusion strategy. Specifically, inspired by prompt learning's ability of aligning the distributions of pre-trained models to the characteristic of downstream tasks by learning some prompts, MAT will first present a modality-adaptive feature extractor (MAFE) to tackle the diverse modality discrepancies by introducing a modality prompt for each modality. In the training stage, a new modality translation contractive (MTC) loss will be further designed to assist MAFE in learning those modality-distinguishable modality prompts. Accordingly, in the testing stage, MAFE can employ those learned modality prompts to adaptively adjust its feature space according to the characteristics of the input modalities, thus being able to extract discriminative unimodal features. Then, MAFE will present a channel-wise and spatial-wise fusion hybrid (CSFH) strategy to meet the demand for dynamic fusion. For that, CSFH dedicates a channel-wise dynamic fusion module (CDFM) and a novel spatial-wise dynamic fusion module (SDFM) to fuse the unimodal features from varying numbers of modalities and meanwhile effectively capture cross-modal complementary semantic and detail information, respectively. Moreover, CSFH will carefully align CDFM and SDFM to different levels of unimodal features based on their characteristics for more effective complementary information exploitation.","sentences":["This paper delves into the task of arbitrary modality salient object detection (AM SOD), aiming to detect salient objects from arbitrary modalities, eg RGB images, RGB-D images, and RGB-D-T images.","A novel modality-adaptive Transformer (MAT) will be proposed to investigate two fundamental challenges of AM SOD, ie more diverse modality discrepancies caused by varying modality types that need to be processed, and dynamic fusion design caused by an uncertain number of modalities present in the inputs of multimodal fusion strategy.","Specifically, inspired by prompt learning's ability of aligning the distributions of pre-trained models to the characteristic of downstream tasks by learning some prompts, MAT will first present a modality-adaptive feature extractor (MAFE) to tackle the diverse modality discrepancies by introducing a modality prompt for each modality.","In the training stage, a new modality translation contractive (MTC) loss will be further designed to assist MAFE in learning those modality-distinguishable modality prompts.","Accordingly, in the testing stage, MAFE can employ those learned modality prompts to adaptively adjust its feature space according to the characteristics of the input modalities, thus being able to extract discriminative unimodal features.","Then, MAFE will present a channel-wise and spatial-wise fusion hybrid (CSFH) strategy to meet the demand for dynamic fusion.","For that, CSFH dedicates a channel-wise dynamic fusion module (CDFM) and a novel spatial-wise dynamic fusion module (SDFM) to fuse the unimodal features from varying numbers of modalities and meanwhile effectively capture cross-modal complementary semantic and detail information, respectively.","Moreover, CSFH will carefully align CDFM and SDFM to different levels of unimodal features based on their characteristics for more effective complementary information exploitation."],"url":"http://arxiv.org/abs/2405.03351v1","category":"cs.CV"}
{"created":"2024-05-06 10:53:13","title":"Accelerated MR Cholangiopancreatography with Deep Learning-based Reconstruction","abstract":"This study accelerates MR cholangiopancreatography (MRCP) acquisitions using deep learning-based (DL) reconstruction at 3T and 0.55T. Thirty healthy volunteers underwent conventional two-fold MRCP scans at field strengths of 3T or 0.55T. We trained a variational network (VN) using retrospectively six-fold undersampled data obtained at 3T. We then evaluated our method against standard techniques such as parallel imaging (PI) and compressed sensing (CS), focusing on peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) as metrics. Furthermore, considering acquiring fully-sampled MRCP is impractical, we added a self-supervised DL reconstruction (SSDU) to the evaluating group. We also tested our method in a prospective accelerated scenario to reflect real-world clinical applications and evaluated its adaptability to MRCP at 0.55T. Our method demonstrated a remarkable reduction of average acquisition time from 599/542 to 255/180 seconds for MRCP at 3T/0.55T. In both retrospective and prospective undersampling scenarios, the PSNR and SSIM of VN were higher than those of PI, CS, and SSDU. At the same time, VN preserved the image quality of undersampled data, i.e., sharpness and the visibility of hepatobiliary ducts. In addition, VN also produced high quality reconstructions at 0.55T resulting in the highest PSNR and SSIM. In summary, VN trained for highly accelerated MRCP allows to reduce the acquisition time by a factor of 2.4/3.0 at 3T/0.55T while maintaining the image quality of the conventional acquisition.","sentences":["This study accelerates MR cholangiopancreatography (MRCP) acquisitions using deep learning-based (DL) reconstruction at 3T and","0.55T.","Thirty healthy volunteers underwent conventional two-fold MRCP scans at field strengths of 3T or 0.55T.","We trained a variational network (VN) using retrospectively six-fold undersampled data obtained at 3T. We then evaluated our method against standard techniques such as parallel imaging (PI) and compressed sensing (CS), focusing on peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) as metrics.","Furthermore, considering acquiring fully-sampled MRCP is impractical, we added a self-supervised DL reconstruction (SSDU) to the evaluating group.","We also tested our method in a prospective accelerated scenario to reflect real-world clinical applications and evaluated its adaptability to MRCP at 0.55T.","Our method demonstrated a remarkable reduction of average acquisition time from 599/542 to 255/180 seconds for MRCP at 3T/0.55T. In both retrospective and prospective undersampling scenarios, the PSNR and SSIM of VN were higher than those of PI, CS, and SSDU.","At the same time, VN preserved the image quality of undersampled data, i.e., sharpness and the visibility of hepatobiliary ducts.","In addition, VN also produced high quality reconstructions at 0.55T resulting in the highest PSNR and SSIM.","In summary, VN trained for highly accelerated MRCP allows to reduce the acquisition time by a factor of 2.4/3.0 at 3T/0.55T while maintaining the image quality of the conventional acquisition."],"url":"http://arxiv.org/abs/2405.03732v1","category":"eess.IV"}
{"created":"2024-05-06 10:50:17","title":"An efficient hierarchical Bayesian method for the Kuopio tomography challenge 2023","abstract":"The aim of Electrical Impedance Tomography (EIT) is to determine the electrical conductivity distribution inside a domain by applying currents and measuring voltages on its boundary. Mathematically, the EIT reconstruction task can be formulated as a non-linear inverse problem. The Bayesian inverse problems framework has been applied expensively to solutions of the EIT inverse problem, in particular in the cases when the unknown conductivity is believed to be blocky. Recently, the Sparsity Promoting Iterative Alternating Sequential (PS-IAS) algorithm, originally proposed for the solution of linear inverse problems, has been adapted for the non linear case of EIT reconstruction in a computationally efficient manner. Here we introduce a hybrid version of the SP-IAS algorithms for the nonlinear EIT inverse problem, providing a detailed description of the implementation details, with a specific focus on parameters selection. The method is applied to the 2023 Kuopio Tomography Challenge dataset, with a comprehensive report of the running times for the different cases and parameter selections.","sentences":["The aim of Electrical Impedance Tomography (EIT) is to determine the electrical conductivity distribution inside a domain by applying currents and measuring voltages on its boundary.","Mathematically, the EIT reconstruction task can be formulated as a non-linear inverse problem.","The Bayesian inverse problems framework has been applied expensively to solutions of the EIT inverse problem, in particular in the cases when the unknown conductivity is believed to be blocky.","Recently, the Sparsity Promoting Iterative Alternating Sequential (PS-IAS) algorithm, originally proposed for the solution of linear inverse problems, has been adapted for the non linear case of EIT reconstruction in a computationally efficient manner.","Here we introduce a hybrid version of the SP-IAS algorithms for the nonlinear EIT inverse problem, providing a detailed description of the implementation details, with a specific focus on parameters selection.","The method is applied to the 2023 Kuopio Tomography Challenge dataset, with a comprehensive report of the running times for the different cases and parameter selections."],"url":"http://arxiv.org/abs/2405.03343v1","category":"math.NA"}
{"created":"2024-05-06 10:49:51","title":"Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning","abstract":"Causal effect estimation under networked interference is an important but challenging problem. Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process. To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks. Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness. Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss. Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model. Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators.","sentences":["Causal effect estimation under networked interference is an important but challenging problem.","Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process.","To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks.","Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness.","Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss.","Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model.","Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators."],"url":"http://arxiv.org/abs/2405.03342v1","category":"cs.LG"}
{"created":"2024-05-06 10:40:34","title":"Functional Equivalence with NARS","abstract":"This study explores the concept of functional equivalence within the framework of the Non-Axiomatic Reasoning System (NARS), specifically through OpenNARS for Applications (ONA). Functional equivalence allows organisms to categorize and respond to varied stimuli based on their utility rather than perceptual similarity, thus enhancing cognitive efficiency and adaptability. In this study, ONA was modified to allow the derivation of functional equivalence. This paper provides practical examples of the capability of ONA to apply learned knowledge across different functional situations, demonstrating its utility in complex problem-solving and decision-making. An extended example is included, where training of ONA aimed to learn basic human-like language abilities, using a systematic procedure in relating spoken words, objects and written words. The research carried out as part of this study extends the understanding of functional equivalence in AGI systems, and argues for its necessity for level of flexibility in learning and adapting necessary for human-level AGI.","sentences":["This study explores the concept of functional equivalence within the framework of the Non-Axiomatic Reasoning System (NARS), specifically through OpenNARS for Applications (ONA).","Functional equivalence allows organisms to categorize and respond to varied stimuli based on their utility rather than perceptual similarity, thus enhancing cognitive efficiency and adaptability.","In this study, ONA was modified to allow the derivation of functional equivalence.","This paper provides practical examples of the capability of ONA to apply learned knowledge across different functional situations, demonstrating its utility in complex problem-solving and decision-making.","An extended example is included, where training of ONA aimed to learn basic human-like language abilities, using a systematic procedure in relating spoken words, objects and written words.","The research carried out as part of this study extends the understanding of functional equivalence in AGI systems, and argues for its necessity for level of flexibility in learning and adapting necessary for human-level AGI."],"url":"http://arxiv.org/abs/2405.03340v1","category":"cs.AI"}
{"created":"2024-05-06 10:05:46","title":"Clustering of Disease Trajectories with Explainable Machine Learning: A Case Study on Postoperative Delirium Phenotypes","abstract":"The identification of phenotypes within complex diseases or syndromes is a fundamental component of precision medicine, which aims to adapt healthcare to individual patient characteristics. Postoperative delirium (POD) is a complex neuropsychiatric condition with significant heterogeneity in its clinical manifestations and underlying pathophysiology. We hypothesize that POD comprises several distinct phenotypes, which cannot be directly observed in clinical practice. Identifying these phenotypes could enhance our understanding of POD pathogenesis and facilitate the development of targeted prevention and treatment strategies. In this paper, we propose an approach that combines supervised machine learning for personalized POD risk prediction with unsupervised clustering techniques to uncover potential POD phenotypes. We first demonstrate our approach using synthetic data, where we simulate patient cohorts with predefined phenotypes based on distinct sets of informative features. We aim to mimic any clinical disease with our synthetic data generation method. By training a predictive model and applying SHAP, we show that clustering patients in the SHAP feature importance space successfully recovers the true underlying phenotypes, outperforming clustering in the raw feature space. We then present a case study using real-world data from a cohort of elderly surgical patients. The results showcase the utility of our approach in uncovering clinically relevant subtypes of complex disorders like POD, paving the way for more precise and personalized treatment strategies.","sentences":["The identification of phenotypes within complex diseases or syndromes is a fundamental component of precision medicine, which aims to adapt healthcare to individual patient characteristics.","Postoperative delirium (POD) is a complex neuropsychiatric condition with significant heterogeneity in its clinical manifestations and underlying pathophysiology.","We hypothesize that POD comprises several distinct phenotypes, which cannot be directly observed in clinical practice.","Identifying these phenotypes could enhance our understanding of POD pathogenesis and facilitate the development of targeted prevention and treatment strategies.","In this paper, we propose an approach that combines supervised machine learning for personalized POD risk prediction with unsupervised clustering techniques to uncover potential POD phenotypes.","We first demonstrate our approach using synthetic data, where we simulate patient cohorts with predefined phenotypes based on distinct sets of informative features.","We aim to mimic any clinical disease with our synthetic data generation method.","By training a predictive model and applying SHAP, we show that clustering patients in the SHAP feature importance space successfully recovers the true underlying phenotypes, outperforming clustering in the raw feature space.","We then present a case study using real-world data from a cohort of elderly surgical patients.","The results showcase the utility of our approach in uncovering clinically relevant subtypes of complex disorders like POD, paving the way for more precise and personalized treatment strategies."],"url":"http://arxiv.org/abs/2405.03327v1","category":"cs.LG"}
{"created":"2024-05-06 09:50:04","title":"Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation","abstract":"The design of the query is crucial for the performance of DETR and its variants. Each query consists of two components: a content part and a positional one. Traditionally, the content query is initialized with a zero or learnable embedding, lacking essential content information and resulting in sub-optimal performance. In this paper, we introduce a novel plug-and-play module, Self-Adaptive Content Query (SACQ), to address this limitation. The SACQ module utilizes features from the transformer encoder to generate content queries via self-attention pooling. This allows candidate queries to adapt to the input image, resulting in a more comprehensive content prior and better focus on target objects. However, this improved concentration poses a challenge for the training process that utilizes the Hungarian matching, which selects only a single candidate and suppresses other similar ones. To overcome this, we propose a query aggregation strategy to cooperate with SACQ. It merges similar predicted candidates from different queries, easing the optimization. Our extensive experiments on the COCO dataset demonstrate the effectiveness of our proposed approaches across six different DETR's variants with multiple configurations, achieving an average improvement of over 1.0 AP.","sentences":["The design of the query is crucial for the performance of DETR and its variants.","Each query consists of two components: a content part and a positional one.","Traditionally, the content query is initialized with a zero or learnable embedding, lacking essential content information and resulting in sub-optimal performance.","In this paper, we introduce a novel plug-and-play module, Self-Adaptive Content Query (SACQ), to address this limitation.","The SACQ module utilizes features from the transformer encoder to generate content queries via self-attention pooling.","This allows candidate queries to adapt to the input image, resulting in a more comprehensive content prior and better focus on target objects.","However, this improved concentration poses a challenge for the training process that utilizes the Hungarian matching, which selects only a single candidate and suppresses other similar ones.","To overcome this, we propose a query aggregation strategy to cooperate with SACQ.","It merges similar predicted candidates from different queries, easing the optimization.","Our extensive experiments on the COCO dataset demonstrate the effectiveness of our proposed approaches across six different DETR's variants with multiple configurations, achieving an average improvement of over 1.0 AP."],"url":"http://arxiv.org/abs/2405.03318v1","category":"cs.CV"}
{"created":"2024-05-06 09:14:58","title":"Deep Learning and genetic algorithms for cosmological Bayesian inference speed-up","abstract":"In this paper, we present a novel approach to accelerate the Bayesian inference process, focusing specifically on the nested sampling algorithms. Bayesian inference plays a crucial role in cosmological parameter estimation, providing a robust framework for extracting theoretical insights from observational data. However, its computational demands can be substantial, primarily due to the need for numerous likelihood function evaluations. Our proposed method utilizes the power of deep learning, employing feedforward neural networks to approximate the likelihood function dynamically during the Bayesian inference process. Unlike traditional approaches, our method trains neural networks on-the-fly using the current set of live points as training data, without the need for pre-training. This flexibility enables adaptation to various theoretical models and datasets. We perform simple hyperparameter optimization using genetic algorithms to suggest initial neural network architectures for learning each likelihood function. Once sufficient accuracy is achieved, the neural network replaces the original likelihood function. The implementation integrates with nested sampling algorithms and has been thoroughly evaluated using both simple cosmological dark energy models and diverse observational datasets. Additionally, we explore the potential of genetic algorithms for generating initial live points within nested sampling inference, opening up new avenues for enhancing the efficiency and effectiveness of Bayesian inference methods.","sentences":["In this paper, we present a novel approach to accelerate the Bayesian inference process, focusing specifically on the nested sampling algorithms.","Bayesian inference plays a crucial role in cosmological parameter estimation, providing a robust framework for extracting theoretical insights from observational data.","However, its computational demands can be substantial, primarily due to the need for numerous likelihood function evaluations.","Our proposed method utilizes the power of deep learning, employing feedforward neural networks to approximate the likelihood function dynamically during the Bayesian inference process.","Unlike traditional approaches, our method trains neural networks on-the-fly using the current set of live points as training data, without the need for pre-training.","This flexibility enables adaptation to various theoretical models and datasets.","We perform simple hyperparameter optimization using genetic algorithms to suggest initial neural network architectures for learning each likelihood function.","Once sufficient accuracy is achieved, the neural network replaces the original likelihood function.","The implementation integrates with nested sampling algorithms and has been thoroughly evaluated using both simple cosmological dark energy models and diverse observational datasets.","Additionally, we explore the potential of genetic algorithms for generating initial live points within nested sampling inference, opening up new avenues for enhancing the efficiency and effectiveness of Bayesian inference methods."],"url":"http://arxiv.org/abs/2405.03293v1","category":"astro-ph.IM"}
{"created":"2024-05-06 09:02:54","title":"A continuous approach for computing the pseudospectra of linear operators","abstract":"We propose a continuous approach for computing the pseudospectra of linear operators following a 'solve-then-discretize' strategy. Instead of taking a finite section approach or using a finite-dimensional matrix to approximate the operator of interest, the new method employs an operator analogue of the Lanczos process to work directly with operators and functions. The method is shown to be free of spectral pollution and spectral invisibility, fully adaptive, nearly optimal in accuracy, and well-conditioned. The advantages of the method are demonstrated by extensive numerical examples and comparison with the traditional method.","sentences":["We propose a continuous approach for computing the pseudospectra of linear operators following a 'solve-then-discretize' strategy.","Instead of taking a finite section approach or using a finite-dimensional matrix to approximate the operator of interest, the new method employs an operator analogue of the Lanczos process to work directly with operators and functions.","The method is shown to be free of spectral pollution and spectral invisibility, fully adaptive, nearly optimal in accuracy, and well-conditioned.","The advantages of the method are demonstrated by extensive numerical examples and comparison with the traditional method."],"url":"http://arxiv.org/abs/2405.03285v1","category":"math.NA"}
{"created":"2024-05-06 08:49:37","title":"Distributed Adaptive Spatial Filtering with Inexact Local Solvers","abstract":"The Distributed Adaptive Signal Fusion (DASF) framework is a meta-algorithm for computing data-driven spatial filters in a distributed sensing platform with limited bandwidth and computational resources, such as a wireless sensor network. The convergence and optimality of the DASF algorithm has been extensively studied under the assumption that an exact, but possibly impractical solver for the local optimization problem at each updating node is available. In this work, we provide convergence and optimality results for the DASF framework when used with an inexact, finite-time solver such as (proximal) gradient descent or Newton's method. We provide sufficient conditions that the solver should satisfy in order to guarantee convergence of the resulting algorithm, and a lower bound for the convergence rate. We also provide numerical simulations to validate these theoretical results.","sentences":["The Distributed Adaptive Signal Fusion (DASF) framework is a meta-algorithm for computing data-driven spatial filters in a distributed sensing platform with limited bandwidth and computational resources, such as a wireless sensor network.","The convergence and optimality of the DASF algorithm has been extensively studied under the assumption that an exact, but possibly impractical solver for the local optimization problem at each updating node is available.","In this work, we provide convergence and optimality results for the DASF framework when used with an inexact, finite-time solver such as (proximal) gradient descent or Newton's method.","We provide sufficient conditions that the solver should satisfy in order to guarantee convergence of the resulting algorithm, and a lower bound for the convergence rate.","We also provide numerical simulations to validate these theoretical results."],"url":"http://arxiv.org/abs/2405.03277v1","category":"eess.SP"}
{"created":"2024-05-06 08:44:17","title":"Evaluation of Drivers' Interaction Ability at Social Scenarios: A Process-Based Framework","abstract":"Assessing drivers' interaction capabilities is crucial for understanding human driving behavior and enhancing the interactive abilities of autonomous vehicles. In scenarios involving strong interaction, existing metrics focused on interaction outcomes struggle to capture the evolutionary process of drivers' interactive behaviors, making it challenging for autonomous vehicles to dynamically assess and respond to other agents during interactions. To address this issue, we propose a framework for assessing drivers' interaction capabilities, oriented towards the interactive process itself, which includes three components: Interaction Risk Perception, Interaction Process Modeling, and Interaction Ability Scoring. We quantify interaction risks through motion state estimation and risk field theory, followed by introducing a dynamic action assessment benchmark based on a game-theoretical rational agent model, and designing a capability scoring metric based on morphological similarity distance. By calculating real-time differences between a driver's actions and the assessment benchmark, the driver's interaction capabilities are scored dynamically. We validated our framework at unsignalized intersections as a typical scenario. Validation analysis on driver behavior datasets from China and the USA shows that our framework effectively distinguishes and evaluates conservative and aggressive driving states during interactions, demonstrating good adaptability and effectiveness in various regional settings.","sentences":["Assessing drivers' interaction capabilities is crucial for understanding human driving behavior and enhancing the interactive abilities of autonomous vehicles.","In scenarios involving strong interaction, existing metrics focused on interaction outcomes struggle to capture the evolutionary process of drivers' interactive behaviors, making it challenging for autonomous vehicles to dynamically assess and respond to other agents during interactions.","To address this issue, we propose a framework for assessing drivers' interaction capabilities, oriented towards the interactive process itself, which includes three components: Interaction Risk Perception, Interaction Process Modeling, and Interaction Ability Scoring.","We quantify interaction risks through motion state estimation and risk field theory, followed by introducing a dynamic action assessment benchmark based on a game-theoretical rational agent model, and designing a capability scoring metric based on morphological similarity distance.","By calculating real-time differences between a driver's actions and the assessment benchmark, the driver's interaction capabilities are scored dynamically.","We validated our framework at unsignalized intersections as a typical scenario.","Validation analysis on driver behavior datasets from China and the USA shows that our framework effectively distinguishes and evaluates conservative and aggressive driving states during interactions, demonstrating good adaptability and effectiveness in various regional settings."],"url":"http://arxiv.org/abs/2405.03273v1","category":"cs.RO"}
{"created":"2024-05-06 08:35:09","title":"Generalized thermodynamic relations for perfect spin hydrodynamics","abstract":"Generalized thermodynamic relations are introduced into the framework of a relativistic perfect spin hydrodynamics. They allow for consistent treatment of spin degrees of freedom, including the use of spin tensors whose structure follows from microscopic calculations. The obtained results are important for establishing consistency between different formulations of spin hydrodynamics and form the basis for introducing dissipative corrections.","sentences":["Generalized thermodynamic relations are introduced into the framework of a relativistic perfect spin hydrodynamics.","They allow for consistent treatment of spin degrees of freedom, including the use of spin tensors whose structure follows from microscopic calculations.","The obtained results are important for establishing consistency between different formulations of spin hydrodynamics and form the basis for introducing dissipative corrections."],"url":"http://arxiv.org/abs/2405.03263v1","category":"hep-ph"}
{"created":"2024-05-06 08:00:43","title":"Communication-Efficient Federated Learning with Adaptive Compression under Dynamic Bandwidth","abstract":"Federated learning can train models without directly providing local data to the server. However, the frequent updating of the local model brings the problem of large communication overhead. Recently, scholars have achieved the communication efficiency of federated learning mainly by model compression. But they ignore two problems: 1) network state of each client changes dynamically; 2) network state among clients is not the same. The clients with poor bandwidth update local model slowly, which leads to low efficiency. To address this challenge, we propose a communication-efficient federated learning algorithm with adaptive compression under dynamic bandwidth (called AdapComFL). Concretely, each client performs bandwidth awareness and bandwidth prediction. Then, each client adaptively compresses its local model via the improved sketch mechanism based on his predicted bandwidth. Further, the server aggregates sketched models with different sizes received. To verify the effectiveness of the proposed method, the experiments are based on real bandwidth data which are collected from the network topology we build, and benchmark datasets which are obtained from open repositories. We show the performance of AdapComFL algorithm, and compare it with existing algorithms. The experimental results show that our AdapComFL achieves more efficient communication as well as competitive accuracy compared to existing algorithms.","sentences":["Federated learning can train models without directly providing local data to the server.","However, the frequent updating of the local model brings the problem of large communication overhead.","Recently, scholars have achieved the communication efficiency of federated learning mainly by model compression.","But they ignore two problems: 1) network state of each client changes dynamically; 2) network state among clients is not the same.","The clients with poor bandwidth update local model slowly, which leads to low efficiency.","To address this challenge, we propose a communication-efficient federated learning algorithm with adaptive compression under dynamic bandwidth (called AdapComFL).","Concretely, each client performs bandwidth awareness and bandwidth prediction.","Then, each client adaptively compresses its local model via the improved sketch mechanism based on his predicted bandwidth.","Further, the server aggregates sketched models with different sizes received.","To verify the effectiveness of the proposed method, the experiments are based on real bandwidth data which are collected from the network topology we build, and benchmark datasets which are obtained from open repositories.","We show the performance of AdapComFL algorithm, and compare it with existing algorithms.","The experimental results show that our AdapComFL achieves more efficient communication as well as competitive accuracy compared to existing algorithms."],"url":"http://arxiv.org/abs/2405.03248v1","category":"cs.LG"}
{"created":"2024-05-06 07:44:46","title":"Cross-Modal Domain Adaptation in Brain Disease Diagnosis: Maximum Mean Discrepancy-based Convolutional Neural Networks","abstract":"Brain disorders are a major challenge to global health, causing millions of deaths each year. Accurate diagnosis of these diseases relies heavily on advanced medical imaging techniques such as Magnetic Resonance Imaging (MRI) and Computed Tomography (CT). However, the scarcity of annotated data poses a significant challenge in deploying machine learning models for medical diagnosis. To address this limitation, deep learning techniques have shown considerable promise. Domain adaptation techniques enhance a model's ability to generalize across imaging modalities by transferring knowledge from one domain (e.g., CT images) to another (e.g., MRI images). Such cross-modality adaptation is essential to improve the ability of models to consistently generalize across different imaging modalities. This study collected relevant resources from the Kaggle website and employed the Maximum Mean Difference (MMD) method - a popular domain adaptation method - to reduce the differences between imaging domains. By combining MMD with Convolutional Neural Networks (CNNs), the accuracy and utility of the model is obviously enhanced. The excellent experimental results highlight the great potential of data-driven domain adaptation techniques to improve diagnostic accuracy and efficiency, especially in resource-limited environments. By bridging the gap between different imaging modalities, the study aims to provide clinicians with more reliable diagnostic tools.","sentences":["Brain disorders are a major challenge to global health, causing millions of deaths each year.","Accurate diagnosis of these diseases relies heavily on advanced medical imaging techniques such as Magnetic Resonance Imaging (MRI) and Computed Tomography (CT).","However, the scarcity of annotated data poses a significant challenge in deploying machine learning models for medical diagnosis.","To address this limitation, deep learning techniques have shown considerable promise.","Domain adaptation techniques enhance a model's ability to generalize across imaging modalities by transferring knowledge from one domain (e.g., CT images) to another (e.g., MRI images).","Such cross-modality adaptation is essential to improve the ability of models to consistently generalize across different imaging modalities.","This study collected relevant resources from the Kaggle website and employed the Maximum Mean Difference (MMD) method - a popular domain adaptation method - to reduce the differences between imaging domains.","By combining MMD with Convolutional Neural Networks (CNNs), the accuracy and utility of the model is obviously enhanced.","The excellent experimental results highlight the great potential of data-driven domain adaptation techniques to improve diagnostic accuracy and efficiency, especially in resource-limited environments.","By bridging the gap between different imaging modalities, the study aims to provide clinicians with more reliable diagnostic tools."],"url":"http://arxiv.org/abs/2405.03235v1","category":"cs.CV"}
{"created":"2024-05-06 07:41:31","title":"Slicing for Dense Smart Factory Network: Current State, Scenarios, Challenges and Expectations","abstract":"In the era of Industry 4.0, smart factories have emerged as a paradigm shift, redefining manufacturing with the integration of advanced digital technologies. Central to this transformation is the deployment of 5G networks, offering unprecedented levels of connectivity, speed, reliability, and ultra-low latency. Among the revolutionary features of 5G is network slicing, a technology that offers enhanced capabilities through the customization of network resources by allowing multiple logical networks (or slices) to run on top of a shared physical infrastructure. This capability is particularly crucial in the densely packed and highly dynamic environment of smart factories, where diverse applications - from robotic automation to real-time analytics - demand varying network requirements. In this paper, we present a comprehensive overview of the integration of slicing in smart factory networks, emphasizing its critical role in enhancing operational efficiency and supporting the diverse requirements of future manufacturing processes. We elaborate on the recent advances, and technical scenarios, including indoor factory propagation conditions, traffic characteristics, system requirements, slice-aware radio resource management, network elements, enabling technologies and current standardisation efforts. Additionally, we identify open research challenges as well as key technical issues stifling deployments. Finally, we speculate on the future trajectory of slicing-enabled smart factories, emphasizing the need for continuous adaptation to emerging technologies.","sentences":["In the era of Industry 4.0, smart factories have emerged as a paradigm shift, redefining manufacturing with the integration of advanced digital technologies.","Central to this transformation is the deployment of 5G networks, offering unprecedented levels of connectivity, speed, reliability, and ultra-low latency.","Among the revolutionary features of 5G is network slicing, a technology that offers enhanced capabilities through the customization of network resources by allowing multiple logical networks (or slices) to run on top of a shared physical infrastructure.","This capability is particularly crucial in the densely packed and highly dynamic environment of smart factories, where diverse applications - from robotic automation to real-time analytics - demand varying network requirements.","In this paper, we present a comprehensive overview of the integration of slicing in smart factory networks, emphasizing its critical role in enhancing operational efficiency and supporting the diverse requirements of future manufacturing processes.","We elaborate on the recent advances, and technical scenarios, including indoor factory propagation conditions, traffic characteristics, system requirements, slice-aware radio resource management, network elements, enabling technologies and current standardisation efforts.","Additionally, we identify open research challenges as well as key technical issues stifling deployments.","Finally, we speculate on the future trajectory of slicing-enabled smart factories, emphasizing the need for continuous adaptation to emerging technologies."],"url":"http://arxiv.org/abs/2405.03230v1","category":"eess.SP"}
{"created":"2024-05-06 07:34:40","title":"Dynamics of spatial phase coherence in a dissipative Bose-Hubbard atomic system","abstract":"We investigate the loss of spatial coherence of one-dimensional bosonic gases in optical lattices illuminated by a near-resonant excitation laser. Because the atoms recoil in a random direction after each spontaneous emission, the atomic momentum distribution progressively broadens. Equivalently, the spatial correlation function (the Fourier-conjugate quantity of the momentum distribution) progressively narrows down as more photons are scattered. Here we measure the correlation function of the matter field for fixed distances corresponding to nearest-neighbor (n-n) and next-nearest-neighbor (n-n-n) sites of the optical lattice as a function of time, hereafter called n-n and n-n-n correlators. For strongly interacting lattice gases, we find that the n-n correlator $C_1$ decays as a power-law at long times, $C_1\\propto 1/t^{\\alpha}$, in stark contrast with the exponential decay expected for independent particles. The power-law decay reflects a non-trivial dissipative many-body dynamics, where interactions change drastically the interplay between fluorescence destroying spatial coherence, and coherent tunnelling between neighboring sites restoring spatial coherence at short distances. The observed decay exponent $\\alpha \\approx 0.54(6) $ is in good agreement with the prediction $\\alpha=1/2$ from a dissipative Bose-Hubbard model accounting for the fluorescence-induced decoherence. Furthermore, we find that the n-n correlator $C_1$ controls the n-n-n correlator $C_2$ through the relation $C_2 \\approx C_1^2$, also in accordance with the dissipative Bose-Hubbard model.","sentences":["We investigate the loss of spatial coherence of one-dimensional bosonic gases in optical lattices illuminated by a near-resonant excitation laser.","Because the atoms recoil in a random direction after each spontaneous emission, the atomic momentum distribution progressively broadens.","Equivalently, the spatial correlation function (the Fourier-conjugate quantity of the momentum distribution) progressively narrows down as more photons are scattered.","Here we measure the correlation function of the matter field for fixed distances corresponding to nearest-neighbor (n-n) and next-nearest-neighbor (n-n-n) sites of the optical lattice as a function of time, hereafter called n-n and n-n-n correlators.","For strongly interacting lattice gases, we find that the n-n correlator $C_1$ decays as a power-law at long times, $C_1\\propto 1/t^{\\alpha}$, in stark contrast with the exponential decay expected for independent particles.","The power-law decay reflects a non-trivial dissipative many-body dynamics, where interactions change drastically the interplay between fluorescence destroying spatial coherence, and coherent tunnelling between neighboring sites restoring spatial coherence at short distances.","The observed decay exponent $\\alpha \\approx 0.54(6) $ is in good agreement with the prediction $\\alpha=1/2$ from a dissipative Bose-Hubbard model accounting for the fluorescence-induced decoherence.","Furthermore, we find that the n-n correlator $C_1$ controls the n-n-n correlator $C_2$ through the relation $C_2 \\approx C_1^2$, also in accordance with the dissipative Bose-Hubbard model."],"url":"http://arxiv.org/abs/2405.03226v2","category":"cond-mat.quant-gas"}
{"created":"2024-05-06 07:12:22","title":"Vietnamese AI Generated Text Detection","abstract":"In recent years, Large Language Models (LLMs) have become integrated into our daily lives, serving as invaluable assistants in completing tasks. Widely embraced by users, the abuse of LLMs is inevitable, particularly in using them to generate text content for various purposes, leading to difficulties in distinguishing between text generated by LLMs and that written by humans. In this study, we present a dataset named ViDetect, comprising 6.800 samples of Vietnamese essay, with 3.400 samples authored by humans and the remainder generated by LLMs, serving the purpose of detecting text generated by AI. We conducted evaluations using state-of-the-art methods, including ViT5, BartPho, PhoBERT, mDeberta V3, and mBERT. These results contribute not only to the growing body of research on detecting text generated by AI but also demonstrate the adaptability and effectiveness of different methods in the Vietnamese language context. This research lays the foundation for future advancements in AI-generated text detection and provides valuable insights for researchers in the field of natural language processing.","sentences":["In recent years, Large Language Models (LLMs) have become integrated into our daily lives, serving as invaluable assistants in completing tasks.","Widely embraced by users, the abuse of LLMs is inevitable, particularly in using them to generate text content for various purposes, leading to difficulties in distinguishing between text generated by LLMs and that written by humans.","In this study, we present a dataset named ViDetect, comprising 6.800 samples of Vietnamese essay, with 3.400 samples authored by humans and the remainder generated by LLMs, serving the purpose of detecting text generated by AI.","We conducted evaluations using state-of-the-art methods, including ViT5, BartPho, PhoBERT, mDeberta V3, and mBERT.","These results contribute not only to the growing body of research on detecting text generated by AI but also demonstrate the adaptability and effectiveness of different methods in the Vietnamese language context.","This research lays the foundation for future advancements in AI-generated text detection and provides valuable insights for researchers in the field of natural language processing."],"url":"http://arxiv.org/abs/2405.03206v1","category":"cs.CL"}
{"created":"2024-05-06 06:31:47","title":"QuadraNet V2: Efficient and Sustainable Training of High-Order Neural Networks with Quadratic Adaptation","abstract":"Machine learning is evolving towards high-order models that necessitate pre-training on extensive datasets, a process associated with significant overheads. Traditional models, despite having pre-trained weights, are becoming obsolete due to architectural differences that obstruct the effective transfer and initialization of these weights. To address these challenges, we introduce a novel framework, QuadraNet V2, which leverages quadratic neural networks to create efficient and sustainable high-order learning models. Our method initializes the primary term of the quadratic neuron using a standard neural network, while the quadratic term is employed to adaptively enhance the learning of data non-linearity or shifts. This integration of pre-trained primary terms with quadratic terms, which possess advanced modeling capabilities, significantly augments the information characterization capacity of the high-order network. By utilizing existing pre-trained weights, QuadraNet V2 reduces the required GPU hours for training by 90\\% to 98.4\\% compared to training from scratch, demonstrating both efficiency and effectiveness.","sentences":["Machine learning is evolving towards high-order models that necessitate pre-training on extensive datasets, a process associated with significant overheads.","Traditional models, despite having pre-trained weights, are becoming obsolete due to architectural differences that obstruct the effective transfer and initialization of these weights.","To address these challenges, we introduce a novel framework, QuadraNet V2, which leverages quadratic neural networks to create efficient and sustainable high-order learning models.","Our method initializes the primary term of the quadratic neuron using a standard neural network, while the quadratic term is employed to adaptively enhance the learning of data non-linearity or shifts.","This integration of pre-trained primary terms with quadratic terms, which possess advanced modeling capabilities, significantly augments the information characterization capacity of the high-order network.","By utilizing existing pre-trained weights, QuadraNet V2 reduces the required GPU hours for training by 90\\% to 98.4\\% compared to training from scratch, demonstrating both efficiency and effectiveness."],"url":"http://arxiv.org/abs/2405.03192v1","category":"cs.LG"}
{"created":"2024-05-06 06:30:17","title":"Adapting Dual-encoder Vision-language Models for Paraphrased Retrieval","abstract":"In the recent years, the dual-encoder vision-language models (\\eg CLIP) have achieved remarkable text-to-image retrieval performance. However, we discover that these models usually results in very different retrievals for a pair of paraphrased queries. Such behavior might render the retrieval system less predictable and lead to user frustration. In this work, we consider the task of paraphrased text-to-image retrieval where a model aims to return similar results given a pair of paraphrased queries. To start with, we collect a dataset of paraphrased image descriptions to facilitate quantitative evaluation for this task. We then hypothesize that the undesired behavior of existing dual-encoder model is due to their text towers which are trained on image-sentence pairs and lack the ability to capture the semantic similarity between paraphrased queries. To improve on this, we investigate multiple strategies for training a dual-encoder model starting from a language model pretrained on a large text corpus. Compared to public dual-encoder models such as CLIP and OpenCLIP, the model trained with our best adaptation strategy achieves a significantly higher ranking similarity for paraphrased queries while maintaining similar zero-shot classification and retrieval accuracy.","sentences":["In the recent years, the dual-encoder vision-language models (\\eg CLIP) have achieved remarkable text-to-image retrieval performance.","However, we discover that these models usually results in very different retrievals for a pair of paraphrased queries.","Such behavior might render the retrieval system less predictable and lead to user frustration.","In this work, we consider the task of paraphrased text-to-image retrieval where a model aims to return similar results given a pair of paraphrased queries.","To start with, we collect a dataset of paraphrased image descriptions to facilitate quantitative evaluation for this task.","We then hypothesize that the undesired behavior of existing dual-encoder model is due to their text towers which are trained on image-sentence pairs and lack the ability to capture the semantic similarity between paraphrased queries.","To improve on this, we investigate multiple strategies for training a dual-encoder model starting from a language model pretrained on a large text corpus.","Compared to public dual-encoder models such as CLIP and OpenCLIP, the model trained with our best adaptation strategy achieves a significantly higher ranking similarity for paraphrased queries while maintaining similar zero-shot classification and retrieval accuracy."],"url":"http://arxiv.org/abs/2405.03190v1","category":"cs.CV"}
{"created":"2024-05-06 06:12:17","title":"Collaborative Satellite Computing through Adaptive DNN Task Splitting and Offloading","abstract":"Satellite computing has emerged as a promising technology for next-generation wireless networks. This innovative technology provides data processing capabilities, which facilitates the widespread implementation of artificial intelligence (AI)-based applications, especially for image processing tasks involving deep neural network (DNN). With the limited computing resources of an individual satellite, independently handling DNN tasks generated by diverse user equipments (UEs) becomes a significant challenge. One viable solution is dividing a DNN task into multiple subtasks and subsequently distributing them across multiple satellites for collaborative computing. However, it is challenging to partition DNN appropriately and allocate subtasks into suitable satellites while ensuring load balancing. To this end, we propose a collaborative satellite computing system designed to improve task processing efficiency in satellite networks. Based on this system, a workload-balanced adaptive task splitting scheme is developed to equitably distribute the workload of DNN slices for collaborative inference, consequently enhancing the utilization of satellite computing resources. Additionally, a self-adaptive task offloading scheme based on a genetic algorithm (GA) is introduced to determine optimal offloading decisions within dynamic network environments. The numerical results illustrate that our proposal can outperform comparable methods in terms of task completion rate, delay, and resource utilization.","sentences":["Satellite computing has emerged as a promising technology for next-generation wireless networks.","This innovative technology provides data processing capabilities, which facilitates the widespread implementation of artificial intelligence (AI)-based applications, especially for image processing tasks involving deep neural network (DNN).","With the limited computing resources of an individual satellite, independently handling DNN tasks generated by diverse user equipments (UEs) becomes a significant challenge.","One viable solution is dividing a DNN task into multiple subtasks and subsequently distributing them across multiple satellites for collaborative computing.","However, it is challenging to partition DNN appropriately and allocate subtasks into suitable satellites while ensuring load balancing.","To this end, we propose a collaborative satellite computing system designed to improve task processing efficiency in satellite networks.","Based on this system, a workload-balanced adaptive task splitting scheme is developed to equitably distribute the workload of DNN slices for collaborative inference, consequently enhancing the utilization of satellite computing resources.","Additionally, a self-adaptive task offloading scheme based on a genetic algorithm (GA) is introduced to determine optimal offloading decisions within dynamic network environments.","The numerical results illustrate that our proposal can outperform comparable methods in terms of task completion rate, delay, and resource utilization."],"url":"http://arxiv.org/abs/2405.03181v1","category":"cs.DC"}
{"created":"2024-05-06 06:00:37","title":"Solutions of the equation $a_n + (a_{n-1} + \\cdots (a_2 + (a_1 + x^{r_1})^{r_2}\\cdots )^{r_{n}} = b\\, x$","abstract":"We establish a novel upper bound for the real solutions of the equation specified in the title, employing a generalized derivation-division algorithm. As a consequence, we also derive a new set of Chebyshev functions adapted specifically for this problem.","sentences":["We establish a novel upper bound for the real solutions of the equation specified in the title, employing a generalized derivation-division algorithm.","As a consequence, we also derive a new set of Chebyshev functions adapted specifically for this problem."],"url":"http://arxiv.org/abs/2405.03179v1","category":"math.DS"}
{"created":"2024-05-06 05:22:40","title":"TF4CTR: Twin Focus Framework for CTR Prediction via Adaptive Sample Differentiation","abstract":"Effective feature interaction modeling is critical for enhancing the accuracy of click-through rate (CTR) prediction in industrial recommender systems. Most of the current deep CTR models resort to building complex network architectures to better capture intricate feature interactions or user behaviors. However, we identify two limitations in these models: (1) the samples given to the model are undifferentiated, which may lead the model to learn a larger number of easy samples in a single-minded manner while ignoring a smaller number of hard samples, thus reducing the model's generalization ability; (2) differentiated feature interaction encoders are designed to capture different interactions information but receive consistent supervision signals, thereby limiting the effectiveness of the encoder. To bridge the identified gaps, this paper introduces a novel CTR prediction framework by integrating the plug-and-play Twin Focus (TF) Loss, Sample Selection Embedding Module (SSEM), and Dynamic Fusion Module (DFM), named the Twin Focus Framework for CTR (TF4CTR). Specifically, the framework employs the SSEM at the bottom of the model to differentiate between samples, thereby assigning a more suitable encoder for each sample. Meanwhile, the TF Loss provides tailored supervision signals to both simple and complex encoders. Moreover, the DFM dynamically fuses the feature interaction information captured by the encoders, resulting in more accurate predictions. Experiments on five real-world datasets confirm the effectiveness and compatibility of the framework, demonstrating its capacity to enhance various representative baselines in a model-agnostic manner. To facilitate reproducible research, our open-sourced code and detailed running logs will be made available at: https://github.com/salmon1802/TF4CTR.","sentences":["Effective feature interaction modeling is critical for enhancing the accuracy of click-through rate (CTR) prediction in industrial recommender systems.","Most of the current deep CTR models resort to building complex network architectures to better capture intricate feature interactions or user behaviors.","However, we identify two limitations in these models: (1) the samples given to the model are undifferentiated, which may lead the model to learn a larger number of easy samples in a single-minded manner while ignoring a smaller number of hard samples, thus reducing the model's generalization ability; (2) differentiated feature interaction encoders are designed to capture different interactions information but receive consistent supervision signals, thereby limiting the effectiveness of the encoder.","To bridge the identified gaps, this paper introduces a novel CTR prediction framework by integrating the plug-and-play Twin Focus (TF) Loss, Sample Selection Embedding Module (SSEM), and Dynamic Fusion Module (DFM), named the Twin Focus Framework for CTR (TF4CTR).","Specifically, the framework employs the SSEM at the bottom of the model to differentiate between samples, thereby assigning a more suitable encoder for each sample.","Meanwhile, the TF Loss provides tailored supervision signals to both simple and complex encoders.","Moreover, the DFM dynamically fuses the feature interaction information captured by the encoders, resulting in more accurate predictions.","Experiments on five real-world datasets confirm the effectiveness and compatibility of the framework, demonstrating its capacity to enhance various representative baselines in a model-agnostic manner.","To facilitate reproducible research, our open-sourced code and detailed running logs will be made available at: https://github.com/salmon1802/TF4CTR."],"url":"http://arxiv.org/abs/2405.03167v1","category":"cs.IR"}
{"created":"2024-05-06 04:36:02","title":"DeepMpMRI: Tensor-decomposition Regularized Learning for Fast and High-Fidelity Multi-Parametric Microstructural MR Imaging","abstract":"Deep learning has emerged as a promising approach for learning the nonlinear mapping between diffusion-weighted MR images and tissue parameters, which enables automatic and deep understanding of the brain microstructures. However, the efficiency and accuracy in the multi-parametric estimations are still limited since previous studies tend to estimate multi-parametric maps with dense sampling and isolated signal modeling. This paper proposes DeepMpMRI, a unified framework for fast and high-fidelity multi-parametric estimation from various diffusion models using sparsely sampled q-space data. DeepMpMRI is equipped with a newly designed tensor-decomposition-based regularizer to effectively capture fine details by exploiting the correlation across parameters. In addition, we introduce a Nesterov-based adaptive learning algorithm that optimizes the regularization parameter dynamically to enhance the performance. DeepMpMRI is an extendable framework capable of incorporating flexible network architecture. Experimental results demonstrate the superiority of our approach over 5 state-of-the-art methods in simultaneously estimating multi-parametric maps for various diffusion models with fine-grained details both quantitatively and qualitatively, achieving 4.5 - 22.5$\\times$ acceleration compared to the dense sampling of a total of 270 diffusion gradients.","sentences":["Deep learning has emerged as a promising approach for learning the nonlinear mapping between diffusion-weighted MR images and tissue parameters, which enables automatic and deep understanding of the brain microstructures.","However, the efficiency and accuracy in the multi-parametric estimations are still limited since previous studies tend to estimate multi-parametric maps with dense sampling and isolated signal modeling.","This paper proposes DeepMpMRI, a unified framework for fast and high-fidelity multi-parametric estimation from various diffusion models using sparsely sampled q-space data.","DeepMpMRI is equipped with a newly designed tensor-decomposition-based regularizer to effectively capture fine details by exploiting the correlation across parameters.","In addition, we introduce a Nesterov-based adaptive learning algorithm that optimizes the regularization parameter dynamically to enhance the performance.","DeepMpMRI is an extendable framework capable of incorporating flexible network architecture.","Experimental results demonstrate the superiority of our approach over 5 state-of-the-art methods in simultaneously estimating multi-parametric maps for various diffusion models with fine-grained details both quantitatively and qualitatively, achieving 4.5 - 22.5$\\times$ acceleration compared to the dense sampling of a total of 270 diffusion gradients."],"url":"http://arxiv.org/abs/2405.03159v1","category":"cs.CV"}
{"created":"2024-05-06 03:39:50","title":"PTQ4SAM: Post-Training Quantization for Segment Anything","abstract":"Segment Anything Model (SAM) has achieved impressive performance in many computer vision tasks. However, as a large-scale model, the immense memory and computation costs hinder its practical deployment. In this paper, we propose a post-training quantization (PTQ) framework for Segment Anything Model, namely PTQ4SAM. First, we investigate the inherent bottleneck of SAM quantization attributed to the bimodal distribution in post-Key-Linear activations. We analyze its characteristics from both per-tensor and per-channel perspectives, and propose a Bimodal Integration strategy, which utilizes a mathematically equivalent sign operation to transform the bimodal distribution into a relatively easy-quantized normal distribution offline. Second, SAM encompasses diverse attention mechanisms (i.e., self-attention and two-way cross-attention), resulting in substantial variations in the post-Softmax distributions. Therefore, we introduce an Adaptive Granularity Quantization for Softmax through searching the optimal power-of-two base, which is hardware-friendly. Extensive experimental results across various vision tasks (instance segmentation, semantic segmentation and object detection), datasets and model variants show the superiority of PTQ4SAM. For example, when quantizing SAM-L to 6-bit, we achieve lossless accuracy for instance segmentation, about 0.5\\% drop with theoretical 3.9$\\times$ acceleration. The code is available at \\url{https://github.com/chengtao-lv/PTQ4SAM}.","sentences":["Segment Anything Model (SAM) has achieved impressive performance in many computer vision tasks.","However, as a large-scale model, the immense memory and computation costs hinder its practical deployment.","In this paper, we propose a post-training quantization (PTQ) framework for Segment Anything Model, namely PTQ4SAM.","First, we investigate the inherent bottleneck of SAM quantization attributed to the bimodal distribution in post-Key-Linear activations.","We analyze its characteristics from both per-tensor and per-channel perspectives, and propose a Bimodal Integration strategy, which utilizes a mathematically equivalent sign operation to transform the bimodal distribution into a relatively easy-quantized normal distribution offline.","Second, SAM encompasses diverse attention mechanisms (i.e., self-attention and two-way cross-attention), resulting in substantial variations in the post-Softmax distributions.","Therefore, we introduce an Adaptive Granularity Quantization for Softmax through searching the optimal power-of-two base, which is hardware-friendly.","Extensive experimental results across various vision tasks (instance segmentation, semantic segmentation and object detection), datasets and model variants show the superiority of PTQ4SAM.","For example, when quantizing SAM-L to 6-bit, we achieve lossless accuracy for instance segmentation, about 0.5\\% drop with theoretical 3.9$\\times$ acceleration.","The code is available at \\url{https://github.com/chengtao-lv/PTQ4SAM}."],"url":"http://arxiv.org/abs/2405.03144v1","category":"cs.CV"}
{"created":"2024-05-06 03:30:02","title":"Generative adversarial learning with optimal input dimension and its adaptive generator architecture","abstract":"We investigate the impact of the input dimension on the generalization error in generative adversarial networks (GANs). In particular, we first provide both theoretical and practical evidence to validate the existence of an optimal input dimension (OID) that minimizes the generalization error. Then, to identify the OID, we introduce a novel framework called generalized GANs (G-GANs), which includes existing GANs as a special case. By incorporating the group penalty and the architecture penalty developed in the paper, G-GANs have several intriguing features. First, our framework offers adaptive dimensionality reduction from the initial dimension to a dimension necessary for generating the target distribution. Second, this reduction in dimensionality also shrinks the required size of the generator network architecture, which is automatically identified by the proposed architecture penalty. Both reductions in dimensionality and the generator network significantly improve the stability and the accuracy of the estimation and prediction. Theoretical support for the consistent selection of the input dimension and the generator network is provided. Third, the proposed algorithm involves an end-to-end training process, and the algorithm allows for dynamic adjustments between the input dimension and the generator network during training, further enhancing the overall performance of G-GANs. Extensive experiments conducted with simulated and benchmark data demonstrate the superior performance of G-GANs. In particular, compared to that of off-the-shelf methods, G-GANs achieves an average improvement of 45.68% in the CT slice dataset, 43.22% in the MNIST dataset and 46.94% in the FashionMNIST dataset in terms of the maximum mean discrepancy or Frechet inception distance. Moreover, the features generated based on the input dimensions identified by G-GANs align with visually significant features.","sentences":["We investigate the impact of the input dimension on the generalization error in generative adversarial networks (GANs).","In particular, we first provide both theoretical and practical evidence to validate the existence of an optimal input dimension (OID) that minimizes the generalization error.","Then, to identify the OID, we introduce a novel framework called generalized GANs (G-GANs), which includes existing GANs as a special case.","By incorporating the group penalty and the architecture penalty developed in the paper, G-GANs have several intriguing features.","First, our framework offers adaptive dimensionality reduction from the initial dimension to a dimension necessary for generating the target distribution.","Second, this reduction in dimensionality also shrinks the required size of the generator network architecture, which is automatically identified by the proposed architecture penalty.","Both reductions in dimensionality and the generator network significantly improve the stability and the accuracy of the estimation and prediction.","Theoretical support for the consistent selection of the input dimension and the generator network is provided.","Third, the proposed algorithm involves an end-to-end training process, and the algorithm allows for dynamic adjustments between the input dimension and the generator network during training, further enhancing the overall performance of G-GANs.","Extensive experiments conducted with simulated and benchmark data demonstrate the superior performance of G-GANs.","In particular, compared to that of off-the-shelf methods, G-GANs achieves an average improvement of 45.68% in the CT slice dataset, 43.22% in the MNIST dataset and 46.94% in the FashionMNIST dataset in terms of the maximum mean discrepancy or Frechet inception distance.","Moreover, the features generated based on the input dimensions identified by G-GANs align with visually significant features."],"url":"http://arxiv.org/abs/2405.03723v1","category":"cs.LG"}
{"created":"2024-05-06 03:06:04","title":"A Multi-Agent Rollout Approach for Highway Bottleneck Decongenston in Mixed Autonomy","abstract":"The integration of autonomous vehicles (AVs) into the existing transportation infrastructure offers a promising solution to alleviate congestion and enhance mobility. This research explores a novel approach to traffic optimization by employing a multi-agent rollout approach within a mixed autonomy environment. The study concentrates on coordinating the speed of human-driven vehicles by longitudinally controlling AVs, aiming to dynamically optimize traffic flow and alleviate congestion at highway bottlenecks in real-time. We model the problem as a decentralized partially observable Markov decision process (Dec-POMDP) and propose an improved multi-agent rollout algorithm. By employing agent-by-agent policy iterations, our approach implicitly considers cooperation among multiple agents and seamlessly adapts to complex scenarios where the number of agents dynamically varies. Validated in a real-world network with varying AV penetration rates and traffic flow, the simulations demonstrate that the multi-agent rollout algorithm significantly enhances performance, reducing average travel time on bottleneck segments by 9.42% with a 10% AV penetration rate.","sentences":["The integration of autonomous vehicles (AVs) into the existing transportation infrastructure offers a promising solution to alleviate congestion and enhance mobility.","This research explores a novel approach to traffic optimization by employing a multi-agent rollout approach within a mixed autonomy environment.","The study concentrates on coordinating the speed of human-driven vehicles by longitudinally controlling AVs, aiming to dynamically optimize traffic flow and alleviate congestion at highway bottlenecks in real-time.","We model the problem as a decentralized partially observable Markov decision process (Dec-POMDP) and propose an improved multi-agent rollout algorithm.","By employing agent-by-agent policy iterations, our approach implicitly considers cooperation among multiple agents and seamlessly adapts to complex scenarios where the number of agents dynamically varies.","Validated in a real-world network with varying AV penetration rates and traffic flow, the simulations demonstrate that the multi-agent rollout algorithm significantly enhances performance, reducing average travel time on bottleneck segments by 9.42% with a 10% AV penetration rate."],"url":"http://arxiv.org/abs/2405.03132v1","category":"cs.MA"}
{"created":"2024-05-06 02:53:51","title":"Active Sensing for Multiuser Beam Tracking with Reconfigurable Intelligent Surface","abstract":"This paper studies a beam tracking problem in which an access point (AP), in collaboration with a reconfigurable intelligent surface (RIS), dynamically adjusts its downlink beamformers and the reflection pattern at the RIS in order to maintain reliable communications with multiple mobile user equipments (UEs). Specifically, the mobile UEs send uplink pilots to the AP periodically during the channel sensing intervals, the AP then adaptively configures the beamformers and the RIS reflection coefficients for subsequent data transmission based on the received pilots. This is an active sensing problem, because channel sensing involves configuring the RIS coefficients during the pilot stage and the optimal sensing strategy should exploit the trajectory of channel state information (CSI) from previously received pilots. Analytical solution to such an active sensing problem is very challenging. In this paper, we propose a deep learning framework utilizing a recurrent neural network (RNN) to automatically summarize the time-varying CSI obtained from the periodically received pilots into state vectors. These state vectors are then mapped to the AP beamformers and RIS reflection coefficients for subsequent downlink data transmissions, as well as the RIS reflection coefficients for the next round of uplink channel sensing. The mappings from the state vectors to the downlink beamformers and the RIS reflection coefficients for both channel sensing and downlink data transmission are performed using graph neural networks (GNNs) to account for the interference among the UEs. Simulations demonstrate significant and interpretable performance improvement of the proposed approach over the existing data-driven methods with nonadaptive channel sensing schemes.","sentences":["This paper studies a beam tracking problem in which an access point (AP), in collaboration with a reconfigurable intelligent surface (RIS), dynamically adjusts its downlink beamformers and the reflection pattern at the RIS in order to maintain reliable communications with multiple mobile user equipments (UEs).","Specifically, the mobile UEs send uplink pilots to the AP periodically during the channel sensing intervals, the AP then adaptively configures the beamformers and the RIS reflection coefficients for subsequent data transmission based on the received pilots.","This is an active sensing problem, because channel sensing involves configuring the RIS coefficients during the pilot stage and the optimal sensing strategy should exploit the trajectory of channel state information (CSI) from previously received pilots.","Analytical solution to such an active sensing problem is very challenging.","In this paper, we propose a deep learning framework utilizing a recurrent neural network (RNN) to automatically summarize the time-varying CSI obtained from the periodically received pilots into state vectors.","These state vectors are then mapped to the AP beamformers and RIS reflection coefficients for subsequent downlink data transmissions, as well as the RIS reflection coefficients for the next round of uplink channel sensing.","The mappings from the state vectors to the downlink beamformers and the RIS reflection coefficients for both channel sensing and downlink data transmission are performed using graph neural networks (GNNs) to account for the interference among the UEs.","Simulations demonstrate significant and interpretable performance improvement of the proposed approach over the existing data-driven methods with nonadaptive channel sensing schemes."],"url":"http://arxiv.org/abs/2405.03129v1","category":"eess.SP"}
{"created":"2024-05-06 02:42:17","title":"MambaJSCC: Deep Joint Source-Channel Coding with Visual State Space Model","abstract":"Lightweight and efficient deep joint source-channel coding (JSCC) is a key technology for semantic communications. In this paper, we design a novel JSCC scheme named MambaJSCC, which utilizes a visual state space model with channel adaptation (VSSM-CA) block as its backbone for transmitting images over wireless channels. The VSSM-CA block utilizes VSSM to integrate two-dimensional images with the state space, enabling feature extraction and encoding processes to operate with linear complexity. It also incorporates channel state information (CSI) via a newly proposed CSI embedding method. This method deploys a shared CSI encoding module within both the encoder and decoder to encode and inject the CSI into each VSSM-CA block, improving the adaptability of a single model to varying channel conditions. Experimental results show that MambaJSCC not only outperforms Swin Transformer based JSCC (SwinJSCC) but also significantly reduces parameter size, computational overhead, and inference delay (ID). For example, with employing an equal number of the VSSM-CA blocks and the Swin Transformer blocks, MambaJSCC achieves a 0.48 dB gain in peak-signal-to-noise ratio (PSNR) over SwinJSCC while requiring only 53.3% multiply-accumulate operations, 53.8% of the parameters, and 44.9% of ID.","sentences":["Lightweight and efficient deep joint source-channel coding (JSCC) is a key technology for semantic communications.","In this paper, we design a novel JSCC scheme named MambaJSCC, which utilizes a visual state space model with channel adaptation (VSSM-CA) block as its backbone for transmitting images over wireless channels.","The VSSM-CA block utilizes VSSM to integrate two-dimensional images with the state space, enabling feature extraction and encoding processes to operate with linear complexity.","It also incorporates channel state information (CSI) via a newly proposed CSI embedding method.","This method deploys a shared CSI encoding module within both the encoder and decoder to encode and inject the CSI into each VSSM-CA block, improving the adaptability of a single model to varying channel conditions.","Experimental results show that MambaJSCC not only outperforms Swin Transformer based JSCC (SwinJSCC) but also significantly reduces parameter size, computational overhead, and inference delay (ID).","For example, with employing an equal number of the VSSM-CA blocks and the Swin Transformer blocks, MambaJSCC achieves a 0.48 dB gain in peak-signal-to-noise ratio (PSNR) over SwinJSCC while requiring only 53.3% multiply-accumulate operations, 53.8% of the parameters, and 44.9% of ID."],"url":"http://arxiv.org/abs/2405.03125v1","category":"cs.IT"}
{"created":"2024-05-06 02:42:01","title":"Dimension of homogeneous iterated function systems with algebraic translations","abstract":"Let $ \\mu $ be the self-similar measure associated with a homogeneous iterated function system $ \\Phi = \\{ \\lambda x + t_j \\}_{j=1}^m $ on ${\\Bbb R}$ and a probability vector $ (p_{j})_{j=1}^m$, where $0\\neq \\lambda\\in (-1,1)$ and $t_j\\in {\\Bbb R}$. Recently by modifying the arguments of Varj\\'u (2019), Rapaport and Varj\\'u (2024) showed that if $t_1,\\ldots, t_m$ are rational numbers and $0<\\lambda<1$, then $$ \\dim \\mu =\\min\\Big \\{ 1, \\; \\frac{\\sum_{j=1}^m p_{j}\\log p_{j}}{ \\log |\\lambda| }\\Big\\}$$ unless $ \\Phi $ has exact overlaps. In this paper, we further show that the above equality holds in the case when $t_1,\\ldots, t_m$ are algebraic numbers and $0<|\\lambda|<1$. This is done by adapting and extending the ideas employed in the recent papers of Breuillard, Rapaport and Varj\\'u.","sentences":["Let $ \\mu $ be the self-similar measure associated with a homogeneous iterated function system $ \\Phi = \\{ \\lambda x + t_j \\}_{j=1}^m $ on ${\\Bbb R}$ and a probability vector $ (p_{j})_{j=1}^m$, where $0\\neq \\lambda\\in (-1,1)$ and $t_j\\in {\\Bbb R}$. Recently by modifying the arguments of Varj\\'u (2019), Rapaport and Varj\\'u (2024) showed that if $t_1,\\ldots, t_m$ are rational numbers and $0<\\lambda<1$, then $$ \\dim \\mu =\\min\\Big \\{ 1, \\; \\frac{\\sum_{j=1}^m p_{j}\\log p_{j}}{ \\log |\\lambda| }\\Big\\}$$ unless $ \\Phi $ has exact overlaps.","In this paper, we further show that the above equality holds in the case when $t_1,\\ldots, t_m$ are algebraic numbers and $0<|\\lambda|<1$. This is done by adapting and extending the ideas employed in the recent papers of Breuillard, Rapaport and Varj\\'u."],"url":"http://arxiv.org/abs/2405.03124v1","category":"math.DS"}
{"created":"2024-05-06 02:32:41","title":"AniTalker: Animate Vivid and Diverse Talking Faces through Identity-Decoupled Facial Motion Encoding","abstract":"The paper introduces AniTalker, an innovative framework designed to generate lifelike talking faces from a single portrait. Unlike existing models that primarily focus on verbal cues such as lip synchronization and fail to capture the complex dynamics of facial expressions and nonverbal cues, AniTalker employs a universal motion representation. This innovative representation effectively captures a wide range of facial dynamics, including subtle expressions and head movements. AniTalker enhances motion depiction through two self-supervised learning strategies: the first involves reconstructing target video frames from source frames within the same identity to learn subtle motion representations, and the second develops an identity encoder using metric learning while actively minimizing mutual information between the identity and motion encoders. This approach ensures that the motion representation is dynamic and devoid of identity-specific details, significantly reducing the need for labeled data. Additionally, the integration of a diffusion model with a variance adapter allows for the generation of diverse and controllable facial animations. This method not only demonstrates AniTalker's capability to create detailed and realistic facial movements but also underscores its potential in crafting dynamic avatars for real-world applications. Synthetic results can be viewed at https://github.com/X-LANCE/AniTalker.","sentences":["The paper introduces AniTalker, an innovative framework designed to generate lifelike talking faces from a single portrait.","Unlike existing models that primarily focus on verbal cues such as lip synchronization and fail to capture the complex dynamics of facial expressions and nonverbal cues, AniTalker employs a universal motion representation.","This innovative representation effectively captures a wide range of facial dynamics, including subtle expressions and head movements.","AniTalker enhances motion depiction through two self-supervised learning strategies: the first involves reconstructing target video frames from source frames within the same identity to learn subtle motion representations, and the second develops an identity encoder using metric learning while actively minimizing mutual information between the identity and motion encoders.","This approach ensures that the motion representation is dynamic and devoid of identity-specific details, significantly reducing the need for labeled data.","Additionally, the integration of a diffusion model with a variance adapter allows for the generation of diverse and controllable facial animations.","This method not only demonstrates AniTalker's capability to create detailed and realistic facial movements but also underscores its potential in crafting dynamic avatars for real-world applications.","Synthetic results can be viewed at https://github.com/X-LANCE/AniTalker."],"url":"http://arxiv.org/abs/2405.03121v1","category":"cs.CV"}
{"created":"2024-05-06 01:54:21","title":"Impact of Postshock Turbulence on the Radio Spectrum of Radio Relic Shocks in Merging Clusters","abstract":"This study investigates the impact of magnetic turbulence on cosmic ray (CR) electrons through Fermi-II acceleration behind merger-driven shocks in the intracluster medium and examines how the ensuing synchrotron radio emission is influenced by the decay of magnetic energy through dissipation in the postshock region. We adopt simplified models for the momentum diffusion coefficient, specifically considering transit-time-damping resonance with fast-mode waves and gyroresonance with Alfv\\'en waves. Utilizing analytic solutions derived from diffusive shock acceleration theory, at the shock location, we introduce a CR spectrum that is either shock-injected or shock-reaccelerated. We then track its temporal evolution along the Lagrangian fluid element in the time domain. The resulting CR spectra are mapped onto a spherical shell configuration to estimate the surface brightness profile of the model radio relics. Turbulent acceleration proves to be a significant factor in delaying the aging of postshock CR electrons, while decaying magnetic fields have marginal impacts due to the dominance of inverse Compton cooling over synchrotron cooling. However, the decay of magnetic fields substantially reduces synchrotron radiation. Consequently, the spatial distribution of the postshock magnetic fields affects the volume-integrated radio spectrum and its spectral index. We demonstrate that the Mach numbers estimated from the integrated spectral index tend to be higher than the actual shock Mach numbers, highlighting the necessity for accurate modeling of postshock magnetic turbulence in interpreting observations of radio relics.","sentences":["This study investigates the impact of magnetic turbulence on cosmic ray (CR) electrons through Fermi-II acceleration behind merger-driven shocks in the intracluster medium and examines how the ensuing synchrotron radio emission is influenced by the decay of magnetic energy through dissipation in the postshock region.","We adopt simplified models for the momentum diffusion coefficient, specifically considering transit-time-damping resonance with fast-mode waves and gyroresonance with Alfv\\'en waves.","Utilizing analytic solutions derived from diffusive shock acceleration theory, at the shock location, we introduce a CR spectrum that is either shock-injected or shock-reaccelerated.","We then track its temporal evolution along the Lagrangian fluid element in the time domain.","The resulting CR spectra are mapped onto a spherical shell configuration to estimate the surface brightness profile of the model radio relics.","Turbulent acceleration proves to be a significant factor in delaying the aging of postshock CR electrons, while decaying magnetic fields have marginal impacts due to the dominance of inverse Compton cooling over synchrotron cooling.","However, the decay of magnetic fields substantially reduces synchrotron radiation.","Consequently, the spatial distribution of the postshock magnetic fields affects the volume-integrated radio spectrum and its spectral index.","We demonstrate that the Mach numbers estimated from the integrated spectral index tend to be higher than the actual shock Mach numbers, highlighting the necessity for accurate modeling of postshock magnetic turbulence in interpreting observations of radio relics."],"url":"http://arxiv.org/abs/2405.03108v1","category":"astro-ph.HE"}
{"created":"2024-05-06 01:38:11","title":"Linear-Quadratic Mean Field Stackelberg Stochastic Differential Game with Partial Information and Common Noise","abstract":"This paper is concerned with a linear-quadratic mean field Stackelberg stochastic differential game with partial information and common noise, which contains a leader and a large number of followers. To be specific, the followers face a large population Nash game after the leader first announces his strategy, while the leader will then optimize his own cost functional on consideration of the followers' reactions. The state equation of the leader and followers are both general stochastic differential equations, where the diffusion terms contain both the control and state variables. However, the followers' average state terms enter into the drift term of the leader's state equation, reflecting that the leader's state is influenced by the followers' states. By virtue of stochastic maximum principle with partial information and optimal filter technique, we deduce the open-loop adapted decentralized strategies and feedback decentralized strategies of this leader-followers system, and demonstrate that the decentralized strategies are the corresponding $\\varepsilon$-Stackelberg-Nash equilibrium.","sentences":["This paper is concerned with a linear-quadratic mean field Stackelberg stochastic differential game with partial information and common noise, which contains a leader and a large number of followers.","To be specific, the followers face a large population Nash game after the leader first announces his strategy, while the leader will then optimize his own cost functional on consideration of the followers' reactions.","The state equation of the leader and followers are both general stochastic differential equations, where the diffusion terms contain both the control and state variables.","However, the followers' average state terms enter into the drift term of the leader's state equation, reflecting that the leader's state is influenced by the followers' states.","By virtue of stochastic maximum principle with partial information and optimal filter technique, we deduce the open-loop adapted decentralized strategies and feedback decentralized strategies of this leader-followers system, and demonstrate that the decentralized strategies are the corresponding $\\varepsilon$-Stackelberg-Nash equilibrium."],"url":"http://arxiv.org/abs/2405.03102v1","category":"math.OC"}
{"created":"2024-05-06 01:05:21","title":"Research on Image Recognition Technology Based on Multimodal Deep Learning","abstract":"This project investigates the human multi-modal behavior identification algorithm utilizing deep neural networks. According to the characteristics of different modal information, different deep neural networks are used to adapt to different modal video information. Through the integration of various deep neural networks, the algorithm successfully identifies behaviors across multiple modalities. In this project, multiple cameras developed by Microsoft Kinect were used to collect corresponding bone point data based on acquiring conventional images. In this way, the motion features in the image can be extracted. Ultimately, the behavioral characteristics discerned through both approaches are synthesized to facilitate the precise identification and categorization of behaviors. The performance of the suggested algorithm was evaluated using the MSR3D data set. The findings from these experiments indicate that the accuracy in recognizing behaviors remains consistently high, suggesting that the algorithm is reliable in various scenarios. Additionally, the tests demonstrate that the algorithm substantially enhances the accuracy of detecting pedestrian behaviors in video footage.","sentences":["This project investigates the human multi-modal behavior identification algorithm utilizing deep neural networks.","According to the characteristics of different modal information, different deep neural networks are used to adapt to different modal video information.","Through the integration of various deep neural networks, the algorithm successfully identifies behaviors across multiple modalities.","In this project, multiple cameras developed by Microsoft Kinect were used to collect corresponding bone point data based on acquiring conventional images.","In this way, the motion features in the image can be extracted.","Ultimately, the behavioral characteristics discerned through both approaches are synthesized to facilitate the precise identification and categorization of behaviors.","The performance of the suggested algorithm was evaluated using the MSR3D data set.","The findings from these experiments indicate that the accuracy in recognizing behaviors remains consistently high, suggesting that the algorithm is reliable in various scenarios.","Additionally, the tests demonstrate that the algorithm substantially enhances the accuracy of detecting pedestrian behaviors in video footage."],"url":"http://arxiv.org/abs/2405.03091v1","category":"cs.CV"}
{"created":"2024-05-05 22:54:43","title":"AnoGAN for Tabular Data: A Novel Approach to Anomaly Detection","abstract":"Anomaly detection, a critical facet in data analysis, involves identifying patterns that deviate from expected behavior. This research addresses the complexities inherent in anomaly detection, exploring challenges and adapting to sophisticated malicious activities. With applications spanning cybersecurity, healthcare, finance, and surveillance, anomalies often signify critical information or potential threats. Inspired by the success of Anomaly Generative Adversarial Network (AnoGAN) in image domains, our research extends its principles to tabular data. Our contributions include adapting AnoGAN's principles to a new domain and promising advancements in detecting previously undetectable anomalies. This paper delves into the multifaceted nature of anomaly detection, considering the dynamic evolution of normal behavior, context-dependent anomaly definitions, and data-related challenges like noise and imbalances.","sentences":["Anomaly detection, a critical facet in data analysis, involves identifying patterns that deviate from expected behavior.","This research addresses the complexities inherent in anomaly detection, exploring challenges and adapting to sophisticated malicious activities.","With applications spanning cybersecurity, healthcare, finance, and surveillance, anomalies often signify critical information or potential threats.","Inspired by the success of Anomaly Generative Adversarial Network (AnoGAN) in image domains, our research extends its principles to tabular data.","Our contributions include adapting AnoGAN's principles to a new domain and promising advancements in detecting previously undetectable anomalies.","This paper delves into the multifaceted nature of anomaly detection, considering the dynamic evolution of normal behavior, context-dependent anomaly definitions, and data-related challenges like noise and imbalances."],"url":"http://arxiv.org/abs/2405.03075v1","category":"cs.LG"}
{"created":"2024-05-05 21:20:46","title":"A Greedy Quantum Route-Generation Algorithm","abstract":"Routing and scheduling problems with time windows have long been important optimization problems for logistics and planning. Many classical heuristics and exact methods exist for such problems. However, there are no satisfactory methods for generating routes using quantum computing (QC), for mainly two reasons: inequality constraints, and the trade-off of feasibility and solution quality. Inequality constraints are typically handled using slack variables; and feasible solutions are found by filtering samples. These challenges are amplified in the presence of noise inherent in QC. Here, we propose a greedy algorithm that generates routes by using information from all samples obtained from the quantum computer. By noticing the relationship between qubits in our formulation as a directed acyclic graph (DAG), we designed an algorithm that adaptively constructs a feasible solution.   We prove its convergence to a feasible solution, and illustrate its efficacy by solving the Fleet Sizing Vehicle Routing Problem with Time Windows (FSVRPTW). Our computational results show that this method obtains a lower objective value than the current state-of-the-art annealing approaches, both classical and hybrid, for the same amount of time using D-Wave Hybrid Solvers. We also show its robustness to noise on D-Wave Advantage 4.1 through computational results as compared to the filtering approach on DWaveSampler, even when the filtering approach is given a longer annealing time, and a larger sample size.","sentences":["Routing and scheduling problems with time windows have long been important optimization problems for logistics and planning.","Many classical heuristics and exact methods exist for such problems.","However, there are no satisfactory methods for generating routes using quantum computing (QC), for mainly two reasons: inequality constraints, and the trade-off of feasibility and solution quality.","Inequality constraints are typically handled using slack variables; and feasible solutions are found by filtering samples.","These challenges are amplified in the presence of noise inherent in QC.","Here, we propose a greedy algorithm that generates routes by using information from all samples obtained from the quantum computer.","By noticing the relationship between qubits in our formulation as a directed acyclic graph (DAG), we designed an algorithm that adaptively constructs a feasible solution.   ","We prove its convergence to a feasible solution, and illustrate its efficacy by solving the Fleet Sizing Vehicle Routing Problem with Time Windows (FSVRPTW).","Our computational results show that this method obtains a lower objective value than the current state-of-the-art annealing approaches, both classical and hybrid, for the same amount of time using D-Wave Hybrid Solvers.","We also show its robustness to noise on D-Wave Advantage 4.1 through computational results as compared to the filtering approach on DWaveSampler, even when the filtering approach is given a longer annealing time, and a larger sample size."],"url":"http://arxiv.org/abs/2405.03054v1","category":"quant-ph"}
{"created":"2024-05-05 17:37:50","title":"AC-MAMBASEG: An adaptive convolution and Mamba-based architecture for enhanced skin lesion segmentation","abstract":"Skin lesion segmentation is a critical task in computer-aided diagnosis systems for dermatological diseases. Accurate segmentation of skin lesions from medical images is essential for early detection, diagnosis, and treatment planning. In this paper, we propose a new model for skin lesion segmentation namely AC-MambaSeg, an enhanced model that has the hybrid CNN-Mamba backbone, and integrates advanced components such as Convolutional Block Attention Module (CBAM), Attention Gate, and Selective Kernel Bottleneck. AC-MambaSeg leverages the Vision Mamba framework for efficient feature extraction, while CBAM and Selective Kernel Bottleneck enhance its ability to focus on informative regions and suppress background noise. We evaluate the performance of AC-MambaSeg on diverse datasets of skin lesion images including ISIC-2018 and PH2; then compare it against existing segmentation methods. Our model shows promising potential for improving computer-aided diagnosis systems and facilitating early detection and treatment of dermatological diseases. Our source code will be made available at: https://github.com/vietthanh2710/AC-MambaSeg.","sentences":["Skin lesion segmentation is a critical task in computer-aided diagnosis systems for dermatological diseases.","Accurate segmentation of skin lesions from medical images is essential for early detection, diagnosis, and treatment planning.","In this paper, we propose a new model for skin lesion segmentation namely AC-MambaSeg, an enhanced model that has the hybrid CNN-Mamba backbone, and integrates advanced components such as Convolutional Block Attention Module (CBAM), Attention Gate, and Selective Kernel Bottleneck.","AC-MambaSeg leverages the Vision Mamba framework for efficient feature extraction, while CBAM and Selective Kernel Bottleneck enhance its ability to focus on informative regions and suppress background noise.","We evaluate the performance of AC-MambaSeg on diverse datasets of skin lesion images including ISIC-2018 and PH2; then compare it against existing segmentation methods.","Our model shows promising potential for improving computer-aided diagnosis systems and facilitating early detection and treatment of dermatological diseases.","Our source code will be made available at: https://github.com/vietthanh2710/AC-MambaSeg."],"url":"http://arxiv.org/abs/2405.03011v1","category":"cs.CV"}
{"created":"2024-05-05 17:27:22","title":"Safe Reinforcement Learning with Learned Non-Markovian Safety Constraints","abstract":"In safe Reinforcement Learning (RL), safety cost is typically defined as a function dependent on the immediate state and actions. In practice, safety constraints can often be non-Markovian due to the insufficient fidelity of state representation, and safety cost may not be known. We therefore address a general setting where safety labels (e.g., safe or unsafe) are associated with state-action trajectories. Our key contributions are: first, we design a safety model that specifically performs credit assignment to assess contributions of partial state-action trajectories on safety. This safety model is trained using a labeled safety dataset. Second, using RL-as-inference strategy we derive an effective algorithm for optimizing a safe policy using the learned safety model. Finally, we devise a method to dynamically adapt the tradeoff coefficient between reward maximization and safety compliance. We rewrite the constrained optimization problem into its dual problem and derive a gradient-based method to dynamically adjust the tradeoff coefficient during training. Our empirical results demonstrate that this approach is highly scalable and able to satisfy sophisticated non-Markovian safety constraints.","sentences":["In safe Reinforcement Learning (RL), safety cost is typically defined as a function dependent on the immediate state and actions.","In practice, safety constraints can often be non-Markovian due to the insufficient fidelity of state representation, and safety cost may not be known.","We therefore address a general setting where safety labels (e.g., safe or unsafe) are associated with state-action trajectories.","Our key contributions are: first, we design a safety model that specifically performs credit assignment to assess contributions of partial state-action trajectories on safety.","This safety model is trained using a labeled safety dataset.","Second, using RL-as-inference strategy we derive an effective algorithm for optimizing a safe policy using the learned safety model.","Finally, we devise a method to dynamically adapt the tradeoff coefficient between reward maximization and safety compliance.","We rewrite the constrained optimization problem into its dual problem and derive a gradient-based method to dynamically adjust the tradeoff coefficient during training.","Our empirical results demonstrate that this approach is highly scalable and able to satisfy sophisticated non-Markovian safety constraints."],"url":"http://arxiv.org/abs/2405.03005v1","category":"cs.LG"}
{"created":"2024-05-05 17:15:24","title":"Parameter-Efficient Fine-Tuning with Discrete Fourier Transform","abstract":"Low-rank adaptation~(LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices $A$ and $B$ to represent the weight change, i.e., $\\Delta W=BA$. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats $\\Delta W$ as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover $\\Delta W$. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M. Our code is released at \\url{https://github.com/Chaos96/fourierft}.","sentences":["Low-rank adaptation~(LoRA) has recently gained much interest in fine-tuning foundation models.","It effectively reduces the number of trainable parameters by incorporating low-rank matrices $A$ and $B$ to represent the weight change, i.e., $\\Delta W=BA$. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models.","In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform.","Specifically, we introduce FourierFT, which treats $\\Delta W$ as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients.","With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover $\\Delta W$.","Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification.","For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M. Our code is released at \\url{https://github.com/Chaos96/fourierft}."],"url":"http://arxiv.org/abs/2405.03003v1","category":"cs.LG"}
{"created":"2024-05-05 17:06:31","title":"MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning","abstract":"Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy. In this work, we propose MedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards biomedical applications. Instead of fine-tuning the entire LLM, MedAdapter effectively adapts the original model by fine-tuning only a small BERT-sized adapter to rank candidate solutions generated by LLMs. Experiments demonstrate that MedAdapter effectively adapts both white-box and black-box LLMs in biomedical reasoning, achieving average performance improvements of 25.48% and 11.31%, respectively, without requiring extensive computational resources or sharing data with third parties. MedAdapter also yields superior performance when combined with train-time adaptation, highlighting a flexible and complementary solution to existing adaptation methods. Faced with the challenges of balancing model performance, computational resources, and data privacy, MedAdapter provides an efficient, privacy-preserving, cost-effective, and transparent solution for adapting LLMs to the biomedical domain.","sentences":["Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy.","In this work, we propose MedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards biomedical applications.","Instead of fine-tuning the entire LLM, MedAdapter effectively adapts the original model by fine-tuning only a small BERT-sized adapter to rank candidate solutions generated by LLMs.","Experiments demonstrate that MedAdapter effectively adapts both white-box and black-box LLMs in biomedical reasoning, achieving average performance improvements of 25.48% and 11.31%, respectively, without requiring extensive computational resources or sharing data with third parties.","MedAdapter also yields superior performance when combined with train-time adaptation, highlighting a flexible and complementary solution to existing adaptation methods.","Faced with the challenges of balancing model performance, computational resources, and data privacy, MedAdapter provides an efficient, privacy-preserving, cost-effective, and transparent solution for adapting LLMs to the biomedical domain."],"url":"http://arxiv.org/abs/2405.03000v1","category":"cs.CL"}
{"created":"2024-05-05 15:27:05","title":"CoverLib: Classifiers-equipped Experience Library by Iterative Problem Distribution Coverage Maximization for Domain-tuned Motion Planning","abstract":"Library-based methods are known to be very effective for fast motion planning by adapting an experience retrieved from a precomputed library. This article presents CoverLib, a principled approach for constructing and utilizing such a library. CoverLib iteratively adds an experience-classifier-pair to the library, where each classifier corresponds to an adaptable region of the experience within the problem space. This iterative process is an active procedure, as it selects the next experience based on its ability to effectively cover the uncovered region. During the query phase, these classifiers are utilized to select an experience that is expected to be adaptable for a given problem. Experimental results demonstrate that CoverLib effectively mitigates the trade-off between plannability and speed observed in global (e.g. sampling-based) and local (e.g. optimization-based) methods. As a result, it achieves both fast planning and high success rates over the problem domain. Moreover, due to its adaptation-algorithm-agnostic nature, CoverLib seamlessly integrates with various adaptation methods, including nonlinear programming-based and sampling-based algorithms.","sentences":["Library-based methods are known to be very effective for fast motion planning by adapting an experience retrieved from a precomputed library.","This article presents CoverLib, a principled approach for constructing and utilizing such a library.","CoverLib iteratively adds an experience-classifier-pair to the library, where each classifier corresponds to an adaptable region of the experience within the problem space.","This iterative process is an active procedure, as it selects the next experience based on its ability to effectively cover the uncovered region.","During the query phase, these classifiers are utilized to select an experience that is expected to be adaptable for a given problem.","Experimental results demonstrate that CoverLib effectively mitigates the trade-off between plannability and speed observed in global (e.g. sampling-based) and local (e.g. optimization-based) methods.","As a result, it achieves both fast planning and high success rates over the problem domain.","Moreover, due to its adaptation-algorithm-agnostic nature, CoverLib seamlessly integrates with various adaptation methods, including nonlinear programming-based and sampling-based algorithms."],"url":"http://arxiv.org/abs/2405.02968v2","category":"cs.RO"}
{"created":"2024-05-05 15:00:10","title":"Silk Damping in Scalar-Induced Gravitational Waves: A Novel Probe for New Physics","abstract":"Silk damping is well known in the study of cosmic microwave background (CMB) and accounts for suppression of the angular power spectrum of CMB on large angular multipoles. In this Letter, we study the effect of Silk damping on the scalar-induced gravitational waves (SIGWs). Resulting from the dissipation of cosmic fluid, the Silk damping notably suppresses the energy-density spectrum of SIGWs on scales comparable to a diffusion scale at the decoupling time of feebly-interacting particles. The effect offers a novel observable for probing the underlying particle interaction, especially for those mediated by heavy gauge bosons beyond the standard model of particles. We anticipate that pulsar timing arrays are sensitive to gauge bosons with mass $\\sim10^{3}-10^{4}\\,\\mathrm{GeV}$, while space- and ground-based interferometers to those with mass $\\sim10^7-10^{12}\\,\\mathrm{GeV}$, leading to essential complements to on-going and future experiments of high-energy physics.","sentences":["Silk damping is well known in the study of cosmic microwave background (CMB) and accounts for suppression of the angular power spectrum of CMB on large angular multipoles.","In this Letter, we study the effect of Silk damping on the scalar-induced gravitational waves (SIGWs).","Resulting from the dissipation of cosmic fluid, the Silk damping notably suppresses the energy-density spectrum of SIGWs on scales comparable to a diffusion scale at the decoupling time of feebly-interacting particles.","The effect offers a novel observable for probing the underlying particle interaction, especially for those mediated by heavy gauge bosons beyond the standard model of particles.","We anticipate that pulsar timing arrays are sensitive to gauge bosons with mass $\\sim10^{3}-10^{4}\\,\\mathrm{GeV}$, while space- and ground-based interferometers to those with mass $\\sim10^7-10^{12}\\,\\mathrm{GeV}$, leading to essential complements to on-going and future experiments of high-energy physics."],"url":"http://arxiv.org/abs/2405.02960v1","category":"astro-ph.CO"}
{"created":"2024-05-05 14:48:13","title":"Source-Free Domain Adaptation Guided by Vision and Vision-Language Pre-Training","abstract":"Source-free domain adaptation (SFDA) aims to adapt a source model trained on a fully-labeled source domain to a related but unlabeled target domain. While the source model is a key avenue for acquiring target pseudolabels, the generated pseudolabels may exhibit source bias. In the conventional SFDA pipeline, a large data (e.g. ImageNet) pre-trained feature extractor is used to initialize the source model at the start of source training, and subsequently discarded. Despite having diverse features important for generalization, the pre-trained feature extractor can overfit to the source data distribution during source training and forget relevant target domain knowledge. Rather than discarding this valuable knowledge, we introduce an integrated framework to incorporate pre-trained networks into the target adaptation process. The proposed framework is flexible and allows us to plug modern pre-trained networks into the adaptation process to leverage their stronger representation learning capabilities. For adaptation, we propose the Co-learn algorithm to improve target pseudolabel quality collaboratively through the source model and a pre-trained feature extractor. Building on the recent success of the vision-language model CLIP in zero-shot image recognition, we present an extension Co-learn++ to further incorporate CLIP's zero-shot classification decisions. We evaluate on 3 benchmark datasets and include more challenging scenarios such as open-set, partial-set and open-partial SFDA. Experimental results demonstrate that our proposed strategy improves adaptation performance and can be successfully integrated with existing SFDA methods.","sentences":["Source-free domain adaptation (SFDA) aims to adapt a source model trained on a fully-labeled source domain to a related but unlabeled target domain.","While the source model is a key avenue for acquiring target pseudolabels, the generated pseudolabels may exhibit source bias.","In the conventional SFDA pipeline, a large data (e.g. ImageNet) pre-trained feature extractor is used to initialize the source model at the start of source training, and subsequently discarded.","Despite having diverse features important for generalization, the pre-trained feature extractor can overfit to the source data distribution during source training and forget relevant target domain knowledge.","Rather than discarding this valuable knowledge, we introduce an integrated framework to incorporate pre-trained networks into the target adaptation process.","The proposed framework is flexible and allows us to plug modern pre-trained networks into the adaptation process to leverage their stronger representation learning capabilities.","For adaptation, we propose the Co-learn algorithm to improve target pseudolabel quality collaboratively through the source model and a pre-trained feature extractor.","Building on the recent success of the vision-language model CLIP in zero-shot image recognition, we present an extension Co-learn++ to further incorporate CLIP's zero-shot classification decisions.","We evaluate on 3 benchmark datasets and include more challenging scenarios such as open-set, partial-set and open-partial SFDA.","Experimental results demonstrate that our proposed strategy improves adaptation performance and can be successfully integrated with existing SFDA methods."],"url":"http://arxiv.org/abs/2405.02954v1","category":"cs.CV"}
{"created":"2024-05-05 10:28:06","title":"FedConPE: Efficient Federated Conversational Bandits with Heterogeneous Clients","abstract":"Conversational recommender systems have emerged as a potent solution for efficiently eliciting user preferences. These systems interactively present queries associated with \"key terms\" to users and leverage user feedback to estimate user preferences more efficiently. Nonetheless, most existing algorithms adopt a centralized approach. In this paper, we introduce FedConPE, a phase elimination-based federated conversational bandit algorithm, where $M$ agents collaboratively solve a global contextual linear bandit problem with the help of a central server while ensuring secure data management. To effectively coordinate all the clients and aggregate their collected data, FedConPE uses an adaptive approach to construct key terms that minimize uncertainty across all dimensions in the feature space. Furthermore, compared with existing federated linear bandit algorithms, FedConPE offers improved computational and communication efficiency as well as enhanced privacy protections. Our theoretical analysis shows that FedConPE is minimax near-optimal in terms of cumulative regret. We also establish upper bounds for communication costs and conversation frequency. Comprehensive evaluations demonstrate that FedConPE outperforms existing conversational bandit algorithms while using fewer conversations.","sentences":["Conversational recommender systems have emerged as a potent solution for efficiently eliciting user preferences.","These systems interactively present queries associated with \"key terms\" to users and leverage user feedback to estimate user preferences more efficiently.","Nonetheless, most existing algorithms adopt a centralized approach.","In this paper, we introduce FedConPE, a phase elimination-based federated conversational bandit algorithm, where $M$ agents collaboratively solve a global contextual linear bandit problem with the help of a central server while ensuring secure data management.","To effectively coordinate all the clients and aggregate their collected data, FedConPE uses an adaptive approach to construct key terms that minimize uncertainty across all dimensions in the feature space.","Furthermore, compared with existing federated linear bandit algorithms, FedConPE offers improved computational and communication efficiency as well as enhanced privacy protections.","Our theoretical analysis shows that FedConPE is minimax near-optimal in terms of cumulative regret.","We also establish upper bounds for communication costs and conversation frequency.","Comprehensive evaluations demonstrate that FedConPE outperforms existing conversational bandit algorithms while using fewer conversations."],"url":"http://arxiv.org/abs/2405.02881v1","category":"cs.LG"}
{"created":"2024-05-05 10:13:55","title":"Exploring the Improvement of Evolutionary Computation via Large Language Models","abstract":"Evolutionary computation (EC), as a powerful optimization algorithm, has been applied across various domains. However, as the complexity of problems increases, the limitations of EC have become more apparent. The advent of large language models (LLMs) has not only transformed natural language processing but also extended their capabilities to diverse fields. By harnessing LLMs' vast knowledge and adaptive capabilities, we provide a forward-looking overview of potential improvements LLMs can bring to EC, focusing on the algorithms themselves, population design, and additional enhancements. This presents a promising direction for future research at the intersection of LLMs and EC.","sentences":["Evolutionary computation (EC), as a powerful optimization algorithm, has been applied across various domains.","However, as the complexity of problems increases, the limitations of EC have become more apparent.","The advent of large language models (LLMs) has not only transformed natural language processing but also extended their capabilities to diverse fields.","By harnessing LLMs' vast knowledge and adaptive capabilities, we provide a forward-looking overview of potential improvements LLMs can bring to EC, focusing on the algorithms themselves, population design, and additional enhancements.","This presents a promising direction for future research at the intersection of LLMs and EC."],"url":"http://arxiv.org/abs/2405.02876v1","category":"cs.NE"}
{"created":"2024-05-05 08:43:07","title":"Halfway Escape Optimization: A Quantum-Inspired Solution for Complex Optimization Problems","abstract":"This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a novel quantum-inspired metaheuristic designed to address complex optimization problems characterized by rugged landscapes and high-dimensionality with an efficient convergence rate. The study presents a comprehensive comparative evaluation of HEO's performance against established optimization algorithms, including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved Particle Swarm Optimization (QPSO). The primary analysis encompasses 14 benchmark functions with dimension 30, demonstrating HEO's effectiveness and adaptability in navigating complex optimization landscapes and providing valuable insights into its performance. The simple test of HEO in Traveling Salesman Problem (TSP) also infers its feasibility in real-time applications.","sentences":["This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a novel quantum-inspired metaheuristic designed to address complex optimization problems characterized by rugged landscapes and high-dimensionality with an efficient convergence rate.","The study presents a comprehensive comparative evaluation of HEO's performance against established optimization algorithms, including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved Particle Swarm Optimization (QPSO).","The primary analysis encompasses 14 benchmark functions with dimension 30, demonstrating HEO's effectiveness and adaptability in navigating complex optimization landscapes and providing valuable insights into its performance.","The simple test of HEO in Traveling Salesman Problem (TSP) also infers its feasibility in real-time applications."],"url":"http://arxiv.org/abs/2405.02850v1","category":"cs.NE"}
{"created":"2024-05-05 08:42:20","title":"Modelling Opaque Bilateral Market Dynamics in Financial Trading: Insights from a Multi-Agent Simulation Study","abstract":"Exploring complex adaptive financial trading environments through multi-agent based simulation methods presents an innovative approach within the realm of quantitative finance. Despite the dominance of multi-agent reinforcement learning approaches in financial markets with observable data, there exists a set of systematically significant financial markets that pose challenges due to their partial or obscured data availability. We, therefore, devise a multi-agent simulation approach employing small-scale meta-heuristic methods. This approach aims to represent the opaque bilateral market for Australian government bond trading, capturing the bilateral nature of bank-to-bank trading, also referred to as \"over-the-counter\" (OTC) trading, and commonly occurring between \"market makers\". The uniqueness of the bilateral market, characterized by negotiated transactions and a limited number of agents, yields valuable insights for agent-based modelling and quantitative finance. The inherent rigidity of this market structure, which is at odds with the global proliferation of multilateral platforms and the decentralization of finance, underscores the unique insights offered by our agent-based model. We explore the implications of market rigidity on market structure and consider the element of stability, in market design. This extends the ongoing discourse on complex financial trading environments, providing an enhanced understanding of their dynamics and implications.","sentences":["Exploring complex adaptive financial trading environments through multi-agent based simulation methods presents an innovative approach within the realm of quantitative finance.","Despite the dominance of multi-agent reinforcement learning approaches in financial markets with observable data, there exists a set of systematically significant financial markets that pose challenges due to their partial or obscured data availability.","We, therefore, devise a multi-agent simulation approach employing small-scale meta-heuristic methods.","This approach aims to represent the opaque bilateral market for Australian government bond trading, capturing the bilateral nature of bank-to-bank trading, also referred to as \"over-the-counter\" (OTC) trading, and commonly occurring between \"market makers\".","The uniqueness of the bilateral market, characterized by negotiated transactions and a limited number of agents, yields valuable insights for agent-based modelling and quantitative finance.","The inherent rigidity of this market structure, which is at odds with the global proliferation of multilateral platforms and the decentralization of finance, underscores the unique insights offered by our agent-based model.","We explore the implications of market rigidity on market structure and consider the element of stability, in market design.","This extends the ongoing discourse on complex financial trading environments, providing an enhanced understanding of their dynamics and implications."],"url":"http://arxiv.org/abs/2405.02849v1","category":"q-fin.CP"}
{"created":"2024-05-07 17:47:57","title":"A Transformer with Stack Attention","abstract":"Natural languages are believed to be (mildly) context-sensitive. Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks. In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism. Our stack-based attention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model. We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-free languages.","sentences":["Natural languages are believed to be (mildly) context-sensitive.","Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks.","In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism.","Our stack-based attention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model.","We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-free languages."],"url":"http://arxiv.org/abs/2405.04515v1","category":"cs.CL"}
{"created":"2024-05-07 16:47:06","title":"Harnack inequality for parabolic equations in double-divergence form with singular lower order coefficients","abstract":"This paper investigates the Harnack inequality for nonnegative solutions to second-order parabolic equations in double divergence form. We impose conditions where the principal coefficients satisfy the Dini mean oscillation condition in $x$, while the drift and zeroth-order coefficients belong to specific Morrey classes. Our analysis contributes to advancing the theoretical foundations of parabolic equations in double divergence form, including Fokker-Planck-Kolmogorov equations for probability densities.","sentences":["This paper investigates the Harnack inequality for nonnegative solutions to second-order parabolic equations in double divergence form.","We impose conditions where the principal coefficients satisfy the Dini mean oscillation condition in $x$, while the drift and zeroth-order coefficients belong to specific Morrey classes.","Our analysis contributes to advancing the theoretical foundations of parabolic equations in double divergence form, including Fokker-Planck-Kolmogorov equations for probability densities."],"url":"http://arxiv.org/abs/2405.04482v1","category":"math.AP"}
{"created":"2024-05-07 15:54:46","title":"Learning local Dirichlet-to-Neumann maps of nonlinear elliptic PDEs with rough coefficients","abstract":"Partial differential equations (PDEs) involving high contrast and oscillating coefficients are common in scientific and industrial applications. Numerical approximation of these PDEs is a challenging task that can be addressed, for example, by multi-scale finite element analysis. For linear problems, multi-scale finite element method (MsFEM) is well established and some viable extensions to non-linear PDEs are known. However, some features of the method seem to be intrinsically based on linearity-based. In particular, traditional MsFEM rely on the reuse of computations. For example, the stiffness matrix can be calculated just once, while being used for several right-hand sides, or as part of a multi-level iterative algorithm. Roughly speaking, the offline phase of the method amounts to pre-assembling the local linear Dirichlet-to-Neumann (DtN) operators. We present some preliminary results concerning the combination of MsFEM with machine learning tools. The extension of MsFEM to nonlinear problems is achieved by means of learning local nonlinear DtN maps. The resulting learning-based multi-scale method is tested on a set of model nonlinear PDEs involving the $p-$Laplacian and degenerate nonlinear diffusion.","sentences":["Partial differential equations (PDEs) involving high contrast and oscillating coefficients are common in scientific and industrial applications.","Numerical approximation of these PDEs is a challenging task that can be addressed, for example, by multi-scale finite element analysis.","For linear problems, multi-scale finite element method (MsFEM) is well established and some viable extensions to non-linear PDEs are known.","However, some features of the method seem to be intrinsically based on linearity-based.","In particular, traditional MsFEM rely on the reuse of computations.","For example, the stiffness matrix can be calculated just once, while being used for several right-hand sides, or as part of a multi-level iterative algorithm.","Roughly speaking, the offline phase of the method amounts to pre-assembling the local linear Dirichlet-to-Neumann (DtN) operators.","We present some preliminary results concerning the combination of MsFEM with machine learning tools.","The extension of MsFEM to nonlinear problems is achieved by means of learning local nonlinear DtN maps.","The resulting learning-based multi-scale method is tested on a set of model nonlinear PDEs involving the $p-$Laplacian and degenerate nonlinear diffusion."],"url":"http://arxiv.org/abs/2405.04433v1","category":"math.NA"}
{"created":"2024-05-07 15:41:20","title":"DistGrid: Scalable Scene Reconstruction with Distributed Multi-resolution Hash Grid","abstract":"Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled and indoor scene reconstruction. However, there exist some challenges when reconstructing large-scale scenes. MLP-based NeRFs suffer from limited network capacity, while volume-based NeRFs are heavily memory-consuming when the scene resolution increases. Recent approaches propose to geographically partition the scene and learn each sub-region using an individual NeRF. Such partitioning strategies help volume-based NeRF exceed the single GPU memory limit and scale to larger scenes. However, this approach requires multiple background NeRF to handle out-of-partition rays, which leads to redundancy of learning. Inspired by the fact that the background of current partition is the foreground of adjacent partition, we propose a scalable scene reconstruction method based on joint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding Boxes, and a novel segmented volume rendering method is proposed to handle cross-boundary rays, thereby eliminating the need for background NeRFs. The experiments demonstrate that our method outperforms existing methods on all evaluated large-scale scenes, and provides visually plausible scene reconstruction. The scalability of our method on reconstruction quality is further evaluated qualitatively and quantitatively.","sentences":["Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled and indoor scene reconstruction.","However, there exist some challenges when reconstructing large-scale scenes.","MLP-based NeRFs suffer from limited network capacity, while volume-based NeRFs are heavily memory-consuming when the scene resolution increases.","Recent approaches propose to geographically partition the scene and learn each sub-region using an individual NeRF.","Such partitioning strategies help volume-based NeRF exceed the single GPU memory limit and scale to larger scenes.","However, this approach requires multiple background NeRF to handle out-of-partition rays, which leads to redundancy of learning.","Inspired by the fact that the background of current partition is the foreground of adjacent partition, we propose a scalable scene reconstruction method based on joint Multi-resolution Hash Grids, named DistGrid.","In this method, the scene is divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding Boxes, and a novel segmented volume rendering method is proposed to handle cross-boundary rays, thereby eliminating the need for background NeRFs.","The experiments demonstrate that our method outperforms existing methods on all evaluated large-scale scenes, and provides visually plausible scene reconstruction.","The scalability of our method on reconstruction quality is further evaluated qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2405.04416v1","category":"cs.CV"}
{"created":"2024-05-07 15:15:12","title":"Geometric approaches to Lagrangian averaging","abstract":"Lagrangian averaging theories, most notably the Generalised Lagrangian Mean (GLM) theory of Andrews & McIntyre (1978), have been primarily developed in Euclidean space and Cartesian coordinates. We re-interpret these theories using a geometric, coordinate-free formulation. This gives central roles to the flow map, its decomposition into mean and perturbation maps, and the momentum 1-form dual to the velocity vector. In this interpretation, the Lagrangian mean of any tensorial quantity is obtained by averaging its pull back to the mean configuration. Crucially, the mean velocity is not a Lagrangian mean in this sense. It can be defined in a variety of ways, leading to alternative Lagrangian mean formulations that include GLM and Soward & Roberts' (2010) glm. These formulations share key features which the geometric approach uncovers. We derive governing equations both for the mean flow and for wave activities constraining the dynamics of the pertubations. The presentation focusses on the Boussinesq model for inviscid rotating stratified flows and reviews the necessary tools of differential geometry.","sentences":["Lagrangian averaging theories, most notably the Generalised Lagrangian Mean (GLM) theory of Andrews & McIntyre (1978), have been primarily developed in Euclidean space and Cartesian coordinates.","We re-interpret these theories using a geometric, coordinate-free formulation.","This gives central roles to the flow map, its decomposition into mean and perturbation maps, and the momentum 1-form dual to the velocity vector.","In this interpretation, the Lagrangian mean of any tensorial quantity is obtained by averaging its pull back to the mean configuration.","Crucially, the mean velocity is not a Lagrangian mean in this sense.","It can be defined in a variety of ways, leading to alternative Lagrangian mean formulations that include GLM and Soward & Roberts' (2010) glm.","These formulations share key features which the geometric approach uncovers.","We derive governing equations both for the mean flow and for wave activities constraining the dynamics of the pertubations.","The presentation focusses on the Boussinesq model for inviscid rotating stratified flows and reviews the necessary tools of differential geometry."],"url":"http://arxiv.org/abs/2405.04394v1","category":"physics.flu-dyn"}
{"created":"2024-05-07 13:48:59","title":"Genetic Drift Regularization: on preventing Actor Injection from breaking Evolution Strategies","abstract":"Evolutionary Algorithms (EA) have been successfully used for the optimization of neural networks for policy search, but they still remain sample inefficient and underperforming in some cases compared to gradient-based reinforcement learning (RL). Various methods combine the two approaches, many of them training a RL algorithm on data from EA evaluations and injecting the RL actor into the EA population. However, when using Evolution Strategies (ES) as the EA, the RL actor can drift genetically far from the the ES distribution and injection can cause a collapse of the ES performance. Here, we highlight the phenomenon of genetic drift where the actor genome and the ES population distribution progressively drift apart, leading to injection having a negative impact on the ES. We introduce Genetic Drift Regularization (GDR), a simple regularization method in the actor training loss that prevents the actor genome from drifting away from the ES. We show that GDR can improve ES convergence on problems where RL learns well, but also helps RL training on other tasks, , fixes the injection issues better than previous controlled injection methods.","sentences":["Evolutionary Algorithms (EA) have been successfully used for the optimization of neural networks for policy search, but they still remain sample inefficient and underperforming in some cases compared to gradient-based reinforcement learning (RL).","Various methods combine the two approaches, many of them training a RL algorithm on data from EA evaluations and injecting the RL actor into the EA population.","However, when using Evolution Strategies (ES) as the EA, the RL actor can drift genetically far from the the ES distribution and injection can cause a collapse of the ES performance.","Here, we highlight the phenomenon of genetic drift where the actor genome and the ES population distribution progressively drift apart, leading to injection having a negative impact on the ES.","We introduce Genetic Drift Regularization (GDR), a simple regularization method in the actor training loss that prevents the actor genome from drifting away from the ES.","We show that GDR can improve ES convergence on problems where RL learns well, but also helps RL training on other tasks, , fixes the injection issues better than previous controlled injection methods."],"url":"http://arxiv.org/abs/2405.04322v1","category":"cs.NE"}
{"created":"2024-05-07 13:41:34","title":"Stress solution of static linear elasticity with mixed boundary conditions via adjoint linear operators","abstract":"We revisit stress problems in linear elasticity to provide a perspective from the geometrical and functionalanalytic points of view. For the static stress problem of linear elasticity with mixed boundary conditions we write the associated pair of unbounded adjoint operators. The stress solution is found as an intersection of affine translations of the fundamental subspaces of the adjoint operators. In particular, we treat the equilibrium equation in the operator form, involving spaces of traces on a part of the boundary, known as Lions-Magenes spaces. Our analysis of the pair of adjoint operators for the problem with mixed boundary conditions relies on the properties of the analogous pair of operators for the problem with the displacement boundary conditions, which we also include in the paper.","sentences":["We revisit stress problems in linear elasticity to provide a perspective from the geometrical and functionalanalytic points of view.","For the static stress problem of linear elasticity with mixed boundary conditions we write the associated pair of unbounded adjoint operators.","The stress solution is found as an intersection of affine translations of the fundamental subspaces of the adjoint operators.","In particular, we treat the equilibrium equation in the operator form, involving spaces of traces on a part of the boundary, known as Lions-Magenes spaces.","Our analysis of the pair of adjoint operators for the problem with mixed boundary conditions relies on the properties of the analogous pair of operators for the problem with the displacement boundary conditions, which we also include in the paper."],"url":"http://arxiv.org/abs/2405.04320v1","category":"math.AP"}
{"created":"2024-05-07 13:24:38","title":"On the existence and uniqueness of weak solutions to elliptic equations with a singular drift","abstract":"In this paper we study the Dirichlet problem for a scalar elliptic equation in a bounded Lipschitz domain $\\Omega \\subset \\mathbb R^3$ with a singular drift of the form $b_0= b-\\alpha \\frac {x'}{|x'|^2}$ where $x'=(x_1,x_2,0)$, $\\alpha \\in \\mathbb R$ is a parameter and $b$ is a divergence free vector field having essentially the same regularity as the potential part of the drift. Such drifts naturally arise in the theory of axially symmetric solutions to the Navier-Stokes equations. For $\\alpha <0$ the divergence of such drifts is positive which potentially can ruin the uniqueness of solutions. Nevertheless, for $\\alpha<0$ we prove existence and H\\\"older continuity of a unique weak solution which vanishes on the axis $\\Gamma:=\\{ ~x\\in \\mathbb R^3:~|x'|=0~\\}$.","sentences":["In this paper we study the Dirichlet problem for a scalar elliptic equation in a bounded Lipschitz domain $\\Omega \\subset \\mathbb R^3$ with a singular drift of the form $b_0= b-\\alpha \\frac {x'}{|x'|^2}$ where $x'=(x_1,x_2,0)$, $\\alpha \\in \\mathbb R$ is a parameter and $b$ is a divergence free vector field having essentially the same regularity as the potential part of the drift.","Such drifts naturally arise in the theory of axially symmetric solutions to the Navier-Stokes equations.","For $\\alpha <0$ the divergence of such drifts is positive which potentially can ruin the uniqueness of solutions.","Nevertheless, for $\\alpha<0$ we prove existence and H\\\"older continuity of a unique weak solution which vanishes on the axis $\\Gamma:=\\{ ~x\\in \\mathbb R^3:~|x'|=0~\\}$."],"url":"http://arxiv.org/abs/2405.04302v1","category":"math.AP"}
{"created":"2024-05-07 13:22:03","title":"Classification of solutions to the isotropic horospherical $p$-Minkowski problem in hyperbolic plane","abstract":"In \\cite{LX}, the first author and Xu introduced and studied the horospherical $p$-Minkowski problem in hyperbolic space $\\mathbb{H}^{n+1}$. In particular, they established the uniqueness result for solutions to this problem when the prescribed function is constant and $p\\ge -n$. This paper focuses on the isotropic horospherical $p$-Minkowski problem in hyperbolic plane $\\mathbb{H}^{2}$, which corresponds to the equation \\begin{equation}\\label{0}   \\varphi^{-p}\\left(\\varphi_{\\theta\\theta}-\\frac{\\varphi_{\\theta}^2}{2\\varphi}+\\frac{\\varphi-\\varphi^{-1}}{2}\\right)=\\gamma\\quad\\text{on}\\ \\mathbb{S}^1, \\end{equation} where $\\gamma$ is a positive constant. We provide a classification of solutions to the above equation for $p\\ge -7$, as well as a nonuniqueness result of solutions for $p<-7$. Furthermore, we extend this problem to the isotropic horospherical $q$-weighted $p$-Minkowski problem in hyperbolic plane and derive some uniqueness and nonuniqueness results.","sentences":["In \\cite{LX}, the first author and Xu introduced and studied the horospherical $p$-Minkowski problem in hyperbolic space $\\mathbb{H}^{n+1}$. In particular, they established the uniqueness result for solutions to this problem when the prescribed function is constant and $p\\ge -n$.","This paper focuses on the isotropic horospherical $p$-Minkowski problem in hyperbolic plane $\\mathbb{H}^{2}$, which corresponds to the equation \\begin{equation}\\label{0}   \\varphi^{-p}\\left(\\varphi_{\\theta\\theta}-\\frac{\\varphi_{\\theta}^2}{2\\varphi}+\\frac{\\varphi-\\varphi^{-1}}{2}\\right)=\\gamma\\quad\\text{on}\\ \\mathbb{S}^1, \\end{equation} where $\\gamma$ is a positive constant.","We provide a classification of solutions to the above equation for $p\\ge -7$, as well as a nonuniqueness result of solutions for $p<-7$. Furthermore, we extend this problem to the isotropic horospherical $q$-weighted $p$-Minkowski problem in hyperbolic plane and derive some uniqueness and nonuniqueness results."],"url":"http://arxiv.org/abs/2405.04301v1","category":"math.DG"}
{"created":"2024-05-07 12:33:06","title":"Self-Stabilizing MIS Computation in the Beeping Model","abstract":"We consider self-stabilizing algorithms to compute a Maximal Independent Set (MIS) in the extremely weak beeping communication model. The model consists of an anonymous network with synchronous rounds. In each round, each vertex can optionally transmit a signal to all its neighbors (beep). After the transmission of a signal, each vertex can only differentiate between no signal received, or at least one signal received. We assume that vertices have some knowledge about the topology of the network.   We revisit the not self-stabilizing algorithm proposed by Jeavons, Scott, and Xu (2013), which computes an MIS in the beeping model. We enhance this algorithm to be self-stabilizing, and explore two different variants, which differ in the knowledge about the topology available to the vertices. In the first variant, every vertex knows an upper bound on the maximum degree $\\Delta$ of the graph. For this case, we prove that the proposed self-stabilizing version maintains the same run-time as the original algorithm, i.e. it stabilizes after $O(\\log n)$ rounds w.h.p. on any $n$-vertex graph. In the second variant, each vertex only knows an upper bound on its own degree. For this case, we prove that the algorithm stabilizes after $O(\\log n\\cdot \\log \\log n)$ rounds on any $n$-vertex graph, w.h.p.","sentences":["We consider self-stabilizing algorithms to compute a Maximal Independent Set (MIS) in the extremely weak beeping communication model.","The model consists of an anonymous network with synchronous rounds.","In each round, each vertex can optionally transmit a signal to all its neighbors (beep).","After the transmission of a signal, each vertex can only differentiate between no signal received, or at least one signal received.","We assume that vertices have some knowledge about the topology of the network.   ","We revisit the not self-stabilizing algorithm proposed by Jeavons, Scott, and Xu (2013), which computes an MIS in the beeping model.","We enhance this algorithm to be self-stabilizing, and explore two different variants, which differ in the knowledge about the topology available to the vertices.","In the first variant, every vertex knows an upper bound on the maximum degree $\\Delta$ of the graph.","For this case, we prove that the proposed self-stabilizing version maintains the same run-time as the original algorithm, i.e. it stabilizes after $O(\\log n)$ rounds w.h.p.","on any $n$-vertex graph.","In the second variant, each vertex only knows an upper bound on its own degree.","For this case, we prove that the algorithm stabilizes after $O(\\log n\\cdot \\log \\log n)$ rounds on any $n$-vertex graph, w.h.p."],"url":"http://arxiv.org/abs/2405.04266v1","category":"cs.DC"}
{"created":"2024-05-07 12:17:07","title":"Ruled Ricci surfaces and curves of constant torsion","abstract":"We show that all non-developable ruled surfaces endowed with Ricci metrics in the three-dimensional Euclidean space may be constructed using curves of constant torsion and its binormal. This allows us to give characterizations of the helicoid as the only surface of this kind that admits a parametrization with plane line of striction, and as the only with constant mean curvature.","sentences":["We show that all non-developable ruled surfaces endowed with Ricci metrics in the three-dimensional Euclidean space may be constructed using curves of constant torsion and its binormal.","This allows us to give characterizations of the helicoid as the only surface of this kind that admits a parametrization with plane line of striction, and as the only with constant mean curvature."],"url":"http://arxiv.org/abs/2405.04255v1","category":"math.DG"}
{"created":"2024-05-07 11:59:12","title":"Investigation of sample paths properties of sub-Gaussian type random fields, with application to stochasic heat equations","abstract":"The paper presents bounds for the distributions of suprema for a particular class of sub-Gaussian type random fields defined over spaces with anisotropic metrics. The results are applied to random fields related to stochastic heat equations with fractional noise: bounds for the tail distributions of suprema and estimates for the rate of growth are provided for such fields.","sentences":["The paper presents bounds for the distributions of suprema for a particular class of sub-Gaussian type random fields defined over spaces with anisotropic metrics.","The results are applied to random fields related to stochastic heat equations with fractional noise: bounds for the tail distributions of suprema and estimates for the rate of growth are provided for such fields."],"url":"http://arxiv.org/abs/2405.04242v1","category":"math.PR"}
{"created":"2024-05-07 11:46:43","title":"The Penrose Inequality for Metrics with Singular Sets","abstract":"We study the Penrose inequality and its rigidity for metrics with singular sets. Our result could be viewed as a complement of Theorem 1.1 of Lu and Miao (J. Funct. Anal. 281, 2021) and Theorem 1.2 of Shi, Wang and Yu (Math. Z. 291, 2019), in which they assume the singular set is a hypersurface and assume an additional condition on the mean curvature. As a complement, this paper study the case of singular set of dimensional less than n-1, without any additional conditions.","sentences":["We study the Penrose inequality and its rigidity for metrics with singular sets.","Our result could be viewed as a complement of Theorem 1.1 of Lu and Miao (J. Funct.","Anal.","281, 2021) and Theorem 1.2 of Shi, Wang and Yu (Math. Z. 291, 2019), in which they assume the singular set is a hypersurface and assume an additional condition on the mean curvature.","As a complement, this paper study the case of singular set of dimensional less than n-1, without any additional conditions."],"url":"http://arxiv.org/abs/2405.04227v1","category":"math.DG"}
{"created":"2024-05-07 11:29:39","title":"DESI 2024: Reconstructing Dark Energy using Crossing Statistics with DESI DR1 BAO data","abstract":"We implement Crossing Statistics to reconstruct in a model-agnostic manner the expansion history of the universe and properties of dark energy, using DESI Data Release 1 (DR1) BAO data in combination with one of three different supernova compilations (PantheonPlus, Union3, and DES-SN5YR) and Planck CMB observations. Our results hint towards an evolving and emergent dark energy behaviour, with negligible presence of dark energy at $z\\gtrsim 1$, at varying significance depending on data sets combined. In all these reconstructions, the cosmological constant lies outside the 95\\% confidence intervals for some redshift ranges. This dark energy behaviour, reconstructed using Crossing Statistics, is in agreement with results from the conventional $w_0$--$w_a$ dark energy equation of state parametrization reported in the DESI Key cosmology paper. Our results add an extensive class of model-agnostic reconstructions with acceptable fits to the data, including models where cosmic acceleration slows down at low redshifts. We also report constraints on \\Hord\\ from our model-agnostic analysis, independent of the pre-recombination physics.","sentences":["We implement Crossing Statistics to reconstruct in a model-agnostic manner the expansion history of the universe and properties of dark energy, using DESI Data Release 1 (DR1) BAO data in combination with one of three different supernova compilations (PantheonPlus, Union3, and DES-SN5YR) and Planck CMB observations.","Our results hint towards an evolving and emergent dark energy behaviour, with negligible presence of dark energy at $z\\gtrsim 1$, at varying significance depending on data sets combined.","In all these reconstructions, the cosmological constant lies outside the 95\\% confidence intervals for some redshift ranges.","This dark energy behaviour, reconstructed using Crossing Statistics, is in agreement with results from the conventional $w_0$--$w_a$ dark energy equation of state parametrization reported in the DESI Key cosmology paper.","Our results add an extensive class of model-agnostic reconstructions with acceptable fits to the data, including models where cosmic acceleration slows down at low redshifts.","We also report constraints on \\Hord\\ from our model-agnostic analysis, independent of the pre-recombination physics."],"url":"http://arxiv.org/abs/2405.04216v1","category":"astro-ph.CO"}
{"created":"2024-05-07 11:21:41","title":"Collapsing immortal K\u00e4hler-Ricci flows","abstract":"We consider the K\\\"ahler-Ricci flow on compact K\\\"ahler manifolds with semiample canonical bundle and intermediate Kodaira dimension, and show that the flow collapses to a canonical metric on the base of the Iitaka fibration in the locally smooth topology and with bounded Ricci curvature away from the singular fibers. This follows from an asymptotic expansion for the evolving metrics, in the spirit of recent work of the first and third-named authors on collapsing Calabi-Yau metrics, and proves two conjectures of Song and Tian.","sentences":["We consider the K\\\"ahler-Ricci flow on compact K\\\"ahler manifolds with semiample canonical bundle and intermediate Kodaira dimension, and show that the flow collapses to a canonical metric on the base of the Iitaka fibration in the locally smooth topology and with bounded Ricci curvature away from the singular fibers.","This follows from an asymptotic expansion for the evolving metrics, in the spirit of recent work of the first and third-named authors on collapsing Calabi-Yau metrics, and proves two conjectures of Song and Tian."],"url":"http://arxiv.org/abs/2405.04208v1","category":"math.DG"}
{"created":"2024-05-07 11:18:58","title":"Control in the coefficients of an elliptic differential operator: topological derivatives and Pontryagin maximum principle","abstract":"We consider optimal control problems, where the control appears in the main part of the operator. We derive the Pontryagin maximum principle as a necessary optimality condition. The proof uses the concept of topological derivatives. In contrast to earlier works, we do not need continuity assumptions for the coefficient or gradients of solutions of partial differential equations. Following classical proofs, we consider perturbations of optimal controls by multiples of characteristic functions of sets, whose scaling factor is send to zero. For $2d$ problems, we can perform an optimization over the elliptic shapes of such sets leading to stronger optimality conditions involving a variational inequality of a new type.","sentences":["We consider optimal control problems, where the control appears in the main part of the operator.","We derive the Pontryagin maximum principle as a necessary optimality condition.","The proof uses the concept of topological derivatives.","In contrast to earlier works, we do not need continuity assumptions for the coefficient or gradients of solutions of partial differential equations.","Following classical proofs, we consider perturbations of optimal controls by multiples of characteristic functions of sets, whose scaling factor is send to zero.","For $2d$ problems, we can perform an optimization over the elliptic shapes of such sets leading to stronger optimality conditions involving a variational inequality of a new type."],"url":"http://arxiv.org/abs/2405.04204v1","category":"math.OC"}
{"created":"2024-05-07 11:15:26","title":"Fibonacci Neural Network Approach for Numerical Solutions of Fractional Order Differential Equations","abstract":"In this paper, the authors propose the utilization of Fibonacci Neural Networks (FNN) for solving arbitrary order differential equations. The FNN architecture comprises input, middle, and output layers, with various degrees of Fibonacci polynomials serving as activation functions in the middle layer. The trial solution of the differential equation is treated as the output of the FNN, which involves adjustable parameters (weights). These weights are iteratively updated during the training of the Fibonacci neural network using backpropagation. The efficacy of the proposed method is evaluated by solving five differential problems with known exact solutions, allowing for an assessment of its accuracy. Comparative analyses are conducted against previously established techniques, demonstrating superior accuracy and efficacy in solving the addressed problems.","sentences":["In this paper, the authors propose the utilization of Fibonacci Neural Networks (FNN) for solving arbitrary order differential equations.","The FNN architecture comprises input, middle, and output layers, with various degrees of Fibonacci polynomials serving as activation functions in the middle layer.","The trial solution of the differential equation is treated as the output of the FNN, which involves adjustable parameters (weights).","These weights are iteratively updated during the training of the Fibonacci neural network using backpropagation.","The efficacy of the proposed method is evaluated by solving five differential problems with known exact solutions, allowing for an assessment of its accuracy.","Comparative analyses are conducted against previously established techniques, demonstrating superior accuracy and efficacy in solving the addressed problems."],"url":"http://arxiv.org/abs/2405.04200v1","category":"math.NT"}
{"created":"2024-05-07 07:31:42","title":"On the representation of C-recursive integer sequences by arithmetic terms","abstract":"We show that, if an integer sequence is given by a linear recurrence of constant rational coefficients, then it can be represented as the difference of two arithmetic terms with exponentiation, which do not contain any irrational constant. We apply our methods to various Lucas sequences including the classical Fibonacci sequence, to the sequence of solutions of the Pell equation and to some natural C-recursive sequences of degree 3.","sentences":["We show that, if an integer sequence is given by a linear recurrence of constant rational coefficients, then it can be represented as the difference of two arithmetic terms with exponentiation, which do not contain any irrational constant.","We apply our methods to various Lucas sequences including the classical Fibonacci sequence, to the sequence of solutions of the Pell equation and to some natural C-recursive sequences of degree 3."],"url":"http://arxiv.org/abs/2405.04083v1","category":"math.LO"}
{"created":"2024-05-07 07:17:40","title":"Weak and Perron's Solutions to Linear Kinetic Fokker-Planck Equations of Divergence Form in Bounded Domains","abstract":"In this paper, we investigate weak solutions and Perron-Wiener-Brelot solutions to the linear kinetic Fokker-Planck equation in bounded domains. We establish the existence of weak solutions by applying the Lions-Lax-Milgram theorem and the vanishing viscosity method in product domains. Additionally, we demonstrate the regularity of weak solutions and establish a strong maximum principle. Furthermore, we construct a Perron solution and provide examples of barriers in arbitrary bounded domains. Our findings are based on recent advancements in the theory of kinetic Fokker-Planck equations with rough coefficients, particularly focusing on the characterization of a weaker notion of trace defined through convolution-translation.","sentences":["In this paper, we investigate weak solutions and Perron-Wiener-Brelot solutions to the linear kinetic Fokker-Planck equation in bounded domains.","We establish the existence of weak solutions by applying the Lions-Lax-Milgram theorem and the vanishing viscosity method in product domains.","Additionally, we demonstrate the regularity of weak solutions and establish a strong maximum principle.","Furthermore, we construct a Perron solution and provide examples of barriers in arbitrary bounded domains.","Our findings are based on recent advancements in the theory of kinetic Fokker-Planck equations with rough coefficients, particularly focusing on the characterization of a weaker notion of trace defined through convolution-translation."],"url":"http://arxiv.org/abs/2405.04070v1","category":"math.AP"}
{"created":"2024-05-07 06:56:11","title":"Expansion of the Many-body Quantum Gibbs State of the Bose-Hubbard Model on a Finite Graph","abstract":"We consider the many-body quantum Gibbs state for the Bose-Hubbard model on a finite graph at positive temperature. We scale the interaction with the inverse temperature, corresponding to a mean-field limit where the temperature is of the order of the average particle number. For this model it is known that the many-body Gibbs state converges, as temperature goes to infinity, to the Gibbs measure of a discrete nonlinear Schr\\\"odinger equation, i.e., a Gibbs measure defined in terms of a one-body theory. In this article we extend these results by proving an expansion to any order of the many-body Gibbs state with inverse temperature as a small parameter. The coefficients in the expansion can be calculated as vacuum expectation values using a recursive formula, and we compute the first two coefficients explicitly.","sentences":["We consider the many-body quantum Gibbs state for the Bose-Hubbard model on a finite graph at positive temperature.","We scale the interaction with the inverse temperature, corresponding to a mean-field limit where the temperature is of the order of the average particle number.","For this model it is known that the many-body Gibbs state converges, as temperature goes to infinity, to the Gibbs measure of a discrete nonlinear Schr\\\"odinger equation, i.e., a Gibbs measure defined in terms of a one-body theory.","In this article we extend these results by proving an expansion to any order of the many-body Gibbs state with inverse temperature as a small parameter.","The coefficients in the expansion can be calculated as vacuum expectation values using a recursive formula, and we compute the first two coefficients explicitly."],"url":"http://arxiv.org/abs/2405.04055v1","category":"math-ph"}
{"created":"2024-05-07 06:29:06","title":"Scalable Vertical Federated Learning via Data Augmentation and Amortized Inference","abstract":"Vertical federated learning (VFL) has emerged as a paradigm for collaborative model estimation across multiple clients, each holding a distinct set of covariates. This paper introduces the first comprehensive framework for fitting Bayesian models in the VFL setting. We propose a novel approach that leverages data augmentation techniques to transform VFL problems into a form compatible with existing Bayesian federated learning algorithms. We present an innovative model formulation for specific VFL scenarios where the joint likelihood factorizes into a product of client-specific likelihoods. To mitigate the dimensionality challenge posed by data augmentation, which scales with the number of observations and clients, we develop a factorized amortized variational approximation that achieves scalability independent of the number of observations. We showcase the efficacy of our framework through extensive numerical experiments on logistic regression, multilevel regression, and a novel hierarchical Bayesian split neural net model. Our work paves the way for privacy-preserving, decentralized Bayesian inference in vertically partitioned data scenarios, opening up new avenues for research and applications in various domains.","sentences":["Vertical federated learning (VFL) has emerged as a paradigm for collaborative model estimation across multiple clients, each holding a distinct set of covariates.","This paper introduces the first comprehensive framework for fitting Bayesian models in the VFL setting.","We propose a novel approach that leverages data augmentation techniques to transform VFL problems into a form compatible with existing Bayesian federated learning algorithms.","We present an innovative model formulation for specific VFL scenarios where the joint likelihood factorizes into a product of client-specific likelihoods.","To mitigate the dimensionality challenge posed by data augmentation, which scales with the number of observations and clients, we develop a factorized amortized variational approximation that achieves scalability independent of the number of observations.","We showcase the efficacy of our framework through extensive numerical experiments on logistic regression, multilevel regression, and a novel hierarchical Bayesian split neural net model.","Our work paves the way for privacy-preserving, decentralized Bayesian inference in vertically partitioned data scenarios, opening up new avenues for research and applications in various domains."],"url":"http://arxiv.org/abs/2405.04043v1","category":"stat.CO"}
{"created":"2024-05-07 06:05:02","title":"$1/N$ Corrections in $\\text{QCD}_2$: Small Mass Limit and Threshold States","abstract":"In this paper we investigate $1/N$ corrections to mesonic spectrum in $1+1$-dimensional Quantum Chromodynamics ($\\text{QCD}_2$) with fundamental quarks using effective Hamiltonian method. We express the corrections in terms of 't Hooft equation solutions. First, we consider 2-flavor model with a heavy and a light quark. We show that, in contrast to some claims in earlier literature, the $1/N$ correction to the mass of the heavy-light meson remains finite when the light quark mass is taken to zero. Nevertheless, the corrections become significantly larger in this limit; we attribute this to the presence of massless modes in the spectrum. We also study the corrections to the lightest meson mass in 1-flavor model and show that they are consistent with recent numerical data, but not with the prediction coming from bosonization. Then we study low energy effective theory for 2 flavors. We show that the 3-meson interaction vertex correctly reproduces Wess-Zumino-Witten (WZW) coupling when both quarks become massless. This coupling does not change even if one of the quarks is massive. We employ Discretized Light Cone Quatization (DLCQ) to check the continuum results and show that the improved version can be used for small quark mass. Finally, we study the states associated with $1\\to 2$ meson thresholds. Using degenerate perturbation theory, we show that when the decay is allowed by parity, the infinite $N$ theory has near-threshold bound states that mix one- and two-meson parts. They are $1/3$ two-meson and $2/3$ one-meson and the corrections to their masses have unusual scaling $\\sim 1/N^{2/3}$.","sentences":["In this paper we investigate $1/N$ corrections to mesonic spectrum in $1+1$-dimensional Quantum Chromodynamics ($\\text{QCD}_2$) with fundamental quarks using effective Hamiltonian method.","We express the corrections in terms of 't Hooft equation solutions.","First, we consider 2-flavor model with a heavy and a light quark.","We show that, in contrast to some claims in earlier literature, the $1/N$ correction to the mass of the heavy-light meson remains finite when the light quark mass is taken to zero.","Nevertheless, the corrections become significantly larger in this limit; we attribute this to the presence of massless modes in the spectrum.","We also study the corrections to the lightest meson mass in 1-flavor model and show that they are consistent with recent numerical data, but not with the prediction coming from bosonization.","Then we study low energy effective theory for 2 flavors.","We show that the 3-meson interaction vertex correctly reproduces Wess-Zumino-Witten (WZW) coupling when both quarks become massless.","This coupling does not change even if one of the quarks is massive.","We employ Discretized Light Cone Quatization (DLCQ) to check the continuum results and show that the improved version can be used for small quark mass.","Finally, we study the states associated with $1\\to 2$ meson thresholds.","Using degenerate perturbation theory, we show that when the decay is allowed by parity, the infinite $N$ theory has near-threshold bound states that mix one-","and two-meson parts.","They are $1/3$ two-meson and $2/3$ one-meson and the corrections to their masses have unusual scaling $\\sim 1/N^{2/3}$."],"url":"http://arxiv.org/abs/2405.04031v1","category":"hep-th"}
{"created":"2024-05-07 04:44:59","title":"High Energy Density Radiative Transfer in the Diffusion Regime with Fourier Neural Operators","abstract":"Radiative heat transfer is a fundamental process in high energy density physics and inertial fusion. Accurately predicting the behavior of Marshak waves across a wide range of material properties and drive conditions is crucial for design and analysis of these systems. Conventional numerical solvers and analytical approximations often face challenges in terms of accuracy and computational efficiency. In this work, we propose a novel approach to model Marshak waves using Fourier Neural Operators (FNO). We develop two FNO-based models: (1) a base model that learns the mapping between the drive condition and material properties to a solution approximation based on the widely used analytic model by Hammer & Rosen (2003), and (2) a model that corrects the inaccuracies of the analytic approximation by learning the mapping to a more accurate numerical solution. Our results demonstrate the strong generalization capabilities of the FNOs and show significant improvements in prediction accuracy compared to the base analytic model.","sentences":["Radiative heat transfer is a fundamental process in high energy density physics and inertial fusion.","Accurately predicting the behavior of Marshak waves across a wide range of material properties and drive conditions is crucial for design and analysis of these systems.","Conventional numerical solvers and analytical approximations often face challenges in terms of accuracy and computational efficiency.","In this work, we propose a novel approach to model Marshak waves using Fourier Neural Operators (FNO).","We develop two FNO-based models: (1) a base model that learns the mapping between the drive condition and material properties to a solution approximation based on the widely used analytic model by Hammer & Rosen (2003), and (2) a model that corrects the inaccuracies of the analytic approximation by learning the mapping to a more accurate numerical solution.","Our results demonstrate the strong generalization capabilities of the FNOs and show significant improvements in prediction accuracy compared to the base analytic model."],"url":"http://arxiv.org/abs/2405.04003v1","category":"physics.comp-ph"}
{"created":"2024-05-07 04:43:51","title":"Quadratic varieties of small codimension","abstract":"Let $X \\subset \\mathbb P^{n+c}$ be a nondegenerate smooth projective variety of dimension $n$ defined by quadratic equations. For such varieties, P. Ionescu and F. Russo proved the Hartshorne conjecture on complete intersections, which states that X is a complete intersection provided that $n\\geq 2c+1$. As the extremal case, they also classified $X$ with $n=2c$. In this paper, we classify $X$ with $n=2c-1$.","sentences":["Let $X \\subset \\mathbb P^{n+c}$ be a nondegenerate smooth projective variety of dimension $n$ defined by quadratic equations.","For such varieties, P. Ionescu and F. Russo proved the Hartshorne conjecture on complete intersections, which states that X is a complete intersection provided that $n\\geq 2c+1$. As the extremal case, they also classified $X$ with $n=2c$. In this paper, we classify $X$ with $n=2c-1$."],"url":"http://arxiv.org/abs/2405.04002v1","category":"math.AG"}
{"created":"2024-05-07 04:39:53","title":"Volume growth and positive scalar curvature","abstract":"For three dimensional complete, non-compact Riemannian manifolds with non-negative Ricci curvature and uniformly positive scalar curvature, we obtain the sharp linear volume growth ratio and the corresponding rigidity.","sentences":["For three dimensional complete, non-compact Riemannian manifolds with non-negative Ricci curvature and uniformly positive scalar curvature, we obtain the sharp linear volume growth ratio and the corresponding rigidity."],"url":"http://arxiv.org/abs/2405.04001v1","category":"math.DG"}
{"created":"2024-05-07 04:13:29","title":"Capillary Surfaces in Manifolds with Nonnegative Scalar Curvature and Strictly Mean Convex Boundary","abstract":"In this paper we use stable capillary surfaces (analogous to the $\\mu$-bubble construction) to study manifolds with strictly mean convex boundary and nonnegative scalar curvature. We give an obstruction to filling 2-manifolds by such 3-manifolds based on the Urysohn width. We also obtain a bandwidth estimate and establish other geometric properties of such manifolds.","sentences":["In this paper we use stable capillary surfaces (analogous to the $\\mu$-bubble construction) to study manifolds with strictly mean convex boundary and nonnegative scalar curvature.","We give an obstruction to filling 2-manifolds by such 3-manifolds based on the Urysohn width.","We also obtain a bandwidth estimate and establish other geometric properties of such manifolds."],"url":"http://arxiv.org/abs/2405.03993v1","category":"math.DG"}
{"created":"2024-05-07 03:48:24","title":"Inhomogeneous wave kinetic equation and its hierarchy in polynomially weighted $L^\\infty$ spaces","abstract":"Inspired by ideas stemming from the analysis of the Boltzmann equation, in this paper we expand well-posedness theory of the spatially inhomogeneous 4-wave kinetic equation, and also analyze an infinite hierarchy of PDE associated with this nonlinear equation. More precisely, we show global in time well-posedness of the spatially inhomogeneous 4-wave kinetic equation for polynomially decaying initial data. For the associated infinite hierarchy, we construct global in time solutions using the solutions of the wave kinetic equation and the Hewitt-Savage theorem. Uniqueness of these solutions is proved by using a combinatorial board game argument tailored to this context, which allows us to control the factorial growth of the Dyson series.","sentences":["Inspired by ideas stemming from the analysis of the Boltzmann equation, in this paper we expand well-posedness theory of the spatially inhomogeneous 4-wave kinetic equation, and also analyze an infinite hierarchy of PDE associated with this nonlinear equation.","More precisely, we show global in time well-posedness of the spatially inhomogeneous 4-wave kinetic equation for polynomially decaying initial data.","For the associated infinite hierarchy, we construct global in time solutions using the solutions of the wave kinetic equation and the Hewitt-Savage theorem.","Uniqueness of these solutions is proved by using a combinatorial board game argument tailored to this context, which allows us to control the factorial growth of the Dyson series."],"url":"http://arxiv.org/abs/2405.03984v1","category":"math.AP"}
{"created":"2024-05-07 03:47:22","title":"Non-preservation of concavity properties by the Dirichlet heat flow on Riemannian manifolds","abstract":"We prove that no concavity properties are preserved by the Dirichlet heat flow in a totally convex domain of a Riemannian manifold unless the sectional curvature vanishes everywhere on the domain.","sentences":["We prove that no concavity properties are preserved by the Dirichlet heat flow in a totally convex domain of a Riemannian manifold unless the sectional curvature vanishes everywhere on the domain."],"url":"http://arxiv.org/abs/2405.03982v1","category":"math.AP"}
{"created":"2024-05-07 03:42:49","title":"Predicting Lung Disease Severity via Image-Based AQI Analysis using Deep Learning Techniques","abstract":"Air pollution is a significant health concern worldwide, contributing to various respiratory diseases. Advances in air quality mapping, driven by the emergence of smart cities and the proliferation of Internet-of-Things sensor devices, have led to an increase in available data, fueling momentum in air pollution forecasting. The objective of this study is to devise an integrated approach for predicting air quality using image data and subsequently assessing lung disease severity based on Air Quality Index (AQI).The aim is to implement an integrated approach by refining existing techniques to improve accuracy in predicting AQI and lung disease severity. The study aims to forecast additional atmospheric pollutants like AQI, PM10, O3, CO, SO2, NO2 in addition to PM2.5 levels. Additionally, the study aims to compare the proposed approach with existing methods to show its effectiveness. The approach used in this paper uses VGG16 model for feature extraction in images and neural network for predicting AQI.In predicting lung disease severity, Support Vector Classifier (SVC) and K-Nearest Neighbors (KNN) algorithms are utilized. The neural network model for predicting AQI achieved training accuracy of 88.54 % and testing accuracy of 87.44%,which was measured using loss function, while the KNN model used for predicting lung disease severity achieved training accuracy of 98.4% and testing accuracy of 97.5% In conclusion, the integrated approach presented in this study forecasts air quality and evaluates lung disease severity, achieving high testing accuracies of 87.44% for AQI and 97.5% for lung disease severity using neural network, KNN, and SVC models. The future scope involves implementing transfer learning and advanced deep learning modules to enhance prediction capabilities. While the current study focuses on India, the objective is to expand its scope to encompass global coverage.","sentences":["Air pollution is a significant health concern worldwide, contributing to various respiratory diseases.","Advances in air quality mapping, driven by the emergence of smart cities and the proliferation of Internet-of-Things sensor devices, have led to an increase in available data, fueling momentum in air pollution forecasting.","The objective of this study is to devise an integrated approach for predicting air quality using image data and subsequently assessing lung disease severity based on Air Quality Index (AQI).The aim is to implement an integrated approach by refining existing techniques to improve accuracy in predicting AQI and lung disease severity.","The study aims to forecast additional atmospheric pollutants like AQI, PM10, O3, CO, SO2, NO2 in addition to PM2.5 levels.","Additionally, the study aims to compare the proposed approach with existing methods to show its effectiveness.","The approach used in this paper uses VGG16 model for feature extraction in images and neural network for predicting AQI.In predicting lung disease severity, Support Vector Classifier (SVC) and K-Nearest Neighbors (KNN) algorithms are utilized.","The neural network model for predicting AQI achieved training accuracy of 88.54 % and testing accuracy of 87.44%,which was measured using loss function, while the KNN model used for predicting lung disease severity achieved training accuracy of 98.4% and testing accuracy of 97.5% In conclusion, the integrated approach presented in this study forecasts air quality and evaluates lung disease severity, achieving high testing accuracies of 87.44% for AQI and 97.5% for lung disease severity using neural network, KNN, and SVC models.","The future scope involves implementing transfer learning and advanced deep learning modules to enhance prediction capabilities.","While the current study focuses on India, the objective is to expand its scope to encompass global coverage."],"url":"http://arxiv.org/abs/2405.03981v1","category":"cs.CV"}
{"created":"2024-05-07 03:42:05","title":"A hydrodynamic approach to reproduce multiple spinning vortices in horizontally rotating three-dimensional liquid helium-4","abstract":"This paper reports a three-dimensional (3D) simulation of a rotating liquid, helium-4, using a two-fluid model with spin-angular momentum conservation. Our model was derived from the particle approximation of an inviscid fluid with residual viscosity. Despite the fully classical mechanical picture, the resulting system equations were consistent with those of the conventional two-fluid model. We consider bulk liquid helium-4 to be an inviscid fluid, assuming that the viscous fluid component remains at finite temperatures. As the temperature decreased, the amount of the viscous fluid component decreased, ultimately becoming a fully inviscid fluid at absolute zero. Weak compressibility is assumed to express the volume change because some helium atoms do not render fluid owing to BECs or change states because of local thermal excitation. One can solve the governing equations for an incompressible fluid using explicit SPH (smoothed particle hydrodynamics), simultaneously reproducing density fluctuations and describing the fluid in a many-particle system. We assume the following fluid-particle duality: a hydrodynamic interfacial tension between the inviscid and viscous components or a local interaction force between two types of fluid particles. The former can be induced in the horizontal direction when non-negligible non-uniformity of the particles occurs during forced two-dimensional rotation, and the latter is non-negligible when the former is negligible. We performed a largescale simulation of 3D liquid helium forced to rotate horizontally using 32 GPUs. Compared with the low-resolution calculation using 2.4 million particles, the high-resolution calculation using 19.6 million particles showed spinning vortices close to those of the theoretical solution. We obtained a promising venue to establish a practical simulation method for bulk liquid helium-4.","sentences":["This paper reports a three-dimensional (3D) simulation of a rotating liquid, helium-4, using a two-fluid model with spin-angular momentum conservation.","Our model was derived from the particle approximation of an inviscid fluid with residual viscosity.","Despite the fully classical mechanical picture, the resulting system equations were consistent with those of the conventional two-fluid model.","We consider bulk liquid helium-4 to be an inviscid fluid, assuming that the viscous fluid component remains at finite temperatures.","As the temperature decreased, the amount of the viscous fluid component decreased, ultimately becoming a fully inviscid fluid at absolute zero.","Weak compressibility is assumed to express the volume change because some helium atoms do not render fluid owing to BECs or change states because of local thermal excitation.","One can solve the governing equations for an incompressible fluid using explicit SPH (smoothed particle hydrodynamics), simultaneously reproducing density fluctuations and describing the fluid in a many-particle system.","We assume the following fluid-particle duality: a hydrodynamic interfacial tension between the inviscid and viscous components or a local interaction force between two types of fluid particles.","The former can be induced in the horizontal direction when non-negligible non-uniformity of the particles occurs during forced two-dimensional rotation, and the latter is non-negligible when the former is negligible.","We performed a largescale simulation of 3D liquid helium forced to rotate horizontally using 32 GPUs.","Compared with the low-resolution calculation using 2.4 million particles, the high-resolution calculation using 19.6 million particles showed spinning vortices close to those of the theoretical solution.","We obtained a promising venue to establish a practical simulation method for bulk liquid helium-4."],"url":"http://arxiv.org/abs/2405.03980v1","category":"physics.flu-dyn"}
{"created":"2024-05-07 03:18:47","title":"Baryonic thermal screening mass at NLO","abstract":"We determine the resummed 1-loop correction to a baryonic thermal screening mass. The calculation is carried out in the framework of a dimensionally reduced effective theory, where quarks are heavy fields due to their non-zero Matsubara frequencies. The correction due to interactions is computed at O($g^2_{ }$) in the coupling constant. In order to solve a 3-body Schr\\\"odinger equation, we exploit a two-dimensional generalization of the hyperspherical harmonics method. At electroweak scale temperatures, the NLO correction represents a $\\sim 4.6 \\%$ increase of the free-theory value $3\\pi T$ of the screening mass.","sentences":["We determine the resummed 1-loop correction to a baryonic thermal screening mass.","The calculation is carried out in the framework of a dimensionally reduced effective theory, where quarks are heavy fields due to their non-zero Matsubara frequencies.","The correction due to interactions is computed at O($g^2_{ }$) in the coupling constant.","In order to solve a 3-body Schr\\\"odinger equation, we exploit a two-dimensional generalization of the hyperspherical harmonics method.","At electroweak scale temperatures, the NLO correction represents a $\\sim 4.6 \\%$ increase of the free-theory value $3\\pi T$ of the screening mass."],"url":"http://arxiv.org/abs/2405.03975v1","category":"hep-ph"}
{"created":"2024-05-07 02:48:15","title":"Structure-based drug design by denoising voxel grids","abstract":"We present VoxBind, a new score-based generative model for 3D molecules conditioned on protein structures. Our approach represents molecules as 3D atomic density grids and leverages a 3D voxel-denoising network for learning and generation. We extend the neural empirical Bayes formalism (Saremi & Hyvarinen, 2019) to the conditional setting and generate structure-conditioned molecules with a two-step procedure: (i) sample noisy molecules from the Gaussian-smoothed conditional distribution with underdamped Langevin MCMC using the learned score function and (ii) estimate clean molecules from the noisy samples with single-step denoising. Compared to the current state of the art, our model is simpler to train, significantly faster to sample from, and achieves better results on extensive in silico benchmarks -- the generated molecules are more diverse, exhibit fewer steric clashes, and bind with higher affinity to protein pockets.","sentences":["We present VoxBind, a new score-based generative model for 3D molecules conditioned on protein structures.","Our approach represents molecules as 3D atomic density grids and leverages a 3D voxel-denoising network for learning and generation.","We extend the neural empirical Bayes formalism (Saremi & Hyvarinen, 2019) to the conditional setting and generate structure-conditioned molecules with a two-step procedure: (i) sample noisy molecules from the Gaussian-smoothed conditional distribution with underdamped Langevin MCMC using the learned score function and (ii) estimate clean molecules from the noisy samples with single-step denoising.","Compared to the current state of the art, our model is simpler to train, significantly faster to sample from, and achieves better results on extensive in silico benchmarks -- the generated molecules are more diverse, exhibit fewer steric clashes, and bind with higher affinity to protein pockets."],"url":"http://arxiv.org/abs/2405.03961v1","category":"cs.LG"}
{"created":"2024-05-07 02:46:11","title":"ESIHGNN: Event-State Interactions Infused Heterogeneous Graph Neural Network for Conversational Emotion Recognition","abstract":"Conversational Emotion Recognition (CER) aims to predict the emotion expressed by an utterance (referred to as an ``event'') during a conversation. Existing graph-based methods mainly focus on event interactions to comprehend the conversational context, while overlooking the direct influence of the speaker's emotional state on the events. In addition, real-time modeling of the conversation is crucial for real-world applications but is rarely considered. Toward this end, we propose a novel graph-based approach, namely Event-State Interactions infused Heterogeneous Graph Neural Network (ESIHGNN), which incorporates the speaker's emotional state and constructs a heterogeneous event-state interaction graph to model the conversation. Specifically, a heterogeneous directed acyclic graph neural network is employed to dynamically update and enhance the representations of events and emotional states at each turn, thereby improving conversational coherence and consistency. Furthermore, to further improve the performance of CER, we enrich the graph's edges with external knowledge. Experimental results on four publicly available CER datasets show the superiority of our approach and the effectiveness of the introduced heterogeneous event-state interaction graph.","sentences":["Conversational Emotion Recognition (CER) aims to predict the emotion expressed by an utterance (referred to as an ``event'') during a conversation.","Existing graph-based methods mainly focus on event interactions to comprehend the conversational context, while overlooking the direct influence of the speaker's emotional state on the events.","In addition, real-time modeling of the conversation is crucial for real-world applications but is rarely considered.","Toward this end, we propose a novel graph-based approach, namely Event-State Interactions infused Heterogeneous Graph Neural Network (ESIHGNN), which incorporates the speaker's emotional state and constructs a heterogeneous event-state interaction graph to model the conversation.","Specifically, a heterogeneous directed acyclic graph neural network is employed to dynamically update and enhance the representations of events and emotional states at each turn, thereby improving conversational coherence and consistency.","Furthermore, to further improve the performance of CER, we enrich the graph's edges with external knowledge.","Experimental results on four publicly available CER datasets show the superiority of our approach and the effectiveness of the introduced heterogeneous event-state interaction graph."],"url":"http://arxiv.org/abs/2405.03960v1","category":"cs.CL"}
{"created":"2024-05-07 02:12:38","title":"FedSC: Provable Federated Self-supervised Learning with Spectral Contrastive Objective over Non-i.i.d. Data","abstract":"Recent efforts have been made to integrate self-supervised learning (SSL) with the framework of federated learning (FL). One unique challenge of federated self-supervised learning (FedSSL) is that the global objective of FedSSL usually does not equal the weighted sum of local SSL objectives. Consequently, conventional approaches, such as federated averaging (FedAvg), fail to precisely minimize the FedSSL global objective, often resulting in suboptimal performance, especially when data is non-i.i.d.. To fill this gap, we propose a provable FedSSL algorithm, named FedSC, based on the spectral contrastive objective. In FedSC, clients share correlation matrices of data representations in addition to model weights periodically, which enables inter-client contrast of data samples in addition to intra-client contrast and contraction, resulting in improved quality of data representations. Differential privacy (DP) protection is deployed to control the additional privacy leakage on local datasets when correlation matrices are shared. We also provide theoretical analysis on the convergence and extra privacy leakage. The experimental results validate the effectiveness of our proposed algorithm.","sentences":["Recent efforts have been made to integrate self-supervised learning (SSL) with the framework of federated learning (FL).","One unique challenge of federated self-supervised learning (FedSSL) is that the global objective of FedSSL usually does not equal the weighted sum of local SSL objectives.","Consequently, conventional approaches, such as federated averaging (FedAvg), fail to precisely minimize the FedSSL global objective, often resulting in suboptimal performance, especially when data is non-i.i.d.. To fill this gap, we propose a provable FedSSL algorithm, named FedSC, based on the spectral contrastive objective.","In FedSC, clients share correlation matrices of data representations in addition to model weights periodically, which enables inter-client contrast of data samples in addition to intra-client contrast and contraction, resulting in improved quality of data representations.","Differential privacy (DP) protection is deployed to control the additional privacy leakage on local datasets when correlation matrices are shared.","We also provide theoretical analysis on the convergence and extra privacy leakage.","The experimental results validate the effectiveness of our proposed algorithm."],"url":"http://arxiv.org/abs/2405.03949v1","category":"cs.LG"}
{"created":"2024-05-07 01:50:49","title":"Zero order meromorphic solutions of $q$-difference equations of Malmquist type","abstract":"We consider the first order $q$-difference equation \\begin{equation}\\tag{\\dag} f(qz)^n=R(z,f), \\end{equation} where $q\\not=0,1$ is a constant and $R(z,f)$ is rational in both arguments. When $|q|\\not=1$, we show that, if $(\\dag)$ has a zero order transcendental meromorphic solution, then $(\\dag)$ reduces to a $q$-difference linear or Riccati equation, or to an equation that can be transformed to a $q$-difference Riccati equation. In the autonomous case, explicit meromorphic solutions of $(\\dag)$ are presented. Given that $(\\dag)$ can be transformed into a difference equation, we proceed to discuss the growth of the composite function $f(\\omega(z))$, where $\\omega(z)$ is an entire function satisfying $\\omega(z+1)=q\\omega(z)$, and demonstrate how the proposed difference Painlev\\'e property, as discussed in the literature, applies for $q$-difference equations.","sentences":["We consider the first order $q$-difference equation \\begin{equation}\\tag{\\dag} f(qz)^n=R(z,f), \\end{equation} where $q\\not=0,1$ is a constant and $R(z,f)$ is rational in both arguments.","When $|q|\\not=1$, we show that, if $(\\dag)$ has a zero order transcendental meromorphic solution, then $(\\dag)$ reduces to a $q$-difference linear or Riccati equation, or to an equation that can be transformed to a $q$-difference Riccati equation.","In the autonomous case, explicit meromorphic solutions of $(\\dag)$ are presented.","Given that $(\\dag)$ can be transformed into a difference equation, we proceed to discuss the growth of the composite function $f(\\omega(z))$, where $\\omega(z)$ is an entire function satisfying $\\omega(z+1)=q\\omega(z)$, and demonstrate how the proposed difference Painlev\\'e property, as discussed in the literature, applies for $q$-difference equations."],"url":"http://arxiv.org/abs/2405.03936v1","category":"math.CV"}
{"created":"2024-05-07 01:42:30","title":"Assessing observational constraints on dark energy","abstract":"Observational constraints on time-varying dark energy ({\\it e.g.}, quintessence) are commonly presented on a $w_0$-$w_a$ plot that assumes the equation of state of dark energy strictly satisfies $w(z)= w_0+ w_a z/(1+z)$ as a function of the redshift $z$. Recent observations favor a sector of the $w_0$-$w_a$ plane in which $w_0 > -1$ and $w_0+w_a< -1$, suggesting that the equation of state underwent a transition from violating the null energy condition (NEC) at large $z$ to obeying it at small $z$. In this paper, we demonstrate that this impression is misleading by showing that simple quintessence models satisfying the NEC for all $z$ predict an observational preference for the same sector. We also find that quintessence models that best fit observational data can predict a value for the dark energy equation of state at present that is significantly different from the best-fit value of $w_0$ obtained assuming the parameterization above. In addition, the analysis reveals an approximate degeneracy of the $w_0$-$w_a$ parameterization that explains the eccentricity and orientation of the likelihood contours presented in recent observational studies.","sentences":["Observational constraints on time-varying dark energy ({\\it e.g.}, quintessence) are commonly presented on a $w_0$-$w_a$ plot that assumes the equation of state of dark energy strictly satisfies $w(z)= w_0+ w_a","z/(1+z)$ as a function of the redshift $z$. Recent observations favor a sector of the $w_0$-$w_a$ plane in which $w_0 > -1$ and $w_0+w_a< -1$, suggesting that the equation of state underwent a transition from violating the null energy condition (NEC) at large $z$ to obeying it at small $z$. In this paper, we demonstrate that this impression is misleading by showing that simple quintessence models satisfying the NEC for all $z$ predict an observational preference for the same sector.","We also find that quintessence models that best fit observational data can predict a value for the dark energy equation of state at present that is significantly different from the best-fit value of $w_0$ obtained assuming the parameterization above.","In addition, the analysis reveals an approximate degeneracy of the $w_0$-$w_a$ parameterization that explains the eccentricity and orientation of the likelihood contours presented in recent observational studies."],"url":"http://arxiv.org/abs/2405.03933v1","category":"astro-ph.CO"}
{"created":"2024-05-07 00:38:50","title":"Classification of two and three dimensional complete gradient Yamabe solitons","abstract":"In this paper, we completely classify nontrivial nonflat three dimensional complete shrinking and steady gradient Yamabe solitons without any assumptions. We also give examples of complete expanding gradient Yamabe solitons. Furthermore, we give a proof of the classification of nontrivial two dimensional complete gradient Yamabe solitons without any assumptions.","sentences":["In this paper, we completely classify nontrivial nonflat three dimensional complete shrinking and steady gradient Yamabe solitons without any assumptions.","We also give examples of complete expanding gradient Yamabe solitons.","Furthermore, we give a proof of the classification of nontrivial two dimensional complete gradient Yamabe solitons without any assumptions."],"url":"http://arxiv.org/abs/2405.03921v1","category":"math.DG"}
{"created":"2024-05-07 00:36:56","title":"Unlearning Backdoor Attacks through Gradient-Based Model Pruning","abstract":"In the era of increasing concerns over cybersecurity threats, defending against backdoor attacks is paramount in ensuring the integrity and reliability of machine learning models. However, many existing approaches require substantial amounts of data for effective mitigation, posing significant challenges in practical deployment. To address this, we propose a novel approach to counter backdoor attacks by treating their mitigation as an unlearning task. We tackle this challenge through a targeted model pruning strategy, leveraging unlearning loss gradients to identify and eliminate backdoor elements within the model. Built on solid theoretical insights, our approach offers simplicity and effectiveness, rendering it well-suited for scenarios with limited data availability. Our methodology includes formulating a suitable unlearning loss and devising a model-pruning technique tailored for convolutional neural networks. Comprehensive evaluations demonstrate the efficacy of our proposed approach compared to state-of-the-art approaches, particularly in realistic data settings.","sentences":["In the era of increasing concerns over cybersecurity threats, defending against backdoor attacks is paramount in ensuring the integrity and reliability of machine learning models.","However, many existing approaches require substantial amounts of data for effective mitigation, posing significant challenges in practical deployment.","To address this, we propose a novel approach to counter backdoor attacks by treating their mitigation as an unlearning task.","We tackle this challenge through a targeted model pruning strategy, leveraging unlearning loss gradients to identify and eliminate backdoor elements within the model.","Built on solid theoretical insights, our approach offers simplicity and effectiveness, rendering it well-suited for scenarios with limited data availability.","Our methodology includes formulating a suitable unlearning loss and devising a model-pruning technique tailored for convolutional neural networks.","Comprehensive evaluations demonstrate the efficacy of our proposed approach compared to state-of-the-art approaches, particularly in realistic data settings."],"url":"http://arxiv.org/abs/2405.03918v1","category":"cs.LG"}
{"created":"2024-05-06 23:41:02","title":"A 65nm 36nJ/Decision Bio-inspired Temporal-Sparsity-Aware Digital Keyword Spotting IC with 0.6V Near-Threshold SRAM","abstract":"This paper introduces, to the best of the authors' knowledge, the first fine-grained temporal sparsity-aware keyword spotting (KWS) IC leveraging temporal similarities between neighboring feature vectors extracted from input frames and network hidden states, eliminating unnecessary operations and memory accesses. This KWS IC, featuring a bio-inspired delta-gated recurrent neural network ({\\Delta}RNN) classifier, achieves an 11-class Google Speech Command Dataset (GSCD) KWS accuracy of 90.5% and energy consumption of 36nJ/decision. At 87% temporal sparsity, computing latency and energy per inference are reduced by 2.4$\\times$/3.4$\\times$, respectively. The 65nm design occupies 0.78mm$^2$ and features two additional blocks, a compact 0.084mm$^2$ digital infinite-impulse-response (IIR)-based band-pass filter (BPF) audio feature extractor (FEx) and a 24kB 0.6V near-Vth weight SRAM with 6.6$\\times$ lower read power compared to the standard SRAM.","sentences":["This paper introduces, to the best of the authors' knowledge, the first fine-grained temporal sparsity-aware keyword spotting (KWS) IC leveraging temporal similarities between neighboring feature vectors extracted from input frames and network hidden states, eliminating unnecessary operations and memory accesses.","This KWS IC, featuring a bio-inspired delta-gated recurrent neural network ({\\Delta}RNN) classifier, achieves an 11-class Google Speech Command Dataset (GSCD) KWS accuracy of 90.5% and energy consumption of 36nJ/decision.","At 87% temporal sparsity, computing latency and energy per inference are reduced by 2.4$\\times$/3.4$\\times$, respectively.","The 65nm design occupies 0.78mm$^2$ and features two additional blocks, a compact 0.084mm$^2$ digital infinite-impulse-response (IIR)-based band-pass filter (BPF) audio feature extractor (FEx) and a 24kB 0.6V near-Vth weight SRAM with 6.6$\\times$ lower read power compared to the standard SRAM."],"url":"http://arxiv.org/abs/2405.03905v1","category":"cs.AR"}
{"created":"2024-05-06 23:07:19","title":"Multi-Mode Array Filtering of Resonance Fluorescence","abstract":"We present a novel frequency-filtering method for measuring and calculating frequency-filtered photon-correlations. This novel method is a cavity-based system we call the multi-mode array filter, which consists of an array of tunable single-mode cavities that are equally spaced in frequency. By introducing a mode-dependent phase modulation, we produce a near rectangular frequency response, allowing us to increase the filter bandwidth -- and thus the temporal response -- without sacrificing frequency isolation. We model the frequency filtering using a cascaded quantum open systems approach which completely neglects any back-action of the filter onto the source system. This allows us to derive a closed set of operator moment equations for source and filter system operators, thus providing an extremely efficient method to calculate frequency-filtered first- and second-order correlation functions. We demonstrate this novel filtering method by applying it to a resonantly driven two-level atom. We present examples of frequency-filtered power spectra to demonstrate the improved frequency isolation of the multi-mode array filter over the single-mode filter. We then present results for the single-mode and multi-mode-array filtered second-order auto- and cross-correlation functions. These are compared against expressions derived in the secular approximation. The improved frequency isolation of the multi-mode array filter allows us to investigate new regimes of frequency-filtered photon correlations, such as two-photon leapfrog processes, and the effect of vanishing bandwidth on filtered auto-correlation functions.","sentences":["We present a novel frequency-filtering method for measuring and calculating frequency-filtered photon-correlations.","This novel method is a cavity-based system we call the multi-mode array filter, which consists of an array of tunable single-mode cavities that are equally spaced in frequency.","By introducing a mode-dependent phase modulation, we produce a near rectangular frequency response, allowing us to increase the filter bandwidth -- and thus the temporal response -- without sacrificing frequency isolation.","We model the frequency filtering using a cascaded quantum open systems approach which completely neglects any back-action of the filter onto the source system.","This allows us to derive a closed set of operator moment equations for source and filter system operators, thus providing an extremely efficient method to calculate frequency-filtered first- and second-order correlation functions.","We demonstrate this novel filtering method by applying it to a resonantly driven two-level atom.","We present examples of frequency-filtered power spectra to demonstrate the improved frequency isolation of the multi-mode array filter over the single-mode filter.","We then present results for the single-mode and multi-mode-array filtered second-order auto- and cross-correlation functions.","These are compared against expressions derived in the secular approximation.","The improved frequency isolation of the multi-mode array filter allows us to investigate new regimes of frequency-filtered photon correlations, such as two-photon leapfrog processes, and the effect of vanishing bandwidth on filtered auto-correlation functions."],"url":"http://arxiv.org/abs/2405.03900v1","category":"quant-ph"}
{"created":"2024-05-06 23:00:28","title":"Quasi-positive curvature and vanishing theorems","abstract":"In this paper, we consider mixed curvature $\\mathcal{C}_{a,b}$, which is a convex combination of Ricci curvature and holomorphic sectional curvature introduced by Chu-Lee-Tam. We prove that if a compact complex manifold M admits a K\\\"{a}hler metric with quasi-positive mixed curvature and $3a+2b\\geq0$, then it is projective. If $a,b\\geq0$, then M is rationally connected. As a corollary, the same result holds for k-Ricci curvature. We also show that any compact K\\\"{a}hler manifold with quasi-positive 2-scalar curvature is projective. Lastly, we generalize the result to the Hermitian case. In particular, any compact Hermitian threefold with quasi-positive real bisectional curvature have vanishing Hodge number $h^{2,0}$. Furthermore, if it is K\\\"{a}hlerian, it is projective.","sentences":["In this paper, we consider mixed curvature $\\mathcal{C}_{a,b}$, which is a convex combination of Ricci curvature and holomorphic sectional curvature introduced by Chu-Lee-Tam.","We prove that if a compact complex manifold M admits a K\\\"{a}hler metric with quasi-positive mixed curvature and $3a+2b\\geq0$, then it is projective.","If $a,b\\geq0$, then M is rationally connected.","As a corollary, the same result holds for k-Ricci curvature.","We also show that any compact K\\\"{a}hler manifold with quasi-positive 2-scalar curvature is projective.","Lastly, we generalize the result to the Hermitian case.","In particular, any compact Hermitian threefold with quasi-positive real bisectional curvature have vanishing Hodge number $h^{2,0}$.","Furthermore, if it is K\\\"{a}hlerian, it is projective."],"url":"http://arxiv.org/abs/2405.03895v1","category":"math.DG"}
{"created":"2024-05-06 22:14:13","title":"Purity for Perfectoidness","abstract":"There has been a long-standing question about whether being perfectoid for an algebra is local in the analytic topology. We provide affirmative answers for the algebras (e.g., over $\\overline{\\mathbb{Z}_p}$) whose spectra are inverse limits of semi-stable affine schemes. In fact, we established a valuative criterion for such an algebra being perfectoid, saying that it suffices to check the perfectoidness of the stalks of the associated Riemann-Zariski space. Combining with Gabber-Ramero's computation of differentials of valuation rings, we obtain a differential criterion for perfectoidness. We also establish a purity result for perfectoidness when the limit preserves generic points of the special fibres.   As an application to limits of smooth $p$-adic varieties (on the generic point), assuming either the poly-stable modification conjecture or working only with curves, we prove that stalk-wise perfectoidness implies vanishing of the higher completed \\'etale cohomology groups of the smooth varieties, which is inspired by Scholze's vanishing for Shimura varieties. Moreover, we give an explicit description of the completed \\'etale cohomology group in the top degree in terms of the colimit of Zariski cohomology groups of the structural sheaves.","sentences":["There has been a long-standing question about whether being perfectoid for an algebra is local in the analytic topology.","We provide affirmative answers for the algebras (e.g., over $\\overline{\\mathbb{Z}_p}$) whose spectra are inverse limits of semi-stable affine schemes.","In fact, we established a valuative criterion for such an algebra being perfectoid, saying that it suffices to check the perfectoidness of the stalks of the associated Riemann-Zariski space.","Combining with Gabber-Ramero's computation of differentials of valuation rings, we obtain a differential criterion for perfectoidness.","We also establish a purity result for perfectoidness when the limit preserves generic points of the special fibres.   ","As an application to limits of smooth $p$-adic varieties (on the generic point), assuming either the poly-stable modification conjecture or working only with curves, we prove that stalk-wise perfectoidness implies vanishing of the higher completed \\'etale cohomology groups of the smooth varieties, which is inspired by Scholze's vanishing for Shimura varieties.","Moreover, we give an explicit description of the completed \\'etale cohomology group in the top degree in terms of the colimit of Zariski cohomology groups of the structural sheaves."],"url":"http://arxiv.org/abs/2405.03886v1","category":"math.AG"}
{"created":"2024-05-06 21:16:27","title":"Theory of inverse Edelstein effect induced by thermal spin injection","abstract":"We theoretically consider a junction composed of a ferromagnetic insulator (FI) and a two-dimensional electron gas (2DEG) with Rashba- and Dresselhaus-type spin-orbit interactions. Using the Boltzmann equation, we calculate an electric current in 2DEG induced by the inverse Edelstein when imposing the temperature difference between FI and 2DEG. We clarify how the induced current depends on the magnetization direction of FI, spin texture on the Fermi surface of 2DEG, and temperature. Our result provides an important foundation for an accurate analysis of the inverse Edelstein effect induced by thermal spin injection.","sentences":["We theoretically consider a junction composed of a ferromagnetic insulator (FI) and a two-dimensional electron gas (2DEG) with Rashba- and Dresselhaus-type spin-orbit interactions.","Using the Boltzmann equation, we calculate an electric current in 2DEG induced by the inverse Edelstein when imposing the temperature difference between FI and 2DEG.","We clarify how the induced current depends on the magnetization direction of FI, spin texture on the Fermi surface of 2DEG, and temperature.","Our result provides an important foundation for an accurate analysis of the inverse Edelstein effect induced by thermal spin injection."],"url":"http://arxiv.org/abs/2405.03858v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-06 21:00:59","title":"Provable Preconditioned Plug-and-Play Approach for Compressed Sensing MRI Reconstruction","abstract":"Model-based methods play a key role in the reconstruction of compressed sensing (CS) MRI. Finding an effective prior to describe the statistical distribution of the image family of interest is crucial for model-based methods. Plug-and-play (PnP) is a general framework that uses denoising algorithms as the prior or regularizer. Recent work showed that PnP methods with denoisers based on pretrained convolutional neural networks outperform other classical regularizers in CS MRI reconstruction. However, the numerical solvers for PnP can be slow for CS MRI reconstruction. This paper proposes a preconditioned PnP (P^2nP) method to accelerate the convergence speed. Moreover, we provide proofs of the fixed-point convergence of the P^2nP iterates. Numerical experiments on CS MRI reconstruction with non-Cartesian sampling trajectories illustrate the effectiveness and efficiency of the P^2nP approach.","sentences":["Model-based methods play a key role in the reconstruction of compressed sensing (CS) MRI.","Finding an effective prior to describe the statistical distribution of the image family of interest is crucial for model-based methods.","Plug-and-play (PnP) is a general framework that uses denoising algorithms as the prior or regularizer.","Recent work showed that PnP methods with denoisers based on pretrained convolutional neural networks outperform other classical regularizers in CS MRI reconstruction.","However, the numerical solvers for PnP can be slow for CS MRI reconstruction.","This paper proposes a preconditioned PnP (P^2nP) method to accelerate the convergence speed.","Moreover, we provide proofs of the fixed-point convergence of the P^2nP iterates.","Numerical experiments on CS MRI reconstruction with non-Cartesian sampling trajectories illustrate the effectiveness and efficiency of the P^2nP approach."],"url":"http://arxiv.org/abs/2405.03854v1","category":"eess.IV"}
{"created":"2024-05-06 20:47:32","title":"Tamanoi equation for orbifold Euler characteristics: revisited","abstract":"Tamanoi equation is a Macdonald type equation for the orbifold Euler characteristic and for its analogues of higher orders. It claims that the generating series of the orbifold Euler characteristics of a fixed order of analogues of the symmetric powers for a space with a finite group action can be represented as a certain unified (explicitly written) power series in the exponent equal to the orbifold Euler characteristic of the same order of the space itself. In the paper, in particular, we explain how the Tamanoi equation follows from its verification for actions of (finite) groups on the one-point space. Statements used for that are generalized to analogues of the orbifold Euler characteristic corresponding to finitely generated groups. It is shown that, for these generalizatins, the analogue of the Tamanoi equation does not hold in general.","sentences":["Tamanoi equation is a Macdonald type equation for the orbifold Euler characteristic and for its analogues of higher orders.","It claims that the generating series of the orbifold Euler characteristics of a fixed order of analogues of the symmetric powers for a space with a finite group action can be represented as a certain unified (explicitly written) power series in the exponent equal to the orbifold Euler characteristic of the same order of the space itself.","In the paper, in particular, we explain how the Tamanoi equation follows from its verification for actions of (finite) groups on the one-point space.","Statements used for that are generalized to analogues of the orbifold Euler characteristic corresponding to finitely generated groups.","It is shown that, for these generalizatins, the analogue of the Tamanoi equation does not hold in general."],"url":"http://arxiv.org/abs/2405.03843v1","category":"math.AG"}
{"created":"2024-05-06 20:44:21","title":"Everything everywhere all at once: A detailed study of GW230529","abstract":"This study investigates the origins of GW230529, delving into its formation from massive stars within isolated binary systems. Utilizing population synthesis models, we present compelling evidence that the neutron star component forms first. However, the event's low signal-to-noise ratio introduces complexities in identifying the underlying physical mechanisms driving its formation. Augmenting our analysis with insights from numerical relativity, we estimate the final black hole mass and spin to be approximately $5.3 M_\\odot$ and $0.53$, respectively. Furthermore, we employ the obtained posterior samples to calculate the ejecta mass and kilonova light curves resulting from r-process nucleosynthesis. We find the ejecta mass to range within $0-0.06 M_{\\odot}$, contingent on the neutron star equation of state. The peak brightness of the kilonovae light curves indicates that targeted follow-up observations with a Rubin-like observatory may have detected this emission.","sentences":["This study investigates the origins of GW230529, delving into its formation from massive stars within isolated binary systems.","Utilizing population synthesis models, we present compelling evidence that the neutron star component forms first.","However, the event's low signal-to-noise ratio introduces complexities in identifying the underlying physical mechanisms driving its formation.","Augmenting our analysis with insights from numerical relativity, we estimate the final black hole mass and spin to be approximately $5.3 M_\\odot$ and $0.53$, respectively.","Furthermore, we employ the obtained posterior samples to calculate the ejecta mass and kilonova light curves resulting from r-process nucleosynthesis.","We find the ejecta mass to range within $0-0.06 M_{\\odot}$, contingent on the neutron star equation of state.","The peak brightness of the kilonovae light curves indicates that targeted follow-up observations with a Rubin-like observatory may have detected this emission."],"url":"http://arxiv.org/abs/2405.03841v1","category":"astro-ph.HE"}
{"created":"2024-05-06 20:43:02","title":"End-to-End Autoencoder for Drill String Acoustic Communications","abstract":"Drill string communications are important for drilling efficiency and safety. The design of a low latency drill string communication system with high throughput and reliability remains an open challenge. In this paper a deep learning autoencoder (AE) based end-to-end communication system, where transmitter and receiver implemented as feed forward neural networks, is proposed for acousticdrill string communications. Simulation shows that the AE system is able to outperform a baseline non-contiguous OFDM system in terms of BER and PAPR, operating with lower latency.","sentences":["Drill string communications are important for drilling efficiency and safety.","The design of a low latency drill string communication system with high throughput and reliability remains an open challenge.","In this paper a deep learning autoencoder (AE) based end-to-end communication system, where transmitter and receiver implemented as feed forward neural networks, is proposed for acousticdrill string communications.","Simulation shows that the AE system is able to outperform a baseline non-contiguous OFDM system in terms of BER and PAPR, operating with lower latency."],"url":"http://arxiv.org/abs/2405.03840v1","category":"cs.LG"}
{"created":"2024-05-06 20:42:06","title":"Topological regularity and stability of noncollapsed spaces with Ricci curvature bounded below","abstract":"We investigate the topological regularity and stability of noncollapsed Ricci limit spaces $(M_i^n,g_i,p_i)\\to (X^n,d)$. We confirm a conjecture proposed by Colding and Naber in dimension $n=4$, showing that the cross-sections of tangent cones at a given point $x\\in X^4$ are all homeomorphic to a fixed spherical space form $S^3/\\Gamma_x$, and $\\Gamma_x$ is trivial away from a $0$-dimensional set. In dimensions $n>4$, we show an analogous statement at points where all tangent cones are $(n-4)$-symmetric. Furthermore, we prove that $(n-3)$-symmetric noncollapsed Ricci limits are topological manifolds, thus confirming a particular case of a conjecture due to Cheeger, Colding, and Tian. Our analysis relies on two key results, whose importance goes beyond their applications in the study of cross-sections of noncollapsed Ricci limit spaces: (i) A new manifold recognition theorem for noncollapsed ${\\rm RCD}(-2,3)$ spaces. (ii) A cone rigidity result ruling out noncollapsed Ricci limit spaces of the form $\\mathbb{R}^{n-3}\\times C(\\mathbb{RP}^2)$.","sentences":["We investigate the topological regularity and stability of noncollapsed Ricci limit spaces $(M_i^n,g_i,p_i)\\to (X^n,d)$.","We confirm a conjecture proposed by Colding and Naber in dimension $n=4$, showing that the cross-sections of tangent cones at a given point $x\\in X^4$ are all homeomorphic to a fixed spherical space form $S^3/\\Gamma_x$, and $\\Gamma_x$ is trivial away from a $0$-dimensional set.","In dimensions $n>4$, we show an analogous statement at points where all tangent cones are $(n-4)$-symmetric.","Furthermore, we prove that $(n-3)$-symmetric noncollapsed Ricci limits are topological manifolds, thus confirming a particular case of a conjecture due to Cheeger, Colding, and Tian.","Our analysis relies on two key results, whose importance goes beyond their applications in the study of cross-sections of noncollapsed Ricci limit spaces: (i) A new manifold recognition theorem for noncollapsed ${\\rm RCD}(-2,3)$ spaces.","(ii) A cone rigidity result ruling out noncollapsed Ricci limit spaces of the form $\\mathbb{R}^{n-3}\\times C(\\mathbb{RP}^2)$."],"url":"http://arxiv.org/abs/2405.03839v1","category":"math.DG"}
{"created":"2024-05-06 20:36:50","title":"Kannappan-Wilson and Van Vleck-Wilson functional equations on semigroups","abstract":"Let $S$ be a semigroup, $Z(S)$ the center of $S$ and $\\sigma:S\\rightarrow S$ is an involutive automorphism. Our main results is that we describe the solutions of the Kannappan-Wilson functional equation \\[\\displaystyle \\int_{S} f(xyt)d\\mu(t) +\\displaystyle \\int_{S} f(\\sigma(y)xt)d\\mu(t)= 2f(x)g(y),\\ x,y\\in S,\\] and the Van Vleck-Wilson functional equation \\[\\displaystyle \\int_{S} f(xyt)d\\mu(t) -\\displaystyle \\int_{S} f(\\sigma(y)xt)d\\mu(t)= 2f(x)g(y),\\ x,y\\in S,\\] where $\\mu$ is a measure that is a linear combination of Dirac measures $(\\delta_{z_i})_{i\\in I}$, such that $z_i\\in Z(S)$ for all $i\\in I$. Interesting consequences of these results are presented.","sentences":["Let $S$ be a semigroup, $Z(S)$ the center of $S$ and $\\sigma:S\\rightarrow S$ is an involutive automorphism.","Our main results is that we describe the solutions of the Kannappan-Wilson functional equation \\[\\displaystyle \\int_{S} f(xyt)d\\mu(t)","+\\displaystyle \\int_{S} f(\\sigma(y)xt)d\\mu(t)= 2f(x)g(y),\\ x,y\\in S,\\] and the Van Vleck-Wilson functional equation \\[\\displaystyle \\int_{S} f(xyt)d\\mu(t) -\\displaystyle \\int_{S} f(\\sigma(y)xt)d\\mu(t)= 2f(x)g(y),\\ x,y\\in S,\\] where $\\mu$ is a measure that is a linear combination of Dirac measures $(\\delta_{z_i})_{i\\in I}$, such that $z_i\\in Z(S)$ for all $i\\in I$. Interesting consequences of these results are presented."],"url":"http://arxiv.org/abs/2405.03835v1","category":"math.FA"}
{"created":"2024-05-06 20:17:10","title":"Direct learning of home vector direction for insect-inspired robot navigation","abstract":"Insects have long been recognized for their ability to navigate and return home using visual cues from their nest's environment. However, the precise mechanism underlying this remarkable homing skill remains a subject of ongoing investigation. Drawing inspiration from the learning flights of honey bees and wasps, we propose a robot navigation method that directly learns the home vector direction from visual percepts during a learning flight in the vicinity of the nest. After learning, the robot will travel away from the nest, come back by means of odometry, and eliminate the resultant drift by inferring the home vector orientation from the currently experienced view. Using a compact convolutional neural network, we demonstrate successful learning in both simulated and real forest environments, as well as successful homing control of a simulated quadrotor. The average errors of the inferred home vectors in general stay well below the 90{\\deg} required for successful homing, and below 24{\\deg} if all images contain sufficient texture and illumination. Moreover, we show that the trajectory followed during the initial learning flight has a pronounced impact on the network's performance. A higher density of sample points in proximity to the nest results in a more consistent return. Code and data are available at https://mavlab.tudelft.nl/learning_to_home .","sentences":["Insects have long been recognized for their ability to navigate and return home using visual cues from their nest's environment.","However, the precise mechanism underlying this remarkable homing skill remains a subject of ongoing investigation.","Drawing inspiration from the learning flights of honey bees and wasps, we propose a robot navigation method that directly learns the home vector direction from visual percepts during a learning flight in the vicinity of the nest.","After learning, the robot will travel away from the nest, come back by means of odometry, and eliminate the resultant drift by inferring the home vector orientation from the currently experienced view.","Using a compact convolutional neural network, we demonstrate successful learning in both simulated and real forest environments, as well as successful homing control of a simulated quadrotor.","The average errors of the inferred home vectors in general stay well below the 90{\\deg} required for successful homing, and below 24{\\deg} if all images contain sufficient texture and illumination.","Moreover, we show that the trajectory followed during the initial learning flight has a pronounced impact on the network's performance.","A higher density of sample points in proximity to the nest results in a more consistent return.","Code and data are available at https://mavlab.tudelft.nl/learning_to_home ."],"url":"http://arxiv.org/abs/2405.03827v1","category":"cs.RO"}
{"created":"2024-05-06 20:05:16","title":"Learning in associative networks through Pavlovian dynamics","abstract":"Hebbian learning theory is rooted in Pavlov's Classical Conditioning. In addition, while mathematical models of the former have been proposed and studied in the past decades, especially in spin glass theory, only recently it has been numerically shown that the Pavlovian neural and synaptic dynamics mechanisms give rise to synaptic weights that correspond to the Hebbian learning rule. In this paper, we show that we can analytically prove this result. Moreover, we obtain the Pavlovian dynamics directly with statistical mechanics considerations, so that we can model the whole process within the same framework. Then, we test and study the model in different numerical settings. Finally, drawing from evidence on pure memory reinforcement during sleep stages, we show how the proposed model can simulate neural networks that undergo sleep-associated memory consolidation processes, thereby proving the compatibility of Pavlovian learning.","sentences":["Hebbian learning theory is rooted in Pavlov's Classical Conditioning.","In addition, while mathematical models of the former have been proposed and studied in the past decades, especially in spin glass theory, only recently it has been numerically shown that the Pavlovian neural and synaptic dynamics mechanisms give rise to synaptic weights that correspond to the Hebbian learning rule.","In this paper, we show that we can analytically prove this result.","Moreover, we obtain the Pavlovian dynamics directly with statistical mechanics considerations, so that we can model the whole process within the same framework.","Then, we test and study the model in different numerical settings.","Finally, drawing from evidence on pure memory reinforcement during sleep stages, we show how the proposed model can simulate neural networks that undergo sleep-associated memory consolidation processes, thereby proving the compatibility of Pavlovian learning."],"url":"http://arxiv.org/abs/2405.03823v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-06 19:53:55","title":"Statistical inference for a stochastic generalized logistic differential equation","abstract":"This research aims to estimate three parameters in a stochastic generalized logistic differential equation. We assume the intrinsic growth rate and shape parameters are constant but unknown. To estimate these two parameters, we use the maximum likelihood method and establish that the estimators for these two parameters are strongly consistent. We estimate the diffusion parameter by using the quadratic variation processes. To test our results, we evaluate two data scenarios, complete and incomplete, with fixed values assigned to the three parameters. In the incomplete data scenario, we apply an Expectation Maximization algorithm.","sentences":["This research aims to estimate three parameters in a stochastic generalized logistic differential equation.","We assume the intrinsic growth rate and shape parameters are constant but unknown.","To estimate these two parameters, we use the maximum likelihood method and establish that the estimators for these two parameters are strongly consistent.","We estimate the diffusion parameter by using the quadratic variation processes.","To test our results, we evaluate two data scenarios, complete and incomplete, with fixed values assigned to the three parameters.","In the incomplete data scenario, we apply an Expectation Maximization algorithm."],"url":"http://arxiv.org/abs/2405.03815v1","category":"stat.ME"}
{"created":"2024-05-06 19:04:13","title":"Approximating a branch of solutions to the Navier--Stokes equations by reduced-order modeling","abstract":"This paper extends a low-rank tensor decomposition (LRTD) reduced order model (ROM) methodology to simulate viscous flows and in particular to predict a smooth branch of solutions for the incompressible Navier-Stokes equations. Additionally, it enhances the LRTD-ROM methodology by introducing a non-interpolatory variant, which demonstrates improved accuracy compared to the interpolatory method utilized in previous LRTD-ROM studies. After presenting both the interpolatory and non-interpolatory LRTD-ROM, we demonstrate that with snapshots from a few different viscosities, the proposed method is able to accurately predict flow statistics in the Reynolds number range $[25,400]$. This is a significantly wider and higher range than state of the art (and similar size) ROMs built for use on varying Reynolds number have been successful on. The paper also discusses how LRTD may offer new insights into the properties of parametric solutions.","sentences":["This paper extends a low-rank tensor decomposition (LRTD) reduced order model (ROM) methodology to simulate viscous flows and in particular to predict a smooth branch of solutions for the incompressible Navier-Stokes equations.","Additionally, it enhances the LRTD-ROM methodology by introducing a non-interpolatory variant, which demonstrates improved accuracy compared to the interpolatory method utilized in previous LRTD-ROM studies.","After presenting both the interpolatory and non-interpolatory LRTD-ROM, we demonstrate that with snapshots from a few different viscosities, the proposed method is able to accurately predict flow statistics in the Reynolds number range $[25,400]$.","This is a significantly wider and higher range than state of the art (and similar size) ROMs built for use on varying Reynolds number have been successful on.","The paper also discusses how LRTD may offer new insights into the properties of parametric solutions."],"url":"http://arxiv.org/abs/2405.03796v1","category":"physics.flu-dyn"}
{"created":"2024-05-06 19:04:13","title":"Tensor Network Computations That Capture Strict Variationality, Volume Law Behavior, and the Efficient Representation of Neural Network States","abstract":"We introduce a change of perspective on tensor network states that is defined by the computational graph of the contraction of an amplitude. The resulting class of states, which we refer to as tensor network functions, inherit the conceptual advantages of tensor network states while removing computational restrictions arising from the need to converge approximate contractions. We use tensor network functions to compute strict variational estimates of the energy on loopy graphs, analyze their expressive power for ground-states, show that we can capture aspects of volume law time evolution, and provide a mapping of general feed-forward neural nets onto efficient tensor network functions. Our work expands the realm of computable tensor networks to ones where accurate contraction methods are not available, and opens up new avenues to use tensor networks.","sentences":["We introduce a change of perspective on tensor network states that is defined by the computational graph of the contraction of an amplitude.","The resulting class of states, which we refer to as tensor network functions, inherit the conceptual advantages of tensor network states while removing computational restrictions arising from the need to converge approximate contractions.","We use tensor network functions to compute strict variational estimates of the energy on loopy graphs, analyze their expressive power for ground-states, show that we can capture aspects of volume law time evolution, and provide a mapping of general feed-forward neural nets onto efficient tensor network functions.","Our work expands the realm of computable tensor networks to ones where accurate contraction methods are not available, and opens up new avenues to use tensor networks."],"url":"http://arxiv.org/abs/2405.03797v1","category":"quant-ph"}
{"created":"2024-05-06 18:53:37","title":"Regularity for Fully Nonlinear Elliptic Equations with Natural Growth in Gradient and Singular Nonlinearity","abstract":"In this article we consider the following boundary value problem   \\begin{equation*}\\label{abs} \\left\\{ \\begin{aligned} F(x,u,Du,D^{2}u)+c(x)u+ p(x)u^{-\\alpha}&=0~\\text{in}~\\Omega\\\\ u&=0~~\\text{on}~~\\partial\\Omega, \\end{aligned} \\right. \\end{equation*} where $\\Omega$ is a bounded and $C^{2}$ smooth domain in $\\mathbb{R}^N$ and $F$ has superlinear growth in gradient and $c(c)<-c_{0}$ for some positive constant $c_{0}.$ Here, we studies the boundary behaviour of the solutions to above equation and establishes the global regularity result similar to one established in [12,16] with linear growth in gradient.","sentences":["In this article we consider the following boundary value problem   \\begin{equation*}\\label{abs} \\left\\{ \\begin{aligned} F(x,u,Du,D^{2}u)+c(x)u+ p(x)u^{-\\alpha}&=0~\\text{in}~\\Omega\\\\ u&=0~~\\text{on}~~\\partial\\Omega, \\end{aligned} \\right.","\\end{equation*} where $\\Omega$ is a bounded and $C^{2}$ smooth domain in $\\mathbb{R}^N$ and $F$ has superlinear growth in gradient and $c(c)<-c_{0}$ for some positive constant $c_{0}.$ Here, we studies the boundary behaviour of the solutions to above equation and establishes the global regularity result similar to one established in [12,16] with linear growth in gradient."],"url":"http://arxiv.org/abs/2405.03791v1","category":"math.AP"}
{"created":"2024-05-06 18:37:35","title":"TOGLL: Correct and Strong Test Oracle Generation with LLMs","abstract":"Test oracles play a crucial role in software testing, enabling effective bug detection. Despite initial promise, neural-based methods for automated test oracle generation often result in a large number of false positives and weaker test oracles. While LLMs have demonstrated impressive effectiveness in various software engineering tasks, including code generation, test case creation, and bug fixing, there remains a notable absence of large-scale studies exploring their effectiveness in test oracle generation. The question of whether LLMs can address the challenges in effective oracle generation is both compelling and requires thorough investigation.   In this research, we present the first comprehensive study to investigate the capabilities of LLMs in generating correct, diverse, and strong test oracles capable of effectively identifying a large number of unique bugs. To this end, we fine-tuned seven code LLMs using six distinct prompts on the SF110 dataset. Utilizing the most effective fine-tuned LLM and prompt pair, we introduce TOGLL, a novel LLM-based method for test oracle generation. To investigate the generalizability of TOGLL, we conduct studies on 25 large-scale Java projects. Besides assessing the correctness, we also assess the diversity and strength of the generated oracles. We compare the results against EvoSuite and the state-of-the-art neural method, TOGA. Our findings reveal that TOGLL can produce 3.8 times more correct assertion oracles and 4.9 times more exception oracles. Moreover, our findings demonstrate that TOGLL is capable of generating significantly diverse test oracles. It can detect 1,023 unique bugs that EvoSuite cannot, which is ten times more than what the previous SOTA neural-based method, TOGA, can detect.","sentences":["Test oracles play a crucial role in software testing, enabling effective bug detection.","Despite initial promise, neural-based methods for automated test oracle generation often result in a large number of false positives and weaker test oracles.","While LLMs have demonstrated impressive effectiveness in various software engineering tasks, including code generation, test case creation, and bug fixing, there remains a notable absence of large-scale studies exploring their effectiveness in test oracle generation.","The question of whether LLMs can address the challenges in effective oracle generation is both compelling and requires thorough investigation.   ","In this research, we present the first comprehensive study to investigate the capabilities of LLMs in generating correct, diverse, and strong test oracles capable of effectively identifying a large number of unique bugs.","To this end, we fine-tuned seven code LLMs using six distinct prompts on the SF110 dataset.","Utilizing the most effective fine-tuned LLM and prompt pair, we introduce TOGLL, a novel LLM-based method for test oracle generation.","To investigate the generalizability of TOGLL, we conduct studies on 25 large-scale Java projects.","Besides assessing the correctness, we also assess the diversity and strength of the generated oracles.","We compare the results against EvoSuite and the state-of-the-art neural method, TOGA.","Our findings reveal that TOGLL can produce 3.8 times more correct assertion oracles and 4.9 times more exception oracles.","Moreover, our findings demonstrate that TOGLL is capable of generating significantly diverse test oracles.","It can detect 1,023 unique bugs that EvoSuite cannot, which is ten times more than what the previous SOTA neural-based method, TOGA, can detect."],"url":"http://arxiv.org/abs/2405.03786v1","category":"cs.SE"}
{"created":"2024-05-06 18:00:26","title":"Staggered bosons and Kahler-Dirac bosons","abstract":"We describe a novel way to think about bosonic lattice theories in Hamiltonian form where each lattice site has only a half boson degree of freedom. The construction requires a non-trivial Poisson bracket between neighboring sites and leads to gapless theories with non-invertible symmetries. We also describe a bosonic version of Kahler-Dirac fermions, dubbed Kahler-Dirac bosons that can be performed on any triangulation of a manifold. This also leads to a straightforward implementation of supersymmetry on the lattice and one immediately deduces the Dirac equation of the corresponding Kahler-Dirac fermions.","sentences":["We describe a novel way to think about bosonic lattice theories in Hamiltonian form where each lattice site has only a half boson degree of freedom.","The construction requires a non-trivial Poisson bracket between neighboring sites and leads to gapless theories with non-invertible symmetries.","We also describe a bosonic version of Kahler-Dirac fermions, dubbed Kahler-Dirac bosons that can be performed on any triangulation of a manifold.","This also leads to a straightforward implementation of supersymmetry on the lattice and one immediately deduces the Dirac equation of the corresponding Kahler-Dirac fermions."],"url":"http://arxiv.org/abs/2405.03758v1","category":"hep-th"}
{"created":"2024-05-06 18:00:10","title":"Differential spinors and Kundt three-manifolds with skew-torsion","abstract":"We develop the theory of spinorial polyforms associated with bundles of irreducible Clifford modules of non-simple real type, obtaining a precise characterization of the square of an irreducible real spinor in signature $(p-q) = 1\\,\\mathrm{mod(8)}$ as a polyform belonging to a semi-algebraic real set. We use this formalism to study differential spinors on Lorentzian three-manifolds, proving that in this dimension and signature, every differential spinor is equivalent to an isotropic line preserved in a given direction by a metric connection with prescribed torsion. We apply this result to investigate Lorentzian three-manifolds equipped with a skew-torsion parallel spinor, namely a spinor parallel with respect to a metric connection with totally skew-symmetric torsion. We obtain several structural results about this class of Lorentzian three-manifolds, which are necessarily Kundt, and in the compact case, we obtain an explicit differential condition that guarantees geodesic completeness. We further elaborate on these results to study the supersymmetric solutions of three-dimensional NS-NS supergravity, which involve skew-torsion parallel spinors whose torsion is given by the curvature of a curving on an abelian bundle gerbe. In particular, we obtain a correspondence between NS-NS supersymmetric solutions and null coframes satisfying an explicit exterior differential system that we solve locally.","sentences":["We develop the theory of spinorial polyforms associated with bundles of irreducible Clifford modules of non-simple real type, obtaining a precise characterization of the square of an irreducible real spinor in signature $(p-q) = 1\\,\\mathrm{mod(8)}$ as a polyform belonging to a semi-algebraic real set.","We use this formalism to study differential spinors on Lorentzian three-manifolds, proving that in this dimension and signature, every differential spinor is equivalent to an isotropic line preserved in a given direction by a metric connection with prescribed torsion.","We apply this result to investigate Lorentzian three-manifolds equipped with a skew-torsion parallel spinor, namely a spinor parallel with respect to a metric connection with totally skew-symmetric torsion.","We obtain several structural results about this class of Lorentzian three-manifolds, which are necessarily Kundt, and in the compact case, we obtain an explicit differential condition that guarantees geodesic completeness.","We further elaborate on these results to study the supersymmetric solutions of three-dimensional NS-NS supergravity, which involve skew-torsion parallel spinors whose torsion is given by the curvature of a curving on an abelian bundle gerbe.","In particular, we obtain a correspondence between NS-NS supersymmetric solutions and null coframes satisfying an explicit exterior differential system that we solve locally."],"url":"http://arxiv.org/abs/2405.03756v1","category":"math.DG"}
{"created":"2024-05-06 17:54:54","title":"AtomGPT: Atomistic Generative Pre-trained Transformer for Forward and Inverse Materials Design","abstract":"Large language models (LLMs) such as generative pretrained transformers (GPTs) have shown potential for various commercial applications, but their applicability for materials design remains underexplored. In this article, we introduce AtomGPT, a model specifically developed for materials design based on transformer architectures, to demonstrate the capability for both atomistic property prediction and structure generation. We show that a combination of chemical and structural text descriptions can efficiently predict material properties with accuracy comparable to graph neural network models, including formation energies, electronic bandgaps from two different methods and superconducting transition temperatures. Furthermore, we demonstrate that AtomGPT can generate atomic structures for tasks such as designing new superconductors, with the predictions validated through density functional theory calculations. This work paves the way for leveraging LLMs in forward and inverse materials design, offering an efficient approach to the discovery and optimization of materials.","sentences":["Large language models (LLMs) such as generative pretrained transformers (GPTs) have shown potential for various commercial applications, but their applicability for materials design remains underexplored.","In this article, we introduce AtomGPT, a model specifically developed for materials design based on transformer architectures, to demonstrate the capability for both atomistic property prediction and structure generation.","We show that a combination of chemical and structural text descriptions can efficiently predict material properties with accuracy comparable to graph neural network models, including formation energies, electronic bandgaps from two different methods and superconducting transition temperatures.","Furthermore, we demonstrate that AtomGPT can generate atomic structures for tasks such as designing new superconductors, with the predictions validated through density functional theory calculations.","This work paves the way for leveraging LLMs in forward and inverse materials design, offering an efficient approach to the discovery and optimization of materials."],"url":"http://arxiv.org/abs/2405.03680v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 17:39:48","title":"Comments on the article \"M.O. Katanaev, Complete separation of variables in the geodesic Hamilton-Jacobi equation in four dimensions. Physica Scripta (2023), 98, 104001\" by V. V. Obukhov, K. E. Osetrin, and A. V. Shapovalov","abstract":"This is a detailed answer to the criticism of my paper.","sentences":["This is a detailed answer to the criticism of my paper."],"url":"http://arxiv.org/abs/2405.03737v1","category":"gr-qc"}
{"created":"2024-05-06 17:36:44","title":"A Construct-Optimize Approach to Sparse View Synthesis without Camera Pose","abstract":"Novel view synthesis from a sparse set of input images is a challenging problem of great practical interest, especially when camera poses are absent or inaccurate. Direct optimization of camera poses and usage of estimated depths in neural radiance field algorithms usually do not produce good results because of the coupling between poses and depths, and inaccuracies in monocular depth estimation. In this paper, we leverage the recent 3D Gaussian splatting method to develop a novel construct-and-optimize method for sparse view synthesis without camera poses. Specifically, we construct a solution progressively by using monocular depth and projecting pixels back into the 3D world. During construction, we optimize the solution by detecting 2D correspondences between training views and the corresponding rendered images. We develop a unified differentiable pipeline for camera registration and adjustment of both camera poses and depths, followed by back-projection. We also introduce a novel notion of an expected surface in Gaussian splatting, which is critical to our optimization. These steps enable a coarse solution, which can then be low-pass filtered and refined using standard optimization methods. We demonstrate results on the Tanks and Temples and Static Hikes datasets with as few as three widely-spaced views, showing significantly better quality than competing methods, including those with approximate camera pose information. Moreover, our results improve with more views and outperform previous InstantNGP and Gaussian Splatting algorithms even when using half the dataset.","sentences":["Novel view synthesis from a sparse set of input images is a challenging problem of great practical interest, especially when camera poses are absent or inaccurate.","Direct optimization of camera poses and usage of estimated depths in neural radiance field algorithms usually do not produce good results because of the coupling between poses and depths, and inaccuracies in monocular depth estimation.","In this paper, we leverage the recent 3D Gaussian splatting method to develop a novel construct-and-optimize method for sparse view synthesis without camera poses.","Specifically, we construct a solution progressively by using monocular depth and projecting pixels back into the 3D world.","During construction, we optimize the solution by detecting 2D correspondences between training views and the corresponding rendered images.","We develop a unified differentiable pipeline for camera registration and adjustment of both camera poses and depths, followed by back-projection.","We also introduce a novel notion of an expected surface in Gaussian splatting, which is critical to our optimization.","These steps enable a coarse solution, which can then be low-pass filtered and refined using standard optimization methods.","We demonstrate results on the Tanks and Temples and Static Hikes datasets with as few as three widely-spaced views, showing significantly better quality than competing methods, including those with approximate camera pose information.","Moreover, our results improve with more views and outperform previous InstantNGP and Gaussian Splatting algorithms even when using half the dataset."],"url":"http://arxiv.org/abs/2405.03659v1","category":"cs.CV"}
{"created":"2024-05-06 17:26:30","title":"Stability of backward-in-time semilinear coupled parabolic systems","abstract":"We consider backward problems for semilinear coupled parabolic systems in bounded domains. We prove conditional stability estimates for linear and semilinear systems of strongly coupled parabolic equations involving general semilinearities. The proof of the stability estimates relies on a modified method by Carleman estimates incorporating the simple weight function $e^{\\lambda t}$ with a sufficiently large parameter $\\lambda$.","sentences":["We consider backward problems for semilinear coupled parabolic systems in bounded domains.","We prove conditional stability estimates for linear and semilinear systems of strongly coupled parabolic equations involving general semilinearities.","The proof of the stability estimates relies on a modified method by Carleman estimates incorporating the simple weight function $e^{\\lambda t}$ with a sufficiently large parameter $\\lambda$."],"url":"http://arxiv.org/abs/2405.03653v1","category":"math.AP"}
{"created":"2024-05-06 17:12:21","title":"Learning Robust Classifiers with Self-Guided Spurious Correlation Mitigation","abstract":"Deep neural classifiers tend to rely on spurious correlations between spurious attributes of inputs and targets to make predictions, which could jeopardize their generalization capability. Training classifiers robust to spurious correlations typically relies on annotations of spurious correlations in data, which are often expensive to get. In this paper, we tackle an annotation-free setting and propose a self-guided spurious correlation mitigation framework. Our framework automatically constructs fine-grained training labels tailored for a classifier obtained with empirical risk minimization to improve its robustness against spurious correlations. The fine-grained training labels are formulated with different prediction behaviors of the classifier identified in a novel spuriousness embedding space. We construct the space with automatically detected conceptual attributes and a novel spuriousness metric which measures how likely a class-attribute correlation is exploited for predictions. We demonstrate that training the classifier to distinguish different prediction behaviors reduces its reliance on spurious correlations without knowing them a priori and outperforms prior methods on five real-world datasets.","sentences":["Deep neural classifiers tend to rely on spurious correlations between spurious attributes of inputs and targets to make predictions, which could jeopardize their generalization capability.","Training classifiers robust to spurious correlations typically relies on annotations of spurious correlations in data, which are often expensive to get.","In this paper, we tackle an annotation-free setting and propose a self-guided spurious correlation mitigation framework.","Our framework automatically constructs fine-grained training labels tailored for a classifier obtained with empirical risk minimization to improve its robustness against spurious correlations.","The fine-grained training labels are formulated with different prediction behaviors of the classifier identified in a novel spuriousness embedding space.","We construct the space with automatically detected conceptual attributes and a novel spuriousness metric which measures how likely a class-attribute correlation is exploited for predictions.","We demonstrate that training the classifier to distinguish different prediction behaviors reduces its reliance on spurious correlations without knowing them a priori and outperforms prior methods on five real-world datasets."],"url":"http://arxiv.org/abs/2405.03649v1","category":"cs.LG"}
{"created":"2024-05-06 17:06:11","title":"Classification of Breast Cancer Histopathology Images using a Modified Supervised Contrastive Learning Method","abstract":"Deep neural networks have reached remarkable achievements in medical image processing tasks, specifically classifying and detecting various diseases. However, when confronted with limited data, these networks face a critical vulnerability, often succumbing to overfitting by excessively memorizing the limited information available. This work addresses the challenge mentioned above by improving the supervised contrastive learning method to reduce the impact of false positives. Unlike most existing methods that rely predominantly on fully supervised learning, our approach leverages the advantages of self-supervised learning in conjunction with employing the available labeled data. We evaluate our method on the BreakHis dataset, which consists of breast cancer histopathology images, and demonstrate an increase in classification accuracy by 1.45% at the image level and 1.42% at the patient level compared to the state-of-the-art method. This improvement corresponds to 93.63% absolute accuracy, highlighting our approach's effectiveness in leveraging data properties to learn more appropriate representation space.","sentences":["Deep neural networks have reached remarkable achievements in medical image processing tasks, specifically classifying and detecting various diseases.","However, when confronted with limited data, these networks face a critical vulnerability, often succumbing to overfitting by excessively memorizing the limited information available.","This work addresses the challenge mentioned above by improving the supervised contrastive learning method to reduce the impact of false positives.","Unlike most existing methods that rely predominantly on fully supervised learning, our approach leverages the advantages of self-supervised learning in conjunction with employing the available labeled data.","We evaluate our method on the BreakHis dataset, which consists of breast cancer histopathology images, and demonstrate an increase in classification accuracy by 1.45% at the image level and 1.42% at the patient level compared to the state-of-the-art method.","This improvement corresponds to 93.63% absolute accuracy, highlighting our approach's effectiveness in leveraging data properties to learn more appropriate representation space."],"url":"http://arxiv.org/abs/2405.03642v1","category":"cs.CV"}
{"created":"2024-05-06 16:55:30","title":"Collage: Light-Weight Low-Precision Strategy for LLM Training","abstract":"Large models training is plagued by the intense compute cost and limited hardware memory. A practical solution is low-precision representation but is troubled by loss in numerical accuracy and unstable training rendering the model less useful. We argue that low-precision floating points can perform well provided the error is properly compensated at the critical locations in the training process. We propose Collage which utilizes multi-component float representation in low-precision to accurately perform operations with numerical errors accounted. To understand the impact of imprecision to training, we propose a simple and novel metric which tracks the lost information during training as well as differentiates various precision strategies. Our method works with commonly used low-precision such as half-precision ($16$-bit floating points) and can be naturally extended to work with even lower precision such as $8$-bit. Experimental results show that pre-training using Collage removes the requirement of using $32$-bit floating-point copies of the model and attains similar/better training performance compared to $(16, 32)$-bit mixed-precision strategy, with up to $3.7\\times$ speedup and $\\sim 15\\%$ to $23\\%$ less memory usage in practice.","sentences":["Large models training is plagued by the intense compute cost and limited hardware memory.","A practical solution is low-precision representation but is troubled by loss in numerical accuracy and unstable training rendering the model less useful.","We argue that low-precision floating points can perform well provided the error is properly compensated at the critical locations in the training process.","We propose Collage which utilizes multi-component float representation in low-precision to accurately perform operations with numerical errors accounted.","To understand the impact of imprecision to training, we propose a simple and novel metric which tracks the lost information during training as well as differentiates various precision strategies.","Our method works with commonly used low-precision such as half-precision ($16$-bit floating points) and can be naturally extended to work with even lower precision such as $8$-bit.","Experimental results show that pre-training using Collage removes the requirement of using $32$-bit floating-point copies of the model and attains similar/better training performance compared to $(16, 32)$-bit mixed-precision strategy, with up to $3.7\\times$ speedup and $\\sim 15\\%$ to $23\\%$ less memory usage in practice."],"url":"http://arxiv.org/abs/2405.03637v1","category":"cs.LG"}
{"created":"2024-05-06 16:50:42","title":"Neural Graph Mapping for Dense SLAM with Efficient Loop Closure","abstract":"Existing neural field-based SLAM methods typically employ a single monolithic field as their scene representation. This prevents efficient incorporation of loop closure constraints and limits scalability. To address these shortcomings, we propose a neural mapping framework which anchors lightweight neural fields to the pose graph of a sparse visual SLAM system. Our approach shows the ability to integrate large-scale loop closures, while limiting necessary reintegration. Furthermore, we verify the scalability of our approach by demonstrating successful building-scale mapping taking multiple loop closures into account during the optimization, and show that our method outperforms existing state-of-the-art approaches on large scenes in terms of quality and runtime. Our code is available at https://kth-rpl.github.io/neural_graph_mapping/.","sentences":["Existing neural field-based SLAM methods typically employ a single monolithic field as their scene representation.","This prevents efficient incorporation of loop closure constraints and limits scalability.","To address these shortcomings, we propose a neural mapping framework which anchors lightweight neural fields to the pose graph of a sparse visual SLAM system.","Our approach shows the ability to integrate large-scale loop closures, while limiting necessary reintegration.","Furthermore, we verify the scalability of our approach by demonstrating successful building-scale mapping taking multiple loop closures into account during the optimization, and show that our method outperforms existing state-of-the-art approaches on large scenes in terms of quality and runtime.","Our code is available at https://kth-rpl.github.io/neural_graph_mapping/."],"url":"http://arxiv.org/abs/2405.03633v1","category":"cs.CV"}
{"created":"2024-05-06 16:41:13","title":"On Reference Frames and Coordinate Transformations","abstract":"This article explores the differences between frame and coordinate transformations in relativistic theories. We highlight the key role of tetrad fields in connecting spacetime and frame indices. Using Maxwell's electrodynamics as an example, we show that Maxwell's equations are invariant under coordinate transformations but exhibit covariant behavior under frame transformations. We also analyze the energy-momentum of an electromagnetic field in different frames, providing deeper insights into the implications of different frames of reference and coordinate systems.","sentences":["This article explores the differences between frame and coordinate transformations in relativistic theories.","We highlight the key role of tetrad fields in connecting spacetime and frame indices.","Using Maxwell's electrodynamics as an example, we show that Maxwell's equations are invariant under coordinate transformations but exhibit covariant behavior under frame transformations.","We also analyze the energy-momentum of an electromagnetic field in different frames, providing deeper insights into the implications of different frames of reference and coordinate systems."],"url":"http://arxiv.org/abs/2405.03623v1","category":"gr-qc"}
{"created":"2024-05-06 16:34:24","title":"A reduction procedure for determining exact solutions of second order hyperbolic equations","abstract":"In this paper we develop a systematic reduction procedure for determining intermediate integrals of second order hyperbolic equations so that exact solutions of the second order PDEs under interest can be obtained by solving first order PDEs. We give some conditions in order that such a procedure holds and, in particular, we characterize classes of linear second order hyperbolic equations for which the general solution can be found.","sentences":["In this paper we develop a systematic reduction procedure for determining intermediate integrals of second order hyperbolic equations so that exact solutions of the second order PDEs under interest can be obtained by solving first order PDEs.","We give some conditions in order that such a procedure holds and, in particular, we characterize classes of linear second order hyperbolic equations for which the general solution can be found."],"url":"http://arxiv.org/abs/2405.03617v1","category":"math-ph"}
{"created":"2024-05-06 16:30:27","title":"Network analysis for the steady-state thermodynamic uncertainty relation","abstract":"We perform network analysis of a system described by the master equation to estimate the lower bound of the steady-state current noise, starting from the level 2.5 large deviation function and using the graph theory approach. When the transition rates are uniform, and the system is driven to a non-equilibrium steady state by unidirectional transitions, we derive a noise lower bound, which accounts for fluctuations of sojourn times at all states and is expressed using mesh currents. This bound is applied to the uncertainty in the signal-to-noise ratio of the fluctuating computation time of a schematic Brownian computation plus reset process described by a graph containing one cycle. Unlike the mixed and pseudo-entropy bounds that increase logarithmically with the length of the intended computation path, this bound depends on the number of extraneous predecessors and thus captures the logical irreversibility.","sentences":["We perform network analysis of a system described by the master equation to estimate the lower bound of the steady-state current noise, starting from the level 2.5 large deviation function and using the graph theory approach.","When the transition rates are uniform, and the system is driven to a non-equilibrium steady state by unidirectional transitions, we derive a noise lower bound, which accounts for fluctuations of sojourn times at all states and is expressed using mesh currents.","This bound is applied to the uncertainty in the signal-to-noise ratio of the fluctuating computation time of a schematic Brownian computation plus reset process described by a graph containing one cycle.","Unlike the mixed and pseudo-entropy bounds that increase logarithmically with the length of the intended computation path, this bound depends on the number of extraneous predecessors and thus captures the logical irreversibility."],"url":"http://arxiv.org/abs/2405.03611v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-06 16:17:48","title":"Strang Splitting for Parametric Inference in Second-order Stochastic Differential Equations","abstract":"We address parameter estimation in second-order stochastic differential equations (SDEs), prevalent in physics, biology, and ecology. Second-order SDE is converted to a first-order system by introducing an auxiliary velocity variable raising two main challenges. First, the system is hypoelliptic since the noise affects only the velocity, making the Euler-Maruyama estimator ill-conditioned. To overcome that, we propose an estimator based on the Strang splitting scheme. Second, since the velocity is rarely observed we adjust the estimator for partial observations. We present four estimators for complete and partial observations, using full likelihood or only velocity marginal likelihood. These estimators are intuitive, easy to implement, and computationally fast, and we prove their consistency and asymptotic normality. Our analysis demonstrates that using full likelihood with complete observations reduces the asymptotic variance of the diffusion estimator. With partial observations, the asymptotic variance increases due to information loss but remains unaffected by the likelihood choice. However, a numerical study on the Kramers oscillator reveals that using marginal likelihood for partial observations yields less biased estimators. We apply our approach to paleoclimate data from the Greenland ice core and fit it to the Kramers oscillator model, capturing transitions between metastable states reflecting observed climatic conditions during glacial eras.","sentences":["We address parameter estimation in second-order stochastic differential equations (SDEs), prevalent in physics, biology, and ecology.","Second-order SDE is converted to a first-order system by introducing an auxiliary velocity variable raising two main challenges.","First, the system is hypoelliptic since the noise affects only the velocity, making the Euler-Maruyama estimator ill-conditioned.","To overcome that, we propose an estimator based on the Strang splitting scheme.","Second, since the velocity is rarely observed we adjust the estimator for partial observations.","We present four estimators for complete and partial observations, using full likelihood or only velocity marginal likelihood.","These estimators are intuitive, easy to implement, and computationally fast, and we prove their consistency and asymptotic normality.","Our analysis demonstrates that using full likelihood with complete observations reduces the asymptotic variance of the diffusion estimator.","With partial observations, the asymptotic variance increases due to information loss but remains unaffected by the likelihood choice.","However, a numerical study on the Kramers oscillator reveals that using marginal likelihood for partial observations yields less biased estimators.","We apply our approach to paleoclimate data from the Greenland ice core and fit it to the Kramers oscillator model, capturing transitions between metastable states reflecting observed climatic conditions during glacial eras."],"url":"http://arxiv.org/abs/2405.03606v1","category":"stat.ME"}
{"created":"2024-05-06 16:14:59","title":"One nose but two nostrils: Learn to align with sparse connections between two olfactory cortices","abstract":"The integration of neural representations in the two hemispheres is an important problem in neuroscience. Recent experiments revealed that odor responses in cortical neurons driven by separate stimulation of the two nostrils are highly correlated. This bilateral alignment points to structured inter-hemispheric connections, but detailed mechanism remains unclear. Here, we hypothesized that continuous exposure to environmental odors shapes these projections and modeled it as online learning with local Hebbian rule. We found that Hebbian learning with sparse connections achieves bilateral alignment, exhibiting a linear trade-off between speed and accuracy. We identified an inverse scaling relationship between the number of cortical neurons and the inter-hemispheric projection density required for desired alignment accuracy, i.e., more cortical neurons allow sparser inter-hemispheric projections. We next compared the alignment performance of local Hebbian rule and the global stochastic-gradient-descent (SGD) learning for artificial neural networks. We found that although SGD leads to the same alignment accuracy with modestly sparser connectivity, the same inverse scaling relation holds. We showed that their similar performance originates from the fact that the update vectors of the two learning rules align significantly throughout the learning process. This insight may inspire efficient sparse local learning algorithms for more complex problems.","sentences":["The integration of neural representations in the two hemispheres is an important problem in neuroscience.","Recent experiments revealed that odor responses in cortical neurons driven by separate stimulation of the two nostrils are highly correlated.","This bilateral alignment points to structured inter-hemispheric connections, but detailed mechanism remains unclear.","Here, we hypothesized that continuous exposure to environmental odors shapes these projections and modeled it as online learning with local Hebbian rule.","We found that Hebbian learning with sparse connections achieves bilateral alignment, exhibiting a linear trade-off between speed and accuracy.","We identified an inverse scaling relationship between the number of cortical neurons and the inter-hemispheric projection density required for desired alignment accuracy, i.e., more cortical neurons allow sparser inter-hemispheric projections.","We next compared the alignment performance of local Hebbian rule and the global stochastic-gradient-descent (SGD) learning for artificial neural networks.","We found that although SGD leads to the same alignment accuracy with modestly sparser connectivity, the same inverse scaling relation holds.","We showed that their similar performance originates from the fact that the update vectors of the two learning rules align significantly throughout the learning process.","This insight may inspire efficient sparse local learning algorithms for more complex problems."],"url":"http://arxiv.org/abs/2405.03602v1","category":"q-bio.NC"}
{"created":"2024-05-06 16:05:12","title":"Towards Utilizing Scanning Gate Microscopy as a High-Resolution Probe of Valley Splitting in Si/SiGe Heterostructures","abstract":"A detailed understanding of the material properties that affect the splitting between the two low-lying valley states in Si/SiGe heterostructures will be increasingly important as the number of spin qubits is increased. Scanning gate microscopy has been proposed as a method to measure the spatial variation of the valley splitting as a tip-induced dot is moved around in the plane of the Si quantum well. We develop a simulation using an electrostatic model of the scanning gate microscope tip and the overlapping gate structure combined with an approximate solution to the three-dimensional Schr\\\"odinger-Poisson equation in the device stack. Using this simulation, we show that a tip-induced quantum dot formed near source and drain electrodes can be adiabatically moved to a region far from the gate electrodes. We argue that by spatially translating the tip-induced dot across a defect in the Si/SiGe interface, changes in valley splitting can be detected.","sentences":["A detailed understanding of the material properties that affect the splitting between the two low-lying valley states in Si/SiGe heterostructures will be increasingly important as the number of spin qubits is increased.","Scanning gate microscopy has been proposed as a method to measure the spatial variation of the valley splitting as a tip-induced dot is moved around in the plane of the Si quantum well.","We develop a simulation using an electrostatic model of the scanning gate microscope tip and the overlapping gate structure combined with an approximate solution to the three-dimensional Schr\\\"odinger-Poisson equation in the device stack.","Using this simulation, we show that a tip-induced quantum dot formed near source and drain electrodes can be adiabatically moved to a region far from the gate electrodes.","We argue that by spatially translating the tip-induced dot across a defect in the Si/SiGe interface, changes in valley splitting can be detected."],"url":"http://arxiv.org/abs/2405.03596v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-06 16:02:56","title":"Rectifiable Reifenberg and uniform positivity under almost calibrations","abstract":"The Reifenberg theorem \\cite{reif_orig} tells us that if a set $S\\subseteq B_2\\subseteq \\mathbb R^n$ is uniformly close on all points and scales to a $k$-dimensional subspace, then $S$ is H\\\"older homeomorphic to a $k$-dimensional Euclidean ball. In general this is sharp, for instance such an $S$ may have infinite volume, be fractal in nature, and have no rectifiable structure.   The goal of this note is to show that we can improve upon this for an almost calibrated Reifenberg set, or more generally under a positivity condition in the context of an $\\epsilon$-calibration $\\Omega$ . An $\\epsilon$-calibration is very general, the condition holds locally for all continuous $k$-forms such that $\\Omega[L]\\leq 1+\\epsilon$ for all $k$-planes $L$. We say an oriented $k$-plane $L$ is $\\alpha$-positive with respect to $\\Omega$ if $\\Omega[L]>\\alpha>0$. If $\\Omega[L]>\\alpha> 1-\\epsilon$ then we call $L$ an $\\epsilon$-calibrated plane.   The main result of this paper is then the following. Assume at all points and scales $B_r(x)\\subseteq B_2$ that $S$ is $\\delta$-Hausdorff close to a subspace $L_{x,r}$ which is uniformly positive $\\Omega[L_{x,r}]>\\alpha $ with respect to an $\\epsilon$-calibration. Then $S$ is $k$-rectifiable with uniform volume bounds.","sentences":["The Reifenberg theorem \\cite{reif_orig} tells us that if a set $S\\subseteq B_2\\subseteq \\mathbb R^n$ is uniformly close on all points and scales to a $k$-dimensional subspace, then $S$ is H\\\"older homeomorphic to a $k$-dimensional Euclidean ball.","In general this is sharp, for instance such an $S$ may have infinite volume, be fractal in nature, and have no rectifiable structure.   ","The goal of this note is to show that we can improve upon this for an almost calibrated Reifenberg set, or more generally under a positivity condition in the context of an $\\epsilon$-calibration $\\Omega$ .","An $\\epsilon$-calibration is very general, the condition holds locally for all continuous $k$-forms such that $\\Omega[L]\\leq 1+\\epsilon$ for all $k$-planes $L$. We say an oriented $k$-plane $L$ is $\\alpha$-positive with respect to $\\Omega$ if $\\Omega[L]>\\alpha>0$. If $\\Omega[L]>\\alpha> 1-\\epsilon$ then we call $L$ an $\\epsilon$-calibrated plane.   ","The main result of this paper is then the following.","Assume at all points and scales $B_r(x)\\subseteq B_2$ that $S$ is $\\delta$-Hausdorff close to a subspace $L_{x,r}$ which is uniformly positive $\\Omega[L_{x,r}]>\\alpha $ with respect to an $\\epsilon$-calibration.","Then $S$ is $k$-rectifiable with uniform volume bounds."],"url":"http://arxiv.org/abs/2405.03593v1","category":"math.AP"}
{"created":"2024-05-06 16:02:12","title":"Emergence of Cosmic Space and Horizon Thermodynamics from Kaniadakis Entropy","abstract":"Utilizing Kaniadakis entropy associated with the apparent horizon of the Friedmann-Robertson-Walker (FRW) Universe and applying the emergence of cosmic space paradigm, we deduce the modified Friedmann equation for a non-flat (n+1)-dimensional universe. Employing the first law of thermodynamics, we arrive at the same modified Friedmann equation, showing the connection between emergence of cosmic space and first law of thermodynamics. We also establish the condition to satisfy the Generalized second law of thermodynamics within the Kaniadakis framework. Our study illuminates the intricate connection between the law of emergence and horizon thermodynamics, offering a deeper insight through the lens of Kaniadakis entropy.","sentences":["Utilizing Kaniadakis entropy associated with the apparent horizon of the Friedmann-Robertson-Walker (FRW) Universe and applying the emergence of cosmic space paradigm, we deduce the modified Friedmann equation for a non-flat (n+1)-dimensional universe.","Employing the first law of thermodynamics, we arrive at the same modified Friedmann equation, showing the connection between emergence of cosmic space and first law of thermodynamics.","We also establish the condition to satisfy the Generalized second law of thermodynamics within the Kaniadakis framework.","Our study illuminates the intricate connection between the law of emergence and horizon thermodynamics, offering a deeper insight through the lens of Kaniadakis entropy."],"url":"http://arxiv.org/abs/2405.03592v2","category":"gr-qc"}
{"created":"2024-05-06 16:01:39","title":"Constrained inhomogeneous spherical equations: average-case hardness","abstract":"In this paper we analyze computational properties of the Diophantine problem (and its search variant) for spherical equations $\\prod_{i=1}^m z_i^{-1} c_i z_i = 1$ (and its variants) over the class of finite metabelian groups $G_{p,n}=\\mathbb{Z}_p^n \\rtimes \\mathbb{Z}_p^\\ast$, where $n\\in\\mathbb{N}$ and $p$ is prime. We prove that the problem of finding solutions for certain constrained spherical equations is computationally hard on average (assuming that some lattice approximation problem is hard in the worst case).","sentences":["In this paper we analyze computational properties of the Diophantine problem (and its search variant) for spherical equations $\\prod_{i=1}^m z_i^{-1} c_i z_i","= 1$ (and its variants) over the class of finite metabelian groups $G_{p,n}=\\mathbb{Z}_p^n \\rtimes \\mathbb{Z}_p^\\ast$, where $n\\in\\mathbb{N}$ and $p$ is prime.","We prove that the problem of finding solutions for certain constrained spherical equations is computationally hard on average (assuming that some lattice approximation problem is hard in the worst case)."],"url":"http://arxiv.org/abs/2405.03591v1","category":"math.GR"}
{"created":"2024-05-06 15:53:55","title":"Functional Latent Dynamics for Irregularly Sampled Time Series Forecasting","abstract":"Irregularly sampled time series with missing values are often observed in multiple real-world applications such as healthcare, climate and astronomy. They pose a significant challenge to standard deep learn- ing models that operate only on fully observed and regularly sampled time series. In order to capture the continuous dynamics of the irreg- ular time series, many models rely on solving an Ordinary Differential Equation (ODE) in the hidden state. These ODE-based models tend to perform slow and require large memory due to sequential operations and a complex ODE solver. As an alternative to complex ODE-based mod- els, we propose a family of models called Functional Latent Dynamics (FLD). Instead of solving the ODE, we use simple curves which exist at all time points to specify the continuous latent state in the model. The coefficients of these curves are learned only from the observed values in the time series ignoring the missing values. Through extensive experi- ments, we demonstrate that FLD achieves better performance compared to the best ODE-based model while reducing the runtime and memory overhead. Specifically, FLD requires an order of magnitude less time to infer the forecasts compared to the best performing forecasting model.","sentences":["Irregularly sampled time series with missing values are often observed in multiple real-world applications such as healthcare, climate and astronomy.","They pose a significant challenge to standard deep learn- ing models that operate only on fully observed and regularly sampled time series.","In order to capture the continuous dynamics of the irreg- ular time series, many models rely on solving an Ordinary Differential Equation (ODE) in the hidden state.","These ODE-based models tend to perform slow and require large memory due to sequential operations and a complex ODE solver.","As an alternative to complex ODE-based mod- els, we propose a family of models called Functional Latent Dynamics (FLD).","Instead of solving the ODE, we use simple curves which exist at all time points to specify the continuous latent state in the model.","The coefficients of these curves are learned only from the observed values in the time series ignoring the missing values.","Through extensive experi- ments, we demonstrate that FLD achieves better performance compared to the best ODE-based model while reducing the runtime and memory overhead.","Specifically, FLD requires an order of magnitude less time to infer the forecasts compared to the best performing forecasting model."],"url":"http://arxiv.org/abs/2405.03582v1","category":"cs.LG"}
{"created":"2024-05-06 15:53:41","title":"$D^0$ meson production in $pp$ collisions at large $Q_s^2$","abstract":"The impact of the non-linear effects in the QCD dynamics on the observables is directly related to the magnitude of the saturation scale $Q_s$, which is predicted to increase with the energy, rapidity and multiplicity. In this paper, we investigate the $D^0$ meson production in $pp$ collisions at forward rapidities and/or high multiplicities considering the Color Glass Condensate (CGC) formalism and the solutions of the running coupling Balitsky - Kovchegov (BK) equation. The contributions of gluon - and charm - initiated processes are taken into account, and a comparison with the current LHCb data is performed. The impact of an intrinsic charm component in the proton's wave function is also estimated. Predictions for the self-normalized yields of $D^0$ mesons as a function of the multiplicity of coproduced charged hadrons are presented, considering $pp$ collisions at $\\sqrt{s} = 13$ TeV and different values of the meson rapidity. A comparison with the predictions for the kaon and isolated photon production is performed. Our results indicate that a future experimental analysis of the $D^0$ meson production at forward rapidities and high multiplicities can be useful to probe the CGC formalism and to disentangle the contribution of initial - and final - state effects.","sentences":["The impact of the non-linear effects in the QCD dynamics on the observables is directly related to the magnitude of the saturation scale $Q_s$, which is predicted to increase with the energy, rapidity and multiplicity.","In this paper, we investigate the $D^0$ meson production in $pp$ collisions at forward rapidities and/or high multiplicities considering the Color Glass Condensate (CGC) formalism and the solutions of the running coupling Balitsky - Kovchegov (BK) equation.","The contributions of gluon - and charm - initiated processes are taken into account, and a comparison with the current LHCb data is performed.","The impact of an intrinsic charm component in the proton's wave function is also estimated.","Predictions for the self-normalized yields of $D^0$ mesons as a function of the multiplicity of coproduced charged hadrons are presented, considering $pp$ collisions at $\\sqrt{s} = 13$ TeV and different values of the meson rapidity.","A comparison with the predictions for the kaon and isolated photon production is performed.","Our results indicate that a future experimental analysis of the $D^0$ meson production at forward rapidities and high multiplicities can be useful to probe the CGC formalism and to disentangle the contribution of initial - and final - state effects."],"url":"http://arxiv.org/abs/2405.03581v1","category":"hep-ph"}
{"created":"2024-05-06 15:32:09","title":"Model- and Data-Based Control of Self-Balancing Robots: Practical Educational Approach with LabVIEW and Arduino","abstract":"A two-wheeled self-balancing robot (TWSBR) is non-linear and unstable system. This study compares the performance of model-based and data-based control strategies for TWSBRs, with an explicit practical educational approach. Model-based control (MBC) algorithms such as Lead-Lag and PID control require a proficient dynamic modeling and mathematical manipulation to drive the linearized equations of motions and develop the appropriate controller. On the other side, data-based control (DBC) methods, like fuzzy control, provide a simpler and quicker approach to designing effective controllers without needing in-depth understanding of the system model. In this paper, the advantages and disadvantages of both MBC and DBC using a TWSBR are illustrated. All controllers were implemented and tested on the OSOYOO self-balancing kit, including an Arduino microcontroller, MPU-6050 sensor, and DC motors. The control law and the user interface are constructed using the LabVIEW-LINX toolkit. A real-time hardware-in-loop experiment validates the results, highlighting controllers that can be implemented on a cost-effective platform.","sentences":["A two-wheeled self-balancing robot (TWSBR) is non-linear and unstable system.","This study compares the performance of model-based and data-based control strategies for TWSBRs, with an explicit practical educational approach.","Model-based control (MBC) algorithms such as Lead-Lag and PID control require a proficient dynamic modeling and mathematical manipulation to drive the linearized equations of motions and develop the appropriate controller.","On the other side, data-based control (DBC) methods, like fuzzy control, provide a simpler and quicker approach to designing effective controllers without needing in-depth understanding of the system model.","In this paper, the advantages and disadvantages of both MBC and DBC using a TWSBR are illustrated.","All controllers were implemented and tested on the OSOYOO self-balancing kit, including an Arduino microcontroller, MPU-6050 sensor, and DC motors.","The control law and the user interface are constructed using the LabVIEW-LINX toolkit.","A real-time hardware-in-loop experiment validates the results, highlighting controllers that can be implemented on a cost-effective platform."],"url":"http://arxiv.org/abs/2405.03561v1","category":"cs.RO"}
{"created":"2024-05-06 15:06:02","title":"Axiomatizing the Logic of Ordinary Discourse","abstract":"Most non-classical logics are subclassical, that is, every inference/theorem they validate is also valid classically. A notable exception is the three-valued propositional Logic of Ordinary Discourse (OL) proposed and extensively motivated by W. S. Cooper as a more adequate candidate for formalizing everyday reasoning (in English). OL challenges classical logic not only by rejecting some theses, but also by accepting non-classically valid principles, such as so-called Aristotle's and Boethius' theses. Formally, OL shows a number of unusual features - it is non-structural, connexive, paraconsistent and contradictory - making it all the more interesting for the mathematical logician. We present our recent findings on OL and its structural companion (that we call sOL). We introduce Hilbert-style multiple-conclusion calculi for OL and sOL that are both modular and analytic, and easily allow us to obtain single-conclusion axiomatizations. We prove that sOL is algebraizable and single out its equivalent semantics, which turns out to be a discriminator variety generated by a three-element algebra. Having observed that sOL can express the connectives of other three-valued logics, we prove that it is definitionally equivalent to an expansion of the three-valued logic J3 of D'Ottaviano and da Costa, itself an axiomatic extension of paraconsistent Nelson logic.","sentences":["Most non-classical logics are subclassical, that is, every inference/theorem they validate is also valid classically.","A notable exception is the three-valued propositional Logic of Ordinary Discourse (OL) proposed and extensively motivated by W. S. Cooper as a more adequate candidate for formalizing everyday reasoning (in English).","OL challenges classical logic not only by rejecting some theses, but also by accepting non-classically valid principles, such as so-called Aristotle's and Boethius' theses.","Formally, OL shows a number of unusual features - it is non-structural, connexive, paraconsistent and contradictory - making it all the more interesting for the mathematical logician.","We present our recent findings on OL and its structural companion (that we call sOL).","We introduce Hilbert-style multiple-conclusion calculi for OL and sOL that are both modular and analytic, and easily allow us to obtain single-conclusion axiomatizations.","We prove that sOL is algebraizable and single out its equivalent semantics, which turns out to be a discriminator variety generated by a three-element algebra.","Having observed that sOL can express the connectives of other three-valued logics, we prove that it is definitionally equivalent to an expansion of the three-valued logic J3 of D'Ottaviano and da Costa, itself an axiomatic extension of paraconsistent Nelson logic."],"url":"http://arxiv.org/abs/2405.03543v1","category":"math.LO"}
{"created":"2024-05-06 14:58:45","title":"Connecting essential triangulations I: via 2-3 and 0-2 moves","abstract":"Suppose that $M$ is a compact, connected three-manifold with boundary. We show that if the universal cover has infinitely many boundary components then $M$ has an ideal triangulation which is essential: no edge can be homotoped into the boundary. Under the same hypotheses, we show that the set of essential triangulations of $M$ is connected via 2-3, 3-2, 0-2, and 2-0 moves.   The above results are special cases of our general theory. We introduce $L$-essential triangulations: boundary components of the universal cover receive labels and no edge has the same label at both ends. As an application, under mild conditions on a representation, we construct an ideal triangulation for which a solution to Thurston's gluing equations recovers the given representation.   Our results also imply that such triangulations are connected via 2-3, 3-2, 0-2, and 2-0 moves. Together with results of Pandey and Wong, this proves that Dimofte and Garoufalidis' 1-loop invariant is independent of the choice of essential triangulation.","sentences":["Suppose that $M$ is a compact, connected three-manifold with boundary.","We show that if the universal cover has infinitely many boundary components then $M$ has an ideal triangulation which is essential: no edge can be homotoped into the boundary.","Under the same hypotheses, we show that the set of essential triangulations of $M$ is connected via 2-3, 3-2, 0-2, and 2-0 moves.   ","The above results are special cases of our general theory.","We introduce $L$-essential triangulations: boundary components of the universal cover receive labels and no edge has the same label at both ends.","As an application, under mild conditions on a representation, we construct an ideal triangulation for which a solution to Thurston's gluing equations recovers the given representation.   ","Our results also imply that such triangulations are connected via 2-3, 3-2, 0-2, and 2-0 moves.","Together with results of Pandey and Wong, this proves that Dimofte and Garoufalidis' 1-loop invariant is independent of the choice of essential triangulation."],"url":"http://arxiv.org/abs/2405.03539v1","category":"math.GT"}
{"created":"2024-05-06 14:49:34","title":"Comomentum sections and Poisson maps in Hamiltonian Lie algebroids","abstract":"In a Hamiltonian Lie algebroid over a pre-symplectic manifold and over a Poisson manifold, we introduce a map corresponding to a comomentum map, called a comomentum section. We show that the comomentum section gives a Lie algebroid morphism among Lie algebroids. Moreover, we prove that a momentum section on a Hamiltonian Lie algebroid is a Poisson map between proper Poisson manifolds, which is a generalization that a momentum map is a Poisson map between the symplectic manifold to dual of the Lie algebra. Finally, a momentum section is reinterpreted as a Dirac morphism on Dirac structures.","sentences":["In a Hamiltonian Lie algebroid over a pre-symplectic manifold and over a Poisson manifold, we introduce a map corresponding to a comomentum map, called a comomentum section.","We show that the comomentum section gives a Lie algebroid morphism among Lie algebroids.","Moreover, we prove that a momentum section on a Hamiltonian Lie algebroid is a Poisson map between proper Poisson manifolds, which is a generalization that a momentum map is a Poisson map between the symplectic manifold to dual of the Lie algebra.","Finally, a momentum section is reinterpreted as a Dirac morphism on Dirac structures."],"url":"http://arxiv.org/abs/2405.03533v1","category":"math.SG"}
{"created":"2024-05-06 14:47:19","title":"Quasi-Monte Carlo for Bayesian design of experiment problems governed by parametric PDEs","abstract":"This paper contributes to the study of optimal experimental design for Bayesian inverse problems governed by partial differential equations (PDEs). We derive estimates for the parametric regularity of multivariate double integration problems over high-dimensional parameter and data domains arising in Bayesian optimal design problems. We provide a detailed analysis for these double integration problems using two approaches: a full tensor product and a sparse tensor product combination of quasi-Monte Carlo (QMC) cubature rules over the parameter and data domains. Specifically, we show that the latter approach significantly improves the convergence rate, exhibiting performance comparable to that of QMC integration of a single high-dimensional integral. Furthermore, we numerically verify the predicted convergence rates for an elliptic PDE problem with an unknown diffusion coefficient in two spatial dimensions, offering empirical evidence supporting the theoretical results and highlighting practical applicability.","sentences":["This paper contributes to the study of optimal experimental design for Bayesian inverse problems governed by partial differential equations (PDEs).","We derive estimates for the parametric regularity of multivariate double integration problems over high-dimensional parameter and data domains arising in Bayesian optimal design problems.","We provide a detailed analysis for these double integration problems using two approaches: a full tensor product and a sparse tensor product combination of quasi-Monte Carlo (QMC) cubature rules over the parameter and data domains.","Specifically, we show that the latter approach significantly improves the convergence rate, exhibiting performance comparable to that of QMC integration of a single high-dimensional integral.","Furthermore, we numerically verify the predicted convergence rates for an elliptic PDE problem with an unknown diffusion coefficient in two spatial dimensions, offering empirical evidence supporting the theoretical results and highlighting practical applicability."],"url":"http://arxiv.org/abs/2405.03529v1","category":"math.NA"}
{"created":"2024-05-07 17:25:14","title":"Physics-data hybrid dynamic model of a multi-axis manipulator for sensorless dexterous manipulation and high-performance motion planning","abstract":"We report on the development of an implementable physics-data hybrid dynamic model for an articulated manipulator to plan and operate in various scenarios. Meanwhile, the physics-based and data-driven dynamic models are studied in this research to select the best model for planning. The physics-based model is constructed using the Lagrangian method, and the loss terms include inertia loss, viscous loss, and friction loss. As for the data-driven model, three methods are explored, including DNN, LSTM, and XGBoost. Our modeling results demonstrate that, after comprehensive hyperparameter optimization, the XGBoost architecture outperforms DNN and LSTM in accurately representing manipulator dynamics. The hybrid model with physics-based and data-driven terms has the best performance among all models based on the RMSE criteria, and it only needs about 24k of training data. In addition, we developed a virtual force sensor of a manipulator using the observed external torque derived from the dynamic model and designed a motion planner through the physics-data hybrid dynamic model. The external torque contributes to forces and torque on the end effector, facilitating interaction with the surroundings, while the internal torque governs manipulator motion dynamics and compensates for internal losses. By estimating external torque via the difference between measured joint torque and internal losses, we implement a sensorless control strategy which is demonstrated through a peg-in-hole task. Lastly, a learning-based motion planner based on the hybrid dynamic model assists in planning time-efficient trajectories for the manipulator. This comprehensive approach underscores the efficacy of integrating physics-based and data-driven models for advanced manipulator control and planning in industrial environments.","sentences":["We report on the development of an implementable physics-data hybrid dynamic model for an articulated manipulator to plan and operate in various scenarios.","Meanwhile, the physics-based and data-driven dynamic models are studied in this research to select the best model for planning.","The physics-based model is constructed using the Lagrangian method, and the loss terms include inertia loss, viscous loss, and friction loss.","As for the data-driven model, three methods are explored, including DNN, LSTM, and XGBoost.","Our modeling results demonstrate that, after comprehensive hyperparameter optimization, the XGBoost architecture outperforms DNN and LSTM in accurately representing manipulator dynamics.","The hybrid model with physics-based and data-driven terms has the best performance among all models based on the RMSE criteria, and it only needs about 24k of training data.","In addition, we developed a virtual force sensor of a manipulator using the observed external torque derived from the dynamic model and designed a motion planner through the physics-data hybrid dynamic model.","The external torque contributes to forces and torque on the end effector, facilitating interaction with the surroundings, while the internal torque governs manipulator motion dynamics and compensates for internal losses.","By estimating external torque via the difference between measured joint torque and internal losses, we implement a sensorless control strategy which is demonstrated through a peg-in-hole task.","Lastly, a learning-based motion planner based on the hybrid dynamic model assists in planning time-efficient trajectories for the manipulator.","This comprehensive approach underscores the efficacy of integrating physics-based and data-driven models for advanced manipulator control and planning in industrial environments."],"url":"http://arxiv.org/abs/2405.04503v1","category":"cs.RO"}
{"created":"2024-05-07 17:04:21","title":"Representation Learning of Daily Movement Data Using Text Encoders","abstract":"Time-series representation learning is a key area of research for remote healthcare monitoring applications. In this work, we focus on a dataset of recordings of in-home activity from people living with Dementia. We design a representation learning method based on converting activity to text strings that can be encoded using a language model fine-tuned to transform data from the same participants within a $30$-day window to similar embeddings in the vector space. This allows for clustering and vector searching over participants and days, and the identification of activity deviations to aid with personalised delivery of care.","sentences":["Time-series representation learning is a key area of research for remote healthcare monitoring applications.","In this work, we focus on a dataset of recordings of in-home activity from people living with Dementia.","We design a representation learning method based on converting activity to text strings that can be encoded using a language model fine-tuned to transform data from the same participants within a $30$-day window to similar embeddings in the vector space.","This allows for clustering and vector searching over participants and days, and the identification of activity deviations to aid with personalised delivery of care."],"url":"http://arxiv.org/abs/2405.04494v1","category":"cs.LG"}
{"created":"2024-05-07 16:13:23","title":"Cosmology from one galaxy in voids?","abstract":"Understanding galaxy properties may be the key to unlocking some of the most intriguing mysteries of modern cosmology. Recent work relied on machine learning to extract cosmological constraints on $\\Omega_\\mathrm{m}$ using only $\\textit{one}$ galaxy. But if this is true, how should we select $\\textit{the}$ galaxy to use for cosmology inference? In this paper, we consider selecting a galaxy that lies in cosmic voids, the underdense regions of the cosmic web, and compare the constraints obtained with the ones obtained when randomly selecting a galaxy in the whole sample. We use the IllustrisTNG galaxy catalog from the CAMELS project and the VIDE void finder to identify galaxies inside voids. We show that void galaxies provide stronger constraints on $\\Omega_\\mathrm{m}$ compared to randomly selected galaxies. This result suggests that the distinctive characteristics of void galaxies may provide a cleaner and more effective environment for extracting cosmological information.","sentences":["Understanding galaxy properties may be the key to unlocking some of the most intriguing mysteries of modern cosmology.","Recent work relied on machine learning to extract cosmological constraints on $\\Omega_\\mathrm{m}$ using only $\\textit{one}$ galaxy.","But if this is true, how should we select $\\textit{the}$ galaxy to use for cosmology inference?","In this paper, we consider selecting a galaxy that lies in cosmic voids, the underdense regions of the cosmic web, and compare the constraints obtained with the ones obtained when randomly selecting a galaxy in the whole sample.","We use the IllustrisTNG galaxy catalog from the CAMELS project and the VIDE void finder to identify galaxies inside voids.","We show that void galaxies provide stronger constraints on $\\Omega_\\mathrm{m}$ compared to randomly selected galaxies.","This result suggests that the distinctive characteristics of void galaxies may provide a cleaner and more effective environment for extracting cosmological information."],"url":"http://arxiv.org/abs/2405.04447v1","category":"astro-ph.CO"}
{"created":"2024-05-07 15:14:20","title":"DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving","abstract":"Vision-centric autonomous driving has recently raised wide attention due to its lower cost. Pre-training is essential for extracting a universal representation. However, current vision-centric pre-training typically relies on either 2D or 3D pre-text tasks, overlooking the temporal characteristics of autonomous driving as a 4D scene understanding task. In this paper, we address this challenge by introducing a world model-based autonomous driving 4D representation learning framework, dubbed \\emph{DriveWorld}, which is capable of pre-training from multi-camera driving videos in a spatio-temporal fashion. Specifically, we propose a Memory State-Space Model for spatio-temporal modelling, which consists of a Dynamic Memory Bank module for learning temporal-aware latent dynamics to predict future changes and a Static Scene Propagation module for learning spatial-aware latent statics to offer comprehensive scene contexts. We additionally introduce a Task Prompt to decouple task-aware features for various downstream tasks. The experiments demonstrate that DriveWorld delivers promising results on various autonomous driving tasks. When pre-trained with the OpenScene dataset, DriveWorld achieves a 7.5% increase in mAP for 3D object detection, a 3.0% increase in IoU for online mapping, a 5.0% increase in AMOTA for multi-object tracking, a 0.1m decrease in minADE for motion forecasting, a 3.0% increase in IoU for occupancy prediction, and a 0.34m reduction in average L2 error for planning.","sentences":["Vision-centric autonomous driving has recently raised wide attention due to its lower cost.","Pre-training is essential for extracting a universal representation.","However, current vision-centric pre-training typically relies on either 2D or 3D pre-text tasks, overlooking the temporal characteristics of autonomous driving as a 4D scene understanding task.","In this paper, we address this challenge by introducing a world model-based autonomous driving 4D representation learning framework, dubbed \\emph{DriveWorld}, which is capable of pre-training from multi-camera driving videos in a spatio-temporal fashion.","Specifically, we propose a Memory State-Space Model for spatio-temporal modelling, which consists of a Dynamic Memory Bank module for learning temporal-aware latent dynamics to predict future changes and a Static Scene Propagation module for learning spatial-aware latent statics to offer comprehensive scene contexts.","We additionally introduce a Task Prompt to decouple task-aware features for various downstream tasks.","The experiments demonstrate that DriveWorld delivers promising results on various autonomous driving tasks.","When pre-trained with the OpenScene dataset, DriveWorld achieves a 7.5% increase in mAP for 3D object detection, a 3.0% increase in IoU for online mapping, a 5.0% increase in AMOTA for multi-object tracking, a 0.1m decrease in minADE for motion forecasting, a 3.0% increase in IoU for occupancy prediction, and a 0.34m reduction in average L2 error for planning."],"url":"http://arxiv.org/abs/2405.04390v1","category":"cs.CV"}
{"created":"2024-05-07 14:49:32","title":"Some Things I Have Learned From Detlef D\u00fcrr","abstract":"Detlef D\\\"urr (1951-2021) was a theoretical and mathematical physicist who worked particularly on the foundations of quantum mechanics, electromagnetism, and statistical mechanics. This piece is a rather personal look back at him and his science.","sentences":["Detlef D\\\"urr (1951-2021) was a theoretical and mathematical physicist who worked particularly on the foundations of quantum mechanics, electromagnetism, and statistical mechanics.","This piece is a rather personal look back at him and his science."],"url":"http://arxiv.org/abs/2405.04368v1","category":"physics.hist-ph"}
{"created":"2024-05-07 13:15:07","title":"ViewFormer: Exploring Spatiotemporal Modeling for Multi-View 3D Occupancy Perception via View-Guided Transformers","abstract":"3D occupancy, an advanced perception technology for driving scenarios, represents the entire scene without distinguishing between foreground and background by quantifying the physical space into a grid map. The widely adopted projection-first deformable attention, efficient in transforming image features into 3D representations, encounters challenges in aggregating multi-view features due to sensor deployment constraints. To address this issue, we propose our learning-first view attention mechanism for effective multi-view feature aggregation. Moreover, we showcase the scalability of our view attention across diverse multi-view 3D tasks, such as map construction and 3D object detection. Leveraging the proposed view attention as well as an additional multi-frame streaming temporal attention, we introduce ViewFormer, a vision-centric transformer-based framework for spatiotemporal feature aggregation. To further explore occupancy-level flow representation, we present FlowOcc3D, a benchmark built on top of existing high-quality datasets. Qualitative and quantitative analyses on this benchmark reveal the potential to represent fine-grained dynamic scenes. Extensive experiments show that our approach significantly outperforms prior state-of-the-art methods. The codes and benchmark will be released soon.","sentences":["3D occupancy, an advanced perception technology for driving scenarios, represents the entire scene without distinguishing between foreground and background by quantifying the physical space into a grid map.","The widely adopted projection-first deformable attention, efficient in transforming image features into 3D representations, encounters challenges in aggregating multi-view features due to sensor deployment constraints.","To address this issue, we propose our learning-first view attention mechanism for effective multi-view feature aggregation.","Moreover, we showcase the scalability of our view attention across diverse multi-view 3D tasks, such as map construction and 3D object detection.","Leveraging the proposed view attention as well as an additional multi-frame streaming temporal attention, we introduce ViewFormer, a vision-centric transformer-based framework for spatiotemporal feature aggregation.","To further explore occupancy-level flow representation, we present FlowOcc3D, a benchmark built on top of existing high-quality datasets.","Qualitative and quantitative analyses on this benchmark reveal the potential to represent fine-grained dynamic scenes.","Extensive experiments show that our approach significantly outperforms prior state-of-the-art methods.","The codes and benchmark will be released soon."],"url":"http://arxiv.org/abs/2405.04299v1","category":"cs.CV"}
{"created":"2024-05-07 10:03:19","title":"LingML: Linguistic-Informed Machine Learning for Enhanced Fake News Detection","abstract":"Nowadays, Information spreads at an unprecedented pace in social media and discerning truth from misinformation and fake news has become an acute societal challenge. Machine learning (ML) models have been employed to identify fake news but are far from perfect with challenging problems like limited accuracy, interpretability, and generalizability. In this paper, we enhance ML-based solutions with linguistics input and we propose LingML, linguistic-informed ML, for fake news detection. We conducted an experimental study with a popular dataset on fake news during the pandemic. The experiment results show that our proposed solution is highly effective. There are fewer than two errors out of every ten attempts with only linguistic input used in ML and the knowledge is highly explainable. When linguistics input is integrated with advanced large-scale ML models for natural language processing, our solution outperforms existing ones with 1.8% average error rate. LingML creates a new path with linguistics to push the frontier of effective and efficient fake news detection. It also sheds light on real-world multi-disciplinary applications requiring both ML and domain expertise to achieve optimal performance.","sentences":["Nowadays, Information spreads at an unprecedented pace in social media and discerning truth from misinformation and fake news has become an acute societal challenge.","Machine learning (ML) models have been employed to identify fake news but are far from perfect with challenging problems like limited accuracy, interpretability, and generalizability.","In this paper, we enhance ML-based solutions with linguistics input and we propose LingML, linguistic-informed ML, for fake news detection.","We conducted an experimental study with a popular dataset on fake news during the pandemic.","The experiment results show that our proposed solution is highly effective.","There are fewer than two errors out of every ten attempts with only linguistic input used in ML and the knowledge is highly explainable.","When linguistics input is integrated with advanced large-scale ML models for natural language processing, our solution outperforms existing ones with 1.8% average error rate.","LingML creates a new path with linguistics to push the frontier of effective and efficient fake news detection.","It also sheds light on real-world multi-disciplinary applications requiring both ML and domain expertise to achieve optimal performance."],"url":"http://arxiv.org/abs/2405.04165v1","category":"cs.CL"}
{"created":"2024-05-07 09:26:20","title":"Multiparameter regularization and aggregation in the context of polynomial functional regression","abstract":"Most of the recent results in polynomial functional regression have been focused on an in-depth exploration of single-parameter regularization schemes. In contrast, in this study we go beyond that framework by introducing an algorithm for multiple parameter regularization and presenting a theoretically grounded method for dealing with the associated parameters. This method facilitates the aggregation of models with varying regularization parameters. The efficacy of the proposed approach is assessed through evaluations on both synthetic and some real-world medical data, revealing promising results.","sentences":["Most of the recent results in polynomial functional regression have been focused on an in-depth exploration of single-parameter regularization schemes.","In contrast, in this study we go beyond that framework by introducing an algorithm for multiple parameter regularization and presenting a theoretically grounded method for dealing with the associated parameters.","This method facilitates the aggregation of models with varying regularization parameters.","The efficacy of the proposed approach is assessed through evaluations on both synthetic and some real-world medical data, revealing promising results."],"url":"http://arxiv.org/abs/2405.04147v1","category":"stat.ML"}
{"created":"2024-05-07 05:06:45","title":"Adjoint Sensitivity Analysis on Multi-Scale Bioprocess Stochastic Reaction Network","abstract":"Motivated by the pressing challenges in the digital twin development for biomanufacturing process, we introduce an adjoint sensitivity analysis (SA) approach to expedite the learning of mechanistic model parameters. In this paper, we consider enzymatic stochastic reaction networks representing a multi-scale bioprocess mechanistic model that allows us to integrate disparate data from diverse production processes and leverage the information from existing macro-kinetic and genome-scale models. To support forward prediction and backward reasoning, we develop a convergent adjoint SA algorithm studying how the perturbations of model parameters and inputs (e.g., initial state) propagate through enzymatic reaction networks and impact on output trajectory predictions. This SA can provide a sample efficient and interpretable way to assess the sensitivities between inputs and outputs accounting for their causal dependencies. Our empirical study underscores the resilience of these sensitivities and illuminates a deeper comprehension of the regulatory mechanisms behind bioprocess through sensitivities.","sentences":["Motivated by the pressing challenges in the digital twin development for biomanufacturing process, we introduce an adjoint sensitivity analysis (SA) approach to expedite the learning of mechanistic model parameters.","In this paper, we consider enzymatic stochastic reaction networks representing a multi-scale bioprocess mechanistic model that allows us to integrate disparate data from diverse production processes and leverage the information from existing macro-kinetic and genome-scale models.","To support forward prediction and backward reasoning, we develop a convergent adjoint SA algorithm studying how the perturbations of model parameters and inputs (e.g., initial state) propagate through enzymatic reaction networks and impact on output trajectory predictions.","This SA can provide a sample efficient and interpretable way to assess the sensitivities between inputs and outputs accounting for their causal dependencies.","Our empirical study underscores the resilience of these sensitivities and illuminates a deeper comprehension of the regulatory mechanisms behind bioprocess through sensitivities."],"url":"http://arxiv.org/abs/2405.04011v1","category":"q-bio.MN"}
{"created":"2024-05-07 04:10:01","title":"Assemblage: Automatic Binary Dataset Construction for Machine Learning","abstract":"Binary code is pervasive, and binary analysis is a key task in reverse engineering, malware classification, and vulnerability discovery. Unfortunately, while there exist large corpuses of malicious binaries, obtaining high-quality corpuses of benign binaries for modern systems has proven challenging (e.g., due to licensing issues). Consequently, machine learning based pipelines for binary analysis utilize either costly commercial corpuses (e.g., VirusTotal) or open-source binaries (e.g., coreutils) available in limited quantities. To address these issues, we present Assemblage: an extensible cloud-based distributed system that crawls, configures, and builds Windows PE binaries to obtain high-quality binary corpuses suitable for training state-of-the-art models in binary analysis. We have run Assemblage on AWS over the past year, producing 890k Windows PE and 428k Linux ELF binaries across 29 configurations. Assemblage is designed to be both reproducible and extensible, enabling users to publish \"recipes\" for their datasets, and facilitating the extraction of a wide array of features. We evaluated Assemblage by using its data to train modern learning-based pipelines for compiler provenance and binary function similarity. Our results illustrate the practical need for robust corpuses of high-quality Windows PE binaries in training modern learning-based binary analyses. Assemblage can be downloaded from https://assemblage-dataset.net","sentences":["Binary code is pervasive, and binary analysis is a key task in reverse engineering, malware classification, and vulnerability discovery.","Unfortunately, while there exist large corpuses of malicious binaries, obtaining high-quality corpuses of benign binaries for modern systems has proven challenging (e.g., due to licensing issues).","Consequently, machine learning based pipelines for binary analysis utilize either costly commercial corpuses (e.g., VirusTotal) or open-source binaries (e.g., coreutils) available in limited quantities.","To address these issues, we present Assemblage: an extensible cloud-based distributed system that crawls, configures, and builds Windows PE binaries to obtain high-quality binary corpuses suitable for training state-of-the-art models in binary analysis.","We have run Assemblage on AWS over the past year, producing 890k Windows PE and 428k Linux ELF binaries across 29 configurations.","Assemblage is designed to be both reproducible and extensible, enabling users to publish \"recipes\" for their datasets, and facilitating the extraction of a wide array of features.","We evaluated Assemblage by using its data to train modern learning-based pipelines for compiler provenance and binary function similarity.","Our results illustrate the practical need for robust corpuses of high-quality Windows PE binaries in training modern learning-based binary analyses.","Assemblage can be downloaded from https://assemblage-dataset.net"],"url":"http://arxiv.org/abs/2405.03991v1","category":"cs.CR"}
{"created":"2024-05-07 03:30:57","title":"VMambaCC: A Visual State Space Model for Crowd Counting","abstract":"As a deep learning model, Visual Mamba (VMamba) has a low computational complexity and a global receptive field, which has been successful applied to image classification and detection. To extend its applications, we apply VMamba to crowd counting and propose a novel VMambaCC (VMamba Crowd Counting) model. Naturally, VMambaCC inherits the merits of VMamba, or global modeling for images and low computational cost. Additionally, we design a Multi-head High-level Feature (MHF) attention mechanism for VMambaCC. MHF is a new attention mechanism that leverages high-level semantic features to augment low-level semantic features, thereby enhancing spatial feature representation with greater precision. Building upon MHF, we further present a High-level Semantic Supervised Feature Pyramid Network (HS2PFN) that progressively integrates and enhances high-level semantic information with low-level semantic information. Extensive experimental results on five public datasets validate the efficacy of our approach. For example, our method achieves a mean absolute error of 51.87 and a mean squared error of 81.3 on the ShangHaiTech\\_PartA dataset. Our code is coming soon.","sentences":["As a deep learning model, Visual Mamba (VMamba) has a low computational complexity and a global receptive field, which has been successful applied to image classification and detection.","To extend its applications, we apply VMamba to crowd counting and propose a novel VMambaCC (VMamba Crowd Counting) model.","Naturally, VMambaCC inherits the merits of VMamba, or global modeling for images and low computational cost.","Additionally, we design a Multi-head High-level Feature (MHF) attention mechanism for VMambaCC.","MHF is a new attention mechanism that leverages high-level semantic features to augment low-level semantic features, thereby enhancing spatial feature representation with greater precision.","Building upon MHF, we further present a High-level Semantic Supervised Feature Pyramid Network (HS2PFN) that progressively integrates and enhances high-level semantic information with low-level semantic information.","Extensive experimental results on five public datasets validate the efficacy of our approach.","For example, our method achieves a mean absolute error of 51.87 and a mean squared error of 81.3 on the ShangHaiTech\\_PartA dataset.","Our code is coming soon."],"url":"http://arxiv.org/abs/2405.03978v1","category":"cs.CV"}
{"created":"2024-05-07 03:01:40","title":"Unified End-to-End V2X Cooperative Autonomous Driving","abstract":"V2X cooperation, through the integration of sensor data from both vehicles and infrastructure, is considered a pivotal approach to advancing autonomous driving technology. Current research primarily focuses on enhancing perception accuracy, often overlooking the systematic improvement of accident prediction accuracy through end-to-end learning, leading to insufficient attention to the safety issues of autonomous driving. To address this challenge, this paper introduces the UniE2EV2X framework, a V2X-integrated end-to-end autonomous driving system that consolidates key driving modules within a unified network. The framework employs a deformable attention-based data fusion strategy, effectively facilitating cooperation between vehicles and infrastructure. The main advantages include: 1) significantly enhancing agents' perception and motion prediction capabilities, thereby improving the accuracy of accident predictions; 2) ensuring high reliability in the data fusion process; 3) superior end-to-end perception compared to modular approaches. Furthermore, We implement the UniE2EV2X framework on the challenging DeepAccident, a simulation dataset designed for V2X cooperative driving.","sentences":["V2X cooperation, through the integration of sensor data from both vehicles and infrastructure, is considered a pivotal approach to advancing autonomous driving technology.","Current research primarily focuses on enhancing perception accuracy, often overlooking the systematic improvement of accident prediction accuracy through end-to-end learning, leading to insufficient attention to the safety issues of autonomous driving.","To address this challenge, this paper introduces the UniE2EV2X framework, a V2X-integrated end-to-end autonomous driving system that consolidates key driving modules within a unified network.","The framework employs a deformable attention-based data fusion strategy, effectively facilitating cooperation between vehicles and infrastructure.","The main advantages include: 1) significantly enhancing agents' perception and motion prediction capabilities, thereby improving the accuracy of accident predictions; 2) ensuring high reliability in the data fusion process; 3) superior end-to-end perception compared to modular approaches.","Furthermore, We implement the UniE2EV2X framework on the challenging DeepAccident, a simulation dataset designed for V2X cooperative driving."],"url":"http://arxiv.org/abs/2405.03971v1","category":"cs.CV"}
{"created":"2024-05-07 02:54:31","title":"SwiftRL: Towards Efficient Reinforcement Learning on Real Processing-In-Memory Systems","abstract":"Reinforcement Learning (RL) trains agents to learn optimal behavior by maximizing reward signals from experience datasets. However, RL training often faces memory limitations, leading to execution latencies and prolonged training times. To overcome this, SwiftRL explores Processing-In-Memory (PIM) architectures to accelerate RL workloads. We achieve near-linear performance scaling by implementing RL algorithms like Tabular Q-learning and SARSA on UPMEM PIM systems and optimizing for hardware. Our experiments on OpenAI GYM environments using UPMEM hardware demonstrate superior performance compared to CPU and GPU implementations.","sentences":["Reinforcement Learning (RL) trains agents to learn optimal behavior by maximizing reward signals from experience datasets.","However, RL training often faces memory limitations, leading to execution latencies and prolonged training times.","To overcome this, SwiftRL explores Processing-In-Memory (PIM) architectures to accelerate RL workloads.","We achieve near-linear performance scaling by implementing RL algorithms like Tabular Q-learning and SARSA on UPMEM PIM systems and optimizing for hardware.","Our experiments on OpenAI GYM environments using UPMEM hardware demonstrate superior performance compared to CPU and GPU implementations."],"url":"http://arxiv.org/abs/2405.03967v1","category":"cs.LG"}
{"created":"2024-05-07 02:49:21","title":"AdsorbDiff: Adsorbate Placement via Conditional Denoising Diffusion","abstract":"Determining the optimal configuration of adsorbates on a slab (adslab) is pivotal in the exploration of novel catalysts across diverse applications. Traditionally, the quest for the lowest energy adslab configuration involves placing the adsorbate onto the slab followed by an optimization process. Prior methodologies have relied on heuristics, problem-specific intuitions, or brute-force approaches to guide adsorbate placement. In this work, we propose a novel framework for adsorbate placement using denoising diffusion. The model is designed to predict the optimal adsorbate site and orientation corresponding to the lowest energy configuration. Further, we have an end-to-end evaluation framework where diffusion-predicted adslab configuration is optimized with a pretrained machine learning force field and finally evaluated with Density Functional Theory (DFT). Our findings demonstrate an acceleration of up to 5x or 3.5x improvement in accuracy compared to the previous best approach. Given the novelty of this framework and application, we provide insights into the impact of pre-training, model architectures, and conduct extensive experiments to underscore the significance of this approach.","sentences":["Determining the optimal configuration of adsorbates on a slab (adslab) is pivotal in the exploration of novel catalysts across diverse applications.","Traditionally, the quest for the lowest energy adslab configuration involves placing the adsorbate onto the slab followed by an optimization process.","Prior methodologies have relied on heuristics, problem-specific intuitions, or brute-force approaches to guide adsorbate placement.","In this work, we propose a novel framework for adsorbate placement using denoising diffusion.","The model is designed to predict the optimal adsorbate site and orientation corresponding to the lowest energy configuration.","Further, we have an end-to-end evaluation framework where diffusion-predicted adslab configuration is optimized with a pretrained machine learning force field and finally evaluated with Density Functional Theory (DFT).","Our findings demonstrate an acceleration of up to 5x or 3.5x improvement in accuracy compared to the previous best approach.","Given the novelty of this framework and application, we provide insights into the impact of pre-training, model architectures, and conduct extensive experiments to underscore the significance of this approach."],"url":"http://arxiv.org/abs/2405.03962v1","category":"cs.LG"}
{"created":"2024-05-07 02:19:16","title":"HAFFormer: A Hierarchical Attention-Free Framework for Alzheimer's Disease Detection From Spontaneous Speech","abstract":"Automatically detecting Alzheimer's Disease (AD) from spontaneous speech plays an important role in its early diagnosis. Recent approaches highly rely on the Transformer architectures due to its efficiency in modelling long-range context dependencies. However, the quadratic increase in computational complexity associated with self-attention and the length of audio poses a challenge when deploying such models on edge devices. In this context, we construct a novel framework, namely Hierarchical Attention-Free Transformer (HAFFormer), to better deal with long speech for AD detection. Specifically, we employ an attention-free module of Multi-Scale Depthwise Convolution to replace the self-attention and thus avoid the expensive computation, and a GELU-based Gated Linear Unit to replace the feedforward layer, aiming to automatically filter out the redundant information. Moreover, we design a hierarchical structure to force it to learn a variety of information grains, from the frame level to the dialogue level. By conducting extensive experiments on the ADReSS-M dataset, the introduced HAFFormer can achieve competitive results (82.6% accuracy) with other recent work, but with significant computational complexity and model size reduction compared to the standard Transformer. This shows the efficiency of HAFFormer in dealing with long audio for AD detection.","sentences":["Automatically detecting Alzheimer's Disease (AD) from spontaneous speech plays an important role in its early diagnosis.","Recent approaches highly rely on the Transformer architectures due to its efficiency in modelling long-range context dependencies.","However, the quadratic increase in computational complexity associated with self-attention and the length of audio poses a challenge when deploying such models on edge devices.","In this context, we construct a novel framework, namely Hierarchical Attention-Free Transformer (HAFFormer), to better deal with long speech for AD detection.","Specifically, we employ an attention-free module of Multi-Scale Depthwise Convolution to replace the self-attention and thus avoid the expensive computation, and a GELU-based Gated Linear Unit to replace the feedforward layer, aiming to automatically filter out the redundant information.","Moreover, we design a hierarchical structure to force it to learn a variety of information grains, from the frame level to the dialogue level.","By conducting extensive experiments on the ADReSS-M dataset, the introduced HAFFormer can achieve competitive results (82.6% accuracy) with other recent work, but with significant computational complexity and model size reduction compared to the standard Transformer.","This shows the efficiency of HAFFormer in dealing with long audio for AD detection."],"url":"http://arxiv.org/abs/2405.03952v1","category":"cs.SD"}
{"created":"2024-05-07 00:25:20","title":"KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization","abstract":"Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As the batch size, context length, or model size increases, the size of the key and value (KV) cache can quickly become the main contributor to GPU memory usage and the bottleneck of inference latency. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. We observe that distinct channels of a key/value activation embedding are highly inter-dependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropies. Based on this insight, we propose Coupled Quantization (CQ), which couples multiple key/value channels together to exploit their inter-dependency and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ outperforms or is competitive with existing baselines in preserving model quality. Furthermore, we demonstrate that CQ can preserve model quality with KV cache quantized down to 1-bit.","sentences":["Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput.","As the batch size, context length, or model size increases, the size of the key and value (KV) cache can quickly become the main contributor to GPU memory usage and the bottleneck of inference latency.","Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths.","We observe that distinct channels of a key/value activation embedding are highly inter-dependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropies.","Based on this insight, we propose Coupled Quantization (CQ), which couples multiple key/value channels together to exploit their inter-dependency and encode the activations in a more information-efficient manner.","Extensive experiments reveal that CQ outperforms or is competitive with existing baselines in preserving model quality.","Furthermore, we demonstrate that CQ can preserve model quality with KV cache quantized down to 1-bit."],"url":"http://arxiv.org/abs/2405.03917v1","category":"cs.LG"}
{"created":"2024-05-07 00:22:13","title":"Digital Twin Calibration for Biological System-of-Systems: Cell Culture Manufacturing Process","abstract":"Biomanufacturing innovation relies on an efficient design of experiments (DoE) to optimize processes and product quality. Traditional DoE methods, ignoring the underlying bioprocessing mechanisms, often suffer from a lack of interpretability and sample efficiency. This limitation motivates us to create a new optimal learning approach that can guide a sequential DoEs for digital twin model calibration. In this study, we consider a multi-scale mechanistic model for cell culture process, also known as Biological Systems-of-Systems (Bio-SoS), as our digital twin. This model with modular design, composed of sub-models, allows us to integrate data across various production processes. To calibrate the Bio-SoS digital twin, we evaluate the mean squared error of model prediction and develop a computational approach to quantify the impact of parameter estimation error of individual sub-models on the prediction accuracy of digital twin, which can guide sample-efficient and interpretable DoEs.","sentences":["Biomanufacturing innovation relies on an efficient design of experiments (DoE) to optimize processes and product quality.","Traditional DoE methods, ignoring the underlying bioprocessing mechanisms, often suffer from a lack of interpretability and sample efficiency.","This limitation motivates us to create a new optimal learning approach that can guide a sequential DoEs for digital twin model calibration.","In this study, we consider a multi-scale mechanistic model for cell culture process, also known as Biological Systems-of-Systems (Bio-SoS), as our digital twin.","This model with modular design, composed of sub-models, allows us to integrate data across various production processes.","To calibrate the Bio-SoS digital twin, we evaluate the mean squared error of model prediction and develop a computational approach to quantify the impact of parameter estimation error of individual sub-models on the prediction accuracy of digital twin, which can guide sample-efficient and interpretable DoEs."],"url":"http://arxiv.org/abs/2405.03913v1","category":"q-bio.QM"}
{"created":"2024-05-06 23:36:03","title":"Transformer models classify random numbers","abstract":"Random numbers are incredibly important in a variety of fields, and the need for their validation remains important. A Quantum Random Number Generator (QRNG) can theoretically generate truly random numbers however this does not remove the need to thoroughly test their randomness. Generally, the task of validating random numbers has been delegated to different statistical tests such as the tests from the NIST Statistical Test Suite (STS) which are often slow and only perform one task at a time. Our work presents a deep learning model that utilizes the transformer architecture to encode some of the tests from the NIST STS in a single model that also runs much faster. This model performs multi-label classification on these tests and outputs the probability of passing each statistical test that it encodes. We perform a thorough hyper-parameter optimization to converge on the best possible model and as a result, achieve a high degree of accuracy with a sample f1 score of above 0.9.","sentences":["Random numbers are incredibly important in a variety of fields, and the need for their validation remains important.","A Quantum Random Number Generator (QRNG) can theoretically generate truly random numbers however this does not remove the need to thoroughly test their randomness.","Generally, the task of validating random numbers has been delegated to different statistical tests such as the tests from the NIST Statistical Test Suite (STS) which are often slow and only perform one task at a time.","Our work presents a deep learning model that utilizes the transformer architecture to encode some of the tests from the NIST STS in a single model that also runs much faster.","This model performs multi-label classification on these tests and outputs the probability of passing each statistical test that it encodes.","We perform a thorough hyper-parameter optimization to converge on the best possible model and as a result, achieve a high degree of accuracy with a sample f1 score of above 0.9."],"url":"http://arxiv.org/abs/2405.03904v1","category":"cs.LG"}
{"created":"2024-05-06 22:55:53","title":"MVDiff: Scalable and Flexible Multi-View Diffusion for 3D Object Reconstruction from Single-View","abstract":"Generating consistent multiple views for 3D reconstruction tasks is still a challenge to existing image-to-3D diffusion models. Generally, incorporating 3D representations into diffusion model decrease the model's speed as well as generalizability and quality. This paper proposes a general framework to generate consistent multi-view images from single image or leveraging scene representation transformer and view-conditioned diffusion model. In the model, we introduce epipolar geometry constraints and multi-view attention to enforce 3D consistency. From as few as one image input, our model is able to generate 3D meshes surpassing baselines methods in evaluation metrics, including PSNR, SSIM and LPIPS.","sentences":["Generating consistent multiple views for 3D reconstruction tasks is still a challenge to existing image-to-3D diffusion models.","Generally, incorporating 3D representations into diffusion model decrease the model's speed as well as generalizability and quality.","This paper proposes a general framework to generate consistent multi-view images from single image or leveraging scene representation transformer and view-conditioned diffusion model.","In the model, we introduce epipolar geometry constraints and multi-view attention to enforce 3D consistency.","From as few as one image input, our model is able to generate 3D meshes surpassing baselines methods in evaluation metrics, including PSNR, SSIM and LPIPS."],"url":"http://arxiv.org/abs/2405.03894v1","category":"cs.CV"}
{"created":"2024-05-06 22:26:08","title":"SVan: A Mobile Hub as a Field Robotics Development and Deployment Platform","abstract":"As robotics becomes increasingly vital for environmental protection, there is a growing need for effective deployment methods that match the pace of robotics innovation. Current strategies often fall short, leaving a gap between the potential of robotics and their practical application in the field. Addressing this challenge, we introduce a mobile hub concept designed to provide the necessary infrastructure and support for deploying a diverse, multi-domain robot team effectively. This paper presents the development and insights into `SVAN' (Synchronous Team-Robot Van), a prototype of our mobile hub concept. We delve into the mechanical construction and software setup of SVAN, offering a comprehensive overview of its capabilities and design considerations. Further, we discuss the hardware specifications and share valuable lessons learned during the prototype's development and deployment. In addition to this paper, an accepted video complements our exploration by depicting SVAN in its envisioned role as an environmental guardian, highlighting its potential in ecological monitoring and preservation. Furthermore, our discussion is enriched by referencing a previously accepted paper detailing a novel methodology for continuous UAV mission cycling enabled by a mobile hub like SVAN. These accompanying works underscore our contribution towards addressing the existing gaps in robot deployment strategies, presenting a scalable and efficient framework to overcome operational challenges in environmental robotics.","sentences":["As robotics becomes increasingly vital for environmental protection, there is a growing need for effective deployment methods that match the pace of robotics innovation.","Current strategies often fall short, leaving a gap between the potential of robotics and their practical application in the field.","Addressing this challenge, we introduce a mobile hub concept designed to provide the necessary infrastructure and support for deploying a diverse, multi-domain robot team effectively.","This paper presents the development and insights into `SVAN' (Synchronous Team-Robot Van), a prototype of our mobile hub concept.","We delve into the mechanical construction and software setup of SVAN, offering a comprehensive overview of its capabilities and design considerations.","Further, we discuss the hardware specifications and share valuable lessons learned during the prototype's development and deployment.","In addition to this paper, an accepted video complements our exploration by depicting SVAN in its envisioned role as an environmental guardian, highlighting its potential in ecological monitoring and preservation.","Furthermore, our discussion is enriched by referencing a previously accepted paper detailing a novel methodology for continuous UAV mission cycling enabled by a mobile hub like SVAN.","These accompanying works underscore our contribution towards addressing the existing gaps in robot deployment strategies, presenting a scalable and efficient framework to overcome operational challenges in environmental robotics."],"url":"http://arxiv.org/abs/2405.03890v1","category":"cs.RO"}
{"created":"2024-05-06 21:55:19","title":"Efficient Radiation Treatment Planning based on Voxel Importance","abstract":"Optimization is a time-consuming part of radiation treatment planning. We propose to reduce the optimization problem by only using a representative subset of informative voxels. This way, we improve planning efficiency while maintaining or enhancing the plan quality. To reduce the computational complexity of the optimization problem, we propose to subsample the set of voxels via importance sampling. We derive a sampling distribution based on an importance score that we obtain from pre-solving an easy optimization problem involving a simplified probing objective. By solving a reduced version of the original optimization problem using this subset, we effectively reduce the problem's size and computational demands while accounting for regions in which satisfactory dose deliveries are challenging. In contrast to other stochastic (sub-)sampling methods, our technique only requires a single sampling step to define a reduced optimization problem. This problem can be efficiently solved using established solvers. Empirical experiments on open benchmark data highlight substantially reduced optimization times, up to 50 times faster than the original ones, for intensity-modulated radiation therapy (IMRT), all while upholding plan quality comparable to traditional methods. Our approach has the potential to significantly accelerate radiation treatment planning by addressing its inherent computational challenges. We reduce the treatment planning time by reducing the size of the optimization problem rather than improving the optimization method. Our efforts are thus complementary to much of the previous developments.","sentences":["Optimization is a time-consuming part of radiation treatment planning.","We propose to reduce the optimization problem by only using a representative subset of informative voxels.","This way, we improve planning efficiency while maintaining or enhancing the plan quality.","To reduce the computational complexity of the optimization problem, we propose to subsample the set of voxels via importance sampling.","We derive a sampling distribution based on an importance score that we obtain from pre-solving an easy optimization problem involving a simplified probing objective.","By solving a reduced version of the original optimization problem using this subset, we effectively reduce the problem's size and computational demands while accounting for regions in which satisfactory dose deliveries are challenging.","In contrast to other stochastic (sub-)sampling methods, our technique only requires a single sampling step to define a reduced optimization problem.","This problem can be efficiently solved using established solvers.","Empirical experiments on open benchmark data highlight substantially reduced optimization times, up to 50 times faster than the original ones, for intensity-modulated radiation therapy (IMRT), all while upholding plan quality comparable to traditional methods.","Our approach has the potential to significantly accelerate radiation treatment planning by addressing its inherent computational challenges.","We reduce the treatment planning time by reducing the size of the optimization problem rather than improving the optimization method.","Our efforts are thus complementary to much of the previous developments."],"url":"http://arxiv.org/abs/2405.03880v1","category":"physics.med-ph"}
{"created":"2024-05-06 21:54:38","title":"Scalable Amortized GPLVMs for Single Cell Transcriptomics Data","abstract":"Dimensionality reduction is crucial for analyzing large-scale single-cell RNA-seq data. Gaussian Process Latent Variable Models (GPLVMs) offer an interpretable dimensionality reduction method, but current scalable models lack effectiveness in clustering cell types. We introduce an improved model, the amortized stochastic variational Bayesian GPLVM (BGPLVM), tailored for single-cell RNA-seq with specialized encoder, kernel, and likelihood designs. This model matches the performance of the leading single-cell variational inference (scVI) approach on synthetic and real-world COVID datasets and effectively incorporates cell-cycle and batch information to reveal more interpretable latent structures as we demonstrate on an innate immunity dataset.","sentences":["Dimensionality reduction is crucial for analyzing large-scale single-cell RNA-seq data.","Gaussian Process Latent Variable Models (GPLVMs) offer an interpretable dimensionality reduction method, but current scalable models lack effectiveness in clustering cell types.","We introduce an improved model, the amortized stochastic variational Bayesian GPLVM (BGPLVM), tailored for single-cell RNA-seq with specialized encoder, kernel, and likelihood designs.","This model matches the performance of the leading single-cell variational inference (scVI) approach on synthetic and real-world COVID datasets and effectively incorporates cell-cycle and batch information to reveal more interpretable latent structures as we demonstrate on an innate immunity dataset."],"url":"http://arxiv.org/abs/2405.03879v1","category":"stat.ML"}
{"created":"2024-05-06 21:49:29","title":"Sequence Compression Speeds Up Credit Assignment in Reinforcement Learning","abstract":"Temporal credit assignment in reinforcement learning is challenging due to delayed and stochastic outcomes. Monte Carlo targets can bridge long delays between action and consequence but lead to high-variance targets due to stochasticity. Temporal difference (TD) learning uses bootstrapping to overcome variance but introduces a bias that can only be corrected through many iterations. TD($\\lambda$) provides a mechanism to navigate this bias-variance tradeoff smoothly. Appropriately selecting $\\lambda$ can significantly improve performance. Here, we propose Chunked-TD, which uses predicted probabilities of transitions from a model for computing $\\lambda$-return targets. Unlike other model-based solutions to credit assignment, Chunked-TD is less vulnerable to model inaccuracies. Our approach is motivated by the principle of history compression and 'chunks' trajectories for conventional TD learning. Chunking with learned world models compresses near-deterministic regions of the environment-policy interaction to speed up credit assignment while still bootstrapping when necessary. We propose algorithms that can be implemented online and show that they solve some problems much faster than conventional TD($\\lambda$).","sentences":["Temporal credit assignment in reinforcement learning is challenging due to delayed and stochastic outcomes.","Monte Carlo targets can bridge long delays between action and consequence but lead to high-variance targets due to stochasticity.","Temporal difference (TD) learning uses bootstrapping to overcome variance but introduces a bias that can only be corrected through many iterations.","TD($\\lambda$) provides a mechanism to navigate this bias-variance tradeoff smoothly.","Appropriately selecting $\\lambda$ can significantly improve performance.","Here, we propose Chunked-TD, which uses predicted probabilities of transitions from a model for computing $\\lambda$-return targets.","Unlike other model-based solutions to credit assignment, Chunked-TD is less vulnerable to model inaccuracies.","Our approach is motivated by the principle of history compression and 'chunks' trajectories for conventional TD learning.","Chunking with learned world models compresses near-deterministic regions of the environment-policy interaction to speed up credit assignment while still bootstrapping when necessary.","We propose algorithms that can be implemented online and show that they solve some problems much faster than conventional TD($\\lambda$)."],"url":"http://arxiv.org/abs/2405.03878v1","category":"cs.LG"}
{"created":"2024-05-06 21:46:10","title":"Rethinking Data Shapley for Data Selection Tasks: Misleads and Merits","abstract":"Data Shapley provides a principled approach to data valuation and plays a crucial role in data-centric machine learning (ML) research. Data selection is considered a standard application of Data Shapley. However, its data selection performance has shown to be inconsistent across settings in the literature. This study aims to deepen our understanding of this phenomenon. We introduce a hypothesis testing framework and show that Data Shapley's performance can be no better than random selection without specific constraints on utility functions. We identify a class of utility functions, monotonically transformed modular functions, within which Data Shapley optimally selects data. Based on this insight, we propose a heuristic for predicting Data Shapley's effectiveness in data selection tasks. Our experiments corroborate these findings, adding new insights into when Data Shapley may or may not succeed.","sentences":["Data Shapley provides a principled approach to data valuation and plays a crucial role in data-centric machine learning (ML) research.","Data selection is considered a standard application of Data Shapley.","However, its data selection performance has shown to be inconsistent across settings in the literature.","This study aims to deepen our understanding of this phenomenon.","We introduce a hypothesis testing framework and show that Data Shapley's performance can be no better than random selection without specific constraints on utility functions.","We identify a class of utility functions, monotonically transformed modular functions, within which Data Shapley optimally selects data.","Based on this insight, we propose a heuristic for predicting Data Shapley's effectiveness in data selection tasks.","Our experiments corroborate these findings, adding new insights into when Data Shapley may or may not succeed."],"url":"http://arxiv.org/abs/2405.03875v1","category":"cs.LG"}
{"created":"2024-05-06 20:04:57","title":"Real-World Problem-Solving Class is Correlated with Higher Student Persistence in Engineering","abstract":"Student persistence in science, technology, engineering, and mathematics (STEM) has long been a focus of educational research, with both quantitative and qualitative methods being used to investigate patterns and mechanisms of attrition. Some studies have used machine learning to predict a student's likelihood to persist given measurable classroom factors and institutional data, while others have framed persistence as a function of a student's social integration in the classroom. While these methods have provided insight into broader underlying patterns of attrition in STEM, they have not investigated class structures or teaching methods that promote persistence. In this study we explore how a research-based instructional format for an introductory calculus-based physics class using real world problem-solving (RPS) was correlated with higher persistence for students at a large research-intensive university. We found that the one-year persistence rates for the RPS course were 74% (fall semester) and 90% (spring semester), while the lecture-based class had a persistence rate of 64% and 78%, respectively. In spring, the RPS persistence rate was significantly higher (p=0.037). The RPS also had higher final grades and larger learning gains than the lecture-based class despite lower scores on a physics diagnostic test. We also note that the higher rates of persistence were not completely explained by higher final grades. This study motivates future work to understand the structural mechanisms that promote student persistence in introductory physics courses.","sentences":["Student persistence in science, technology, engineering, and mathematics (STEM) has long been a focus of educational research, with both quantitative and qualitative methods being used to investigate patterns and mechanisms of attrition.","Some studies have used machine learning to predict a student's likelihood to persist given measurable classroom factors and institutional data, while others have framed persistence as a function of a student's social integration in the classroom.","While these methods have provided insight into broader underlying patterns of attrition in STEM, they have not investigated class structures or teaching methods that promote persistence.","In this study we explore how a research-based instructional format for an introductory calculus-based physics class using real world problem-solving (RPS) was correlated with higher persistence for students at a large research-intensive university.","We found that the one-year persistence rates for the RPS course were 74% (fall semester) and 90% (spring semester), while the lecture-based class had a persistence rate of 64% and 78%, respectively.","In spring, the RPS persistence rate was significantly higher (p=0.037).","The RPS also had higher final grades and larger learning gains than the lecture-based class despite lower scores on a physics diagnostic test.","We also note that the higher rates of persistence were not completely explained by higher final grades.","This study motivates future work to understand the structural mechanisms that promote student persistence in introductory physics courses."],"url":"http://arxiv.org/abs/2405.03822v1","category":"physics.ed-ph"}
{"created":"2024-05-06 19:31:25","title":"UniGen: Unified Modeling of Initial Agent States and Trajectories for Generating Autonomous Driving Scenarios","abstract":"This paper introduces UniGen, a novel approach to generating new traffic scenarios for evaluating and improving autonomous driving software through simulation. Our approach models all driving scenario elements in a unified model: the position of new agents, their initial state, and their future motion trajectories. By predicting the distributions of all these variables from a shared global scenario embedding, we ensure that the final generated scenario is fully conditioned on all available context in the existing scene. Our unified modeling approach, combined with autoregressive agent injection, conditions the placement and motion trajectory of every new agent on all existing agents and their trajectories, leading to realistic scenarios with low collision rates. Our experimental results show that UniGen outperforms prior state of the art on the Waymo Open Motion Dataset.","sentences":["This paper introduces UniGen, a novel approach to generating new traffic scenarios for evaluating and improving autonomous driving software through simulation.","Our approach models all driving scenario elements in a unified model: the position of new agents, their initial state, and their future motion trajectories.","By predicting the distributions of all these variables from a shared global scenario embedding, we ensure that the final generated scenario is fully conditioned on all available context in the existing scene.","Our unified modeling approach, combined with autoregressive agent injection, conditions the placement and motion trajectory of every new agent on all existing agents and their trajectories, leading to realistic scenarios with low collision rates.","Our experimental results show that UniGen outperforms prior state of the art on the Waymo Open Motion Dataset."],"url":"http://arxiv.org/abs/2405.03807v1","category":"cs.RO"}
{"created":"2024-05-07 17:57:15","title":"Comparing Ways of Obtaining Candidate Orderings from Approval Ballots","abstract":"To understand and summarize approval preferences and other binary evaluation data, it is useful to order the items on an axis which explains the data. In a political election using approval voting, this could be an ideological left-right axis such that each voter approves adjacent candidates, an analogue of single-peakedness. In a perfect axis, every approval set would be an interval, which is usually not possible, and so we need to choose an axis that gets closest to this ideal. The literature has developed algorithms for optimizing several objective functions (e.g., minimize the number of added approvals needed to get a perfect axis), but provides little help with choosing among different objectives. In this paper, we take a social choice approach and compare 5 different axis selection rules axiomatically, by studying the properties they satisfy. We establish some impossibility theorems, and characterize (within the class of scoring rules) the rule that chooses the axes that maximize the number of votes that form intervals, using the axioms of ballot monotonicity and resistance to cloning. Finally, we study the behavior of the rules on data from French election surveys, on the votes of justices of the US Supreme Court, and on synthetic data.","sentences":["To understand and summarize approval preferences and other binary evaluation data, it is useful to order the items on an axis which explains the data.","In a political election using approval voting, this could be an ideological left-right axis such that each voter approves adjacent candidates, an analogue of single-peakedness.","In a perfect axis, every approval set would be an interval, which is usually not possible, and so we need to choose an axis that gets closest to this ideal.","The literature has developed algorithms for optimizing several objective functions (e.g., minimize the number of added approvals needed to get a perfect axis), but provides little help with choosing among different objectives.","In this paper, we take a social choice approach and compare 5 different axis selection rules axiomatically, by studying the properties they satisfy.","We establish some impossibility theorems, and characterize (within the class of scoring rules) the rule that chooses the axes that maximize the number of votes that form intervals, using the axioms of ballot monotonicity and resistance to cloning.","Finally, we study the behavior of the rules on data from French election surveys, on the votes of justices of the US Supreme Court, and on synthetic data."],"url":"http://arxiv.org/abs/2405.04525v1","category":"cs.GT"}
{"created":"2024-05-07 16:54:16","title":"Quantum Rabin oblivious transfer using two pure states","abstract":"Oblivious transfer between two untrusting parties is an important primitive in cryptography. There are different variants of oblivious transfer. In Rabin oblivious transfer, the sender Alice holds a bit, and the receiver Bob either obtains the bit, or obtains no information with probability $p_?$. Alice should not know whether or not Bob obtained the bit. We examine a quantum Rabin oblivious transfer protocol that uses two pure states. Investigating different cheating scenarios for the sender and for the receiver, we determine optimal cheating probabilities in each case. Comparing the quantum Rabin oblivious transfer protocol to classical Rabin oblivious transfer protocols, we show that the quantum protocol outperforms classical protocols which do not use a third party, for some values of $p_?$.","sentences":["Oblivious transfer between two untrusting parties is an important primitive in cryptography.","There are different variants of oblivious transfer.","In Rabin oblivious transfer, the sender Alice holds a bit, and the receiver Bob either obtains the bit, or obtains no information with probability $p_?$. Alice should not know whether or not Bob obtained the bit.","We examine a quantum Rabin oblivious transfer protocol that uses two pure states.","Investigating different cheating scenarios for the sender and for the receiver, we determine optimal cheating probabilities in each case.","Comparing the quantum Rabin oblivious transfer protocol to classical Rabin oblivious transfer protocols, we show that the quantum protocol outperforms classical protocols which do not use a third party, for some values of $p_?$."],"url":"http://arxiv.org/abs/2405.04486v1","category":"quant-ph"}
{"created":"2024-05-07 16:26:24","title":"A Constructive Winning Maker Strategy in the Maker-Breaker $C_4$-Game","abstract":"Maker-Breaker subgraph games are among the most famous combinatorial games. For given $n,q \\in \\mathbb{N}$ and a subgraph $C$ of the complete graph $K_n$, the two players, called Maker and Breaker, alternately claim edges of $K_n$. In each round of the game Maker claims one edge and Breaker is allowed to claim up to $q$ edges. If Maker is able to claim all edges of a copy of $C$, he wins the game. Otherwise Breaker wins. In this work we introduce the first constructive strategy for Maker for the $C_4$-Maker-Breaker game and show that he can win the game if $q < 0.16 n^{2/3}$. According to the theorem of Bednarska and Luczak (2000) $n^{2/3}$ is asymptotically optimal for this game, but the constant given there for a random Maker strategy is magnitudes apart from our constant 0.16.","sentences":["Maker-Breaker subgraph games are among the most famous combinatorial games.","For given $n,q \\in \\mathbb{N}$ and a subgraph $C$ of the complete graph $K_n$, the two players, called Maker and Breaker, alternately claim edges of $K_n$. In each round of the game Maker claims one edge and Breaker is allowed to claim up to $q$ edges.","If Maker is able to claim all edges of a copy of $C$, he wins the game.","Otherwise Breaker wins.","In this work we introduce the first constructive strategy for Maker for the $C_4$-Maker-Breaker game and show that he can win the game if $q < 0.16 n^{2/3}$.","According to the theorem of Bednarska and Luczak (2000) $n^{2/3}$ is asymptotically optimal for this game, but the constant given there for a random Maker strategy is magnitudes apart from our constant 0.16."],"url":"http://arxiv.org/abs/2405.04462v1","category":"math.CO"}
{"created":"2024-05-07 11:17:01","title":"Exact calculation of the probabilities of rare events in cluster-cluster aggregation","abstract":"We develop an action formalism to calculate probabilities of rare events in cluster-cluster aggregation for arbitrary collision kernels and establish a pathwise large deviation principle with total mass being the rate. As an application, the rate function for the number of surviving particles as well as the optimal evolution trajectory are calculated exactly for the constant, sum and product kernels. For the product kernel, we argue that the second derivative of the rate function has a discontinuity. The theoretical results agree with simulations tailored to the calculation of rare events.","sentences":["We develop an action formalism to calculate probabilities of rare events in cluster-cluster aggregation for arbitrary collision kernels and establish a pathwise large deviation principle with total mass being the rate.","As an application, the rate function for the number of surviving particles as well as the optimal evolution trajectory are calculated exactly for the constant, sum and product kernels.","For the product kernel, we argue that the second derivative of the rate function has a discontinuity.","The theoretical results agree with simulations tailored to the calculation of rare events."],"url":"http://arxiv.org/abs/2405.04201v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-07 06:29:52","title":"DMOFC: Discrimination Metric-Optimized Feature Compression","abstract":"Feature compression, as an important branch of video coding for machines (VCM), has attracted significant attention and exploration. However, the existing methods mainly focus on intra-feature similarity, such as the Mean Squared Error (MSE) between the reconstructed and original features, while neglecting the importance of inter-feature relationships. In this paper, we analyze the inter-feature relationships, focusing on feature discriminability in machine vision and underscoring its significance in feature compression. To maintain the feature discriminability of reconstructed features, we introduce a discrimination metric for feature compression. The discrimination metric is designed to ensure that the distance between features of the same category is smaller than the distance between features of different categories. Furthermore, we explore the relationship between the discrimination metric and the discriminability of the original features. Experimental results confirm the effectiveness of the proposed discrimination metric and reveal there exists a trade-off between the discrimination metric and the discriminability of the original features.","sentences":["Feature compression, as an important branch of video coding for machines (VCM), has attracted significant attention and exploration.","However, the existing methods mainly focus on intra-feature similarity, such as the Mean Squared Error (MSE) between the reconstructed and original features, while neglecting the importance of inter-feature relationships.","In this paper, we analyze the inter-feature relationships, focusing on feature discriminability in machine vision and underscoring its significance in feature compression.","To maintain the feature discriminability of reconstructed features, we introduce a discrimination metric for feature compression.","The discrimination metric is designed to ensure that the distance between features of the same category is smaller than the distance between features of different categories.","Furthermore, we explore the relationship between the discrimination metric and the discriminability of the original features.","Experimental results confirm the effectiveness of the proposed discrimination metric and reveal there exists a trade-off between the discrimination metric and the discriminability of the original features."],"url":"http://arxiv.org/abs/2405.04044v1","category":"cs.CV"}
{"created":"2024-05-07 02:17:34","title":"Entanglement swapping via lossy channels using photon-number-encoded states","abstract":"Entanglement shared between distant parties is a key resource in quantum networks. However, photon losses in quantum channels significantly reduce the success probability of entanglement sharing, which scales quadratically with the channel transmission. Quantum repeaters using entanglement swapping can mitigate this effect, but usually require high-performance photonic quantum memories to synchronize photonic qubits. In this work, we theoretically and experimentally investigate an entanglement swapping protocol using photon-number-encoded states that can effectively alleviate quantum channel losses without requiring photonic quantum memories. We demonstrate that the protocol exhibits a success probability scaling linearly with the channel transmission. Furthermore, we show that while unbalanced channel losses can degrade the shared entanglement, this effect can be compensated by optimally adjusting the initial entangled states. Our results highlight the potential of photon-number encoding for realizing robust entanglement distribution in lossy quantum networks.","sentences":["Entanglement shared between distant parties is a key resource in quantum networks.","However, photon losses in quantum channels significantly reduce the success probability of entanglement sharing, which scales quadratically with the channel transmission.","Quantum repeaters using entanglement swapping can mitigate this effect, but usually require high-performance photonic quantum memories to synchronize photonic qubits.","In this work, we theoretically and experimentally investigate an entanglement swapping protocol using photon-number-encoded states that can effectively alleviate quantum channel losses without requiring photonic quantum memories.","We demonstrate that the protocol exhibits a success probability scaling linearly with the channel transmission.","Furthermore, we show that while unbalanced channel losses can degrade the shared entanglement, this effect can be compensated by optimally adjusting the initial entangled states.","Our results highlight the potential of photon-number encoding for realizing robust entanglement distribution in lossy quantum networks."],"url":"http://arxiv.org/abs/2405.03951v1","category":"quant-ph"}
{"created":"2024-05-07 01:08:32","title":"Generalized Nash equilibrium problems with quasi-linear constraints","abstract":"We study generalized Nash equilibrium problems (GNEPs) such that objectives are polynomial functions, and each player's constraints are linear in their own strategy. For such GNEPs, the KKT sets can be represented as unions of simpler sets by Carath\\'{e}odory's theorem. We give a convenient representation for KKT sets using partial Lagrange multiplier expressions. This produces a set of branch polynomial optimization problems, which can be efficiently solved by Moment-SOS relaxations. By doing this, we can compute all generalized Nash equilibria or detect their nonexistence. Numerical experiments are also provided to demonstrate the computational efficiency.","sentences":["We study generalized Nash equilibrium problems (GNEPs) such that objectives are polynomial functions, and each player's constraints are linear in their own strategy.","For such GNEPs, the KKT sets can be represented as unions of simpler sets by Carath\\'{e}odory's theorem.","We give a convenient representation for KKT sets using partial Lagrange multiplier expressions.","This produces a set of branch polynomial optimization problems, which can be efficiently solved by Moment-SOS relaxations.","By doing this, we can compute all generalized Nash equilibria or detect their nonexistence.","Numerical experiments are also provided to demonstrate the computational efficiency."],"url":"http://arxiv.org/abs/2405.03926v1","category":"math.OC"}
{"created":"2024-05-07 00:38:01","title":"A design method of an ultra wideband and easy-to-array Magic-T: A 6-14 GHz scaled model for a mm/submm camera","abstract":"We established a design method for a Magic-T with a single-layer dielectric/metal structure suitable for both wideband and multi-element applications for millimeter and submillimeter wave imaging observations. The design method was applied to a Magic-T with a coupled-line, stubs, and single-stage impedance transformers in a frequency-scaled model (6-14 GHz) that is relatively easy to demonstrate through manufacturing and evaluation. The major problem is that using the conventional perfect matching condition for a coupled-line alone produces an impractically large width coplanar coupled-line (CPCL) to satisfy the desired bandwidth ratio. In our study, by removing this constraint and optimizing impedances utilizing a circuit simulator with high computation speed, we found a solution with a $\\sim$ 180 $\\rm \\mu$m wide CPCL, which is approximately an order of magnitude smaller than the conventional analytical solution. Furthermore, considering the effect of transition discontinuities in the transmission lines, we optimized the line length and obtained a design solution with return loss < -20 dB, amplitude imbalance < 0.1 dB, and phase imbalance < 0.5$^\\circ$ from 6.1 GHz to 14.1 GHz.","sentences":["We established a design method for a Magic-T with a single-layer dielectric/metal structure suitable for both wideband and multi-element applications for millimeter and submillimeter wave imaging observations.","The design method was applied to a Magic-T with a coupled-line, stubs, and single-stage impedance transformers in a frequency-scaled model (6-14 GHz) that is relatively easy to demonstrate through manufacturing and evaluation.","The major problem is that using the conventional perfect matching condition for a coupled-line alone produces an impractically large width coplanar coupled-line (CPCL) to satisfy the desired bandwidth ratio.","In our study, by removing this constraint and optimizing impedances utilizing a circuit simulator with high computation speed, we found a solution with a $\\sim$ 180 $\\rm \\mu$m wide CPCL, which is approximately an order of magnitude smaller than the conventional analytical solution.","Furthermore, considering the effect of transition discontinuities in the transmission lines, we optimized the line length and obtained a design solution with return loss < -20 dB, amplitude imbalance < 0.1 dB, and phase imbalance < 0.5$^\\circ$ from 6.1 GHz to 14.1 GHz."],"url":"http://arxiv.org/abs/2405.03919v1","category":"astro-ph.IM"}
{"created":"2024-05-07 00:24:54","title":"Robust Optimization for Spot Scanning Proton Therapy based on Dose-Linear Energy Transfer (LET) Volume Constraints","abstract":"Purpose: Historically, spot scanning proton therapy (SSPT) treatment planning utilizes dose volume constraints and linear-energy-transfer (LET) volume constraints separately to balance tumor control and organs-at-risk (OARs) protection. We propose a novel dose-LET volume constraint (DLVC)-based robust optimization (DLVCRO) method for SSPT in treating prostate cancer to obtain a desirable joint dose and LET distribution to minimize adverse events (AEs).   Methods: DLVCRO treats DLVC as soft constraints controlling the joint distribution of dose and LET. Ten prostate cancer patients were included with rectum and bladder as OARs. DLVCRO was compared with the conventional robust optimization (RO) method using the worst-case analysis method. Besides the dose-volume histogram (DVH) indices, the analogous LETVH and extra-biological-dose (xBD)-volume histogram indices were also used. The Wilcoxon signed rank test was used to measure statistical significance.   Results: In nominal scenario, DLVCRO significantly improved dose, LET and xBD distributions to protect OARs (rectum: V70Gy: 3.07\\% vs. 2.90\\%, p = .0063, RO vs. DLVCRO; $\\text{LET}_{\\max}$ (keV/um): 11.53 vs. 9.44, p = .0101; $\\text{xBD}_{\\max}$ (Gy$\\cdot$keV/um): 420.55 vs. 398.79, p = .0086; bladder: V65Gy: 4.82\\% vs. 4.61\\%, p = .0032; $\\text{LET}_{\\max}$ 8.97 vs. 7.51, p = .0047; $\\text{xBD}_{\\max}$ 490.11 vs. 476.71, p = .0641). The physical dose distributions in targets are comparable (D2%: 98.57\\% vs. 98.39\\%; p = .0805; CTV D2% - D98%: 7.10\\% vs. 7.75\\%, p = .4624). In the worst-case scenario, DLVCRO robustly enhanced OAR while maintaining the similar plan robustness in target dose coverage and homogeneity.   Conclusion: DLVCRO upgrades 2D DVH-based to 3D DLVH-based treatment planning to adjust dose/LET distributions simultaneously and robustly. DLVCRO is potentially a powerful tool to improve patient outcomes in SSPT.","sentences":["Purpose: Historically, spot scanning proton therapy (SSPT) treatment planning utilizes dose volume constraints and linear-energy-transfer (LET) volume constraints separately to balance tumor control and organs-at-risk (OARs) protection.","We propose a novel dose-LET volume constraint (DLVC)-based robust optimization (DLVCRO) method for SSPT in treating prostate cancer to obtain a desirable joint dose and LET distribution to minimize adverse events (AEs).   ","Methods: DLVCRO treats DLVC as soft constraints controlling the joint distribution of dose and LET.","Ten prostate cancer patients were included with rectum and bladder as OARs.","DLVCRO was compared with the conventional robust optimization (RO) method using the worst-case analysis method.","Besides the dose-volume histogram (DVH) indices, the analogous LETVH and extra-biological-dose (xBD)-volume histogram indices were also used.","The Wilcoxon signed rank test was used to measure statistical significance.   ","Results:","In nominal scenario, DLVCRO significantly improved dose, LET and xBD distributions to protect OARs (rectum: V70Gy:","3.07\\% vs. 2.90\\%, p = .0063, RO vs. DLVCRO; $\\text{LET}_{\\max}$ (keV/um): 11.53 vs. 9.44, p = .0101; $\\text{xBD}_{\\max}$ (Gy$\\cdot$keV/um): 420.55 vs. 398.79, p = .0086; bladder: V65Gy: 4.82\\% vs. 4.61\\%, p = .0032; $\\text{LET}_{\\max}$ 8.97 vs. 7.51, p = .0047; $\\text{xBD}_{\\max}$ 490.11 vs. 476.71, p = .0641).","The physical dose distributions in targets are comparable (D2%: 98.57\\% vs. 98.39\\%; p = .0805; CTV D2% - D98%: 7.10\\% vs. 7.75\\%, p = .4624).","In the worst-case scenario, DLVCRO robustly enhanced OAR while maintaining the similar plan robustness in target dose coverage and homogeneity.   ","Conclusion: DLVCRO upgrades 2D DVH-based to 3D DLVH-based treatment planning to adjust dose/LET distributions simultaneously and robustly.","DLVCRO is potentially a powerful tool to improve patient outcomes in SSPT."],"url":"http://arxiv.org/abs/2405.03916v1","category":"physics.med-ph"}
{"created":"2024-05-06 23:52:20","title":"Deterministic Expander Routing: Faster and More Versatile","abstract":"We consider the expander routing problem formulated by Ghaffari, Kuhn, and Su (PODC 2017), where the goal is to route all the tokens to their destinations given that each vertex is the source and the destination of at most $\\deg(v)$ tokens. They developed $\\textit{randomized algorithms}$ that solve this problem in $\\text{poly}(\\phi^{-1}) \\cdot 2^{O(\\sqrt{\\log n \\log \\log n})}$ rounds in the $\\textsf{CONGEST}$ model, where $\\phi$ is the conductance of the graph. Later, Ghaffari and Li (DISC 2018) gave an improved algorithm. However, both algorithms are randomized, which means that all the resulting applications are also randomized. Recently, Chang and Saranurak (FOCS 2020) gave a deterministic algorithm that solves an expander routing instance in $2^{O(\\log^{2/3} n \\cdot \\log^{1/3} \\log n)}$ rounds. The deterministic algorithm is less efficient and does not allow preprocessing/query tradeoffs, which precludes the de-randomization of algorithms that require this feature, such as the $k$-clique enumeration algorithm in general graphs.   The main contribution of our work is a new deterministic expander routing algorithm that not only matches the randomized bound of [GKS 2017] but also allows preprocessing/query tradeoffs. Our algorithm solves a single instance of routing query in $2^{{O}(\\sqrt{\\log n \\cdot \\log \\log n})}$ rounds. Our algorithm achieves the following preprocessing and query tradeoffs: For $0 < \\epsilon < 1$, we can answer every routing query in $\\log^{O(1/\\epsilon)} n$ rounds at the cost of a $(n^{O(\\epsilon)} + \\log^{O(1/\\epsilon)} n)$-round preprocessing procedure. Combining this with the approach of Censor-Hillel, Leitersdorf, and Vulakh (PODC 2022), we obtain a near-optimal $\\tilde{O}(n^{1-2/k})$-round deterministic algorithm for $k$-clique enumeration in general graphs, improving the previous state-of-the-art $n^{1-2/k+o(1)}$.","sentences":["We consider the expander routing problem formulated by Ghaffari, Kuhn, and Su (PODC 2017), where the goal is to route all the tokens to their destinations given that each vertex is the source and the destination of at most $\\deg(v)$ tokens.","They developed $\\textit{randomized algorithms}$ that solve this problem in $\\text{poly}(\\phi^{-1})","\\cdot 2^{O(\\sqrt{\\log n \\log \\log n})}$ rounds in the $\\textsf{CONGEST}$ model, where $\\phi$ is the conductance of the graph.","Later, Ghaffari and Li (DISC 2018) gave an improved algorithm.","However, both algorithms are randomized, which means that all the resulting applications are also randomized.","Recently, Chang and Saranurak (FOCS 2020) gave a deterministic algorithm that solves an expander routing instance in $2^{O(\\log^{2/3} n \\cdot \\log^{1/3} \\log n)}$ rounds.","The deterministic algorithm is less efficient and does not allow preprocessing/query tradeoffs, which precludes the de-randomization of algorithms that require this feature, such as the $k$-clique enumeration algorithm in general graphs.   ","The main contribution of our work is a new deterministic expander routing algorithm that not only matches the randomized bound of [GKS 2017] but also allows preprocessing/query tradeoffs.","Our algorithm solves a single instance of routing query in $2^{{O}(\\sqrt{\\log n \\cdot \\log \\log n})}$ rounds.","Our algorithm achieves the following preprocessing and query tradeoffs: For $0 < \\epsilon < 1$, we can answer every routing query in $\\log^{O(1/\\epsilon)} n$ rounds at the cost of a $(n^{O(\\epsilon)} + \\log^{O(1/\\epsilon)} n)$-round preprocessing procedure.","Combining this with the approach of Censor-Hillel, Leitersdorf, and Vulakh (PODC 2022), we obtain a near-optimal $\\tilde{O}(n^{1-2/k})$-round deterministic algorithm for $k$-clique enumeration in general graphs, improving the previous state-of-the-art $n^{1-2/k+o(1)}$."],"url":"http://arxiv.org/abs/2405.03908v1","category":"cs.DC"}
{"created":"2024-05-06 23:01:33","title":"Quantum sensing in the fractional Fourier domain","abstract":"Certain quantum sensing protocols rely on qubits that are initialized, coherently driven in the presence of a stimulus to be measured, then read out. Most widely employed pulse sequences used to drive sensing qubits act locally in either the time or frequency domain. We introduce a generalized set of sequences that effect a measurement in any fractional Fourier domain, i.e. along a linear trajectory of arbitrary angle through the time-frequency plane. Using an ensemble of nitrogen-vacancy centers we experimentally demonstrate advantages in sensing signals with time-varying spectra.","sentences":["Certain quantum sensing protocols rely on qubits that are initialized, coherently driven in the presence of a stimulus to be measured, then read out.","Most widely employed pulse sequences used to drive sensing qubits act locally in either the time or frequency domain.","We introduce a generalized set of sequences that effect a measurement in any fractional Fourier domain, i.e. along a linear trajectory of arbitrary angle through the time-frequency plane.","Using an ensemble of nitrogen-vacancy centers we experimentally demonstrate advantages in sensing signals with time-varying spectra."],"url":"http://arxiv.org/abs/2405.03896v1","category":"quant-ph"}
{"created":"2024-05-06 21:42:16","title":"Non-locality and Spillover Effects of Residential Flood Damage on Community Recovery: Insights from High-resolution Flood Claim and Mobility Data","abstract":"Examining the relationship between vulnerability of the built environment and community recovery is crucial for understanding disaster resilience. Yet, this relationship is rather neglected in the existing literature due to previous limitations in the availability of empirical datasets needed for such analysis. In this study, we combine fine-resolution flood damage claims data (composed of both insured and uninsured losses) and human mobility data (composed of millions of movement trajectories) during the 2017 Hurricane Harvey in Harris County, Texas, to specify the extent to which vulnerability of the built environment (i.e., flood property damage) affects community recovery (based on the speed of human mobility recovery) locally and regionally. We examine this relationship using a spatial lag, spatial reach, and spatial decay models to measure the extent of spillover effects of residential damage on community recovery. The findings show that: first, the severity of residential damage significantly affects the speed of community recovery. A greater extent of residential damage suppresses community recovery not only locally but also in the surrounding areas. Second, the spatial spillover effect of residential damage on community recovery speed decays with distance from the highly damaged areas. Third, spatial areas display heterogeneous spatial decay coefficients, which are associated with urban structure features such as the density of points-of-interest facilities and roads. These findings provide a novel data-driven characterization of the spatial diffusion of residential flood damage effects on community recovery and move us closer to a better understanding of complex spatial processes that shape community resilience to hazards. This study also provides valuable insights for emergency managers and public officials seeking to mitigate the non-local effects of residential damage.","sentences":["Examining the relationship between vulnerability of the built environment and community recovery is crucial for understanding disaster resilience.","Yet, this relationship is rather neglected in the existing literature due to previous limitations in the availability of empirical datasets needed for such analysis.","In this study, we combine fine-resolution flood damage claims data (composed of both insured and uninsured losses) and human mobility data (composed of millions of movement trajectories) during the 2017 Hurricane Harvey in Harris County, Texas, to specify the extent to which vulnerability of the built environment (i.e., flood property damage) affects community recovery (based on the speed of human mobility recovery) locally and regionally.","We examine this relationship using a spatial lag, spatial reach, and spatial decay models to measure the extent of spillover effects of residential damage on community recovery.","The findings show that: first, the severity of residential damage significantly affects the speed of community recovery.","A greater extent of residential damage suppresses community recovery not only locally but also in the surrounding areas.","Second, the spatial spillover effect of residential damage on community recovery speed decays with distance from the highly damaged areas.","Third, spatial areas display heterogeneous spatial decay coefficients, which are associated with urban structure features such as the density of points-of-interest facilities and roads.","These findings provide a novel data-driven characterization of the spatial diffusion of residential flood damage effects on community recovery and move us closer to a better understanding of complex spatial processes that shape community resilience to hazards.","This study also provides valuable insights for emergency managers and public officials seeking to mitigate the non-local effects of residential damage."],"url":"http://arxiv.org/abs/2405.03874v1","category":"stat.AP"}
{"created":"2024-05-06 20:59:53","title":"Lifting Directional Fields to Minimal Sections","abstract":"Directional fields, including unit vector, line, and cross fields, are essential tools in the geometry processing toolkit. The topology of directional fields is characterized by their singularities. While singularities play an important role in downstream applications such as meshing, existing methods for computing directional fields either require them to be specified in advance, ignore them altogether, or treat them as zeros of a relaxed field. While fields are ill-defined at their singularities, the graphs of directional fields with singularities are well-defined surfaces in a circle bundle. By lifting optimization of fields to optimization over their graphs, we can exploit a natural convex relaxation to a minimal section problem over the space of currents in the bundle. This relaxation treats singularities as first-class citizens, expressing the relationship between fields and singularities as an explicit boundary condition. As curvature frustrates finite element discretization of the bundle, we devise a hybrid spectral method for representing and optimizing minimal sections. Our method supports field optimization on both flat and curved domains and enables more precise control over singularity placement.","sentences":["Directional fields, including unit vector, line, and cross fields, are essential tools in the geometry processing toolkit.","The topology of directional fields is characterized by their singularities.","While singularities play an important role in downstream applications such as meshing, existing methods for computing directional fields either require them to be specified in advance, ignore them altogether, or treat them as zeros of a relaxed field.","While fields are ill-defined at their singularities, the graphs of directional fields with singularities are well-defined surfaces in a circle bundle.","By lifting optimization of fields to optimization over their graphs, we can exploit a natural convex relaxation to a minimal section problem over the space of currents in the bundle.","This relaxation treats singularities as first-class citizens, expressing the relationship between fields and singularities as an explicit boundary condition.","As curvature frustrates finite element discretization of the bundle, we devise a hybrid spectral method for representing and optimizing minimal sections.","Our method supports field optimization on both flat and curved domains and enables more precise control over singularity placement."],"url":"http://arxiv.org/abs/2405.03853v1","category":"cs.GR"}
{"created":"2024-05-06 20:34:18","title":"Optimized Tone Reservation for Opportunistic OFDM Communications and Sensing","abstract":"We consider the problem of peak-to-average power ratio (PAPR) reduction in orthogonal frequency division multiplexing (OFDM) systems via optimized sparsification of tone reservation (TR). In particular, we propose a novel TR optimization method in which the minimum number of effectively used peak-reserved tones (PRTs) required to satisfy a prescribed PAPR level is found, leaving the remaining PRTs free to be opportunistically utilized by other functionalities, such as joint communication and sensing (JCAS), index modulation (IM), cognitive radio (CR) and others. The proposed method relies on an l0 norm regularization approach to penalize the number of PRTs, leading to a problem convexized via fractional programming (FP), whose solution is shown to ensure that the prescribed PAPR is achieved with high probability with a smaller number of PRTs than state of the art (SotA) methods. The contribution can be seen as a mechanism to enable the opportunistic integration of adjacent functionalities into existing OFDM-based systems.","sentences":["We consider the problem of peak-to-average power ratio (PAPR) reduction in orthogonal frequency division multiplexing (OFDM) systems via optimized sparsification of tone reservation (TR).","In particular, we propose a novel TR optimization method in which the minimum number of effectively used peak-reserved tones (PRTs) required to satisfy a prescribed PAPR level is found, leaving the remaining PRTs free to be opportunistically utilized by other functionalities, such as joint communication and sensing (JCAS), index modulation (IM), cognitive radio (CR) and others.","The proposed method relies on an l0 norm regularization approach to penalize the number of PRTs, leading to a problem convexized via fractional programming (FP), whose solution is shown to ensure that the prescribed PAPR is achieved with high probability with a smaller number of PRTs than state of the art (SotA) methods.","The contribution can be seen as a mechanism to enable the opportunistic integration of adjacent functionalities into existing OFDM-based systems."],"url":"http://arxiv.org/abs/2405.03833v1","category":"eess.SP"}
{"created":"2024-05-06 19:54:49","title":"Search for joint multimessenger signals from potential Galactic PeVatrons with HAWC and IceCube","abstract":"Galactic PeVatrons are sources that can accelerate cosmic rays to PeV energies. The high-energy cosmic rays are expected to interact with the surrounding ambient material or radiation, resulting in the production of gamma rays and neutrinos. To optimize for the detection of such associated production of gamma rays and neutrinos for a given source morphology and spectrum, a multi-messenger analysis that combines gamma rays and neutrinos is required. In this study, we use the Multi-Mission Maximum Likelihood framework (3ML) with IceCube Maximum Likelihood Analysis software (i3mla) and HAWC Accelerated Likelihood (HAL) to search for a correlation between 22 known gamma-ray sources from the third HAWC gamma-ray catalog and 14 years of IceCube track-like data. No significant neutrino emission from the direction of the HAWC sources was found. We report the best-fit gamma-ray model and 90% CL neutrino flux limit from the 22 sources. From the neutrino flux limit, we conclude that the gamma-ray emission from five of the sources can not be produced purely from hadronic interactions. We report the limit for the fraction of gamma rays produced by hadronic interactions for these five sources.","sentences":["Galactic PeVatrons are sources that can accelerate cosmic rays to PeV energies.","The high-energy cosmic rays are expected to interact with the surrounding ambient material or radiation, resulting in the production of gamma rays and neutrinos.","To optimize for the detection of such associated production of gamma rays and neutrinos for a given source morphology and spectrum, a multi-messenger analysis that combines gamma rays and neutrinos is required.","In this study, we use the Multi-Mission Maximum Likelihood framework (3ML) with IceCube Maximum Likelihood Analysis software (i3mla) and HAWC Accelerated Likelihood (HAL) to search for a correlation between 22 known gamma-ray sources from the third HAWC gamma-ray catalog and 14 years of IceCube track-like data.","No significant neutrino emission from the direction of the HAWC sources was found.","We report the best-fit gamma-ray model and 90% CL neutrino flux limit from the 22 sources.","From the neutrino flux limit, we conclude that the gamma-ray emission from five of the sources can not be produced purely from hadronic interactions.","We report the limit for the fraction of gamma rays produced by hadronic interactions for these five sources."],"url":"http://arxiv.org/abs/2405.03817v1","category":"astro-ph.HE"}
{"created":"2024-05-06 19:20:32","title":"EPOC: A Novel Pulse Generation Framework Incorporating Advanced Synthesis Techniques for Quantum Circuits","abstract":"In this paper we propose EPOC, an efficient pulse generation framework for quantum circuits that combines ZX-Calculus, circuit partitioning, and circuit synthesis to accelerate pulse generation. Unlike previous works that focus on generating pulses from unitary matrices without exploring equivalent representations, EPOC employs a finer granularity approach by grouping quantum gates and decomposing the resulting unitary matrices into smaller ones using synthesis techniques. This enables increased parallelism and decreased latency in quantum pulses. EPOC also continuously optimizes the circuit by identifying equivalent representations, leading to further reductions in circuit latency while minimizing the computational overhead associated with quantum optimal control. We introduce circuit synthesis into the workflow of quantum optimal control for the first time and achieve a 31.74% reduction in latency compared to previous work and a 76.80% reduction compared to the gate-based method for creating pulses. The approach demonstrates the potential for significant performance improvements in quantum circuits while minimizing computational overhead.","sentences":["In this paper we propose EPOC, an efficient pulse generation framework for quantum circuits that combines ZX-Calculus, circuit partitioning, and circuit synthesis to accelerate pulse generation.","Unlike previous works that focus on generating pulses from unitary matrices without exploring equivalent representations, EPOC employs a finer granularity approach by grouping quantum gates and decomposing the resulting unitary matrices into smaller ones using synthesis techniques.","This enables increased parallelism and decreased latency in quantum pulses.","EPOC also continuously optimizes the circuit by identifying equivalent representations, leading to further reductions in circuit latency while minimizing the computational overhead associated with quantum optimal control.","We introduce circuit synthesis into the workflow of quantum optimal control for the first time and achieve a 31.74% reduction in latency compared to previous work and a 76.80% reduction compared to the gate-based method for creating pulses.","The approach demonstrates the potential for significant performance improvements in quantum circuits while minimizing computational overhead."],"url":"http://arxiv.org/abs/2405.03804v1","category":"quant-ph"}
{"created":"2024-05-06 19:19:20","title":"MoDiPO: text-to-motion alignment via AI-feedback-driven Direct Preference Optimization","abstract":"Diffusion Models have revolutionized the field of human motion generation by offering exceptional generation quality and fine-grained controllability through natural language conditioning. Their inherent stochasticity, that is the ability to generate various outputs from a single input, is key to their success. However, this diversity should not be unrestricted, as it may lead to unlikely generations. Instead, it should be confined within the boundaries of text-aligned and realistic generations. To address this issue, we propose MoDiPO (Motion Diffusion DPO), a novel methodology that leverages Direct Preference Optimization (DPO) to align text-to-motion models. We streamline the laborious and expensive process of gathering human preferences needed in DPO by leveraging AI feedback instead. This enables us to experiment with novel DPO strategies, using both online and offline generated motion-preference pairs. To foster future research we contribute with a motion-preference dataset which we dub Pick-a-Move. We demonstrate, both qualitatively and quantitatively, that our proposed method yields significantly more realistic motions. In particular, MoDiPO substantially improves Frechet Inception Distance (FID) while retaining the same RPrecision and Multi-Modality performances.","sentences":["Diffusion Models have revolutionized the field of human motion generation by offering exceptional generation quality and fine-grained controllability through natural language conditioning.","Their inherent stochasticity, that is the ability to generate various outputs from a single input, is key to their success.","However, this diversity should not be unrestricted, as it may lead to unlikely generations.","Instead, it should be confined within the boundaries of text-aligned and realistic generations.","To address this issue, we propose MoDiPO (Motion Diffusion DPO), a novel methodology that leverages Direct Preference Optimization (DPO) to align text-to-motion models.","We streamline the laborious and expensive process of gathering human preferences needed in DPO by leveraging AI feedback instead.","This enables us to experiment with novel DPO strategies, using both online and offline generated motion-preference pairs.","To foster future research we contribute with a motion-preference dataset which we dub Pick-a-Move.","We demonstrate, both qualitatively and quantitatively, that our proposed method yields significantly more realistic motions.","In particular, MoDiPO substantially improves Frechet Inception Distance (FID) while retaining the same RPrecision and Multi-Modality performances."],"url":"http://arxiv.org/abs/2405.03803v1","category":"cs.CV"}
{"created":"2024-05-06 18:01:13","title":"Deep learning classifier of locally advanced rectal cancer treatment response from endoscopy images","abstract":"We developed a deep learning classifier of rectal cancer response (tumor vs. no-tumor) to total neoadjuvant treatment (TNT) from endoscopic images acquired before, during, and following TNT. We further evaluated the network's ability in a near out-of-distribution (OOD) problem to identify local regrowth (LR) from follow-up endoscopy images acquired several months to years after completing TNT. We addressed endoscopic image variability by using optimal mass transport-based image harmonization. We evaluated multiple training regularization schemes to study the ResNet-50 network's in-distribution and near-OOD generalization ability. Test time augmentation resulted in the most considerable accuracy improvement. Image harmonization resulted in slight accuracy improvement for the near-OOD cases. Our results suggest that off-the-shelf deep learning classifiers can detect rectal cancer from endoscopic images at various stages of therapy for surveillance.","sentences":["We developed a deep learning classifier of rectal cancer response (tumor vs. no-tumor) to total neoadjuvant treatment (TNT) from endoscopic images acquired before, during, and following TNT.","We further evaluated the network's ability in a near out-of-distribution (OOD) problem to identify local regrowth (LR) from follow-up endoscopy images acquired several months to years after completing TNT.","We addressed endoscopic image variability by using optimal mass transport-based image harmonization.","We evaluated multiple training regularization schemes to study the ResNet-50 network's in-distribution and near-OOD generalization ability.","Test time augmentation resulted in the most considerable accuracy improvement.","Image harmonization resulted in slight accuracy improvement for the near-OOD cases.","Our results suggest that off-the-shelf deep learning classifiers can detect rectal cancer from endoscopic images at various stages of therapy for surveillance."],"url":"http://arxiv.org/abs/2405.03762v1","category":"eess.IV"}
{"created":"2024-05-06 17:59:36","title":"Pose Priors from Language Models","abstract":"We present a zero-shot pose optimization method that enforces accurate physical contact constraints when estimating the 3D pose of humans. Our central insight is that since language is often used to describe physical interaction, large pretrained text-based models can act as priors on pose estimation.   We can thus leverage this insight to improve pose estimation by converting natural language descriptors, generated by a large multimodal model (LMM), into tractable losses to constrain the 3D pose optimization. Despite its simplicity, our method produces surprisingly compelling pose reconstructions of people in close contact, correctly capturing the semantics of the social and physical interactions. We demonstrate that our method rivals more complex state-of-the-art approaches that require expensive human annotation of contact points and training specialized models. Moreover, unlike previous approaches, our method provides a unified framework for resolving self-contact and person-to-person contact.","sentences":["We present a zero-shot pose optimization method that enforces accurate physical contact constraints when estimating the 3D pose of humans.","Our central insight is that since language is often used to describe physical interaction, large pretrained text-based models can act as priors on pose estimation.   ","We can thus leverage this insight to improve pose estimation by converting natural language descriptors, generated by a large multimodal model (LMM), into tractable losses to constrain the 3D pose optimization.","Despite its simplicity, our method produces surprisingly compelling pose reconstructions of people in close contact, correctly capturing the semantics of the social and physical interactions.","We demonstrate that our method rivals more complex state-of-the-art approaches that require expensive human annotation of contact points and training specialized models.","Moreover, unlike previous approaches, our method provides a unified framework for resolving self-contact and person-to-person contact."],"url":"http://arxiv.org/abs/2405.03689v1","category":"cs.CV"}
{"created":"2024-05-06 17:44:40","title":"Data-Driven Model Identification Near a Supercritical Hopf Bifurcation Using Phase-Based Approaches","abstract":"A data-driven model identification strategy is developed for dynamical systems near a supercritical Hopf bifurcation with nonautonomous inputs. This strategy draws on phase-amplitude reduction techniques, leveraging an analytical representation for the phase and amplitude response curves of the Hopf normal form to infer system parameters. Fitting can be performed by recording the system output during the relaxation to the stable limit cycle after applying as few as two carefully timed pulse inputs. This strategy is illustrated in two examples with relevance to circadian oscillations. In each example, the proposed model identification strategy allows for the formulation, solution, and implementation of a closed loop nonlinear optimal control problem.","sentences":["A data-driven model identification strategy is developed for dynamical systems near a supercritical Hopf bifurcation with nonautonomous inputs.","This strategy draws on phase-amplitude reduction techniques, leveraging an analytical representation for the phase and amplitude response curves of the Hopf normal form to infer system parameters.","Fitting can be performed by recording the system output during the relaxation to the stable limit cycle after applying as few as two carefully timed pulse inputs.","This strategy is illustrated in two examples with relevance to circadian oscillations.","In each example, the proposed model identification strategy allows for the formulation, solution, and implementation of a closed loop nonlinear optimal control problem."],"url":"http://arxiv.org/abs/2405.03668v1","category":"math.DS"}
{"created":"2024-05-06 17:42:18","title":"Distributed Estimation in Blockchain-aided Internet of Things in the Presence of Attacks","abstract":"Distributed estimation in a blockchain-aided Internet of Things (BIoT) is considered, where the integrated blockchain secures data exchanges across the BIoT and the storage of data at BIoT agents. This paper focuses on developing a performance guarantee for the distributed estimation in a BIoT in the presence of malicious attacks which jointly exploits vulnerabilities present in both IoT devices and the employed blockchain within the BIoT. To achieve this, we adopt the Cramer-Rao Bound (CRB) as the performance metric, and maximize the CRB for estimating the parameter of interest over the attack domain. However, the maximization problem is inherently non-convex, making it infeasible to obtain the globally optimal solution in general. To address this issue, we develop a relaxation method capable of transforming the original non-convex optimization problem into a convex optimization problem. Moreover, we derive the analytical expression for the optimal solution to the relaxed optimization problem. The optimal value of the relaxed optimization problem can be used to provide a valid estimation performance guarantee for the BIoT in the presence of attacks.","sentences":["Distributed estimation in a blockchain-aided Internet of Things (BIoT) is considered, where the integrated blockchain secures data exchanges across the BIoT and the storage of data at BIoT agents.","This paper focuses on developing a performance guarantee for the distributed estimation in a BIoT in the presence of malicious attacks which jointly exploits vulnerabilities present in both IoT devices and the employed blockchain within the BIoT. To achieve this, we adopt the Cramer-Rao Bound (CRB) as the performance metric, and maximize the CRB for estimating the parameter of interest over the attack domain.","However, the maximization problem is inherently non-convex, making it infeasible to obtain the globally optimal solution in general.","To address this issue, we develop a relaxation method capable of transforming the original non-convex optimization problem into a convex optimization problem.","Moreover, we derive the analytical expression for the optimal solution to the relaxed optimization problem.","The optimal value of the relaxed optimization problem can be used to provide a valid estimation performance guarantee for the BIoT in the presence of attacks."],"url":"http://arxiv.org/abs/2405.03665v1","category":"eess.SP"}
{"created":"2024-05-06 17:29:31","title":"Exponential optimization of quantum state preparation via adiabatic thermalization","abstract":"The preparation of a given quantum state on a quantum computing register is a typically demanding operation, requiring a number of elementary gates that scales exponentially with the size of the problem. Using the adiabatic theorem for state preparation, whose error decreases exponentially as a function of the thermalization time, we derive an explicit analytic expression for the dependence of the characteristic time on the Hamiltonian used in the adiabatic evolution. Exploiting this knowledge, we then design a preconditioning term that modifies the adiabatic preparation, thus reducing its characteristic time and hence giving an exponential advantage in state preparation. We prove the efficiency of our method with extensive numerical experiments on prototypical spin-models, which gives a promising strategy to perform quantum simulations of manybody models via Trotter evolution on near-term quantum processors.","sentences":["The preparation of a given quantum state on a quantum computing register is a typically demanding operation, requiring a number of elementary gates that scales exponentially with the size of the problem.","Using the adiabatic theorem for state preparation, whose error decreases exponentially as a function of the thermalization time, we derive an explicit analytic expression for the dependence of the characteristic time on the Hamiltonian used in the adiabatic evolution.","Exploiting this knowledge, we then design a preconditioning term that modifies the adiabatic preparation, thus reducing its characteristic time and hence giving an exponential advantage in state preparation.","We prove the efficiency of our method with extensive numerical experiments on prototypical spin-models, which gives a promising strategy to perform quantum simulations of manybody models via Trotter evolution on near-term quantum processors."],"url":"http://arxiv.org/abs/2405.03656v1","category":"quant-ph"}
{"created":"2024-05-06 16:56:16","title":"Cosine Annealing Optimized Denoising Diffusion Error Correction Codes","abstract":"To address the issue of increased bit error rates during the later stages of linear search in denoising diffusion error correction codes, we propose a novel method that optimizes denoising diffusion error correction codes (ECC) using cosine annealing. In response to the challenge of decoding long codewords, the proposed method employs a variance adjustment strategy during the reverse diffusion process, rather than maintaining a constant variance. By leveraging cosine annealing, this method effectively lowers the bit error rate and enhances decoding effciency. This letter extensively validates the approach through experiments and demonstrates signifcant improvements in bit error rate reduction and iteration effciency compared to existing methods. This advancement offers a promising solution for improving ECC decoding performance, potentially impacting secure digital communication practices.","sentences":["To address the issue of increased bit error rates during the later stages of linear search in denoising diffusion error correction codes, we propose a novel method that optimizes denoising diffusion error correction codes (ECC) using cosine annealing.","In response to the challenge of decoding long codewords, the proposed method employs a variance adjustment strategy during the reverse diffusion process, rather than maintaining a constant variance.","By leveraging cosine annealing, this method effectively lowers the bit error rate and enhances decoding effciency.","This letter extensively validates the approach through experiments and demonstrates signifcant improvements in bit error rate reduction and iteration effciency compared to existing methods.","This advancement offers a promising solution for improving ECC decoding performance, potentially impacting secure digital communication practices."],"url":"http://arxiv.org/abs/2405.03638v1","category":"cs.IT"}
{"created":"2024-05-06 16:47:01","title":"Configuration-Constrained Tube MPC for Tracking","abstract":"This paper proposes a novel tube-based Model Predictive Control (MPC) framework for tracking varying setpoint references with linear systems subject to additive and multiplicative uncertainties. The MPC controllers designed using this framework exhibit recursively feasible for changing references, and robust asymptotic stability for piecewise constant references. The framework leverages configuration-constrained polytopes to parameterize the tubes, offering flexibility to optimize their shape. The efficacy of the approach is demonstrated through two numerical examples. The first example illustrates the theoretical results, and the second uses the framework to design a lane-change controller for an autonomous vehicle.","sentences":["This paper proposes a novel tube-based Model Predictive Control (MPC) framework for tracking varying setpoint references with linear systems subject to additive and multiplicative uncertainties.","The MPC controllers designed using this framework exhibit recursively feasible for changing references, and robust asymptotic stability for piecewise constant references.","The framework leverages configuration-constrained polytopes to parameterize the tubes, offering flexibility to optimize their shape.","The efficacy of the approach is demonstrated through two numerical examples.","The first example illustrates the theoretical results, and the second uses the framework to design a lane-change controller for an autonomous vehicle."],"url":"http://arxiv.org/abs/2405.03629v1","category":"eess.SY"}
{"created":"2024-05-06 16:45:48","title":"State-Aware Timeliness in Energy Harvesting IoT Systems Monitoring a Markovian Source","abstract":"In this study, we investigate the optimal transmission policies within an energy harvesting status update system, where the demand for status updates depends on the state of the source. The system monitors a two-state Markovian source that characterizes a stochastic process, which can be in either a normal state or an alarm state, with a higher demand for fresh updates when the source is in the alarm state. We propose a metric to capture the freshness of status updates for each state of the stochastic process by introducing two Age of Information (AoI) variables, extending the definition of AoI to account for the state changes of the stochastic process. We formulate the problem as a Markov Decision Process (MDP), utilizing a transition cost function that applies linear and non-linear penalties based on AoI and the state of the stochastic process. Through analytical investigation, we delve into the structure of the optimal transmission policy for the resulting MDP problem. Furthermore, we evaluate the derived policies via numerical results and demonstrate their effectiveness in reserving energy in anticipation of forthcoming alarm states.","sentences":["In this study, we investigate the optimal transmission policies within an energy harvesting status update system, where the demand for status updates depends on the state of the source.","The system monitors a two-state Markovian source that characterizes a stochastic process, which can be in either a normal state or an alarm state, with a higher demand for fresh updates when the source is in the alarm state.","We propose a metric to capture the freshness of status updates for each state of the stochastic process by introducing two Age of Information (AoI) variables, extending the definition of AoI to account for the state changes of the stochastic process.","We formulate the problem as a Markov Decision Process (MDP), utilizing a transition cost function that applies linear and non-linear penalties based on AoI and the state of the stochastic process.","Through analytical investigation, we delve into the structure of the optimal transmission policy for the resulting MDP problem.","Furthermore, we evaluate the derived policies via numerical results and demonstrate their effectiveness in reserving energy in anticipation of forthcoming alarm states."],"url":"http://arxiv.org/abs/2405.03628v1","category":"cs.IT"}
{"created":"2024-05-06 16:41:52","title":"$\u03b5$-Policy Gradient for Online Pricing","abstract":"Combining model-based and model-free reinforcement learning approaches, this paper proposes and analyzes an $\\epsilon$-policy gradient algorithm for the online pricing learning task. The algorithm extends $\\epsilon$-greedy algorithm by replacing greedy exploitation with gradient descent step and facilitates learning via model inference. We optimize the regret of the proposed algorithm by quantifying the exploration cost in terms of the exploration probability $\\epsilon$ and the exploitation cost in terms of the gradient descent optimization and gradient estimation errors. The algorithm achieves an expected regret of order $\\mathcal{O}(\\sqrt{T})$ (up to a logarithmic factor) over $T$ trials.","sentences":["Combining model-based and model-free reinforcement learning approaches, this paper proposes and analyzes an $\\epsilon$-policy gradient algorithm for the online pricing learning task.","The algorithm extends $\\epsilon$-greedy algorithm by replacing greedy exploitation with gradient descent step and facilitates learning via model inference.","We optimize the regret of the proposed algorithm by quantifying the exploration cost in terms of the exploration probability $\\epsilon$ and the exploitation cost in terms of the gradient descent optimization and gradient estimation errors.","The algorithm achieves an expected regret of order $\\mathcal{O}(\\sqrt{T})$ (up to a logarithmic factor) over $T$ trials."],"url":"http://arxiv.org/abs/2405.03624v1","category":"cs.LG"}
{"created":"2024-05-06 16:34:44","title":"The trade-offs between Monolithic vs. Distributed Architectures","abstract":"Software architects frequently engage in trade-off analysis, often confronting sub-optimal solutions due to unforeseen or overlooked disadvantages. Such outcomes can detrimentally affect a company's business operations and resource allocation. This article conducts a critical review of archi- tectural styles, particularly focusing on the strengths and weaknesses of both monolithic and distributed architectures, and their relationship to architectural characteristics. It also explores the role of cloud computing in transitioning from monolithic to distributed-based applications. Utilizing a broad range of sources, including papers and books from both industry and academia, this research provides an overview from theoretical foundations to practical applications. A notable trend observed is a shift back from distributed to monolithic architectures, possibly due to factors such as cost, complexity, and performance.","sentences":["Software architects frequently engage in trade-off analysis, often confronting sub-optimal solutions due to unforeseen or overlooked disadvantages.","Such outcomes can detrimentally affect a company's business operations and resource allocation.","This article conducts a critical review of archi- tectural styles, particularly focusing on the strengths and weaknesses of both monolithic and distributed architectures, and their relationship to architectural characteristics.","It also explores the role of cloud computing in transitioning from monolithic to distributed-based applications.","Utilizing a broad range of sources, including papers and books from both industry and academia, this research provides an overview from theoretical foundations to practical applications.","A notable trend observed is a shift back from distributed to monolithic architectures, possibly due to factors such as cost, complexity, and performance."],"url":"http://arxiv.org/abs/2405.03619v1","category":"cs.SE"}
{"created":"2024-05-06 16:23:12","title":"Decision algorithms for reversibility of one-dimensional non-linear cellular automata under null boundary conditions","abstract":"The property of reversibility is quite meaningful for the classic theoretical computer science model, cellular automata. For the reversibility problem for a CA under null boundary conditions, while linear rules have been studied a lot, the non-linear rules remain unexplored at present. The paper investigates the reversibility problem of general one-dimensional CA on a finite field $\\mathbb{Z}_p$, and proposes an approach to optimize the Amoroso's infinite CA surjectivity detection algorithm. This paper proposes algorithms for deciding the reversibility of one-dimensional CA under null boundary conditions. We propose a method to decide the strict reversibility of one-dimensional CA under null boundary conditions. We also provide a bucket chain based algorithm for calculating the reversibility function of one-dimensional CA under null boundary conditions. These decision algorithms work for not only linear rules but also non-linear rules. In addition, it has been confirmed that the reversibility function always has a period, and its periodicity is related to the periodicity of the corresponding bucket chain. Some of our experiment results of reversible CA are presented in the paper, complementing and validating the theoretical aspects, and thereby further supporting the research conclusions of this paper.","sentences":["The property of reversibility is quite meaningful for the classic theoretical computer science model, cellular automata.","For the reversibility problem for a CA under null boundary conditions, while linear rules have been studied a lot, the non-linear rules remain unexplored at present.","The paper investigates the reversibility problem of general one-dimensional CA on a finite field $\\mathbb{Z}_p$, and proposes an approach to optimize the Amoroso's infinite CA surjectivity detection algorithm.","This paper proposes algorithms for deciding the reversibility of one-dimensional CA under null boundary conditions.","We propose a method to decide the strict reversibility of one-dimensional CA under null boundary conditions.","We also provide a bucket chain based algorithm for calculating the reversibility function of one-dimensional CA under null boundary conditions.","These decision algorithms work for not only linear rules but also non-linear rules.","In addition, it has been confirmed that the reversibility function always has a period, and its periodicity is related to the periodicity of the corresponding bucket chain.","Some of our experiment results of reversible CA are presented in the paper, complementing and validating the theoretical aspects, and thereby further supporting the research conclusions of this paper."],"url":"http://arxiv.org/abs/2405.03609v1","category":"cs.CC"}
{"created":"2024-05-06 16:21:47","title":"Energy-Based Optimization of Physical-Layer Challenge-Response Authentication with Drones","abstract":"Drones are expected to be used for many tasks in the future and require secure communication protocols. In this work, we propose a novel physical layer authentication (PLA)-based challenge-response (CR) protocol in which a drone Bob authenticates the sender (either on the ground or air) by exploiting his prior knowledge of the wireless channel statistic (fading, path loss, and shadowing). In particular, Bob will move to a set of positions in the space, and by estimating the attenuations of the received signals he will authenticate the sender. We take into account the energy consumption in the design and provide three solutions: a purely greedy solution (PG), an optimal Bellman iterative solution (BI), and a heuristic solution based on the evaluation of the standard deviation of the attenuations in the space. Finally, we demonstrate the effectiveness of our approach through numerical simulations.","sentences":["Drones are expected to be used for many tasks in the future and require secure communication protocols.","In this work, we propose a novel physical layer authentication (PLA)-based challenge-response (CR) protocol in which a drone Bob authenticates the sender (either on the ground or air) by exploiting his prior knowledge of the wireless channel statistic (fading, path loss, and shadowing).","In particular, Bob will move to a set of positions in the space, and by estimating the attenuations of the received signals he will authenticate the sender.","We take into account the energy consumption in the design and provide three solutions: a purely greedy solution (PG), an optimal Bellman iterative solution (BI), and a heuristic solution based on the evaluation of the standard deviation of the attenuations in the space.","Finally, we demonstrate the effectiveness of our approach through numerical simulations."],"url":"http://arxiv.org/abs/2405.03608v1","category":"eess.SP"}
{"created":"2024-05-06 16:07:27","title":"Improving the Ranging Performance of Random ISAC Signals Through Pulse Shaping Design","abstract":"In this paper, we propose a novel pulse shaping design for single-carrier integrated sensing and communication (ISAC) transmission. Due to the communication information embedded in the ISAC signal, the resulting auto-correlation function (ACF) is determined by both the information-conveying random symbol sequence and the signaling pulse, where the former leads to random fluctuations in the sidelobes of the ACF, impairing the range estimation performance. To overcome this challenge, we first analyze the statistical characteristics of the random ACF under the symbol-wise pulse shaping (SWPS) regime. As a step further, we formulate an optimization problem to design ISAC pulse shaping filters, which minimizes the average integrated sidelobe level ratio (ISLR) while meeting the Nyquist criterion, subject to power and bandwidth constraints. We then show that the problem can be recast as a convex quadratic program by expressing it in the frequency domain, which can be readily solved through standard tools. Numerical results demonstrate that the proposed pulse shaping design achieves substantial ranging sidelobe reduction compared to the celebrated root-raised cosine (RRC) pulse shaping, given that the communication throughput is unchanged.","sentences":["In this paper, we propose a novel pulse shaping design for single-carrier integrated sensing and communication (ISAC) transmission.","Due to the communication information embedded in the ISAC signal, the resulting auto-correlation function (ACF) is determined by both the information-conveying random symbol sequence and the signaling pulse, where the former leads to random fluctuations in the sidelobes of the ACF, impairing the range estimation performance.","To overcome this challenge, we first analyze the statistical characteristics of the random ACF under the symbol-wise pulse shaping (SWPS) regime.","As a step further, we formulate an optimization problem to design ISAC pulse shaping filters, which minimizes the average integrated sidelobe level ratio (ISLR) while meeting the Nyquist criterion, subject to power and bandwidth constraints.","We then show that the problem can be recast as a convex quadratic program by expressing it in the frequency domain, which can be readily solved through standard tools.","Numerical results demonstrate that the proposed pulse shaping design achieves substantial ranging sidelobe reduction compared to the celebrated root-raised cosine (RRC) pulse shaping, given that the communication throughput is unchanged."],"url":"http://arxiv.org/abs/2405.03597v2","category":"eess.SP"}
{"created":"2024-05-06 16:01:13","title":"Communities for the Lagrangian Dynamics of the Turbulent Velocity Gradient Tensor: A Network Participation Approach","abstract":"Complex network analysis methods have been widely applied to nonlinear systems, but applications within fluid mechanics are relatively few. In this paper, we use a network for the Lagrangian dynamics of the velocity gradient tensor (VGT), where each node is a flow state, and the probability of transitioning between states follows from a direct numerical simulation of statistically steady and isotropic turbulence. The network representation of the VGT dynamics is much more compact than the continuous, joint distribution of a set of invariants for the tensor. We focus on choosing optimal variables to discretize and classify the VGT states. To this end, we test several classifications based on topology and various properties of the background flow coherent structures. We do this using the notion of \"community\" or \"module\", namely clusters of nodes that are optimally distinct while also containing diverse nodal functions. The best classification based upon VGT invariants often adopted in the literature combines the sign of the principal invariants, $Q$ and $R$, and the sign of the discriminant function, $\\Delta$, separating regions where the VGT eigenvalues are real and complex. We further improve this classification by including the relative magnitude of the non-normal contribution to the dynamics of the enstrophy and straining stemming from a Schur decomposition of the VGT. The traditional focus on the second VGT principal invariant, $Q$, implies consideration of the difference between the enstrophy and strain-rate magnitude without the non-normal parts. The fact that including the non-normality leads to a better VGT classification highlights the importance of unclosed and complex terms contributing to the VGT dynamics, namely the pressure Hessian and viscous terms, to which the VGT non-normality is intrinsically related.","sentences":["Complex network analysis methods have been widely applied to nonlinear systems, but applications within fluid mechanics are relatively few.","In this paper, we use a network for the Lagrangian dynamics of the velocity gradient tensor (VGT), where each node is a flow state, and the probability of transitioning between states follows from a direct numerical simulation of statistically steady and isotropic turbulence.","The network representation of the VGT dynamics is much more compact than the continuous, joint distribution of a set of invariants for the tensor.","We focus on choosing optimal variables to discretize and classify the VGT states.","To this end, we test several classifications based on topology and various properties of the background flow coherent structures.","We do this using the notion of \"community\" or \"module\", namely clusters of nodes that are optimally distinct while also containing diverse nodal functions.","The best classification based upon VGT invariants often adopted in the literature combines the sign of the principal invariants, $Q$ and $R$, and the sign of the discriminant function, $\\Delta$, separating regions where the VGT eigenvalues are real and complex.","We further improve this classification by including the relative magnitude of the non-normal contribution to the dynamics of the enstrophy and straining stemming from a Schur decomposition of the VGT.","The traditional focus on the second VGT principal invariant, $Q$, implies consideration of the difference between the enstrophy and strain-rate magnitude without the non-normal parts.","The fact that including the non-normality leads to a better VGT classification highlights the importance of unclosed and complex terms contributing to the VGT dynamics, namely the pressure Hessian and viscous terms, to which the VGT non-normality is intrinsically related."],"url":"http://arxiv.org/abs/2405.03589v1","category":"physics.flu-dyn"}
{"created":"2024-05-06 15:49:46","title":"ILILT: Implicit Learning of Inverse Lithography Technologies","abstract":"Lithography, transferring chip design masks to the silicon wafer, is the most important phase in modern semiconductor manufacturing flow. Due to the limitations of lithography systems, Extensive design optimizations are required to tackle the design and silicon mismatch. Inverse lithography technology (ILT) is one of the promising solutions to perform pre-fabrication optimization, termed mask optimization. Because of mask optimization problems' constrained non-convexity, numerical ILT solvers rely heavily on good initialization to avoid getting stuck on sub-optimal solutions. Machine learning (ML) techniques are hence proposed to generate mask initialization for ILT solvers with one-shot inference, targeting faster and better convergence during ILT. This paper addresses the question of \\textit{whether ML models can directly generate high-quality optimized masks without engaging ILT solvers in the loop}. We propose an implicit learning ILT framework: ILILT, which leverages the implicit layer learning method and lithography-conditioned inputs to ground the model. Trained to understand the ILT optimization procedure, ILILT can outperform the state-of-the-art machine learning solutions, significantly improving efficiency and quality.","sentences":["Lithography, transferring chip design masks to the silicon wafer, is the most important phase in modern semiconductor manufacturing flow.","Due to the limitations of lithography systems, Extensive design optimizations are required to tackle the design and silicon mismatch.","Inverse lithography technology (ILT) is one of the promising solutions to perform pre-fabrication optimization, termed mask optimization.","Because of mask optimization problems' constrained non-convexity, numerical ILT solvers rely heavily on good initialization to avoid getting stuck on sub-optimal solutions.","Machine learning (ML) techniques are hence proposed to generate mask initialization for ILT solvers with one-shot inference, targeting faster and better convergence during ILT.","This paper addresses the question of \\textit{whether ML models can directly generate high-quality optimized masks without engaging ILT solvers in the loop}.","We propose an implicit learning ILT framework: ILILT, which leverages the implicit layer learning method and lithography-conditioned inputs to ground the model.","Trained to understand the ILT optimization procedure, ILILT can outperform the state-of-the-art machine learning solutions, significantly improving efficiency and quality."],"url":"http://arxiv.org/abs/2405.03574v1","category":"cs.LG"}
{"created":"2024-05-06 15:29:55","title":"Converse Lyapunov Results for Stability of Switched Systems with Average Dwell-Time","abstract":"This article provides a characterization of stability for switched nonlinear systems under average dwell-time constraints, in terms of necessary and sufficient conditions involving multiple Lyapunov functions. Earlier converse results focus on switched systems with dwell-time constraints only, and the resulting inequalities depend on the flow of individual subsystems. With the help of a counterexample, we show that a lower bound that guarantees stability for dwell-time switching signals may not necessarily imply stability for switching signals with same lower bound on the average dwell-time. Based on these two observations, we provide a converse result for the average dwell-time constrained systems in terms of inequalities which do not depend on the flow of individual subsystems and are easier to check. The particular case of linear switched systems is studied as a corollary to our main result.","sentences":["This article provides a characterization of stability for switched nonlinear systems under average dwell-time constraints, in terms of necessary and sufficient conditions involving multiple Lyapunov functions.","Earlier converse results focus on switched systems with dwell-time constraints only, and the resulting inequalities depend on the flow of individual subsystems.","With the help of a counterexample, we show that a lower bound that guarantees stability for dwell-time switching signals may not necessarily imply stability for switching signals with same lower bound on the average dwell-time.","Based on these two observations, we provide a converse result for the average dwell-time constrained systems in terms of inequalities which do not depend on the flow of individual subsystems and are easier to check.","The particular case of linear switched systems is studied as a corollary to our main result."],"url":"http://arxiv.org/abs/2405.03560v1","category":"math.OC"}
{"created":"2024-05-06 15:14:15","title":"A cavity-microscope for micrometer-scale control of atom-photon interactions","abstract":"Cavity quantum electrodynamics offers the possibility to observe and control the motion of few or individual atoms, enabling the realization of various quantum technological tasks such as quantum-enhanced metrology or quantum simulation of strongly-correlated matter. A core limitation of these experiments lies in the mode structure of the cavity field, which is hard-coded in the shape and geometry of the mirrors. As a result, most applications of cavity QED trade spatial resolution for enhanced sensitivity. Here, we propose and demonstrate a cavity-microscope device capable of controlling in space and time the coupling between atoms and light in a single-mode high-finesse cavity, reaching a spatial resolution an order-of-magnitude lower than the cavity mode waist. This is achieved through local Floquet engineering of the atomic level structure, imprinting a corresponding atom-field coupling. We illustrate this capability by engineering micrometer-scale coupling, using cavity-assisted atomic measurements and optimization. Our system forms an optical device with a single optical axis and has the same footprint and complexity as a standard Fabry-Perot cavity or confocal lens pair, and can be used for any atomic species. This technique opens a wide range of perspectives from ultra-fast, cavity-enhanced mid-circuit readout to the quantum simulation of fully connected models of quantum matter such as the Sachdev-Ye-Kitaev model.","sentences":["Cavity quantum electrodynamics offers the possibility to observe and control the motion of few or individual atoms, enabling the realization of various quantum technological tasks such as quantum-enhanced metrology or quantum simulation of strongly-correlated matter.","A core limitation of these experiments lies in the mode structure of the cavity field, which is hard-coded in the shape and geometry of the mirrors.","As a result, most applications of cavity QED trade spatial resolution for enhanced sensitivity.","Here, we propose and demonstrate a cavity-microscope device capable of controlling in space and time the coupling between atoms and light in a single-mode high-finesse cavity, reaching a spatial resolution an order-of-magnitude lower than the cavity mode waist.","This is achieved through local Floquet engineering of the atomic level structure, imprinting a corresponding atom-field coupling.","We illustrate this capability by engineering micrometer-scale coupling, using cavity-assisted atomic measurements and optimization.","Our system forms an optical device with a single optical axis and has the same footprint and complexity as a standard Fabry-Perot cavity or confocal lens pair, and can be used for any atomic species.","This technique opens a wide range of perspectives from ultra-fast, cavity-enhanced mid-circuit readout to the quantum simulation of fully connected models of quantum matter such as the Sachdev-Ye-Kitaev model."],"url":"http://arxiv.org/abs/2405.03550v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-06 15:10:16","title":"Optimizing Hand Region Detection in MediaPipe Holistic Full-Body Pose Estimation to Improve Accuracy and Avoid Downstream Errors","abstract":"This paper addresses a critical flaw in MediaPipe Holistic's hand Region of Interest (ROI) prediction, which struggles with non-ideal hand orientations, affecting sign language recognition accuracy. We propose a data-driven approach to enhance ROI estimation, leveraging an enriched feature set including additional hand keypoints and the z-dimension. Our results demonstrate better estimates, with higher Intersection-over-Union compared to the current method. Our code and optimizations are available at https://github.com/sign-language-processing/mediapipe-hand-crop-fix.","sentences":["This paper addresses a critical flaw in MediaPipe Holistic's hand Region of Interest (ROI) prediction, which struggles with non-ideal hand orientations, affecting sign language recognition accuracy.","We propose a data-driven approach to enhance ROI estimation, leveraging an enriched feature set including additional hand keypoints and the z-dimension.","Our results demonstrate better estimates, with higher Intersection-over-Union compared to the current method.","Our code and optimizations are available at https://github.com/sign-language-processing/mediapipe-hand-crop-fix."],"url":"http://arxiv.org/abs/2405.03545v1","category":"cs.CV"}
{"created":"2024-05-06 14:47:40","title":"Semi-autonomous Robotic Disassembly Enhanced by Mixed Reality","abstract":"In this study, we introduce \"SARDiM,\" a modular semi-autonomous platform enhanced with mixed reality for industrial disassembly tasks. Through a case study focused on EV battery disassembly, SARDiM integrates Mixed Reality, object segmentation, teleoperation, force feedback, and variable autonomy. Utilising the ROS, Unity, and MATLAB platforms, alongside a joint impedance controller, SARDiM facilitates teleoperated disassembly. The approach combines FastSAM for real-time object segmentation, generating data which is subsequently processed through a cluster analysis algorithm to determine the centroid and orientation of the components, categorizing them by size and disassembly priority. This data guides the MoveIt platform in trajectory planning for the Franka Robot arm. SARDiM provides the capability to switch between two teleoperation modes: manual and semi-autonomous with variable autonomy. Each was evaluated using four different Interface Methods (IM): direct view, monitor feed, mixed reality with monitor feed, and point cloud mixed reality. Evaluations across the eight IMs demonstrated a 40.61% decrease in joint limit violations using Mode 2. Moreover, Mode 2-IM4 outperformed Mode 1-IM1 by achieving a 2.33%-time reduction while considerably increasing safety, making it optimal for operating in hazardous environments at a safe distance, with the same ease of use as teleoperation with a direct view of the environment.","sentences":["In this study, we introduce \"SARDiM,\" a modular semi-autonomous platform enhanced with mixed reality for industrial disassembly tasks.","Through a case study focused on EV battery disassembly, SARDiM integrates Mixed Reality, object segmentation, teleoperation, force feedback, and variable autonomy.","Utilising the ROS, Unity, and MATLAB platforms, alongside a joint impedance controller, SARDiM facilitates teleoperated disassembly.","The approach combines FastSAM for real-time object segmentation, generating data which is subsequently processed through a cluster analysis algorithm to determine the centroid and orientation of the components, categorizing them by size and disassembly priority.","This data guides the MoveIt platform in trajectory planning for the Franka Robot arm.","SARDiM provides the capability to switch between two teleoperation modes: manual and semi-autonomous with variable autonomy.","Each was evaluated using four different Interface Methods (IM): direct view, monitor feed, mixed reality with monitor feed, and point cloud mixed reality.","Evaluations across the eight IMs demonstrated a 40.61% decrease in joint limit violations using Mode 2.","Moreover, Mode 2-IM4 outperformed Mode 1-IM1 by achieving a 2.33%-time reduction while considerably increasing safety, making it optimal for operating in hazardous environments at a safe distance, with the same ease of use as teleoperation with a direct view of the environment."],"url":"http://arxiv.org/abs/2405.03530v1","category":"cs.RO"}
{"created":"2024-05-06 14:44:06","title":"ReinWiFi: A Reinforcement-Learning-Based Framework for the Application-Layer QoS Optimization of WiFi Networks","abstract":"In this paper, a reinforcement-learning-based scheduling framework is proposed and implemented to optimize the application-layer quality-of-service (QoS) of a practical wireless local area network (WLAN) suffering from unknown interference. Particularly, application-layer tasks of file delivery and delay-sensitive communication, e.g., screen projection, in a WLAN with enhanced distributed channel access (EDCA) mechanism, are jointly scheduled by adjusting the contention window sizes and application-layer throughput limitation, such that their QoS, including the throughput of file delivery and the round trip time of the delay-sensitive communication, can be optimized. Due to the unknown interference and vendor-dependent implementation of the network interface card, the relation between the scheduling policy and the system QoS is unknown. Hence, a reinforcement learning method is proposed, in which a novel Q-network is trained to map from the historical scheduling parameters and QoS observations to the current scheduling action. It is demonstrated on a testbed that the proposed framework can achieve a significantly better QoS than the conventional EDCA mechanism.","sentences":["In this paper, a reinforcement-learning-based scheduling framework is proposed and implemented to optimize the application-layer quality-of-service (QoS) of a practical wireless local area network (WLAN) suffering from unknown interference.","Particularly, application-layer tasks of file delivery and delay-sensitive communication, e.g., screen projection, in a WLAN with enhanced distributed channel access (EDCA) mechanism, are jointly scheduled by adjusting the contention window sizes and application-layer throughput limitation, such that their QoS, including the throughput of file delivery and the round trip time of the delay-sensitive communication, can be optimized.","Due to the unknown interference and vendor-dependent implementation of the network interface card, the relation between the scheduling policy and the system QoS is unknown.","Hence, a reinforcement learning method is proposed, in which a novel Q-network is trained to map from the historical scheduling parameters and QoS observations to the current scheduling action.","It is demonstrated on a testbed that the proposed framework can achieve a significantly better QoS than the conventional EDCA mechanism."],"url":"http://arxiv.org/abs/2405.03526v1","category":"cs.NI"}
{"created":"2024-05-06 14:40:44","title":"Basilisk: Achieving Competitive Performance with Open EDA Tools on an Open-Source Linux-Capable RISC-V SoC","abstract":"We introduce Basilisk, an optimized application-specific integrated circuit (ASIC) implementation and design flow building on the end-to-end open-source Iguana system-on-chip (SoC). We present enhancements to synthesis tools and logic optimization scripts improving quality of results (QoR), as well as an optimized physical design with an improved power grid and cell placement integration enabling a higher core utilization. The tapeout-ready version of Basilisk implemented in IHP's open 130 nm technology achieves an operation frequency of 77 MHz (51 logic levels) under typical conditions, a 2.3x improvement compared to the baseline open-source EDA design flow presented in Iguana, and a higher 55 % core utilization compared to 50 % in the baseline design. Through collaboration with EDA tool developers and domain experts, Basilisk exemplifies a synergistic effort towards competitive open-source electronic design automation (EDA) tools for research and industry applications.","sentences":["We introduce Basilisk, an optimized application-specific integrated circuit (ASIC) implementation and design flow building on the end-to-end open-source Iguana system-on-chip (SoC).","We present enhancements to synthesis tools and logic optimization scripts improving quality of results (QoR), as well as an optimized physical design with an improved power grid and cell placement integration enabling a higher core utilization.","The tapeout-ready version of Basilisk implemented in IHP's open 130 nm technology achieves an operation frequency of 77 MHz (51 logic levels) under typical conditions, a 2.3x improvement compared to the baseline open-source EDA design flow presented in Iguana, and a higher 55 % core utilization compared to 50 % in the baseline design.","Through collaboration with EDA tool developers and domain experts, Basilisk exemplifies a synergistic effort towards competitive open-source electronic design automation (EDA) tools for research and industry applications."],"url":"http://arxiv.org/abs/2405.03523v1","category":"cs.AR"}
{"created":"2024-05-06 14:36:01","title":"Low-light Object Detection","abstract":"In this competition we employed a model fusion approach to achieve object detection results close to those of real images. Our method is based on the CO-DETR model, which was trained on two sets of data: one containing images under dark conditions and another containing images enhanced with low-light conditions. We used various enhancement techniques on the test data to generate multiple sets of prediction results. Finally, we applied a clustering aggregation method guided by IoU thresholds to select the optimal results.","sentences":["In this competition we employed a model fusion approach to achieve object detection results close to those of real images.","Our method is based on the CO-DETR model, which was trained on two sets of data: one containing images under dark conditions and another containing images enhanced with low-light conditions.","We used various enhancement techniques on the test data to generate multiple sets of prediction results.","Finally, we applied a clustering aggregation method guided by IoU thresholds to select the optimal results."],"url":"http://arxiv.org/abs/2405.03519v1","category":"cs.CV"}
{"created":"2024-05-06 14:33:35","title":"Reinforcement Nash Equilibrium Solver","abstract":"Nash Equilibrium (NE) is the canonical solution concept of game theory, which provides an elegant tool to understand the rationalities. Though mixed strategy NE exists in any game with finite players and actions, computing NE in two- or multi-player general-sum games is PPAD-Complete. Various alternative solutions, e.g., Correlated Equilibrium (CE), and learning methods, e.g., fictitious play (FP), are proposed to approximate NE. For convenience, we call these methods as \"inexact solvers\", or \"solvers\" for short. However, the alternative solutions differ from NE and the learning methods generally fail to converge to NE. Therefore, in this work, we propose REinforcement Nash Equilibrium Solver (RENES), which trains a single policy to modify the games with different sizes and applies the solvers on the modified games where the obtained solution is evaluated on the original games. Specifically, our contributions are threefold. i) We represent the games as $\\alpha$-rank response graphs and leverage graph neural network (GNN) to handle the games with different sizes as inputs; ii) We use tensor decomposition, e.g., canonical polyadic (CP), to make the dimension of modifying actions fixed for games with different sizes; iii) We train the modifying strategy for games with the widely-used proximal policy optimization (PPO) and apply the solvers to solve the modified games, where the obtained solution is evaluated on original games. Extensive experiments on large-scale normal-form games show that our method can further improve the approximation of NE of different solvers, i.e., $\\alpha$-rank, CE, FP and PRD, and can be generalized to unseen games.","sentences":["Nash Equilibrium (NE) is the canonical solution concept of game theory, which provides an elegant tool to understand the rationalities.","Though mixed strategy NE exists in any game with finite players and actions, computing NE in two- or multi-player general-sum games is PPAD-Complete.","Various alternative solutions, e.g., Correlated Equilibrium (CE), and learning methods, e.g., fictitious play (FP), are proposed to approximate NE.","For convenience, we call these methods as \"inexact solvers\", or \"solvers\" for short.","However, the alternative solutions differ from NE and the learning methods generally fail to converge to NE.","Therefore, in this work, we propose REinforcement Nash Equilibrium Solver (RENES), which trains a single policy to modify the games with different sizes and applies the solvers on the modified games where the obtained solution is evaluated on the original games.","Specifically, our contributions are threefold.","i)","We represent the games as $\\alpha$-rank response graphs and leverage graph neural network (GNN) to handle the games with different sizes as inputs; ii) We use tensor decomposition, e.g., canonical polyadic (CP), to make the dimension of modifying actions fixed for games with different sizes; iii)","We train the modifying strategy for games with the widely-used proximal policy optimization (PPO) and apply the solvers to solve the modified games, where the obtained solution is evaluated on original games.","Extensive experiments on large-scale normal-form games show that our method can further improve the approximation of NE of different solvers, i.e., $\\alpha$-rank, CE, FP and PRD, and can be generalized to unseen games."],"url":"http://arxiv.org/abs/2405.03518v1","category":"cs.GT"}
{"created":"2024-05-06 14:29:24","title":"GI-SMN: Gradient Inversion Attack against Federated Learning without Prior Knowledge","abstract":"Federated learning (FL) has emerged as a privacy-preserving machine learning approach where multiple parties share gradient information rather than original user data. Recent work has demonstrated that gradient inversion attacks can exploit the gradients of FL to recreate the original user data, posing significant privacy risks. However, these attacks make strong assumptions about the attacker, such as altering the model structure or parameters, gaining batch normalization statistics, or acquiring prior knowledge of the original training set, etc. Consequently, these attacks are not possible in real-world scenarios. To end it, we propose a novel Gradient Inversion attack based on Style Migration Network (GI-SMN), which breaks through the strong assumptions made by previous gradient inversion attacks. The optimization space is reduced by the refinement of the latent code and the use of regular terms to facilitate gradient matching. GI-SMN enables the reconstruction of user data with high similarity in batches. Experimental results have demonstrated that GI-SMN outperforms state-of-the-art gradient inversion attacks in both visual effect and similarity metrics. Additionally, it also can overcome gradient pruning and differential privacy defenses.","sentences":["Federated learning (FL) has emerged as a privacy-preserving machine learning approach where multiple parties share gradient information rather than original user data.","Recent work has demonstrated that gradient inversion attacks can exploit the gradients of FL to recreate the original user data, posing significant privacy risks.","However, these attacks make strong assumptions about the attacker, such as altering the model structure or parameters, gaining batch normalization statistics, or acquiring prior knowledge of the original training set, etc.","Consequently, these attacks are not possible in real-world scenarios.","To end it, we propose a novel Gradient Inversion attack based on Style Migration Network (GI-SMN), which breaks through the strong assumptions made by previous gradient inversion attacks.","The optimization space is reduced by the refinement of the latent code and the use of regular terms to facilitate gradient matching.","GI-SMN enables the reconstruction of user data with high similarity in batches.","Experimental results have demonstrated that GI-SMN outperforms state-of-the-art gradient inversion attacks in both visual effect and similarity metrics.","Additionally, it also can overcome gradient pruning and differential privacy defenses."],"url":"http://arxiv.org/abs/2405.03516v1","category":"cs.LG"}
{"created":"2024-05-06 14:23:29","title":"Prolonging The Inevitable: Maximising survival time of an engine-equipped spacecraft between spatial hypersurfaces, as applied to the Schwarzschild spacetime","abstract":"The fate of an astronaut unfortunate -- or foolish -- enough to find themselves hurtling towards spaghettification after passing the event horizon of a black hole is a common anecdote told by scientists to the regular population. However, despite the fact the Schwarzschild spacetime has been discovered over a century ago, the simple question of how long can such a space traveller live has not been fully elaborated on since. In fact, a few textbooks even give a mistaken or easily misread description of what happens. We address those inconsistencies. We calculate the proper time a space traveller equipped with means of propulsion can expect to live in these circumstances, giving analytical expressions (as elliptic integrals) wherever possible. We prove a principle that explains the best strategy to extend their life, and show its' generalisation for other spacetimes. Finally, we give quantitative answers to what gains due to optimal control can be expected in typical and somewhat `realistic' circumstances.","sentences":["The fate of an astronaut unfortunate -- or foolish -- enough to find themselves hurtling towards spaghettification after passing the event horizon of a black hole is a common anecdote told by scientists to the regular population.","However, despite the fact the Schwarzschild spacetime has been discovered over a century ago, the simple question of how long can such a space traveller live has not been fully elaborated on since.","In fact, a few textbooks even give a mistaken or easily misread description of what happens.","We address those inconsistencies.","We calculate the proper time a space traveller equipped with means of propulsion can expect to live in these circumstances, giving analytical expressions (as elliptic integrals) wherever possible.","We prove a principle that explains the best strategy to extend their life, and show its' generalisation for other spacetimes.","Finally, we give quantitative answers to what gains due to optimal control can be expected in typical and somewhat `realistic' circumstances."],"url":"http://arxiv.org/abs/2405.03510v2","category":"gr-qc"}
{"created":"2024-05-06 14:14:30","title":"Human-Variability-Respecting Optimal Control for Physical Human-Machine Interaction","abstract":"Physical Human-Machine Interaction plays a pivotal role in facilitating collaboration across various domains. When designing appropriate model-based controllers to assist a human in the interaction, the accuracy of the human model is crucial for the resulting overall behavior of the coupled system. When looking at state-of-the-art control approaches, most methods rely on a deterministic model or no model at all of the human behavior. This poses a gap to the current neuroscientific standard regarding human movement modeling, which uses stochastic optimal control models that include signal-dependent noise processes and therefore describe the human behavior much more accurate than the deterministic counterparts. To close this gap by including these stochastic human models in the control design, we introduce a novel design methodology resulting in a Human-Variability-Respecting Optimal Control that explicitly incorporates the human noise processes and their influence on the mean and variability behavior of a physically coupled human-machine system. Our approach results in an improved overall system performance, i.e. higher accuracy and lower variability in target point reaching, while allowing to shape the joint variability, for example to preserve human natural variability patterns.","sentences":["Physical Human-Machine Interaction plays a pivotal role in facilitating collaboration across various domains.","When designing appropriate model-based controllers to assist a human in the interaction, the accuracy of the human model is crucial for the resulting overall behavior of the coupled system.","When looking at state-of-the-art control approaches, most methods rely on a deterministic model or no model at all of the human behavior.","This poses a gap to the current neuroscientific standard regarding human movement modeling, which uses stochastic optimal control models that include signal-dependent noise processes and therefore describe the human behavior much more accurate than the deterministic counterparts.","To close this gap by including these stochastic human models in the control design, we introduce a novel design methodology resulting in a Human-Variability-Respecting Optimal Control that explicitly incorporates the human noise processes and their influence on the mean and variability behavior of a physically coupled human-machine system.","Our approach results in an improved overall system performance, i.e. higher accuracy and lower variability in target point reaching, while allowing to shape the joint variability, for example to preserve human natural variability patterns."],"url":"http://arxiv.org/abs/2405.03502v1","category":"eess.SY"}
{"created":"2024-05-06 14:07:02","title":"Price-Aware Automated Market Makers: Models Beyond Brownian Prices and Static Liquidity","abstract":"In this paper, we introduce a suite of models for price-aware automated market making platforms willing to optimize their quotes. These models incorporate advanced price dynamics, including stochastic volatility, jumps, and microstructural price models based on Hawkes processes. Additionally, we address the variability in demand from liquidity takers through models that employ either Hawkes or Markov-modulated Poisson processes. Each model is analyzed with particular emphasis placed on the complexity of the numerical methods required to compute optimal quotes.","sentences":["In this paper, we introduce a suite of models for price-aware automated market making platforms willing to optimize their quotes.","These models incorporate advanced price dynamics, including stochastic volatility, jumps, and microstructural price models based on Hawkes processes.","Additionally, we address the variability in demand from liquidity takers through models that employ either Hawkes or Markov-modulated Poisson processes.","Each model is analyzed with particular emphasis placed on the complexity of the numerical methods required to compute optimal quotes."],"url":"http://arxiv.org/abs/2405.03496v1","category":"q-fin.TR"}
{"created":"2024-05-06 14:01:05","title":"On the Influence of Data Resampling for Deep Learning-Based Log Anomaly Detection: Insights and Recommendations","abstract":"Numerous DL-based approaches have garnered considerable attention in the field of software Log Anomaly Detection. However, a practical challenge persists: the class imbalance in the public data commonly used to train the DL models. This imbalance is characterized by a substantial disparity in the number of abnormal log sequences compared to normal ones, for example, anomalies represent less than 1% of one of the most popular datasets. Previous research has indicated that existing DLLAD approaches may exhibit unsatisfactory performance, particularly when confronted with datasets featuring severe class imbalances. Mitigating class imbalance through data resampling has proven effective for other software engineering tasks, however, it has been unexplored for LAD thus far. This study aims to fill this gap by providing an in-depth analysis of the impact of diverse data resampling methods on existing DLLAD approaches from two distinct perspectives. Firstly, we assess the performance of these DLLAD approaches across three datasets and explore the impact of resampling ratios of normal to abnormal data on ten data resampling methods. Secondly, we evaluate the effectiveness of the data resampling methods when utilizing optimal resampling ratios of normal to abnormal data. Our findings indicate that oversampling methods generally outperform undersampling and hybrid methods. Data resampling on raw data yields superior results compared to data resampling in the feature space. In most cases, certain undersampling and hybrid methods show limited effectiveness. Additionally, by exploring the resampling ratio of normal to abnormal data, we suggest generating more data for minority classes through oversampling while removing less data from majority classes through undersampling. In conclusion, our study provides valuable insights into the intricate relationship between data resampling methods and DLLAD.","sentences":["Numerous DL-based approaches have garnered considerable attention in the field of software Log Anomaly Detection.","However, a practical challenge persists: the class imbalance in the public data commonly used to train the DL models.","This imbalance is characterized by a substantial disparity in the number of abnormal log sequences compared to normal ones, for example, anomalies represent less than 1% of one of the most popular datasets.","Previous research has indicated that existing DLLAD approaches may exhibit unsatisfactory performance, particularly when confronted with datasets featuring severe class imbalances.","Mitigating class imbalance through data resampling has proven effective for other software engineering tasks, however, it has been unexplored for LAD thus far.","This study aims to fill this gap by providing an in-depth analysis of the impact of diverse data resampling methods on existing DLLAD approaches from two distinct perspectives.","Firstly, we assess the performance of these DLLAD approaches across three datasets and explore the impact of resampling ratios of normal to abnormal data on ten data resampling methods.","Secondly, we evaluate the effectiveness of the data resampling methods when utilizing optimal resampling ratios of normal to abnormal data.","Our findings indicate that oversampling methods generally outperform undersampling and hybrid methods.","Data resampling on raw data yields superior results compared to data resampling in the feature space.","In most cases, certain undersampling and hybrid methods show limited effectiveness.","Additionally, by exploring the resampling ratio of normal to abnormal data, we suggest generating more data for minority classes through oversampling while removing less data from majority classes through undersampling.","In conclusion, our study provides valuable insights into the intricate relationship between data resampling methods and DLLAD."],"url":"http://arxiv.org/abs/2405.03489v1","category":"cs.SE"}
{"created":"2024-05-06 13:56:56","title":"LGTM: Local-to-Global Text-Driven Human Motion Diffusion Model","abstract":"In this paper, we introduce LGTM, a novel Local-to-Global pipeline for Text-to-Motion generation. LGTM utilizes a diffusion-based architecture and aims to address the challenge of accurately translating textual descriptions into semantically coherent human motion in computer animation. Specifically, traditional methods often struggle with semantic discrepancies, particularly in aligning specific motions to the correct body parts. To address this issue, we propose a two-stage pipeline to overcome this challenge: it first employs large language models (LLMs) to decompose global motion descriptions into part-specific narratives, which are then processed by independent body-part motion encoders to ensure precise local semantic alignment. Finally, an attention-based full-body optimizer refines the motion generation results and guarantees the overall coherence. Our experiments demonstrate that LGTM gains significant improvements in generating locally accurate, semantically-aligned human motion, marking a notable advancement in text-to-motion applications. Code and data for this paper are available at https://github.com/L-Sun/LGTM","sentences":["In this paper, we introduce LGTM, a novel Local-to-Global pipeline for Text-to-Motion generation.","LGTM utilizes a diffusion-based architecture and aims to address the challenge of accurately translating textual descriptions into semantically coherent human motion in computer animation.","Specifically, traditional methods often struggle with semantic discrepancies, particularly in aligning specific motions to the correct body parts.","To address this issue, we propose a two-stage pipeline to overcome this challenge: it first employs large language models (LLMs) to decompose global motion descriptions into part-specific narratives, which are then processed by independent body-part motion encoders to ensure precise local semantic alignment.","Finally, an attention-based full-body optimizer refines the motion generation results and guarantees the overall coherence.","Our experiments demonstrate that LGTM gains significant improvements in generating locally accurate, semantically-aligned human motion, marking a notable advancement in text-to-motion applications.","Code and data for this paper are available at https://github.com/L-Sun/LGTM"],"url":"http://arxiv.org/abs/2405.03485v1","category":"cs.CV"}
{"created":"2024-05-06 13:48:51","title":"A Minimum-Jerk Approach to Handle Singularities in Virtual Fixtures","abstract":"Implementing virtual fixtures in guiding tasks constrains the movement of the robot's end effector to specific curves within its workspace. However, incorporating guiding frameworks may encounter discontinuities when optimizing the reference target position to the nearest point relative to the current robot position. This article aims to give a geometric interpretation of such discontinuities, with specific reference to the commonly adopted Gauss-Newton algorithm. The effect of such discontinuities, defined as Euclidean Distance Singularities, is experimentally proved. We then propose a solution that is based on a Linear Quadratic Tracking problem with minimum jerk command, then compare and validate the performances of the proposed framework in two different human-robot interaction scenarios.","sentences":["Implementing virtual fixtures in guiding tasks constrains the movement of the robot's end effector to specific curves within its workspace.","However, incorporating guiding frameworks may encounter discontinuities when optimizing the reference target position to the nearest point relative to the current robot position.","This article aims to give a geometric interpretation of such discontinuities, with specific reference to the commonly adopted Gauss-Newton algorithm.","The effect of such discontinuities, defined as Euclidean Distance Singularities, is experimentally proved.","We then propose a solution that is based on a Linear Quadratic Tracking problem with minimum jerk command, then compare and validate the performances of the proposed framework in two different human-robot interaction scenarios."],"url":"http://arxiv.org/abs/2405.03473v1","category":"cs.RO"}
{"created":"2024-05-06 13:47:09","title":"A Symplectic Analysis of Alternating Mirror Descent","abstract":"Motivated by understanding the behavior of the Alternating Mirror Descent (AMD) algorithm for bilinear zero-sum games, we study the discretization of continuous-time Hamiltonian flow via the symplectic Euler method. We provide a framework for analysis using results from Hamiltonian dynamics, Lie algebra, and symplectic numerical integrators, with an emphasis on the existence and properties of a conserved quantity, the modified Hamiltonian (MH), for the symplectic Euler method. We compute the MH in closed-form when the original Hamiltonian is a quadratic function, and show that it generally differs from the other conserved quantity known previously in that case. We derive new error bounds on the MH when truncated at orders in the stepsize in terms of the number of iterations, $K$, and utilize this bound to show an improved $\\mathcal{O}(K^{1/5})$ total regret bound and an $\\mathcal{O}(K^{-4/5})$ duality gap of the average iterates for AMD. Finally, we propose a conjecture which, if true, would imply that the total regret for AMD goes as $\\mathcal{O}\\left(K^{\\varepsilon}\\right)$ and the duality gap of the average iterates as $\\mathcal{O}\\left(K^{-1+\\varepsilon}\\right)$ for any $\\varepsilon>0$, and we can take $\\varepsilon=0$ upon certain convergence conditions for the MH.","sentences":["Motivated by understanding the behavior of the Alternating Mirror Descent (AMD) algorithm for bilinear zero-sum games, we study the discretization of continuous-time Hamiltonian flow via the symplectic Euler method.","We provide a framework for analysis using results from Hamiltonian dynamics, Lie algebra, and symplectic numerical integrators, with an emphasis on the existence and properties of a conserved quantity, the modified Hamiltonian (MH), for the symplectic Euler method.","We compute the MH in closed-form when the original Hamiltonian is a quadratic function, and show that it generally differs from the other conserved quantity known previously in that case.","We derive new error bounds on the MH when truncated at orders in the stepsize in terms of the number of iterations, $K$, and utilize this bound to show an improved $\\mathcal{O}(K^{1/5})$ total regret bound and an $\\mathcal{O}(K^{-4/5})$ duality gap of the average iterates for AMD.","Finally, we propose a conjecture which, if true, would imply that the total regret for AMD goes as $\\mathcal{O}\\left(K^{\\varepsilon}\\right)$ and the duality gap of the average iterates as $\\mathcal{O}\\left(K^{-1+\\varepsilon}\\right)$ for any $\\varepsilon>0$, and we can take $\\varepsilon=0$ upon certain convergence conditions for the MH."],"url":"http://arxiv.org/abs/2405.03472v1","category":"math.OC"}
{"created":"2024-05-06 13:44:51","title":"Hierarchic Flows to Estimate and Sample High-dimensional Probabilities","abstract":"Finding low-dimensional interpretable models of complex physical fields such as turbulence remains an open question, 80 years after the pioneer work of Kolmogorov. Estimating high-dimensional probability distributions from data samples suffers from an optimization and an approximation curse of dimensionality. It may be avoided by following a hierarchic probability flow from coarse to fine scales. This inverse renormalization group is defined by conditional probabilities across scales, renormalized in a wavelet basis. For a $\\varphi^4$ scalar potential, sampling these hierarchic models avoids the critical slowing down at the phase transition. An outstanding issue is to also approximate non-Gaussian fields having long-range interactions in space and across scales. We introduce low-dimensional models with robust multiscale approximations of high order polynomial energies. They are calculated with a second wavelet transform, which defines interactions over two hierarchies of scales. We estimate and sample these wavelet scattering models to generate 2D vorticity fields of turbulence, and images of dark matter densities.","sentences":["Finding low-dimensional interpretable models of complex physical fields such as turbulence remains an open question, 80 years after the pioneer work of Kolmogorov.","Estimating high-dimensional probability distributions from data samples suffers from an optimization and an approximation curse of dimensionality.","It may be avoided by following a hierarchic probability flow from coarse to fine scales.","This inverse renormalization group is defined by conditional probabilities across scales, renormalized in a wavelet basis.","For a $\\varphi^4$ scalar potential, sampling these hierarchic models avoids the critical slowing down at the phase transition.","An outstanding issue is to also approximate non-Gaussian fields having long-range interactions in space and across scales.","We introduce low-dimensional models with robust multiscale approximations of high order polynomial energies.","They are calculated with a second wavelet transform, which defines interactions over two hierarchies of scales.","We estimate and sample these wavelet scattering models to generate 2D vorticity fields of turbulence, and images of dark matter densities."],"url":"http://arxiv.org/abs/2405.03468v1","category":"stat.ML"}
{"created":"2024-05-06 13:29:34","title":"SSyncOA: Self-synchronizing Object-aligned Watermarking to Resist Cropping-paste Attacks","abstract":"Modern image processing tools have made it easy for attackers to crop the region or object of interest in images and paste it into other images. The challenge this cropping-paste attack poses to the watermarking technology is that it breaks the synchronization of the image watermark, introducing multiple superimposed desynchronization distortions, such as rotation, scaling, and translation. However, current watermarking methods can only resist a single type of desynchronization and cannot be applied to protect the object's copyright under the cropping-paste attack. With the finding that the key to resisting the cropping-paste attack lies in robust features of the object to protect, this paper proposes a self-synchronizing object-aligned watermarking method, called SSyncOA. Specifically, we first constrain the watermarked region to be aligned with the protected object, and then synchronize the watermark's translation, rotation, and scaling distortions by normalizing the object invariant features, i.e., its centroid, principal orientation, and minimum bounding square, respectively. To make the watermark embedded in the protected object, we introduce the object-aligned watermarking model, which incorporates the real cropping-paste attack into the encoder-noise layer-decoder pipeline and is optimized end-to-end. Besides, we illustrate the effect of different desynchronization distortions on the watermark training, which confirms the necessity of the self-synchronization process. Extensive experiments demonstrate the superiority of our method over other SOTAs.","sentences":["Modern image processing tools have made it easy for attackers to crop the region or object of interest in images and paste it into other images.","The challenge this cropping-paste attack poses to the watermarking technology is that it breaks the synchronization of the image watermark, introducing multiple superimposed desynchronization distortions, such as rotation, scaling, and translation.","However, current watermarking methods can only resist a single type of desynchronization and cannot be applied to protect the object's copyright under the cropping-paste attack.","With the finding that the key to resisting the cropping-paste attack lies in robust features of the object to protect, this paper proposes a self-synchronizing object-aligned watermarking method, called SSyncOA.","Specifically, we first constrain the watermarked region to be aligned with the protected object, and then synchronize the watermark's translation, rotation, and scaling distortions by normalizing the object invariant features, i.e., its centroid, principal orientation, and minimum bounding square, respectively.","To make the watermark embedded in the protected object, we introduce the object-aligned watermarking model, which incorporates the real cropping-paste attack into the encoder-noise layer-decoder pipeline and is optimized end-to-end.","Besides, we illustrate the effect of different desynchronization distortions on the watermark training, which confirms the necessity of the self-synchronization process.","Extensive experiments demonstrate the superiority of our method over other SOTAs."],"url":"http://arxiv.org/abs/2405.03458v1","category":"cs.CV"}
{"created":"2024-05-06 13:29:14","title":"Performance of H-Matrix-Vector Multiplication with Floating Point Compression","abstract":"Matrix-vector multiplication forms the basis of many iterative solution algorithms and as such is an important algorithm also for hierarchical matrices. However, due to its low computational intensity, its performance is typically limited by the available memory bandwidth. By optimizing the storage representation of the data within such matrices, this limitation can be lifted and the performance increased. This applies not only to hierarchical matrices but for also for other low-rank approximation schemes, e.g. block low-rank matrices.","sentences":["Matrix-vector multiplication forms the basis of many iterative solution algorithms and as such is an important algorithm also for hierarchical matrices.","However, due to its low computational intensity, its performance is typically limited by the available memory bandwidth.","By optimizing the storage representation of the data within such matrices, this limitation can be lifted and the performance increased.","This applies not only to hierarchical matrices but for also for other low-rank approximation schemes, e.g. block low-rank matrices."],"url":"http://arxiv.org/abs/2405.03456v1","category":"cs.DC"}
{"created":"2024-05-06 13:25:02","title":"A weighted multilevel Monte Carlo method","abstract":"The Multilevel Monte Carlo (MLMC) method has been applied successfully in a wide range of settings since its first introduction by Giles (2008). When using only two levels, the method can be viewed as a kind of control-variate approach to reduce variance, as earlier proposed by Kebaier (2005). We introduce a generalization of the MLMC formulation by extending this control variate approach to any number of levels and deriving a recursive formula for computing the weights associated with the control variates and the optimal numbers of samples at the various levels.   We also show how the generalisation can also be applied to the \\emph{multi-index} MLMC method of Haji-Ali, Nobile, Tempone (2015), at the cost of solving a $(2^d-1)$-dimensional minimisation problem at each node when $d$ index dimensions are used.   The comparative performance of the weighted MLMC method is illustrated in a range of numerical settings. While the addition of weights does not change the \\emph{asymptotic} complexity of the method, the results show that significant efficiency improvements over the standard MLMC formulation are possible, particularly when the coarse level approximations are poorly correlated.","sentences":["The Multilevel Monte Carlo (MLMC) method has been applied successfully in a wide range of settings since its first introduction by Giles (2008).","When using only two levels, the method can be viewed as a kind of control-variate approach to reduce variance, as earlier proposed by Kebaier (2005).","We introduce a generalization of the MLMC formulation by extending this control variate approach to any number of levels and deriving a recursive formula for computing the weights associated with the control variates and the optimal numbers of samples at the various levels.   ","We also show how the generalisation can also be applied to the \\emph{multi-index} MLMC method of Haji-Ali, Nobile, Tempone (2015), at the cost of solving a $(2^d-1)$-dimensional minimisation problem at each node when $d$ index dimensions are used.   ","The comparative performance of the weighted MLMC method is illustrated in a range of numerical settings.","While the addition of weights does not change the \\emph{asymptotic} complexity of the method, the results show that significant efficiency improvements over the standard MLMC formulation are possible, particularly when the coarse level approximations are poorly correlated."],"url":"http://arxiv.org/abs/2405.03453v1","category":"q-fin.CP"}
{"created":"2024-05-06 13:22:54","title":"Byzantine-Robust Gossip: Insights from a Dual Approach","abstract":"Distributed approaches have many computational benefits, but they are vulnerable to attacks from a subset of devices transmitting incorrect information. This paper investigates Byzantine-resilient algorithms in a decentralized setting, where devices communicate directly with one another. We leverage the so-called dual approach to design a general robust decentralized optimization method. We provide both global and local clipping rules in the special case of average consensus, with tight convergence guarantees. These clipping rules are practical, and yield results that finely characterize the impact of Byzantine nodes, highlighting for instance a qualitative difference in convergence between global and local clipping thresholds. Lastly, we demonstrate that they can serve as a basis for designing efficient attacks.","sentences":["Distributed approaches have many computational benefits, but they are vulnerable to attacks from a subset of devices transmitting incorrect information.","This paper investigates Byzantine-resilient algorithms in a decentralized setting, where devices communicate directly with one another.","We leverage the so-called dual approach to design a general robust decentralized optimization method.","We provide both global and local clipping rules in the special case of average consensus, with tight convergence guarantees.","These clipping rules are practical, and yield results that finely characterize the impact of Byzantine nodes, highlighting for instance a qualitative difference in convergence between global and local clipping thresholds.","Lastly, we demonstrate that they can serve as a basis for designing efficient attacks."],"url":"http://arxiv.org/abs/2405.03449v1","category":"cs.LG"}
{"created":"2024-05-06 12:54:22","title":"Improved Forward-Forward Contrastive Learning","abstract":"The backpropagation algorithm, or backprop, is a widely utilized optimization technique in deep learning. While there's growing evidence suggesting that models trained with backprop can accurately explain neuronal data, no backprop-like method has yet been discovered in the biological brain for learning. Moreover, employing a naive implementation of backprop in the brain has several drawbacks. In 2022, Geoffrey Hinton proposed a biologically plausible learning method known as the Forward-Forward (FF) algorithm. Shortly after this paper, a modified version called FFCL was introduced. However, FFCL had limitations, notably being a three-stage learning system where the final stage still relied on regular backpropagation. In our approach, we address these drawbacks by eliminating the last two stages of FFCL and completely removing regular backpropagation. Instead, we rely solely on local updates, offering a more biologically plausible alternative.","sentences":["The backpropagation algorithm, or backprop, is a widely utilized optimization technique in deep learning.","While there's growing evidence suggesting that models trained with backprop can accurately explain neuronal data, no backprop-like method has yet been discovered in the biological brain for learning.","Moreover, employing a naive implementation of backprop in the brain has several drawbacks.","In 2022, Geoffrey Hinton proposed a biologically plausible learning method known as the Forward-Forward (FF) algorithm.","Shortly after this paper, a modified version called FFCL was introduced.","However, FFCL had limitations, notably being a three-stage learning system where the final stage still relied on regular backpropagation.","In our approach, we address these drawbacks by eliminating the last two stages of FFCL and completely removing regular backpropagation.","Instead, we rely solely on local updates, offering a more biologically plausible alternative."],"url":"http://arxiv.org/abs/2405.03432v1","category":"cs.LG"}
{"created":"2024-05-06 12:42:35","title":"Dirichlet problem for degenerate Hessian quotient type curvature equations","abstract":"In the paper, we prove the existence and uniqueness results of the $C^{1,1}$ regular graphic hypersurface for Dirichlet problem of a class of degenerate Hessian quotient type curvature equations under the condition $\\psi^{\\frac{1}{k-l}}\\in C^{1,1}(\\overline{\\Omega}\\times\\mathbb{R}\\times\\mathbb{S}^n)$. Specially, we also consider the second order derivative estimates for the corresponding degenerate Hessian type curvature equations under the optimal condition $\\psi^{\\frac{1}{k-1}}\\in C^{1,1}(\\overline{\\Omega}\\times\\mathbb{R}\\times\\mathbb{S}^n)$.","sentences":["In the paper, we prove the existence and uniqueness results of the $C^{1,1}$ regular graphic hypersurface for Dirichlet problem of a class of degenerate Hessian quotient type curvature equations under the condition $\\psi^{\\frac{1}{k-l}}\\in C^{1,1}(\\overline{\\Omega}\\times\\mathbb{R}\\times\\mathbb{S}^n)$. Specially, we also consider the second order derivative estimates for the corresponding degenerate Hessian type curvature equations under the optimal condition $\\psi^{\\frac{1}{k-1}}\\in C^{1,1}(\\overline{\\Omega}\\times\\mathbb{R}\\times\\mathbb{S}^n)$."],"url":"http://arxiv.org/abs/2405.03422v1","category":"math.AP"}
{"created":"2024-05-06 12:40:25","title":"Homotopy methods for higher order shape optimization: A globalized shape-Newton method and Pareto-front tracing","abstract":"First order shape optimization methods, in general, require a large number of iterations until they reach a locally optimal design. While higher order methods can significantly reduce the number of iterations, they exhibit only local convergence properties, necessitating a sufficiently close initial guess. In this work, we present an unregularized shape-Newton method and combine shape optimization with homotopy (or continuation) methods in order to allow for the use of higher order methods even if the initial design is far from a solution. The idea of homotopy methods is to continuously connect the problem of interest with a simpler problem and to follow the corresponding solution path by a predictor-corrector scheme. We use a shape-Newton method as a corrector and arbitrary order shape derivatives for the predictor. Moreover, we apply homotopy methods also to the case of multi-objective shape optimization to efficiently obtain well-distributed points on a Pareto front. Finally, our results are substantiated with a set of numerical experiments.","sentences":["First order shape optimization methods, in general, require a large number of iterations until they reach a locally optimal design.","While higher order methods can significantly reduce the number of iterations, they exhibit only local convergence properties, necessitating a sufficiently close initial guess.","In this work, we present an unregularized shape-Newton method and combine shape optimization with homotopy (or continuation) methods in order to allow for the use of higher order methods even if the initial design is far from a solution.","The idea of homotopy methods is to continuously connect the problem of interest with a simpler problem and to follow the corresponding solution path by a predictor-corrector scheme.","We use a shape-Newton method as a corrector and arbitrary order shape derivatives for the predictor.","Moreover, we apply homotopy methods also to the case of multi-objective shape optimization to efficiently obtain well-distributed points on a Pareto front.","Finally, our results are substantiated with a set of numerical experiments."],"url":"http://arxiv.org/abs/2405.03421v1","category":"math.NA"}
{"created":"2024-05-06 12:30:08","title":"Unique solvability and error analysis of the Lagrange multiplier approach for gradient flows","abstract":"The unique solvability and error analysis of the original Lagrange multiplier approach proposed in [8] for gradient flows is studied in this paper. We identify a necessary and sufficient condition that must be satisfied for the nonlinear algebraic equation arising from the original Lagrange multiplier approach to admit a unique solution in the neighborhood of its exact solution, and propose a modified Lagrange multiplier approach so that the computation can continue even if the aforementioned condition is not satisfied. Using Cahn-Hilliard equation as an example, we prove rigorously the unique solvability and establish optimal error estimates of a second-order Lagrange multiplier scheme assuming this condition and that the time step is sufficient small. We also present numerical results to demonstrate that the modified Lagrange multiplier approach is much more robust and can use much larger time step than the original Lagrange multiplier approach.","sentences":["The unique solvability and error analysis of the original Lagrange multiplier approach proposed in [8] for gradient flows is studied in this paper.","We identify a necessary and sufficient condition that must be satisfied for the nonlinear algebraic equation arising from the original Lagrange multiplier approach to admit a unique solution in the neighborhood of its exact solution, and propose a modified Lagrange multiplier approach so that the computation can continue even if the aforementioned condition is not satisfied.","Using Cahn-Hilliard equation as an example, we prove rigorously the unique solvability and establish optimal error estimates of a second-order Lagrange multiplier scheme assuming this condition and that the time step is sufficient small.","We also present numerical results to demonstrate that the modified Lagrange multiplier approach is much more robust and can use much larger time step than the original Lagrange multiplier approach."],"url":"http://arxiv.org/abs/2405.03415v1","category":"math.NA"}
{"created":"2024-05-06 12:22:11","title":"Greedy Heuristics for Sampling-based Motion Planning in High-Dimensional State Spaces","abstract":"Sampling-based motion planning algorithms are very effective at finding solutions in high-dimensional continuous state spaces as they do not require prior approximations of the problem domain compared to traditional discrete graph-based searches. The anytime version of the Rapidly-exploring Random Trees (RRT) algorithm, denoted as RRT*, often finds high-quality solutions by incrementally approximating and searching the problem domain through random sampling. However, due to its low sampling efficiency and slow convergence rate, research has proposed many variants of RRT*, incorporating different heuristics and sampling strategies to overcome the constraints in complex planning problems. Yet, these approaches address specific convergence aspects of RRT* limitations, leaving a need for a sampling-based algorithm that can quickly find better solutions in complex high-dimensional state spaces with a faster convergence rate for practical motion planning applications. This article unifies and leverages the greedy search and heuristic techniques used in various RRT* variants to develop a greedy version of the anytime Rapidly-exploring Random Trees algorithm, denoted as Greedy RRT* (G-RRT*). It improves the initial solution-finding time of RRT* by maintaining two trees rooted at both the start and goal ends, advancing toward each other using greedy connection heuristics. It also accelerates the convergence rate of RRT* by introducing a greedy version of direct informed sampling procedure, which guides the sampling towards the promising region of the problem domain based on heuristics. We validate our approach on simulated planning problems, manipulation problems on Barrett WAM Arms, and on a self-reconfigurable robot, Panthera. Results show that G-RRT* produces asymptotically optimal solution paths and outperforms state-of-the-art RRT* variants, especially in high-dimensional planning problems.","sentences":["Sampling-based motion planning algorithms are very effective at finding solutions in high-dimensional continuous state spaces as they do not require prior approximations of the problem domain compared to traditional discrete graph-based searches.","The anytime version of the Rapidly-exploring Random Trees (RRT) algorithm, denoted as RRT*, often finds high-quality solutions by incrementally approximating and searching the problem domain through random sampling.","However, due to its low sampling efficiency and slow convergence rate, research has proposed many variants of RRT*, incorporating different heuristics and sampling strategies to overcome the constraints in complex planning problems.","Yet, these approaches address specific convergence aspects of RRT* limitations, leaving a need for a sampling-based algorithm that can quickly find better solutions in complex high-dimensional state spaces with a faster convergence rate for practical motion planning applications.","This article unifies and leverages the greedy search and heuristic techniques used in various RRT* variants to develop a greedy version of the anytime Rapidly-exploring Random Trees algorithm, denoted as Greedy RRT* (G-RRT*).","It improves the initial solution-finding time of RRT* by maintaining two trees rooted at both the start and goal ends, advancing toward each other using greedy connection heuristics.","It also accelerates the convergence rate of RRT* by introducing a greedy version of direct informed sampling procedure, which guides the sampling towards the promising region of the problem domain based on heuristics.","We validate our approach on simulated planning problems, manipulation problems on Barrett WAM Arms, and on a self-reconfigurable robot, Panthera.","Results show that G-RRT* produces asymptotically optimal solution paths and outperforms state-of-the-art RRT* variants, especially in high-dimensional planning problems."],"url":"http://arxiv.org/abs/2405.03411v1","category":"cs.RO"}
{"created":"2024-05-06 12:20:55","title":"LightTR: A Lightweight Framework for Federated Trajectory Recovery","abstract":"With the proliferation of GPS-equipped edge devices, huge trajectory data is generated and accumulated in various domains, motivating a variety of urban applications. Due to the limited acquisition capabilities of edge devices, a lot of trajectories are recorded at a low sampling rate, which may lead to the effectiveness drop of urban applications. We aim to recover a high-sampled trajectory based on the low-sampled trajectory in free space, i.e., without road network information, to enhance the usability of trajectory data and support urban applications more effectively. Recent proposals targeting trajectory recovery often assume that trajectories are available at a central location, which fail to handle the decentralized trajectories and hurt privacy. To bridge the gap between decentralized training and trajectory recovery, we propose a lightweight framework, LightTR, for federated trajectory recovery based on a client-server architecture, while keeping the data decentralized and private in each client/platform center (e.g., each data center of a company). Specifically, considering the limited processing capabilities of edge devices, LightTR encompasses a light local trajectory embedding module that offers improved computational efficiency without compromising its feature extraction capabilities. LightTR also features a meta-knowledge enhanced local-global training scheme to reduce communication costs between the server and clients and thus further offer efficiency improvement. Extensive experiments demonstrate the effectiveness and efficiency of the proposed framework.","sentences":["With the proliferation of GPS-equipped edge devices, huge trajectory data is generated and accumulated in various domains, motivating a variety of urban applications.","Due to the limited acquisition capabilities of edge devices, a lot of trajectories are recorded at a low sampling rate, which may lead to the effectiveness drop of urban applications.","We aim to recover a high-sampled trajectory based on the low-sampled trajectory in free space, i.e., without road network information, to enhance the usability of trajectory data and support urban applications more effectively.","Recent proposals targeting trajectory recovery often assume that trajectories are available at a central location, which fail to handle the decentralized trajectories and hurt privacy.","To bridge the gap between decentralized training and trajectory recovery, we propose a lightweight framework, LightTR, for federated trajectory recovery based on a client-server architecture, while keeping the data decentralized and private in each client/platform center (e.g., each data center of a company).","Specifically, considering the limited processing capabilities of edge devices, LightTR encompasses a light local trajectory embedding module that offers improved computational efficiency without compromising its feature extraction capabilities.","LightTR also features a meta-knowledge enhanced local-global training scheme to reduce communication costs between the server and clients and thus further offer efficiency improvement.","Extensive experiments demonstrate the effectiveness and efficiency of the proposed framework."],"url":"http://arxiv.org/abs/2405.03409v1","category":"cs.LG"}
{"created":"2024-05-06 12:13:47","title":"Improved scalar auxiliary variable schemes for original energy stability of gradient flows","abstract":"Scalar auxiliary variable (SAV) methods are a class of linear schemes for solving gradient flows that are known for the stability of a `modified' energy. In this paper, we propose an improved SAV (iSAV) scheme that not only retains the complete linearity but also ensures rigorously the stability of the original energy. The convergence and optimal error bound are rigorously established for the iSAV scheme and discussions are made for its high-order extension. Extensive numerical experiments are done to validate the convergence, robustness and energy stability of iSAV, and some comparisons are made.","sentences":["Scalar auxiliary variable (SAV) methods are a class of linear schemes for solving gradient flows that are known for the stability of a `modified' energy.","In this paper, we propose an improved SAV (iSAV) scheme that not only retains the complete linearity but also ensures rigorously the stability of the original energy.","The convergence and optimal error bound are rigorously established for the iSAV scheme and discussions are made for its high-order extension.","Extensive numerical experiments are done to validate the convergence, robustness and energy stability of iSAV, and some comparisons are made."],"url":"http://arxiv.org/abs/2405.03403v1","category":"math.NA"}
{"created":"2024-05-06 12:12:03","title":"Distributional Reference Class Forecasting of Corporate Sales Growth With Multiple Reference Variables","abstract":"This paper introduces an approach to reference class selection in distributional forecasting with an application to corporate sales growth rates using several co-variates as reference variables, that are implicit predictors. The method can be used to detect expert or model-based forecasts exposed to (behavioral) bias or to forecast distributions with reference classes. These are sets of similar entities, here firms, and rank based algorithms for their selection are proposed, including an optional preprocessing data dimension reduction via principal components analysis. Forecasts are optimal if they match the underlying distribution as closely as possible. Probability integral transform values rank the forecast capability of different reference variable sets and algorithms in a backtest on a data set of 21,808 US firms over the time period 1950 - 2019. In particular, algorithms on dimension reduced variables perform well using contemporaneous balance sheet and financial market parameters along with past sales growth rates and past operating margins changes. Comparisions of actual analysts' estimates to distributional forecasts and of historic distributional forecasts to realized sales growth illustrate the practical use of the method.","sentences":["This paper introduces an approach to reference class selection in distributional forecasting with an application to corporate sales growth rates using several co-variates as reference variables, that are implicit predictors.","The method can be used to detect expert or model-based forecasts exposed to (behavioral) bias or to forecast distributions with reference classes.","These are sets of similar entities, here firms, and rank based algorithms for their selection are proposed, including an optional preprocessing data dimension reduction via principal components analysis.","Forecasts are optimal if they match the underlying distribution as closely as possible.","Probability integral transform values rank the forecast capability of different reference variable sets and algorithms in a backtest on a data set of 21,808 US firms over the time period 1950 - 2019.","In particular, algorithms on dimension reduced variables perform well using contemporaneous balance sheet and financial market parameters along with past sales growth rates and past operating margins changes.","Comparisions of actual analysts' estimates to distributional forecasts and of historic distributional forecasts to realized sales growth illustrate the practical use of the method."],"url":"http://arxiv.org/abs/2405.03402v1","category":"q-fin.ST"}
{"created":"2024-05-07 17:43:02","title":"Letter of Intent: Open Charm at JLab with the sPHENIX MAPS tracker","abstract":"We propose a physics program at JLab with CLAS12 focusing on open-charm measurements, aiming to complement and expand current studies of $J/\\psi$ at (sub) threshold. This program will aid us in elucidating the $J/\\psi$ production mechanisms, which is crucial for interpreting data in terms of gluon form factors and offer potential insights into the intrinsic charm hypothesis and cold-nuclear matter effects. We discuss the technical feasibility of integrating the sPHENIX monolithic-active-pixel sensor (MAPS) tracker, known as MVTX, with the CLAS12 detector. The sPHENIX MTVX would support an open-charm program by providing excellent secondary-vertex performance for tagging $D$ mesons. We study the kinematics of $\\gamma p \\to \\bar{D}^{0}\\Lambda_{c}$ through phase-space simulations and estimate rates for the tagged quasi-photoproduction regime available with the CLAS12 forward tagger. While open-charm cross-sections at threshold remain uncertain, various predictions suggest that these measurements could be feasible when combined with conservative estimates of detector acceptance and luminosity. These preliminary estimates motivate detailed Geant detector simulations of signals and backgrounds, along with thorough technical assessments of operating conditions, to further explore the feasibility of these measurements in future dedicated CLAS12 experiments at JLab.","sentences":["We propose a physics program at JLab with CLAS12 focusing on open-charm measurements, aiming to complement and expand current studies of $J/\\psi$ at (sub) threshold.","This program will aid us in elucidating the $J/\\psi$ production mechanisms, which is crucial for interpreting data in terms of gluon form factors and offer potential insights into the intrinsic charm hypothesis and cold-nuclear matter effects.","We discuss the technical feasibility of integrating the sPHENIX monolithic-active-pixel sensor (MAPS) tracker, known as MVTX, with the CLAS12 detector.","The sPHENIX MTVX would support an open-charm program by providing excellent secondary-vertex performance for tagging $D$ mesons.","We study the kinematics of $\\gamma p \\to \\bar{D}^{0}\\Lambda_{c}$ through phase-space simulations and estimate rates for the tagged quasi-photoproduction regime available with the CLAS12 forward tagger.","While open-charm cross-sections at threshold remain uncertain, various predictions suggest that these measurements could be feasible when combined with conservative estimates of detector acceptance and luminosity.","These preliminary estimates motivate detailed Geant detector simulations of signals and backgrounds, along with thorough technical assessments of operating conditions, to further explore the feasibility of these measurements in future dedicated CLAS12 experiments at JLab."],"url":"http://arxiv.org/abs/2405.04511v1","category":"nucl-ex"}
{"created":"2024-05-07 17:21:34","title":"Limiting distributions of the Spherical model and Spin $O(N)$ model: Appearance of GFF","abstract":"We revisit the relation between the spherical model of Berlin-Kac and the spin $O(N)$ model in the limit $N \\to \\infty$ and explain how they are related via the discrete Gaussian free field (GFF).   More precisely, using probabilistic limit theorems and concentration of measure we first prove that the infinite volume limit of the spherical model on a $d$-dimensional torus is a massive GFF in the high-temperature regime, a standard GFF at the critical temperature, and a standard GFF plus a Rademacher random constant in the low-temperature regime. The proof in the case of the critical temperature appears to be new and requires a fine understanding of the zero-average Green's function on the torus.   For the spin $O(N)$ model, we study the model in the double limit of the spin-dimensionality and the torus size. We take the limit as first the spin-dimension $N$ goes to infinity, and then the size of the torus, and obtain that the different spin coordinates become i.i.d. fields, whose distribution in the high-temperature regime is a massive GFF, a standard GFF at the critical temperature, and a standard GFF plus a Gaussian random constant in the low-temperature regime.   In particular, this means that although the limiting free energies per site of the two models agree at all temperatures, their actual finite-dimensional laws still differ in terms of their zero modes in the low-temperature regime.","sentences":["We revisit the relation between the spherical model of Berlin-Kac and the spin $O(N)$ model in the limit $N \\to \\infty$ and explain how they are related via the discrete Gaussian free field (GFF).   ","More precisely, using probabilistic limit theorems and concentration of measure we first prove that the infinite volume limit of the spherical model on a $d$-dimensional torus is a massive GFF in the high-temperature regime, a standard GFF at the critical temperature, and a standard GFF plus a Rademacher random constant in the low-temperature regime.","The proof in the case of the critical temperature appears to be new and requires a fine understanding of the zero-average Green's function on the torus.   ","For the spin $O(N)$ model, we study the model in the double limit of the spin-dimensionality and the torus size.","We take the limit as first the spin-dimension $N$ goes to infinity, and then the size of the torus, and obtain that the different spin coordinates become i.i.d. fields, whose distribution in the high-temperature regime is a massive GFF, a standard GFF at the critical temperature, and a standard GFF plus a Gaussian random constant in the low-temperature regime.   ","In particular, this means that although the limiting free energies per site of the two models agree at all temperatures, their actual finite-dimensional laws still differ in terms of their zero modes in the low-temperature regime."],"url":"http://arxiv.org/abs/2405.04501v1","category":"math.PR"}
{"created":"2024-05-07 16:46:59","title":"Measurement of Spin-Polarized Photoemission from Wurtzite and Zinc-Blende Gallium Nitride Photocathodes","abstract":"Spin-polarized photoemission from wurtzite and zinc-blende gallium nitride (GaN) photocathodes has been observed and measured for the first time. The p-doped GaN photocathodes were epitaxially grown and activated to negative electron affinity (NEA) with a cesium monolayer deposited on their surfaces. A field-retarding Mott polarimeter was used to measure the spin-polarization of electrons photoemitted from the top of the valence band. A spectral scan with a tunable optical parametric amplifier (OPA) constructed to provide low-bandwidth light revealed peak spin polarizations of 17% and 29% in the wurtzite and zinc-blende photocathodes, respectively. Zinc-blende GaN results are analyzed with a spin-polarization model accounting for experimental parameters used in the measurements, while possible mechanisms influencing the obtained spin polarization values of wurtzite GaN are discussed.","sentences":["Spin-polarized photoemission from wurtzite and zinc-blende gallium nitride (GaN) photocathodes has been observed and measured for the first time.","The p-doped GaN photocathodes were epitaxially grown and activated to negative electron affinity (NEA) with a cesium monolayer deposited on their surfaces.","A field-retarding Mott polarimeter was used to measure the spin-polarization of electrons photoemitted from the top of the valence band.","A spectral scan with a tunable optical parametric amplifier (OPA) constructed to provide low-bandwidth light revealed peak spin polarizations of 17% and 29% in the wurtzite and zinc-blende photocathodes, respectively.","Zinc-blende GaN results are analyzed with a spin-polarization model accounting for experimental parameters used in the measurements, while possible mechanisms influencing the obtained spin polarization values of wurtzite GaN are discussed."],"url":"http://arxiv.org/abs/2405.04481v1","category":"physics.acc-ph"}
{"created":"2024-05-07 16:31:32","title":"Galactic transient sources with the Cherenkov Telescope Array","abstract":"A wide variety of Galactic sources show transient emission at soft and hard X-ray energies: low-mass and high-mass X-ray binaries containing compact objects (e.g., novae, microquasars, transitional millisecond pulsars, supergiant fast X-ray transients), isolated neutron stars exhibiting extreme variability as magnetars as well as pulsar wind nebulae. Although most of them can show emission up to MeV and/or GeV energies, many have not yet been detected in the TeV domain by Imaging Atmospheric Cherenkov Telescopes. In this paper, we explore the feasibility of detecting new Galactic transients with the Cherenkov Telescope Array (CTA) and the prospects for studying them with Target of Opportunity observations. We show that CTA will likely detect new sources in the TeV regime, such as the massive microquasars in the Cygnus region, low-mass X-ray binaries with low-viewing angle, flaring emission from the Crab pulsar-wind nebula or other novae explosions, among others. We also discuss the multi-wavelength synergies with other instruments and large astronomical facilities.","sentences":["A wide variety of Galactic sources show transient emission at soft and hard X-ray energies: low-mass and high-mass X-ray binaries containing compact objects (e.g., novae, microquasars, transitional millisecond pulsars, supergiant fast X-ray transients), isolated neutron stars exhibiting extreme variability as magnetars as well as pulsar wind nebulae.","Although most of them can show emission up to MeV and/or GeV energies, many have not yet been detected in the TeV domain by Imaging Atmospheric Cherenkov Telescopes.","In this paper, we explore the feasibility of detecting new Galactic transients with the Cherenkov Telescope Array (CTA) and the prospects for studying them with Target of Opportunity observations.","We show that CTA will likely detect new sources in the TeV regime, such as the massive microquasars in the Cygnus region, low-mass X-ray binaries with low-viewing angle, flaring emission from the Crab pulsar-wind nebula or other novae explosions, among others.","We also discuss the multi-wavelength synergies with other instruments and large astronomical facilities."],"url":"http://arxiv.org/abs/2405.04469v1","category":"astro-ph.HE"}
{"created":"2024-05-07 16:20:10","title":"Chemical abundances along the quasar main sequence","abstract":"The 4D eigenvector 1 (E1) sequence has emerged as a powerful tool for organizing the observational and physical characteristics of type-1 active galactic nuclei (AGNs). In this study, we present a comprehensive analysis of the metallicity of the broad line region gas, incorporating both new data and previously published findings, to assess the presence of any trend along the sequence. We perform a multi-component analysis on the strongest UV and optical emission lines, compute $\\sim 10$ diagnostic ratios, and compare them with the prediction of CLOUDY photoionization simulations, identifying a photoionization solution closest to the data. Our investigation reveals a consistent pattern along the optical plane of the E1. We observe a systematic progression in metallicity, ranging from sub-solar values to metallicity levels several times higher than solar values. These findings underscore the role of metallicity as a fundamental correlate of the 4DE1/main sequence. Extreme values of metallicity, at least several tens solar, are confirmed in low-$z$ AGNs radiating at a high Eddington ratio, although the origin of the extreme enrichment remains open to debate.","sentences":["The 4D eigenvector 1 (E1) sequence has emerged as a powerful tool for organizing the observational and physical characteristics of type-1 active galactic nuclei (AGNs).","In this study, we present a comprehensive analysis of the metallicity of the broad line region gas, incorporating both new data and previously published findings, to assess the presence of any trend along the sequence.","We perform a multi-component analysis on the strongest UV and optical emission lines, compute $\\sim 10$ diagnostic ratios, and compare them with the prediction of CLOUDY photoionization simulations, identifying a photoionization solution closest to the data.","Our investigation reveals a consistent pattern along the optical plane of the E1.","We observe a systematic progression in metallicity, ranging from sub-solar values to metallicity levels several times higher than solar values.","These findings underscore the role of metallicity as a fundamental correlate of the 4DE1/main sequence.","Extreme values of metallicity, at least several tens solar, are confirmed in low-$z$ AGNs radiating at a high Eddington ratio, although the origin of the extreme enrichment remains open to debate."],"url":"http://arxiv.org/abs/2405.04456v1","category":"astro-ph.GA"}
{"created":"2024-05-07 16:16:08","title":"Constraining Millicharged dark matter with Gravitational positivity bounds","abstract":"Gravitational positivity bounds provide consistency conditions for effective field theories with gravity. They turn out to be phenomenologically useful by providing lower bounds in parameters of new physics beyond the Standard Models (BSM). In this paper, we derive constraints on millicharged fermion dark matter models with massless dark photon using gravitational positivity bounds. Combining them with upper bounds from cosmological and astrophysical observations, we can severely constrain the parameter space of the model. In particular, we show that when the dark matter mass is lighter than the solar core temperature, most of the parameter region is excluded by combining gravitational positivity bounds and the stellar bounds.","sentences":["Gravitational positivity bounds provide consistency conditions for effective field theories with gravity.","They turn out to be phenomenologically useful by providing lower bounds in parameters of new physics beyond the Standard Models (BSM).","In this paper, we derive constraints on millicharged fermion dark matter models with massless dark photon using gravitational positivity bounds.","Combining them with upper bounds from cosmological and astrophysical observations, we can severely constrain the parameter space of the model.","In particular, we show that when the dark matter mass is lighter than the solar core temperature, most of the parameter region is excluded by combining gravitational positivity bounds and the stellar bounds."],"url":"http://arxiv.org/abs/2405.04454v1","category":"hep-ph"}
{"created":"2024-05-07 15:49:38","title":"Enhancement of the Curie temperature in single crystalline ferromagnetic LaCrGe$_3$ by electron irradiation-induced disorder","abstract":"LaCrGe$_3$ has attracted attention as a potential candidate for studies of quantum phase transitions in a ferromagnetic material. The application of pressure avoids a quantum critical point by developing a new magnetic phase. It was suggested that the disorder may provide an alternative route to a quantum critical point. We used low-temperature 2.5 MeV electron irradiation to induce relatively small amounts of point-like disorder in single crystals of LaCrGe$_3$. Irradiation leads to an increase of the resistivity at all temperatures with some deviation from the Matthiessen rule. Hall effect measurements show that electron irradiation does not cause any detectable change in the carrier density. Unexpectedly, the Curie temperature, $T_{\\text{FM}}$, \\emph{increases} with the increase of disorder from approximately 90 K in pristine samples up to nearly 100 K in the heavily irradiated sample, with a tendency towards saturation at higher doses. Although the mechanism of this effect is not entirely clear, we conclude that it cannot be caused by effective ``doping\" or ``pressure\" due to electron irradiation. We suggest that disorder-induced broadening of a sharp peak in the density of states, $D(E)$, situated at $E_p=E_F-0.25$ eV below the Fermi energy, $E_F$, causes an increase in $D(E_F)$, leading to an enhancement of $T_\\text{FM}$ in this itinerant ferromagnet.","sentences":["LaCrGe$_3$ has attracted attention as a potential candidate for studies of quantum phase transitions in a ferromagnetic material.","The application of pressure avoids a quantum critical point by developing a new magnetic phase.","It was suggested that the disorder may provide an alternative route to a quantum critical point.","We used low-temperature 2.5 MeV electron irradiation to induce relatively small amounts of point-like disorder in single crystals of LaCrGe$_3$. Irradiation leads to an increase of the resistivity at all temperatures with some deviation from the Matthiessen rule.","Hall effect measurements show that electron irradiation does not cause any detectable change in the carrier density.","Unexpectedly, the Curie temperature, $T_{\\text{FM}}$, \\emph{increases} with the increase of disorder from approximately 90 K in pristine samples up to nearly 100 K in the heavily irradiated sample, with a tendency towards saturation at higher doses.","Although the mechanism of this effect is not entirely clear, we conclude that it cannot be caused by effective ``doping\" or ``pressure\" due to electron irradiation.","We suggest that disorder-induced broadening of a sharp peak in the density of states, $D(E)$, situated at $E_p=E_F-0.25$ eV below the Fermi energy, $E_F$, causes an increase in $D(E_F)$, leading to an enhancement of $T_\\text{FM}$ in this itinerant ferromagnet."],"url":"http://arxiv.org/abs/2405.04429v1","category":"cond-mat.str-el"}
{"created":"2024-05-07 15:44:20","title":"Transportability of Principal Causal Effects","abstract":"Recent research in causal inference has made important progress in addressing challenges to the external validity of trial findings. Such methods weight trial participant data to more closely resemble the distribution of effect-modifying covariates in a well-defined target population. In the presence of participant non-adherence to study medication, these methods effectively transport an intention-to-treat effect that averages over heterogeneous compliance behaviors. In this paper, we develop a principal stratification framework to identify causal effects conditioning on both on compliance behavior and membership in the target population. We also develop non-parametric efficiency theory for and construct efficient estimators of such \"transported\" principal causal effects and characterize their finite-sample performance in simulation experiments. While this work focuses on treatment non-adherence, the framework is applicable to a broad class of estimands that target effects in clinically-relevant, possibly latent subsets of a target population.","sentences":["Recent research in causal inference has made important progress in addressing challenges to the external validity of trial findings.","Such methods weight trial participant data to more closely resemble the distribution of effect-modifying covariates in a well-defined target population.","In the presence of participant non-adherence to study medication, these methods effectively transport an intention-to-treat effect that averages over heterogeneous compliance behaviors.","In this paper, we develop a principal stratification framework to identify causal effects conditioning on both on compliance behavior and membership in the target population.","We also develop non-parametric efficiency theory for and construct efficient estimators of such \"transported\" principal causal effects and characterize their finite-sample performance in simulation experiments.","While this work focuses on treatment non-adherence, the framework is applicable to a broad class of estimands that target effects in clinically-relevant, possibly latent subsets of a target population."],"url":"http://arxiv.org/abs/2405.04419v1","category":"stat.ME"}
{"created":"2024-05-07 15:06:32","title":"COMAP Galactic Science I: Observations of Spinning Dust Emission at 30GHz in Dark Clouds Surrounding the \u03bb-Orionis Hii Region","abstract":"Anomalous Microwave Emission (AME) is a major component of Galactic emission in the frequency band 10 to 60 GHz and is commonly modelled as rapidly rotating spinning dust grains. The photodissociation region (PDR) at the boundary of the $\\lambda$-Orionis Hii region has been identified by several recent analyses as one of the brightest spinning dust emitting sources in the sky. We investigate the Barnard 30 dark cloud, a dark cloud embedded within the $\\lambda$-Orionis PDR. We use total-power observations of Barnard 30 from the CO Mapping Array Project (COMAP) pathfinder instrument at 26 to 34GHz with a resolution of 4.5 arcminutes alongside existing data from Planck, WISE, IRAS, ACT, and the 1.447GHz GALFACTS survey. We use aperture photometry and template fitting to measure the spectral energy distribution of Barnard 30. We find that the spinning dust is the dominant emission component in the 26 to 34GHz range at the $7 \\sigma$ level ($S_{30GHz} = 2.85\\pm0.43$Jy). We find no evidence that polycyclic aromatic hydrocarbons are the preferred carrier for the spinning dust emission, suggesting that the spinning dust carriers are due to a mixed population of very small grains. Finally, we find evidence for variations in spinning dust emissivity and peak frequency within Barnard 30, and that these variations are possibly driven by changes in dust grain population and the total radiation field. Confirming the origin of the variations in the spinning dust spectrum will require both future COMAP observations at 15GHz combined with spectroscopic mid-infrared data of Barnard 30.","sentences":["Anomalous Microwave Emission (AME) is a major component of Galactic emission in the frequency band 10 to 60 GHz and is commonly modelled as rapidly rotating spinning dust grains.","The photodissociation region (PDR) at the boundary of the $\\lambda$-Orionis Hii region has been identified by several recent analyses as one of the brightest spinning dust emitting sources in the sky.","We investigate the Barnard 30 dark cloud, a dark cloud embedded within the $\\lambda$-Orionis PDR.","We use total-power observations of Barnard 30 from the CO Mapping Array Project (COMAP) pathfinder instrument at 26 to 34GHz with a resolution of 4.5 arcminutes alongside existing data from Planck, WISE, IRAS, ACT, and the 1.447GHz GALFACTS survey.","We use aperture photometry and template fitting to measure the spectral energy distribution of Barnard 30.","We find that the spinning dust is the dominant emission component in the 26 to 34GHz range at the $7 \\sigma$ level ($S_{30GHz} = 2.85\\pm0.43$Jy).","We find no evidence that polycyclic aromatic hydrocarbons are the preferred carrier for the spinning dust emission, suggesting that the spinning dust carriers are due to a mixed population of very small grains.","Finally, we find evidence for variations in spinning dust emissivity and peak frequency within Barnard 30, and that these variations are possibly driven by changes in dust grain population and the total radiation field.","Confirming the origin of the variations in the spinning dust spectrum will require both future COMAP observations at 15GHz combined with spectroscopic mid-infrared data of Barnard 30."],"url":"http://arxiv.org/abs/2405.04383v1","category":"astro-ph.GA"}
{"created":"2024-05-07 14:14:46","title":"Investigations on the weak decays of $D\\bar{B}$ molecules","abstract":"The decays of exotic states discovered experimentally always proceed via the strong and electromagnetic interactions. Recently, a tetraquark state with the quark content $bc\\bar{q}\\bar{q}$ was predicted by Lattice QCD simulations. It is below the mass threshold of $D\\bar{B}$, which can only decay via the weak interaction. In this work, based on the decay mechanism of $T_{cc}$ as a $DD^*$ molecule, we propose that the decays of the $bc\\bar{q}\\bar{q}$ tertaquark state as a $D\\bar{B}$ molecule proceed via the Cabibbo-favored weak decays of the $\\bar{B}$ or $D$ meson, accompanied by the tree-level decay modes and the triangle decay modes. Our results indicate that the branching fraction of the $D\\bar{B}$ molecule decaying into $\\pi^+ K^{-} \\bar{B}^0$ is sizable, which is a good channel to observe the $D\\bar{B}$ molecule in future experiments.","sentences":["The decays of exotic states discovered experimentally always proceed via the strong and electromagnetic interactions.","Recently, a tetraquark state with the quark content $bc\\bar{q}\\bar{q}$ was predicted by Lattice QCD simulations.","It is below the mass threshold of $D\\bar{B}$, which can only decay via the weak interaction.","In this work, based on the decay mechanism of $T_{cc}$ as a $DD^*$ molecule, we propose that the decays of the $bc\\bar{q}\\bar{q}$ tertaquark state as a $D\\bar{B}$ molecule proceed via the Cabibbo-favored weak decays of the $\\bar{B}$ or $D$ meson, accompanied by the tree-level decay modes and the triangle decay modes.","Our results indicate that the branching fraction of the $D\\bar{B}$ molecule decaying into $\\pi^+ K^{-} \\bar{B}^0$ is sizable, which is a good channel to observe the $D\\bar{B}$ molecule in future experiments."],"url":"http://arxiv.org/abs/2405.04341v1","category":"hep-ph"}
{"created":"2024-05-07 14:11:13","title":"On the Kauffman bracket skein module of $(S^1 \\times S^2) \\ \\# \\ (S^1 \\times S^2)$","abstract":"Determining the structure of the Kauffman bracket skein module of all $3$-manifolds over the ring of Laurent polynomials $\\mathbb Z[A^{\\pm 1}]$ is a big open problem in skein theory. Very little is known about the skein module of non-prime manifolds over this ring. In this paper, we compute the Kauffman bracket skein module of the $3$-manifold $(S^1 \\times S^2) \\ \\# \\ (S^1 \\times S^2)$ over the ring $\\mathbb Z[A^{\\pm 1}]$. We do this by analysing the submodule of handle sliding relations, for which we provide a suitable basis. Along the way we also compute the Kauffman bracket skein module of $(S^1 \\times S^2) \\ \\# \\ (S^1 \\times D^2)$. Furthermore, we show that the skein module of $(S^1 \\times S^2) \\ \\# \\ (S^1 \\times S^2)$ does not split into the sum of free and $(A^k-A^{-k})$-torsion modules, for each $k\\geq 1$.","sentences":["Determining the structure of the Kauffman bracket skein module of all $3$-manifolds over the ring of Laurent polynomials $\\mathbb Z[A^{\\pm 1}]$ is a big open problem in skein theory.","Very little is known about the skein module of non-prime manifolds over this ring.","In this paper, we compute the Kauffman bracket skein module of the $3$-manifold $(S^1 \\times S^2)","\\ \\# \\ (S^1 \\times S^2)$ over the ring $\\mathbb Z[A^{\\pm 1}]$.","We do this by analysing the submodule of handle sliding relations, for which we provide a suitable basis.","Along the way we also compute the Kauffman bracket skein module of $(S^1 \\times S^2)","\\ \\# \\ (S^1 \\times","D^2)$. Furthermore, we show that the skein module of $(S^1 \\times S^2)","\\ \\# \\ (S^1 \\times S^2)$ does not split into the sum of free and $(A^k-A^{-k})$-torsion modules, for each $k\\geq 1$."],"url":"http://arxiv.org/abs/2405.04337v1","category":"math.GT"}
{"created":"2024-05-07 13:55:16","title":"A Calibratable Model for Fast Energy Estimation of MVM Operations on RRAM Crossbars","abstract":"The surge in AI usage demands innovative power reduction strategies. Novel Compute-in-Memory (CIM) architectures, leveraging advanced memory technologies, hold the potential for significantly lowering energy consumption by integrating storage with parallel Matrix-Vector-Multiplications (MVMs). This study addresses the 1T1R RRAM crossbar, a core component in numerous CIM architectures. We introduce an abstract model and a calibration methodology for estimating operational energy. Our tool condenses circuit-level behaviour into a few parameters, facilitating energy assessments for DNN workloads. Validation against low-level SPICE simulations demonstrates speedups of up to 1000x and energy estimations with errors below 1%.","sentences":["The surge in AI usage demands innovative power reduction strategies.","Novel Compute-in-Memory (CIM) architectures, leveraging advanced memory technologies, hold the potential for significantly lowering energy consumption by integrating storage with parallel Matrix-Vector-Multiplications (MVMs).","This study addresses the 1T1R RRAM crossbar, a core component in numerous CIM architectures.","We introduce an abstract model and a calibration methodology for estimating operational energy.","Our tool condenses circuit-level behaviour into a few parameters, facilitating energy assessments for DNN workloads.","Validation against low-level SPICE simulations demonstrates speedups of up to 1000x and energy estimations with errors below 1%."],"url":"http://arxiv.org/abs/2405.04326v1","category":"eess.SP"}
{"created":"2024-05-07 13:39:57","title":"Long-range magnetic order in CePdAl$_3$ enabled by orthorhombic deformation","abstract":"We investigate the effect of structural deformation on the magnetic properties of orthorhombic CePdAl$_3$ in relation to its tetragonal polymorph. Utilizing x-ray and neutron diffraction we establish that the crystal structure has the $Cmcm$ space group symmetry and exhibits pseudo-tetragonal twinning. According to density-functional calculations the tetragonal-orthorhombic deformation mechanism has its grounds in relatively small free enthalpy difference between the polymorphs, allowing either phase to be quenched and fully accounts for the twinned microstructure of the orthorhombic phase. Neutron diffraction measurements show that orthorhombic CePdAl$_3$ establishes long-range magnetic order below $T_\\mathrm{N}$=5.29 (5) K characterized by a collinear, antiferromagnetic arrangement of magnetic moments. Magnetic anisotropies of orthorhombic CePdAl$_3$ arise from strong spin-orbit coupling as evidenced by the crystal-field splitting of the $4f$ multiplet, fully characterised with neutron spectroscopy. We discuss the potential mechanism of frustration posed by antiferromagnetic interactions between nearest neighbours in the tetragonal phase, which hinders the formation of long-range magnetic order in tetragonal CePdAl$_3$. We propose that orthorhombic deformation releases the frustration and allows for long-range magnetic order.","sentences":["We investigate the effect of structural deformation on the magnetic properties of orthorhombic CePdAl$_3$ in relation to its tetragonal polymorph.","Utilizing x-ray and neutron diffraction we establish that the crystal structure has the $Cmcm$ space group symmetry and exhibits pseudo-tetragonal twinning.","According to density-functional calculations the tetragonal-orthorhombic deformation mechanism has its grounds in relatively small free enthalpy difference between the polymorphs, allowing either phase to be quenched and fully accounts for the twinned microstructure of the orthorhombic phase.","Neutron diffraction measurements show that orthorhombic CePdAl$_3","$ establishes long-range magnetic order below $T_\\mathrm{N}$=5.29 (5) K characterized by a collinear, antiferromagnetic arrangement of magnetic moments.","Magnetic anisotropies of orthorhombic CePdAl$_3$ arise from strong spin-orbit coupling as evidenced by the crystal-field splitting of the $4f$ multiplet, fully characterised with neutron spectroscopy.","We discuss the potential mechanism of frustration posed by antiferromagnetic interactions between nearest neighbours in the tetragonal phase, which hinders the formation of long-range magnetic order in tetragonal CePdAl$_3$.","We propose that orthorhombic deformation releases the frustration and allows for long-range magnetic order."],"url":"http://arxiv.org/abs/2405.04318v1","category":"cond-mat.str-el"}
{"created":"2024-05-07 12:26:10","title":"Morphological Evidence for the eROSITA Bubbles Being Giant and Distant Structures","abstract":"There are two contradictory views of the eROSITA bubbles: either a 10 kpc-scale pair of giant bubbles blown by the Galactic center (GC), or a 100 pc-scale local structure coincidentally located in the direction of GC. A key element of this controversy is the distance to the bubbles. Based on the 3D dust distribution in the Galactic plane, we found three isolated, distant (500-800 pc) clouds at intermediate Galactic latitudes. Their projected morphologies perfectly match the X-ray shadows on the defining features of the north eROSITA bubble, i.e., the North Polar Spur (NPS) and the Lotus Petal Cloud (LPC), indicating that both the NPS and LPC are distant with a distance lower limit of nearly 1kpc. In the X-ray dark region between the NPS and LPC, we found a few polarized radio arcs and attributed them to the bubble's shock front. These arcs match up perfectly with the outer border of the NPS and LPC and provide a way to define the bubble's border. The border defined in this way can be well described by the line-of-sight tangent of a 3D skewed cup model rooted in the GC. We conclude that, instead of being two independent, distant features, NPS and LPC compose a single, giant bubble, which, therefore, is most plausibly a 10-kpc scale bubble rooted at the GC.","sentences":["There are two contradictory views of the eROSITA bubbles: either a 10 kpc-scale pair of giant bubbles blown by the Galactic center (GC), or a 100 pc-scale local structure coincidentally located in the direction of GC.","A key element of this controversy is the distance to the bubbles.","Based on the 3D dust distribution in the Galactic plane, we found three isolated, distant (500-800 pc) clouds at intermediate Galactic latitudes.","Their projected morphologies perfectly match the X-ray shadows on the defining features of the north eROSITA bubble, i.e., the North Polar Spur (NPS) and the Lotus Petal Cloud (LPC), indicating that both the NPS and LPC are distant with a distance lower limit of nearly 1kpc.","In the X-ray dark region between the NPS and LPC, we found a few polarized radio arcs and attributed them to the bubble's shock front.","These arcs match up perfectly with the outer border of the NPS and LPC and provide a way to define the bubble's border.","The border defined in this way can be well described by the line-of-sight tangent of a 3D skewed cup model rooted in the GC.","We conclude that, instead of being two independent, distant features, NPS and LPC compose a single, giant bubble, which, therefore, is most plausibly a 10-kpc scale bubble rooted at the GC."],"url":"http://arxiv.org/abs/2405.04264v1","category":"astro-ph.GA"}
{"created":"2024-05-07 12:26:05","title":"Energy-Efficient Deployment of Stateful FaaS Vertical Applications on Edge Data Networks","abstract":"5G and beyond support the deployment of vertical applications, which is particularly appealing in combination with network slicing and edge computing to create a logically isolated environment for executing customer services. Even if serverless computing has gained significant interest as a cloud-native technology its adoption at the edge is lagging, especially because of the need to support stateful tasks, which are commonplace in, e.g., cognitive services, but not fully amenable to being deployed on limited and decentralized computing infrastructures. In this work, we study the emerging paradigm of stateful Function as a Service (FaaS) with lightweight task abstractions in WebAssembly. Specifically, we assess the implications of deploying inter-dependent tasks with an internal state on edge computing resources using a stateless vs. stateful approach and then derive a mathematical model to estimate the energy consumption of a workload with given characteristics, considering the power used for both processing and communication. The model is used in extensive simulations to determine the impact of key factors and assess the energy trade-offs of stateless vs. stateful.","sentences":["5G and beyond support the deployment of vertical applications, which is particularly appealing in combination with network slicing and edge computing to create a logically isolated environment for executing customer services.","Even if serverless computing has gained significant interest as a cloud-native technology its adoption at the edge is lagging, especially because of the need to support stateful tasks, which are commonplace in, e.g., cognitive services, but not fully amenable to being deployed on limited and decentralized computing infrastructures.","In this work, we study the emerging paradigm of stateful Function as a Service (FaaS) with lightweight task abstractions in WebAssembly.","Specifically, we assess the implications of deploying inter-dependent tasks with an internal state on edge computing resources using a stateless vs. stateful approach and then derive a mathematical model to estimate the energy consumption of a workload with given characteristics, considering the power used for both processing and communication.","The model is used in extensive simulations to determine the impact of key factors and assess the energy trade-offs of stateless vs. stateful."],"url":"http://arxiv.org/abs/2405.04263v1","category":"cs.NI"}
{"created":"2024-05-07 12:20:12","title":"Interacting supernovae","abstract":"Modern photometric surveys of the sky suggest that many, perhaps most supernovae (SNe) associated with the explosion of massive stars are influenced at an appreciable level by their interaction with circumstellar material (CSM). The photometric and spectroscopic diversity of these transients point to a wide range of CSM properties in terms of mass, extent, composition, and location relative to the exploding star, suggesting progenitors that cover from standard to the most extreme mass loss rates. Surveys at high-cadence catch massive stars at shock breakout and inform us on the immediate mass loss history before core collapse. In contrast, long-term monitoring of these SNe cover the transition to the birth of a SN remnant and document the progenitor mass loss that took place centuries to millennia before explosion. Interacting SNe are therefore not just extraordinary astrophysical laboratories to study radiation-dominated shocks and probe the distant Universe, they also open the path to novel and fundamental studies on stellar evolution, stellar stability, or mass loss in single and binary massive stars.","sentences":["Modern photometric surveys of the sky suggest that many, perhaps most supernovae (SNe) associated with the explosion of massive stars are influenced at an appreciable level by their interaction with circumstellar material (CSM).","The photometric and spectroscopic diversity of these transients point to a wide range of CSM properties in terms of mass, extent, composition, and location relative to the exploding star, suggesting progenitors that cover from standard to the most extreme mass loss rates.","Surveys at high-cadence catch massive stars at shock breakout and inform us on the immediate mass loss history before core collapse.","In contrast, long-term monitoring of these SNe cover the transition to the birth of a SN remnant and document the progenitor mass loss that took place centuries to millennia before explosion.","Interacting SNe are therefore not just extraordinary astrophysical laboratories to study radiation-dominated shocks and probe the distant Universe, they also open the path to novel and fundamental studies on stellar evolution, stellar stability, or mass loss in single and binary massive stars."],"url":"http://arxiv.org/abs/2405.04259v1","category":"astro-ph.SR"}
{"created":"2024-05-07 10:47:51","title":"Self-Similar Properties of the Proton Structure at Low x within the \\textbf{xFitter} framework","abstract":"The structure of the proton exhibits Fractal behavior at low \\textit{x}, where \\textit{x} is the fraction of the proton's momentum carried by the interacting partons. This Fractal behavior is characterized by self-similar properties at different scales and can be quantified using the concept of Fractal dimension. An investigation into the Fractal properties of the proton structure at low \\textit{x} is critical for understanding the fundamental properties of the strong force and developing a more comprehensive understanding of the hadron structure. Fractals, characterized by self-similar patterns across scales, demonstrate a direct correlation between their Fractal dimension and entropy, where higher Fractal dimensions correspond to increased informational content. Furthermore, it is essential for designing high-energy physics experiments and developing more accurate models of subatomic particle interactions. This paper has a fresh look at the self-similar properties of the proton structure at low \\textit{x}. Our study involves the use of the \\textbf{xFitter} framework to parameterize the proton structure functions with a Fractal formalism at low \\textit{x}. We also examine how the inclusion of new data affects the results of our analysis.","sentences":["The structure of the proton exhibits Fractal behavior at low \\textit{x}, where \\textit{x} is the fraction of the proton's momentum carried by the interacting partons.","This Fractal behavior is characterized by self-similar properties at different scales and can be quantified using the concept of Fractal dimension.","An investigation into the Fractal properties of the proton structure at low \\textit{x} is critical for understanding the fundamental properties of the strong force and developing a more comprehensive understanding of the hadron structure.","Fractals, characterized by self-similar patterns across scales, demonstrate a direct correlation between their Fractal dimension and entropy, where higher Fractal dimensions correspond to increased informational content.","Furthermore, it is essential for designing high-energy physics experiments and developing more accurate models of subatomic particle interactions.","This paper has a fresh look at the self-similar properties of the proton structure at low \\textit{x}.","Our study involves the use of the \\textbf{xFitter} framework to parameterize the proton structure functions with a Fractal formalism at low \\textit{x}.","We also examine how the inclusion of new data affects the results of our analysis."],"url":"http://arxiv.org/abs/2405.04187v1","category":"hep-ph"}
{"created":"2024-05-07 10:39:32","title":"Baryonic screening masses in QCD at high temperature","abstract":"We compute the baryonic screening masses with nucleon quantum numbers and its negative parity partner in thermal QCD with $N_f=3$ massless quarks for a wide range of temperatures, from $T \\sim 1$ GeV up to $\\sim 160$ GeV. The computation is performed by Monte Carlo simulations of lattice QCD with $O(a)$-improved Wilson fermions by exploiting a recently proposed strategy to study non-perturbatively QCD at very high temperature. Very large spatial extensions are considered in order to have negligible finite volume effects. For each temperature we have simulated $3$ or $4$ values of the lattice spacing, so as to perform the continuum limit extrapolation with confidence at a few permille accuracy. The degeneracy of the positive and negative parity-state screening masses, expected from Ward identities associated to non-singlet axial transformations, provides further evidence for the restoration of chiral symmetry in the high temperature regime of QCD. In the entire range of temperatures explored, the baryonic masses deviate from the free theory result, $3 \\pi T$, by $4$-$8\\%$. The contribution due to the interactions is clearly visible up to the highest temperature considered, and cannot be explained by the expected leading behavior in the QCD coupling constant $g$ over the entire range of temperatures explored.","sentences":["We compute the baryonic screening masses with nucleon quantum numbers and its negative parity partner in thermal QCD with $N_f=3$ massless quarks for a wide range of temperatures, from $T \\sim 1$ GeV up to $\\sim 160$ GeV. The computation is performed by Monte Carlo simulations of lattice QCD with $O(a)$-improved Wilson fermions by exploiting a recently proposed strategy to study non-perturbatively QCD at very high temperature.","Very large spatial extensions are considered in order to have negligible finite volume effects.","For each temperature we have simulated $3$ or $4$ values of the lattice spacing, so as to perform the continuum limit extrapolation with confidence at a few permille accuracy.","The degeneracy of the positive and negative parity-state screening masses, expected from Ward identities associated to non-singlet axial transformations, provides further evidence for the restoration of chiral symmetry in the high temperature regime of QCD.","In the entire range of temperatures explored, the baryonic masses deviate from the free theory result, $3 \\pi T$, by $4$-$8\\%$. The contribution due to the interactions is clearly visible up to the highest temperature considered, and cannot be explained by the expected leading behavior in the QCD coupling constant $g$ over the entire range of temperatures explored."],"url":"http://arxiv.org/abs/2405.04182v1","category":"hep-lat"}
{"created":"2024-05-07 10:28:49","title":"Constraining the mass-spectra in the presence of a light sterile neutrino from absolute mass-related observables","abstract":"The framework of three-flavor neutrino oscillation is a well-established phenomenon, but results from the short-baseline experiments, such as the Liquid Scintillator Neutrino Detector (LSND) and MiniBooster Neutrino Experiment (MiniBooNE), hint at the potential existence of an additional light neutrino state characterized by a mass-squared difference of approximately $1\\,\\rm eV^2$. The new neutrino state is devoid of all Standard Model (SM) interactions, commonly referred to as a 'sterile' state. In addition, a sterile neutrino with a mass-squared difference of $10^{-2}$ $\\rm eV^2$ has been proposed to improve the tension between the results obtained from the Tokai to Kamioka (T2K) and the NuMI Off-axis $\\nu_e$ Appearance (NO$\\nu$A) experiments. Further, the non-observation of the predicted upturn in the solar neutrino spectra below 8 MeV can be explained by postulating an extra light sterile neutrino state with a mass-squared difference around $10^{-5} \\rm eV^2$. The hypothesis of an additional light sterile neutrino state introduces four distinct mass spectra depending on the sign of the mass-squared difference. In this paper, we discuss the implications of the above scenarios on the observables that depend on the absolute mass of the neutrinos, namely the sum of the light neutrino masses $(\\Sigma)$ from cosmology, the effective mass of the electron neutrino from beta decay $(m_{\\beta})$, and the effective Majorana mass $( m_{\\beta\\beta})$ from neutrinoless double beta decay. We show that some scenarios can be disfavored by the current constraints of the above variables. The implications for projected sensitivity of Karlsruhe Tritium Neutrino Experiment (KATRIN) and future experiments like Project-8, next Enriched Xenon Observatory (nEXO) have been discussed.","sentences":["The framework of three-flavor neutrino oscillation is a well-established phenomenon, but results from the short-baseline experiments, such as the Liquid Scintillator Neutrino Detector (LSND) and MiniBooster Neutrino Experiment (MiniBooNE), hint at the potential existence of an additional light neutrino state characterized by a mass-squared difference of approximately $1\\,\\rm eV^2$.","The new neutrino state is devoid of all Standard Model (SM) interactions, commonly referred to as a 'sterile' state.","In addition, a sterile neutrino with a mass-squared difference of $10^{-2}$ $\\rm eV^2$ has been proposed to improve the tension between the results obtained from the Tokai to Kamioka (T2K) and the NuMI Off-axis $\\nu_e$ Appearance (NO$\\nu$A) experiments.","Further, the non-observation of the predicted upturn in the solar neutrino spectra below 8 MeV can be explained by postulating an extra light sterile neutrino state with a mass-squared difference around $10^{-5} \\rm eV^2$.","The hypothesis of an additional light sterile neutrino state introduces four distinct mass spectra depending on the sign of the mass-squared difference.","In this paper, we discuss the implications of the above scenarios on the observables that depend on the absolute mass of the neutrinos, namely the sum of the light neutrino masses $(\\Sigma)$ from cosmology, the effective mass of the electron neutrino from beta decay $(m_{\\beta})$, and the effective Majorana mass $( m_{\\beta\\beta})$ from neutrinoless double beta decay.","We show that some scenarios can be disfavored by the current constraints of the above variables.","The implications for projected sensitivity of Karlsruhe Tritium Neutrino Experiment (KATRIN) and future experiments like Project-8, next Enriched Xenon Observatory (nEXO) have been discussed."],"url":"http://arxiv.org/abs/2405.04176v1","category":"hep-ph"}
{"created":"2024-05-07 10:11:14","title":"D-NLP at SemEval-2024 Task 2: Evaluating Clinical Inference Capabilities of Large Language Models","abstract":"Large language models (LLMs) have garnered significant attention and widespread usage due to their impressive performance in various tasks. However, they are not without their own set of challenges, including issues such as hallucinations, factual inconsistencies, and limitations in numerical-quantitative reasoning. Evaluating LLMs in miscellaneous reasoning tasks remains an active area of research. Prior to the breakthrough of LLMs, Transformers had already proven successful in the medical domain, effectively employed for various natural language understanding (NLU) tasks. Following this trend, LLMs have also been trained and utilized in the medical domain, raising concerns regarding factual accuracy, adherence to safety protocols, and inherent limitations. In this paper, we focus on evaluating the natural language inference capabilities of popular open-source and closed-source LLMs using clinical trial reports as the dataset. We present the performance results of each LLM and further analyze their performance on a development set, particularly focusing on challenging instances that involve medical abbreviations and require numerical-quantitative reasoning. Gemini, our leading LLM, achieved a test set F1-score of 0.748, securing the ninth position on the task scoreboard. Our work is the first of its kind, offering a thorough examination of the inference capabilities of LLMs within the medical domain.","sentences":["Large language models (LLMs) have garnered significant attention and widespread usage due to their impressive performance in various tasks.","However, they are not without their own set of challenges, including issues such as hallucinations, factual inconsistencies, and limitations in numerical-quantitative reasoning.","Evaluating LLMs in miscellaneous reasoning tasks remains an active area of research.","Prior to the breakthrough of LLMs, Transformers had already proven successful in the medical domain, effectively employed for various natural language understanding (NLU) tasks.","Following this trend, LLMs have also been trained and utilized in the medical domain, raising concerns regarding factual accuracy, adherence to safety protocols, and inherent limitations.","In this paper, we focus on evaluating the natural language inference capabilities of popular open-source and closed-source LLMs using clinical trial reports as the dataset.","We present the performance results of each LLM and further analyze their performance on a development set, particularly focusing on challenging instances that involve medical abbreviations and require numerical-quantitative reasoning.","Gemini, our leading LLM, achieved a test set F1-score of 0.748, securing the ninth position on the task scoreboard.","Our work is the first of its kind, offering a thorough examination of the inference capabilities of LLMs within the medical domain."],"url":"http://arxiv.org/abs/2405.04170v1","category":"cs.CL"}
{"created":"2024-05-07 08:58:42","title":"Large Bulk Photovoltaic Effect of Nitride Perovskite LaWN3 as Photocatalyst for Hydrogen Evolution Reaction: First Principles Calculation","abstract":"Bulk photovoltaic effect in noncentrosymmetric materials is a fundamental and significant property that holds potential for high-efficiency energy harvesting, such as photoelectric application and photocatalysis. Here, based on first principles calculation, we explore the electronic structure, dielectric property, shift current, and photocatalytic performance of novel nitride perovskite LaWN3. Our calculations show that LaWN3 possesses large dielectric constants and shift current. The shift current can be enhanced by considering spin-orbit coupling and is switchable by ferroelectric polarization, which suggests LaWN3 is a promising candidate for logic and neuromorphic photovoltaic devices driven by ferroelectric polarization. Additionally, LaWN3 shows advanced photocatalytic hydrogen evolution reaction as a photocatalyst. Especially, the (110) surface represents low surface energy and Gibbes free energy, implying that the (110) surface may be exposed to the active surface. Our finding highlights potential applications of novel polar nitride perovskite LaWN3 in various fields not only photoelectric devices but also photocatalysis.","sentences":["Bulk photovoltaic effect in noncentrosymmetric materials is a fundamental and significant property that holds potential for high-efficiency energy harvesting, such as photoelectric application and photocatalysis.","Here, based on first principles calculation, we explore the electronic structure, dielectric property, shift current, and photocatalytic performance of novel nitride perovskite LaWN3.","Our calculations show that LaWN3 possesses large dielectric constants and shift current.","The shift current can be enhanced by considering spin-orbit coupling and is switchable by ferroelectric polarization, which suggests LaWN3 is a promising candidate for logic and neuromorphic photovoltaic devices driven by ferroelectric polarization.","Additionally, LaWN3 shows advanced photocatalytic hydrogen evolution reaction as a photocatalyst.","Especially, the (110) surface represents low surface energy and Gibbes free energy, implying that the (110) surface may be exposed to the active surface.","Our finding highlights potential applications of novel polar nitride perovskite LaWN3 in various fields not only photoelectric devices but also photocatalysis."],"url":"http://arxiv.org/abs/2405.04132v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 08:34:07","title":"Free space daylight ground-ground QKD in the near-IR","abstract":"We report a daylight km-range free space QKD demonstration at 850nm obtaining a QBER of 1.9\\% and a raw key-rate of 14 kbit/s. We used the BB84 protocol with polarisation encoding and two supporting optical beams for classical communication and clock synchronisation.","sentences":["We report a daylight km-range free space QKD demonstration at 850nm obtaining a QBER of 1.9\\% and a raw key-rate of 14 kbit/s. We used the BB84 protocol with polarisation encoding and two supporting optical beams for classical communication and clock synchronisation."],"url":"http://arxiv.org/abs/2405.04113v1","category":"quant-ph"}
{"created":"2024-05-07 08:27:12","title":"A Self-regulated Stochastic Acceleration Model of Pulsar Wind Nebulae","abstract":"Pulsar wind nebulae (PWNe) are clouds of the magnetized relativistic electron/positron plasma supplied from the central pulsar. However, the number of radio-emitting particles inside a PWN is larger than the expectation from the study of pulsar magnetospheres and then their origin is still unclear. A stochastic acceleration of externally injected particles by a turbulence inside the PWN is proposed by our previous studies. In this paper, the previous stochastic acceleration model of the PWN broadband spectra is improved by taking into account the time evolution of the turbulent energy and then the total energy balance inside a PWN is maintained. The turbulent energy supplied from the central pulsar is wasted by the backreaction from the stochastic particle acceleration and the adiabatic cooling according the PWN expansion. The model is applied to the Crab Nebula and reproduce the current broadband emission spectrum, especially the flat radio spectrum although time evolution of the turbulent energy (diffusion coefficient) is a bit complicated compared with our previous studies, where we assumed an exponential behavior of the diffusion coefficient.","sentences":["Pulsar wind nebulae (PWNe) are clouds of the magnetized relativistic electron/positron plasma supplied from the central pulsar.","However, the number of radio-emitting particles inside a PWN is larger than the expectation from the study of pulsar magnetospheres and then their origin is still unclear.","A stochastic acceleration of externally injected particles by a turbulence inside the PWN is proposed by our previous studies.","In this paper, the previous stochastic acceleration model of the PWN broadband spectra is improved by taking into account the time evolution of the turbulent energy and then the total energy balance inside a PWN is maintained.","The turbulent energy supplied from the central pulsar is wasted by the backreaction from the stochastic particle acceleration and the adiabatic cooling according the PWN expansion.","The model is applied to the Crab Nebula and reproduce the current broadband emission spectrum, especially the flat radio spectrum although time evolution of the turbulent energy (diffusion coefficient) is a bit complicated compared with our previous studies, where we assumed an exponential behavior of the diffusion coefficient."],"url":"http://arxiv.org/abs/2405.04110v1","category":"astro-ph.HE"}
{"created":"2024-05-07 07:20:47","title":"2d Sinh-Gordon model on the infinite cylinder","abstract":"For $R>0$, we give a rigorous probabilistic construction on the cylinder $\\mathbb{R} \\times (\\mathbb{R}/(2\\pi R\\mathbb{Z}))$ of the (massless) Sinh-Gordon model. In particular we define the $n$-point correlation functions of the model and show that these exhibit a scaling relation with respect to $R$. The construction, which relies on the massless Gaussian Free Field, is based on the spectral analysis of a quantum operator associated to the model. Using the theory of Gaussian multiplicative chaos, we prove that this operator has discrete spectrum and a strictly positive ground state.","sentences":["For $R>0$, we give a rigorous probabilistic construction on the cylinder $\\mathbb{R} \\times (\\mathbb{R}/(2\\pi R\\mathbb{Z}))$ of the (massless) Sinh-Gordon model.","In particular we define the $n$-point correlation functions of the model and show that these exhibit a scaling relation with respect to $R$. The construction, which relies on the massless Gaussian Free Field, is based on the spectral analysis of a quantum operator associated to the model.","Using the theory of Gaussian multiplicative chaos, we prove that this operator has discrete spectrum and a strictly positive ground state."],"url":"http://arxiv.org/abs/2405.04076v1","category":"math.PR"}
{"created":"2024-05-07 07:16:54","title":"Lam-Tung relation breaking in $Z$ boson production as a probe of SMEFT effects","abstract":"The violation of Lam-Tung relation in the high-$p_T^{\\ell\\ell}$ region of the Drell-Yan process at the LHC presents a long-standing discrepancy with the standard model prediction at $\\mathcal{O}(\\alpha_s^3)$ accuracy. In this Letter, we employed a model-independent analysis to investigate this anomaly within the framework of the Standard Model Effective Field Theory (SMEFT). Our findings revealed that the leading contributions from SMEFT to this violation appear at the $1/\\Lambda^4$ order with $\\mathcal{O}(\\alpha_s)$ accuracy in QCD interaction. Notably, we demonstrated that the quadratic effect of dimension-6 dipole operators, associated with the $Z$ boson, dominates the breaking effects induced by various dimension-6 and dimension-8 operators. This provides a compelling explanation for the observed discrepancy with the Standard Model predictions at the LHC without assuming other new physics operators, and thereby offers the potential to extract valuable information about the underlying physics at the TeV scale.","sentences":["The violation of Lam-Tung relation in the high-$p_T^{\\ell\\ell}$ region of the Drell-Yan process at the LHC presents a long-standing discrepancy with the standard model prediction at $\\mathcal{O}(\\alpha_s^3)$ accuracy.","In this Letter, we employed a model-independent analysis to investigate this anomaly within the framework of the Standard Model Effective Field Theory (SMEFT).","Our findings revealed that the leading contributions from SMEFT to this violation appear at the $1/\\Lambda^4$ order with $\\mathcal{O}(\\alpha_s)$ accuracy in QCD interaction.","Notably, we demonstrated that the quadratic effect of dimension-6 dipole operators, associated with the $Z$ boson, dominates the breaking effects induced by various dimension-6 and dimension-8 operators.","This provides a compelling explanation for the observed discrepancy with the Standard Model predictions at the LHC without assuming other new physics operators, and thereby offers the potential to extract valuable information about the underlying physics at the TeV scale."],"url":"http://arxiv.org/abs/2405.04069v1","category":"hep-ph"}
{"created":"2024-05-07 07:14:38","title":"FlashBack:Efficient Retrieval-Augmented Language Modeling for Long Context Inference","abstract":"Retrieval-Augmented Language Modeling (RALM) by integrating large language models (LLM) with relevant documents from an external corpus is a proven method for enabling the LLM to generate information beyond the scope of its pre-training corpus. Previous work using utilizing retrieved content by simply prepending retrieved contents to the input poses a high runtime issue, which degrades the inference efficiency of the LLMs because they fail to use the Key-Value (KV) cache efficiently. In this paper, we propose \\textsc{FlashBack}, a modular RALM designed to improve the inference efficiency of RALM with appending context pattern while maintaining decent performance after specific fine-tuning without heavily destruct the knowledge integrity of the LLM. \\textsc{FlashBack} appends retrieved documents at the end of the context for efficiently utilizing the KV cache instead of prepending them. Our experiment shows that the inference speed of \\textsc{FlashBack} is up to $4\\times$ faster than the prepending method on a 7B LLM (Llama 2). Via bypassing unnecessary re-computation, it demonstrates an advancement by achieving significantly faster inference speed, and this heightened efficiency will substantially reduce inferential cost. Our code will be publicly available.","sentences":["Retrieval-Augmented Language Modeling (RALM) by integrating large language models (LLM) with relevant documents from an external corpus is a proven method for enabling the LLM to generate information beyond the scope of its pre-training corpus.","Previous work using utilizing retrieved content by simply prepending retrieved contents to the input poses a high runtime issue, which degrades the inference efficiency of the LLMs because they fail to use the Key-Value (KV) cache efficiently.","In this paper, we propose \\textsc{FlashBack}, a modular RALM designed to improve the inference efficiency of RALM with appending context pattern while maintaining decent performance after specific fine-tuning without heavily destruct the knowledge integrity of the LLM. \\textsc{FlashBack} appends retrieved documents at the end of the context for efficiently utilizing the KV cache instead of prepending them.","Our experiment shows that the inference speed of \\textsc{FlashBack} is up to $4\\times$ faster than the prepending method on a 7B LLM (Llama 2).","Via bypassing unnecessary re-computation, it demonstrates an advancement by achieving significantly faster inference speed, and this heightened efficiency will substantially reduce inferential cost.","Our code will be publicly available."],"url":"http://arxiv.org/abs/2405.04065v1","category":"cs.CL"}
{"created":"2024-05-07 07:09:00","title":"Geodesic completeness, cosmological bounces and inflation","abstract":"The question of geodesic completeness of cosmological spacetimes has recently received renewed scrutiny. A particularly interesting result is the observation that the well-known Borde-Guth-Vilenkin (BGV) theorem may misdiagnose geodesically complete cosmologies. We propose a simple amendment to the BGV theorem which addresses such loopholes while retaining much of its generality. We give straightforward proofs of some recently offered conjectures concerning (generalized) Friedmann-Lema\\^itre-Robertson-Walker spacetimes: geodesic completeness implies (i) the existence of a bounce, loitering phase or an emergent cosmology, and (ii) a phase of accelerated expansion with strictly increasing Hubble rate. Our results are purely kinematic and do not assume general relativity or energy conditions.","sentences":["The question of geodesic completeness of cosmological spacetimes has recently received renewed scrutiny.","A particularly interesting result is the observation that the well-known Borde-Guth-Vilenkin (BGV) theorem may misdiagnose geodesically complete cosmologies.","We propose a simple amendment to the BGV theorem which addresses such loopholes while retaining much of its generality.","We give straightforward proofs of some recently offered conjectures concerning (generalized) Friedmann-Lema\\^itre-Robertson-Walker spacetimes: geodesic completeness implies (i) the existence of a bounce, loitering phase or an emergent cosmology, and (ii) a phase of accelerated expansion with strictly increasing Hubble rate.","Our results are purely kinematic and do not assume general relativity or energy conditions."],"url":"http://arxiv.org/abs/2405.04062v1","category":"gr-qc"}
{"created":"2024-05-07 06:06:45","title":"On the Detection and Characterization of Quasiperiodic Oscillations in Astronomical Time Series: Gamma-Ray Burst X-Ray Light Curves as a Test Case","abstract":"The study of temporal properties of variable sources can elucidate their physical processes. In this context, we present a critical study comparing three approaches to periodic or quasiperiodic behavior: Gaussian process, power spectrum, and wavelet analysis, using celerite, Lomb-Scargle periodograms, and weighted wavelet-Z transforms, respectively. We use 15 Swift-X-ray Telescope light curves of short gamma-ray bursts (sGRBs) as examples. A comprehensive analysis of two sGRB X-ray light curves is performed. The results reveal the importance of artifacts, largely in the form of false quasiperiodic oscillation signals, possibly introduced by preprocessing (such as detrending) or other aspects of the analysis. The exploration described in this paper can be helpful for future studies of variability in GRBs, active galactic nuclei, and other astronomical sources.","sentences":["The study of temporal properties of variable sources can elucidate their physical processes.","In this context, we present a critical study comparing three approaches to periodic or quasiperiodic behavior: Gaussian process, power spectrum, and wavelet analysis, using celerite, Lomb-Scargle periodograms, and weighted wavelet-Z transforms, respectively.","We use 15 Swift-X-ray Telescope light curves of short gamma-ray bursts (sGRBs) as examples.","A comprehensive analysis of two sGRB X-ray light curves is performed.","The results reveal the importance of artifacts, largely in the form of false quasiperiodic oscillation signals, possibly introduced by preprocessing (such as detrending) or other aspects of the analysis.","The exploration described in this paper can be helpful for future studies of variability in GRBs, active galactic nuclei, and other astronomical sources."],"url":"http://arxiv.org/abs/2405.04033v1","category":"astro-ph.HE"}
{"created":"2024-05-07 05:57:55","title":"Stellar Population near NGC 2021: Procession of Star Formation in the South Rim of Supergiant Shell LMC 4","abstract":"Supergiant shells (SGSs) are the largest interstellar structures where heated and enriched gas flows into the host galaxy's halo. The SGSs in the Large Magellanic Cloud (LMC) are so close that their stars can be resolved with ground-based telescopes to allow studies of star formation history. Aiming to study the star formation history and energy budget of LMC 4, we have conducted a pilot study of the cluster NGC 2021 and the OB associations in its vicinity near the south rim of LMC 4. We use the Magellanic Cloud Photometric Survey data of the LMC to establish a methodology to examine the stellar population and assess the massive star formation history. We find a radial procession of massive star formation from the northwest part of the OB association LH79 through NGC 2021 to the OB association LH78 in the south. Using the stellar content of NGC 2021 and the assumption of Salpeter's initial mass function, we estimate that $\\sim$4 supernovae have occurred in NGC 2021, injecting at least $4\\times10^{51}$ ergs of kinetic energy into the interior of LMC 4.","sentences":["Supergiant shells (SGSs) are the largest interstellar structures where heated and enriched gas flows into the host galaxy's halo.","The SGSs in the Large Magellanic Cloud (LMC) are so close that their stars can be resolved with ground-based telescopes to allow studies of star formation history.","Aiming to study the star formation history and energy budget of LMC 4, we have conducted a pilot study of the cluster NGC 2021 and the OB associations in its vicinity near the south rim of LMC 4.","We use the Magellanic Cloud Photometric Survey data of the LMC to establish a methodology to examine the stellar population and assess the massive star formation history.","We find a radial procession of massive star formation from the northwest part of the OB association LH79 through NGC 2021 to the OB association LH78 in the south.","Using the stellar content of NGC 2021 and the assumption of Salpeter's initial mass function, we estimate that $\\sim$4 supernovae have occurred in NGC 2021, injecting at least $4\\times10^{51}$ ergs of kinetic energy into the interior of LMC 4."],"url":"http://arxiv.org/abs/2405.04024v1","category":"astro-ph.GA"}
{"created":"2024-05-07 05:27:48","title":"Star Proper Motions Based on Two-epoch Observations from the SDSS and DESI Imaging Surveys","abstract":"In this study, we present the construction of a new proper motion catalog utilizing the photometric data from the Sloan Digital Sky Survey (SDSS) and Dark Energy Spectroscopic Instrument (DESI) imaging surveys, with a median time baseline of about 13 years. To mitigate systematic errors, the DESI galaxy positions are employed to establish a reference frame and to correct the position-, magnitude-, and color-dependent discrepancies between SDSS and DESI imaging datasets. Spanning 12,589 square degrees, the catalog encompasses about 206.6 million non-Gaia objects down to $m_r \\sim$ 23. Based on 734k quasars, the assessment of the global systematic errors in DESI-SDSS proper motion catalog yields values of 0.14 mas yr$^{-1}$ for $\\mu_{\\alpha *}$ and 0.11 mas yr$^{-1}$ for $\\mu_{\\delta}$. The catalog exhibits a precision surpassing 3.7 mas yr$^{-1}$, albeit varying with position, color, and magnitude. An additional evaluation employing approximately 5,300 distant star samples yields an overall precision of approximately 3.0 and 2.9 mas yr$^{-1}$ for $\\mu_{\\alpha *}$ and $\\mu_{\\delta}$, respectively. Further comparisons with proper motions from SDSS Stripe 82 reveal a strong consistency between the two datasets. As a practical application, we utilize fainter non-Gaia objects in our catalog to update the proper motions of 17 star clusters. The resulting proper motions for these clusters exhibit excellent consistency with those derived from Gaia data. Our proper motion measurements, characterized by a deeper limiting magnitude, stands as a valuable complement to the Gaia dataset. The catalog is publicly available at \\url{https://www.scidb.cn/s/YzaIv2}.","sentences":["In this study, we present the construction of a new proper motion catalog utilizing the photometric data from the Sloan Digital Sky Survey (SDSS) and Dark Energy Spectroscopic Instrument (DESI) imaging surveys, with a median time baseline of about 13 years.","To mitigate systematic errors, the DESI galaxy positions are employed to establish a reference frame and to correct the position-, magnitude-, and color-dependent discrepancies between SDSS and DESI imaging datasets.","Spanning 12,589 square degrees, the catalog encompasses about 206.6 million non-Gaia objects down to $m_r \\sim$ 23.","Based on 734k quasars, the assessment of the global systematic errors in DESI-SDSS proper motion catalog yields values of 0.14 mas yr$^{-1}$ for $\\mu_{\\alpha *}$ and 0.11 mas yr$^{-1}$ for $\\mu_{\\delta}$. The catalog exhibits a precision surpassing 3.7 mas yr$^{-1}$, albeit varying with position, color, and magnitude.","An additional evaluation employing approximately 5,300 distant star samples yields an overall precision of approximately 3.0 and 2.9 mas yr$^{-1}$ for $\\mu_{\\alpha *}$ and $\\mu_{\\delta}$, respectively.","Further comparisons with proper motions from SDSS Stripe 82 reveal a strong consistency between the two datasets.","As a practical application, we utilize fainter non-Gaia objects in our catalog to update the proper motions of 17 star clusters.","The resulting proper motions for these clusters exhibit excellent consistency with those derived from Gaia data.","Our proper motion measurements, characterized by a deeper limiting magnitude, stands as a valuable complement to the Gaia dataset.","The catalog is publicly available at \\url{https://www.scidb.cn/s/YzaIv2}."],"url":"http://arxiv.org/abs/2405.04016v1","category":"astro-ph.GA"}
{"created":"2024-05-07 03:53:17","title":"Bayesian Multilevel Compositional Data Analysis: Introduction, Evaluation, and Application","abstract":"Multilevel compositional data commonly occur in various fields, particularly in intensive, longitudinal studies using ecological momentary assessments. Examples include data repeatedly measured over time that are non-negative and sum to a constant value, such as sleep-wake movement behaviours in a 24-hour day. This article presents a novel methodology for analysing multilevel compositional data using a Bayesian inference approach. This method can be used to investigate how reallocation of time between sleep-wake movement behaviours may be associated with other phenomena (e.g., emotions, cognitions) at a daily level. We explain the theoretical details of the data and the models, and outline the steps necessary to implement this method. We introduce the R package multilevelcoda to facilitate the application of this method and illustrate using a real data example. An extensive parameter recovery simulation study verified the robust performance of the method. Across all simulation conditions investigated in the simulation study, the model had minimal convergence issues (convergence rate > 99%) and achieved excellent quality of parameter estimates and inference, with an average bias of 0.00 (range -0.09, 0.05) and coverage of 0.95 (range 0.93, 0.97). We conclude the article with recommendations on the use of the Bayesian compositional multilevel modelling approach, and hope to promote wider application of this method to answer robust questions using the increasingly available data from intensive, longitudinal studies.","sentences":["Multilevel compositional data commonly occur in various fields, particularly in intensive, longitudinal studies using ecological momentary assessments.","Examples include data repeatedly measured over time that are non-negative and sum to a constant value, such as sleep-wake movement behaviours in a 24-hour day.","This article presents a novel methodology for analysing multilevel compositional data using a Bayesian inference approach.","This method can be used to investigate how reallocation of time between sleep-wake movement behaviours may be associated with other phenomena (e.g., emotions, cognitions) at a daily level.","We explain the theoretical details of the data and the models, and outline the steps necessary to implement this method.","We introduce the R package multilevelcoda to facilitate the application of this method and illustrate using a real data example.","An extensive parameter recovery simulation study verified the robust performance of the method.","Across all simulation conditions investigated in the simulation study, the model had minimal convergence issues (convergence rate > 99%) and achieved excellent quality of parameter estimates and inference, with an average bias of 0.00 (range -0.09, 0.05) and coverage of 0.95 (range 0.93, 0.97).","We conclude the article with recommendations on the use of the Bayesian compositional multilevel modelling approach, and hope to promote wider application of this method to answer robust questions using the increasingly available data from intensive, longitudinal studies."],"url":"http://arxiv.org/abs/2405.03985v1","category":"stat.ME"}
{"created":"2024-05-07 03:47:26","title":"MEET-U Project I: The key drivers of the preference for dynamic dark energy","abstract":"Joint analysis of the baryon acoustic oscillations (BAO) measurement by the Dark Energy Spectroscopic Instrument (DESI) first data release, Type Ia supernovae (SNe) of the Dark Energy Survey Year 5 (DES5YR) release and cosmic microwave background (CMB) data favors a quintom-like dynamic dark energy model over the standard Lambda cold dark matter ($\\Lambda$CDM) model at $3.9\\sigma$ level (Adame et al. 2024). We demonstrate that the preference for dynamic dark energy does not rely on the detailed modeling of CMB physics and remains at $3.2\\sigma$ level when the full CMB likelihood is replaced by a CMB acoustic-oscillation angle ($\\theta_\\star$) prior and a baryon abundance ($\\Omega_bh^2$) prior. By comparing the data with over $10^4$ $\\Lambda$CDM-based simulations, we find that both the SNe and BAO data contribute significantly to the preference for dynamic dark energy. The preference for dynamic dark energy is unlikely (probability $\\lesssim 0.02$) due to unknown systematics in DES5YR SNe and statistical fluctuations in DESI BAO, or vice versa.","sentences":["Joint analysis of the baryon acoustic oscillations (BAO) measurement by the Dark Energy Spectroscopic Instrument (DESI) first data release, Type Ia supernovae (SNe) of the Dark Energy Survey Year 5 (DES5YR) release and cosmic microwave background (CMB) data favors a quintom-like dynamic dark energy model over the standard Lambda cold dark matter ($\\Lambda$CDM) model at $3.9\\sigma$ level (Adame et al. 2024).","We demonstrate that the preference for dynamic dark energy does not rely on the detailed modeling of CMB physics and remains at $3.2\\sigma$ level when the full CMB likelihood is replaced by a CMB acoustic-oscillation angle ($\\theta_\\star$) prior and a baryon abundance ($\\Omega_bh^2$) prior.","By comparing the data with over $10^4$ $\\Lambda$CDM-based simulations, we find that both the SNe and BAO data contribute significantly to the preference for dynamic dark energy.","The preference for dynamic dark energy is unlikely (probability $\\lesssim 0.02$) due to unknown systematics in DES5YR SNe and statistical fluctuations in DESI BAO, or vice versa."],"url":"http://arxiv.org/abs/2405.03983v1","category":"astro-ph.CO"}
{"created":"2024-05-07 03:37:06","title":"Dimension quotients as boundary limits","abstract":"For a functor from the category of free presentations of a group to the category of all groups we define the boundary limit as an image of the natural map from limit to colimit. We show that the fourth dimension quotient of a group can be naturally described as the boundary limit of a simply-defined functor.","sentences":["For a functor from the category of free presentations of a group to the category of all groups we define the boundary limit as an image of the natural map from limit to colimit.","We show that the fourth dimension quotient of a group can be naturally described as the boundary limit of a simply-defined functor."],"url":"http://arxiv.org/abs/2405.03979v1","category":"math.GR"}
{"created":"2024-05-07 02:55:48","title":"Dynamical and Static Structure Factors in Hedgehog-Antihedgehog Order in Icosahedral 1/1 Approximant Crystal","abstract":"Recent discoveries of magnetic long-range orders in the icosahedral quasicrystal and topological magnetic structures on the icosahedron (IC) as the hedgehog state and the antihedgehog state have attracted great interest. Here, we report our theoretical analysis of the dynamical as well as static structure of the hedgehog-antihedgehog order in the 1/1 approximant crystal (AC). By constructing the effective magnetic model for the rare-earth based AC, on the basis of the linear spin-wave theory, the excitation energy is shown to exhibit the reciprocal dispersion, as a consequence of preservation of the spatial inversion symmetry by the hedgehog-antihedgehog ordering. The static structure factor is shown to be expressed generally in the convolution form of the lattice structure factor and the magnetic structure factor on the IC(s) and the numerical calculation reveals the extinction rule. The dynamical structure factor shows that the high intensities appear in the low-energy branch along the $\\Gamma$-X line and the R-$\\Gamma$-M line in the reciprocal space.","sentences":["Recent discoveries of magnetic long-range orders in the icosahedral quasicrystal and topological magnetic structures on the icosahedron (IC) as the hedgehog state and the antihedgehog state have attracted great interest.","Here, we report our theoretical analysis of the dynamical as well as static structure of the hedgehog-antihedgehog order in the 1/1 approximant crystal (AC).","By constructing the effective magnetic model for the rare-earth based AC, on the basis of the linear spin-wave theory, the excitation energy is shown to exhibit the reciprocal dispersion, as a consequence of preservation of the spatial inversion symmetry by the hedgehog-antihedgehog ordering.","The static structure factor is shown to be expressed generally in the convolution form of the lattice structure factor and the magnetic structure factor on the IC(s) and the numerical calculation reveals the extinction rule.","The dynamical structure factor shows that the high intensities appear in the low-energy branch along the $\\Gamma$-X line and the R-$\\Gamma$-M line in the reciprocal space."],"url":"http://arxiv.org/abs/2405.03968v1","category":"cond-mat.str-el"}
{"created":"2024-05-07 02:53:30","title":"Twisted vortices in two-component Ginzburg-Landau theory","abstract":"In this note, a brief introduction to the physical and mathematical background of the two-component Ginzburg-Landau theory is given. From this theory we derive a boundary value problem whose solution can be obtained in part by solving a minimization problem using the technique of variational method, except that two of the eight boundary conditions cannot be satisfied. To overcome the difficulty of recovering the full set of boundary conditions, we employ a variety of methods, including the uniform estimation method and the bounded monotonic theorem, which may be applied to other complicated vortex problems in gauge field theories. The twisted vortex solutions are obtained as energy-minimizing cylindrically symmetric field configurations. We also give the sharp asymptotic estimates for the twisted vortex solutions at the origin and infinity.","sentences":["In this note, a brief introduction to the physical and mathematical background of the two-component Ginzburg-Landau theory is given.","From this theory we derive a boundary value problem whose solution can be obtained in part by solving a minimization problem using the technique of variational method, except that two of the eight boundary conditions cannot be satisfied.","To overcome the difficulty of recovering the full set of boundary conditions, we employ a variety of methods, including the uniform estimation method and the bounded monotonic theorem, which may be applied to other complicated vortex problems in gauge field theories.","The twisted vortex solutions are obtained as energy-minimizing cylindrically symmetric field configurations.","We also give the sharp asymptotic estimates for the twisted vortex solutions at the origin and infinity."],"url":"http://arxiv.org/abs/2405.03965v1","category":"math-ph"}
{"created":"2024-05-07 02:11:18","title":"Probing the spectrum of the magnetar 4U 0142+61 with JWST","abstract":"JWST observed the magnetar 4U 0142+61 with the MIRI and NIRCam instruments within a 77 min time interval on 2022 September 20-21. The low-resolution MIRI spectrum and NIRCam photometry show that the spectrum in the wavelength range 1.4-11 $\\mu$m range can be satisfactorily described by an absorbed power-law model, $f_{\\nu}\\propto \\nu^{-\\alpha}$, with a spectral slope $\\alpha =0.96\\pm0.02$, interstellar extinction $A_V= 3.9\\pm0.2$, and normalization $f_0 = 59.4\\pm 0.5$ $\\mu$Jy at $\\lambda = 8$ $\\mu$m. These observations do not support the passive disk model proposed by Wang et al. (2006), based on the Spitzer photometry, which was interpreted as evidence for a fallback disk from debris formed during the supernova explosion. We suggest a nonthermal origin for this emission and source variability as the most likely cause of discrepancies between the JWST data and other IR-optical observing campaigns. However, we cannot firmly exclude the presence of a large disk with a different dependence of the effective disk temperature on distance from the magnetar. Comparison with the power-law fit to the hard X-ray spectrum above 10 keV, measured by NuSTAR contemporaneously with JWST, shows that the X-ray spectrum is significantly harder. This may imply that the X-ray and IR nonthermal emission come from different sites in the magnetosphere of the magnetar.","sentences":["JWST observed the magnetar 4U 0142+61 with the MIRI and NIRCam instruments within a 77 min time interval on 2022 September 20-21.","The low-resolution MIRI spectrum and NIRCam photometry show that the spectrum in the wavelength range 1.4-11 $\\mu$m range can be satisfactorily described by an absorbed power-law model, $f_{\\nu}\\propto \\nu^{-\\alpha}$, with a spectral slope $\\alpha =0.96\\pm0.02$, interstellar extinction $A_V= 3.9\\pm0.2$, and normalization $f_0 = 59.4\\pm 0.5$ $\\mu$Jy at $\\lambda = 8$ $\\mu$m.","These observations do not support the passive disk model proposed by Wang et al. (2006), based on the Spitzer photometry, which was interpreted as evidence for a fallback disk from debris formed during the supernova explosion.","We suggest a nonthermal origin for this emission and source variability as the most likely cause of discrepancies between the JWST data and other IR-optical observing campaigns.","However, we cannot firmly exclude the presence of a large disk with a different dependence of the effective disk temperature on distance from the magnetar.","Comparison with the power-law fit to the hard X-ray spectrum above 10 keV, measured by NuSTAR contemporaneously with JWST, shows that the X-ray spectrum is significantly harder.","This may imply that the X-ray and IR nonthermal emission come from different sites in the magnetosphere of the magnetar."],"url":"http://arxiv.org/abs/2405.03947v1","category":"astro-ph.HE"}
{"created":"2024-05-07 02:11:11","title":"Association between centrality and flourishing trait: analyzing student co-occurrence networks drawn from dining activities","abstract":"Comprehending the association between social capabilities and individual psychological traits is paramount for educational administrators. Presently, many studies heavily depend on online questionnaires and self-reported data, while analysis of the connection between offline social networks and mental health status remains scarce. By leveraging a public dataset encompassing on-campus dining activities over 21 weeks, we establish student co-occurrence networks and closely observe the changes in network topology over time. Empirical analysis shows that the node centralities of the student co-occurrence networks exhibit significantly positive correlation with the enhancement of the flourishing trait within the field of mental well-being. Our findings offer potential guidance for assisting students in maintaining a positive mental health status.","sentences":["Comprehending the association between social capabilities and individual psychological traits is paramount for educational administrators.","Presently, many studies heavily depend on online questionnaires and self-reported data, while analysis of the connection between offline social networks and mental health status remains scarce.","By leveraging a public dataset encompassing on-campus dining activities over 21 weeks, we establish student co-occurrence networks and closely observe the changes in network topology over time.","Empirical analysis shows that the node centralities of the student co-occurrence networks exhibit significantly positive correlation with the enhancement of the flourishing trait within the field of mental well-being.","Our findings offer potential guidance for assisting students in maintaining a positive mental health status."],"url":"http://arxiv.org/abs/2405.03946v1","category":"cs.SI"}
{"created":"2024-05-07 01:55:52","title":"The stringy scaling loop expansion and stringy scaling violation","abstract":"We propose a systematic approximation scheme to calculate general $n$-point $HSSA$ of open bosonic string theory. This stringy scaling loop expansion contains finite number of vacuum diagram terms at each loop order of scattering energy due to a vacuum diagram contraint and a topological graph constraint. In addition, we calculate coefficient and give the vacuum diagram representation and its Feynman rules for each term in the expansion of the $HSSA$. As an application to extending our previous calculation of $n$-point leading order stringy scaling behavior of $HSSA$, we explicitly calculate some examples of $4$-point next to leading order stringy scaling violation terms.","sentences":["We propose a systematic approximation scheme to calculate general $n$-point $HSSA$ of open bosonic string theory.","This stringy scaling loop expansion contains finite number of vacuum diagram terms at each loop order of scattering energy due to a vacuum diagram contraint and a topological graph constraint.","In addition, we calculate coefficient and give the vacuum diagram representation and its Feynman rules for each term in the expansion of the $HSSA$. As an application to extending our previous calculation of $n$-point leading order stringy scaling behavior of $HSSA$, we explicitly calculate some examples of $4$-point next to leading order stringy scaling violation terms."],"url":"http://arxiv.org/abs/2405.03938v1","category":"hep-th"}
{"created":"2024-05-07 01:52:38","title":"On a convergence of positive continuous additive functionals in terms of their smooth measures","abstract":"A compactness of the Revuz map is established in the sense that the locally uniform convergence of a sequence of positive continuous additive functionals is derived in terms of their smooth measures. To this end, we first introduce a metric on the space of measures of finite energy integrals and show some structures of the metric. Then, we show the compactness and give some examples of positive continuous additive functionals that the convergence holds in terms of the associated smooth measures.","sentences":["A compactness of the Revuz map is established in the sense that the locally uniform convergence of a sequence of positive continuous additive functionals is derived in terms of their smooth measures.","To this end, we first introduce a metric on the space of measures of finite energy integrals and show some structures of the metric.","Then, we show the compactness and give some examples of positive continuous additive functionals that the convergence holds in terms of the associated smooth measures."],"url":"http://arxiv.org/abs/2405.03937v1","category":"math.PR"}
{"created":"2024-05-07 01:28:59","title":"Incorporating changeable attitudes toward vaccination into an SIR infectious disease model","abstract":"We develop a mechanistic model that classifies individuals both in terms of epidemiological status (SIR) and vaccination attitude (willing or unwilling), with the goal of discovering how disease spread is influenced by changing opinions about vaccination. Analysis of the model identifies existence and stability criteria for both disease-free and endemic disease equilibria. The analytical results, supported by numerical simulations, show that attitude changes induced by disease prevalence can destabilize endemic disease equilibria, resulting in limit cycles.","sentences":["We develop a mechanistic model that classifies individuals both in terms of epidemiological status (SIR) and vaccination attitude (willing or unwilling), with the goal of discovering how disease spread is influenced by changing opinions about vaccination.","Analysis of the model identifies existence and stability criteria for both disease-free and endemic disease equilibria.","The analytical results, supported by numerical simulations, show that attitude changes induced by disease prevalence can destabilize endemic disease equilibria, resulting in limit cycles."],"url":"http://arxiv.org/abs/2405.03931v1","category":"math.DS"}
{"created":"2024-05-07 01:15:59","title":"MSene: A new large family of two-dimensional transition metal sulfide with MXene structure","abstract":"Nowadays, discovering new two-dimensional (2D) materials with unique properties and potential applications remains a focus in condensed matter physics. Here, based on first-principles calculations, we report a new large family of transition metal sulfides M2S with MXene structure in 2H and 1T phases, which we name as MSene. Twenty-four out of fifty-eight MSenes are proved to be stable. Notably, this family includes twelve superconducting (SC) materials, seven SC topological metals (SCTMs), four charge density wave (CDW) materials, and five magnetic materials including one ferromagnetic (FM) and four antiferromagnetic (AFM) materials. For example, 2H-W2S is a SCTM which exhibits SC transition temperature (Tc) of 12.4 K and nontrivial topological properties as evidenced by topological invariant Z2=1 and the presence of edge states; 1T-Hf2S is a CDW material with the CDW originating from electron-phonon coupling. The CDW can be suppressed by compressive strain, leading to the emergence of superconductivity; 2H-Cr2S and 1T-Mn2S show FM and AFM properties, respectively. Thus, the new large family we predicted shows rich physical properties and significantly expands the repertoire of 2D materials. It serves as a novel platform for investigating the competition or coexistence of multiple orders such as SC, CDW, FM, AFM and topological orders in 2D materials.","sentences":["Nowadays, discovering new two-dimensional (2D) materials with unique properties and potential applications remains a focus in condensed matter physics.","Here, based on first-principles calculations, we report a new large family of transition metal sulfides M2S with MXene structure in 2H and 1T phases, which we name as MSene.","Twenty-four out of fifty-eight MSenes are proved to be stable.","Notably, this family includes twelve superconducting (SC) materials, seven SC topological metals (SCTMs), four charge density wave (CDW) materials, and five magnetic materials including one ferromagnetic (FM) and four antiferromagnetic (AFM) materials.","For example, 2H-W2S is a SCTM which exhibits SC transition temperature (Tc) of 12.4 K and nontrivial topological properties as evidenced by topological invariant Z2=1 and the presence of edge states; 1T-Hf2S is a CDW material with the CDW originating from electron-phonon coupling.","The CDW can be suppressed by compressive strain, leading to the emergence of superconductivity; 2H-Cr2S and 1T-Mn2S show FM and AFM properties, respectively.","Thus, the new large family we predicted shows rich physical properties and significantly expands the repertoire of 2D materials.","It serves as a novel platform for investigating the competition or coexistence of multiple orders such as SC, CDW, FM, AFM and topological orders in 2D materials."],"url":"http://arxiv.org/abs/2405.03928v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 01:11:14","title":"Codexity: Secure AI-assisted Code Generation","abstract":"Despite the impressive performance of Large Language Models (LLMs) in software development activities, recent studies show the concern of introducing vulnerabilities into software codebase by AI programming assistants (e.g., Copilot, CodeWhisperer). In this work, we present Codexity, a security-focused code generation framework integrated with five LLMs. Codexity leverages the feedback of static analysis tools such as Infer and CppCheck to mitigate security vulnerabilities in LLM-generated programs. Our evaluation in a real-world benchmark with 751 automatically generated vulnerable subjects demonstrates Codexity can prevent 60% of the vulnerabilities being exposed to the software developer.","sentences":["Despite the impressive performance of Large Language Models (LLMs) in software development activities, recent studies show the concern of introducing vulnerabilities into software codebase by AI programming assistants (e.g., Copilot, CodeWhisperer).","In this work, we present Codexity, a security-focused code generation framework integrated with five LLMs.","Codexity leverages the feedback of static analysis tools such as Infer and CppCheck to mitigate security vulnerabilities in LLM-generated programs.","Our evaluation in a real-world benchmark with 751 automatically generated vulnerable subjects demonstrates Codexity can prevent 60% of the vulnerabilities being exposed to the software developer."],"url":"http://arxiv.org/abs/2405.03927v1","category":"cs.SE"}
{"created":"2024-05-07 01:07:02","title":"Einstein-Grisaru-Zanon gravity","abstract":"The leading $\\alpha'$-correction to the gravitational low-energy effective action of closed (type II) superstring theory in four-spacetime dimensions defines the Einstein-Grisaru-Zanon gravity action that is applied for a calculation of the leading corrections to the Schwarzschild solution and the Hubble function in the Friedmann-Lemaitre-Robertson-Walker universe, in the first order with respect to the effective string-generated coupling. The solutions found are compared to the corresponding solutions in the Einstein-Bel-Robinson gravity that also modifies the Einstein gravity by the terms quartic in the spacetime curvature. We consider the black hole shadows in the Einstein-Grisaru-Zanon gravity theory and derive the upper bound on the string coupling parameter from the Hawking temperature of a black hole.","sentences":["The leading $\\alpha'$-correction to the gravitational low-energy effective action of closed (type II) superstring theory in four-spacetime dimensions defines the Einstein-Grisaru-Zanon gravity action that is applied for a calculation of the leading corrections to the Schwarzschild solution and the Hubble function in the Friedmann-Lemaitre-Robertson-Walker universe, in the first order with respect to the effective string-generated coupling.","The solutions found are compared to the corresponding solutions in the Einstein-Bel-Robinson gravity that also modifies the Einstein gravity by the terms quartic in the spacetime curvature.","We consider the black hole shadows in the Einstein-Grisaru-Zanon gravity theory and derive the upper bound on the string coupling parameter from the Hawking temperature of a black hole."],"url":"http://arxiv.org/abs/2405.03925v1","category":"hep-th"}
{"created":"2024-05-07 00:23:14","title":"VLBA Astrometry of the Galactic Double Neutron Stars PSR J0509+3801 and PSR J1930-1852: A Preliminary Transverse Velocity Distribution of Double Neutron Stars and Its Implications","abstract":"The mergers of double neutron stars (DNSs) systems are believed to drive the majority of short $\\gamma$-ray bursts (SGRBs), while also serving as production sites of heavy r-process elements. Despite being key to i) confirming the nature of the extragalactic SGRBs, ii) addressing the poorly-understood r-process enrichment in the ultra-faint dwarf galaxies (UFDGs), and iii) probing the formation process of DNS systems, the space velocity distribution of DNSs is still poorly constrained due to the small number of DNSs with well-determined astrometry. In this work, we determine new proper motions and parallaxes of two Galactic DNSs -- PSR J0509+3801 and PSR J1930-1852, using the Very Long Baseline Array, and estimate the transverse velocities $v_\\perp$ of all the 11 isolated Galactic DNSs having proper motion measurements in a consistent manner. Our correlation analysis reveals that the DNS $v_\\perp$ is tentatively correlated with three parameters: spin period, orbital eccentricity, and companion mass. With the preliminary $v_\\perp$ distribution, we obtain the following findings. Firstly, the refined $v_\\perp$ distribution is confirmed to agree with the observed displacements of the localized SGRBs from their host galaxy birth sites. Secondly, we estimate that around 11% and 25% of DNSs remain gravitationally bound to UFDGs with escape velocities of 15$\\mathrm{~km~s^{-1}}$ and 25$\\mathrm{~km~s^{-1}}$, respectively. Hence, the retained DNSs might indeed be responsible for the r-process enrichment confirmed so far in a few UFDGs. Finally, we discuss how a future ensemble of astrometrically determined DNSs may probe the multimodality of the $v_\\perp$ distribution.","sentences":["The mergers of double neutron stars (DNSs) systems are believed to drive the majority of short $\\gamma$-ray bursts (SGRBs), while also serving as production sites of heavy r-process elements.","Despite being key to i) confirming the nature of the extragalactic SGRBs, ii) addressing the poorly-understood r-process enrichment in the ultra-faint dwarf galaxies (UFDGs), and iii) probing the formation process of DNS systems, the space velocity distribution of DNSs is still poorly constrained due to the small number of DNSs with well-determined astrometry.","In this work, we determine new proper motions and parallaxes of two Galactic DNSs -- PSR J0509+3801 and PSR J1930-1852, using the Very Long Baseline Array, and estimate the transverse velocities $v_\\perp$ of all the 11 isolated Galactic DNSs having proper motion measurements in a consistent manner.","Our correlation analysis reveals that the DNS $v_\\perp$ is tentatively correlated with three parameters: spin period, orbital eccentricity, and companion mass.","With the preliminary $v_\\perp$ distribution, we obtain the following findings.","Firstly, the refined $v_\\perp$ distribution is confirmed to agree with the observed displacements of the localized SGRBs from their host galaxy birth sites.","Secondly, we estimate that around 11% and 25% of DNSs remain gravitationally bound to UFDGs with escape velocities of 15$\\mathrm{~km~s^{-1}}$ and 25$\\mathrm{~km~s^{-1}}$, respectively.","Hence, the retained DNSs might indeed be responsible for the r-process enrichment confirmed so far in a few UFDGs.","Finally, we discuss how a future ensemble of astrometrically determined DNSs may probe the multimodality of the $v_\\perp$ distribution."],"url":"http://arxiv.org/abs/2405.03914v1","category":"astro-ph.HE"}
{"created":"2024-05-06 23:51:18","title":"Charged lepton-flavor violating constraints to non-unitarity in the Linear Seesaw scheme","abstract":"We analyze the non-unitary effects in the linear seesaw mechanism using the current constraints and future sensitivity of the charged Lepton Flavor Violation (cLFV) processes. We perform a random scan confronting the non-unitary parameters with the limits to the cLFV processes. We show our results for the normal and inverted ordering of the oscillation data. We also discuss the equivalence of different parametrizations of the non-unitary mixing matrix and show our results in terms of the different parameterizations. We found that the stronger restrictions in the non-unitary parameter space come from rare muon decay searches.","sentences":["We analyze the non-unitary effects in the linear seesaw mechanism using the current constraints and future sensitivity of the charged Lepton Flavor Violation (cLFV) processes.","We perform a random scan confronting the non-unitary parameters with the limits to the cLFV processes.","We show our results for the normal and inverted ordering of the oscillation data.","We also discuss the equivalence of different parametrizations of the non-unitary mixing matrix and show our results in terms of the different parameterizations.","We found that the stronger restrictions in the non-unitary parameter space come from rare muon decay searches."],"url":"http://arxiv.org/abs/2405.03907v1","category":"hep-ph"}
{"created":"2024-05-06 22:01:50","title":"sqlelf: a SQL-centric Approach to ELF Analysis","abstract":"The exploration and understanding of Executable and Linkable Format (ELF) objects underpin various critical activities in computer systems, from debugging to reverse engineering. Traditional UNIX tooling like readelf, nm, and objdump have served the community reliably over the years. However, as the complexity and scale of software projects has grown, there arises a need for more intuitive, flexible, and powerful methods to investigate ELF objects. In this paper, we introduce sqlelf, an innovative tool that empowers users to probe ELF objects through the expressive power of SQL. By modeling ELF objects as relational databases, sqlelf offers the following advantages over conventional methods.   Our evaluations demonstrate that sqlelf not only provides more nuanced and comprehensive insights into ELF objects but also significantly reduces the effort and time traditionally required for ELF exploration tasks","sentences":["The exploration and understanding of Executable and Linkable Format (ELF) objects underpin various critical activities in computer systems, from debugging to reverse engineering.","Traditional UNIX tooling like readelf, nm, and objdump have served the community reliably over the years.","However, as the complexity and scale of software projects has grown, there arises a need for more intuitive, flexible, and powerful methods to investigate ELF objects.","In this paper, we introduce sqlelf, an innovative tool that empowers users to probe ELF objects through the expressive power of SQL.","By modeling ELF objects as relational databases, sqlelf offers the following advantages over conventional methods.   ","Our evaluations demonstrate that sqlelf not only provides more nuanced and comprehensive insights into ELF objects but also significantly reduces the effort and time traditionally required for ELF exploration tasks"],"url":"http://arxiv.org/abs/2405.03883v1","category":"cs.SE"}
{"created":"2024-05-06 21:46:50","title":"Energy and temperature dependencies for electron-induced sputtering from H$_2$O-ice: Implications for the icy Galilean moons","abstract":"To better assess the role that electrons play in exosphere production on icy-rich bodies, we measured the total and O$_2$ sputtering yields from H$_2$O-ice for electrons with energies between 0.75 and 10 keV and temperatures between 15 and 124.5 K. We find that both total and O$_2$ yields increase with decreasing energy over our studied range, increase rapidly at temperatures above 60 K, and that the relative amount of H$_2$O in the sputtered flux decreases quickly with increasing energy. Combining our data with other electron data in literature, we show that the accuracy of a widely used sputtering model can be improved significantly for electrons by adjusting some of the intrinsic parameter values. Applying our results to Europa, we estimate that electrons contribute to the production of the O$_2$ exosphere equally to all ion types combined. In contrast, sputtering of O$_2$ from Ganymede and Callisto appears to be dominated by irradiating ions, though electrons still likely contribute a non-negligible amount. While our estimates could be further refined by examining the importance of spatial variations in electron flux, we conclude that, at the very least, electrons seem to be important for exosphere production on icy surfaces and should be included in future modeling efforts.","sentences":["To better assess the role that electrons play in exosphere production on icy-rich bodies, we measured the total and O$_2$ sputtering yields from H$_2$O-ice for electrons with energies between 0.75 and 10 keV and temperatures between 15 and 124.5 K. We find that both total and O$_2$ yields increase with decreasing energy over our studied range, increase rapidly at temperatures above 60 K, and that the relative amount of H$_2$O in the sputtered flux decreases quickly with increasing energy.","Combining our data with other electron data in literature, we show that the accuracy of a widely used sputtering model can be improved significantly for electrons by adjusting some of the intrinsic parameter values.","Applying our results to Europa, we estimate that electrons contribute to the production of the O$_2$ exosphere equally to all ion types combined.","In contrast, sputtering of O$_2$ from Ganymede and Callisto appears to be dominated by irradiating ions, though electrons still likely contribute a non-negligible amount.","While our estimates could be further refined by examining the importance of spatial variations in electron flux, we conclude that, at the very least, electrons seem to be important for exosphere production on icy surfaces and should be included in future modeling efforts."],"url":"http://arxiv.org/abs/2405.03876v1","category":"astro-ph.EP"}
{"created":"2024-05-06 21:32:16","title":"Tunable linear-optical phase amplification","abstract":"We combine lossless, phase-only transformations with fully-transmitting linear-optical scatterers to define the principle of linear-optical phase amplification. This enables a physical phase shift $\\phi$ to be nonlinearly mapped to a new space $\\gamma(\\phi)$ using linear optics, resulting in a completely general and enhanced phase shifter that can replace any standard one. A particular phase amplifier is experimentally realized, allowing the phase enhancement parameter $d\\gamma/d\\phi$ to be continuously tuned. Placing this enhanced phase shifter in one arm of a Mach-Zehnder interferometer led to an intensity-phase slope more than twenty times steeper than what can be obtained with its unamplified counterpart.","sentences":["We combine lossless, phase-only transformations with fully-transmitting linear-optical scatterers to define the principle of linear-optical phase amplification.","This enables a physical phase shift $\\phi$ to be nonlinearly mapped to a new space $\\gamma(\\phi)$ using linear optics, resulting in a completely general and enhanced phase shifter that can replace any standard one.","A particular phase amplifier is experimentally realized, allowing the phase enhancement parameter $d\\gamma/d\\phi$ to be continuously tuned.","Placing this enhanced phase shifter in one arm of a Mach-Zehnder interferometer led to an intensity-phase slope more than twenty times steeper than what can be obtained with its unamplified counterpart."],"url":"http://arxiv.org/abs/2405.03868v1","category":"physics.optics"}
{"created":"2024-05-06 21:12:53","title":"The MOST Hosts Survey: spectroscopic observation of the host galaxies of ~40,000 transients using DESI","abstract":"We present the MOST Hosts survey (Multi-Object Spectroscopy of Transient Hosts). The survey is planned to run throughout the five years of operation of the Dark Energy Spectroscopic Instrument (DESI) and will generate a spectroscopic catalog of the hosts of most transients observed to date, in particular all the supernovae observed by most public, untargeted, wide-field, optical surveys (PTF/iPTF, SDSS II, ZTF, DECAT, DESIRT). Scientific questions for which the MOST Hosts survey will be useful include Type Ia supernova cosmology, fundamental plane and peculiar velocity measurements, and the understanding of the correlations between transients and their host galaxy properties. Here, we present the first release of the MOST Hosts survey: 21,931 hosts of 20,235 transients. These numbers represent 36% of the final MOST Hosts sample, consisting of 60,212 potential host galaxies of 38,603 transients (a transient can be assigned multiple potential hosts). Of these galaxies, 40% do not appear in the DESI primary target list and therefore require a specific program like MOST Hosts. Of all the transients in the MOST Hosts list, only 26.7% have existing classifications, and so the survey will provide redshifts (and luminosities) for nearly 30,000 transients. A preliminary Hubble diagram and a transient luminosity-duration diagram are shown as examples of future potential uses of the MOST Hosts survey. The survey will also provide a training sample of spectroscopically observed transients for photometry-only classifiers, as we enter an era when most newly observed transients will lack spectroscopic classification. The MOST Hosts DESI survey data will be released through the Wiserep platform on a rolling cadence and updated to match the DESI releases. Dates of future releases and updates are available through the https://mosthosts.desi.lbl.gov website.","sentences":["We present the MOST Hosts survey (Multi-Object Spectroscopy of Transient Hosts).","The survey is planned to run throughout the five years of operation of the Dark Energy Spectroscopic Instrument (DESI) and will generate a spectroscopic catalog of the hosts of most transients observed to date, in particular all the supernovae observed by most public, untargeted, wide-field, optical surveys (PTF/iPTF, SDSS II, ZTF, DECAT, DESIRT).","Scientific questions for which the MOST Hosts survey will be useful include Type Ia supernova cosmology, fundamental plane and peculiar velocity measurements, and the understanding of the correlations between transients and their host galaxy properties.","Here, we present the first release of the MOST Hosts survey: 21,931 hosts of 20,235 transients.","These numbers represent 36% of the final MOST Hosts sample, consisting of 60,212 potential host galaxies of 38,603 transients (a transient can be assigned multiple potential hosts).","Of these galaxies, 40% do not appear in the DESI primary target list and therefore require a specific program like MOST Hosts.","Of all the transients in the MOST Hosts list, only 26.7% have existing classifications, and so the survey will provide redshifts (and luminosities) for nearly 30,000 transients.","A preliminary Hubble diagram and a transient luminosity-duration diagram are shown as examples of future potential uses of the MOST Hosts survey.","The survey will also provide a training sample of spectroscopically observed transients for photometry-only classifiers, as we enter an era when most newly observed transients will lack spectroscopic classification.","The MOST Hosts DESI survey data will be released through the Wiserep platform on a rolling cadence and updated to match the DESI releases.","Dates of future releases and updates are available through the https://mosthosts.desi.lbl.gov website."],"url":"http://arxiv.org/abs/2405.03857v1","category":"astro-ph.HE"}
