{"created":"2024-02-01 18:59:56","title":"AToM: Amortized Text-to-Mesh using 2D Diffusion","abstract":"We introduce Amortized Text-to-Mesh (AToM), a feed-forward text-to-mesh framework optimized across multiple text prompts simultaneously. In contrast to existing text-to-3D methods that often entail time-consuming per-prompt optimization and commonly output representations other than polygonal meshes, AToM directly generates high-quality textured meshes in less than 1 second with around 10 times reduction in the training cost, and generalizes to unseen prompts. Our key idea is a novel triplane-based text-to-mesh architecture with a two-stage amortized optimization strategy that ensures stable training and enables scalability. Through extensive experiments on various prompt benchmarks, AToM significantly outperforms state-of-the-art amortized approaches with over 4 times higher accuracy (in DF415 dataset) and produces more distinguishable and higher-quality 3D outputs. AToM demonstrates strong generalizability, offering finegrained 3D assets for unseen interpolated prompts without further optimization during inference, unlike per-prompt solutions.","sentences":["We introduce Amortized Text-to-Mesh (AToM), a feed-forward text-to-mesh framework optimized across multiple text prompts simultaneously.","In contrast to existing text-to-3D methods that often entail time-consuming per-prompt optimization and commonly output representations other than polygonal meshes, AToM directly generates high-quality textured meshes in less than 1 second with around 10 times reduction in the training cost, and generalizes to unseen prompts.","Our key idea is a novel triplane-based text-to-mesh architecture with a two-stage amortized optimization strategy that ensures stable training and enables scalability.","Through extensive experiments on various prompt benchmarks, AToM significantly outperforms state-of-the-art amortized approaches with over 4 times higher accuracy (in DF415 dataset) and produces more distinguishable and higher-quality 3D outputs.","AToM demonstrates strong generalizability, offering finegrained 3D assets for unseen interpolated prompts without further optimization during inference, unlike per-prompt solutions."],"url":"http://arxiv.org/abs/2402.00867v1","category":"cs.CV"}
{"created":"2024-02-01 18:59:56","title":"We're Not Using Videos Effectively: An Updated Domain Adaptive Video Segmentation Baseline","abstract":"There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain. While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames. However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking. In this work, we address this gap. Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets. To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark. Code available at https://github.com/SimarKareer/UnifiedVideoDA","sentences":["There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain.","While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames.","However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking.","In this work, we address this gap.","Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets.","To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark.","Code available at https://github.com/SimarKareer/UnifiedVideoDA"],"url":"http://arxiv.org/abs/2402.00868v1","category":"cs.CV"}
{"created":"2024-02-01 18:59:22","title":"Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection","abstract":"Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection. These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between in-distribution (ID) and OOD samples. However, existing feature-shaping methods usually employ rules manually designed for specific model architectures and OOD datasets, which consequently limit their generalization ability. To address this gap, we first formulate an abstract optimization framework for studying feature-shaping methods. We then propose a concrete reduction of the framework with a simple piecewise constant shaping function and show that existing feature-shaping methods approximate the optimal solution to the concrete optimization problem. Further, assuming that OOD data is inaccessible, we propose a formulation that yields a closed-form solution for the piecewise constant shaping function, utilizing solely the ID data. Through extensive experiments, we show that the feature-shaping function optimized by our method improves the generalization ability of OOD detection across a large variety of datasets and model architectures.","sentences":["Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection.","These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between in-distribution (ID) and OOD samples.","However, existing feature-shaping methods usually employ rules manually designed for specific model architectures and OOD datasets, which consequently limit their generalization ability.","To address this gap, we first formulate an abstract optimization framework for studying feature-shaping methods.","We then propose a concrete reduction of the framework with a simple piecewise constant shaping function and show that existing feature-shaping methods approximate the optimal solution to the concrete optimization problem.","Further, assuming that OOD data is inaccessible, we propose a formulation that yields a closed-form solution for the piecewise constant shaping function, utilizing solely the ID data.","Through extensive experiments, we show that the feature-shaping function optimized by our method improves the generalization ability of OOD detection across a large variety of datasets and model architectures."],"url":"http://arxiv.org/abs/2402.00865v1","category":"cs.CV"}
{"created":"2024-02-01 18:59:09","title":"ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields","abstract":"We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the scene's appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available.","sentences":["We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions.","In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency.","For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views.","For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene.","Incorporating these two strategies, our ViCA-NeRF operates in two stages.","In the initial stage, we blend edits from different views to create a preliminary 3D edit.","This is followed by a second stage of NeRF training, dedicated to further refining the scene's appearance.","Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art.","Our code is publicly available."],"url":"http://arxiv.org/abs/2402.00864v1","category":"cs.CV"}
{"created":"2024-02-01 18:58:44","title":"Geometry Transfer for Stylizing Radiance Fields","abstract":"Shape and geometric patterns are essential in defining stylistic identity. However, current 3D style transfer methods predominantly focus on transferring colors and textures, often overlooking geometric aspects. In this paper, we introduce Geometry Transfer, a novel method that leverages geometric deformation for 3D style transfer. This technique employs depth maps to extract a style guide, subsequently applied to stylize the geometry of radiance fields. Moreover, we propose new techniques that utilize geometric cues from the 3D scene, thereby enhancing aesthetic expressiveness and more accurately reflecting intended styles. Our extensive experiments show that Geometry Transfer enables a broader and more expressive range of stylizations, thereby significantly expanding the scope of 3D style transfer.","sentences":["Shape and geometric patterns are essential in defining stylistic identity.","However, current 3D style transfer methods predominantly focus on transferring colors and textures, often overlooking geometric aspects.","In this paper, we introduce Geometry Transfer, a novel method that leverages geometric deformation for 3D style transfer.","This technique employs depth maps to extract a style guide, subsequently applied to stylize the geometry of radiance fields.","Moreover, we propose new techniques that utilize geometric cues from the 3D scene, thereby enhancing aesthetic expressiveness and more accurately reflecting intended styles.","Our extensive experiments show that Geometry Transfer enables a broader and more expressive range of stylizations, thereby significantly expanding the scope of 3D style transfer."],"url":"http://arxiv.org/abs/2402.00863v1","category":"cs.CV"}
{"created":"2024-02-01 18:56:18","title":"Evaluating Large Language Models for Generalization and Robustness via Data Compression","abstract":"Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such as Mistral and Llama-2 demonstrate a good balance between performance and robustness. Results also suggest that models struggle to generalize on news and code data, but work especially well on arXiv papers. We also find the context size and tokenization implementation have a big impact of on the overall compression performance.","sentences":["Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation.","To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff.","Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff.","We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness.","Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data.","We find that the compression rate of many models reduces significantly after their cutoff date, but models such as Mistral and Llama-2 demonstrate a good balance between performance and robustness.","Results also suggest that models struggle to generalize on news and code data, but work especially well on arXiv papers.","We also find the context size and tokenization implementation have a big impact of on the overall compression performance."],"url":"http://arxiv.org/abs/2402.00861v1","category":"cs.CL"}
{"created":"2024-02-01 18:55:29","title":"Can Large Language Models Understand Context?","abstract":"Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models' ability to understand context. First, we evaluate the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results.","sentences":["Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent.","However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features.","This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models.","This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models' ability to understand context.","First, we evaluate the performance of LLMs under the in-context learning pretraining scenario.","Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models.","Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings.","We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark.","We conduct an extensive analysis of these scenarios to substantiate our experimental results."],"url":"http://arxiv.org/abs/2402.00858v1","category":"cs.CL"}
{"created":"2024-02-01 18:54:34","title":"Early Time Classification with Accumulated Accuracy Gap Control","abstract":"Early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input. In this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule. This data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification. We start by presenting a novel method that builds on the Learn-then-Test calibration framework to control this gap marginally, on average over i.i.d. instances. As this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times. Numerical experiments demonstrate the effectiveness, applicability, and usefulness of our method. We show that our proposed early stopping mechanism reduces up to 94% of timesteps used for classification while achieving rigorous accuracy gap control.","sentences":["Early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input.","In this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule.","This data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification.","We start by presenting a novel method that builds on the Learn-then-Test calibration framework to control this gap marginally, on average over i.i.d. instances.","As this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times.","Numerical experiments demonstrate the effectiveness, applicability, and usefulness of our method.","We show that our proposed early stopping mechanism reduces up to 94% of timesteps used for classification while achieving rigorous accuracy gap control."],"url":"http://arxiv.org/abs/2402.00857v1","category":"cs.LG"}
{"created":"2024-02-01 18:51:54","title":"Towards Efficient and Exact Optimization of Language Model Alignment","abstract":"The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient optimization by circumventing the complexities associated with RL algorithms. We compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data.","sentences":["The alignment of language models with human preferences is vital for their application in real-world tasks.","The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy.","While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement.","Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data.","Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   ","In this paper, we propose efficient exact optimization (EXO) of the alignment objective.","We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient optimization by circumventing the complexities associated with RL algorithms.","We compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data."],"url":"http://arxiv.org/abs/2402.00856v1","category":"cs.CL"}
{"created":"2024-02-01 18:50:53","title":"'Egalitarian pooling and sharing of longevity risk', a.k.a. 'The many ways to skin a tontine cat'","abstract":"There is little disagreement among insurance actuaries and financial economists about the societal benefits of longevity-risk pooling in the form of life annuities, defined benefit pensions, self-annuitization funds, and even tontine schemes. Indeed, the discounted value or cost of providing an income for life is lower -- in other words, the amount of upfront capital required to generate a similar income stream with the same level of statistical safety is lower -- when participants pool their financial resources versus going it alone. Moreover, when participants' financial circumstances and lifespans are homogenous, there is consensus on how to share the \"winnings\" among survivors, namely by distributing them equally among survivors, a.k.a. a uniform rule. Alas, what is lesser-known and much more problematic is allocating the winnings in such a pool when participants differ in wealth (contributions) and health (longevity), especially when the pools are relatively small in size. The same problems arise when viewed from the dual perspective of decentralized risk sharing (DRS). The positive correlation between health and income and the fact that wealthier participants are likely to live longer is a growing concern among pension and retirement policymakers. With that motivation in mind, this paper offers a modelling framework for distributing longevity-risk pools' income and benefits (or tontine winnings) when participants are heterogeneous. Similar to the nascent literature on decentralized risk sharing, there are several equally plausible arrangements for sharing benefits (a.k.a. \"skinning the cat\") among survivors. Moreover, the selected rule depends on the extent of social cohesion within the longevity risk pool, ranging from solidarity and altruism to pure individualism. In sum, actuarial science cannot really offer or guarantee uniqueness, only a methodology.","sentences":["There is little disagreement among insurance actuaries and financial economists about the societal benefits of longevity-risk pooling in the form of life annuities, defined benefit pensions, self-annuitization funds, and even tontine schemes.","Indeed, the discounted value or cost of providing an income for life is lower -- in other words, the amount of upfront capital required to generate a similar income stream with the same level of statistical safety is lower -- when participants pool their financial resources versus going it alone.","Moreover, when participants' financial circumstances and lifespans are homogenous, there is consensus on how to share the \"winnings\" among survivors, namely by distributing them equally among survivors, a.k.a. a uniform rule.","Alas, what is lesser-known and much more problematic is allocating the winnings in such a pool when participants differ in wealth (contributions) and health (longevity), especially when the pools are relatively small in size.","The same problems arise when viewed from the dual perspective of decentralized risk sharing (DRS).","The positive correlation between health and income and the fact that wealthier participants are likely to live longer is a growing concern among pension and retirement policymakers.","With that motivation in mind, this paper offers a modelling framework for distributing longevity-risk pools' income and benefits (or tontine winnings) when participants are heterogeneous.","Similar to the nascent literature on decentralized risk sharing, there are several equally plausible arrangements for sharing benefits (a.k.a. \"skinning the cat\") among survivors.","Moreover, the selected rule depends on the extent of social cohesion within the longevity risk pool, ranging from solidarity and altruism to pure individualism.","In sum, actuarial science cannot really offer or guarantee uniqueness, only a methodology."],"url":"http://arxiv.org/abs/2402.00855v1","category":"q-fin.RM"}
{"created":"2024-02-01 18:50:50","title":"SymbolicAI: A framework for logic-based approaches combining generative models and solvers","abstract":"We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. In turn, the framework facilitates the creation and evaluation of explainable computational graphs. We conclude by introducing a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the \"Vector Embedding for Relational Trajectory Evaluation through Cross-similarity\", or VERTEX score for short. The framework codebase and benchmark are linked below.","sentences":["We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes.","SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI.","We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths.","The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives.","As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems.","In turn, the framework facilitates the creation and evaluation of explainable computational graphs.","We conclude by introducing a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows.","We refer to the empirical score as the \"Vector Embedding for Relational Trajectory Evaluation through Cross-similarity\", or VERTEX score for short.","The framework codebase and benchmark are linked below."],"url":"http://arxiv.org/abs/2402.00854v1","category":"cs.LG"}
{"created":"2024-02-01 18:50:42","title":"LTAU-FF: Loss Trajectory Analysis for Uncertainty in Atomistic Force Fields","abstract":"Model ensembles are simple and effective tools for estimating the prediction uncertainty of deep learning atomistic force fields. Despite this, widespread adoption of ensemble-based uncertainty quantification (UQ) techniques is limited by the high computational costs incurred by ensembles during both training and inference. In this work we leverage the cumulative distribution functions (CDFs) of per-sample errors obtained over the course of training to efficiently represent the model ensemble, and couple them with a distance-based similarity search in the model latent space. Using these tools, we develop a simple UQ metric (which we call LTAU) that leverages the strengths of ensemble-based techniques without requiring the evaluation of multiple models during either training or inference. As an initial test, we apply our method towards estimating the epistemic uncertainty in atomistic force fields (LTAU-FF) and demonstrate that it can be easily calibrated to accurately predict test errors on multiple datasets from the literature. We then illustrate the utility of LTAU-FF in two practical applications: 1) tuning the training-validation gap for an example dataset, and 2) predicting errors in relaxation trajectories on the OC20 IS2RS task. Though in this work we focus on the use of LTAU with deep learning atomistic force fields, we emphasize that it can be readily applied to any regression task, or any ensemble-generation technique, to provide a reliable and easy-to-implement UQ metric.","sentences":["Model ensembles are simple and effective tools for estimating the prediction uncertainty of deep learning atomistic force fields.","Despite this, widespread adoption of ensemble-based uncertainty quantification (UQ) techniques is limited by the high computational costs incurred by ensembles during both training and inference.","In this work we leverage the cumulative distribution functions (CDFs) of per-sample errors obtained over the course of training to efficiently represent the model ensemble, and couple them with a distance-based similarity search in the model latent space.","Using these tools, we develop a simple UQ metric (which we call LTAU) that leverages the strengths of ensemble-based techniques without requiring the evaluation of multiple models during either training or inference.","As an initial test, we apply our method towards estimating the epistemic uncertainty in atomistic force fields (LTAU-FF) and demonstrate that it can be easily calibrated to accurately predict test errors on multiple datasets from the literature.","We then illustrate the utility of LTAU-FF in two practical applications: 1) tuning the training-validation gap for an example dataset, and 2) predicting errors in relaxation trajectories on the OC20 IS2RS task.","Though in this work we focus on the use of LTAU with deep learning atomistic force fields, we emphasize that it can be readily applied to any regression task, or any ensemble-generation technique, to provide a reliable and easy-to-implement UQ metric."],"url":"http://arxiv.org/abs/2402.00853v1","category":"cs.LG"}
{"created":"2024-02-01 18:50:35","title":"Two-stroke thermal machine using spin squeezing operation","abstract":"Quantum thermal machines are powerful platforms to investigate how quantum effects impact the energy flow between different systems. We here investigate a two-stroke cycle in which spin squeezing effects are intrinsically switched on during all the operation time. By using the Kitagawa and Ueda's parameter and the l1-norm to compute the degree of spin squeezing and the quantum coherence, we firstly show that the more the spin squeezing effect the more the amount of coherence in the energy basis. Then we employ the characteristic function approach to investigate the engine performance in view of the amount of spin squeezing into the system. Our results show that even assuming an always-on spin squeezing, which is directly associated with the amount of entropy production in the cycle, it is possible to find a better set of efficiency and extracted power for the engine provided a high control over the relevant parameters, i.e., the operation time and the squeezing intensity.","sentences":["Quantum thermal machines are powerful platforms to investigate how quantum effects impact the energy flow between different systems.","We here investigate a two-stroke cycle in which spin squeezing effects are intrinsically switched on during all the operation time.","By using the Kitagawa and Ueda's parameter and the l1-norm to compute the degree of spin squeezing and the quantum coherence, we firstly show that the more the spin squeezing effect the more the amount of coherence in the energy basis.","Then we employ the characteristic function approach to investigate the engine performance in view of the amount of spin squeezing into the system.","Our results show that even assuming an always-on spin squeezing, which is directly associated with the amount of entropy production in the cycle, it is possible to find a better set of efficiency and extracted power for the engine provided a high control over the relevant parameters, i.e., the operation time and the squeezing intensity."],"url":"http://arxiv.org/abs/2402.00852v1","category":"quant-ph"}
{"created":"2024-02-01 18:46:28","title":"Data Augmentation Scheme for Raman Spectra with Highly Correlated Annotations","abstract":"In biotechnology Raman Spectroscopy is rapidly gaining popularity as a process analytical technology (PAT) that measures cell densities, substrate- and product concentrations. As it records vibrational modes of molecules it provides that information non-invasively in a single spectrum. Typically, partial least squares (PLS) is the model of choice to infer information about variables of interest from the spectra. However, biological processes are known for their complexity where convolutional neural networks (CNN) present a powerful alternative. They can handle non-Gaussian noise and account for beam misalignment, pixel malfunctions or the presence of additional substances. However, they require a lot of data during model training, and they pick up non-linear dependencies in the process variables. In this work, we exploit the additive nature of spectra in order to generate additional data points from a given dataset that have statistically independent labels so that a network trained on such data exhibits low correlations between the model predictions. We show that training a CNN on these generated data points improves the performance on datasets where the annotations do not bear the same correlation as the dataset that was used for model training. This data augmentation technique enables us to reuse spectra as training data for new contexts that exhibit different correlations. The additional data allows for building a better and more robust model. This is of interest in scenarios where large amounts of historical data are available but are currently not used for model training. We demonstrate the capabilities of the proposed method using synthetic spectra of Ralstonia eutropha batch cultivations to monitor substrate, biomass and polyhydroxyalkanoate (PHA) biopolymer concentrations during of the experiments.","sentences":["In biotechnology Raman Spectroscopy is rapidly gaining popularity as a process analytical technology (PAT) that measures cell densities, substrate- and product concentrations.","As it records vibrational modes of molecules it provides that information non-invasively in a single spectrum.","Typically, partial least squares (PLS) is the model of choice to infer information about variables of interest from the spectra.","However, biological processes are known for their complexity where convolutional neural networks (CNN) present a powerful alternative.","They can handle non-Gaussian noise and account for beam misalignment, pixel malfunctions or the presence of additional substances.","However, they require a lot of data during model training, and they pick up non-linear dependencies in the process variables.","In this work, we exploit the additive nature of spectra in order to generate additional data points from a given dataset that have statistically independent labels so that a network trained on such data exhibits low correlations between the model predictions.","We show that training a CNN on these generated data points improves the performance on datasets where the annotations do not bear the same correlation as the dataset that was used for model training.","This data augmentation technique enables us to reuse spectra as training data for new contexts that exhibit different correlations.","The additional data allows for building a better and more robust model.","This is of interest in scenarios where large amounts of historical data are available but are currently not used for model training.","We demonstrate the capabilities of the proposed method using synthetic spectra of Ralstonia eutropha batch cultivations to monitor substrate, biomass and polyhydroxyalkanoate (PHA) biopolymer concentrations during of the experiments."],"url":"http://arxiv.org/abs/2402.00851v1","category":"cs.LG"}
{"created":"2024-02-01 18:40:08","title":"Constant Degree Direct Product Testers with Small Soundness","abstract":"Let $X$ be a $d$-dimensional simplicial complex. A function $F\\colon X(k)\\to \\{0,1\\}^k$ is said to be a direct product function if there exists a function $f\\colon X(1)\\to \\{0,1\\}$ such that $F(\\sigma) = (f(\\sigma_1), \\ldots, f(\\sigma_k))$ for each $k$-face $\\sigma$. In an effort to simplify components of the PCP theorem, Goldreich and Safra introduced the problem of direct product testing, which asks whether one can test if $F\\colon X(k)\\to \\{0,1\\}^k$ is correlated with a direct product function by querying $F$ on only $2$ inputs. Dinur and Kaufman conjectured that there exist bounded degree complexes with a direct product test in the small soundness regime. We resolve their conjecture by showing that for all $\\delta>0$, there exists a family of high-dimensional expanders with degree $O_{\\delta}(1)$ and a $2$-query direct product tester with soundness $\\delta$.   We use the characterization given by a subset of the authors and independently by Dikstein and Dinur, who showed that some form of non-Abelian coboundary expansion (which they called \"Unique-Games coboundary expansion\") is a necessary and sufficient condition for a complex to admit such direct product testers. Our main technical contribution is a general technique for showing coboundary expansion of complexes with coefficients in a non-Abelian group. This allows us to prove that the high dimensional expanders constructed by Chapman and Lubotzky satisfies the necessary conditions, thus admitting a 2-query direct product tester with small soundness.","sentences":["Let $X$ be a $d$-dimensional simplicial complex.","A function $F\\colon X(k)\\to \\{0,1\\}^k$ is said to be a direct product function if there exists a function $f\\colon X(1)\\to \\{0,1\\}$ such that $F(\\sigma) = (f(\\sigma_1), \\ldots, f(\\sigma_k))$ for each $k$-face $\\sigma$. In an effort to simplify components of the PCP theorem, Goldreich and Safra introduced the problem of direct product testing, which asks whether one can test if $F\\colon X(k)\\to \\{0,1\\}^k$ is correlated with a direct product function by querying $F$ on only $2$ inputs.","Dinur and Kaufman conjectured that there exist bounded degree complexes with a direct product test in the small soundness regime.","We resolve their conjecture by showing that for all $\\delta>0$, there exists a family of high-dimensional expanders with degree $O_{\\delta}(1)$ and a $2$-query direct product tester with soundness $\\delta$.   We use the characterization given by a subset of the authors and independently by Dikstein and Dinur, who showed that some form of non-Abelian coboundary expansion (which they called \"Unique-Games coboundary expansion\") is a necessary and sufficient condition for a complex to admit such direct product testers.","Our main technical contribution is a general technique for showing coboundary expansion of complexes with coefficients in a non-Abelian group.","This allows us to prove that the high dimensional expanders constructed by Chapman and Lubotzky satisfies the necessary conditions, thus admitting a 2-query direct product tester with small soundness."],"url":"http://arxiv.org/abs/2402.00850v1","category":"cs.CC"}
{"created":"2024-02-01 18:40:03","title":"Score-based Causal Representation Learning: Linear and General Transformations","abstract":"This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \\emph{identifiability} and \\emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \\emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \\emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \\emph{linear} transformations and shows that one stochastic hard intervention per node suffices to guarantee identifiability. It also provides partial identifiability guarantees for soft interventions, including identifiability up to ancestors for general causal models and perfect latent graph recovery for sufficiently non-linear causal models. Secondly, it focuses on \\emph{general} transformations and shows that two stochastic hard interventions per node suffice for identifiability. Notably, one does \\emph{not} need to know which pair of interventional environments have the same node intervened.","sentences":["This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables.","Linear and general transformations are investigated.","The paper addresses both the \\emph{identifiability} and \\emph{achievability} aspects.","Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them.","Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees.","By drawing novel connections between \\emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \\emph{score-based class of algorithms} that ensures both identifiability and achievability.","First, the paper focuses on \\emph{linear} transformations and shows that one stochastic hard intervention per node suffices to guarantee identifiability.","It also provides partial identifiability guarantees for soft interventions, including identifiability up to ancestors for general causal models and perfect latent graph recovery for sufficiently non-linear causal models.","Secondly, it focuses on \\emph{general} transformations and shows that two stochastic hard interventions per node suffice for identifiability.","Notably, one does \\emph{not} need to know which pair of interventional environments have the same node intervened."],"url":"http://arxiv.org/abs/2402.00849v1","category":"cs.LG"}
{"created":"2024-02-01 18:38:55","title":"BootsTAP: Bootstrapped Training for Tracking-Any-Point","abstract":"To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time. Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion. In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup. We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.","sentences":["To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes.","This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time.","Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion.","In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup.","We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%."],"url":"http://arxiv.org/abs/2402.00847v1","category":"cs.CV"}
{"created":"2024-02-01 18:35:50","title":"When to Preempt in a Status Update System?","abstract":"We consider a time slotted status update system with an error-free preemptive queue. The goal of the sampler-scheduler pair is to minimize the age of information at the monitor by sampling and transmitting the freshly sampled update packets to the monitor. The sampler-scheduler pair also has a choice to preempt an old update packet from the server and transmit a new update packet to the server. We formulate this problem as a Markov decision process and find the optimal sampling policy. We show that it is optimal for the sampler-scheduler pair to sample a new packet immediately upon the reception of an update packet at the monitor. We also show that the optimal choice for the scheduler is to preempt an update packet in the server, if the age of that packet crosses a fixed threshold. Finally, we find the optimal preemption threshold when the range of the service time of the server is finite, otherwise we find the $\\epsilon$-optimal preemption threshold.","sentences":["We consider a time slotted status update system with an error-free preemptive queue.","The goal of the sampler-scheduler pair is to minimize the age of information at the monitor by sampling and transmitting the freshly sampled update packets to the monitor.","The sampler-scheduler pair also has a choice to preempt an old update packet from the server and transmit a new update packet to the server.","We formulate this problem as a Markov decision process and find the optimal sampling policy.","We show that it is optimal for the sampler-scheduler pair to sample a new packet immediately upon the reception of an update packet at the monitor.","We also show that the optimal choice for the scheduler is to preempt an update packet in the server, if the age of that packet crosses a fixed threshold.","Finally, we find the optimal preemption threshold when the range of the service time of the server is finite, otherwise we find the $\\epsilon$-optimal preemption threshold."],"url":"http://arxiv.org/abs/2402.00845v1","category":"cs.IT"}
{"created":"2024-02-01 18:34:32","title":"Kinematic reconstruction of torsion as dark energy in Friedmann cosmology","abstract":"In this paper we study the effects of torsion of space-time in the expansion of the universe as a candidate to dark energy. The analysis is done by reconstructing the torsion function along cosmic evolution by using observational data of Supernovae type Ia and Hubble parameter measurements. We have used a kinematic model for the parameterization of the comoving distance and the Hubble parameter, then the free parameters of the models are constrained by observational data. The reconstruction of the torsion function is obtained directly from the data, using the kinematic parameterizations, and the values for the Hubble parameter and the deceleration parameter are in good agreement to the standard model estimates.","sentences":["In this paper we study the effects of torsion of space-time in the expansion of the universe as a candidate to dark energy.","The analysis is done by reconstructing the torsion function along cosmic evolution by using observational data of Supernovae type Ia and Hubble parameter measurements.","We have used a kinematic model for the parameterization of the comoving distance and the Hubble parameter, then the free parameters of the models are constrained by observational data.","The reconstruction of the torsion function is obtained directly from the data, using the kinematic parameterizations, and the values for the Hubble parameter and the deceleration parameter are in good agreement to the standard model estimates."],"url":"http://arxiv.org/abs/2402.00844v1","category":"gr-qc"}
{"created":"2024-02-01 18:31:34","title":"Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?","abstract":"Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-shot Larger LLMs (from 7B to above 70B parameters), while being significantly smaller. This makes compact LLMs like FLAN-T5 a suitable cost-efficient solution for real-world industrial deployment.","sentences":["Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets.","However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources.","In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world.","In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2).","We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets.","However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-shot Larger LLMs (from 7B to above 70B parameters), while being significantly smaller.","This makes compact LLMs like FLAN-T5 a suitable cost-efficient solution for real-world industrial deployment."],"url":"http://arxiv.org/abs/2402.00841v1","category":"cs.CL"}
{"created":"2024-02-01 18:30:10","title":"Scattering wave packets of hadrons in gauge theories: Preparation on a quantum computer","abstract":"Quantum simulation holds promise of enabling a complete description of high-energy scattering processes rooted in gauge theories of the Standard Model. A first step in such simulations is preparation of interacting hadronic wave packets. To create the wave packets, one typically resorts to adiabatic evolution to bridge between wave packets in the free theory and those in the interacting theory, rendering the simulation resource intensive. In this work, we construct a wave-packet creation operator directly in the interacting theory to circumvent adiabatic evolution, taking advantage of resource-efficient schemes for ground-state preparation, such as variational quantum eigensolvers. By means of an ansatz for bound mesonic excitations in confining gauge theories, which is subsequently optimized using classical or quantum methods, we show that interacting mesonic wave packets can be created efficiently and accurately using digital quantum algorithms that we develop. Specifically, we obtain high-fidelity mesonic wave packets in the $Z_2$ and $U(1)$ lattice gauge theories coupled to fermionic matter in 1+1 dimensions. Our method is applicable to both perturbative and non-perturbative regimes of couplings. The wave-packet creation circuit for the case of the $Z_2$ lattice gauge theory is built and implemented on the Quantinuum H1-1 trapped-ion quantum computer using 13 qubits and up to 308 entangling gates. The fidelities agree well with classical benchmark calculations after employing a simple symmetry-based noise-mitigation technique. This work serves as a step toward quantum computing scattering processes in quantum chromodynamics.","sentences":["Quantum simulation holds promise of enabling a complete description of high-energy scattering processes rooted in gauge theories of the Standard Model.","A first step in such simulations is preparation of interacting hadronic wave packets.","To create the wave packets, one typically resorts to adiabatic evolution to bridge between wave packets in the free theory and those in the interacting theory, rendering the simulation resource intensive.","In this work, we construct a wave-packet creation operator directly in the interacting theory to circumvent adiabatic evolution, taking advantage of resource-efficient schemes for ground-state preparation, such as variational quantum eigensolvers.","By means of an ansatz for bound mesonic excitations in confining gauge theories, which is subsequently optimized using classical or quantum methods, we show that interacting mesonic wave packets can be created efficiently and accurately using digital quantum algorithms that we develop.","Specifically, we obtain high-fidelity mesonic wave packets in the $Z_2$ and $U(1)$ lattice gauge theories coupled to fermionic matter in 1+1 dimensions.","Our method is applicable to both perturbative and non-perturbative regimes of couplings.","The wave-packet creation circuit for the case of the $Z_2$ lattice gauge theory is built and implemented on the Quantinuum H1-1 trapped-ion quantum computer using 13 qubits and up to 308 entangling gates.","The fidelities agree well with classical benchmark calculations after employing a simple symmetry-based noise-mitigation technique.","This work serves as a step toward quantum computing scattering processes in quantum chromodynamics."],"url":"http://arxiv.org/abs/2402.00840v1","category":"quant-ph"}
{"created":"2024-02-01 18:29:16","title":"X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System","abstract":"The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex. Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks. However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making. This transparency gap in IDS research is significant, affecting confidence and accountability. To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology. Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to improve detection capabilities and adapt to novel threats. Through empirical testing, we establish that our approach not only achieves high accuracy with 99.47% in threat detection but also advances the field by providing clear, actionable explanations of its analytical outcomes. This research also aims to bridge the current gap and facilitate the broader integration of ML/DL technologies in cybersecurity defenses by offering a local and global explainability solution that is both precise and interpretable.","sentences":["The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex.","Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks.","However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making.","This transparency gap in IDS research is significant, affecting confidence and accountability.","To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology.","Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to improve detection capabilities and adapt to novel threats.","Through empirical testing, we establish that our approach not only achieves high accuracy with 99.47% in threat detection but also advances the field by providing clear, actionable explanations of its analytical outcomes.","This research also aims to bridge the current gap and facilitate the broader integration of ML/DL technologies in cybersecurity defenses by offering a local and global explainability solution that is both precise and interpretable."],"url":"http://arxiv.org/abs/2402.00839v1","category":"cs.CR"}
{"created":"2024-02-01 18:28:55","title":"OLMo: Accelerating the Science of Language Models","abstract":"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.","sentences":["Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings.","As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed.","Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs.","To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling.","Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code.","We hope this release will empower and strengthen the open research community and inspire a new wave of innovation."],"url":"http://arxiv.org/abs/2402.00838v1","category":"cs.CL"}
{"created":"2024-02-01 18:22:32","title":"ALISON: Fast and Effective Stylometric Authorship Obfuscation","abstract":"Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON.","sentences":["Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research.","Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier.","AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship.","To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours.","To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation.","We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics.","To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON."],"url":"http://arxiv.org/abs/2402.00835v1","category":"cs.CL"}
{"created":"2024-02-01 18:20:55","title":"Approximating maximum-size properly colored forests","abstract":"In the Properly Colored Spanning Tree problem, we are given an edge-colored undirected graph and the goal is to find a properly colored spanning tree, i.e., a spanning tree in which any two adjacent edges have distinct colors. The problem is interesting not only from a graph coloring point of view, but is also closely related to the Degree Bounded Spanning Tree and (1,2)-Traveling Salesman problems, two classical questions that have attracted considerable interest in combinatorial optimization and approximation theory. Previous work on properly colored spanning trees has mainly focused on determining the existence of such a tree and hence has not considered the question from an algorithmic perspective. We propose an optimization version called Maximum-size Properly Colored Forest problem, which aims to find a properly colored forest with as many edges as possible. We consider the problem in different graph classes and for different numbers of colors, and present polynomial-time approximation algorithms as well as inapproximability results for these settings. Our proof technique relies on the sum of matching matroids defined by the color classes, a connection that might be of independent combinatorial interest.   We also consider the Maximum-size Properly Colored Tree problem, which asks for the maximum size of a properly colored tree not necessarily spanning all the vertices. We show that the optimum is significantly more difficult to approximate than in the forest case, and provide an approximation algorithm for complete multigraphs.","sentences":["In the Properly Colored Spanning Tree problem, we are given an edge-colored undirected graph and the goal is to find a properly colored spanning tree, i.e., a spanning tree in which any two adjacent edges have distinct colors.","The problem is interesting not only from a graph coloring point of view, but is also closely related to the Degree Bounded Spanning Tree and (1,2)-Traveling Salesman problems, two classical questions that have attracted considerable interest in combinatorial optimization and approximation theory.","Previous work on properly colored spanning trees has mainly focused on determining the existence of such a tree and hence has not considered the question from an algorithmic perspective.","We propose an optimization version called Maximum-size Properly Colored Forest problem, which aims to find a properly colored forest with as many edges as possible.","We consider the problem in different graph classes and for different numbers of colors, and present polynomial-time approximation algorithms as well as inapproximability results for these settings.","Our proof technique relies on the sum of matching matroids defined by the color classes, a connection that might be of independent combinatorial interest.   ","We also consider the Maximum-size Properly Colored Tree problem, which asks for the maximum size of a properly colored tree not necessarily spanning all the vertices.","We show that the optimum is significantly more difficult to approximate than in the forest case, and provide an approximation algorithm for complete multigraphs."],"url":"http://arxiv.org/abs/2402.00834v1","category":"cs.DS"}
{"created":"2024-02-01 18:20:20","title":"Linear optics and the problem of Bell-like state discrimination","abstract":"A linear optics-based scheme to implement various quantum information processing tasks is of paramount importance due to ease of implementation and low noise. Many information-theoretic tasks depend on the successful discrimination of Bell states. A no-go theorem has been proved in literature which tells that it is not possible to perfectly discriminate among the four Bell states by restricting measurement apparatus to linear optical elements. The success probability is only $50\\%$. Through using extra resources such as hyper entanglement, ancillary entanglement, and even a minimum amount of non-linearity complete Bell-state discrimination can be achieved. The success probability for Bell-like state discrimination is only $25\\%$. We find that this can be boosted up to $50\\%$ using hyperentanglement in polarization, momentum, or OAM degrees of freedom of the photons which is in contrast to the Bell-state discrimination scenario where $100\\%$ can be achieved. Furthermore, we find that by using correlation in time of the photons all four Bell states can be distinguished with $100\\%$ success probability while for the Bell-like state discrimination, it strictly lies between $25\\%$ and $50\\%$ depending on the state parameter with only three Bell-like states being distinguishable. We also observe a similar contrast when we use ancillary entangled photons. While the success probability for all four Bell-state discrimination increases as $1-\\frac{1}{2^N}$ where N is the number of ancillary photons for Bell-like states it depends again on the state parameters and can be less than $25\\%$ in some cases. Also adding further ancillary photons decreases the success probability. We then show that using non-linear gadgets namely SFG $100\\%$ success probability can be achieved even for Bell-like state discrimination.","sentences":["A linear optics-based scheme to implement various quantum information processing tasks is of paramount importance due to ease of implementation and low noise.","Many information-theoretic tasks depend on the successful discrimination of Bell states.","A no-go theorem has been proved in literature which tells that it is not possible to perfectly discriminate among the four Bell states by restricting measurement apparatus to linear optical elements.","The success probability is only $50\\%$. Through using extra resources such as hyper entanglement, ancillary entanglement, and even a minimum amount of non-linearity complete Bell-state discrimination can be achieved.","The success probability for Bell-like state discrimination is only $25\\%$. We find that this can be boosted up to $50\\%$ using hyperentanglement in polarization, momentum, or OAM degrees of freedom of the photons which is in contrast to the Bell-state discrimination scenario where $100\\%$ can be achieved.","Furthermore, we find that by using correlation in time of the photons all four Bell states can be distinguished with $100\\%$ success probability while for the Bell-like state discrimination, it strictly lies between $25\\%$ and $50\\%$ depending on the state parameter with only three Bell-like states being distinguishable.","We also observe a similar contrast when we use ancillary entangled photons.","While the success probability for all four Bell-state discrimination increases as $1-\\frac{1}{2^N}$ where N is the number of ancillary photons for Bell-like states it depends again on the state parameters and can be less than $25\\%$ in some cases.","Also adding further ancillary photons decreases the success probability.","We then show that using non-linear gadgets namely SFG $100\\%$ success probability can be achieved even for Bell-like state discrimination."],"url":"http://arxiv.org/abs/2402.00832v1","category":"quant-ph"}
{"created":"2024-02-01 18:17:37","title":"A YANG-aided Unified Strategy for Black Hole Detection for Backbone Networks","abstract":"Despite the crucial importance of addressing Black Hole failures in Internet backbone networks, effective detection strategies in backbone networks are lacking. This is largely because previous research has been centered on Mobile Ad-hoc Networks (MANETs), which operate under entirely different dynamics, protocols, and topologies, making their findings not directly transferable to backbone networks. Furthermore, detecting Black Hole failures in backbone networks is particularly challenging. It requires a comprehensive range of network data due to the wide variety of conditions that need to be considered, making data collection and analysis far from straightforward. Addressing this gap, our study introduces a novel approach for Black Hole detection in backbone networks using specialized Yet Another Next Generation (YANG) data models with Black Hole-sensitive Metric Matrix (BHMM) analysis. This paper details our method of selecting and analyzing four YANG models relevant to Black Hole detection in ISP networks, focusing on routing protocols and ISP-specific configurations. Our BHMM approach derived from these models demonstrates a 10% improvement in detection accuracy and a 13% increase in packet delivery rate, highlighting the efficiency of our approach. Additionally, we evaluate the Machine Learning approach leveraged with BHMM analysis in two different network settings, a commercial ISP network, and a scientific research-only network topology. This evaluation also demonstrates the practical applicability of our method, yielding significantly improved prediction outcomes in both environments.","sentences":["Despite the crucial importance of addressing Black Hole failures in Internet backbone networks, effective detection strategies in backbone networks are lacking.","This is largely because previous research has been centered on Mobile Ad-hoc Networks (MANETs), which operate under entirely different dynamics, protocols, and topologies, making their findings not directly transferable to backbone networks.","Furthermore, detecting Black Hole failures in backbone networks is particularly challenging.","It requires a comprehensive range of network data due to the wide variety of conditions that need to be considered, making data collection and analysis far from straightforward.","Addressing this gap, our study introduces a novel approach for Black Hole detection in backbone networks using specialized","Yet Another Next Generation (YANG) data models with Black Hole-sensitive Metric Matrix (BHMM) analysis.","This paper details our method of selecting and analyzing four YANG models relevant to Black Hole detection in ISP networks, focusing on routing protocols and ISP-specific configurations.","Our BHMM approach derived from these models demonstrates a 10% improvement in detection accuracy and a 13% increase in packet delivery rate, highlighting the efficiency of our approach.","Additionally, we evaluate the Machine Learning approach leveraged with BHMM analysis in two different network settings, a commercial ISP network, and a scientific research-only network topology.","This evaluation also demonstrates the practical applicability of our method, yielding significantly improved prediction outcomes in both environments."],"url":"http://arxiv.org/abs/2402.00831v1","category":"cs.NI"}
{"created":"2024-02-01 18:17:29","title":"Common errors in Generative AI systems used for knowledge extraction in the climate action domain","abstract":"Large Language Models (LLMs) and, more specifically, the Generative Pre-Trained Transformers (GPT) can help stakeholders in climate action explore digital knowledge bases and extract and utilize climate action knowledge in a sustainable manner. However, LLMs are \"probabilistic models of knowledge bases\" that excel at generating convincing texts but cannot be entirely relied upon due to the probabilistic nature of the information produced. This brief report illustrates the problem space with examples of LLM responses to some of the questions of relevance to climate action.","sentences":["Large Language Models (LLMs) and, more specifically, the Generative Pre-Trained Transformers (GPT) can help stakeholders in climate action explore digital knowledge bases and extract and utilize climate action knowledge in a sustainable manner.","However, LLMs are \"probabilistic models of knowledge bases\" that excel at generating convincing texts but cannot be entirely relied upon due to the probabilistic nature of the information produced.","This brief report illustrates the problem space with examples of LLM responses to some of the questions of relevance to climate action."],"url":"http://arxiv.org/abs/2402.00830v1","category":"cs.CY"}
{"created":"2024-02-01 18:16:53","title":"The En Route Truck-Drone Delivery Problem","abstract":"We study the truck-drone cooperative delivery problem in a setting where a single truck carrying a drone travels at constant speed on a straight-line trajectory/street. Delivery to clients located in the plane and not on the truck's trajectory is performed by the drone, which has limited carrying capacity and flying range, and whose battery can be recharged when on the truck. We show that the problem of maximizing the number of deliveries is strongly NP-hard even in this simple setting. We present a 2-approximation algorithm for the problem, and an optimal algorithm for a non-trivial family of instances.","sentences":["We study the truck-drone cooperative delivery problem in a setting where a single truck carrying a drone travels at constant speed on a straight-line trajectory/street.","Delivery to clients located in the plane and not on the truck's trajectory is performed by the drone, which has limited carrying capacity and flying range, and whose battery can be recharged when on the truck.","We show that the problem of maximizing the number of deliveries is strongly NP-hard even in this simple setting.","We present a 2-approximation algorithm for the problem, and an optimal algorithm for a non-trivial family of instances."],"url":"http://arxiv.org/abs/2402.00829v1","category":"cs.DS"}
{"created":"2024-02-01 18:16:04","title":"Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters","abstract":"Mixture of Experts (MoE) architectures have recently started burgeoning due to their ability to scale model's capacity while maintaining the computational cost affordable. Furthermore, they can be applied to both Transformers and State Space Models, the current state-of-the-art models in numerous fields. While MoE has been mostly investigated for the pre-training stage, its use in parameter-efficient transfer learning settings is under-explored. To narrow this gap, this paper attempts to demystify the use of MoE for parameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and speech downstream tasks. Specifically, we propose Soft Mixture of Adapters (Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft MoE method, it relies on a soft assignment between the input tokens and experts to keep the computational time limited. Extensive experiments across 4 benchmarks demonstrate that Soft-MoA outperforms the single adapter method and performs on par with the dense MoA counterpart. We finally present ablation studies on key elements of Soft-MoA, showing for example that Soft-MoA achieves better scaling with more experts, as well as ensuring that all experts contribute to the computation of the output tokens, thus dispensing with the expert imbalance issue.","sentences":["Mixture of Experts (MoE) architectures have recently started burgeoning due to their ability to scale model's capacity while maintaining the computational cost affordable.","Furthermore, they can be applied to both Transformers and State Space Models, the current state-of-the-art models in numerous fields.","While MoE has been mostly investigated for the pre-training stage, its use in parameter-efficient transfer learning settings is under-explored.","To narrow this gap, this paper attempts to demystify the use of MoE for parameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and speech downstream tasks.","Specifically, we propose Soft Mixture of Adapters (Soft-MoA).","It exploits adapters as the experts and, leveraging the recent Soft MoE method, it relies on a soft assignment between the input tokens and experts to keep the computational time limited.","Extensive experiments across 4 benchmarks demonstrate that Soft-MoA outperforms the single adapter method and performs on par with the dense MoA counterpart.","We finally present ablation studies on key elements of Soft-MoA, showing for example that Soft-MoA achieves better scaling with more experts, as well as ensuring that all experts contribute to the computation of the output tokens, thus dispensing with the expert imbalance issue."],"url":"http://arxiv.org/abs/2402.00828v1","category":"eess.AS"}
{"created":"2024-02-01 18:14:42","title":"Emo-Avatar: Efficient Monocular Video Style Avatar through Texture Rendering","abstract":"Artistic video portrait generation is a significant and sought-after task in the fields of computer graphics and vision. While various methods have been developed that integrate NeRFs or StyleGANs with instructional editing models for creating and editing drivable portraits, these approaches face several challenges. They often rely heavily on large datasets, require extensive customization processes, and frequently result in reduced image quality. To address the above problems, we propose the Efficient Monotonic Video Style Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's capacity for producing dynamic, drivable portrait videos. We proposed a two-stage deferred neural rendering pipeline. In the first stage, we utilize few-shot PTI initialization to initialize the StyleGAN generator through several extreme poses sampled from the video to capture the consistent representation of aligned faces from the target portrait. In the second stage, we propose a Laplacian pyramid for high-frequency texture sampling from UV maps deformed by dynamic flow of expression for motion-aware texture prior integration to provide torso features to enhance StyleGAN's ability to generate complete and upper body for portrait video rendering. Emo-Avatar reduces style customization time from hours to merely 5 minutes compared with existing methods. In addition, Emo-Avatar requires only a single reference image for editing and employs region-aware contrastive learning with semantic invariant CLIP guidance, ensuring consistent high-resolution output and identity preservation. Through both quantitative and qualitative assessments, Emo-Avatar demonstrates superior performance over existing methods in terms of training efficiency, rendering quality and editability in self- and cross-reenactment.","sentences":["Artistic video portrait generation is a significant and sought-after task in the fields of computer graphics and vision.","While various methods have been developed that integrate NeRFs or StyleGANs with instructional editing models for creating and editing drivable portraits, these approaches face several challenges.","They often rely heavily on large datasets, require extensive customization processes, and frequently result in reduced image quality.","To address the above problems, we propose the Efficient Monotonic Video Style Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's capacity for producing dynamic, drivable portrait videos.","We proposed a two-stage deferred neural rendering pipeline.","In the first stage, we utilize few-shot PTI initialization to initialize the StyleGAN generator through several extreme poses sampled from the video to capture the consistent representation of aligned faces from the target portrait.","In the second stage, we propose a Laplacian pyramid for high-frequency texture sampling from UV maps deformed by dynamic flow of expression for motion-aware texture prior integration to provide torso features to enhance StyleGAN's ability to generate complete and upper body for portrait video rendering.","Emo-Avatar reduces style customization time from hours to merely 5 minutes compared with existing methods.","In addition, Emo-Avatar requires only a single reference image for editing and employs region-aware contrastive learning with semantic invariant CLIP guidance, ensuring consistent high-resolution output and identity preservation.","Through both quantitative and qualitative assessments, Emo-Avatar demonstrates superior performance over existing methods in terms of training efficiency, rendering quality and editability in self- and cross-reenactment."],"url":"http://arxiv.org/abs/2402.00827v1","category":"cs.CV"}
{"created":"2024-02-01 18:11:22","title":"Resolution invariant deep operator network for PDEs with complex geometries","abstract":"Neural operators (NO) are discretization invariant deep learning methods with functional output and can approximate any continuous operator. NO have demonstrated the superiority of solving partial differential equations (PDEs) over other deep learning methods. However, the spatial domain of its input function needs to be identical to its output, which limits its applicability. For instance, the widely used Fourier neural operator (FNO) fails to approximate the operator that maps the boundary condition to the PDE solution. To address this issue, we propose a novel framework called resolution-invariant deep operator (RDO) that decouples the spatial domain of the input and output. RDO is motivated by the Deep operator network (DeepONet) and it does not require retraining the network when the input/output is changed compared with DeepONet. RDO takes functional input and its output is also functional so that it keeps the resolution invariant property of NO. It can also resolve PDEs with complex geometries whereas NO fail. Various numerical experiments demonstrate the advantage of our method over DeepONet and FNO.","sentences":["Neural operators (NO) are discretization invariant deep learning methods with functional output and can approximate any continuous operator.","NO have demonstrated the superiority of solving partial differential equations (PDEs) over other deep learning methods.","However, the spatial domain of its input function needs to be identical to its output, which limits its applicability.","For instance, the widely used Fourier neural operator (FNO) fails to approximate the operator that maps the boundary condition to the PDE solution.","To address this issue, we propose a novel framework called resolution-invariant deep operator (RDO) that decouples the spatial domain of the input and output.","RDO is motivated by the Deep operator network (DeepONet) and it does not require retraining the network when the input/output is changed compared with DeepONet.","RDO takes functional input and its output is also functional so that it keeps the resolution invariant property of NO.","It can also resolve PDEs with complex geometries whereas NO fail.","Various numerical experiments demonstrate the advantage of our method over DeepONet and FNO."],"url":"http://arxiv.org/abs/2402.00825v1","category":"math.NA"}
{"created":"2024-02-01 18:08:28","title":"Role of filler lanthanide ions on lattice dynamics of phosphide skutterudites RFe$_4$P$_{12}$ (R = La, Ce, and Pr) from first principles","abstract":"Phosphide skutterudites primarily show promise for thermoelectric applications due to their chemical stability at high temperatures and relatively low cost. Ion doping and band gap engineering have been used to enhance their typically poor thermoelectric performance, opening avenues for practical applications. Herein, we report a comparative lattice dynamics study on the impact of filler and temperature on the structural and vibrational properties of RFe$_4$P$_{12}$ (R = La, Ce, and Pr) skutterudites. Calculations are performed within the quasi-harmonic approximation, and the results are critically compared against experimental data and other \\textit{ab initio} calculations. We found gaps between the heat-carrying acoustic and optical modes, a-o gaps, of approximately 4, -2, and 0.01\\,cm$^{-1}$ for La, Ce, and Pr compounds, respectively. These results suggest a filler-induced reduction in the a-o gap is attributed to the softening of the optical modes instead of the conventionally considered upward shift of acoustic modes proposed in the \\textit{rattling} scenario. The distinct softening of the optical modes is rationalized by the stiffening of chemical bonds between the filler and host lattice.","sentences":["Phosphide skutterudites primarily show promise for thermoelectric applications due to their chemical stability at high temperatures and relatively low cost.","Ion doping and band gap engineering have been used to enhance their typically poor thermoelectric performance, opening avenues for practical applications.","Herein, we report a comparative lattice dynamics study on the impact of filler and temperature on the structural and vibrational properties of RFe$_4$P$_{12}$ (R = La, Ce, and Pr) skutterudites.","Calculations are performed within the quasi-harmonic approximation, and the results are critically compared against experimental data and other \\textit{ab initio} calculations.","We found gaps between the heat-carrying acoustic and optical modes, a-o gaps, of approximately 4, -2, and 0.01\\,cm$^{-1}$ for La, Ce, and Pr compounds, respectively.","These results suggest a filler-induced reduction in the a-o gap is attributed to the softening of the optical modes instead of the conventionally considered upward shift of acoustic modes proposed in the \\textit{rattling} scenario.","The distinct softening of the optical modes is rationalized by the stiffening of chemical bonds between the filler and host lattice."],"url":"http://arxiv.org/abs/2402.00824v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-01 18:07:33","title":"SLIM: Skill Learning with Multiple Critics","abstract":"Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop manipulation, we demonstrate the applicability of our novel skill discovery approach to acquire safe and efficient motor primitives in a hierarchical reinforcement learning fashion and leverage them through planning, surpassing the state-of-the-art approaches for skill discovery by a large margin.","sentences":["Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment.","Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation.","As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors.","To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation.","Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills.","Furthermore, in the context of tabletop manipulation, we demonstrate the applicability of our novel skill discovery approach to acquire safe and efficient motor primitives in a hierarchical reinforcement learning fashion and leverage them through planning, surpassing the state-of-the-art approaches for skill discovery by a large margin."],"url":"http://arxiv.org/abs/2402.00823v1","category":"cs.LG"}
{"created":"2024-02-01 18:05:38","title":"WiOpen: A Robust Wi-Fi-based Open-set Gesture Recognition Framework","abstract":"Recent years have witnessed a growing interest in Wi-Fi-based gesture recognition. However, existing works have predominantly focused on closed-set paradigms, where all testing gestures are predefined during training. This poses a significant challenge in real-world applications, as unseen gestures might be misclassified as known classes during testing. To address this issue, we propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR) framework. Implementing OSGR requires addressing challenges caused by the unique uncertainty in Wi-Fi sensing. This uncertainty, resulting from noise and domains, leads to widely scattered and irregular data distributions in collected Wi-Fi sensing data. Consequently, data ambiguity between classes and challenges in defining appropriate decision boundaries to identify unknowns arise. To tackle these challenges, WiOpen adopts a two-fold approach to eliminate uncertainty and define precise decision boundaries. Initially, it addresses uncertainty induced by noise during data preprocessing by utilizing the CSI ratio. Next, it designs the OSGR network based on an uncertainty quantification method. Throughout the learning process, this network effectively mitigates uncertainty stemming from domains. Ultimately, the network leverages relationships among samples' neighbors to dynamically define open-set decision boundaries, successfully realizing OSGR. Comprehensive experiments on publicly accessible datasets confirm WiOpen's effectiveness. Notably, WiOpen also demonstrates superiority in cross-domain tasks when compared to state-of-the-art approaches.","sentences":["Recent years have witnessed a growing interest in Wi-Fi-based gesture recognition.","However, existing works have predominantly focused on closed-set paradigms, where all testing gestures are predefined during training.","This poses a significant challenge in real-world applications, as unseen gestures might be misclassified as known classes during testing.","To address this issue, we propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR) framework.","Implementing OSGR requires addressing challenges caused by the unique uncertainty in Wi-Fi sensing.","This uncertainty, resulting from noise and domains, leads to widely scattered and irregular data distributions in collected Wi-Fi sensing data.","Consequently, data ambiguity between classes and challenges in defining appropriate decision boundaries to identify unknowns arise.","To tackle these challenges, WiOpen adopts a two-fold approach to eliminate uncertainty and define precise decision boundaries.","Initially, it addresses uncertainty induced by noise during data preprocessing by utilizing the CSI ratio.","Next, it designs the OSGR network based on an uncertainty quantification method.","Throughout the learning process, this network effectively mitigates uncertainty stemming from domains.","Ultimately, the network leverages relationships among samples' neighbors to dynamically define open-set decision boundaries, successfully realizing OSGR.","Comprehensive experiments on publicly accessible datasets confirm WiOpen's effectiveness.","Notably, WiOpen also demonstrates superiority in cross-domain tasks when compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2402.00822v1","category":"cs.HC"}
{"created":"2024-02-01 18:05:24","title":"A High-Finesse Suspended Interferometric Sensor for Macroscopic Quantum Mechanics with Femtometre Sensitivity","abstract":"We present an interferometric sensor for investigating macroscopic quantum mechanics on a table-top scale. The sensor consists of pair of suspended optical cavities with a finesse in excess of 100,000 comprising 10 g fused-silica mirrors. In the current room-temperature operation, we achieve a peak sensitivity of \\SI{0.5}{\\fmasd} in the acoustic frequency band, limited by the readout noise. With additional suppression of the readout noise, we will be able to reach the quantum radiation pressure noise, which would represent a novel measurement of the quantum back-action effect. Such a sensor can eventually be utilised for demonstrating macroscopic entanglement and testing semi-classical and quantum gravity models.","sentences":["We present an interferometric sensor for investigating macroscopic quantum mechanics on a table-top scale.","The sensor consists of pair of suspended optical cavities with a finesse in excess of 100,000 comprising 10 g fused-silica mirrors.","In the current room-temperature operation, we achieve a peak sensitivity of \\SI{0.5}{\\fmasd} in the acoustic frequency band, limited by the readout noise.","With additional suppression of the readout noise, we will be able to reach the quantum radiation pressure noise, which would represent a novel measurement of the quantum back-action effect.","Such a sensor can eventually be utilised for demonstrating macroscopic entanglement and testing semi-classical and quantum gravity models."],"url":"http://arxiv.org/abs/2402.00821v1","category":"quant-ph"}
{"created":"2024-02-01 18:00:00","title":"Further understanding the interaction between dark energy and dark matter: current status and future directions","abstract":"The interaction between dark matter and dark energy can be incorporated into field theory models of dark energy that have proved successful in alleviating the coincidence problem. We review recent advances in this field, including new models and constraints from different astronomical data sets. We show that interactions are allowed by observations and can reduce the current tensions among different measurements of cosmological parameters. We extend our discussion to include constraints from non-linear effects and results from cosmological simulations. Finally, we discuss forthcoming multi-messenger data from current and future observational facilities that will help to improve our understanding of the interactions within the dark sector.","sentences":["The interaction between dark matter and dark energy can be incorporated into field theory models of dark energy that have proved successful in alleviating the coincidence problem.","We review recent advances in this field, including new models and constraints from different astronomical data sets.","We show that interactions are allowed by observations and can reduce the current tensions among different measurements of cosmological parameters.","We extend our discussion to include constraints from non-linear effects and results from cosmological simulations.","Finally, we discuss forthcoming multi-messenger data from current and future observational facilities that will help to improve our understanding of the interactions within the dark sector."],"url":"http://arxiv.org/abs/2402.00819v1","category":"astro-ph.CO"}
{"created":"2024-02-01 17:58:47","title":"The Entropy of Dynamical Black Holes","abstract":"We propose a new formula for the entropy of a dynamical black hole$-$valid to leading order for perturbations off of a stationary black hole background$-$in an arbitrary classical diffeomorphism covariant Lagrangian theory of gravity in $n$ dimensions. In stationary eras, this formula agrees with the usual Noether charge formula, but in nonstationary eras, we obtain a nontrivial correction term. In general relativity, our formula for the entropy of a dynamical black hole is $1/4$ of the horizon area plus a term involving the integral of the expansion of the null generators of the horizon, which we show is $1/4$ of the area of the apparent horizon to leading order. Our formula for entropy in a general theory of gravity obeys a \"local physical process version\" of the first law of black hole thermodynamics. For first order perturbations sourced by external matter that satisfies the null energy condition, our entropy obeys the second law of black hole thermodynamics. For vacuum perturbations, the second law is obeyed at leading order if and only if the \"modified canonical energy flux\" is positive (as is the case in general relativity but presumably would not hold in general theories). We obtain a general relationship between our formula for the entropy of a dynamical black hole and a formula proposed independently by Dong and by Wall. We then consider the generalized second law in semiclassical gravity for first order perturbations of a stationary black hole. We show that the validity of the quantum null energy condition (QNEC) on a Killing horizon is equivalent to the generalized second law using our notion of black hole entropy but using a modified notion of von Neumann entropy for matter. On the other hand, the generalized second law for the Dong-Wall entropy is equivalent to an integrated version of QNEC, using the unmodified von Neumann entropy for the entropy of matter.","sentences":["We propose a new formula for the entropy of a dynamical black hole$-$valid to leading order for perturbations off of a stationary black hole background$-$in","an arbitrary classical diffeomorphism covariant Lagrangian theory of gravity in $n$ dimensions.","In stationary eras, this formula agrees with the usual Noether charge formula, but in nonstationary eras, we obtain a nontrivial correction term.","In general relativity, our formula for the entropy of a dynamical black hole is $1/4$ of the horizon area plus a term involving the integral of the expansion of the null generators of the horizon, which we show is $1/4$ of the area of the apparent horizon to leading order.","Our formula for entropy in a general theory of gravity obeys a \"local physical process version\" of the first law of black hole thermodynamics.","For first order perturbations sourced by external matter that satisfies the null energy condition, our entropy obeys the second law of black hole thermodynamics.","For vacuum perturbations, the second law is obeyed at leading order if and only if the \"modified canonical energy flux\" is positive (as is the case in general relativity but presumably would not hold in general theories).","We obtain a general relationship between our formula for the entropy of a dynamical black hole and a formula proposed independently by Dong and by Wall.","We then consider the generalized second law in semiclassical gravity for first order perturbations of a stationary black hole.","We show that the validity of the quantum null energy condition (QNEC) on a Killing horizon is equivalent to the generalized second law using our notion of black hole entropy but using a modified notion of von Neumann entropy for matter.","On the other hand, the generalized second law for the Dong-Wall entropy is equivalent to an integrated version of QNEC, using the unmodified von Neumann entropy for the entropy of matter."],"url":"http://arxiv.org/abs/2402.00818v1","category":"hep-th"}
{"created":"2024-02-01 17:55:08","title":"Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous Environments","abstract":"Shielding is a popular technique for achieving safe reinforcement learning (RL). However, classical shielding approaches come with quite restrictive assumptions making them difficult to deploy in complex environments, particularly those with continuous state or action spaces. In this paper we extend the more versatile approximate model-based shielding (AMBS) framework to the continuous setting. In particular we use Safety Gym as our test-bed, allowing for a more direct comparison of AMBS with popular constrained RL algorithms. We also provide strong probabilistic safety guarantees for the continuous setting. In addition, we propose two novel penalty techniques that directly modify the policy gradient, which empirically provide more stable convergence in our experiments.","sentences":["Shielding is a popular technique for achieving safe reinforcement learning (RL).","However, classical shielding approaches come with quite restrictive assumptions making them difficult to deploy in complex environments, particularly those with continuous state or action spaces.","In this paper we extend the more versatile approximate model-based shielding (AMBS) framework to the continuous setting.","In particular we use Safety Gym as our test-bed, allowing for a more direct comparison of AMBS with popular constrained RL algorithms.","We also provide strong probabilistic safety guarantees for the continuous setting.","In addition, we propose two novel penalty techniques that directly modify the policy gradient, which empirically provide more stable convergence in our experiments."],"url":"http://arxiv.org/abs/2402.00816v1","category":"cs.LG"}
{"created":"2024-02-01 17:51:38","title":"Impact of anti-symmetric contributions to signal multipoles in the measurement of black-hole spins","abstract":"Many current models for the gravitational-wave signal from precessing black-hole binaries neglect an asymmetry in the $\\pm m$ multipoles. The asymmetry is weak, but is responsible for out-of-plane recoil, which for the final black hole can be several thousand km/s. In this work we show that the multipole asymmetry is also necessary to accurately measure the black-hole spins. We consider synthetic signals calculated from the numerical relativity surrogate model NRSur7dq4, which includes the multipole asymmetry, and measure the signal parameters using two versions of the same model, one with and one without the multipole asymmetry included. We find that in high signal-to-noise-ratio observations where the spin magnitude and direction can in principle be measured accurately, neglecting the multipole asymmetry can result in biased measurements of these quantities. Measurements of the black-hole masses and the standard aligned-spin combination $\\chi_{\\rm eff}$ are not in general strongly affected. As an illustration of the impact of the multipole asymmetry on a real signal we consider the LVK observation GW200129_065458, and find that the inclusion of the multipole asymmetry is necessary to identify the binary as unequal-mass and a high in-plane spin in the primary.","sentences":["Many current models for the gravitational-wave signal from precessing black-hole binaries neglect an asymmetry in the $\\pm m$ multipoles.","The asymmetry is weak, but is responsible for out-of-plane recoil, which for the final black hole can be several thousand km/s.","In this work we show that the multipole asymmetry is also necessary to accurately measure the black-hole spins.","We consider synthetic signals calculated from the numerical relativity surrogate model NRSur7dq4, which includes the multipole asymmetry, and measure the signal parameters using two versions of the same model, one with and one without the multipole asymmetry included.","We find that in high signal-to-noise-ratio observations where the spin magnitude and direction can in principle be measured accurately, neglecting the multipole asymmetry can result in biased measurements of these quantities.","Measurements of the black-hole masses and the standard aligned-spin combination $\\chi_{\\rm eff}$ are not in general strongly affected.","As an illustration of the impact of the multipole asymmetry on a real signal we consider the LVK observation GW200129_065458, and find that the inclusion of the multipole asymmetry is necessary to identify the binary as unequal-mass and a high in-plane spin in the primary."],"url":"http://arxiv.org/abs/2402.00813v1","category":"gr-qc"}
{"created":"2024-02-01 17:46:19","title":"An Analysis of the Variance of Diffusion-based Speech Enhancement","abstract":"Diffusion models proved to be powerful models for generative speech enhancement. In recent SGMSE+ approaches, training involves a stochastic differential equation for the diffusion process, adding both Gaussian and environmental noise to the clean speech signal gradually. The speech enhancement performance varies depending on the choice of the stochastic differential equation that controls the evolution of the mean and the variance along the diffusion processes when adding environmental and Gaussian noise. In this work, we highlight that the scale of the variance is a dominant parameter for speech enhancement performance and show that it controls the tradeoff between noise attenuation and speech distortions. More concretely, we show that a larger variance increases the noise attenuation and allows for reducing the computational footprint, as fewer function evaluations for generating the estimate are required.","sentences":["Diffusion models proved to be powerful models for generative speech enhancement.","In recent SGMSE+ approaches, training involves a stochastic differential equation for the diffusion process, adding both Gaussian and environmental noise to the clean speech signal gradually.","The speech enhancement performance varies depending on the choice of the stochastic differential equation that controls the evolution of the mean and the variance along the diffusion processes when adding environmental and Gaussian noise.","In this work, we highlight that the scale of the variance is a dominant parameter for speech enhancement performance and show that it controls the tradeoff between noise attenuation and speech distortions.","More concretely, we show that a larger variance increases the noise attenuation and allows for reducing the computational footprint, as fewer function evaluations for generating the estimate are required."],"url":"http://arxiv.org/abs/2402.00811v1","category":"eess.AS"}
{"created":"2024-02-01 17:45:26","title":"Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI","abstract":"In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.","sentences":["In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets.","However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention.","Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings.","This paper posits that BDL can elevate the capabilities of deep learning.","It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles.","Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential."],"url":"http://arxiv.org/abs/2402.00809v1","category":"cs.LG"}
{"created":"2024-02-01 17:44:46","title":"Exploring the Dynamics between Cobot's Production Rhythm, Locus of Control and Emotional State in a Collaborative Assembly Scenario","abstract":"In industrial scenarios, there is widespread use of collaborative robots (cobots), and growing interest is directed at evaluating and measuring the impact of some characteristics of the cobot on the human factor. In the present pilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 - Adapted to the participant's pace) of a cobot has on the Experiential Locus of Control (ELoC) and the emotional state of 31 participants has been examined. The operators' performance, the degree of basic internal Locus of Control, and the attitude towards the robots were also considered. No difference was found regarding the emotional state and the ELoC in the three conditions, but considering the other psychological variables, a more complex situation emerges. Overall, results seem to indicate a need to consider the person's psychological characteristics to offer a differentiated and optimal interaction experience.","sentences":["In industrial scenarios, there is widespread use of collaborative robots (cobots), and growing interest is directed at evaluating and measuring the impact of some characteristics of the cobot on the human factor.","In the present pilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 - Adapted to the participant's pace) of a cobot has on the Experiential Locus of Control (ELoC) and the emotional state of 31 participants has been examined.","The operators' performance, the degree of basic internal Locus of Control, and the attitude towards the robots were also considered.","No difference was found regarding the emotional state and the ELoC in the three conditions, but considering the other psychological variables, a more complex situation emerges.","Overall, results seem to indicate a need to consider the person's psychological characteristics to offer a differentiated and optimal interaction experience."],"url":"http://arxiv.org/abs/2402.00808v1","category":"cs.RO"}
{"created":"2024-02-01 17:44:11","title":"Distilling Conditional Diffusion Models for Offline Reinforcement Learning through Trajectory Stitching","abstract":"Deep generative models have recently emerged as an effective approach to offline reinforcement learning. However, their large model size poses challenges in computation. We address this issue by proposing a knowledge distillation method based on data augmentation. In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator. Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks.","sentences":["Deep generative models have recently emerged as an effective approach to offline reinforcement learning.","However, their large model size poses challenges in computation.","We address this issue by proposing a knowledge distillation method based on data augmentation.","In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator.","Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks."],"url":"http://arxiv.org/abs/2402.00807v1","category":"cs.LG"}
{"created":"2024-02-01 17:42:17","title":"Linearly coupled quantum harmonic oscillators and their quantum entanglement","abstract":"Quantum harmonic oscillators linearly coupled through coordinates and momenta, represented by the Hamiltonian $ {\\hat H}=\\sum^2_{i=1}\\left( \\frac{ {\\hat p}^{2}_i}{2 m_i } + \\frac{m_i \\omega^2_i}{2} x^2_i\\right) +{\\hat H}_{int} $, where the interaction of two oscillators ${\\hat H}_{int} = i k_1 x_1 { \\hat p }_2+ i k_2 x_2 {\\hat p}_1 + k_3 x_1 x_2-k_4 {\\hat p}_1 {\\hat p}_2$, found in many applications of quantum optics, nonlinear physics, molecular chemistry and biophysics. Despite this, there is currently no general solution to the Schr\\\"{o}dinger equation for such a system. This is especially relevant for quantum entanglement of such a system in quantum optics applications. Here this problem is solved and it is shown that quantum entanglement depends on only one coefficient $R \\in (0,1)$, which includes all the parameters of the system under consideration. It has been shown that quantum entanglement can be very large at certain values of this coefficient. The results obtained have a fairly simple analytical form, which facilitates analysis.","sentences":["Quantum harmonic oscillators linearly coupled through coordinates and momenta, represented by the Hamiltonian $ {\\hat H}=\\sum^2_{i=1}\\left( \\frac{ {\\hat p}^{2}_i}{2 m_i }","+ \\frac{m_i \\omega^2_i}{2} x^2_i\\right)","+{\\hat H}_{int} $, where the interaction of two oscillators ${\\hat H}_{int} = i k_1 x_1 { \\hat p }_2","+ i k_2 x_2 {\\hat p}_1 + k_3 x_1 x_2-k_4 {\\hat p}_1 {\\hat p}_2$, found in many applications of quantum optics, nonlinear physics, molecular chemistry and biophysics.","Despite this, there is currently no general solution to the Schr\\\"{o}dinger equation for such a system.","This is especially relevant for quantum entanglement of such a system in quantum optics applications.","Here this problem is solved and it is shown that quantum entanglement depends on only one coefficient $R \\in (0,1)$, which includes all the parameters of the system under consideration.","It has been shown that quantum entanglement can be very large at certain values of this coefficient.","The results obtained have a fairly simple analytical form, which facilitates analysis."],"url":"http://arxiv.org/abs/2402.00806v1","category":"quant-ph"}
{"created":"2024-02-01 17:40:10","title":"Signal Quality Auditing for Time-series Data","abstract":"Signal quality assessment (SQA) is required for monitoring the reliability of data acquisition systems, especially in AI-driven Predictive Maintenance (PMx) application contexts. SQA is vital for addressing \"silent failures\" of data acquisition hardware and software, which when unnoticed, misinform the users of data, creating the risk for incorrect decisions with unintended or even catastrophic consequences. We have developed an open-source software implementation of signal quality indices (SQIs) for the analysis of time-series data. We codify a range of SQIs, demonstrate them using established benchmark data, and show that they can be effective for signal quality assessment. We also study alternative approaches to denoising time-series data in an attempt to improve the quality of the already degraded signal, and evaluate them empirically on relevant real-world data. To our knowledge, our software toolkit is the first to provide an open source implementation of a broad range of signal quality assessment and improvement techniques validated on publicly available benchmark data for ease of reproducibility. The generality of our framework can be easily extended to assessing reliability of arbitrary time-series measurements in complex systems, especially when morphological patterns of the waveform shapes and signal periodicity are of key interest in downstream analyses.","sentences":["Signal quality assessment (SQA) is required for monitoring the reliability of data acquisition systems, especially in AI-driven Predictive Maintenance (PMx) application contexts.","SQA is vital for addressing \"silent failures\" of data acquisition hardware and software, which when unnoticed, misinform the users of data, creating the risk for incorrect decisions with unintended or even catastrophic consequences.","We have developed an open-source software implementation of signal quality indices (SQIs) for the analysis of time-series data.","We codify a range of SQIs, demonstrate them using established benchmark data, and show that they can be effective for signal quality assessment.","We also study alternative approaches to denoising time-series data in an attempt to improve the quality of the already degraded signal, and evaluate them empirically on relevant real-world data.","To our knowledge, our software toolkit is the first to provide an open source implementation of a broad range of signal quality assessment and improvement techniques validated on publicly available benchmark data for ease of reproducibility.","The generality of our framework can be easily extended to assessing reliability of arbitrary time-series measurements in complex systems, especially when morphological patterns of the waveform shapes and signal periodicity are of key interest in downstream analyses."],"url":"http://arxiv.org/abs/2402.00803v1","category":"cs.LG"}
{"created":"2024-02-01 17:39:36","title":"Test of the physical significance of Bell nonlocality","abstract":"The experimental violation of a Bell inequality implies that at least one of a set of assumptions fails in nature. However, existing tests are inconclusive about which of the assumptions is the one that fails. Here, we show that there are quantum correlations that cannot be simulated with hidden variables that allow the slightest free will (or, equivalently, that limit, even minimally, retrocausal influences) or restrict, even minimally, actions at a distance. This result goes beyond Bell's theorem and demolishes the arguably most attractive motivation for considering hidden-variable theories with measurement dependence or actions at distance, namely, that simulating quantum correlations typically requires a small amount of these resources. We show that there is a feasible experiment that can discard any hidden-variable theory allowing for arbitrarily small free will and having arbitrarily small limitations to actions at a distance. The experiment involves two observers, each of them choosing between two measurements with $2^N$ outcomes. The larger $N$ for which a specific Bell-like inequality is violated, the larger the set of excluded hidden-variable theories. In the limit of $N$ tending to infinity, the only alternatives to the absence of hidden variables are complete superdeterminism or complete parameter dependence. We also explore the implications of this result for quantum information.","sentences":["The experimental violation of a Bell inequality implies that at least one of a set of assumptions fails in nature.","However, existing tests are inconclusive about which of the assumptions is the one that fails.","Here, we show that there are quantum correlations that cannot be simulated with hidden variables that allow the slightest free will (or, equivalently, that limit, even minimally, retrocausal influences) or restrict, even minimally, actions at a distance.","This result goes beyond Bell's theorem and demolishes the arguably most attractive motivation for considering hidden-variable theories with measurement dependence or actions at distance, namely, that simulating quantum correlations typically requires a small amount of these resources.","We show that there is a feasible experiment that can discard any hidden-variable theory allowing for arbitrarily small free will and having arbitrarily small limitations to actions at a distance.","The experiment involves two observers, each of them choosing between two measurements with $2^N$ outcomes.","The larger $N$ for which a specific Bell-like inequality is violated, the larger the set of excluded hidden-variable theories.","In the limit of $N$ tending to infinity, the only alternatives to the absence of hidden variables are complete superdeterminism or complete parameter dependence.","We also explore the implications of this result for quantum information."],"url":"http://arxiv.org/abs/2402.00801v1","category":"quant-ph"}
{"created":"2024-02-01 17:30:50","title":"Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents","abstract":"Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans. Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential. The work is open-sourced at https://github.com/agiresearch/Formal-LLM.","sentences":["Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks.","However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents.","In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language.","Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton.","A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable.","We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans.","Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential.","The work is open-sourced at https://github.com/agiresearch/Formal-LLM."],"url":"http://arxiv.org/abs/2402.00798v1","category":"cs.LG"}
{"created":"2024-02-01 17:28:10","title":"LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law","abstract":"Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.","sentences":["Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting.","However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models.","In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest.","Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering.","Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law.","Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs."],"url":"http://arxiv.org/abs/2402.00795v1","category":"cs.LG"}
{"created":"2024-02-01 17:25:51","title":"ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models","abstract":"Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent). Our method updates the token importance distribution in a recursive manner. For each update, we compute the difference in the probability distribution over the vocabulary for predicting the next token between using the original input and using a modified version where a part of the input is replaced with RoBERTa predictions. Our intuition is that replacing an important token in the context should have resulted in a larger change in the model's confidence in predicting the token than replacing an unimportant token. Our method can be universally applied to any generative LM without accessing internal model weights or additional training and fine-tuning, as most other FAs require. We extensively compare the faithfulness of ReAGent with seven popular FAs across six decoder-only LMs of various sizes. The results show that our method consistently provides more faithful token importance distributions.","sentences":["Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions.","Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks.","However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively.","Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks.","This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute.","To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent).","Our method updates the token importance distribution in a recursive manner.","For each update, we compute the difference in the probability distribution over the vocabulary for predicting the next token between using the original input and using a modified version where a part of the input is replaced with RoBERTa predictions.","Our intuition is that replacing an important token in the context should have resulted in a larger change in the model's confidence in predicting the token than replacing an unimportant token.","Our method can be universally applied to any generative LM without accessing internal model weights or additional training and fine-tuning, as most other FAs require.","We extensively compare the faithfulness of ReAGent with seven popular FAs across six decoder-only LMs of various sizes.","The results show that our method consistently provides more faithful token importance distributions."],"url":"http://arxiv.org/abs/2402.00794v1","category":"cs.CL"}
{"created":"2024-02-01 17:23:54","title":"Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction","abstract":"We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration.","sentences":["We introduce a novel framework for incorporating human expertise into algorithmic predictions.","Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm.","We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data.","We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor.","We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante).","In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population.","Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration."],"url":"http://arxiv.org/abs/2402.00793v1","category":"cs.LG"}
{"created":"2024-02-01 17:23:32","title":"Quantum fluctuation dynamics of open quantum systems with collective operator-valued rates, and applications to Hopfield-like networks","abstract":"We consider a class of open quantum many-body systems that evolves in a Markovian fashion, the dynamical generator being in GKS-Lindblad form. Here, the Hamiltonian contribution is characterized by an all-to-all coupling, and the dissipation features local transitions that depend on collective, operator-valued rates, encoding average properties of the system. These types of generators can be formally obtained by generalizing, to the quantum realm, classical (mean-field) stochastic Markov dynamics, with state-dependent transitions. Focusing on the dynamics emerging in the limit of infinitely large systems, we build on the exactness of the mean-field equations for the dynamics of average operators. In this framework, we derive the dynamics of quantum fluctuation operators, that can be used in turn to understand the fate of quantum correlations in the system. We apply our results to quantum generalized Hopfield associative memories, showing that, asymptotically and at the mesoscopic scale only a very weak amount of quantum correlations, in the form of quantum discord, emerges beyond classical correlations.","sentences":["We consider a class of open quantum many-body systems that evolves in a Markovian fashion, the dynamical generator being in GKS-Lindblad form.","Here, the Hamiltonian contribution is characterized by an all-to-all coupling, and the dissipation features local transitions that depend on collective, operator-valued rates, encoding average properties of the system.","These types of generators can be formally obtained by generalizing, to the quantum realm, classical (mean-field) stochastic Markov dynamics, with state-dependent transitions.","Focusing on the dynamics emerging in the limit of infinitely large systems, we build on the exactness of the mean-field equations for the dynamics of average operators.","In this framework, we derive the dynamics of quantum fluctuation operators, that can be used in turn to understand the fate of quantum correlations in the system.","We apply our results to quantum generalized Hopfield associative memories, showing that, asymptotically and at the mesoscopic scale only a very weak amount of quantum correlations, in the form of quantum discord, emerges beyond classical correlations."],"url":"http://arxiv.org/abs/2402.00792v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-01 17:23:21","title":"Hausdorff Reductions and the Exponential Hierarchies","abstract":"The Strong Exponential Hierarchy $SEH$ was shown to collapse to $P^{NExp}$ by Hemachandra by proving $P^{NExp} = NP^{NExp}$ via a census argument. Nonetheless, Hemachandra also asked for certificate-based and alternating Turing machine characterizations of the $SEH$ levels, in the hope that these might have revealed deeper structural reasons behind the collapse. These open questions have thus far remained unanswered.   To close them, by building upon the notion of Hausdorff reductions, we investigate a natural normal form for the intermediate levels of the (generalized) exponential hierarchies, i.e., the single-, the double-Exponential Hierarchy, and so on. Although the two characterizations asked for derive from our Hausdorff characterization, it is nevertheless from the latter that a surprising structural reason behind the collapse of $SEH$ is uncovered as a consequence of a very general result: the intermediate levels of the exponential hierarchies are precisely characterized by specific \"Hausdorff classes\", which define these levels without resorting to oracle machines. By this, contrarily to oracle classes, which may have different shapes for a same class (e.g., $P^{NP}_{||} = P^{NP[Log]} = LogSpace^{NP}$), hierarchy intermediate levels are univocally identified by Hausdorff classes (under the hypothesis of no hierarchy collapse). In fact, we show that the rather simple reason behind many equivalences of oracle classes is that they just refer to different ways of deciding the languages of a same Hausdorff class, and this happens also for $P^{NExp}$ and $NP^{NExp}$.   In addition, via Hausdorff classes, we define complete problems for various intermediate levels of the exponential hierarchies. Through these, we obtain matching lower-bounds for problems known to be in $P^{NExp[Log]}$, but whose hardness was left open due to the lack of known $P^{NExp[Log]}$-complete problems.","sentences":["The Strong Exponential Hierarchy $SEH$ was shown to collapse to $P^{NExp}$ by Hemachandra by proving $P^{NExp} = NP^{NExp}$ via a census argument.","Nonetheless, Hemachandra also asked for certificate-based and alternating Turing machine characterizations of the $SEH$ levels, in the hope that these might have revealed deeper structural reasons behind the collapse.","These open questions have thus far remained unanswered.   ","To close them, by building upon the notion of Hausdorff reductions, we investigate a natural normal form for the intermediate levels of the (generalized) exponential hierarchies, i.e., the single-, the double-Exponential Hierarchy, and so on.","Although the two characterizations asked for derive from our Hausdorff characterization, it is nevertheless from the latter that a surprising structural reason behind the collapse of $SEH$ is uncovered as a consequence of a very general result: the intermediate levels of the exponential hierarchies are precisely characterized by specific \"Hausdorff classes\", which define these levels without resorting to oracle machines.","By this, contrarily to oracle classes, which may have different shapes for a same class (e.g., $P^{NP}_{||} = P^{NP[Log]} = LogSpace^{NP}$), hierarchy intermediate levels are univocally identified by Hausdorff classes (under the hypothesis of no hierarchy collapse).","In fact, we show that the rather simple reason behind many equivalences of oracle classes is that they just refer to different ways of deciding the languages of a same Hausdorff class, and this happens also for $P^{NExp}$ and $NP^{NExp}$.   In addition, via Hausdorff classes, we define complete problems for various intermediate levels of the exponential hierarchies.","Through these, we obtain matching lower-bounds for problems known to be in $P^{NExp[Log]}$, but whose hardness was left open due to the lack of known $P^{NExp[Log]}$-complete problems."],"url":"http://arxiv.org/abs/2402.00791v1","category":"cs.CC"}
{"created":"2024-02-01 17:22:48","title":"From Pre-Quantum to Post-Quantum IoT Security: A Survey on Quantum-Resistant Cryptosystems for the Internet of Things","abstract":"This article provides a survey on what can be called post-quantum IoT systems (IoT systems protected from the currently known quantum computing attacks): the main post-quantum cryptosystems and initiatives are reviewed, the most relevant IoT architectures and challenges are analyzed, and the expected future trends are indicated. Thus, this paper is aimed at providing a wide view of post-quantum IoT security and give useful guidelines to the future post-quantum IoT developers.","sentences":["This article provides a survey on what can be called post-quantum IoT systems (IoT systems protected from the currently known quantum computing attacks): the main post-quantum cryptosystems and initiatives are reviewed, the most relevant IoT architectures and challenges are analyzed, and the expected future trends are indicated.","Thus, this paper is aimed at providing a wide view of post-quantum IoT security and give useful guidelines to the future post-quantum IoT developers."],"url":"http://arxiv.org/abs/2402.00790v1","category":"cs.CR"}
{"created":"2024-02-01 17:21:53","title":"Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces","abstract":"Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantial improvement in predictive performance. Extensive experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms state-of-the-art methods in long-range graph prediction tasks, with a fraction of the computational cost in both FLOPs and GPU memory consumption. The code and models are publicly available at https://github.com/bowang-lab/Graph-Mamba.","sentences":["Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers.","Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs.","Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning.","State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data.","However, adapting SSMs to non-sequential graph data presents a notable challenge.","In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism.","Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantial improvement in predictive performance.","Extensive experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms state-of-the-art methods in long-range graph prediction tasks, with a fraction of the computational cost in both FLOPs and GPU memory consumption.","The code and models are publicly available at https://github.com/bowang-lab/Graph-Mamba."],"url":"http://arxiv.org/abs/2402.00789v1","category":"cs.LG"}
{"created":"2024-02-01 17:21:45","title":"Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-Agent Reinforcement Learning","abstract":"Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis. However, a critical concern is the manual definition of behavioural rules in ABMs. Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification. This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents. However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled. To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework. The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation. Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods. To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels. Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours. We validate our model's effectiveness using real-world data on a range of canonical $n$-agent settings, demonstrating significantly improved predictive capability.","sentences":["Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis.","However, a critical concern is the manual definition of behavioural rules in ABMs.","Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification.","This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents.","However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled.","To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework.","The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation.","Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods.","To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels.","Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours.","We validate our model's effectiveness using real-world data on a range of canonical $n$-agent settings, demonstrating significantly improved predictive capability."],"url":"http://arxiv.org/abs/2402.00787v1","category":"cs.MA"}
{"created":"2024-02-01 17:17:55","title":"CroissantLLM: A Truly Bilingual French-English Language Model","abstract":"We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, and training steps, as well as fine-tuned Chat models, and strong translation models. We evaluate our model through the FMTI framework, and validate 81 % of the transparency criteria, far beyond the scores of even most open initiatives. This work enriches the NLP landscape, breaking away from previous English-centric work in order to strengthen our understanding of multilinguality in language models.","sentences":["We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware.","To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets.","We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources.","To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language.","Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, and training steps, as well as fine-tuned Chat models, and strong translation models.","We evaluate our model through the FMTI framework, and validate 81 % of the transparency criteria, far beyond the scores of even most open initiatives.","This work enriches the NLP landscape, breaking away from previous English-centric work in order to strengthen our understanding of multilinguality in language models."],"url":"http://arxiv.org/abs/2402.00786v1","category":"cs.CL"}
{"created":"2024-02-01 17:13:12","title":"Apparent Dark Matter Inspired by Einstein Equation of State","abstract":"The purpose of this article is twofold. First, by means of Padmanabhan's proposal on the emergence nature of gravity, we recover the $\\Lambda$CDM model and the effect of the dark matter in the context of cosmology. Toward this goal, we use the key idea of Padmanabhan that states cosmic space emerges as the cosmic time progress and link the emergence of space to the difference between the number of degrees of freedom on the boundary and in the bulk. Interestingly enough, we show that the effect of the cold dark matter in the cosmological setup can be understood by assuming an interaction between the numbers of degrees of freedom in the bulk. In the second part, we follow the Jacobson's argument and obtain the modified Einstein field equations with additional dark matter component emerging due to the interaction term between dark energy and baryonic matter related by $\\Omega_{DM,0}=\\sqrt{2 \\alpha \\Omega_{M,0} \\Omega_{DE,0}}$, where $\\alpha$ is a coupling constant. Finally, a correspondence with Yukawa cosmology is pointed out, and the role of massive gravitons as a possibility in explaining the nature of the dark sector as well as the theoretical origin of the Modified Newtonian Dynamics (MOND) are addressed. We speculate that the interaction coupling $\\alpha$ fundamentally measures the entanglement between the gravitons and matter fields and there exists a fundamental limitation in measuring the gravitons wavelength.","sentences":["The purpose of this article is twofold.","First, by means of Padmanabhan's proposal on the emergence nature of gravity, we recover the $\\Lambda$CDM model and the effect of the dark matter in the context of cosmology.","Toward this goal, we use the key idea of Padmanabhan that states cosmic space emerges as the cosmic time progress and link the emergence of space to the difference between the number of degrees of freedom on the boundary and in the bulk.","Interestingly enough, we show that the effect of the cold dark matter in the cosmological setup can be understood by assuming an interaction between the numbers of degrees of freedom in the bulk.","In the second part, we follow the Jacobson's argument and obtain the modified Einstein field equations with additional dark matter component emerging due to the interaction term between dark energy and baryonic matter related by $\\Omega_{DM,0}=\\sqrt{2 \\alpha \\Omega_{M,0} \\Omega_{DE,0}}$, where $\\alpha$ is a coupling constant.","Finally, a correspondence with Yukawa cosmology is pointed out, and the role of massive gravitons as a possibility in explaining the nature of the dark sector as well as the theoretical origin of the Modified Newtonian Dynamics (MOND) are addressed.","We speculate that the interaction coupling $\\alpha$ fundamentally measures the entanglement between the gravitons and matter fields and there exists a fundamental limitation in measuring the gravitons wavelength."],"url":"http://arxiv.org/abs/2402.00785v1","category":"gr-qc"}
{"created":"2024-02-01 17:10:35","title":"Dense Reward for Free in Reinforcement Learning from Human Feedback","abstract":"Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many \"actions\" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling. We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima.","sentences":["Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance.","Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion.","As an auto-regressive process, the LLM has to take many \"actions\" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning.","In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture.","We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling.","We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged.","Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima."],"url":"http://arxiv.org/abs/2402.00782v1","category":"cs.LG"}
{"created":"2024-02-01 17:05:37","title":"Hybrid Quantum Vision Transformers for Event Classification in High Energy Physics","abstract":"Models based on vision transformer architectures are considered state-of-the-art when it comes to image classification tasks. However, they require extensive computational resources both for training and deployment. The problem is exacerbated as the amount and complexity of the data increases. Quantum-based vision transformer models could potentially alleviate this issue by reducing the training and operating time while maintaining the same predictive power. Although current quantum computers are not yet able to perform high-dimensional tasks yet, they do offer one of the most efficient solutions for the future. In this work, we construct several variations of a quantum hybrid vision transformer for a classification problem in high energy physics (distinguishing photons and electrons in the electromagnetic calorimeter). We test them against classical vision transformer architectures. Our findings indicate that the hybrid models can achieve comparable performance to their classical analogues with a similar number of parameters.","sentences":["Models based on vision transformer architectures are considered state-of-the-art when it comes to image classification tasks.","However, they require extensive computational resources both for training and deployment.","The problem is exacerbated as the amount and complexity of the data increases.","Quantum-based vision transformer models could potentially alleviate this issue by reducing the training and operating time while maintaining the same predictive power.","Although current quantum computers are not yet able to perform high-dimensional tasks yet, they do offer one of the most efficient solutions for the future.","In this work, we construct several variations of a quantum hybrid vision transformer for a classification problem in high energy physics (distinguishing photons and electrons in the electromagnetic calorimeter).","We test them against classical vision transformer architectures.","Our findings indicate that the hybrid models can achieve comparable performance to their classical analogues with a similar number of parameters."],"url":"http://arxiv.org/abs/2402.00776v1","category":"quant-ph"}
{"created":"2024-02-01 17:04:41","title":"Adaptive Control for Triadic Human-Robot-FES Collaboration in Gait Rehabilitation: A Pilot Study","abstract":"The hybridisation of robot-assisted gait training and functional electrical stimulation (FES) can provide numerous physiological benefits to neurological patients. However, the design of an effective hybrid controller poses significant challenges. In this over-actuated system, it is extremely difficult to find the right balance between robotic assistance and FES that will provide personalised assistance, prevent muscle fatigue and encourage the patient's active participation in order to accelerate recovery. In this paper, we present an adaptive hybrid robot-FES controller to do this and enable the triadic collaboration between the patient, the robot and FES. A patient-driven controller is designed where the voluntary movement of the patient is prioritised and assistance is provided using FES and the robot in a hierarchical order depending on the patient's performance and their muscles' fitness. The performance of this hybrid adaptive controller is tested in simulation and on one healthy subject. Our results indicate an increase in tracking performance with lower overall assistance, and less muscle fatigue when the hybrid adaptive controller is used, compared to its non adaptive equivalent. This suggests that our hybrid adaptive controller may be able to adapt to the behaviour of the user to provide assistance as needed and prevent the early termination of physical therapy due to muscle fatigue.","sentences":["The hybridisation of robot-assisted gait training and functional electrical stimulation (FES) can provide numerous physiological benefits to neurological patients.","However, the design of an effective hybrid controller poses significant challenges.","In this over-actuated system, it is extremely difficult to find the right balance between robotic assistance and FES that will provide personalised assistance, prevent muscle fatigue and encourage the patient's active participation in order to accelerate recovery.","In this paper, we present an adaptive hybrid robot-FES controller to do this and enable the triadic collaboration between the patient, the robot and FES.","A patient-driven controller is designed where the voluntary movement of the patient is prioritised and assistance is provided using FES and the robot in a hierarchical order depending on the patient's performance and their muscles' fitness.","The performance of this hybrid adaptive controller is tested in simulation and on one healthy subject.","Our results indicate an increase in tracking performance with lower overall assistance, and less muscle fatigue when the hybrid adaptive controller is used, compared to its non adaptive equivalent.","This suggests that our hybrid adaptive controller may be able to adapt to the behaviour of the user to provide assistance as needed and prevent the early termination of physical therapy due to muscle fatigue."],"url":"http://arxiv.org/abs/2402.00775v1","category":"cs.RO"}
{"created":"2024-02-01 17:04:04","title":"Mesh motion in fluid-structure interaction with deep operator networks","abstract":"A mesh motion model based on deep operator networks is presented. The model is trained on and evaluated against a biharmonic mesh motion model on a fluid-structure interaction benchmark problem and further evaluated in a setting where biharmonic mesh motion fails. The performance of the proposed mesh motion model is comparable to the biharmonic mesh motion on the test problems.","sentences":["A mesh motion model based on deep operator networks is presented.","The model is trained on and evaluated against a biharmonic mesh motion model on a fluid-structure interaction benchmark problem and further evaluated in a setting where biharmonic mesh motion fails.","The performance of the proposed mesh motion model is comparable to the biharmonic mesh motion on the test problems."],"url":"http://arxiv.org/abs/2402.00774v1","category":"math.NA"}
{"created":"2024-02-01 16:59:59","title":"Mixed Static and Reconfigurable Metasurface Deployment in Indoor Dense Spaces: How Much Reconfigurability is Needed?","abstract":"In this paper, we investigate how metasurfaces can be deployed to deliver high data rates in a millimeter-wave (mmWave) indoor dense space with many blocking objects. These surfaces can either be static metasurfaces (SMSs) that reflect with fixed phase-shifts or reconfigurable intelligent surfaces (RISs) that can reconfigure their phase-shifts to the currently served user. The latter comes with an increased power, cabling, and signaling cost. To see how reconfigurability affects the network performance, we propose an iterative algorithm based on the feasible point pursuit successive convex approximation method. We jointly optimize the types and phase-shifts of the surfaces and the time portion allocated to each user equipment to maximize the minimum data rate achieved by the network. Our numerical results demonstrate that the minimum data rate improves as more RISs are introduced but the gain diminishes after some point. Therefore, introducing more reconfigurability is not always necessary. Another result shows that to reach the same data rate achieved by using 22 SMSs, at least 18 RISs are needed. This suggests that when it is costly to deploy many RISs, as an inexpensive alternative solution, one can reach the same data rate just by densely deploying more SMSs.","sentences":["In this paper, we investigate how metasurfaces can be deployed to deliver high data rates in a millimeter-wave (mmWave) indoor dense space with many blocking objects.","These surfaces can either be static metasurfaces (SMSs) that reflect with fixed phase-shifts or reconfigurable intelligent surfaces (RISs) that can reconfigure their phase-shifts to the currently served user.","The latter comes with an increased power, cabling, and signaling cost.","To see how reconfigurability affects the network performance, we propose an iterative algorithm based on the feasible point pursuit successive convex approximation method.","We jointly optimize the types and phase-shifts of the surfaces and the time portion allocated to each user equipment to maximize the minimum data rate achieved by the network.","Our numerical results demonstrate that the minimum data rate improves as more RISs are introduced but the gain diminishes after some point.","Therefore, introducing more reconfigurability is not always necessary.","Another result shows that to reach the same data rate achieved by using 22 SMSs, at least 18 RISs are needed.","This suggests that when it is costly to deploy many RISs, as an inexpensive alternative solution, one can reach the same data rate just by densely deploying more SMSs."],"url":"http://arxiv.org/abs/2402.00771v1","category":"cs.IT"}
{"created":"2024-02-01 16:58:11","title":"AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning","abstract":"Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation). we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed. We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results. Experimental results validate the effectiveness of our proposed method. Code and weights will be made public. More details are available at https://github.com/G-U-N/AnimateLCM.","sentences":["Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity.","However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications.","Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps.","Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality.","Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation).","we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed.","We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results.","Experimental results validate the effectiveness of our proposed method.","Code and weights will be made public.","More details are available at https://github.com/G-U-N/AnimateLCM."],"url":"http://arxiv.org/abs/2402.00769v1","category":"cs.CV"}
{"created":"2024-02-01 16:55:17","title":"Loop soup representation of zeta-regularised determinants and equivariant Symanzik identities","abstract":"We derive a stochastic representation for determinants of Laplace-type operators on vectors bundles over manifolds. Namely, inverse powers of those determinants are written as the expectation of a product of holonomies defined over Brownian loop soups. Our results hold over compact manifolds of dimension 2 or 3, in the presence of mass or a boundary. We derive a few consequences, including some regularity as a function of the operator and the conformal invariance of the zeta function on surfaces.   Our second main result is the rigorous construction of a stochastic gauge theory minimally coupling a scalar field to a prescribed random smooth gauge field, which we prove obeys the so-called Symanzik identities. Some of these results are continuous analogues of the work of A. Kassel and T. L\\'evy in the discrete.","sentences":["We derive a stochastic representation for determinants of Laplace-type operators on vectors bundles over manifolds.","Namely, inverse powers of those determinants are written as the expectation of a product of holonomies defined over Brownian loop soups.","Our results hold over compact manifolds of dimension 2 or 3, in the presence of mass or a boundary.","We derive a few consequences, including some regularity as a function of the operator and the conformal invariance of the zeta function on surfaces.   ","Our second main result is the rigorous construction of a stochastic gauge theory minimally coupling a scalar field to a prescribed random smooth gauge field, which we prove obeys the so-called Symanzik identities.","Some of these results are continuous analogues of the work of A. Kassel and T. L\\'evy in the discrete."],"url":"http://arxiv.org/abs/2402.00767v1","category":"math.PR"}
{"created":"2024-02-01 16:55:07","title":"Benchmarking Multipartite Entanglement Generation with Graph States","abstract":"As quantum computing technology slowly matures and the number of available qubits on a QPU gradually increases, interest in assessing the capabilities of quantum computing hardware in a scalable manner is growing. One of the key properties for quantum computing is the ability to generate multipartite entangled states. In this paper, aspects of benchmarking entanglement generation capabilities of noisy intermediate-scale quantum (NISQ) devices are discussed based on the preparation of graph states and the verification of entanglement in the prepared states. Thereby, we use entanglement witnesses that are specifically suited for a scalable experiment design. This choice of entanglement witnesses can detect A) bipartite entanglement and B) genuine multipartite entanglement for graph states with constant two measurement settings if the prepared graph state is based on a 2-colorable graph, e.g., a square grid graph or one of its subgraphs. With this, we experimentally verify that a fully bipartite entangled state can be prepared on a 127-qubit IBM Quantum superconducting QPU, and genuine multipartite entanglement can be detected for states of up to 23 qubits with quantum readout error mitigation.","sentences":["As quantum computing technology slowly matures and the number of available qubits on a QPU gradually increases, interest in assessing the capabilities of quantum computing hardware in a scalable manner is growing.","One of the key properties for quantum computing is the ability to generate multipartite entangled states.","In this paper, aspects of benchmarking entanglement generation capabilities of noisy intermediate-scale quantum (NISQ) devices are discussed based on the preparation of graph states and the verification of entanglement in the prepared states.","Thereby, we use entanglement witnesses that are specifically suited for a scalable experiment design.","This choice of entanglement witnesses can detect A) bipartite entanglement and B) genuine multipartite entanglement for graph states with constant two measurement settings if the prepared graph state is based on a 2-colorable graph, e.g., a square grid graph or one of its subgraphs.","With this, we experimentally verify that a fully bipartite entangled state can be prepared on a 127-qubit IBM Quantum superconducting QPU, and genuine multipartite entanglement can be detected for states of up to 23 qubits with quantum readout error mitigation."],"url":"http://arxiv.org/abs/2402.00766v1","category":"quant-ph"}
{"created":"2024-02-01 16:53:15","title":"To Search or To Gen? Exploring the Synergy between Generative AI and Web Search in Programming","abstract":"The convergence of generative AI and web search is reshaping problem-solving for programmers. However, the lack of understanding regarding their interplay in the information-seeking process often leads programmers to perceive them as alternatives rather than complementary tools. To analyze this interaction and explore their synergy, we conducted an interview study with eight experienced programmers. Drawing from the results and literature, we have identified three major challenges and proposed three decision-making stages, each with its own relevant factors. Additionally, we present a comprehensive process model that captures programmers' interaction patterns. This model encompasses decision-making stages, the information-foraging loop, and cognitive activities during system interaction, offering a holistic framework to comprehend and optimize the use of these convergent tools in programming.","sentences":["The convergence of generative AI and web search is reshaping problem-solving for programmers.","However, the lack of understanding regarding their interplay in the information-seeking process often leads programmers to perceive them as alternatives rather than complementary tools.","To analyze this interaction and explore their synergy, we conducted an interview study with eight experienced programmers.","Drawing from the results and literature, we have identified three major challenges and proposed three decision-making stages, each with its own relevant factors.","Additionally, we present a comprehensive process model that captures programmers' interaction patterns.","This model encompasses decision-making stages, the information-foraging loop, and cognitive activities during system interaction, offering a holistic framework to comprehend and optimize the use of these convergent tools in programming."],"url":"http://arxiv.org/abs/2402.00764v1","category":"cs.HC"}
{"created":"2024-02-01 16:52:21","title":"360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming","abstract":"3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\\circ}$ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel $360^{\\circ}$ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios.","sentences":["3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings.","This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians.","However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\\circ}$ images using 2D Gaussians.","In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality.","In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views.","To address these issues, we propose 360-GS, a novel $360^{\\circ}$ Gaussian splatting for a limited set of panoramic inputs.","Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections.","This adaptation enables the representation of the projection using Gaussians.","We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene.","Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios."],"url":"http://arxiv.org/abs/2402.00763v1","category":"cs.CV"}
{"created":"2024-02-01 16:51:11","title":"Control-Theoretic Techniques for Online Adaptation of Deep Neural Networks in Dynamical Systems","abstract":"Deep neural networks (DNNs), trained with gradient-based optimization and backpropagation, are currently the primary tool in modern artificial intelligence, machine learning, and data science. In many applications, DNNs are trained offline, through supervised learning or reinforcement learning, and deployed online for inference. However, training DNNs with standard backpropagation and gradient-based optimization gives no intrinsic performance guarantees or bounds on the DNN, which is essential for applications such as controls. Additionally, many offline-training and online-inference problems, such as sim2real transfer of reinforcement learning policies, experience domain shift from the training distribution to the real-world distribution. To address these stability and transfer learning issues, we propose using techniques from control theory to update DNN parameters online. We formulate the fully-connected feedforward DNN as a continuous-time dynamical system, and we propose novel last-layer update laws that guarantee desirable error convergence under various conditions on the time derivative of the DNN input vector. We further show that training the DNN under spectral normalization controls the upper bound of the error trajectories of the online DNN predictions, which is desirable when numerically differentiated quantities or noisy state measurements are input to the DNN. The proposed online DNN adaptation laws are validated in simulation to learn the dynamics of the Van der Pol system under domain shift, where parameters are varied in inference from the training dataset. The simulations demonstrate the effectiveness of using control-theoretic techniques to derive performance improvements and guarantees in DNN-based learning systems.","sentences":["Deep neural networks (DNNs), trained with gradient-based optimization and backpropagation, are currently the primary tool in modern artificial intelligence, machine learning, and data science.","In many applications, DNNs are trained offline, through supervised learning or reinforcement learning, and deployed online for inference.","However, training DNNs with standard backpropagation and gradient-based optimization gives no intrinsic performance guarantees or bounds on the DNN, which is essential for applications such as controls.","Additionally, many offline-training and online-inference problems, such as sim2real transfer of reinforcement learning policies, experience domain shift from the training distribution to the real-world distribution.","To address these stability and transfer learning issues, we propose using techniques from control theory to update DNN parameters online.","We formulate the fully-connected feedforward DNN as a continuous-time dynamical system, and we propose novel last-layer update laws that guarantee desirable error convergence under various conditions on the time derivative of the DNN input vector.","We further show that training the DNN under spectral normalization controls the upper bound of the error trajectories of the online DNN predictions, which is desirable when numerically differentiated quantities or noisy state measurements are input to the DNN.","The proposed online DNN adaptation laws are validated in simulation to learn the dynamics of the Van der Pol system under domain shift, where parameters are varied in inference from the training dataset.","The simulations demonstrate the effectiveness of using control-theoretic techniques to derive performance improvements and guarantees in DNN-based learning systems."],"url":"http://arxiv.org/abs/2402.00761v1","category":"cs.LG"}
{"created":"2024-02-01 16:50:41","title":"EuroPED-NN: Uncertainty aware surrogate model","abstract":"This work successfully generates uncertainty aware surrogate models, via the Bayesian neural network with noise contrastive prior (BNN-NCP) technique, of the EuroPED plasma pedestal model using data from the JET-ILW pedestal database and subsequent model evaluations. All this conform EuroPED-NN. The BNN-NCP technique is proven to be a good fit for uncertainty aware surrogate models, matching the output results as a regular neural network, providing prediction's confidence as uncertainties, and highlighting the out of distribution (OOD) regions using surrogate model uncertainties. This provides critical insights into model robustness and reliability. EuroPED-NN has been physically validated, first, analyzing electron density $n_e\\!\\left(\\psi_{\\text{pol}}=0.94\\right)$ with respect to increasing plasma current, $I_p$, and second, validating the $\\Delta-\\beta_{p,ped}$ relation associated with the EuroPED model. Affirming the robustness of the underlying physics learned by the surrogate model.","sentences":["This work successfully generates uncertainty aware surrogate models, via the Bayesian neural network with noise contrastive prior (BNN-NCP) technique, of the EuroPED plasma pedestal model using data from the JET-ILW pedestal database and subsequent model evaluations.","All this conform EuroPED-NN.","The BNN-NCP technique is proven to be a good fit for uncertainty aware surrogate models, matching the output results as a regular neural network, providing prediction's confidence as uncertainties, and highlighting the out of distribution (OOD) regions using surrogate model uncertainties.","This provides critical insights into model robustness and reliability.","EuroPED-NN has been physically validated, first, analyzing electron density $n_e\\!\\left(\\psi_{\\text{pol}}=0.94\\right)$ with respect to increasing plasma current, $I_p$, and second, validating the $\\Delta-\\beta_{p,ped}$ relation associated with the EuroPED model.","Affirming the robustness of the underlying physics learned by the surrogate model."],"url":"http://arxiv.org/abs/2402.00760v1","category":"physics.plasm-ph"}
{"created":"2024-02-01 16:49:27","title":"Building Expressive and Tractable Probabilistic Generative Models: A Review","abstract":"We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.","sentences":["We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs).","We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field.","We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field."],"url":"http://arxiv.org/abs/2402.00759v1","category":"cs.LG"}
{"created":"2024-02-01 16:43:58","title":"GS++: Error Analyzing and Optimal Gaussian Splatting","abstract":"3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering. Concurrently, concerns have been raised about the limitations of this technology in aspects such as point cloud storage, performance , and robustness in sparse viewpoints , leading to various improvements. However, there has been a notable lack of attention to the projection errors introduced by the local affine approximation inherent in the splatting itself, and the consequential impact of these errors on the quality of photo-realistic rendering. This paper addresses the projection error function of 3D Gaussian Splatting, commencing with the residual error from the first-order Taylor expansion of the projection function $\\phi$. The analysis establishes a correlation between the error and the Gaussian mean position. Subsequently, leveraging function optimization theory, this paper analyzes the function's minima to provide an optimal projection strategy for Gaussian Splatting referred to Optimal Gaussian Splatting. Experimental validation further confirms that this projection methodology reduces artifacts, resulting in a more convincingly realistic rendering.","sentences":["3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering.","Concurrently, concerns have been raised about the limitations of this technology in aspects such as point cloud storage, performance , and robustness in sparse viewpoints , leading to various improvements.","However, there has been a notable lack of attention to the projection errors introduced by the local affine approximation inherent in the splatting itself, and the consequential impact of these errors on the quality of photo-realistic rendering.","This paper addresses the projection error function of 3D Gaussian Splatting, commencing with the residual error from the first-order Taylor expansion of the projection function $\\phi$. The analysis establishes a correlation between the error and the Gaussian mean position.","Subsequently, leveraging function optimization theory, this paper analyzes the function's minima to provide an optimal projection strategy for Gaussian Splatting referred to Optimal Gaussian Splatting.","Experimental validation further confirms that this projection methodology reduces artifacts, resulting in a more convincingly realistic rendering."],"url":"http://arxiv.org/abs/2402.00752v1","category":"cs.CV"}
{"created":"2024-02-01 16:43:04","title":"Unlearnable Algorithms for In-context Learning","abstract":"Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests.","sentences":["Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance.","However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining.","In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM).","We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data.","We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets.","We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches.","This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests."],"url":"http://arxiv.org/abs/2402.00751v1","category":"cs.LG"}
{"created":"2024-02-01 16:40:32","title":"Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model","abstract":"Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease prediction. We have conducted disease prediction experiments on a large number of health reports to assess the effectiveness of Health-LLM. The results of the experiments indicate that the proposed method surpasses traditional methods and has the potential to revolutionize disease prediction and personalized health management. The code is available at https://github.com/jmyissb/HealthLLM.","sentences":["Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment.","However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges.","Hence, a more professional and detailed intelligent healthcare method is needed for development.","To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring.","Compared to traditional health management methods, our approach has three main advantages.","First, our method integrates health reports into a large model to provide detailed task information.","Second, professional medical expertise is used to adjust the weighted scores of health characteristics.","Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease prediction.","We have conducted disease prediction experiments on a large number of health reports to assess the effectiveness of Health-LLM.","The results of the experiments indicate that the proposed method surpasses traditional methods and has the potential to revolutionize disease prediction and personalized health management.","The code is available at https://github.com/jmyissb/HealthLLM."],"url":"http://arxiv.org/abs/2402.00746v1","category":"cs.CL"}
{"created":"2024-02-01 16:39:51","title":"Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement","abstract":"An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models' reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs.","sentences":["An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities.","Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains.","In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs.","Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy.","An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models' reasoning.","As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs."],"url":"http://arxiv.org/abs/2402.00745v1","category":"cs.CL"}
{"created":"2024-02-01 16:39:47","title":"BATON: Aligning Text-to-Audio Model with Human Preference Feedback","abstract":"With the development of AI-Generated Content (AIGC), text-to-audio models are gaining widespread attention. However, it is challenging for these models to generate audio aligned with human preference due to the inherent information density of natural language and limited model understanding ability. To alleviate this issue, we formulate the BATON, a framework designed to enhance the alignment between generated audio and text prompt using human preference feedback. Our BATON comprises three key stages: Firstly, we curated a dataset containing both prompts and the corresponding generated audio, which was then annotated based on human feedback. Secondly, we introduced a reward model using the constructed dataset, which can mimic human preference by assigning rewards to input text-audio pairs. Finally, we employed the reward model to fine-tune an off-the-shelf text-to-audio model. The experiment results demonstrate that our BATON can significantly improve the generation quality of the original text-to-audio models, concerning audio integrity, temporal relationship, and alignment with human preference.","sentences":["With the development of AI-Generated Content (AIGC), text-to-audio models are gaining widespread attention.","However, it is challenging for these models to generate audio aligned with human preference due to the inherent information density of natural language and limited model understanding ability.","To alleviate this issue, we formulate the BATON, a framework designed to enhance the alignment between generated audio and text prompt using human preference feedback.","Our BATON comprises three key stages: Firstly, we curated a dataset containing both prompts and the corresponding generated audio, which was then annotated based on human feedback.","Secondly, we introduced a reward model using the constructed dataset, which can mimic human preference by assigning rewards to input text-audio pairs.","Finally, we employed the reward model to fine-tune an off-the-shelf text-to-audio model.","The experiment results demonstrate that our BATON can significantly improve the generation quality of the original text-to-audio models, concerning audio integrity, temporal relationship, and alignment with human preference."],"url":"http://arxiv.org/abs/2402.00744v1","category":"cs.SD"}
{"created":"2024-02-01 16:39:45","title":"Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data","abstract":"In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \\citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \\cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token next to $x_i$ for each example; (2) positional encoding can further improve the performance; and (3) multi-head attention with a high input embedding dimension has a better prediction performance than single-head attention.","sentences":["In practice, it is observed that transformer-based models can learn concepts in context in the inference stage.","While existing literature, e.g., \\citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data).","However, in reality, they are presented in two tokens (i.e., unstructured data \\cite{wibisono2023role}).","In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data.","We study the exact components in a transformer that facilitate the in-context learning.","In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token next to $x_i$ for each example; (2) positional encoding can further improve the performance; and (3) multi-head attention with a high input embedding dimension has a better prediction performance than single-head attention."],"url":"http://arxiv.org/abs/2402.00743v1","category":"cs.LG"}
{"created":"2024-02-01 16:39:28","title":"Transforming and Combining Rewards for Aligning Large Language Models","abstract":"A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model). Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise. Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach.","sentences":["A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model.","We study two closely related problems that arise in this approach.","First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others?","Second, we often wish to align language models to multiple properties: how should we combine multiple reward models?","Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models.","This derived transformation has two important properties.","First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well.","This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model).","Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise.","Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach."],"url":"http://arxiv.org/abs/2402.00742v1","category":"cs.CL"}
{"created":"2024-02-01 16:38:51","title":"DRSM: efficient neural 4d decomposition for dynamic reconstruction in stationary monocular cameras","abstract":"With the popularity of monocular videos generated by video sharing and live broadcasting applications, reconstructing and editing dynamic scenes in stationary monocular cameras has become a special but anticipated technology. In contrast to scene reconstructions that exploit multi-view observations, the problem of modeling a dynamic scene from a single view is significantly more under-constrained and ill-posed. Inspired by recent progress in neural rendering, we present a novel framework to tackle 4D decomposition problem for dynamic scenes in monocular cameras. Our framework utilizes decomposed static and dynamic feature planes to represent 4D scenes and emphasizes the learning of dynamic regions through dense ray casting. Inadequate 3D clues from a single-view and occlusion are also particular challenges in scene reconstruction. To overcome these difficulties, we propose deep supervised optimization and ray casting strategies. With experiments on various videos, our method generates higher-fidelity results than existing methods for single-view dynamic scene representation.","sentences":["With the popularity of monocular videos generated by video sharing and live broadcasting applications, reconstructing and editing dynamic scenes in stationary monocular cameras has become a special but anticipated technology.","In contrast to scene reconstructions that exploit multi-view observations, the problem of modeling a dynamic scene from a single view is significantly more under-constrained and ill-posed.","Inspired by recent progress in neural rendering, we present a novel framework to tackle 4D decomposition problem for dynamic scenes in monocular cameras.","Our framework utilizes decomposed static and dynamic feature planes to represent 4D scenes and emphasizes the learning of dynamic regions through dense ray casting.","Inadequate 3D clues from a single-view and occlusion are also particular challenges in scene reconstruction.","To overcome these difficulties, we propose deep supervised optimization and ray casting strategies.","With experiments on various videos, our method generates higher-fidelity results than existing methods for single-view dynamic scene representation."],"url":"http://arxiv.org/abs/2402.00740v1","category":"cs.CV"}
{"created":"2024-02-01 16:37:21","title":"FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum Markov Game","abstract":"Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years. However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability. In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs. Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with neural networks is proposed to implement FM3Q and obtain the deterministic and decentralized minimax policies for two-team players. A theoretical analysis is provided to prove the convergence of FM3Q. Empirically, we use three environments to evaluate the learning efficiency and final performance of FM3Q and show its superiority on 2t0sMGs.","sentences":["Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team.","The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years.","However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability.","In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs.","Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs.","Moreover, an online learning algorithm with neural networks is proposed to implement FM3Q and obtain the deterministic and decentralized minimax policies for two-team players.","A theoretical analysis is provided to prove the convergence of FM3Q. Empirically, we use three environments to evaluate the learning efficiency and final performance of FM3Q and show its superiority on 2t0sMGs."],"url":"http://arxiv.org/abs/2402.00738v1","category":"cs.AI"}
{"created":"2024-02-01 16:33:06","title":"BIOMERO: BioImage analysis in OMERO","abstract":"In the rapidly evolving field of bioimaging, the integration and orchestration of Findable, Accessible, Interoperable, and Reusable (FAIR) image analysis workflows remains a challenge. We introduce BIOMERO, a bridge connecting OMERO, a renowned bioimaging data management platform, FAIR workflows and high-performance computing (HPC) environments. BIOMERO, featuring our opensource Python library \"OMERO Slurm Client\", facilitates seamless execution of FAIR workflows, particularly for large datasets from High Content or High Throughput Screening. BIOMERO empowers researchers by eliminating the need for specialized knowledge, enabling scalable image processing directly from OMERO. BIOMERO notably supports the sharing and utilization of FAIR workflows between OMERO, Cytomine/BIAFLOWS, and other bioimaging communities. BIOMERO will promote the widespread adoption of FAIR workflows, emphasizing reusability, across the realm of bioimaging research. Its user-friendly interface will empower users, including those without technical expertise, to seamlessly apply these workflows to their datasets, democratizing the utilization of AI by the broader research community.","sentences":["In the rapidly evolving field of bioimaging, the integration and orchestration of Findable, Accessible, Interoperable, and Reusable (FAIR) image analysis workflows remains a challenge.","We introduce BIOMERO, a bridge connecting OMERO, a renowned bioimaging data management platform, FAIR workflows and high-performance computing (HPC) environments.","BIOMERO, featuring our opensource Python library \"OMERO Slurm Client\", facilitates seamless execution of FAIR workflows, particularly for large datasets from High Content or High Throughput Screening.","BIOMERO empowers researchers by eliminating the need for specialized knowledge, enabling scalable image processing directly from OMERO.","BIOMERO notably supports the sharing and utilization of FAIR workflows between OMERO, Cytomine/BIAFLOWS, and other bioimaging communities.","BIOMERO will promote the widespread adoption of FAIR workflows, emphasizing reusability, across the realm of bioimaging research.","Its user-friendly interface will empower users, including those without technical expertise, to seamlessly apply these workflows to their datasets, democratizing the utilization of AI by the broader research community."],"url":"http://arxiv.org/abs/2402.00734v1","category":"cs.SE"}
{"created":"2024-02-01 16:30:00","title":"MobilityDL: A Review of Deep Learning From Trajectory Data","abstract":"Trajectory data combines the complexities of time series, spatial data, and (sometimes irrational) movement behavior. As data availability and computing power have increased, so has the popularity of deep learning from trajectory data. This review paper provides the first comprehensive overview of deep learning approaches for trajectory data. We have identified eight specific mobility use cases which we analyze with regards to the deep learning models and the training data used. Besides a comprehensive quantitative review of the literature since 2018, the main contribution of our work is the data-centric analysis of recent work in this field, placing it along the mobility data continuum which ranges from detailed dense trajectories of individual movers (quasi-continuous tracking data), to sparse trajectories (such as check-in data), and aggregated trajectories (crowd information).","sentences":["Trajectory data combines the complexities of time series, spatial data, and (sometimes irrational) movement behavior.","As data availability and computing power have increased, so has the popularity of deep learning from trajectory data.","This review paper provides the first comprehensive overview of deep learning approaches for trajectory data.","We have identified eight specific mobility use cases which we analyze with regards to the deep learning models and the training data used.","Besides a comprehensive quantitative review of the literature since 2018, the main contribution of our work is the data-centric analysis of recent work in this field, placing it along the mobility data continuum which ranges from detailed dense trajectories of individual movers (quasi-continuous tracking data), to sparse trajectories (such as check-in data), and aggregated trajectories (crowd information)."],"url":"http://arxiv.org/abs/2402.00732v1","category":"cs.LG"}
{"created":"2024-02-01 16:29:07","title":"Resource-efficient and loss-aware photonic graph state preparation using an array of quantum emitters, and application to all-photonic quantum repeaters","abstract":"Multi-qubit photonic graph states are necessary for quantum communication and computation. Preparing photonic graph states using probabilistic stitching of single photons using linear optics results in a formidable resource requirement due to the need of multiplexing. Quantum emitters present a viable solution to prepare photonic graph states, as they enable controlled production of photons entangled with the emitter qubit, and deterministic two-qubit interactions among emitters. A handful of emitters often suffice to generate useful photonic graph states that would otherwise require millions of single photon sources using the linear-optics method. But, photon loss poses an impediment to this method due to the large depth, i.e., age of the oldest photon, of the graph state, given the typically large number of slow and noisy two-qubit CNOT gates required on emitters. We propose an algorithm that can trade the number of emitters with the graph-state depth, while minimizing the number of emitter CNOTs. We apply our algorithm to generating a repeater graph state (RGS) for all-photonic repeaters. We find that our scheme achieves a far superior rate-vs.-distance performance than using the least number of emitters needed to generate the RGS. Yet, our scheme is able to get the same performance as the linear-optics method of generating the RGS where each emitter is used as a single-photon source, but with orders of magnitude fewer emitters.","sentences":["Multi-qubit photonic graph states are necessary for quantum communication and computation.","Preparing photonic graph states using probabilistic stitching of single photons using linear optics results in a formidable resource requirement due to the need of multiplexing.","Quantum emitters present a viable solution to prepare photonic graph states, as they enable controlled production of photons entangled with the emitter qubit, and deterministic two-qubit interactions among emitters.","A handful of emitters often suffice to generate useful photonic graph states that would otherwise require millions of single photon sources using the linear-optics method.","But, photon loss poses an impediment to this method due to the large depth, i.e., age of the oldest photon, of the graph state, given the typically large number of slow and noisy two-qubit CNOT gates required on emitters.","We propose an algorithm that can trade the number of emitters with the graph-state depth, while minimizing the number of emitter CNOTs.","We apply our algorithm to generating a repeater graph state (RGS) for all-photonic repeaters.","We find that our scheme achieves a far superior rate-vs.-distance performance than using the least number of emitters needed to generate the RGS.","Yet, our scheme is able to get the same performance as the linear-optics method of generating the RGS where each emitter is used as a single-photon source, but with orders of magnitude fewer emitters."],"url":"http://arxiv.org/abs/2402.00731v1","category":"quant-ph"}
{"created":"2024-02-01 16:26:30","title":"Profiling and Modeling of Power Characteristics of Leadership-Scale HPC System Workloads","abstract":"In the exascale era in which application behavior has large power & energy footprints, per-application job-level awareness of such impression is crucial in taking steps towards achieving efficiency goals beyond performance, such as energy efficiency, and sustainability.   To achieve these goals, we have developed a novel low-latency job power profiling machine learning pipeline that can group job-level power profiles based on their shapes as they complete. This pipeline leverages a comprehensive feature extraction and clustering pipeline powered by a generative adversarial network (GAN) model to handle the feature-rich time series of job-level power measurements. The output is then used to train a classification model that can predict whether an incoming job power profile is similar to a known group of profiles or is completely new. With extensive evaluations, we demonstrate the effectiveness of each component in our pipeline. Also, we provide a preliminary analysis of the resulting clusters that depict the power profile landscape of the Summit supercomputer from more than 60K jobs sampled from the year 2021.","sentences":["In the exascale era in which application behavior has large power & energy footprints, per-application job-level awareness of such impression is crucial in taking steps towards achieving efficiency goals beyond performance, such as energy efficiency, and sustainability.   ","To achieve these goals, we have developed a novel low-latency job power profiling machine learning pipeline that can group job-level power profiles based on their shapes as they complete.","This pipeline leverages a comprehensive feature extraction and clustering pipeline powered by a generative adversarial network (GAN) model to handle the feature-rich time series of job-level power measurements.","The output is then used to train a classification model that can predict whether an incoming job power profile is similar to a known group of profiles or is completely new.","With extensive evaluations, we demonstrate the effectiveness of each component in our pipeline.","Also, we provide a preliminary analysis of the resulting clusters that depict the power profile landscape of the Summit supercomputer from more than 60K jobs sampled from the year 2021."],"url":"http://arxiv.org/abs/2402.00729v1","category":"cs.DC"}
{"created":"2024-02-01 16:25:00","title":"Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation","abstract":"Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples. This presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications. Measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the Rashomon set, in potentially huge hypothesis spaces. To address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the Rashomon set. We provide rigorous theoretical derivations to connect the dropout parameters to properties of the Rashomon set, and empirically evaluate our framework through extensive experimentation. Numerical results show that our technique consistently outperforms baselines in terms of the effectiveness of predictive multiplicity metric estimation, with runtime speedup up to $20\\times \\sim 5000\\times$. With efficient Rashomon set exploration and metric estimation, mitigation of predictive multiplicity is then achieved through dropout ensemble and model selection.","sentences":["Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples.","This presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications.","Measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the Rashomon set, in potentially huge hypothesis spaces.","To address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the Rashomon set.","We provide rigorous theoretical derivations to connect the dropout parameters to properties of the Rashomon set, and empirically evaluate our framework through extensive experimentation.","Numerical results show that our technique consistently outperforms baselines in terms of the effectiveness of predictive multiplicity metric estimation, with runtime speedup up to $20\\times \\sim 5000\\times$. With efficient Rashomon set exploration and metric estimation, mitigation of predictive multiplicity is then achieved through dropout ensemble and model selection."],"url":"http://arxiv.org/abs/2402.00728v1","category":"cs.LG"}
{"created":"2024-02-01 16:16:10","title":"Quantum Nonlocality: how does Nature do it?","abstract":"We answer the question asked by Nicolas Gisin in his article in Science few years ago. He claimed that quantum correlations are coming from outside space time. We explain that Bell Tests allow only rejecting probabilistic coupling provided by a local hidden variable model, but they do not justify metaphysical speculations about quantum nonlocality and objects which know about each other state, even when separated by large distances. We claim that the violation of Bell inequalities in physics and in cognitive science can be explained using the notion of Bohr contextuality. If contextual variables, describing varying experimental contexts, are correctly incorporated into a probabilistic model, then the inequalities cannot be proven and nonlocal correlations may be explained in an intuitive way. We elucidate the meaning of statistical independence assumption called incorrectly: free choice, measurement independence or no conspiracy. Since the correlation does not mean causation the violation of statistical independence should be rather called contextuality and it does not restrict experimenter freedom of choice. Therefore, contrary to what is believed, closing freedom of choice loophole does not prove statistical independence. We claim that quantum correlations are not coming from outside space time, but instead they are due to global space time symmetries.","sentences":["We answer the question asked by Nicolas Gisin in his article in Science few years ago.","He claimed that quantum correlations are coming from outside space time.","We explain that Bell Tests allow only rejecting probabilistic coupling provided by a local hidden variable model, but they do not justify metaphysical speculations about quantum nonlocality and objects which know about each other state, even when separated by large distances.","We claim that the violation of Bell inequalities in physics and in cognitive science can be explained using the notion of Bohr contextuality.","If contextual variables, describing varying experimental contexts, are correctly incorporated into a probabilistic model, then the inequalities cannot be proven and nonlocal correlations may be explained in an intuitive way.","We elucidate the meaning of statistical independence assumption called incorrectly: free choice, measurement independence or no conspiracy.","Since the correlation does not mean causation the violation of statistical independence should be rather called contextuality and it does not restrict experimenter freedom of choice.","Therefore, contrary to what is believed, closing freedom of choice loophole does not prove statistical independence.","We claim that quantum correlations are not coming from outside space time, but instead they are due to global space time symmetries."],"url":"http://arxiv.org/abs/2402.00725v1","category":"quant-ph"}
{"created":"2024-02-01 16:14:54","title":"Automatic Segmentation of the Spinal Cord Nerve Rootlets","abstract":"Precise identification of spinal nerve rootlets is relevant to delineate spinal levels for the study of functional activity in the spinal cord. The goal of this study was to develop an automatic method for the semantic segmentation of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI) scans. Images from two open-access MRI datasets were used to train a 3D multi-class convolutional neural network using an active learning approach to segment C2-C8 dorsal nerve rootlets. Each output class corresponds to a spinal level. The method was tested on 3T T2-weighted images from datasets unseen during training to assess inter-site, inter-session, and inter-resolution variability. The test Dice score was 0.67 +- 0.16 (mean +- standard deviation across rootlets levels), suggesting a good performance. The method also demonstrated low inter-vendor and inter-site variability (coefficient of variation <= 1.41 %), as well as low inter-session variability (coefficient of variation <= 1.30 %) indicating stable predictions across different MRI vendors, sites, and sessions. The proposed methodology is open-source and readily available in the Spinal Cord Toolbox (SCT) v6.2 and higher.","sentences":["Precise identification of spinal nerve rootlets is relevant to delineate spinal levels for the study of functional activity in the spinal cord.","The goal of this study was to develop an automatic method for the semantic segmentation of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI) scans.","Images from two open-access MRI datasets were used to train a 3D multi-class convolutional neural network using an active learning approach to segment C2-C8 dorsal nerve rootlets.","Each output class corresponds to a spinal level.","The method was tested on 3T T2-weighted images from datasets unseen during training to assess inter-site, inter-session, and inter-resolution variability.","The test Dice score was 0.67 +- 0.16 (mean +- standard deviation across rootlets levels), suggesting a good performance.","The method also demonstrated low inter-vendor and inter-site variability (coefficient of variation <= 1.41 %), as well as low inter-session variability (coefficient of variation <= 1.30 %) indicating stable predictions across different MRI vendors, sites, and sessions.","The proposed methodology is open-source and readily available in the Spinal Cord Toolbox (SCT) v6.2 and higher."],"url":"http://arxiv.org/abs/2402.00724v1","category":"cs.CV"}
{"created":"2024-02-01 16:14:35","title":"Improving Semantic Control in Discrete Latent Spaces with Transformer Quantized Variational Autoencoders","abstract":"Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon. Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism. To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoders (VQVAEs) to improve semantic control and generation in Transformer-based VAEs. In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAEs to guide the self-attention mechanism in T5 at the token-level, exploiting its full generalization capabilities. Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of controllability and preservation of semantic information across different tasks such as auto-encoding of sentences and mathematical expressions, text transfer, and inference. Moreover, T5VQVAE exhibits improved inference capabilities, suggesting potential applications for downstream natural language and symbolic reasoning tasks.","sentences":["Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon.","Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism.","To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoders (VQVAEs) to improve semantic control and generation in Transformer-based VAEs.","In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAEs to guide the self-attention mechanism in T5 at the token-level, exploiting its full generalization capabilities.","Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of controllability and preservation of semantic information across different tasks such as auto-encoding of sentences and mathematical expressions, text transfer, and inference.","Moreover, T5VQVAE exhibits improved inference capabilities, suggesting potential applications for downstream natural language and symbolic reasoning tasks."],"url":"http://arxiv.org/abs/2402.00723v1","category":"cs.CL"}
{"created":"2024-02-01 16:14:32","title":"Neural Style Transfer with Twin-Delayed DDPG for Shared Control of Robotic Manipulators","abstract":"Neural Style Transfer (NST) refers to a class of algorithms able to manipulate an element, most often images, to adopt the appearance or style of another one. Each element is defined as a combination of Content and Style: the Content can be conceptually defined as the what and the Style as the how of said element. In this context, we propose a custom NST framework for transferring a set of styles to the motion of a robotic manipulator, e.g., the same robotic task can be carried out in an angry, happy, calm, or sad way. An autoencoder architecture extracts and defines the Content and the Style of the target robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3) network generates the robot control policy using the loss defined by the autoencoder. The proposed Neural Policy Style Transfer TD3 (NPST3) alters the robot motion by introducing the trained style. Such an approach can be implemented either offline, for carrying out autonomous robot motions in dynamic environments, or online, for adapting at runtime the style of a teleoperated robot. The considered styles can be learned online from human demonstrations. We carried out an evaluation with human subjects enrolling 73 volunteers, asking them to recognize the style behind some representative robotic motions. Results show a good recognition rate, proving that it is possible to convey different styles to a robot using this approach.","sentences":["Neural Style Transfer (NST) refers to a class of algorithms able to manipulate an element, most often images, to adopt the appearance or style of another one.","Each element is defined as a combination of Content and Style: the Content can be conceptually defined as the what and the Style as the how of said element.","In this context, we propose a custom NST framework for transferring a set of styles to the motion of a robotic manipulator, e.g., the same robotic task can be carried out in an angry, happy, calm, or sad way.","An autoencoder architecture extracts and defines the Content and the Style of the target robot motions.","A Twin Delayed Deep Deterministic Policy Gradient (TD3) network generates the robot control policy using the loss defined by the autoencoder.","The proposed Neural Policy Style Transfer TD3 (NPST3) alters the robot motion by introducing the trained style.","Such an approach can be implemented either offline, for carrying out autonomous robot motions in dynamic environments, or online, for adapting at runtime the style of a teleoperated robot.","The considered styles can be learned online from human demonstrations.","We carried out an evaluation with human subjects enrolling 73 volunteers, asking them to recognize the style behind some representative robotic motions.","Results show a good recognition rate, proving that it is possible to convey different styles to a robot using this approach."],"url":"http://arxiv.org/abs/2402.00722v1","category":"cs.RO"}
{"created":"2024-02-01 16:12:15","title":"Orientation-aware Incremental Potential Contact","abstract":"The Incremental Potential Contact (IPC) method enables robust complex simulations of deformable objects with contact and friction. The key to IPC's robustness is its strict adherence to geometric constraints, avoiding intersections, which are a common cause of robustness issues in contact mechanics. A key element of the IPC approach to contact is a geometric barrier function, which is defined directly in the discrete setting. While IPC achieves its main goal of providing guarantees for contact constraints, its parameters need to be chosen carefully to avoid significant simulation artifacts and inaccuracies. We present a systematic derivation of an IPC-like continuum potential defined for smooth and piecewise smooth surfaces, starting from identifying a set of natural requirements for contact potentials, including the barrier property, locality, differentiable dependence of shape, and absence of forces in rest configurations, based on the idea of candidate sets. Our potential is formulated in a way independent of surface discretization.   This new potential is suitable for piecewise-linear surfaces and its efficiency is similar to standard IPC. We demonstrate its behavior and compare it to IPC on a range of challenging contact examples.","sentences":["The Incremental Potential Contact (IPC) method enables robust complex simulations of deformable objects with contact and friction.","The key to IPC's robustness is its strict adherence to geometric constraints, avoiding intersections, which are a common cause of robustness issues in contact mechanics.","A key element of the IPC approach to contact is a geometric barrier function, which is defined directly in the discrete setting.","While IPC achieves its main goal of providing guarantees for contact constraints, its parameters need to be chosen carefully to avoid significant simulation artifacts and inaccuracies.","We present a systematic derivation of an IPC-like continuum potential defined for smooth and piecewise smooth surfaces, starting from identifying a set of natural requirements for contact potentials, including the barrier property, locality, differentiable dependence of shape, and absence of forces in rest configurations, based on the idea of candidate sets.","Our potential is formulated in a way independent of surface discretization.   ","This new potential is suitable for piecewise-linear surfaces and its efficiency is similar to standard IPC.","We demonstrate its behavior and compare it to IPC on a range of challenging contact examples."],"url":"http://arxiv.org/abs/2402.00719v1","category":"cs.GR"}
{"created":"2024-02-01 16:09:39","title":"Investigation of fluorescence versus transmission readout for three-photon Rydberg excitation used in electrometry","abstract":"We present a three-photon based fluorescence readout method where the strength of the fluorescence scales with the strength of the radio-frequency (RF) field being applied. We compare this method to conventional three-photon electromagnetically-induced transparency (EIT) and electromagnetically-induced absorption (EIA). Our demonstrated EIA/EIT sensitivity in the collinear three-photon Cesium system is the best reported to date at roughly 30 uVm^{-1}Hz^{-1/2}. The fluorescence is nearly 4 fold better in sensitivity compared to EIA/EIT readout.","sentences":["We present a three-photon based fluorescence readout method where the strength of the fluorescence scales with the strength of the radio-frequency (RF) field being applied.","We compare this method to conventional three-photon electromagnetically-induced transparency (EIT) and electromagnetically-induced absorption (EIA).","Our demonstrated EIA/EIT sensitivity in the collinear three-photon Cesium system is the best reported to date at roughly 30 uVm^{-1}Hz^{-1/2}.","The fluorescence is nearly 4 fold better in sensitivity compared to EIA/EIT readout."],"url":"http://arxiv.org/abs/2402.00718v1","category":"quant-ph"}
{"created":"2024-02-01 16:09:19","title":"Intent Assurance using LLMs guided by Intent Drift","abstract":"Intent-Based Networking (IBN) presents a paradigm shift for network management, by promising to align intents and business objectives with network operations--in an automated manner. However, its practical realization is challenging: 1) processing intents, i.e., translate, decompose and identify the logic to fulfill the intent, and 2) intent conformance, that is, considering dynamic networks, the logic should be adequately adapted to assure intents. To address the latter, intent assurance is tasked with continuous verification and validation, including taking the necessary actions to align the operational and target states. In this paper, we define an assurance framework that allows us to detect and act when intent drift occurs. To do so, we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents.","sentences":["Intent-Based Networking (IBN) presents a paradigm shift for network management, by promising to align intents and business objectives with network operations--in an automated manner.","However, its practical realization is challenging: 1) processing intents, i.e., translate, decompose and identify the logic to fulfill the intent, and 2) intent conformance, that is, considering dynamic networks, the logic should be adequately adapted to assure intents.","To address the latter, intent assurance is tasked with continuous verification and validation, including taking the necessary actions to align the operational and target states.","In this paper, we define an assurance framework that allows us to detect and act when intent drift occurs.","To do so, we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents."],"url":"http://arxiv.org/abs/2402.00715v1","category":"cs.AI"}
{"created":"2024-02-01 16:07:12","title":"ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction","abstract":"Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster readiness, reduced economic risk, and improved policy-making amidst climate change. Yet, S2S prediction remains challenging due to the chaotic nature of the system. At present, existing benchmarks for weather and climate applications, tend to (1) have shorter forecasting range of up-to 14 days, (2) do not include a wide range of operational baseline forecasts, and (3) lack physics-based constraints for explainability. Thus, we propose ChaosBench, a large-scale, multi-channel, physics-based benchmark for S2S prediction. ChaosBench has over 460K frames of real-world observations and simulations, each with 60 variable-channels and spanning for up-to 45 years. We also propose several physics-based, in addition to vision-based metrics, that enables for a more physically-consistent model. Furthermore, we include a diverse set of physics-based forecasts from 4 national weather agencies as baselines to our data-driven counterpart. We establish two tasks that vary in complexity: full and sparse dynamics prediction. Our benchmark is one of the first to perform large-scale evaluation on existing models including PanguWeather, FourCastNetV2, GraphCast, and ClimaX, and finds methods originally developed for weather-scale applications fails on S2S task. We release our benchmark code and datasets at https://leap-stc.github.io/ChaosBench.","sentences":["Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster readiness, reduced economic risk, and improved policy-making amidst climate change.","Yet, S2S prediction remains challenging due to the chaotic nature of the system.","At present, existing benchmarks for weather and climate applications, tend to (1) have shorter forecasting range of up-to 14 days, (2) do not include a wide range of operational baseline forecasts, and (3) lack physics-based constraints for explainability.","Thus, we propose ChaosBench, a large-scale, multi-channel, physics-based benchmark for S2S prediction.","ChaosBench has over 460K frames of real-world observations and simulations, each with 60 variable-channels and spanning for up-to 45 years.","We also propose several physics-based, in addition to vision-based metrics, that enables for a more physically-consistent model.","Furthermore, we include a diverse set of physics-based forecasts from 4 national weather agencies as baselines to our data-driven counterpart.","We establish two tasks that vary in complexity: full and sparse dynamics prediction.","Our benchmark is one of the first to perform large-scale evaluation on existing models including PanguWeather, FourCastNetV2, GraphCast, and ClimaX, and finds methods originally developed for weather-scale applications fails on S2S task.","We release our benchmark code and datasets at https://leap-stc.github.io/ChaosBench."],"url":"http://arxiv.org/abs/2402.00712v1","category":"cs.CV"}
{"created":"2024-02-01 16:06:35","title":"Explaining Text Classifiers with Counterfactual Representations","abstract":"One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the representation space. Second, we study a real world scenario where our counterfactuals can be leveraged both for explaining a classifier and for bias mitigation.","sentences":["One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature.","Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events.","In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation.","We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework.","To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the representation space.","Second, we study a real world scenario where our counterfactuals can be leveraged both for explaining a classifier and for bias mitigation."],"url":"http://arxiv.org/abs/2402.00711v1","category":"cs.LG"}
{"created":"2024-02-01 16:05:15","title":"Towards an autonomous industry 4.0 warehouse: A UAV and blockchain-based system for inventory and traceability applications in big data-driven supply chain management","abstract":"In this paper we present the design and evaluation of a UAV-based system aimed at automating inventory tasks and keeping the traceability of industrial items attached to Radio-Frequency IDentification (RFID) tags. To confront current shortcomings, such a system is developed under a versatile, modular and scalable architecture aimed to reinforce cyber security and decentralization while fostering external audits and big data analytics. Therefore, the system uses a blockchain and a distributed ledger to store certain inventory data collected by UAVs, validate them, ensure their trustworthiness and make them available to the interested parties. In order to show the performance of the proposed system, different tests were performed in a real industrial warehouse, concluding that the system is able to obtain the inventory data really fast in comparison to traditional manual tasks, while being also able to estimate the position of the items when hovering over them thanks to their tag's signal strength. In addition, the performance of the proposed blockchain-based architecture was evaluated in different scenarios.","sentences":["In this paper we present the design and evaluation of a UAV-based system aimed at automating inventory tasks and keeping the traceability of industrial items attached to Radio-Frequency IDentification (RFID) tags.","To confront current shortcomings, such a system is developed under a versatile, modular and scalable architecture aimed to reinforce cyber security and decentralization while fostering external audits and big data analytics.","Therefore, the system uses a blockchain and a distributed ledger to store certain inventory data collected by UAVs, validate them, ensure their trustworthiness and make them available to the interested parties.","In order to show the performance of the proposed system, different tests were performed in a real industrial warehouse, concluding that the system is able to obtain the inventory data really fast in comparison to traditional manual tasks, while being also able to estimate the position of the items when hovering over them thanks to their tag's signal strength.","In addition, the performance of the proposed blockchain-based architecture was evaluated in different scenarios."],"url":"http://arxiv.org/abs/2402.00709v1","category":"cs.CR"}
{"created":"2024-02-01 16:04:36","title":"Benchmarking human-robot collaborative assembly tasks","abstract":"Manufacturing assembly tasks can vary in complexity and level of automation. Yet, achieving full automation can be challenging and inefficient, particularly due to the complexity of certain assembly operations. Human-robot collaborative work, leveraging the strengths of human labor alongside the capabilities of robots, can be a solution for enhancing efficiency. This paper introduces the CT benchmark, a benchmark and model set designed to facilitate the testing and evaluation of human-robot collaborative assembly scenarios. It was designed to compare manual and automatic processes using metrics such as the assembly time and human workload. The components of the model set can be assembled through the most common assembly tasks, each with varying levels of difficulty. The CT benchmark was designed with a focus on its applicability in human-robot collaborative environments, with the aim of ensuring the reproducibility and replicability of experiments. Experiments were carried out to assess assembly performance in three different setups (manual, automatic and collaborative), measuring metrics related to the assembly time and the workload on human operators. The results suggest that the collaborative approach takes longer than the fully manual assembly, with an increase of 70.8%. However, users reported a lower overall workload, as well as reduced mental demand, physical demand, and effort according to the NASA-TLX questionnaire.","sentences":["Manufacturing assembly tasks can vary in complexity and level of automation.","Yet, achieving full automation can be challenging and inefficient, particularly due to the complexity of certain assembly operations.","Human-robot collaborative work, leveraging the strengths of human labor alongside the capabilities of robots, can be a solution for enhancing efficiency.","This paper introduces the CT benchmark, a benchmark and model set designed to facilitate the testing and evaluation of human-robot collaborative assembly scenarios.","It was designed to compare manual and automatic processes using metrics such as the assembly time and human workload.","The components of the model set can be assembled through the most common assembly tasks, each with varying levels of difficulty.","The CT benchmark was designed with a focus on its applicability in human-robot collaborative environments, with the aim of ensuring the reproducibility and replicability of experiments.","Experiments were carried out to assess assembly performance in three different setups (manual, automatic and collaborative), measuring metrics related to the assembly time and the workload on human operators.","The results suggest that the collaborative approach takes longer than the fully manual assembly, with an increase of 70.8%.","However, users reported a lower overall workload, as well as reduced mental demand, physical demand, and effort according to the NASA-TLX questionnaire."],"url":"http://arxiv.org/abs/2402.00708v1","category":"cs.RO"}
{"created":"2024-02-01 16:04:04","title":"Non-Exchangeable Conformal Language Generation with Nearest Neighbors","abstract":"Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good coverage, we thus give a more theoretically principled way to perform sampling with conformal guarantees.","sentences":["Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable.","Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic.","In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage.","The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors.","Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees.","Experiments in machine translation and language modeling show encouraging results in generation quality.","By also producing tighter prediction sets with good coverage, we thus give a more theoretically principled way to perform sampling with conformal guarantees."],"url":"http://arxiv.org/abs/2402.00707v1","category":"cs.CL"}
{"created":"2024-02-01 16:00:21","title":"Combining the Strengths of Dutch Survey and Register Data in a Data Challenge to Predict Fertility (PreFer)","abstract":"The social sciences have produced an impressive body of research on determinants of fertility outcomes, or whether and when people have children. However, the strength of these determinants and underlying theories are rarely evaluated on their predictive ability on new data. This prevents us from systematically comparing studies, hindering the evaluation and accumulation of knowledge. In this paper, we present two datasets which can be used to study the predictability of fertility outcomes in the Netherlands. One dataset is based on the LISS panel, a longitudinal survey which includes thousands of variables on a wide range of topics, including individual preferences and values. The other is based on the Dutch register data which lacks attitudinal data but includes detailed information about the life courses of millions of Dutch residents. We provide information about the datasets and the samples, and describe the fertility outcome of interest. We also introduce the fertility prediction data challenge PreFer which is based on these datasets and will start in Spring 2024. We outline the ways in which measuring the predictability of fertility outcomes using these datasets and combining their strengths in the data challenge can advance our understanding of fertility behaviour and computational social science. We further provide details for participants on how to take part in the data challenge.","sentences":["The social sciences have produced an impressive body of research on determinants of fertility outcomes, or whether and when people have children.","However, the strength of these determinants and underlying theories are rarely evaluated on their predictive ability on new data.","This prevents us from systematically comparing studies, hindering the evaluation and accumulation of knowledge.","In this paper, we present two datasets which can be used to study the predictability of fertility outcomes in the Netherlands.","One dataset is based on the LISS panel, a longitudinal survey which includes thousands of variables on a wide range of topics, including individual preferences and values.","The other is based on the Dutch register data which lacks attitudinal data but includes detailed information about the life courses of millions of Dutch residents.","We provide information about the datasets and the samples, and describe the fertility outcome of interest.","We also introduce the fertility prediction data challenge PreFer which is based on these datasets and will start in Spring 2024.","We outline the ways in which measuring the predictability of fertility outcomes using these datasets and combining their strengths in the data challenge can advance our understanding of fertility behaviour and computational social science.","We further provide details for participants on how to take part in the data challenge."],"url":"http://arxiv.org/abs/2402.00705v1","category":"cs.LG"}
{"created":"2024-02-01 15:59:49","title":"Measuring, processing, and generating partially coherent light with self-configuring optics","abstract":"Optical phenomena always display some degree of partial coherence between their respective degrees of freedom. Partial coherence is of particular interest in multimodal systems, where classical and quantum correlations between spatial, polarization, and spectral degrees of freedom can lead to fascinating phenomena (e.g., entanglement) and be leveraged for advanced imaging and sensing modalities (e.g., in hyperspectral, polarization, and ghost imaging). Here, we present a universal method to analyze, process, and generate spatially partially coherent light in multimode systems by using self-configuring optical networks. Our method relies on cascaded self-configuring layers whose average power outputs are sequentially optimized. Once optimized, the network separates the input light into its mutually incoherent components, which is formally equivalent to a diagonalization of the input density matrix. We illustrate our method with arrays of Mach-Zehnder interferometers and show how this method can be used to perform partially coherent environmental light sensing, generation of multimode partially coherent light with arbitrary coherency matrices, and unscrambling of quantum optical mixtures. We provide guidelines for the experimental realization of this method, paving the way for self-configuring photonic devices that can automatically learn optimal modal representations of partially coherent light fields.","sentences":["Optical phenomena always display some degree of partial coherence between their respective degrees of freedom.","Partial coherence is of particular interest in multimodal systems, where classical and quantum correlations between spatial, polarization, and spectral degrees of freedom can lead to fascinating phenomena (e.g., entanglement) and be leveraged for advanced imaging and sensing modalities (e.g., in hyperspectral, polarization, and ghost imaging).","Here, we present a universal method to analyze, process, and generate spatially partially coherent light in multimode systems by using self-configuring optical networks.","Our method relies on cascaded self-configuring layers whose average power outputs are sequentially optimized.","Once optimized, the network separates the input light into its mutually incoherent components, which is formally equivalent to a diagonalization of the input density matrix.","We illustrate our method with arrays of Mach-Zehnder interferometers and show how this method can be used to perform partially coherent environmental light sensing, generation of multimode partially coherent light with arbitrary coherency matrices, and unscrambling of quantum optical mixtures.","We provide guidelines for the experimental realization of this method, paving the way for self-configuring photonic devices that can automatically learn optimal modal representations of partially coherent light fields."],"url":"http://arxiv.org/abs/2402.00704v1","category":"physics.optics"}
{"created":"2024-02-01 15:59:16","title":"Vehicle Perception from Satellite","abstract":"Satellites are capable of capturing high-resolution videos. It makes vehicle perception from satellite become possible. Compared to street surveillance, drive recorder or other equipments, satellite videos provide a much broader city-scale view, so that the global dynamic scene of the traffic are captured and displayed. Traffic monitoring from satellite is a new task with great potential applications, including traffic jams prediction, path planning, vehicle dispatching, \\emph{etc.}. Practically, limited by the resolution and view, the captured vehicles are very tiny (a few pixels) and move slowly. Worse still, these satellites are in Low Earth Orbit (LEO) to capture such high-resolution videos, so the background is also moving. Under this circumstance, traffic monitoring from the satellite view is an extremely challenging task. To attract more researchers into this field, we build a large-scale benchmark for traffic monitoring from satellite. It supports several tasks, including tiny object detection, counting and density estimation. The dataset is constructed based on 12 satellite videos and 14 synthetic videos recorded from GTA-V. They are separated into 408 video clips, which contain 7,336 real satellite images and 1,960 synthetic images. 128,801 vehicles are annotated totally, and the number of vehicles in each image varies from 0 to 101. Several classic and state-of-the-art approaches in traditional computer vision are evaluated on the datasets, so as to compare the performance of different approaches, analyze the challenges in this task, and discuss the future prospects. The dataset is available at: https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos.","sentences":["Satellites are capable of capturing high-resolution videos.","It makes vehicle perception from satellite become possible.","Compared to street surveillance, drive recorder or other equipments, satellite videos provide a much broader city-scale view, so that the global dynamic scene of the traffic are captured and displayed.","Traffic monitoring from satellite is a new task with great potential applications, including traffic jams prediction, path planning, vehicle dispatching, \\emph{etc.}.","Practically, limited by the resolution and view, the captured vehicles are very tiny (a few pixels) and move slowly.","Worse still, these satellites are in Low Earth Orbit (LEO) to capture such high-resolution videos, so the background is also moving.","Under this circumstance, traffic monitoring from the satellite view is an extremely challenging task.","To attract more researchers into this field, we build a large-scale benchmark for traffic monitoring from satellite.","It supports several tasks, including tiny object detection, counting and density estimation.","The dataset is constructed based on 12 satellite videos and 14 synthetic videos recorded from GTA-V.","They are separated into 408 video clips, which contain 7,336 real satellite images and 1,960 synthetic images.","128,801 vehicles are annotated totally, and the number of vehicles in each image varies from 0 to 101.","Several classic and state-of-the-art approaches in traditional computer vision are evaluated on the datasets, so as to compare the performance of different approaches, analyze the challenges in this task, and discuss the future prospects.","The dataset is available at: https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos."],"url":"http://arxiv.org/abs/2402.00703v1","category":"cs.CV"}
{"created":"2024-02-01 15:57:11","title":"In-Bed Pose Estimation: A Review","abstract":"Human pose estimation, the process of identifying joint positions in a person's body from images or videos, represents a widely utilized technology across diverse fields, including healthcare. One such healthcare application involves in-bed pose estimation, where the body pose of an individual lying under a blanket is analyzed. This task, for instance, can be used to monitor a person's sleep behavior and detect symptoms early for potential disease diagnosis in homes and hospitals. Several studies have utilized unimodal and multimodal methods to estimate in-bed human poses. The unimodal studies generally employ RGB images, whereas the multimodal studies use modalities including RGB, long-wavelength infrared, pressure map, and depth map. Multimodal studies have the advantage of using modalities in addition to RGB that might capture information useful to cope with occlusions. Moreover, some multimodal studies exclude RGB and, this way, better suit privacy preservation. To expedite advancements in this domain, we conduct a review of existing datasets and approaches. Our objectives are to show the limitations of the previous studies, current challenges, and provide insights for future works on the in-bed human pose estimation field.","sentences":["Human pose estimation, the process of identifying joint positions in a person's body from images or videos, represents a widely utilized technology across diverse fields, including healthcare.","One such healthcare application involves in-bed pose estimation, where the body pose of an individual lying under a blanket is analyzed.","This task, for instance, can be used to monitor a person's sleep behavior and detect symptoms early for potential disease diagnosis in homes and hospitals.","Several studies have utilized unimodal and multimodal methods to estimate in-bed human poses.","The unimodal studies generally employ RGB images, whereas the multimodal studies use modalities including RGB, long-wavelength infrared, pressure map, and depth map.","Multimodal studies have the advantage of using modalities in addition to RGB that might capture information useful to cope with occlusions.","Moreover, some multimodal studies exclude RGB and, this way, better suit privacy preservation.","To expedite advancements in this domain, we conduct a review of existing datasets and approaches.","Our objectives are to show the limitations of the previous studies, current challenges, and provide insights for future works on the in-bed human pose estimation field."],"url":"http://arxiv.org/abs/2402.00700v1","category":"cs.CV"}
{"created":"2024-02-01 15:55:50","title":"PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software","abstract":"The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse. This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatically extract model metadata, including the model's training datasets, parameters, and evaluation metrics. Our analysis of this dataset provides the first summary statistics for the PTM supply chain, showing the trend of PTM development and common shortcomings of PTM package documentation. Our example application reveals inconsistencies in software licenses across PTMs and their dependent projects. PeaTMOSS lays the foundation for future research, offering rich opportunities to investigate the PTM supply chain. We outline mining opportunities on PTMs, their downstream usage, and cross-cutting questions.","sentences":["The development and training of deep learning models have become increasingly costly and complex.","Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications.","The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models.","Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse.","This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models.","Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use.","To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatically extract model metadata, including the model's training datasets, parameters, and evaluation metrics.","Our analysis of this dataset provides the first summary statistics for the PTM supply chain, showing the trend of PTM development and common shortcomings of PTM package documentation.","Our example application reveals inconsistencies in software licenses across PTMs and their dependent projects.","PeaTMOSS lays the foundation for future research, offering rich opportunities to investigate the PTM supply chain.","We outline mining opportunities on PTMs, their downstream usage, and cross-cutting questions."],"url":"http://arxiv.org/abs/2402.00699v1","category":"cs.SE"}
{"created":"2024-02-01 15:55:25","title":"Time-Series Analysis Approach for Improving Energy Efficiency of a Fixed-Route Vessel in Short-Sea Shipping","abstract":"Several approaches have been developed for improving the ship energy efficiency, thereby reducing operating costs and ensuring compliance with climate change mitigation regulations. Many of these approaches will heavily depend on measured data from onboard IoT devices, including operational and environmental information, as well as external data sources for additional navigational data. In this paper, we develop a framework that implements time-series analysis techniques to optimize the vessel's speed profile for improving the vessel's energy efficiency. We present a case study involving a real-world data from a passenger vessel that was collected over a span of 15 months in the south of Sweden. The results indicate that the implemented models exhibit a range of outcomes and adaptability across different scenarios. The findings highlight the effectiveness of time-series analysis approach for optimizing vessel voyages within the context of constrained landscapes, as often seen in short-sea shipping.","sentences":["Several approaches have been developed for improving the ship energy efficiency, thereby reducing operating costs and ensuring compliance with climate change mitigation regulations.","Many of these approaches will heavily depend on measured data from onboard IoT devices, including operational and environmental information, as well as external data sources for additional navigational data.","In this paper, we develop a framework that implements time-series analysis techniques to optimize the vessel's speed profile for improving the vessel's energy efficiency.","We present a case study involving a real-world data from a passenger vessel that was collected over a span of 15 months in the south of Sweden.","The results indicate that the implemented models exhibit a range of outcomes and adaptability across different scenarios.","The findings highlight the effectiveness of time-series analysis approach for optimizing vessel voyages within the context of constrained landscapes, as often seen in short-sea shipping."],"url":"http://arxiv.org/abs/2402.00698v1","category":"cs.CE"}
{"created":"2024-02-01 15:51:46","title":"Approximating Optimal Morphing Attacks using Template Inversion","abstract":"Recent works have demonstrated the feasibility of inverting face recognition systems, enabling to recover convincing face images using only their embeddings. We leverage such template inversion models to develop a novel type ofdeep morphing attack based on inverting a theoretical optimal morph embedding, which is obtained as an average of the face embeddings of source images. We experiment with two variants of this approach: the first one exploits a fully self-contained embedding-to-image inversion model, while the second leverages the synthesis network of a pretrained StyleGAN network for increased morph realism. We generate morphing attacks from several source datasets and study the effectiveness of those attacks against several face recognition networks. We showcase that our method can compete with and regularly beat the previous state of the art for deep-learning based morph generation in terms of effectiveness, both in white-box and black-box attack scenarios, and is additionally much faster to run. We hope this might facilitate the development of large scale deep morph datasets for training detection models.","sentences":["Recent works have demonstrated the feasibility of inverting face recognition systems, enabling to recover convincing face images using only their embeddings.","We leverage such template inversion models to develop a novel type ofdeep morphing attack based on inverting a theoretical optimal morph embedding, which is obtained as an average of the face embeddings of source images.","We experiment with two variants of this approach: the first one exploits a fully self-contained embedding-to-image inversion model, while the second leverages the synthesis network of a pretrained StyleGAN network for increased morph realism.","We generate morphing attacks from several source datasets and study the effectiveness of those attacks against several face recognition networks.","We showcase that our method can compete with and regularly beat the previous state of the art for deep-learning based morph generation in terms of effectiveness, both in white-box and black-box attack scenarios, and is additionally much faster to run.","We hope this might facilitate the development of large scale deep morph datasets for training detection models."],"url":"http://arxiv.org/abs/2402.00695v1","category":"cs.CV"}
{"created":"2024-02-01 15:50:58","title":"Multiway junction conditions I: booklets and webs","abstract":"We explore the construction of gluing together an arbitrary number of spacetimes along a common interface that gives a geometric structure we refer to as the booklet geometry. We derive junction conditions across the interface, in both Einstein gravity and dilaton gravity, using two methods (1) by geometrically introducing an auxiliary reverse extension manifold, and (2) by varying the action of the theory under consideration, and the two methods give the same result. Our junction condition works for both spacelike and timelike interfaces. We also provide simple examples that illustrate applications of the junction condition. We further comment on the geometry of a web of spacetime in the presence of multiple interfaces and its resemblance to Feynman diagrams.","sentences":["We explore the construction of gluing together an arbitrary number of spacetimes along a common interface that gives a geometric structure we refer to as the booklet geometry.","We derive junction conditions across the interface, in both Einstein gravity and dilaton gravity, using two methods (1) by geometrically introducing an auxiliary reverse extension manifold, and (2) by varying the action of the theory under consideration, and the two methods give the same result.","Our junction condition works for both spacelike and timelike interfaces.","We also provide simple examples that illustrate applications of the junction condition.","We further comment on the geometry of a web of spacetime in the presence of multiple interfaces and its resemblance to Feynman diagrams."],"url":"http://arxiv.org/abs/2402.00694v1","category":"hep-th"}
{"created":"2024-02-01 15:50:40","title":"A Framework for Building Point Cloud Cleaning, Plane Detection and Semantic Segmentation","abstract":"This paper presents a framework to address the challenges involved in building point cloud cleaning, plane detection, and semantic segmentation, with the ultimate goal of enhancing building modeling. We focus in the cleaning stage on removing outliers from the acquired point cloud data by employing an adaptive threshold technique based on z-score measure. Following the cleaning process, we perform plane detection using the robust RANSAC paradigm. The goal is to carry out multiple plane segmentations, and to classify segments into distinct categories, such as floors, ceilings, and walls. The resulting segments can generate accurate and detailed point clouds representing the building's architectural elements. Moreover, we address the problem of semantic segmentation, which plays a vital role in the identification and classification of different components within the building, such as walls, windows, doors, roofs, and objects. Inspired by the PointNet architecture, we propose a deep learning architecture for efficient semantic segmentation in buildings. The results demonstrate the effectiveness of the proposed framework in handling building modeling tasks, paving the way for improved accuracy and efficiency in the field of building modelization.","sentences":["This paper presents a framework to address the challenges involved in building point cloud cleaning, plane detection, and semantic segmentation, with the ultimate goal of enhancing building modeling.","We focus in the cleaning stage on removing outliers from the acquired point cloud data by employing an adaptive threshold technique based on z-score measure.","Following the cleaning process, we perform plane detection using the robust RANSAC paradigm.","The goal is to carry out multiple plane segmentations, and to classify segments into distinct categories, such as floors, ceilings, and walls.","The resulting segments can generate accurate and detailed point clouds representing the building's architectural elements.","Moreover, we address the problem of semantic segmentation, which plays a vital role in the identification and classification of different components within the building, such as walls, windows, doors, roofs, and objects.","Inspired by the PointNet architecture, we propose a deep learning architecture for efficient semantic segmentation in buildings.","The results demonstrate the effectiveness of the proposed framework in handling building modeling tasks, paving the way for improved accuracy and efficiency in the field of building modelization."],"url":"http://arxiv.org/abs/2402.00692v1","category":"cs.CV"}
{"created":"2024-02-01 15:50:37","title":"Comparative Study of Large Language Model Architectures on Frontier","abstract":"Large language models (LLMs) have garnered significant attention in both the AI community and beyond. Among these, the Generative Pre-trained Transformer (GPT) has emerged as the dominant architecture, spawning numerous variants. However, these variants have undergone pre-training under diverse conditions, including variations in input data, data preprocessing, and training methodologies, resulting in a lack of controlled comparative studies. Here we meticulously examine two prominent open-sourced GPT architectures, GPT-NeoX and LLaMA, leveraging the computational power of Frontier, the world's first Exascale supercomputer. Employing the same materials science text corpus and a comprehensive end-to-end pipeline, we conduct a comparative analysis of their training and downstream performance. Our efforts culminate in achieving state-of-the-art performance on a challenging materials science benchmark. Furthermore, we investigate the computation and energy efficiency, and propose a computationally efficient method for architecture design. To our knowledge, these pre-trained models represent the largest available for materials science. Our findings provide practical guidance for building LLMs on HPC platforms.","sentences":["Large language models (LLMs) have garnered significant attention in both the AI community and beyond.","Among these, the Generative Pre-trained Transformer (GPT) has emerged as the dominant architecture, spawning numerous variants.","However, these variants have undergone pre-training under diverse conditions, including variations in input data, data preprocessing, and training methodologies, resulting in a lack of controlled comparative studies.","Here we meticulously examine two prominent open-sourced GPT architectures, GPT-NeoX and LLaMA, leveraging the computational power of Frontier, the world's first Exascale supercomputer.","Employing the same materials science text corpus and a comprehensive end-to-end pipeline, we conduct a comparative analysis of their training and downstream performance.","Our efforts culminate in achieving state-of-the-art performance on a challenging materials science benchmark.","Furthermore, we investigate the computation and energy efficiency, and propose a computationally efficient method for architecture design.","To our knowledge, these pre-trained models represent the largest available for materials science.","Our findings provide practical guidance for building LLMs on HPC platforms."],"url":"http://arxiv.org/abs/2402.00691v1","category":"cs.DC"}
{"created":"2024-02-01 15:49:47","title":"Ocassionally Secure: A Comparative Analysis of Code Generation Assistants","abstract":"$ $Large Language Models (LLMs) are being increasingly utilized in various applications, with code generations being a notable example. While previous research has shown that LLMs have the capability to generate both secure and insecure code, the literature does not take into account what factors help generate secure and effective code. Therefore in this paper we focus on identifying and understanding the conditions and contexts in which LLMs can be effectively and safely deployed in real-world scenarios to generate quality code. We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and GPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to assess each model's code generation capabilities. We contextualized our study to represent the typical use cases of a real-life developer employing LLMs for everyday tasks as work. Additionally, we place an emphasis on security awareness which is represented through the use of two distinct versions of our developer persona. In total, we collected 61 code outputs and analyzed them across several aspects: functionality, security, performance, complexity, and reliability. These insights are crucial for understanding the models' capabilities and limitations, guiding future development and practical applications in the field of automated code generation.","sentences":["$ $Large Language Models (LLMs) are being increasingly utilized in various applications, with code generations being a notable example.","While previous research has shown that LLMs have the capability to generate both secure and insecure code, the literature does not take into account what factors help generate secure and effective code.","Therefore in this paper we focus on identifying and understanding the conditions and contexts in which LLMs can be effectively and safely deployed in real-world scenarios to generate quality code.","We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and GPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to assess each model's code generation capabilities.","We contextualized our study to represent the typical use cases of a real-life developer employing LLMs for everyday tasks as work.","Additionally, we place an emphasis on security awareness which is represented through the use of two distinct versions of our developer persona.","In total, we collected 61 code outputs and analyzed them across several aspects: functionality, security, performance, complexity, and reliability.","These insights are crucial for understanding the models' capabilities and limitations, guiding future development and practical applications in the field of automated code generation."],"url":"http://arxiv.org/abs/2402.00689v1","category":"cs.CR"}
{"created":"2024-02-01 15:49:07","title":"Carrollian $Lw_{1+\\infty}$ representation from twistor space","abstract":"We construct an explicit realization of the action of the $Lw_{1+\\infty}$ loop algebra on fields at null infinity. This action is directly derived by Penrose transform of the geometrical action of $Lw_{1+\\infty}$ symmetries in twistor space, ensuring that it forms a representation of the algebra. Finally, we show that this action coincides with the canonical action of $Lw_{1+\\infty}$ Noether charges on the asymptotic phase space.","sentences":["We construct an explicit realization of the action of the $Lw_{1+\\infty}$ loop algebra on fields at null infinity.","This action is directly derived by Penrose transform of the geometrical action of $Lw_{1+\\infty}$ symmetries in twistor space, ensuring that it forms a representation of the algebra.","Finally, we show that this action coincides with the canonical action of $Lw_{1+\\infty}$ Noether charges on the asymptotic phase space."],"url":"http://arxiv.org/abs/2402.00688v1","category":"hep-th"}
{"created":"2024-02-01 15:48:45","title":"A Review on the Use of Blockchain for the Internet of Things","abstract":"The paradigm of Internet of Things (IoT) is paving the way for a world, where many of our daily objects will be interconnected and will interact with their environment in order to collect information and automate certain tasks. Such a vision requires, among other things, seamless authentication, data privacy, security, robustness against attacks, easy deployment, and self-maintenance. Such features can be brought by blockchain, a technology born with a cryptocurrency called Bitcoin. In this paper, a thorough review on how to adapt blockchain to the specific needs of IoT in order to develop Blockchain-based IoT (BIoT) applications is presented. After describing the basics of blockchain, the most relevant BIoT applications are described with the objective of emphasizing how blockchain can impact traditional cloud-centered IoT applications. Then, the current challenges and possible optimizations are detailed regarding many aspects that affect the design, development, and deployment of a BIoT application. Finally, some recommendations are enumerated with the aim of guiding future BIoT researchers and developers on some of the issues that will have to be tackled before deploying the next generation of BIoT applications.","sentences":["The paradigm of Internet of Things (IoT) is paving the way for a world, where many of our daily objects will be interconnected and will interact with their environment in order to collect information and automate certain tasks.","Such a vision requires, among other things, seamless authentication, data privacy, security, robustness against attacks, easy deployment, and self-maintenance.","Such features can be brought by blockchain, a technology born with a cryptocurrency called Bitcoin.","In this paper, a thorough review on how to adapt blockchain to the specific needs of IoT in order to develop Blockchain-based IoT (BIoT) applications is presented.","After describing the basics of blockchain, the most relevant BIoT applications are described with the objective of emphasizing how blockchain can impact traditional cloud-centered IoT applications.","Then, the current challenges and possible optimizations are detailed regarding many aspects that affect the design, development, and deployment of a BIoT application.","Finally, some recommendations are enumerated with the aim of guiding future BIoT researchers and developers on some of the issues that will have to be tackled before deploying the next generation of BIoT applications."],"url":"http://arxiv.org/abs/2402.00687v1","category":"cs.CR"}
{"created":"2024-02-01 15:47:01","title":"An Investigation of Hardware Security Bug Characteristics in Open-Source Projects","abstract":"Hardware security is an important concern of system security as vulnerabilities can arise from design errors introduced throughout the development lifecycle. Recent works have proposed techniques to detect hardware security bugs, such as static analysis, fuzzing, and symbolic execution. However, the fundamental properties of hardware security bugs remain relatively unexplored. To gain a better understanding of hardware security bugs, we perform a deep dive into the popular OpenTitan project, including its bug reports and bug fixes. We manually classify the bugs as relevant to functionality or security and analyze characteristics, such as the impact and location of security bugs, and the size of their bug fixes. We also investigate relationships between security impact and bug management during development. Finally, we propose an abstract syntax tree-based analysis to identify the syntactic characteristics of bug fixes. Our results show that 53% of the bugs in OpenTitan have potential security implications and that 55% of all bug fixes modify only one file. Our findings underscore the importance of security-aware development practices and tools and motivate the development of techniques that leverage the highly localized nature of hardware bugs.","sentences":["Hardware security is an important concern of system security as vulnerabilities can arise from design errors introduced throughout the development lifecycle.","Recent works have proposed techniques to detect hardware security bugs, such as static analysis, fuzzing, and symbolic execution.","However, the fundamental properties of hardware security bugs remain relatively unexplored.","To gain a better understanding of hardware security bugs, we perform a deep dive into the popular OpenTitan project, including its bug reports and bug fixes.","We manually classify the bugs as relevant to functionality or security and analyze characteristics, such as the impact and location of security bugs, and the size of their bug fixes.","We also investigate relationships between security impact and bug management during development.","Finally, we propose an abstract syntax tree-based analysis to identify the syntactic characteristics of bug fixes.","Our results show that 53% of the bugs in OpenTitan have potential security implications and that 55% of all bug fixes modify only one file.","Our findings underscore the importance of security-aware development practices and tools and motivate the development of techniques that leverage the highly localized nature of hardware bugs."],"url":"http://arxiv.org/abs/2402.00684v1","category":"cs.CR"}
{"created":"2024-02-01 15:46:04","title":"WayFASTER: a Self-Supervised Traversability Prediction for Increased Navigation Awareness","abstract":"Accurate and robust navigation in unstructured environments requires fusing data from multiple sensors. Such fusion ensures that the robot is better aware of its surroundings, including areas of the environment that are not immediately visible, but were visible at a different time. To solve this problem, we propose a method for traversability prediction in challenging outdoor environments using a sequence of RGB and depth images fused with pose estimations. Our method, termed WayFASTER (Waypoints-Free Autonomous System for Traversability with Enhanced Robustness), uses experience data recorded from a receding horizon estimator to train a self-supervised neural network for traversability prediction, eliminating the need for heuristics. Our experiments demonstrate that our method excels at avoiding geometric obstacles, and correctly detects that traversable terrains, such as tall grass, can be navigable. By using a sequence of images, WayFASTER significantly enhances the robot's awareness of its surroundings, enabling it to predict the traversability of terrains that are not immediately visible. This enhanced awareness contributes to better navigation performance in environments where such predictive capabilities are essential.","sentences":["Accurate and robust navigation in unstructured environments requires fusing data from multiple sensors.","Such fusion ensures that the robot is better aware of its surroundings, including areas of the environment that are not immediately visible, but were visible at a different time.","To solve this problem, we propose a method for traversability prediction in challenging outdoor environments using a sequence of RGB and depth images fused with pose estimations.","Our method, termed WayFASTER (Waypoints-Free Autonomous System for Traversability with Enhanced Robustness), uses experience data recorded from a receding horizon estimator to train a self-supervised neural network for traversability prediction, eliminating the need for heuristics.","Our experiments demonstrate that our method excels at avoiding geometric obstacles, and correctly detects that traversable terrains, such as tall grass, can be navigable.","By using a sequence of images, WayFASTER significantly enhances the robot's awareness of its surroundings, enabling it to predict the traversability of terrains that are not immediately visible.","This enhanced awareness contributes to better navigation performance in environments where such predictive capabilities are essential."],"url":"http://arxiv.org/abs/2402.00683v1","category":"cs.RO"}
{"created":"2024-02-01 15:38:21","title":"Real Evaluations Tractability using Continuous Goal-Directed Actions in Smart City Applications","abstract":"One of the most important challenges of Smart City Applications is to adapt the system to interact with non-expert users. Robot imitation frameworks aim to simplify and reduce times of robot programming by allowing users to program directly through demonstrations. In classical frameworks, actions are modeled using joint or Cartesian space trajectories. Other features, such as visual ones, are not always well represented with these pure geometrical approaches. Continuous Goal-Directed Actions (CGDA) is an alternative to these methods, as it encodes actions as changes of any feature that can be extracted from the environment. As a consequence of this, the robot joint trajectories for execution must be fully computed to comply with this feature-agnostic encoding. This is achieved using Evolutionary Algorithms (EA), which usually requires too many evaluations to perform this evolution step in the actual robot. Current strategies involve performing evaluations in a simulation, transferring the final joint trajectory to the actual robot. Smart City applications involve working in highly dynamic and complex environments, where having a precise model is not always achievable. Our goal is to study the tractability of performing these evaluations directly in a real-world scenario. Two different approaches to reduce the number of evaluations using EA, are proposed and compared. In the first approach, Particle Swarm Optimization (PSO)-based methods have been studied and compared within CGDA: naive PSO, Fitness Inheritance PSO (FI-PSO), and Adaptive Fuzzy Fitness Granulation with PSO (AFFG-PSO). The second approach studied the introduction of geometrical and velocity constraints within CGDA. The effects of both approaches were analyzed and compared in the wax and paint actions, two CGDA commonly studied use cases. Results from this paper depict an important reduction in the number of evaluations.","sentences":["One of the most important challenges of Smart City Applications is to adapt the system to interact with non-expert users.","Robot imitation frameworks aim to simplify and reduce times of robot programming by allowing users to program directly through demonstrations.","In classical frameworks, actions are modeled using joint or Cartesian space trajectories.","Other features, such as visual ones, are not always well represented with these pure geometrical approaches.","Continuous Goal-Directed Actions (CGDA) is an alternative to these methods, as it encodes actions as changes of any feature that can be extracted from the environment.","As a consequence of this, the robot joint trajectories for execution must be fully computed to comply with this feature-agnostic encoding.","This is achieved using Evolutionary Algorithms (EA), which usually requires too many evaluations to perform this evolution step in the actual robot.","Current strategies involve performing evaluations in a simulation, transferring the final joint trajectory to the actual robot.","Smart City applications involve working in highly dynamic and complex environments, where having a precise model is not always achievable.","Our goal is to study the tractability of performing these evaluations directly in a real-world scenario.","Two different approaches to reduce the number of evaluations using EA, are proposed and compared.","In the first approach, Particle Swarm Optimization (PSO)-based methods have been studied and compared within CGDA: naive PSO, Fitness Inheritance PSO (FI-PSO), and Adaptive Fuzzy Fitness Granulation with PSO (AFFG-PSO).","The second approach studied the introduction of geometrical and velocity constraints within CGDA.","The effects of both approaches were analyzed and compared in the wax and paint actions, two CGDA commonly studied use cases.","Results from this paper depict an important reduction in the number of evaluations."],"url":"http://arxiv.org/abs/2402.00678v1","category":"cs.RO"}
{"created":"2024-02-01 15:37:42","title":"Neural Policy Style Transfer","abstract":"Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories. We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure. Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content. The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter. Different policies are defined via Deep Q-Network architectures. These models are trained using demonstrations through Inverse Reinforcement Learning. Two different sets of user demonstrations are performed, one for content and other for style. Different styles are encoded as defined by user demonstrations. The generated policy is the result of feeding a content policy and a style policy to the NPST algorithm. Experiments are performed in a catch-ball game inspired by the Deep Reinforcement Learning classical Atari games; and a real-world painting scenario with a full-sized humanoid robot, based on previous works of the authors. The implementation of three different Q-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode the policies within the NPST framework is proposed and the results obtained in the experiments with each of these architectures compared.","sentences":["Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories.","We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure.","Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content.","The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style.","The Neural Policy Style Transfer (NPST) algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter.","Different policies are defined via Deep Q-Network architectures.","These models are trained using demonstrations through Inverse Reinforcement Learning.","Two different sets of user demonstrations are performed, one for content and other for style.","Different styles are encoded as defined by user demonstrations.","The generated policy is the result of feeding a content policy and a style policy to the NPST algorithm.","Experiments are performed in a catch-ball game inspired by the Deep Reinforcement Learning classical Atari games; and a real-world painting scenario with a full-sized humanoid robot, based on previous works of the authors.","The implementation of three different Q-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode the policies within the NPST framework is proposed and the results obtained in the experiments with each of these architectures compared."],"url":"http://arxiv.org/abs/2402.00677v1","category":"cs.RO"}
{"created":"2024-02-01 15:37:23","title":"Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching","abstract":"The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science. Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments. In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications. Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics. DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments. Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments. In this work, the introduction of DQN within an art painting robot application is proposed. The goal is to study how the introduction of a complex control policy impacts the performance of a basic art painting robot application. The main expected contribution of this work is to serve as a first baseline for future works introducing DQN methods for complex art painting robot frameworks. Experiments consist of real world executions of human drawn sketches using the DQN generated policy and TEO, the humanoid robot. Results are compared in terms of similarity and obtained reward with respect to the reference inputs","sentences":["The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science.","Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments.","In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications.","Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics.","DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments.","Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments.","In this work, the introduction of DQN within an art painting robot application is proposed.","The goal is to study how the introduction of a complex control policy impacts the performance of a basic art painting robot application.","The main expected contribution of this work is to serve as a first baseline for future works introducing DQN methods for complex art painting robot frameworks.","Experiments consist of real world executions of human drawn sketches using the DQN generated policy and TEO, the humanoid robot.","Results are compared in terms of similarity and obtained reward with respect to the reference inputs"],"url":"http://arxiv.org/abs/2402.00676v1","category":"cs.RO"}
{"created":"2024-02-01 15:34:59","title":"On Modular Algorithms and Butterfly Operations in Number Theoretic Transform","abstract":"Number theoretic transform (NTT) has been a very useful tool in computations for number theory, algebra and cryptography. Its performance affects some post-quantum cryptosystems. In this paper, we discuss the butterfly operation of NTT. This basic module of NTT requires heavy modular arithmetics. Montgomery reduction is commonly used in this setting. Recently several variants of Montgomery algorithm have been proposed for the purpose of speeding up NTT. We observe that the Chinese remainder theorem (CRT) can be involved in this type of algorithms in nature and transparent ways. In this paper, a framework of using CRT to model Montgomery type algorithms is described. The derivation of these algorithms as well as their correctness are all treated in the CRT framework. Under our approach, some problems of a modular reduction algorithm (published in IACR Transactions on Cryptographic Hardware and Embedded Systems, doi:10.46586/tches.v2022.i4.614-636 ) are identified, and a counterexample is generated to show that the algorithm is incorrect.","sentences":["Number theoretic transform (NTT) has been a very useful tool in computations for number theory, algebra and cryptography.","Its performance affects some post-quantum cryptosystems.","In this paper, we discuss the butterfly operation of NTT.","This basic module of NTT requires heavy modular arithmetics.","Montgomery reduction is commonly used in this setting.","Recently several variants of Montgomery algorithm have been proposed for the purpose of speeding up NTT.","We observe that the Chinese remainder theorem (CRT) can be involved in this type of algorithms in nature and transparent ways.","In this paper, a framework of using CRT to model Montgomery type algorithms is described.","The derivation of these algorithms as well as their correctness are all treated in the CRT framework.","Under our approach, some problems of a modular reduction algorithm (published in IACR Transactions on Cryptographic Hardware and Embedded Systems, doi:10.46586/tches.v2022.i4.614-636 ) are identified, and a counterexample is generated to show that the algorithm is incorrect."],"url":"http://arxiv.org/abs/2402.00675v1","category":"cs.CR"}
{"created":"2024-02-01 15:33:17","title":"Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID","abstract":"Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the impact of noisy pseudo-labels while simultaneously aligning different modalities, coupled with a Modality-Invariant Representation Learning (MIRL) framework. Experiments demonstrate that our proposed method outperforms existing USL-VI-ReID methods, highlighting the superiority of our MULT in comparison to other cross-modality association methods. The code will be available.","sentences":["Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations.","While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations.","In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations.","It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures.","Additionally, a straightforward plug-and-play Online Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the impact of noisy pseudo-labels while simultaneously aligning different modalities, coupled with a Modality-Invariant Representation Learning (MIRL) framework.","Experiments demonstrate that our proposed method outperforms existing USL-VI-ReID methods, highlighting the superiority of our MULT in comparison to other cross-modality association methods.","The code will be available."],"url":"http://arxiv.org/abs/2402.00672v1","category":"cs.CV"}
{"created":"2024-02-01 15:30:19","title":"Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning","abstract":"This paper presents a follow-up study to OpenAI's recent superalignment work on Weak-to-Strong Generalization (W2SG). Superalignment focuses on ensuring that high-level AI systems remain consistent with human values and intentions when dealing with complex, high-risk tasks. The W2SG framework has opened new possibilities for empirical research in this evolving field. Our study simulates two phases of superalignment under the W2SG framework: the development of general superhuman models and the progression towards superintelligence. In the first phase, based on human supervision, the quality of weak supervision is enhanced through a combination of scalable oversight and ensemble learning, reducing the capability gap between weak teachers and strong students. In the second phase, an automatic alignment evaluator is employed as the weak supervisor. By recursively updating this auto aligner, the capabilities of the weak teacher models are synchronously enhanced, achieving weak-to-strong supervision over stronger student models.We also provide an initial validation of the proposed approach for the first phase. Using the SciQ task as example, we explore ensemble learning for weak teacher models through bagging and boosting. Scalable oversight is explored through two auxiliary settings: human-AI interaction and AI-AI debate. Additionally, the paper discusses the impact of improved weak supervision on enhancing weak-to-strong generalization based on in-context learning. Experiment code and dataset will be released at https://github.com/ADaM-BJTU/W2SG.","sentences":["This paper presents a follow-up study to OpenAI's recent superalignment work on Weak-to-Strong Generalization (W2SG).","Superalignment focuses on ensuring that high-level AI systems remain consistent with human values and intentions when dealing with complex, high-risk tasks.","The W2SG framework has opened new possibilities for empirical research in this evolving field.","Our study simulates two phases of superalignment under the W2SG framework: the development of general superhuman models and the progression towards superintelligence.","In the first phase, based on human supervision, the quality of weak supervision is enhanced through a combination of scalable oversight and ensemble learning, reducing the capability gap between weak teachers and strong students.","In the second phase, an automatic alignment evaluator is employed as the weak supervisor.","By recursively updating this auto aligner, the capabilities of the weak teacher models are synchronously enhanced, achieving weak-to-strong supervision over stronger student models.","We also provide an initial validation of the proposed approach for the first phase.","Using the SciQ task as example, we explore ensemble learning for weak teacher models through bagging and boosting.","Scalable oversight is explored through two auxiliary settings: human-AI interaction and AI-AI debate.","Additionally, the paper discusses the impact of improved weak supervision on enhancing weak-to-strong generalization based on in-context learning.","Experiment code and dataset will be released at https://github.com/ADaM-BJTU/W2SG."],"url":"http://arxiv.org/abs/2402.00667v1","category":"cs.CL"}
{"created":"2024-02-01 15:26:09","title":"Revising Apetrei's bounding volume hierarchy construction algorithm to allow stackless traversal","abstract":"Stackless traversal is a technique to speed up range queries by avoiding usage of a stack during the tree traversal. One way to achieve that is to transform a given binary tree to store a left child and a skip-connection (also called an escape index). In general, this operation requires an additional tree traversal during the tree construction. For some tree structures, however, it is possible achieve the same result at a reduced cost. We propose one such algorithm for a GPU hierarchy construction algorithm proposed by Karras in [Karras, 2012]. Furthermore, we show that our algorithm also works with the improved algorithm proposed by Apetrei in [Apetrei, 2014], despite a different ordering of the internal nodes. We achieve that by modifying the Apetrei's algorithm to restore the original Karras' ordering of the internal nodes. Using the modified algorithm, we show how to construct a hierarchy suitable for a stackless traversal in a single bottom-up pass.","sentences":["Stackless traversal is a technique to speed up range queries by avoiding usage of a stack during the tree traversal.","One way to achieve that is to transform a given binary tree to store a left child and a skip-connection (also called an escape index).","In general, this operation requires an additional tree traversal during the tree construction.","For some tree structures, however, it is possible achieve the same result at a reduced cost.","We propose one such algorithm for a GPU hierarchy construction algorithm proposed by Karras in [Karras, 2012].","Furthermore, we show that our algorithm also works with the improved algorithm proposed by Apetrei in [Apetrei, 2014], despite a different ordering of the internal nodes.","We achieve that by modifying the Apetrei's algorithm to restore the original Karras' ordering of the internal nodes.","Using the modified algorithm, we show how to construct a hierarchy suitable for a stackless traversal in a single bottom-up pass."],"url":"http://arxiv.org/abs/2402.00665v1","category":"cs.DS"}
{"created":"2024-02-01 15:23:31","title":"Transferring human emotions to robot motions using Neural Policy Style Transfer","abstract":"Neural Style Transfer (NST) was originally proposed to use feature extraction capabilities of Neural Networks as a way to perform Style Transfer with images. Pre-trained image classification architectures were selected for feature extraction, leading to new images showing the same content as the original but with a different style. In robotics, Style Transfer can be employed to transfer human motion styles to robot motions. The challenge lies in the lack of pre-trained classification architectures for robot motions that could be used for feature extraction. Neural Policy Style Transfer TD3 (NPST3) is proposed for the transfer of human motion styles to robot motions. This framework allows the same robot motion to be executed in different human-centered motion styles, such as in an angry, happy, calm, or sad fashion. The Twin Delayed Deep Deterministic Policy Gradient (TD3) network is introduced for the generation of control policies. An autoencoder network is in charge of feature extraction for the Style Transfer step. The Style Transfer step can be performed both offline and online: offline for the autonomous executions of human-style robot motions, and online for adapting at runtime the style of e.g., a teleoperated robot. The framework is tested using two different robotic platforms: a robotic manipulator designed for telemanipulation tasks, and a humanoid robot designed for social interaction. The proposed approach was evaluated for both platforms, performing a total of 147 questionnaires asking human subjects to recognize the human motion style transferred to the robot motion for a predefined set of actions.","sentences":["Neural Style Transfer (NST) was originally proposed to use feature extraction capabilities of Neural Networks as a way to perform Style Transfer with images.","Pre-trained image classification architectures were selected for feature extraction, leading to new images showing the same content as the original but with a different style.","In robotics, Style Transfer can be employed to transfer human motion styles to robot motions.","The challenge lies in the lack of pre-trained classification architectures for robot motions that could be used for feature extraction.","Neural Policy Style Transfer TD3 (NPST3) is proposed for the transfer of human motion styles to robot motions.","This framework allows the same robot motion to be executed in different human-centered motion styles, such as in an angry, happy, calm, or sad fashion.","The Twin Delayed Deep Deterministic Policy Gradient (TD3) network is introduced for the generation of control policies.","An autoencoder network is in charge of feature extraction for the Style Transfer step.","The Style Transfer step can be performed both offline and online: offline for the autonomous executions of human-style robot motions, and online for adapting at runtime the style of e.g., a teleoperated robot.","The framework is tested using two different robotic platforms: a robotic manipulator designed for telemanipulation tasks, and a humanoid robot designed for social interaction.","The proposed approach was evaluated for both platforms, performing a total of 147 questionnaires asking human subjects to recognize the human motion style transferred to the robot motion for a predefined set of actions."],"url":"http://arxiv.org/abs/2402.00663v1","category":"cs.RO"}
{"created":"2024-02-01 15:20:43","title":"The spectral boundary of the Asymmetric Simple Exclusion Process (ASEP) -- free fermions, Bethe ansatz and random matrix theory","abstract":"In non-equilibrium statistical mechanics, the Asymmetric Simple Exclusion Process (ASEP) serves as a paradigmatic example. We investigate the spectral characteristics of the ASEP, focusing on the spectral boundary of its generator matrix. We examine finite ASEP chains of length $L$, under periodic (pbc) and open boundary conditions (obc). Notably, the spectral boundary exhibits $L$ spikes for pbc and $L+1$ spikes for obc. Treating the ASEP generator as an interacting non-Hermitian fermionic model, we extend the model to have tunable interaction. In the non-interacting case, the analytically computed many-body spectrum shows a spectral boundary with prominent spikes. For pbc, we use the coordinate Bethe ansatz to interpolate between the noninteracting case to the ASEP limit, and show that these spikes stem from clustering of Bethe roots. The robustness of the spikes in the spectral boundary is demonstrated by linking the ASEP generator to random matrices with trace correlations or, equivalently, random graphs with distinct cycle structures, both displaying similar spiked spectral boundaries.","sentences":["In non-equilibrium statistical mechanics, the Asymmetric Simple Exclusion Process (ASEP) serves as a paradigmatic example.","We investigate the spectral characteristics of the ASEP, focusing on the spectral boundary of its generator matrix.","We examine finite ASEP chains of length $L$, under periodic (pbc) and open boundary conditions (obc).","Notably, the spectral boundary exhibits $L$ spikes for pbc and $L+1$ spikes for obc.","Treating the ASEP generator as an interacting non-Hermitian fermionic model, we extend the model to have tunable interaction.","In the non-interacting case, the analytically computed many-body spectrum shows a spectral boundary with prominent spikes.","For pbc, we use the coordinate Bethe ansatz to interpolate between the noninteracting case to the ASEP limit, and show that these spikes stem from clustering of Bethe roots.","The robustness of the spikes in the spectral boundary is demonstrated by linking the ASEP generator to random matrices with trace correlations or, equivalently, random graphs with distinct cycle structures, both displaying similar spiked spectral boundaries."],"url":"http://arxiv.org/abs/2402.00662v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-01 15:18:48","title":"Modeling Freight Mode Choice Using Machine Learning Classifiers: A Comparative Study Using the Commodity Flow Survey (CFS) Data","abstract":"This study explores the usefulness of machine learning classifiers for modeling freight mode choice. We investigate eight commonly used machine learning classifiers, namely Naive Bayes, Support Vector Machine, Artificial Neural Network, K-Nearest Neighbors, Classification and Regression Tree, Random Forest, Boosting and Bagging, along with the classical Multinomial Logit model. US 2012 Commodity Flow Survey data are used as the primary data source; we augment it with spatial attributes from secondary data sources. The performance of the classifiers is compared based on prediction accuracy results. The current research also examines the role of sample size and training-testing data split ratios on the predictive ability of the various approaches. In addition, the importance of variables is estimated to determine how the variables influence freight mode choice. The results show that the tree-based ensemble classifiers perform the best. Specifically, Random Forest produces the most accurate predictions, closely followed by Boosting and Bagging. With regard to variable importance, shipment characteristics, such as shipment distance, industry classification of the shipper and shipment size, are the most significant factors for freight mode choice decisions.","sentences":["This study explores the usefulness of machine learning classifiers for modeling freight mode choice.","We investigate eight commonly used machine learning classifiers, namely Naive Bayes, Support Vector Machine, Artificial Neural Network, K-Nearest Neighbors, Classification and Regression Tree, Random Forest, Boosting and Bagging, along with the classical Multinomial Logit model.","US 2012 Commodity Flow Survey data are used as the primary data source; we augment it with spatial attributes from secondary data sources.","The performance of the classifiers is compared based on prediction accuracy results.","The current research also examines the role of sample size and training-testing data split ratios on the predictive ability of the various approaches.","In addition, the importance of variables is estimated to determine how the variables influence freight mode choice.","The results show that the tree-based ensemble classifiers perform the best.","Specifically, Random Forest produces the most accurate predictions, closely followed by Boosting and Bagging.","With regard to variable importance, shipment characteristics, such as shipment distance, industry classification of the shipper and shipment size, are the most significant factors for freight mode choice decisions."],"url":"http://arxiv.org/abs/2402.00659v1","category":"cs.LG"}
{"created":"2024-02-01 15:18:33","title":"Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing","abstract":"Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.","sentences":["Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation.","However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process.","Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales.","Some approaches model reasoning as planning, while others focus on annotating for process supervision.","Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space.","Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training.","To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards.","Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo."],"url":"http://arxiv.org/abs/2402.00658v1","category":"cs.AI"}
{"created":"2024-02-01 15:18:19","title":"Pre-training by Predicting Program Dependencies for Vulnerability Analysis Tasks","abstract":"Vulnerability analysis is crucial for software security. This work focuses on using pre-training techniques to enhance the understanding of vulnerable code and boost vulnerability analysis. The code understanding ability of a pre-trained model is highly related to its pre-training objectives. The semantic structure, e.g., control and data dependencies, of code is important for vulnerability analysis. However, existing pre-training objectives either ignore such structure or focus on learning to use it. The feasibility and benefits of learning the knowledge of analyzing semantic structure have not been investigated. To this end, this work proposes two novel pre-training objectives, namely Control Dependency Prediction (CDP) and Data Dependency Prediction (DDP), which aim to predict the statement-level control dependencies and token-level data dependencies, respectively, in a code snippet only based on its source code. During pre-training, CDP and DDP can guide the model to learn the knowledge required for analyzing fine-grained dependencies in code. After pre-training, the pre-trained model can boost the understanding of vulnerable code during fine-tuning and can directly be used to perform dependence analysis for both partial and complete functions. To demonstrate the benefits of our pre-training objectives, we pre-train a Transformer model named PDBERT with CDP and DDP, fine-tune it on three vulnerability analysis tasks, i.e., vulnerability detection, vulnerability classification, and vulnerability assessment, and also evaluate it on program dependence analysis. Experimental results show that PDBERT benefits from CDP and DDP, leading to state-of-the-art performance on the three downstream tasks. Also, PDBERT achieves F1-scores of over 99% and 94% for predicting control and data dependencies, respectively, in partial and complete functions.","sentences":["Vulnerability analysis is crucial for software security.","This work focuses on using pre-training techniques to enhance the understanding of vulnerable code and boost vulnerability analysis.","The code understanding ability of a pre-trained model is highly related to its pre-training objectives.","The semantic structure, e.g., control and data dependencies, of code is important for vulnerability analysis.","However, existing pre-training objectives either ignore such structure or focus on learning to use it.","The feasibility and benefits of learning the knowledge of analyzing semantic structure have not been investigated.","To this end, this work proposes two novel pre-training objectives, namely Control Dependency Prediction (CDP) and Data Dependency Prediction (DDP), which aim to predict the statement-level control dependencies and token-level data dependencies, respectively, in a code snippet only based on its source code.","During pre-training, CDP and DDP can guide the model to learn the knowledge required for analyzing fine-grained dependencies in code.","After pre-training, the pre-trained model can boost the understanding of vulnerable code during fine-tuning and can directly be used to perform dependence analysis for both partial and complete functions.","To demonstrate the benefits of our pre-training objectives, we pre-train a Transformer model named PDBERT with CDP and DDP, fine-tune it on three vulnerability analysis tasks, i.e., vulnerability detection, vulnerability classification, and vulnerability assessment, and also evaluate it on program dependence analysis.","Experimental results show that PDBERT benefits from CDP and DDP, leading to state-of-the-art performance on the three downstream tasks.","Also, PDBERT achieves F1-scores of over 99% and 94% for predicting control and data dependencies, respectively, in partial and complete functions."],"url":"http://arxiv.org/abs/2402.00657v1","category":"cs.SE"}
{"created":"2024-02-01 15:14:16","title":"Improving the accuracy of freight mode choice models: A case study using the 2017 CFS PUF data set and ensemble learning techniques","abstract":"The US Census Bureau has collected two rounds of experimental data from the Commodity Flow Survey, providing shipment-level characteristics of nationwide commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017 (i.e., Public Use File). With this information, data-driven methods have become increasingly valuable for understanding detailed patterns in freight logistics. In this study, we used the 2017 Commodity Flow Survey Public Use File data set to explore building a high-performance freight mode choice model, considering three main improvements: (1) constructing local models for each separate commodity/industry category; (2) extracting useful geographical features, particularly the derived distance of each freight mode between origin/destination zones; and (3) applying additional ensemble learning methods such as stacking or voting to combine results from local and unified models for improved performance. The proposed method achieved over 92% accuracy without incorporating external information, an over 19% increase compared to directly fitting Random Forests models over 10,000 samples. Furthermore, SHAP (Shapely Additive Explanations) values were computed to explain the outputs and major patterns obtained from the proposed model. The model framework could enhance the performance and interpretability of existing freight mode choice models.","sentences":["The US Census Bureau has collected two rounds of experimental data from the Commodity Flow Survey, providing shipment-level characteristics of nationwide commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017 (i.e., Public Use File).","With this information, data-driven methods have become increasingly valuable for understanding detailed patterns in freight logistics.","In this study, we used the 2017 Commodity Flow Survey Public Use File data set to explore building a high-performance freight mode choice model, considering three main improvements: (1) constructing local models for each separate commodity/industry category; (2) extracting useful geographical features, particularly the derived distance of each freight mode between origin/destination zones; and (3) applying additional ensemble learning methods such as stacking or voting to combine results from local and unified models for improved performance.","The proposed method achieved over 92% accuracy without incorporating external information, an over 19% increase compared to directly fitting Random Forests models over 10,000 samples.","Furthermore, SHAP (Shapely Additive Explanations) values were computed to explain the outputs and major patterns obtained from the proposed model.","The model framework could enhance the performance and interpretability of existing freight mode choice models."],"url":"http://arxiv.org/abs/2402.00654v1","category":"cs.LG"}
{"created":"2024-02-01 15:13:26","title":"Coherent Feed Forward Quantum Neural Network","abstract":"Quantum machine learning, focusing on quantum neural networks (QNNs), remains a vastly uncharted field of study. Current QNN models primarily employ variational circuits on an ansatz or a quantum feature map, often requiring multiple entanglement layers. This methodology not only increases the computational cost of the circuit beyond what is practical on near-term quantum devices but also misleadingly labels these models as neural networks, given their divergence from the structure of a typical feed-forward neural network (FFNN). Moreover, the circuit depth and qubit needs of these models scale poorly with the number of data features, resulting in an efficiency challenge for real-world machine-learning tasks. We introduce a bona fide QNN model, which seamlessly aligns with the versatility of a traditional FFNN in terms of its adaptable intermediate layers and nodes, absent from intermediate measurements such that our entire model is coherent. This model stands out with its reduced circuit depth and number of requisite C-NOT gates to outperform prevailing QNN models. Furthermore, the qubit count in our model remains unaffected by the data's feature quantity. We test our proposed model on various benchmarking datasets such as the diagnostic breast cancer (Wisconsin) and credit card fraud detection datasets. We compare the outcomes of our model with the existing QNN methods to showcase the advantageous efficacy of our approach, even with a reduced requirement on quantum resources. Our model paves the way for application of quantum neural networks to real relevant machine learning problems.","sentences":["Quantum machine learning, focusing on quantum neural networks (QNNs), remains a vastly uncharted field of study.","Current QNN models primarily employ variational circuits on an ansatz or a quantum feature map, often requiring multiple entanglement layers.","This methodology not only increases the computational cost of the circuit beyond what is practical on near-term quantum devices but also misleadingly labels these models as neural networks, given their divergence from the structure of a typical feed-forward neural network (FFNN).","Moreover, the circuit depth and qubit needs of these models scale poorly with the number of data features, resulting in an efficiency challenge for real-world machine-learning tasks.","We introduce a bona fide QNN model, which seamlessly aligns with the versatility of a traditional FFNN in terms of its adaptable intermediate layers and nodes, absent from intermediate measurements such that our entire model is coherent.","This model stands out with its reduced circuit depth and number of requisite C-NOT gates to outperform prevailing QNN models.","Furthermore, the qubit count in our model remains unaffected by the data's feature quantity.","We test our proposed model on various benchmarking datasets such as the diagnostic breast cancer (Wisconsin) and credit card fraud detection datasets.","We compare the outcomes of our model with the existing QNN methods to showcase the advantageous efficacy of our approach, even with a reduced requirement on quantum resources.","Our model paves the way for application of quantum neural networks to real relevant machine learning problems."],"url":"http://arxiv.org/abs/2402.00653v1","category":"quant-ph"}
{"created":"2024-02-01 15:13:14","title":"Polycube Layouts via Iterative Dual Loops","abstract":"Polycube layouts for 3D models effectively support a wide variety of methods such as hex-mesh construction, seamless texture mapping, spline fitting, and multi-block grid generation. Our study of polycube layouts is motivated by conformal mesh generation for aerospace modelling. In this setting, quality and correctness guarantees are of the utmost importance. However, currently the fully automatic construction of valid polycube layouts still poses significant challenges: state-of-the-art methods are generally not guaranteed to return a proper solution, even after post-processing, or they use a prohibitively large number of voxels that add detail indiscriminately.   In this paper we present a robust, flexible, and efficient method to generate polycube layouts. Our approach is based on a dual representation for polycube layouts and builds a layout by iteratively adding dual loops. Our construction is robust by design: at any iterative step we maintain a valid polycube layout. We offer the flexibility of manual intervention if the user so desires: while our method is able to compute a complete polycube layout without user intervention, the user can interrupt after each iteration and target further refinement on both the local and the global level. Last but not least, our method is efficient and can be implemented using comparatively simple algorithmic building blocks. Our implementation is publicly available and we present its output for numerous benchmark models.","sentences":["Polycube layouts for 3D models effectively support a wide variety of methods such as hex-mesh construction, seamless texture mapping, spline fitting, and multi-block grid generation.","Our study of polycube layouts is motivated by conformal mesh generation for aerospace modelling.","In this setting, quality and correctness guarantees are of the utmost importance.","However, currently the fully automatic construction of valid polycube layouts still poses significant challenges: state-of-the-art methods are generally not guaranteed to return a proper solution, even after post-processing, or they use a prohibitively large number of voxels that add detail indiscriminately.   ","In this paper we present a robust, flexible, and efficient method to generate polycube layouts.","Our approach is based on a dual representation for polycube layouts and builds a layout by iteratively adding dual loops.","Our construction is robust by design: at any iterative step we maintain a valid polycube layout.","We offer the flexibility of manual intervention if the user so desires: while our method is able to compute a complete polycube layout without user intervention, the user can interrupt after each iteration and target further refinement on both the local and the global level.","Last but not least, our method is efficient and can be implemented using comparatively simple algorithmic building blocks.","Our implementation is publicly available and we present its output for numerous benchmark models."],"url":"http://arxiv.org/abs/2402.00652v1","category":"cs.GR"}
{"created":"2024-02-01 15:12:00","title":"Improving the Representativeness of Simulation Intervals for the Cache Memory System","abstract":"Accurate simulation techniques are indispensable to efficiently propose new memory or architectural organizations. As implementing new hardware concepts in real systems is often not feasible, cycle-accurate simulators employed together with certain benchmarks are commonly used. However, detailed simulators may take too much time to execute these programs until completion. Therefore, several techniques aimed at reducing this time are usually employed. These schemes select fragments of the source code considered as representative of the entire application's behaviour -- mainly in terms of performance, but not plenty considering the behaviour of cache memory levels -- and only these intervals are simulated. Our hypothesis is that the different simulation windows currently employed when evaluating microarchitectural proposals, especially those involving the last level cache (LLC), do not reproduce the overall cache behaviour during the entire execution, potentially leading to wrong conclusions on the real performance of the proposals assessed. In this work, we first demonstrate this hypothesis by evaluating different cache replacement policies using various typical simulation approaches. Consequently, we also propose a simulation strategy, based on the applications' LLC activity, which mimics the overall behaviour of the cache much closer than conventional simulation intervals. Our proposal allows a fairer comparison between cache-related approaches as it reports, on average, a number of changes in the relative order among the policies assessed -- with respect to the full simulation -- more than 30\\% lower than that of conventional strategies, maintaining the simulation time largely unchanged and without losing accuracy on performance terms, especially for memory-intensive applications.","sentences":["Accurate simulation techniques are indispensable to efficiently propose new memory or architectural organizations.","As implementing new hardware concepts in real systems is often not feasible, cycle-accurate simulators employed together with certain benchmarks are commonly used.","However, detailed simulators may take too much time to execute these programs until completion.","Therefore, several techniques aimed at reducing this time are usually employed.","These schemes select fragments of the source code considered as representative of the entire application's behaviour -- mainly in terms of performance, but not plenty considering the behaviour of cache memory levels -- and only these intervals are simulated.","Our hypothesis is that the different simulation windows currently employed when evaluating microarchitectural proposals, especially those involving the last level cache (LLC), do not reproduce the overall cache behaviour during the entire execution, potentially leading to wrong conclusions on the real performance of the proposals assessed.","In this work, we first demonstrate this hypothesis by evaluating different cache replacement policies using various typical simulation approaches.","Consequently, we also propose a simulation strategy, based on the applications' LLC activity, which mimics the overall behaviour of the cache much closer than conventional simulation intervals.","Our proposal allows a fairer comparison between cache-related approaches as it reports, on average, a number of changes in the relative order among the policies assessed -- with respect to the full simulation -- more than 30\\% lower than that of conventional strategies, maintaining the simulation time largely unchanged and without losing accuracy on performance terms, especially for memory-intensive applications."],"url":"http://arxiv.org/abs/2402.00649v1","category":"cs.AR"}
{"created":"2024-02-01 15:08:53","title":"Observational Viability of the Intermediate DBI Inflation in the Presence of a Minimal Length","abstract":"We consider an intermediate Dirac-Born-Infeld (DBI) inflationary model in the presence of a minimal measurable length in the theory. We show that, the presence of a minimal measurable length modifies the definitions of the scalar and tensor spectral indices and also other inflation observables. This is due to modification of the momentum and corresponding wave number of the perturbations in the presence of a minimal length. By using the deformed definition of the scalar and tensor spectral indices, we perform numerical analysis on the intermediate DBI inflation model to find some constraints on the deformation parameter. In this regard, we compare our numerical results with both Planck2018 TT, TE, EE +lowE +lensing +BAO+ BK14 and Planck2018 TT, TE,EE +lowE+lensing+BK14 +BAO+LIGO $\\&$ Virgo2016 data at the $68\\%$ CL and $95\\%$ CL. Our numerical study shows that the intermediate DBI inflation model in the presence of a minimal measurable length is observationally viable if the upper bound on the deformation parameter to be considered of the order of $10^{48}$ at $68\\%$ CL and $10^{49}$ at $95\\%$ CL. This is consistent with the results of other approaches to constrain such a quantity.","sentences":["We consider an intermediate Dirac-Born-Infeld (DBI) inflationary model in the presence of a minimal measurable length in the theory.","We show that, the presence of a minimal measurable length modifies the definitions of the scalar and tensor spectral indices and also other inflation observables.","This is due to modification of the momentum and corresponding wave number of the perturbations in the presence of a minimal length.","By using the deformed definition of the scalar and tensor spectral indices, we perform numerical analysis on the intermediate DBI inflation model to find some constraints on the deformation parameter.","In this regard, we compare our numerical results with both Planck2018 TT, TE, EE +lowE +lensing +BAO+ BK14 and Planck2018 TT, TE,EE +lowE+lensing+BK14 +BAO+LIGO $\\&$ Virgo2016 data at the $68\\%$ CL and $95\\%$ CL.","Our numerical study shows that the intermediate DBI inflation model in the presence of a minimal measurable length is observationally viable if the upper bound on the deformation parameter to be considered of the order of $10^{48}$ at $68\\%$ CL and $10^{49}$ at $95\\%$ CL.","This is consistent with the results of other approaches to constrain such a quantity."],"url":"http://arxiv.org/abs/2402.00647v1","category":"astro-ph.CO"}
{"created":"2024-02-01 15:08:41","title":"Cell-Free Massive MIMO SWIPT with Beyond Diagonal Reconfigurable Intelligent Surfaces","abstract":"This paper investigates the integration of beyond-diagonal reconfigurable intelligent surfaces (BD-RISs) into cell-free massive multiple-input multiple-output (CF-mMIMO) systems, focusing on applications involving simultaneous wireless information and power transfer (SWIPT). The system supports concurrently two user groups: information users (IUs) and energy users (EUs). A BD-RIS is employed to enhance the wireless power transfer (WPT) directed towards the EUs. To comprehensively evaluate the system's performance, we present an analytical framework for the spectral efficiency (SE) of IUs and the average harvested energy (HE) of EUs in the presence of spatial correlation among the BD-RIS elements and for a non-linear energy harvesting circuit. Our findings offer important insights into the transformative potential of BD-RIS, setting the stage for the development of more efficient and effective SWIPT networks. Finally, incorporating a heuristic scattering matrix design at the BD-RIS results in a substantial improvement compared to the scenario with random scattering matrix design.","sentences":["This paper investigates the integration of beyond-diagonal reconfigurable intelligent surfaces (BD-RISs) into cell-free massive multiple-input multiple-output (CF-mMIMO) systems, focusing on applications involving simultaneous wireless information and power transfer (SWIPT).","The system supports concurrently two user groups: information users (IUs) and energy users (EUs).","A BD-RIS is employed to enhance the wireless power transfer (WPT) directed towards the EUs.","To comprehensively evaluate the system's performance, we present an analytical framework for the spectral efficiency (SE) of IUs and the average harvested energy (HE) of EUs in the presence of spatial correlation among the BD-RIS elements and for a non-linear energy harvesting circuit.","Our findings offer important insights into the transformative potential of BD-RIS, setting the stage for the development of more efficient and effective SWIPT networks.","Finally, incorporating a heuristic scattering matrix design at the BD-RIS results in a substantial improvement compared to the scenario with random scattering matrix design."],"url":"http://arxiv.org/abs/2402.00646v1","category":"cs.IT"}
{"created":"2024-02-01 15:07:31","title":"Spectrally Transformed Kernel Regression","abstract":"Unlabeled data is a key component of modern machine learning. In general, the role of unlabeled data is to impose a form of smoothness, usually from the similarity information encoded in a base kernel, such as the $\\epsilon$-neighbor kernel or the adjacency matrix of a graph. This work revisits the classical idea of spectrally transformed kernel regression (STKR), and provides a new class of general and scalable STKR estimators able to leverage unlabeled data. Intuitively, via spectral transformation, STKR exploits the data distribution for which unlabeled data can provide additional information. First, we show that STKR is a principled and general approach, by characterizing a universal type of \"target smoothness\", and proving that any sufficiently smooth function can be learned by STKR. Second, we provide scalable STKR implementations for the inductive setting and a general transformation function, while prior work is mostly limited to the transductive setting. Third, we derive statistical guarantees for two scenarios: STKR with a known polynomial transformation, and STKR with kernel PCA when the transformation is unknown. Overall, we believe that this work helps deepen our understanding of how to work with unlabeled data, and its generality makes it easier to inspire new methods.","sentences":["Unlabeled data is a key component of modern machine learning.","In general, the role of unlabeled data is to impose a form of smoothness, usually from the similarity information encoded in a base kernel, such as the $\\epsilon$-neighbor kernel or the adjacency matrix of a graph.","This work revisits the classical idea of spectrally transformed kernel regression (STKR), and provides a new class of general and scalable STKR estimators able to leverage unlabeled data.","Intuitively, via spectral transformation, STKR exploits the data distribution for which unlabeled data can provide additional information.","First, we show that STKR is a principled and general approach, by characterizing a universal type of \"target smoothness\", and proving that any sufficiently smooth function can be learned by STKR.","Second, we provide scalable STKR implementations for the inductive setting and a general transformation function, while prior work is mostly limited to the transductive setting.","Third, we derive statistical guarantees for two scenarios: STKR with a known polynomial transformation, and STKR with kernel PCA when the transformation is unknown.","Overall, we believe that this work helps deepen our understanding of how to work with unlabeled data, and its generality makes it easier to inspire new methods."],"url":"http://arxiv.org/abs/2402.00645v1","category":"stat.ML"}
{"created":"2024-02-01 14:56:54","title":"Testing side-channel security of cryptographic implementations against future microarchitectures","abstract":"How will future microarchitectures impact the security of existing cryptographic implementations? As we cannot keep reducing the size of transistors, chip vendors have started developing new microarchitectural optimizations to speed up computation. A recent study (Sanchez Vicarte et al., ISCA 2021) suggests that these optimizations might open the Pandora's box of microarchitectural attacks. However, there is little guidance on how to evaluate the security impact of future optimization proposals.   To help chip vendors explore the impact of microarchitectural optimizations on cryptographic implementations, we develop (i) an expressive domain-specific language, called LmSpec, that allows them to specify the leakage model for the given optimization and (ii) a testing framework, called LmTest, to automatically detect leaks under the specified leakage model within the given implementation. Using this framework, we conduct an empirical study of 18 proposed microarchitectural optimizations on 25 implementations of eight cryptographic primitives in five popular libraries. We find that every implementation would contain secret-dependent leaks, sometimes sufficient to recover a victim's secret key, if these optimizations were realized. Ironically, some leaks are possible only because of coding idioms used to prevent leaks under the standard constant-time model.","sentences":["How will future microarchitectures impact the security of existing cryptographic implementations?","As we cannot keep reducing the size of transistors, chip vendors have started developing new microarchitectural optimizations to speed up computation.","A recent study (Sanchez Vicarte et al., ISCA 2021) suggests that these optimizations might open the Pandora's box of microarchitectural attacks.","However, there is little guidance on how to evaluate the security impact of future optimization proposals.   ","To help chip vendors explore the impact of microarchitectural optimizations on cryptographic implementations, we develop (i) an expressive domain-specific language, called LmSpec, that allows them to specify the leakage model for the given optimization and (ii) a testing framework, called LmTest, to automatically detect leaks under the specified leakage model within the given implementation.","Using this framework, we conduct an empirical study of 18 proposed microarchitectural optimizations on 25 implementations of eight cryptographic primitives in five popular libraries.","We find that every implementation would contain secret-dependent leaks, sometimes sufficient to recover a victim's secret key, if these optimizations were realized.","Ironically, some leaks are possible only because of coding idioms used to prevent leaks under the standard constant-time model."],"url":"http://arxiv.org/abs/2402.00641v1","category":"cs.CR"}
{"created":"2024-02-01 14:54:17","title":"Random Forest-Based Prediction of Stroke Outcome","abstract":"We research into the clinical, biochemical and neuroimaging factors associated with the outcome of stroke patients to generate a predictive model using machine learning techniques for prediction of mortality and morbidity 3 months after admission. The dataset consisted of patients with ischemic stroke (IS) and non-traumatic intracerebral hemorrhage (ICH) admitted to Stroke Unit of a European Tertiary Hospital prospectively registered. We identified the main variables for machine learning Random Forest (RF), generating a predictive model that can estimate patient mortality/morbidity. In conclusion, machine learning algorithms RF can be effectively used in stroke patients for long-term outcome prediction of mortality and morbidity.","sentences":["We research into the clinical, biochemical and neuroimaging factors associated with the outcome of stroke patients to generate a predictive model using machine learning techniques for prediction of mortality and morbidity 3 months after admission.","The dataset consisted of patients with ischemic stroke (IS) and non-traumatic intracerebral hemorrhage (ICH) admitted to Stroke Unit of a European Tertiary Hospital prospectively registered.","We identified the main variables for machine learning Random Forest (RF), generating a predictive model that can estimate patient mortality/morbidity.","In conclusion, machine learning algorithms RF can be effectively used in stroke patients for long-term outcome prediction of mortality and morbidity."],"url":"http://arxiv.org/abs/2402.00638v1","category":"cs.LG"}
{"created":"2024-02-01 14:52:16","title":"Fisheye Camera and Ultrasonic Sensor Fusion For Near-Field Obstacle Perception in Bird's-Eye-View","abstract":"Accurate obstacle identification represents a fundamental challenge within the scope of near-field perception for autonomous driving. Conventionally, fisheye cameras are frequently employed for comprehensive surround-view perception, including rear-view obstacle localization. However, the performance of such cameras can significantly deteriorate in low-light conditions, during nighttime, or when subjected to intense sun glare. Conversely, cost-effective sensors like ultrasonic sensors remain largely unaffected under these conditions. Therefore, we present, to our knowledge, the first end-to-end multimodal fusion model tailored for efficient obstacle perception in a bird's-eye-view (BEV) perspective, utilizing fisheye cameras and ultrasonic sensors. Initially, ResNeXt-50 is employed as a set of unimodal encoders to extract features specific to each modality. Subsequently, the feature space associated with the visible spectrum undergoes transformation into BEV. The fusion of these two modalities is facilitated via concatenation. At the same time, the ultrasonic spectrum-based unimodal feature maps pass through content-aware dilated convolution, applied to mitigate the sensor misalignment between two sensors in the fused feature space. Finally, the fused features are utilized by a two-stage semantic occupancy decoder to generate grid-wise predictions for precise obstacle perception. We conduct a systematic investigation to determine the optimal strategy for multimodal fusion of both sensors. We provide insights into our dataset creation procedures, annotation guidelines, and perform a thorough data analysis to ensure adequate coverage of all scenarios. When applied to our dataset, the experimental results underscore the robustness and effectiveness of our proposed multimodal fusion approach.","sentences":["Accurate obstacle identification represents a fundamental challenge within the scope of near-field perception for autonomous driving.","Conventionally, fisheye cameras are frequently employed for comprehensive surround-view perception, including rear-view obstacle localization.","However, the performance of such cameras can significantly deteriorate in low-light conditions, during nighttime, or when subjected to intense sun glare.","Conversely, cost-effective sensors like ultrasonic sensors remain largely unaffected under these conditions.","Therefore, we present, to our knowledge, the first end-to-end multimodal fusion model tailored for efficient obstacle perception in a bird's-eye-view (BEV) perspective, utilizing fisheye cameras and ultrasonic sensors.","Initially, ResNeXt-50 is employed as a set of unimodal encoders to extract features specific to each modality.","Subsequently, the feature space associated with the visible spectrum undergoes transformation into BEV.","The fusion of these two modalities is facilitated via concatenation.","At the same time, the ultrasonic spectrum-based unimodal feature maps pass through content-aware dilated convolution, applied to mitigate the sensor misalignment between two sensors in the fused feature space.","Finally, the fused features are utilized by a two-stage semantic occupancy decoder to generate grid-wise predictions for precise obstacle perception.","We conduct a systematic investigation to determine the optimal strategy for multimodal fusion of both sensors.","We provide insights into our dataset creation procedures, annotation guidelines, and perform a thorough data analysis to ensure adequate coverage of all scenarios.","When applied to our dataset, the experimental results underscore the robustness and effectiveness of our proposed multimodal fusion approach."],"url":"http://arxiv.org/abs/2402.00637v1","category":"cs.CV"}
{"created":"2024-02-01 14:51:51","title":"Double-scaled SYK, Chords and de Sitter Gravity","abstract":"We study the partition function of 3D de Sitter gravity defined as the trace over the Hilbert space obtained by quantizing the phase space of non-rotating Schwarzschild-de Sitter spacetime. Motivated by the correspondence with double scaled SYK, we identify the Hamiltonian with the gravitational Wilson-line that measures the conical deficit angle. We express the Hamiltonian in terms of canonical variables and find that it leads to the exact same chord rules and energy spectrum as the double scaled SYK model. We use the obtained match to compute the partition function and scalar two-point function in 3D de Sitter gravity.","sentences":["We study the partition function of 3D de Sitter gravity defined as the trace over the Hilbert space obtained by quantizing the phase space of non-rotating Schwarzschild-de Sitter spacetime.","Motivated by the correspondence with double scaled SYK, we identify the Hamiltonian with the gravitational Wilson-line that measures the conical deficit angle.","We express the Hamiltonian in terms of canonical variables and find that it leads to the exact same chord rules and energy spectrum as the double scaled SYK model.","We use the obtained match to compute the partition function and scalar two-point function in 3D de Sitter gravity."],"url":"http://arxiv.org/abs/2402.00635v1","category":"hep-th"}
{"created":"2024-02-01 14:46:35","title":"Prosody in Cascade and Direct Speech-to-Text Translation: a case study on Korean Wh-Phrases","abstract":"Speech-to-Text Translation (S2TT) has typically been addressed with cascade systems, where speech recognition systems generate a transcription that is subsequently passed to a translation model. While there has been a growing interest in developing direct speech translation systems to avoid propagating errors and losing non-verbal content, prior work in direct S2TT has struggled to conclusively establish the advantages of integrating the acoustic signal directly into the translation process. This work proposes using contrastive evaluation to quantitatively measure the ability of direct S2TT systems to disambiguate utterances where prosody plays a crucial role. Specifically, we evaluated Korean-English translation systems on a test set containing wh-phrases, for which prosodic features are necessary to produce translations with the correct intent, whether it's a statement, a yes/no question, a wh-question, and more. Our results clearly demonstrate the value of direct translation systems over cascade translation models, with a notable 12.9% improvement in overall accuracy in ambiguous cases, along with up to a 15.6% increase in F1 scores for one of the major intent categories. To the best of our knowledge, this work stands as the first to provide quantitative evidence that direct S2TT models can effectively leverage prosody. The code for our evaluation is openly accessible and freely available for review and utilisation.","sentences":["Speech-to-Text Translation (S2TT) has typically been addressed with cascade systems, where speech recognition systems generate a transcription that is subsequently passed to a translation model.","While there has been a growing interest in developing direct speech translation systems to avoid propagating errors and losing non-verbal content, prior work in direct S2TT has struggled to conclusively establish the advantages of integrating the acoustic signal directly into the translation process.","This work proposes using contrastive evaluation to quantitatively measure the ability of direct S2TT systems to disambiguate utterances where prosody plays a crucial role.","Specifically, we evaluated Korean-English translation systems on a test set containing wh-phrases, for which prosodic features are necessary to produce translations with the correct intent, whether it's a statement, a yes/no question, a wh-question, and more.","Our results clearly demonstrate the value of direct translation systems over cascade translation models, with a notable 12.9% improvement in overall accuracy in ambiguous cases, along with up to a 15.6% increase in F1 scores for one of the major intent categories.","To the best of our knowledge, this work stands as the first to provide quantitative evidence that direct S2TT models can effectively leverage prosody.","The code for our evaluation is openly accessible and freely available for review and utilisation."],"url":"http://arxiv.org/abs/2402.00632v1","category":"cs.CL"}
{"created":"2024-02-01 14:44:57","title":"Cocco: Hardware-Mapping Co-Exploration towards Memory Capacity-Communication Optimization","abstract":"Memory is a critical design consideration in current data-intensive DNN accelerators, as it profoundly determines energy consumption, bandwidth requirements, and area costs. As DNN structures become more complex, a larger on-chip memory capacity is required to reduce data movement overhead, but at the expense of silicon costs. Some previous works have proposed memory-oriented optimizations, such as different data reuse and layer fusion schemes. However, these methods are not general and potent enough to cope with various graph structures.   In this paper, we explore the intrinsic connection between network structures and memory features to optimize both hardware and mapping. First, we introduce a graph-level execution scheme with a corresponding dataflow and memory management method. This scheme enables the execution of arbitrary graph patterns with high data reuse and low hardware overhead. Subsequently, we propose Cocco, a hardware-mapping co-exploration framework leveraging graph-level features of networks. It aims to minimize communication overhead, such as energy consumption and bandwidth requirements, with a smaller memory capacity. We formulate the graph-partition scheduling and memory configuration search as an optimization problem and employ a genetic-based method to achieve efficient co-exploration for large and irregular networks. Experiments demonstrate that Cocco obtains lower external memory access, lower bandwidth requirements, and more stable optimization for graph partition compared to the greedy algorithm and dynamic programming introduced in prior works. Cocco also reduces the costs by 1.89% to 50.33% using co-exploration compared to other typical methods.","sentences":["Memory is a critical design consideration in current data-intensive DNN accelerators, as it profoundly determines energy consumption, bandwidth requirements, and area costs.","As DNN structures become more complex, a larger on-chip memory capacity is required to reduce data movement overhead, but at the expense of silicon costs.","Some previous works have proposed memory-oriented optimizations, such as different data reuse and layer fusion schemes.","However, these methods are not general and potent enough to cope with various graph structures.   ","In this paper, we explore the intrinsic connection between network structures and memory features to optimize both hardware and mapping.","First, we introduce a graph-level execution scheme with a corresponding dataflow and memory management method.","This scheme enables the execution of arbitrary graph patterns with high data reuse and low hardware overhead.","Subsequently, we propose Cocco, a hardware-mapping co-exploration framework leveraging graph-level features of networks.","It aims to minimize communication overhead, such as energy consumption and bandwidth requirements, with a smaller memory capacity.","We formulate the graph-partition scheduling and memory configuration search as an optimization problem and employ a genetic-based method to achieve efficient co-exploration for large and irregular networks.","Experiments demonstrate that Cocco obtains lower external memory access, lower bandwidth requirements, and more stable optimization for graph partition compared to the greedy algorithm and dynamic programming introduced in prior works.","Cocco also reduces the costs by 1.89% to 50.33% using co-exploration compared to other typical methods."],"url":"http://arxiv.org/abs/2402.00629v1","category":"cs.AR"}
{"created":"2024-02-01 14:41:59","title":"CapHuman: Capture Your Moments in Parallel Universes","abstract":"We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align\" paradigm, which enables generalizable identity preservation for new individuals without cumbersome tuning at inference. CapHuman encodes identity features and then learns to align them into the latent space. Moreover, we introduce the 3D facial prior to equip our model with control over the human head in a flexible and 3D-consistent manner. Extensive qualitative and quantitative analyses demonstrate our CapHuman can produce well-identity-preserved, photo-realistic, and high-fidelity portraits with content-rich representations and various head renditions, superior to established baselines. Code and checkpoint will be released at https://github.com/VamosC/CapHuman.","sentences":["We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts.","To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation.","(2) generalizable identity preservation ability.","(3) flexible and fine-grained head control.","Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation.","As a basis, we aim to unleash the above two capabilities of the pre-trained model.","In this work, we present a new framework named CapHuman.","We embrace the ``encode then learn to align\" paradigm, which enables generalizable identity preservation for new individuals without cumbersome tuning at inference.","CapHuman encodes identity features and then learns to align them into the latent space.","Moreover, we introduce the 3D facial prior to equip our model with control over the human head in a flexible and 3D-consistent manner.","Extensive qualitative and quantitative analyses demonstrate our CapHuman can produce well-identity-preserved, photo-realistic, and high-fidelity portraits with content-rich representations and various head renditions, superior to established baselines.","Code and checkpoint will be released at https://github.com/VamosC/CapHuman."],"url":"http://arxiv.org/abs/2402.00627v1","category":"cs.CV"}
{"created":"2024-02-01 14:41:20","title":"Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks","abstract":"Recently, significant progress has been made on Large Vision-Language Models (LVLMs); a new class of VL models that make use of large pre-trained language models. Yet, their vulnerability to Typographic attacks, which involve superimposing misleading text onto an image remain unstudied. Furthermore, prior work typographic attacks rely on sampling a random misleading class from a predefined set of classes. However, the random chosen class might not be the most effective attack. To address these issues, we first introduce a novel benchmark uniquely designed to test LVLMs vulnerability to typographic attacks. Furthermore, we introduce a new and more effective typographic attack: Self-Generated typographic attacks. Indeed, our method, given an image, make use of the strong language capabilities of models like GPT-4V by simply prompting them to recommend a typographic attack. Using our novel benchmark, we uncover that typographic attacks represent a significant threat against LVLM(s). Furthermore, we uncover that typographic attacks recommended by GPT-4V using our new method are not only more effective against GPT-4V itself compared to prior work attacks, but also against a host of less capable yet popular open source models like LLaVA, InstructBLIP, and MiniGPT4.","sentences":["Recently, significant progress has been made on Large Vision-Language Models (LVLMs); a new class of VL models that make use of large pre-trained language models.","Yet, their vulnerability to Typographic attacks, which involve superimposing misleading text onto an image remain unstudied.","Furthermore, prior work typographic attacks rely on sampling a random misleading class from a predefined set of classes.","However, the random chosen class might not be the most effective attack.","To address these issues, we first introduce a novel benchmark uniquely designed to test LVLMs vulnerability to typographic attacks.","Furthermore, we introduce a new and more effective typographic attack: Self-Generated typographic attacks.","Indeed, our method, given an image, make use of the strong language capabilities of models like GPT-4V by simply prompting them to recommend a typographic attack.","Using our novel benchmark, we uncover that typographic attacks represent a significant threat against LVLM(s).","Furthermore, we uncover that typographic attacks recommended by GPT-4V using our new method are not only more effective against GPT-4V itself compared to prior work attacks, but also against a host of less capable yet popular open source models like LLaVA, InstructBLIP, and MiniGPT4."],"url":"http://arxiv.org/abs/2402.00626v1","category":"cs.CV"}
{"created":"2024-02-01 14:40:17","title":"Bialgebraic Reasoning on Higher-Order Program Equivalence","abstract":"Logical relations constitute a key method for reasoning about contextual equivalence of programs in higher-order languages. They are usually developed on a per-case basis, with a new theory required for each variation of the language or of the desired notion of equivalence. In the present paper we introduce a general construction of (step-indexed) logical relations at the level of Higher-Order Mathematical Operational Semantics, a highly parametric categorical framework for modeling the operational semantics of higher-order languages. Our main result asserts that for languages whose weak operational model forms a lax bialgebra, the logical relation is automatically sound for contextual equivalence. Our abstract theory is shown to instantiate to combinatory logics and $\\lambda$-calculi with recursive types, and to different flavours of contextual equivalence.","sentences":["Logical relations constitute a key method for reasoning about contextual equivalence of programs in higher-order languages.","They are usually developed on a per-case basis, with a new theory required for each variation of the language or of the desired notion of equivalence.","In the present paper we introduce a general construction of (step-indexed) logical relations at the level of Higher-Order Mathematical Operational Semantics, a highly parametric categorical framework for modeling the operational semantics of higher-order languages.","Our main result asserts that for languages whose weak operational model forms a lax bialgebra, the logical relation is automatically sound for contextual equivalence.","Our abstract theory is shown to instantiate to combinatory logics and $\\lambda$-calculi with recursive types, and to different flavours of contextual equivalence."],"url":"http://arxiv.org/abs/2402.00625v1","category":"cs.LO"}
{"created":"2024-02-01 14:39:59","title":"Bayesian Causal Inference with Gaussian Process Networks","abstract":"Causal discovery and inference from observational data is an essential problem in statistics posing both modeling and computational challenges. These are typically addressed by imposing strict assumptions on the joint distribution such as linearity. We consider the problem of the Bayesian estimation of the effects of hypothetical interventions in the Gaussian Process Network (GPN) model, a flexible causal framework which allows describing the causal relationships nonparametrically. We detail how to perform causal inference on GPNs by simulating the effect of an intervention across the whole network and propagating the effect of the intervention on downstream variables. We further derive a simpler computational approximation by estimating the intervention distribution as a function of local variables only, modeling the conditional distributions via additive Gaussian processes. We extend both frameworks beyond the case of a known causal graph, incorporating uncertainty about the causal structure via Markov chain Monte Carlo methods. Simulation studies show that our approach is able to identify the effects of hypothetical interventions with non-Gaussian, non-linear observational data and accurately reflect the posterior uncertainty of the causal estimates. Finally we compare the results of our GPN-based causal inference approach to existing methods on a dataset of $A.~thaliana$ gene expressions.","sentences":["Causal discovery and inference from observational data is an essential problem in statistics posing both modeling and computational challenges.","These are typically addressed by imposing strict assumptions on the joint distribution such as linearity.","We consider the problem of the Bayesian estimation of the effects of hypothetical interventions in the Gaussian Process Network (GPN) model, a flexible causal framework which allows describing the causal relationships nonparametrically.","We detail how to perform causal inference on GPNs by simulating the effect of an intervention across the whole network and propagating the effect of the intervention on downstream variables.","We further derive a simpler computational approximation by estimating the intervention distribution as a function of local variables only, modeling the conditional distributions via additive Gaussian processes.","We extend both frameworks beyond the case of a known causal graph, incorporating uncertainty about the causal structure via Markov chain Monte Carlo methods.","Simulation studies show that our approach is able to identify the effects of hypothetical interventions with non-Gaussian, non-linear observational data and accurately reflect the posterior uncertainty of the causal estimates.","Finally we compare the results of our GPN-based causal inference approach to existing methods on a dataset of $A.~thaliana$ gene expressions."],"url":"http://arxiv.org/abs/2402.00623v1","category":"stat.ML"}
{"created":"2024-02-01 14:39:39","title":"Gain of Grain: A Film Grain Handling Toolchain for VVC-based Open Implementations","abstract":"Film grain is a distinctive visual characteristic cherished by filmmakers and cinephiles for its ability to evoke nostalgia and artistic aesthetics. However, faithful preservation of film grain during encoding poses unique challenges. Film grain introduces random noise, complicating traditional compression techniques. Consequently, specialized algorithms and encoding strategies have emerged, aiming to strike a harmonious equilibrium. This paper delves into the nuanced realm of film grain handling in Versatile Video Coding (VVC) encoding. We explore the delicate balance between retaining the cinematic charm of film grain and achieving efficient compression. Moreover, we discuss the importance of perceptual quality assessment and adaptive encoding techniques in preserving film grain fidelity. Additionally, we delve into the impact of film grain handling on bitrate control and compression efficiency using VVenC, an open and optimized VVC encoder. Understanding the role of film grain and its nuanced treatment within encoders becomes increasingly pivotal for delivering high-quality, grain-inclusive content in the digital age.","sentences":["Film grain is a distinctive visual characteristic cherished by filmmakers and cinephiles for its ability to evoke nostalgia and artistic aesthetics.","However, faithful preservation of film grain during encoding poses unique challenges.","Film grain introduces random noise, complicating traditional compression techniques.","Consequently, specialized algorithms and encoding strategies have emerged, aiming to strike a harmonious equilibrium.","This paper delves into the nuanced realm of film grain handling in Versatile Video Coding (VVC) encoding.","We explore the delicate balance between retaining the cinematic charm of film grain and achieving efficient compression.","Moreover, we discuss the importance of perceptual quality assessment and adaptive encoding techniques in preserving film grain fidelity.","Additionally, we delve into the impact of film grain handling on bitrate control and compression efficiency using VVenC, an open and optimized VVC encoder.","Understanding the role of film grain and its nuanced treatment within encoders becomes increasingly pivotal for delivering high-quality, grain-inclusive content in the digital age."],"url":"http://arxiv.org/abs/2402.00622v1","category":"cs.MM"}
{"created":"2024-02-01 14:31:23","title":"Estimate for the bulk viscosity of strongly coupled quark matter","abstract":"Modern hydrodynamic simulations of core-collapse supernovae and neutron-star mergers require knowledge not only of the equilibrium properties of strongly interacting matter, but also of the system's response to perturbations, encoded in various transport coefficients. Using perturbative and holographic tools, we derive here an improved weak-coupling and a new strong-coupling result for the most important transport coefficient of unpaired quark matter, its bulk viscosity. These results are combined in a simple analytic pocket formula for the quantity that is rooted in perturbative Quantum Chromodynamics at high densities but takes into account nonperturbative holographic input at neutron-star densities, where the system is strongly coupled. This expression can be used in the modeling of unpaired quark matter at astrophysically relevant temperatures and densities.","sentences":["Modern hydrodynamic simulations of core-collapse supernovae and neutron-star mergers require knowledge not only of the equilibrium properties of strongly interacting matter, but also of the system's response to perturbations, encoded in various transport coefficients.","Using perturbative and holographic tools, we derive here an improved weak-coupling and a new strong-coupling result for the most important transport coefficient of unpaired quark matter, its bulk viscosity.","These results are combined in a simple analytic pocket formula for the quantity that is rooted in perturbative Quantum Chromodynamics at high densities but takes into account nonperturbative holographic input at neutron-star densities, where the system is strongly coupled.","This expression can be used in the modeling of unpaired quark matter at astrophysically relevant temperatures and densities."],"url":"http://arxiv.org/abs/2402.00621v1","category":"hep-ph"}
{"created":"2024-02-01 14:30:39","title":"Actor Identification in Discourse: A Challenge for LLMs?","abstract":"The identification of political actors who put forward claims in public debate is a crucial step in the construction of discourse networks, which are helpful to analyze societal debates. Actor identification is, however, rather challenging: Often, the locally mentioned speaker of a claim is only a pronoun (\"He proposed that [claim]\"), so recovering the canonical actor name requires discourse understanding. We compare a traditional pipeline of dedicated NLP components (similar to those applied to the related task of coreference) with a LLM, which appears a good match for this generation task. Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse. Further analysis reveals that the LLM is very good at identifying the right reference, but struggles to generate the correct canonical form. This points to an underlying issue in LLMs with controlling generated output. Indeed, a hybrid model combining the LLM with a classifier to normalize its output substantially outperforms both initial models.","sentences":["The identification of political actors who put forward claims in public debate is a crucial step in the construction of discourse networks, which are helpful to analyze societal debates.","Actor identification is, however, rather challenging: Often, the locally mentioned speaker of a claim is only a pronoun (\"He proposed that [claim]\"), so recovering the canonical actor name requires discourse understanding.","We compare a traditional pipeline of dedicated NLP components (similar to those applied to the related task of coreference) with a LLM, which appears a good match for this generation task.","Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse.","Further analysis reveals that the LLM is very good at identifying the right reference, but struggles to generate the correct canonical form.","This points to an underlying issue in LLMs with controlling generated output.","Indeed, a hybrid model combining the LLM with a classifier to normalize its output substantially outperforms both initial models."],"url":"http://arxiv.org/abs/2402.00620v1","category":"cs.CL"}
{"created":"2024-02-01 14:21:39","title":"Metropolitan-scale Entanglement Distribution with Co existing Quantum and Classical Signals in a single fiber","abstract":"The development of prototype metropolitan-scale quantum networks is underway and entails transmitting quantum information via single photons through deployed optical fibers spanning several tens of kilometers. The major challenges in building metropolitan-scale quantum networks are compensation of polarization mode dispersion, high-precision clock synchronization, and compensation for cumulative transmission time fluctuations. One approach addressing these challenges is to co-propagate classical probe signals in the same fiber as the quantum signal. Thus, both signals experience the same conditions, and the changes of the fiber can therefore be monitored and compensated. Here, we demonstrate the distribution of polarization entangled quantum signals co-propagating with the White Rabbit Precision Time Protocol (WR-PTP) classical signals in the same single-core fiber strand at metropolitan-scale distances. Our results demonstrate the feasibility of this quantum-classical coexistence by achieving high-fidelity entanglement distribution between nodes separated by 100 km of optical fiber. This advancement is a significant step towards the practical implementation of robust and efficient metropolitan-scale quantum networks.","sentences":["The development of prototype metropolitan-scale quantum networks is underway and entails transmitting quantum information via single photons through deployed optical fibers spanning several tens of kilometers.","The major challenges in building metropolitan-scale quantum networks are compensation of polarization mode dispersion, high-precision clock synchronization, and compensation for cumulative transmission time fluctuations.","One approach addressing these challenges is to co-propagate classical probe signals in the same fiber as the quantum signal.","Thus, both signals experience the same conditions, and the changes of the fiber can therefore be monitored and compensated.","Here, we demonstrate the distribution of polarization entangled quantum signals co-propagating with the White Rabbit Precision Time Protocol (WR-PTP) classical signals in the same single-core fiber strand at metropolitan-scale distances.","Our results demonstrate the feasibility of this quantum-classical coexistence by achieving high-fidelity entanglement distribution between nodes separated by 100 km of optical fiber.","This advancement is a significant step towards the practical implementation of robust and efficient metropolitan-scale quantum networks."],"url":"http://arxiv.org/abs/2402.00617v1","category":"quant-ph"}
{"created":"2024-02-01 14:15:20","title":"A double scaling for the 4d/3d reduction of $\\mathcal{N}=1$ dualities","abstract":"In this paper we revisit the $S^1$ reduction of 4d $\\mathcal{N}=1$ gauge theories, considering a double scaling on the radius of the circle and on the real masses arising from the global symmetries in the compactification. We discuss the implication of this double scaling for SQCD with gauge algebra of ABCD type. We then show how our prescription translates in the reduction of the 4d superconformal index to the 3d squashed three sphere partition function. This allows us to derive the expected integral identities for the 3d dualities directly from the four dimensional ones. This is relevant for the study of orthogonal SQCD, where the derivation from the 4d index is not possible in absence of the double scaling, because of a divergence due to a flat direction in the Coulomb branch of the effective theory on the circle. Furthermore, we obtain, for the even orthogonal case, a 3d duality with a quadratic fundamental monopole superpotential already discussed in the literature, that receives in this way an explanation from 4d.","sentences":["In this paper we revisit the $S^1$ reduction of 4d $\\mathcal{N}=1$ gauge theories, considering a double scaling on the radius of the circle and on the real masses arising from the global symmetries in the compactification.","We discuss the implication of this double scaling for SQCD with gauge algebra of ABCD type.","We then show how our prescription translates in the reduction of the 4d superconformal index to the 3d squashed three sphere partition function.","This allows us to derive the expected integral identities for the 3d dualities directly from the four dimensional ones.","This is relevant for the study of orthogonal SQCD, where the derivation from the 4d index is not possible in absence of the double scaling, because of a divergence due to a flat direction in the Coulomb branch of the effective theory on the circle.","Furthermore, we obtain, for the even orthogonal case, a 3d duality with a quadratic fundamental monopole superpotential already discussed in the literature, that receives in this way an explanation from 4d."],"url":"http://arxiv.org/abs/2402.00613v1","category":"hep-th"}
{"created":"2024-02-01 14:08:59","title":"Rhoban Football Club: RoboCup Humanoid Kid-Size 2023 Champion Team Paper","abstract":"In 2023, Rhoban Football Club reached the first place of the KidSize soccer competition for the fifth time, and received the best humanoid award. This paper presents and reviews important points in robots architecture and workflow, with hindsights from the competition.","sentences":["In 2023, Rhoban Football Club reached the first place of the KidSize soccer competition for the fifth time, and received the best humanoid award.","This paper presents and reviews important points in robots architecture and workflow, with hindsights from the competition."],"url":"http://arxiv.org/abs/2402.00612v1","category":"cs.RO"}
{"created":"2024-02-01 14:02:30","title":"A new 4d $\\mathcal{N}=1$ duality from the superconformal index","abstract":"In this paper we propose a physical derivation of a 4d conjectural duality for $USp(2N)$ with an anti-symmetric rank-two tensor and fundamental flavors, in presence of a non-trivial superpotential. This duality has been conjectured as a consequence of an exact identity between the superconformal indices of the two phases, proved in the mathematical literature. Here we show that the duality can be derived by a combined sequence of known dualities, deconfinement of tensor matter, RG flow and Higgsing. Furthermore, by following these steps on the superconformal index, we provide an alternative derivation of the integral identity as well.","sentences":["In this paper we propose a physical derivation of a 4d conjectural duality for $USp(2N)$ with an anti-symmetric rank-two tensor and fundamental flavors, in presence of a non-trivial superpotential.","This duality has been conjectured as a consequence of an exact identity between the superconformal indices of the two phases, proved in the mathematical literature.","Here we show that the duality can be derived by a combined sequence of known dualities, deconfinement of tensor matter, RG flow and Higgsing.","Furthermore, by following these steps on the superconformal index, we provide an alternative derivation of the integral identity as well."],"url":"http://arxiv.org/abs/2402.00609v1","category":"hep-th"}
{"created":"2024-02-01 14:02:06","title":"Deep Clustering Using the Soft Silhouette Score: Towards Compact and Well-Separated Clusters","abstract":"Unsupervised learning has gained prominence in the big data era, offering a means to extract valuable insights from unlabeled datasets. Deep clustering has emerged as an important unsupervised category, aiming to exploit the non-linear mapping capabilities of neural networks in order to enhance clustering performance. The majority of deep clustering literature focuses on minimizing the inner-cluster variability in some embedded space while keeping the learned representation consistent with the original high-dimensional dataset. In this work, we propose soft silhoutte, a probabilistic formulation of the silhouette coefficient. Soft silhouette rewards compact and distinctly separated clustering solutions like the conventional silhouette coefficient. When optimized within a deep clustering framework, soft silhouette guides the learned representations towards forming compact and well-separated clusters. In addition, we introduce an autoencoder-based deep learning architecture that is suitable for optimizing the soft silhouette objective function. The proposed deep clustering method has been tested and compared with several well-studied deep clustering methods on various benchmark datasets, yielding very satisfactory clustering results.","sentences":["Unsupervised learning has gained prominence in the big data era, offering a means to extract valuable insights from unlabeled datasets.","Deep clustering has emerged as an important unsupervised category, aiming to exploit the non-linear mapping capabilities of neural networks in order to enhance clustering performance.","The majority of deep clustering literature focuses on minimizing the inner-cluster variability in some embedded space while keeping the learned representation consistent with the original high-dimensional dataset.","In this work, we propose soft silhoutte, a probabilistic formulation of the silhouette coefficient.","Soft silhouette rewards compact and distinctly separated clustering solutions like the conventional silhouette coefficient.","When optimized within a deep clustering framework, soft silhouette guides the learned representations towards forming compact and well-separated clusters.","In addition, we introduce an autoencoder-based deep learning architecture that is suitable for optimizing the soft silhouette objective function.","The proposed deep clustering method has been tested and compared with several well-studied deep clustering methods on various benchmark datasets, yielding very satisfactory clustering results."],"url":"http://arxiv.org/abs/2402.00608v1","category":"cs.LG"}
{"created":"2024-02-01 13:59:04","title":"Are Synthetic Time-series Data Really not as Good as Real Data?","abstract":"Time-series data presents limitations stemming from data quality issues, bias and vulnerabilities, and generalization problem. Integrating universal data synthesis methods holds promise in improving generalization. However, current methods cannot guarantee that the generator's output covers all unseen real data. In this paper, we introduce InfoBoost -- a highly versatile cross-domain data synthesizing framework with time series representation learning capability. We have developed a method based on synthetic data that enables model training without the need for real data, surpassing the performance of models trained with real data. Additionally, we have trained a universal feature extractor based on our synthetic data that is applicable to all time-series data. Our approach overcomes interference from multiple sources rhythmic signal, noise interference, and long-period features that exceed sampling window capabilities. Through experiments, our non-deep-learning synthetic data enables models to achieve superior reconstruction performance and universal explicit representation extraction without the need for real data.","sentences":["Time-series data presents limitations stemming from data quality issues, bias and vulnerabilities, and generalization problem.","Integrating universal data synthesis methods holds promise in improving generalization.","However, current methods cannot guarantee that the generator's output covers all unseen real data.","In this paper, we introduce InfoBoost -- a highly versatile cross-domain data synthesizing framework with time series representation learning capability.","We have developed a method based on synthetic data that enables model training without the need for real data, surpassing the performance of models trained with real data.","Additionally, we have trained a universal feature extractor based on our synthetic data that is applicable to all time-series data.","Our approach overcomes interference from multiple sources rhythmic signal, noise interference, and long-period features that exceed sampling window capabilities.","Through experiments, our non-deep-learning synthetic data enables models to achieve superior reconstruction performance and universal explicit representation extraction without the need for real data."],"url":"http://arxiv.org/abs/2402.00607v1","category":"cs.LG"}
{"created":"2024-02-01 13:58:32","title":"Dynamic Texture Transfer using PatchMatch and Transformers","abstract":"How to automatically transfer the dynamic texture of a given video to the target still image is a challenging and ongoing problem. In this paper, we propose to handle this task via a simple yet effective model that utilizes both PatchMatch and Transformers. The key idea is to decompose the task of dynamic texture transfer into two stages, where the start frame of the target video with the desired dynamic texture is synthesized in the first stage via a distance map guided texture transfer module based on the PatchMatch algorithm. Then, in the second stage, the synthesized image is decomposed into structure-agnostic patches, according to which their corresponding subsequent patches can be predicted by exploiting the powerful capability of Transformers equipped with VQ-VAE for processing long discrete sequences. After getting all those patches, we apply a Gaussian weighted average merging strategy to smoothly assemble them into each frame of the target stylized video. Experimental results demonstrate the effectiveness and superiority of the proposed method in dynamic texture transfer compared to the state of the art.","sentences":["How to automatically transfer the dynamic texture of a given video to the target still image is a challenging and ongoing problem.","In this paper, we propose to handle this task via a simple yet effective model that utilizes both PatchMatch and Transformers.","The key idea is to decompose the task of dynamic texture transfer into two stages, where the start frame of the target video with the desired dynamic texture is synthesized in the first stage via a distance map guided texture transfer module based on the PatchMatch algorithm.","Then, in the second stage, the synthesized image is decomposed into structure-agnostic patches, according to which their corresponding subsequent patches can be predicted by exploiting the powerful capability of Transformers equipped with VQ-VAE for processing long discrete sequences.","After getting all those patches, we apply a Gaussian weighted average merging strategy to smoothly assemble them into each frame of the target stylized video.","Experimental results demonstrate the effectiveness and superiority of the proposed method in dynamic texture transfer compared to the state of the art."],"url":"http://arxiv.org/abs/2402.00606v1","category":"cs.CV"}
{"created":"2024-02-01 13:56:42","title":"Efficiently Separating the Source of the Teukolsky Equation","abstract":"Recent gravitational wave detections from black hole mergers have underscored the critical role black hole perturbation theory and the Teukolsky equation play in understanding the behaviour of black holes. The separable nature of the Teukolsky equation has long been leveraged to study the vacuum linear Teukolsky equation; however, as theory and measurements advance, solving the sourced Teukolsky equation is becoming a frontier of research. In particular, second-order calculations, such as in quasi-normal mode and self-force problems, have extended sources. This paper presents a novel method for efficiently separating the Teukolsky equation's source analytically, a non-trivial problem due to the angular and radial mixing of generic quantities in Kerr spacetime. We provide a proof of concept demonstration of our method's accuracy, separating the Teukolsky source produced by the stress-energy tensor of an ideal gas cloud surrounding a Kerr black hole. The detailed application of our method is provided in an accompanying \\textit{Mathematica} notebook. Our approach opens up a new avenue for efficient, accurate black hole perturbation theory calculations with sources in both the time and frequency domain.","sentences":["Recent gravitational wave detections from black hole mergers have underscored the critical role black hole perturbation theory and the Teukolsky equation play in understanding the behaviour of black holes.","The separable nature of the Teukolsky equation has long been leveraged to study the vacuum linear Teukolsky equation; however, as theory and measurements advance, solving the sourced Teukolsky equation is becoming a frontier of research.","In particular, second-order calculations, such as in quasi-normal mode and self-force problems, have extended sources.","This paper presents a novel method for efficiently separating the Teukolsky equation's source analytically, a non-trivial problem due to the angular and radial mixing of generic quantities in Kerr spacetime.","We provide a proof of concept demonstration of our method's accuracy, separating the Teukolsky source produced by the stress-energy tensor of an ideal gas cloud surrounding a Kerr black hole.","The detailed application of our method is provided in an accompanying \\textit{Mathematica} notebook.","Our approach opens up a new avenue for efficient, accurate black hole perturbation theory calculations with sources in both the time and frequency domain."],"url":"http://arxiv.org/abs/2402.00604v1","category":"gr-qc"}
{"created":"2024-02-01 13:54:10","title":"Small-scale clustering of Primordial Black Holes: cloud-in-cloud and exclusion effects","abstract":"Using an excursion-set approach, we revisit the initial spatial clustering of Primordial Black Holes (PBHs) originating from the Hubble re-entry of large Gaussian density fluctuations in the early universe. We derive the two-point correlation functions of PBHs, properly accounting for the \"cloud-in-cloud\" mechanism. Our expressions naturally and intrinsically correlate the formation of pairs of PBHs, which is a key difference with the Poisson model of clustering. Our approach effectively includes short-range exclusion effects and clarifies the clustering behaviors at small scale: PBHs are anti-correlated at short distances. Using a scale-independent collapse threshold, we derive explicit expressions for the excess probability to find pairs of PBHs separated by a distance $r$, as well as the excess probability to find pairs with asymmetric mass ratio. Our framework is model-independent by construction and we discuss possible other applications.","sentences":["Using an excursion-set approach, we revisit the initial spatial clustering of Primordial Black Holes (PBHs) originating from the Hubble re-entry of large Gaussian density fluctuations in the early universe.","We derive the two-point correlation functions of PBHs, properly accounting for the \"cloud-in-cloud\" mechanism.","Our expressions naturally and intrinsically correlate the formation of pairs of PBHs, which is a key difference with the Poisson model of clustering.","Our approach effectively includes short-range exclusion effects and clarifies the clustering behaviors at small scale: PBHs are anti-correlated at short distances.","Using a scale-independent collapse threshold, we derive explicit expressions for the excess probability to find pairs of PBHs separated by a distance $r$, as well as the excess probability to find pairs with asymmetric mass ratio.","Our framework is model-independent by construction and we discuss possible other applications."],"url":"http://arxiv.org/abs/2402.00600v1","category":"astro-ph.CO"}
{"created":"2024-02-01 13:51:35","title":"A Promise Theory Perspective on the Role of Intent in Group Dynamics","abstract":"We present a simple argument using Promise Theory and dimensional analysis for the Dunbar scaling hierarchy, supported by recent data from group formation in Wikipedia editing. We show how the assumption of a common priority seeds group alignment until the costs associated with attending to the group outweigh the benefits in a detailed balance scenario. Subject to partial efficiency of implementing promised intentions, we can reproduce a series of compatible rates that balance growth with entropy.","sentences":["We present a simple argument using Promise Theory and dimensional analysis for the Dunbar scaling hierarchy, supported by recent data from group formation in Wikipedia editing.","We show how the assumption of a common priority seeds group alignment until the costs associated with attending to the group outweigh the benefits in a detailed balance scenario.","Subject to partial efficiency of implementing promised intentions, we can reproduce a series of compatible rates that balance growth with entropy."],"url":"http://arxiv.org/abs/2402.00598v1","category":"cs.SI"}
{"created":"2024-02-01 13:45:12","title":"Group Related Phenomena in Wikipedia Edits","abstract":"Human communities have self-organizing properties that give rise to very specific natural grouping patterns, reflected in the Dunbar Number and its layered structure (a Dunbar Graph). Since work-groups are necessarily also social groups, we might expect the same principles to apply here as well. One factor likely to be important in limiting the size of groups is that conflicts typically escalate with the number of people involved. Here we analyse Wikipedia editing histories across a wide range of topics to show that there is an emergent coherence in the size of groups formed transiently to edit the content of subject texts, with two peaks averaging at around $N=8$ for the size corresponding to maximal contention, and at around $N=4$ as a regular team. These values are consistent with the observed sizes of conversational groups, as well as the hierarchical structuring of Dunbar graphs. We use the Promise Theory of trust to suggest a scaling law that may apply to all group distributions based on seeded attraction. In addition to providing further evidence that even natural communities of strangers are self-organising, the results have important implications for the governance of the Wikipedia commons and for the security of all online social platforms and associations.","sentences":["Human communities have self-organizing properties that give rise to very specific natural grouping patterns, reflected in the Dunbar Number and its layered structure (a Dunbar Graph).","Since work-groups are necessarily also social groups, we might expect the same principles to apply here as well.","One factor likely to be important in limiting the size of groups is that conflicts typically escalate with the number of people involved.","Here we analyse Wikipedia editing histories across a wide range of topics to show that there is an emergent coherence in the size of groups formed transiently to edit the content of subject texts, with two peaks averaging at around $N=8$ for the size corresponding to maximal contention, and at around $N=4$ as a regular team.","These values are consistent with the observed sizes of conversational groups, as well as the hierarchical structuring of Dunbar graphs.","We use the Promise Theory of trust to suggest a scaling law that may apply to all group distributions based on seeded attraction.","In addition to providing further evidence that even natural communities of strangers are self-organising, the results have important implications for the governance of the Wikipedia commons and for the security of all online social platforms and associations."],"url":"http://arxiv.org/abs/2402.00595v1","category":"cs.SI"}
{"created":"2024-02-01 13:45:06","title":"Identifying relevant Factors of Requirements Quality: an industrial Case Study","abstract":"[Context and Motivation]: The quality of requirements specifications impacts subsequent, dependent software engineering activities. Requirements quality defects like ambiguous statements can result in incomplete or wrong features and even lead to budget overrun or project failure. [Problem]: Attempts at measuring the impact of requirements quality have been held back by the vast amount of interacting factors. Requirements quality research lacks an understanding of which factors are relevant in practice. [Principal Ideas and Results]: We conduct a case study considering data from both interview transcripts and issue reports to identify relevant factors of requirements quality. The results include 17 factors and 11 interaction effects relevant to the case company. [Contribution]: The results contribute empirical evidence that (1) strengthens existing requirements engineering theories and (2) advances industry-relevant requirements quality research.","sentences":["[Context and Motivation]: The quality of requirements specifications impacts subsequent, dependent software engineering activities.","Requirements quality defects like ambiguous statements can result in incomplete or wrong features and even lead to budget overrun or project failure.","[Problem]: Attempts at measuring the impact of requirements quality have been held back by the vast amount of interacting factors.","Requirements quality research lacks an understanding of which factors are relevant in practice.","[Principal Ideas and Results]: We conduct a case study considering data from both interview transcripts and issue reports to identify relevant factors of requirements quality.","The results include 17 factors and 11 interaction effects relevant to the case company.","[Contribution]: The results contribute empirical evidence that (1) strengthens existing requirements engineering theories and (2) advances industry-relevant requirements quality research."],"url":"http://arxiv.org/abs/2402.00594v1","category":"cs.SE"}
{"created":"2024-02-01 13:41:44","title":"Uncertainty-Aware Partial-Label Learning","abstract":"In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting. While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory. Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. Additionally, we prove that our algorithm is risk-consistent.","sentences":["In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels.","Partial-label learning allows training classifiers in this weakly supervised setting.","While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates.","However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving.","In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory.","Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance.","Additionally, we prove that our algorithm is risk-consistent."],"url":"http://arxiv.org/abs/2402.00592v1","category":"cs.LG"}
{"created":"2024-02-01 13:37:53","title":"Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations","abstract":"This paper presents sandra, a neuro-symbolic reasoner combining vectorial representations with deductive reasoning. Sandra builds a vector space constrained by an ontology and performs reasoning over it. The geometric nature of the reasoner allows its combination with neural networks, bridging the gap with symbolic knowledge representations. Sandra is based on the Description and Situation (DnS) ontology design pattern, a formalization of frame semantics. Given a set of facts (a situation) it allows to infer all possible perspectives (descriptions) that can provide a plausible interpretation for it, even in presence of incomplete information. We prove that our method is correct with respect to the DnS model. We experiment with two different tasks and their standard benchmarks, demonstrating that, without increasing complexity, sandra (i) outperforms all the baselines (ii) provides interpretability in the classification process, and (iii) allows control over the vector space, which is designed a priori.","sentences":["This paper presents sandra, a neuro-symbolic reasoner combining vectorial representations with deductive reasoning.","Sandra builds a vector space constrained by an ontology and performs reasoning over it.","The geometric nature of the reasoner allows its combination with neural networks, bridging the gap with symbolic knowledge representations.","Sandra is based on the Description and Situation (DnS) ontology design pattern, a formalization of frame semantics.","Given a set of facts (a situation) it allows to infer all possible perspectives (descriptions) that can provide a plausible interpretation for it, even in presence of incomplete information.","We prove that our method is correct with respect to the DnS model.","We experiment with two different tasks and their standard benchmarks, demonstrating that, without increasing complexity, sandra (i) outperforms all the baselines (ii) provides interpretability in the classification process, and (iii) allows control over the vector space, which is designed a priori."],"url":"http://arxiv.org/abs/2402.00591v1","category":"cs.AI"}
{"created":"2024-02-01 13:34:59","title":"BrainSLAM: SLAM on Neural Population Activity Data","abstract":"Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments. Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data. We present BrainSLAM; a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex. This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze. The CNN's output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs `loop closure' (detecting previously visited locations and correcting map aliasing errors). Together, these three components can construct faithful representations of the environment while simultaneously tracking the animal's location. This is the first demonstration of inference of a spatial map from brain recordings. Our findings expand SLAM to a new modality, enabling a new method of mapping environments and facilitating a better understanding of the role of cognitive maps in navigation and decision making.","sentences":["Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments.","Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data.","We present BrainSLAM; a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex.","This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze.","The CNN's output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs `loop closure' (detecting previously visited locations and correcting map aliasing errors).","Together, these three components can construct faithful representations of the environment while simultaneously tracking the animal's location.","This is the first demonstration of inference of a spatial map from brain recordings.","Our findings expand SLAM to a new modality, enabling a new method of mapping environments and facilitating a better understanding of the role of cognitive maps in navigation and decision making."],"url":"http://arxiv.org/abs/2402.00588v1","category":"cs.RO"}
{"created":"2024-02-01 13:32:04","title":"SATac: A Thermoluminescence Enabled Tactile Sensor for Concurrent Perception of Temperature, Pressure, and Shear","abstract":"Most vision-based tactile sensors use elastomer deformation to infer tactile information, which can not sense some modalities, like temperature. As an important part of human tactile perception, temperature sensing can help robots better interact with the environment. In this work, we propose a novel multimodal vision-based tactile sensor, SATac, which can simultaneously perceive information of temperature, pressure, and shear. SATac utilizes thermoluminescence of strontium aluminate (SA) to sense a wide range of temperatures with exceptional resolution. Additionally, the pressure and shear can also be perceived by analyzing Voronoi diagram. A series of experiments are conducted to verify the performance of our proposed sensor. We also discuss the possible application scenarios and demonstrate how SATac could benefit robot perception capabilities.","sentences":["Most vision-based tactile sensors use elastomer deformation to infer tactile information, which can not sense some modalities, like temperature.","As an important part of human tactile perception, temperature sensing can help robots better interact with the environment.","In this work, we propose a novel multimodal vision-based tactile sensor, SATac, which can simultaneously perceive information of temperature, pressure, and shear.","SATac utilizes thermoluminescence of strontium aluminate (SA) to sense a wide range of temperatures with exceptional resolution.","Additionally, the pressure and shear can also be perceived by analyzing Voronoi diagram.","A series of experiments are conducted to verify the performance of our proposed sensor.","We also discuss the possible application scenarios and demonstrate how SATac could benefit robot perception capabilities."],"url":"http://arxiv.org/abs/2402.00585v1","category":"cs.RO"}
{"created":"2024-02-01 13:14:38","title":"Tropical Decision Boundaries for Neural Networks Are Robust Against Adversarial Attacks","abstract":"We introduce a simple, easy to implement, and computationally efficient tropical convolutional neural network architecture that is robust against adversarial attacks. We exploit the tropical nature of piece-wise linear neural networks by embedding the data in the tropical projective torus in a single hidden layer which can be added to any model. We study the geometry of its decision boundary theoretically and show its robustness against adversarial attacks on image datasets using computational experiments.","sentences":["We introduce a simple, easy to implement, and computationally efficient tropical convolutional neural network architecture that is robust against adversarial attacks.","We exploit the tropical nature of piece-wise linear neural networks by embedding the data in the tropical projective torus in a single hidden layer which can be added to any model.","We study the geometry of its decision boundary theoretically and show its robustness against adversarial attacks on image datasets using computational experiments."],"url":"http://arxiv.org/abs/2402.00576v1","category":"cs.LG"}
{"created":"2024-02-01 13:13:16","title":"Diffusion-based Light Field Synthesis","abstract":"Light fields (LFs), conducive to comprehensive scene radiance recorded across angular dimensions, find wide applications in 3D reconstruction, virtual reality, and computational photography.However, the LF acquisition is inevitably time-consuming and resource-intensive due to the mainstream acquisition strategy involving manual capture or laborious software synthesis.Given such a challenge, we introduce LFdiff, a straightforward yet effective diffusion-based generative framework tailored for LF synthesis, which adopts only a single RGB image as input.LFdiff leverages disparity estimated by a monocular depth estimation network and incorporates two distinctive components: a novel condition scheme and a noise estimation network tailored for LF data.Specifically, we design a position-aware warping condition scheme, enhancing inter-view geometry learning via a robust conditional signal.We then propose DistgUnet, a disentanglement-based noise estimation network, to harness comprehensive LF representations.Extensive experiments demonstrate that LFdiff excels in synthesizing visually pleasing and disparity-controllable light fields with enhanced generalization capability.Additionally, comprehensive results affirm the broad applicability of the generated LF data, spanning applications like LF super-resolution and refocusing.","sentences":["Light fields (LFs), conducive to comprehensive scene radiance recorded across angular dimensions, find wide applications in 3D reconstruction, virtual reality, and computational photography.","However, the LF acquisition is inevitably time-consuming and resource-intensive due to the mainstream acquisition strategy involving manual capture or laborious software synthesis.","Given such a challenge, we introduce LFdiff, a straightforward yet effective diffusion-based generative framework tailored for LF synthesis, which adopts only a single RGB image as input.","LFdiff leverages disparity estimated by a monocular depth estimation network and incorporates two distinctive components: a novel condition scheme and a noise estimation network tailored for LF data.","Specifically, we design a position-aware warping condition scheme, enhancing inter-view geometry learning via a robust conditional signal.","We then propose DistgUnet, a disentanglement-based noise estimation network, to harness comprehensive LF representations.","Extensive experiments demonstrate that LFdiff excels in synthesizing visually pleasing and disparity-controllable light fields with enhanced generalization capability.","Additionally, comprehensive results affirm the broad applicability of the generated LF data, spanning applications like LF super-resolution and refocusing."],"url":"http://arxiv.org/abs/2402.00575v1","category":"cs.CV"}
{"created":"2024-02-01 13:01:47","title":"Secure Supervised Learning-Based Smart Home Authentication Framework","abstract":"The Smart home possesses the capability of facilitating home services to their users with the systematic advance in The Internet of Things (IoT) and information and communication technologies (ICT) in recent decades. The home service offered by the smart devices helps the users in utilize maximized level of comfort for the objective of improving life quality. As the user and smart devices communicate through an insecure channel, the smart home environment is prone to security and privacy problems. A secure authentication protocol needs to be established between the smart devices and the user, such that a situation for device authentication can be made feasible in smart home environments. Most of the existing smart home authentication protocols were identified to fail in facilitating a secure mutual authentication and increases the possibility of lunching the attacks of session key disclosure, impersonation and stolen smart device. In this paper, Secure Supervised Learning-based Smart Home Authentication Framework (SSL-SHAF) is proposed as are liable mutual authentication that can be contextually imposed for better security. The formal analysis of the proposed SSL-SHAF confirmed better resistance against session key disclosure, impersonation and stolen smart device attacks. The results of SSL-SHAF confirmed minimized computational costs and security compared to the baseline protocols considered for investigation.","sentences":["The Smart home possesses the capability of facilitating home services to their users with the systematic advance in The Internet of Things (IoT) and information and communication technologies (ICT) in recent decades.","The home service offered by the smart devices helps the users in utilize maximized level of comfort for the objective of improving life quality.","As the user and smart devices communicate through an insecure channel, the smart home environment is prone to security and privacy problems.","A secure authentication protocol needs to be established between the smart devices and the user, such that a situation for device authentication can be made feasible in smart home environments.","Most of the existing smart home authentication protocols were identified to fail in facilitating a secure mutual authentication and increases the possibility of lunching the attacks of session key disclosure, impersonation and stolen smart device.","In this paper, Secure Supervised Learning-based Smart Home Authentication Framework (SSL-SHAF) is proposed as are liable mutual authentication that can be contextually imposed for better security.","The formal analysis of the proposed SSL-SHAF confirmed better resistance against session key disclosure, impersonation and stolen smart device attacks.","The results of SSL-SHAF confirmed minimized computational costs and security compared to the baseline protocols considered for investigation."],"url":"http://arxiv.org/abs/2402.00568v1","category":"cs.CR"}
{"created":"2024-02-01 12:50:48","title":"A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification","abstract":"Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized accelerator on FPGA for the proposed model with several optimizations to improve its performance. Our experimental results on benchmark grayscale image datasets demonstrate the effectiveness of the proposed model, achieving vastly lower latency (up to 16$\\times$ less) and competitive or leading performance compared to other state-of-the-art image classification models on various domain-specific grayscale image classification datasets.","sentences":["Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications.","Additionally, many image classification models work on both RGB and grayscale datasets.","Classifiers that operate solely on grayscale images are much less common.","Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR).","Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images.","We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting.","We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model.","Moreover, we develop a customized accelerator on FPGA for the proposed model with several optimizations to improve its performance.","Our experimental results on benchmark grayscale image datasets demonstrate the effectiveness of the proposed model, achieving vastly lower latency (up to 16$\\times$ less) and competitive or leading performance compared to other state-of-the-art image classification models on various domain-specific grayscale image classification datasets."],"url":"http://arxiv.org/abs/2402.00564v1","category":"cs.CV"}
{"created":"2024-02-01 12:49:17","title":"Endomorphisms of Linear Block Codes","abstract":"The automorphism groups of various linear codes are well-studied yielding valuable insights into the respective code structure. This knowledge is successfully applied in, e.g., theoretical analysis and in improving decoding performance motivating the analyses of endomorphisms of linear codes. In this work, we discuss the structure of the set of transformation matrices of code endomorphisms, defined as a generalization of code automorphisms, and provide an explicit construction of a bijective mapping between the image of an endomorphism and its canonical quotient space. Furthermore, we introduce a one-to-one mapping between the set of transformation matrices of endomorphisms and a larger linear block code enabling the use of well-known algorithms for the search for suitable endomorphisms. Additionally, we propose an approach to obtain unknown code endomorphisms based on automorphisms of the code. Furthermore, we consider ensemble decoding as a possible use case for endomorphisms by introducing endomorphism ensemble decoding. Interestingly, EED can improve decoding performance when other ensemble decoding schemes are not applicable.","sentences":["The automorphism groups of various linear codes are well-studied yielding valuable insights into the respective code structure.","This knowledge is successfully applied in, e.g., theoretical analysis and in improving decoding performance motivating the analyses of endomorphisms of linear codes.","In this work, we discuss the structure of the set of transformation matrices of code endomorphisms, defined as a generalization of code automorphisms, and provide an explicit construction of a bijective mapping between the image of an endomorphism and its canonical quotient space.","Furthermore, we introduce a one-to-one mapping between the set of transformation matrices of endomorphisms and a larger linear block code enabling the use of well-known algorithms for the search for suitable endomorphisms.","Additionally, we propose an approach to obtain unknown code endomorphisms based on automorphisms of the code.","Furthermore, we consider ensemble decoding as a possible use case for endomorphisms by introducing endomorphism ensemble decoding.","Interestingly, EED can improve decoding performance when other ensemble decoding schemes are not applicable."],"url":"http://arxiv.org/abs/2402.00562v1","category":"cs.IT"}
