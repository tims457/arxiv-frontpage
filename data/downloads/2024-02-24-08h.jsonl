{"created":"2024-02-21 18:54:37","title":"Corrective Machine Unlearning","abstract":"Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard retraining-from-scratch, require most of the manipulated data to be identified for effective corrective unlearning. However, one approach, SSD, achieves limited success in unlearning adverse effects with just a small portion of the manipulated samples, showing the tractability of this setting. We hope our work spurs research towards developing better methods for corrective unlearning and offers practitioners a new strategy to handle data integrity challenges arising from web-scale training.","sentences":["Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet.","We study what model developers can do if they detect that some data was manipulated or incorrect.","Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains.","Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   ","We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples.","We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning.","We find most existing unlearning methods, including the gold-standard retraining-from-scratch, require most of the manipulated data to be identified for effective corrective unlearning.","However, one approach, SSD, achieves limited success in unlearning adverse effects with just a small portion of the manipulated samples, showing the tractability of this setting.","We hope our work spurs research towards developing better methods for corrective unlearning and offers practitioners a new strategy to handle data integrity challenges arising from web-scale training."],"url":"http://arxiv.org/abs/2402.14015v1","category":"cs.LG"}
{"created":"2024-02-21 18:48:38","title":"Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models","abstract":"Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.","sentences":["Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse.","In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages.","Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages.","Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language.","CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss.","Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA."],"url":"http://arxiv.org/abs/2402.14007v1","category":"cs.CL"}
{"created":"2024-02-21 18:44:38","title":"Information Elicitation in Agency Games","abstract":"Rapid progress in scalable, commoditized tools for data collection and data processing has made it possible for firms and policymakers to employ ever more complex metrics as guides for decision-making. These developments have highlighted a prevailing challenge -- deciding *which* metrics to compute. In particular, a firm's ability to compute a wider range of existing metrics does not address the problem of *unknown unknowns*, which reflects informational limitations on the part of the firm. To guide the choice of metrics in the face of this informational problem, we turn to the evaluated agents themselves, who may have more information than a principal about how to measure outcomes effectively. We model this interaction as a simple agency game, where we ask: *When does an agent have an incentive to reveal the observability of a cost-correlated variable to the principal?* There are two effects: better information reduces the agent's information rents but also makes some projects go forward that otherwise would fail. We show that the agent prefers to reveal information that exposes a strong enough differentiation between high and low costs. Expanding the agent's action space to include the ability to *garble* their information, we show that the agent often prefers to garble over full revelation. Still, giving the agent the ability to garble can lead to higher total welfare. Our model has analogies with price discrimination, and we leverage some of these synergies to analyze total welfare.","sentences":["Rapid progress in scalable, commoditized tools for data collection and data processing has made it possible for firms and policymakers to employ ever more complex metrics as guides for decision-making.","These developments have highlighted a prevailing challenge -- deciding *which* metrics to compute.","In particular, a firm's ability to compute a wider range of existing metrics does not address the problem of *unknown unknowns*, which reflects informational limitations on the part of the firm.","To guide the choice of metrics in the face of this informational problem, we turn to the evaluated agents themselves, who may have more information than a principal about how to measure outcomes effectively.","We model this interaction as a simple agency game, where we ask: *When does an agent have an incentive to reveal the observability of a cost-correlated variable to the principal?","*","There are two effects: better information reduces the agent's information rents but also makes some projects go forward that otherwise would fail.","We show that the agent prefers to reveal information that exposes a strong enough differentiation between high and low costs.","Expanding the agent's action space to include the ability to *garble* their information, we show that the agent often prefers to garble over full revelation.","Still, giving the agent the ability to garble can lead to higher total welfare.","Our model has analogies with price discrimination, and we leverage some of these synergies to analyze total welfare."],"url":"http://arxiv.org/abs/2402.14005v1","category":"cs.GT"}
{"created":"2024-02-21 18:40:24","title":"Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models","abstract":"Large Language Models with transformer architecture have revolutionized the domain of text generation, setting unprecedented benchmarks. Despite their impressive capabilities, LLMs have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations. This term, however, has often been misapplied to any results deviating from the instructor's expectations, which this paper defines as attention misdirection rather than true hallucinations. Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models. This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error rate of only 3,15 percent across 4,000 responses generated by GPT in response to a real business challenge. It emphasizes that by equipping experimentation with knowledge, businesses can unlock opportunities for innovation through the use of these natively pre-trained models. This reinforces the notion that strategic application grounded in a skilled team can maximize the benefits of emergent technologies such as the LLMs.","sentences":["Large Language Models with transformer architecture have revolutionized the domain of text generation, setting unprecedented benchmarks.","Despite their impressive capabilities, LLMs have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations.","This term, however, has often been misapplied to any results deviating from the instructor's expectations, which this paper defines as attention misdirection rather than true hallucinations.","Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models.","This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error rate of only 3,15 percent across 4,000 responses generated by GPT in response to a real business challenge.","It emphasizes that by equipping experimentation with knowledge, businesses can unlock opportunities for innovation through the use of these natively pre-trained models.","This reinforces the notion that strategic application grounded in a skilled team can maximize the benefits of emergent technologies such as the LLMs."],"url":"http://arxiv.org/abs/2402.14002v1","category":"cs.CL"}
{"created":"2024-02-21 18:09:04","title":"The Importance of Architecture Choice in Deep Learning for Climate Applications","abstract":"Machine Learning has become a pervasive tool in climate science applications. However, current models fail to address nonstationarity induced by anthropogenic alterations in greenhouse emissions and do not routinely quantify the uncertainty of proposed projections. In this paper, we model the Atlantic Meridional Overturning Circulation (AMOC) which is of major importance to climate in Europe and the US East Coast by transporting warm water to these regions, and has the potential for abrupt collapse. We can generate arbitrarily extreme climate scenarios through arbitrary time scales which we then predict using neural networks. Our analysis shows that the AMOC is predictable using neural networks under a diverse set of climate scenarios. Further experiments reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead of imitating its progression through autocorrelation. With quantified uncertainty, an intriguing pattern of \"spikes\" before critical points of collapse in the AMOC casts doubt on previous analyses that predicted an AMOC collapse within this century. Our results show that Bayesian Neural Networks perform poorly compared to more dense architectures and care should be taken when applying neural networks to nonstationary scenarios such as climate projections. Further, our results highlight that big NN models might have difficulty in modeling global Earth System dynamics accurately and be successfully applied in nonstationary climate scenarios due to the physics being challenging for neural networks to capture.","sentences":["Machine Learning has become a pervasive tool in climate science applications.","However, current models fail to address nonstationarity induced by anthropogenic alterations in greenhouse emissions and do not routinely quantify the uncertainty of proposed projections.","In this paper, we model the Atlantic Meridional Overturning Circulation (AMOC) which is of major importance to climate in Europe and the US East Coast by transporting warm water to these regions, and has the potential for abrupt collapse.","We can generate arbitrarily extreme climate scenarios through arbitrary time scales which we then predict using neural networks.","Our analysis shows that the AMOC is predictable using neural networks under a diverse set of climate scenarios.","Further experiments reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead of imitating its progression through autocorrelation.","With quantified uncertainty, an intriguing pattern of \"spikes\" before critical points of collapse in the AMOC casts doubt on previous analyses that predicted an AMOC collapse within this century.","Our results show that Bayesian Neural Networks perform poorly compared to more dense architectures and care should be taken when applying neural networks to nonstationary scenarios such as climate projections.","Further, our results highlight that big NN models might have difficulty in modeling global Earth System dynamics accurately and be successfully applied in nonstationary climate scenarios due to the physics being challenging for neural networks to capture."],"url":"http://arxiv.org/abs/2402.13979v1","category":"cs.LG"}
{"created":"2024-02-21 17:15:47","title":"Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty in Scientific Machine Learning","abstract":"This paper investigates the use of probabilistic neural networks (PNNs) to model aleatoric uncertainty, which refers to the inherent variability in the input-output relationships of a system, often characterized by unequal variance or heteroscedasticity. Unlike traditional neural networks that produce deterministic outputs, PNNs generate probability distributions for the target variable, allowing the determination of both predicted means and intervals in regression scenarios. Contributions of this paper include the development of a probabilistic distance metric to optimize PNN architecture, and the deployment of PNNs in controlled data sets as well as a practical material science case involving fiber-reinforced composites. The findings confirm that PNNs effectively model aleatoric uncertainty, proving to be more appropriate than the commonly employed Gaussian process regression for this purpose. Specifically, in a real-world scientific machine learning context, PNNs yield remarkably accurate output mean estimates with R-squared scores approaching 0.97, and their predicted intervals exhibit a high correlation coefficient of nearly 0.80, closely matching observed data intervals. Hence, this research contributes to the ongoing exploration of leveraging the sophisticated representational capacity of neural networks to delineate complex input-output relationships in scientific problems.","sentences":["This paper investigates the use of probabilistic neural networks (PNNs) to model aleatoric uncertainty, which refers to the inherent variability in the input-output relationships of a system, often characterized by unequal variance or heteroscedasticity.","Unlike traditional neural networks that produce deterministic outputs, PNNs generate probability distributions for the target variable, allowing the determination of both predicted means and intervals in regression scenarios.","Contributions of this paper include the development of a probabilistic distance metric to optimize PNN architecture, and the deployment of PNNs in controlled data sets as well as a practical material science case involving fiber-reinforced composites.","The findings confirm that PNNs effectively model aleatoric uncertainty, proving to be more appropriate than the commonly employed Gaussian process regression for this purpose.","Specifically, in a real-world scientific machine learning context, PNNs yield remarkably accurate output mean estimates with R-squared scores approaching 0.97, and their predicted intervals exhibit a high correlation coefficient of nearly 0.80, closely matching observed data intervals.","Hence, this research contributes to the ongoing exploration of leveraging the sophisticated representational capacity of neural networks to delineate complex input-output relationships in scientific problems."],"url":"http://arxiv.org/abs/2402.13945v1","category":"stat.ML"}
{"created":"2024-02-21 17:07:09","title":"What is the focus of XAI in UI design? Prioritizing UI design principles for enhancing XAI user experience","abstract":"With the widespread application of artificial intelligence(AI), the explainable AI (XAI) field has undergone a notable resurgence. In this background, the importance of user experience in XAI has become increasingly prominent. Simultaneously, the user interface (UI) serves as a crucial link between XAI and users. However, despite the existence of UI design principles for XAI, there is a lack of prioritization based on their significance. This will lead practitioners to have a vague understanding of different design principles, making it difficult to allocate design space reasonably and emphasize design focal points. This paper aims to prioritize four design principles, providing clear guidance for UI design in XAI. Initially, we conducted a lightweight summary to derive five user experience standards for non-expert users in XAI. Subsequently, we developed four corresponding webpage prototypes for the four design principles. Nineteen participants then interacted with these prototypes, providing ratings based on five user experience standards, and We calculated the weights of the design principles. Our findings indicate that, for non-expert users, \"sensitivity\" is the optimal UI design principle (weight = 0.3296), followed by \"flexibility\" (weight = 0.3014). Finally, we engage in further discussion and summarization of our research results, and present future works and limitations.","sentences":["With the widespread application of artificial intelligence(AI), the explainable AI (XAI) field has undergone a notable resurgence.","In this background, the importance of user experience in XAI has become increasingly prominent.","Simultaneously, the user interface (UI) serves as a crucial link between XAI and users.","However, despite the existence of UI design principles for XAI, there is a lack of prioritization based on their significance.","This will lead practitioners to have a vague understanding of different design principles, making it difficult to allocate design space reasonably and emphasize design focal points.","This paper aims to prioritize four design principles, providing clear guidance for UI design in XAI.","Initially, we conducted a lightweight summary to derive five user experience standards for non-expert users in XAI.","Subsequently, we developed four corresponding webpage prototypes for the four design principles.","Nineteen participants then interacted with these prototypes, providing ratings based on five user experience standards, and We calculated the weights of the design principles.","Our findings indicate that, for non-expert users, \"sensitivity\" is the optimal UI design principle (weight = 0.3296), followed by \"flexibility\" (weight = 0.3014).","Finally, we engage in further discussion and summarization of our research results, and present future works and limitations."],"url":"http://arxiv.org/abs/2402.13939v1","category":"cs.HC"}
{"created":"2024-02-21 17:00:56","title":"Do Efficient Transformers Really Save Computation?","abstract":"As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size. Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer. We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses.","sentences":["As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable.","While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer.","This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation.","In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer.","We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems.","Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size.","Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer.","We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses."],"url":"http://arxiv.org/abs/2402.13934v1","category":"cs.LG"}
{"created":"2024-02-21 16:51:05","title":"SDXL-Lightning: Progressive Adversarial Diffusion Distillation","abstract":"We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.","sentences":["We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL.","Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage.","In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques.","We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights."],"url":"http://arxiv.org/abs/2402.13929v1","category":"cs.CV"}
{"created":"2024-02-21 16:48:07","title":"The Delusional Hedge Algorithm as a Model of Human Learning from Diverse Opinions","abstract":"Whereas cognitive models of learning often assume direct experience with both the features of an event and with a true label or outcome, much of everyday learning arises from hearing the opinions of others, without direct access to either the experience or the ground truth outcome. We consider how people can learn which opinions to trust in such scenarios by extending the hedge algorithm: a classic solution for learning from diverse information sources. We first introduce a semi-supervised variant we call the delusional hedge capable of learning from both supervised and unsupervised experiences. In two experiments, we examine the alignment between human judgments and predictions from the standard hedge, the delusional hedge, and a heuristic baseline model. Results indicate that humans effectively incorporate both labeled and unlabeled information in a manner consistent with the delusional hedge algorithm -- suggesting that human learners not only gauge the accuracy of information sources but also their consistency with other reliable sources. The findings advance our understanding of human learning from diverse opinions, with implications for the development of algorithms that better capture how people learn to weigh conflicting information sources.","sentences":["Whereas cognitive models of learning often assume direct experience with both the features of an event and with a true label or outcome, much of everyday learning arises from hearing the opinions of others, without direct access to either the experience or the ground truth outcome.","We consider how people can learn which opinions to trust in such scenarios by extending the hedge algorithm: a classic solution for learning from diverse information sources.","We first introduce a semi-supervised variant we call the delusional hedge capable of learning from both supervised and unsupervised experiences.","In two experiments, we examine the alignment between human judgments and predictions from the standard hedge, the delusional hedge, and a heuristic baseline model.","Results indicate that humans effectively incorporate both labeled and unlabeled information in a manner consistent with the delusional hedge algorithm -- suggesting that human learners not only gauge the accuracy of information sources but also their consistency with other reliable sources.","The findings advance our understanding of human learning from diverse opinions, with implications for the development of algorithms that better capture how people learn to weigh conflicting information sources."],"url":"http://arxiv.org/abs/2402.13927v1","category":"cs.AI"}
{"created":"2024-02-21 16:46:36","title":"Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content","abstract":"The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts. In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks. In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives. The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs. In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations.","sentences":["The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts.","In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks.","In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives.","The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs.","In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations."],"url":"http://arxiv.org/abs/2402.13926v1","category":"cs.CL"}
{"created":"2024-02-21 16:33:22","title":"SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization","abstract":"Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation quality. This work leverages GPT's advanced capabilities in clinical NLP to offer expert-level edit feedback. Through the use of two distinct alignment algorithms (DPO and SALT) based on GPT edit feedback, our goal is to reduce hallucinations and align closely with medical facts, endeavoring to narrow the divide between AI-generated content and factual accuracy. This highlights the substantial potential of GPT edits in enhancing the alignment of clinical factuality.","sentences":["Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences.","To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization.","Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations.","Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation quality.","This work leverages GPT's advanced capabilities in clinical NLP to offer expert-level edit feedback.","Through the use of two distinct alignment algorithms (DPO and SALT) based on GPT edit feedback, our goal is to reduce hallucinations and align closely with medical facts, endeavoring to narrow the divide between AI-generated content and factual accuracy.","This highlights the substantial potential of GPT edits in enhancing the alignment of clinical factuality."],"url":"http://arxiv.org/abs/2402.13919v1","category":"cs.CL"}
{"created":"2024-02-21 16:32:38","title":"What Linguistic Features and Languages are Important in LLM Translation?","abstract":"Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation. Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the current landscape of LLMs, raising the possibility that LLMs centered around languages other than English may offer a more effective foundation for a multilingual model.","sentences":["Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation.","Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data.","Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen.","Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count.","Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality.","Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English.","Our discoveries here give new perspectives for the current landscape of LLMs, raising the possibility that LLMs centered around languages other than English may offer a more effective foundation for a multilingual model."],"url":"http://arxiv.org/abs/2402.13917v1","category":"cs.CL"}
{"created":"2024-02-21 16:31:07","title":"A Combined Learning and Optimization Framework to Transfer Human Whole-body Loco-manipulation Skills to Mobile Manipulators","abstract":"Humans' ability to smoothly switch between locomotion and manipulation is a remarkable feature of sensorimotor coordination. Leaning and replication of such human-like strategies can lead to the development of more sophisticated robots capable of performing complex whole-body tasks in real-world environments. To this end, this paper proposes a combined learning and optimization framework for transferring human's loco-manipulation soft-switching skills to mobile manipulators. The methodology departs from data collection of human demonstrations for a locomotion-integrated manipulation task through a vision system. Next, the wrist and pelvis motions are mapped to mobile manipulators' End-Effector (EE) and mobile base. A kernelized movement primitive algorithm learns the wrist and pelvis trajectories and generalizes to new desired points according to task requirements. Next, the reference trajectories are sent to a hierarchical quadratic programming controller, where the EE and the mobile base reference trajectories are provided as the first and second priority tasks, generating the feasible and optimal joint level commands. A locomotion-integrated pick-and-place task is executed to validate the proposed approach. After a human demonstrates the task, a mobile manipulator executes the task with the same and new settings, grasping a bottle at non-zero velocity. The results showed that the proposed approach successfully transfers the human loco-manipulation skills to mobile manipulators, even with different geometry.","sentences":["Humans' ability to smoothly switch between locomotion and manipulation is a remarkable feature of sensorimotor coordination.","Leaning and replication of such human-like strategies can lead to the development of more sophisticated robots capable of performing complex whole-body tasks in real-world environments.","To this end, this paper proposes a combined learning and optimization framework for transferring human's loco-manipulation soft-switching skills to mobile manipulators.","The methodology departs from data collection of human demonstrations for a locomotion-integrated manipulation task through a vision system.","Next, the wrist and pelvis motions are mapped to mobile manipulators' End-Effector (EE) and mobile base.","A kernelized movement primitive algorithm learns the wrist and pelvis trajectories and generalizes to new desired points according to task requirements.","Next, the reference trajectories are sent to a hierarchical quadratic programming controller, where the EE and the mobile base reference trajectories are provided as the first and second priority tasks, generating the feasible and optimal joint level commands.","A locomotion-integrated pick-and-place task is executed to validate the proposed approach.","After a human demonstrates the task, a mobile manipulator executes the task with the same and new settings, grasping a bottle at non-zero velocity.","The results showed that the proposed approach successfully transfers the human loco-manipulation skills to mobile manipulators, even with different geometry."],"url":"http://arxiv.org/abs/2402.13915v1","category":"cs.RO"}
{"created":"2024-02-21 16:30:24","title":"Explain to Question not to Justify","abstract":"Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area.","sentences":["Explainable Artificial Intelligence (XAI) is a young but very promising field of research.","Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals.","In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI).","We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems.","We conclude this paper by presenting promising challenges in this area."],"url":"http://arxiv.org/abs/2402.13914v1","category":"cs.AI"}
{"created":"2024-02-21 16:22:21","title":"Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction","abstract":"Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models. We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations. These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents. Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure. Our evaluations on three diverse domains in both English and Hebrew indicate that our method extracts meaningful collection-wide structure, and we hope that future work will leverage our method for multi-document applications and structure-aware models.","sentences":["Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models.","We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations.","These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents.","Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure.","Our evaluations on three diverse domains in both English and Hebrew indicate that our method extracts meaningful collection-wide structure, and we hope that future work will leverage our method for multi-document applications and structure-aware models."],"url":"http://arxiv.org/abs/2402.13906v1","category":"cs.CL"}
{"created":"2024-02-21 16:09:25","title":"Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning","abstract":"Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings significant advancements in terms of transparency, logical thinking, and comprehensive understanding in the field of scientific information retrieval.","sentences":["Information retrieval is a rapidly evolving field.","However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models.","In this paper, we introduce a two-block approach to tackle these hurdles for long documents.","The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents.","The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement.","At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning.","We believe this bidirectional approach brings significant advancements in terms of transparency, logical thinking, and comprehensive understanding in the field of scientific information retrieval."],"url":"http://arxiv.org/abs/2402.13897v1","category":"cs.IR"}
{"created":"2024-02-21 15:23:21","title":"An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach","abstract":"Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Transformer Interpret to explain how our model makes predictions in the context of text classification for phishing emails.","sentences":["Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm.","Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging.","Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape.","Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges.","In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails.","In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues.","Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well.","Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Transformer Interpret to explain how our model makes predictions in the context of text classification for phishing emails."],"url":"http://arxiv.org/abs/2402.13871v1","category":"cs.LG"}
{"created":"2024-02-21 15:14:20","title":"Kuaiji: the First Chinese Accounting Large Language Model","abstract":"Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting. To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed. Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios.","sentences":["Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language.","However, they encounter difficulties when tasked with adapting to specialized domains such as accounting.","To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model.","Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes.","Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed.","Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios."],"url":"http://arxiv.org/abs/2402.13866v1","category":"cs.CL"}
{"created":"2024-02-21 15:13:00","title":"Measurement of energy correlators inside jets and determination of the strong coupling $\u03b1_\\mathrm{S}(m_\\mathrm{Z})$","abstract":"Energy correlators that describe energy-weighted distances between two or three particles in a jet are measured using an event sample of $\\sqrt{s}$ = 13 TeV proton-proton collisions collected by the CMS experiment and corresponding to an integrated luminosity of 36.3 fb$^{-1}$. The measured distributions reveal two key features of the strong interaction: confinement and asymptotic freedom. By comparing the ratio of the two measured distributions with theoretical calculations that resum collinear emissions at approximate next-to-next-to-leading logarithmic accuracy matched to a next-to-leading order calculation, the strong coupling is determined at the Z boson mass: $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ = 0.1229$^{+0.0040}_{-0.0050}$, the most precise $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ value obtained using jet substructure observables.","sentences":["Energy correlators that describe energy-weighted distances between two or three particles in a jet are measured using an event sample of $\\sqrt{s}$ = 13 TeV proton-proton collisions collected by the CMS experiment and corresponding to an integrated luminosity of 36.3 fb$^{-1}$. The measured distributions reveal two key features of the strong interaction: confinement and asymptotic freedom.","By comparing the ratio of the two measured distributions with theoretical calculations that resum collinear emissions at approximate next-to-next-to-leading logarithmic accuracy matched to a next-to-leading order calculation, the strong coupling is determined at the Z boson mass: $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ = 0.1229$^{+0.0040}_{-0.0050}$, the most precise $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ value obtained using jet substructure observables."],"url":"http://arxiv.org/abs/2402.13864v1","category":"hep-ex"}
{"created":"2024-02-21 14:59:49","title":"What we can learn from TikTok through its Research API","abstract":"TikTok is a social media platform that has gained immense popularity over the last few years, particularly among younger demographics, due to the viral trends and challenges shared worldwide. The recent release of a free Research API opens doors to collect data on posted videos, associated comments, and user activities. Our study focuses on evaluating the reliability of results returned by the Research API, by collecting and analyzing a random sample of TikTok videos posted in a span of 6 years. Our preliminary results are instrumental for future research that aims to study the platform, highlighting caveats on the geographical distribution of videos and on the global prevalence of viral hashtags.","sentences":["TikTok is a social media platform that has gained immense popularity over the last few years, particularly among younger demographics, due to the viral trends and challenges shared worldwide.","The recent release of a free Research API opens doors to collect data on posted videos, associated comments, and user activities.","Our study focuses on evaluating the reliability of results returned by the Research API, by collecting and analyzing a random sample of TikTok videos posted in a span of 6 years.","Our preliminary results are instrumental for future research that aims to study the platform, highlighting caveats on the geographical distribution of videos and on the global prevalence of viral hashtags."],"url":"http://arxiv.org/abs/2402.13855v1","category":"cs.CY"}
{"created":"2024-02-21 14:59:46","title":"RealDex: Towards Human-like Grasping for Robotic Dexterous Hand","abstract":"In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and multimodal visual data. Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time. This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely. RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios. Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing Multimodal Large Language Models. Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets. The complete dataset and code will be made available upon the publication of this work.","sentences":["In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and multimodal visual data.","Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time.","This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely.","RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios.","Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing Multimodal Large Language Models.","Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets.","The complete dataset and code will be made available upon the publication of this work."],"url":"http://arxiv.org/abs/2402.13853v1","category":"cs.RO"}
{"created":"2024-02-21 14:56:36","title":"Neural Control System for Continuous Glucose Monitoring and Maintenance","abstract":"Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.","sentences":["Precise glucose level management is pivotal for individuals with diabetes, averting severe complications.","In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control.","Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization.","This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings."],"url":"http://arxiv.org/abs/2402.13852v1","category":"cs.LG"}
{"created":"2024-02-21 14:44:00","title":"Large Language Models are Advanced Anonymizers","abstract":"Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts. With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. This raises the question of how individuals can effectively protect their personal data in sharing online texts. In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure. In our experimental evaluation, we show on real-world and synthetic online texts how adversarial anonymization outperforms current industry-grade anonymizers both in terms of the resulting utility and privacy.","sentences":["Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts.","With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats.","This raises the question of how individuals can effectively protect their personal data in sharing online texts.","In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics.","We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure.","In our experimental evaluation, we show on real-world and synthetic online texts how adversarial anonymization outperforms current industry-grade anonymizers both in terms of the resulting utility and privacy."],"url":"http://arxiv.org/abs/2402.13846v1","category":"cs.AI"}
{"created":"2024-02-21 14:38:02","title":"LLM4SBR: A Lightweight and Effective Framework for Integrating Large Language Models in Session-based Recommendation","abstract":"Traditional session-based recommendation (SBR) utilizes session behavior sequences from anonymous users for recommendation. Although this strategy is highly efficient, it sacrifices the inherent semantic information of the items, making it difficult for the model to understand the true intent of the session and resulting in a lack of interpretability in the recommended results. Recently, large language models (LLMs) have flourished across various domains, offering a glimpse of hope in addressing the aforementioned challenges. Inspired by the impact of LLMs, research exploring the integration of LLMs with the Recommender system (RS) has surged like mushrooms after rain. However, constrained by high time and space costs, as well as the brief and anonymous nature of session data, the first LLM recommendation framework suitable for industrial deployment has yet to emerge in the field of SBR. To address the aforementioned challenges, we have proposed the LLM Integration Framework for SBR (LLM4SBR). Serving as a lightweight and plug-and-play framework, LLM4SBR adopts a two-step strategy. Firstly, we transform session data into a bimodal form of text and behavior. In the first step, leveraging the inferential capabilities of LLMs, we conduct inference on session text data from different perspectives and design the component for auxiliary enhancement. In the second step, the SBR model is trained on behavior data, aligning and averaging two modal session representations from different perspectives. Finally, we fuse session representations from different perspectives and modalities as the ultimate session representation for recommendation. We conducted experiments on two real-world datasets, and the results demonstrate that LLM4SBR significantly improves the performance of traditional SBR models and is highly lightweight and efficient, making it suitable for industrial deployment.","sentences":["Traditional session-based recommendation (SBR) utilizes session behavior sequences from anonymous users for recommendation.","Although this strategy is highly efficient, it sacrifices the inherent semantic information of the items, making it difficult for the model to understand the true intent of the session and resulting in a lack of interpretability in the recommended results.","Recently, large language models (LLMs) have flourished across various domains, offering a glimpse of hope in addressing the aforementioned challenges.","Inspired by the impact of LLMs, research exploring the integration of LLMs with the Recommender system (RS) has surged like mushrooms after rain.","However, constrained by high time and space costs, as well as the brief and anonymous nature of session data, the first LLM recommendation framework suitable for industrial deployment has yet to emerge in the field of SBR.","To address the aforementioned challenges, we have proposed the LLM Integration Framework for SBR (LLM4SBR).","Serving as a lightweight and plug-and-play framework, LLM4SBR adopts a two-step strategy.","Firstly, we transform session data into a bimodal form of text and behavior.","In the first step, leveraging the inferential capabilities of LLMs, we conduct inference on session text data from different perspectives and design the component for auxiliary enhancement.","In the second step, the SBR model is trained on behavior data, aligning and averaging two modal session representations from different perspectives.","Finally, we fuse session representations from different perspectives and modalities as the ultimate session representation for recommendation.","We conducted experiments on two real-world datasets, and the results demonstrate that LLM4SBR significantly improves the performance of traditional SBR models and is highly lightweight and efficient, making it suitable for industrial deployment."],"url":"http://arxiv.org/abs/2402.13840v1","category":"cs.IR"}
{"created":"2024-02-21 14:29:27","title":"Design of a Miniature Underwater Vehicle and Data Collection System for Indoor Experimentation","abstract":"This paper describes the design of a miniature uncrewed underwater vehicle (MiniUUV) and related instrumentation for indoor experimentation. The MiniUUV was developed using 3D printed components and low-cost, off-the-shelf electronics. The vehicle uses a propeller differential propulsion drive and a peristaltic pump with a syringe for buoyancy control. A water tank with an overhead camera system was constructed to allow for convenient indoor data collection in a controlled environment. Several tests were conducted to demonstrate the capabilities of the MiniUUV and data collection system, including buoyancy pump actuation tests and straight line, circular, and zig-zag motion tests on the surface. During each planar motion test an AprilTag was attached to the MiniUUV and an overhead camera system obtained video recordings that were processed offline to estimate vehicle position, surge velocity, sway velocity, yaw angle, and yaw rate.","sentences":["This paper describes the design of a miniature uncrewed underwater vehicle (MiniUUV) and related instrumentation for indoor experimentation.","The MiniUUV was developed using 3D printed components and low-cost, off-the-shelf electronics.","The vehicle uses a propeller differential propulsion drive and a peristaltic pump with a syringe for buoyancy control.","A water tank with an overhead camera system was constructed to allow for convenient indoor data collection in a controlled environment.","Several tests were conducted to demonstrate the capabilities of the MiniUUV and data collection system, including buoyancy pump actuation tests and straight line, circular, and zig-zag motion tests on the surface.","During each planar motion test an AprilTag was attached to the MiniUUV and an overhead camera system obtained video recordings that were processed offline to estimate vehicle position, surge velocity, sway velocity, yaw angle, and yaw rate."],"url":"http://arxiv.org/abs/2402.13837v1","category":"cs.RO"}
{"created":"2024-02-21 13:59:21","title":"FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning","abstract":"Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage. To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions. The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms. The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training. With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed. By leveraging the identified spatial-temporal structure, our work opens new possibilities for future advancements in general motion representation and learning algorithms.","sentences":["Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage.","To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions.","The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms.","The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training.","With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed.","By leveraging the identified spatial-temporal structure, our work opens new possibilities for future advancements in general motion representation and learning algorithms."],"url":"http://arxiv.org/abs/2402.13820v1","category":"cs.LG"}
{"created":"2024-02-21 13:53:25","title":"An Empirical Study on Oculus Virtual Reality Applications: Security and Privacy Perspectives","abstract":"Although Virtual Reality (VR) has accelerated its prevalent adoption in emerging metaverse applications, it is not a fundamentally new technology. On one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS. As a result, VR apps also inherit privacy and security deficiencies from conventional mobile apps. On the other hand, in contrast to conventional mobile apps, VR apps can achieve immersive experience via diverse VR devices, such as head-mounted displays, body sensors, and controllers though achieving this requires the extensive collection of privacy-sensitive human biometrics. Moreover, VR apps have been typically implemented by 3D gaming engines (e.g., Unity), which also contain intrinsic security vulnerabilities. Inappropriate use of these technologies may incur privacy leaks and security vulnerabilities although these issues have not received significant attention compared to the proliferation of diverse VR apps. In this paper, we develop a security and privacy assessment tool, namely the VR-SP detector for VR apps. The VR-SP detector has integrated program static analysis tools and privacy-policy analysis methods. Using the VR-SP detector, we conduct a comprehensive empirical study on 500 popular VR apps. We obtain the original apps from the popular Oculus and SideQuest app stores and extract APK files via the Meta Oculus Quest 2 device. We evaluate security vulnerabilities and privacy data leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy analysis. We find that a number of security vulnerabilities and privacy leaks widely exist in VR apps. Moreover, our results also reveal conflicting representations in the privacy policies of these apps and inconsistencies of the actual data collection with the privacy-policy statements of the apps. Based on these findings, we make suggestions for the future development of VR apps.","sentences":["Although Virtual Reality (VR) has accelerated its prevalent adoption in emerging metaverse applications, it is not a fundamentally new technology.","On one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS.","As a result, VR apps also inherit privacy and security deficiencies from conventional mobile apps.","On the other hand, in contrast to conventional mobile apps, VR apps can achieve immersive experience via diverse VR devices, such as head-mounted displays, body sensors, and controllers though achieving this requires the extensive collection of privacy-sensitive human biometrics.","Moreover, VR apps have been typically implemented by 3D gaming engines (e.g., Unity), which also contain intrinsic security vulnerabilities.","Inappropriate use of these technologies may incur privacy leaks and security vulnerabilities although these issues have not received significant attention compared to the proliferation of diverse VR apps.","In this paper, we develop a security and privacy assessment tool, namely the VR-SP detector for VR apps.","The VR-SP detector has integrated program static analysis tools and privacy-policy analysis methods.","Using the VR-SP detector, we conduct a comprehensive empirical study on 500 popular VR apps.","We obtain the original apps from the popular Oculus and SideQuest app stores and extract APK files via the Meta Oculus Quest 2 device.","We evaluate security vulnerabilities and privacy data leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy analysis.","We find that a number of security vulnerabilities and privacy leaks widely exist in VR apps.","Moreover, our results also reveal conflicting representations in the privacy policies of these apps and inconsistencies of the actual data collection with the privacy-policy statements of the apps.","Based on these findings, we make suggestions for the future development of VR apps."],"url":"http://arxiv.org/abs/2402.13815v1","category":"cs.SE"}
{"created":"2024-02-21 13:46:25","title":"NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion","abstract":"Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval of the brain. A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.). Moreover, LDMs would generate different image results even under the same conditions. For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details. We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details. We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a variety of results. We obtain the state-of-the-art performance of NeuralDiffuser on the Natural Senses Dataset (NSD), which offers more faithful details and consistent results.","sentences":["Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval of the brain.","A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.).","Moreover, LDMs would generate different image results even under the same conditions.","For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details.","We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details.","We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a variety of results.","We obtain the state-of-the-art performance of NeuralDiffuser on the Natural Senses Dataset (NSD), which offers more faithful details and consistent results."],"url":"http://arxiv.org/abs/2402.13809v1","category":"cs.NE"}
{"created":"2024-02-21 13:45:07","title":"A search for bottom-type vector-like quark pair production in dileptonic and fully hadronic final states in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search is described for the production of a pair of bottom-type vector-like quarks (B VLQs) with mass greater than 1000 GeV. Each B VLQ decays into a b quark and a Higgs boson, a b quark and a Z boson, or a t quark and a W boson. This analysis considers both fully hadronic final states and those containing a charged lepton pair from a Z boson decay. The products of the H $to$ bb boson decay and of the hadronic Z or W boson decays can be resolved as two distinct jets or merged into a single jet, so the final states are classified by the number of reconstructed jets. The analysis uses data corresponding to an integrated luminosity of 138 fb$^{-1}$ collected in proton-proton collisions at $\\sqrt{s}$ = 13 TeV with the CMS detector at the LHC from 2016 to 2018. No excess over the expected background is observed. Lower limits are set on the B VLQ mass at 95% confidence level. These depend on the B VLQ branching fractions and are 1570 and 1540 GeV for 100% B $\\to$ bH and 100% B $\\to$ bZ, respectively. In most cases, the mass limits obtained exceed previous limits by at least 100 GeV.","sentences":["A search is described for the production of a pair of bottom-type vector-like quarks (B VLQs) with mass greater than 1000 GeV.","Each B VLQ decays into a b quark and a Higgs boson, a b quark and a Z boson, or a t quark and a W boson.","This analysis considers both fully hadronic final states and those containing a charged lepton pair from a Z boson decay.","The products of the H $to$ bb boson decay and of the hadronic Z or W boson decays can be resolved as two distinct jets or merged into a single jet, so the final states are classified by the number of reconstructed jets.","The analysis uses data corresponding to an integrated luminosity of 138 fb$^{-1}$ collected in proton-proton collisions at $\\sqrt{s}$ = 13 TeV with the CMS detector at the LHC from 2016 to 2018.","No excess over the expected background is observed.","Lower limits are set on the B VLQ mass at 95% confidence level.","These depend on the B VLQ branching fractions and are 1570 and 1540 GeV for 100% B $\\to$ bH and 100% B $\\to$ bZ, respectively.","In most cases, the mass limits obtained exceed previous limits by at least 100 GeV."],"url":"http://arxiv.org/abs/2402.13808v1","category":"hep-ex"}
{"created":"2024-02-21 13:37:43","title":"Reconfigurable Intelligent Surfaces for THz: Hardware Impairments and Switching Technologies","abstract":"The demand for unprecedented performance in the upcoming 6G wireless networks is fomenting the research on THz communications empowered by Reconfigurable Inteligent Surfaces (RISs). A wide range of use cases have been proposed, most of them, assuming high-level RIS models that overlook some of the hardware impairments that this technology faces. The expectation is that the emergent reconfigurable THz technologies will eventually overcome its current limitations. This disassociation from the hardware may mask nonphysical assumptions, perceived as hardware limitations. In this paper, a top-down approach bounded by physical constraints is presented, distilling from system-level specifications, hardware requirements, and upper bounds for the RIS-aided system performance. We consider D-band indoor and outdoor scenarios where a more realistic assessment of the state-of-the-art solution can be made. The goal is to highlight the intricacies of the design procedure based on sound assumptions for the RIS performance. For a given signal range and angular coverage, we quantify the required RIS size, number of switching elements, and maximum achievable bandwidth and capacity.","sentences":["The demand for unprecedented performance in the upcoming 6G wireless networks is fomenting the research on THz communications empowered by Reconfigurable Inteligent Surfaces (RISs).","A wide range of use cases have been proposed, most of them, assuming high-level RIS models that overlook some of the hardware impairments that this technology faces.","The expectation is that the emergent reconfigurable THz technologies will eventually overcome its current limitations.","This disassociation from the hardware may mask nonphysical assumptions, perceived as hardware limitations.","In this paper, a top-down approach bounded by physical constraints is presented, distilling from system-level specifications, hardware requirements, and upper bounds for the RIS-aided system performance.","We consider D-band indoor and outdoor scenarios where a more realistic assessment of the state-of-the-art solution can be made.","The goal is to highlight the intricacies of the design procedure based on sound assumptions for the RIS performance.","For a given signal range and angular coverage, we quantify the required RIS size, number of switching elements, and maximum achievable bandwidth and capacity."],"url":"http://arxiv.org/abs/2402.13804v1","category":"cs.IT"}
{"created":"2024-02-21 13:10:58","title":"Synthesis of Hierarchical Controllers Based on Deep Reinforcement Learning Policies","abstract":"We propose a novel approach to the problem of controller design for environments modeled as Markov decision processes (MDPs). Specifically, we consider a hierarchical MDP a graph with each vertex populated by an MDP called a \"room\". We first apply deep reinforcement learning (DRL) to obtain low-level policies for each room, scaling to large rooms of unknown structure. We then apply reactive synthesis to obtain a high-level planner that chooses which low-level policy to execute in each room. The central challenge in synthesizing the planner is the need for modeling rooms. We address this challenge by developing a DRL procedure to train concise \"latent\" policies together with PAC guarantees on their performance. Unlike previous approaches, ours circumvents a model distillation step. Our approach combats sparse rewards in DRL and enables reusability of low-level policies. We demonstrate feasibility in a case study involving agent navigation amid moving obstacles.","sentences":["We propose a novel approach to the problem of controller design for environments modeled as Markov decision processes (MDPs).","Specifically, we consider a hierarchical MDP a graph with each vertex populated by an MDP called a \"room\".","We first apply deep reinforcement learning (DRL) to obtain low-level policies for each room, scaling to large rooms of unknown structure.","We then apply reactive synthesis to obtain a high-level planner that chooses which low-level policy to execute in each room.","The central challenge in synthesizing the planner is the need for modeling rooms.","We address this challenge by developing a DRL procedure to train concise \"latent\" policies together with PAC guarantees on their performance.","Unlike previous approaches, ours circumvents a model distillation step.","Our approach combats sparse rewards in DRL and enables reusability of low-level policies.","We demonstrate feasibility in a case study involving agent navigation amid moving obstacles."],"url":"http://arxiv.org/abs/2402.13785v1","category":"cs.AI"}
{"created":"2024-02-21 13:06:52","title":"Semirings for Probabilistic and Neuro-Symbolic Logic Programming","abstract":"The field of probabilistic logic programming (PLP) focuses on integrating probabilistic models into programming languages based on logic. Over the past 30 years, numerous languages and frameworks have been developed for modeling, inference and learning in probabilistic logic programs. While originally PLP focused on discrete probability, more recent approaches have incorporated continuous distributions as well as neural networks, effectively yielding neural-symbolic methods. We provide a unified algebraic perspective on PLP, showing that many if not most of the extensions of PLP can be cast within a common algebraic logic programming framework, in which facts are labeled with elements of a semiring and disjunction and conjunction are replaced by addition and multiplication. This does not only hold for the PLP variations itself but also for the underlying execution mechanism that is based on (algebraic) model counting.","sentences":["The field of probabilistic logic programming (PLP) focuses on integrating probabilistic models into programming languages based on logic.","Over the past 30 years, numerous languages and frameworks have been developed for modeling, inference and learning in probabilistic logic programs.","While originally PLP focused on discrete probability, more recent approaches have incorporated continuous distributions as well as neural networks, effectively yielding neural-symbolic methods.","We provide a unified algebraic perspective on PLP, showing that many if not most of the extensions of PLP can be cast within a common algebraic logic programming framework, in which facts are labeled with elements of a semiring and disjunction and conjunction are replaced by addition and multiplication.","This does not only hold for the PLP variations itself but also for the underlying execution mechanism that is based on (algebraic) model counting."],"url":"http://arxiv.org/abs/2402.13782v1","category":"cs.AI"}
{"created":"2024-02-21 12:58:40","title":"Contextual Molecule Representation Learning from Chemical Reaction Knowledge","abstract":"In recent years, self-supervised learning has emerged as a powerful tool to harness abundant unlabelled data for representation learning and has been broadly adopted in diverse areas. However, when applied to molecular representation learning (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm. To tackle this challenge, we introduce REMO, a self-supervised learning framework that takes advantage of well-defined atom-combination rules in common chemistry. Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature. We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO offers a novel solution to MRL by exploiting the underlying shared patterns in chemical reactions as \\textit{context} for pre-training, which effectively infers meaningful representations of common chemistry knowledge. Such contextual representations can then be utilized to support diverse downstream molecular tasks with minimum finetuning, such as affinity prediction and drug-drug interaction prediction. Extensive experimental results on MoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type classification show that across all tested downstream tasks, REMO outperforms the standard baseline of single-molecule masked modeling used in current MRL. Remarkably, REMO is the pioneering deep learning model surpassing fingerprint-based methods in activity cliff benchmarks.","sentences":["In recent years, self-supervised learning has emerged as a powerful tool to harness abundant unlabelled data for representation learning and has been broadly adopted in diverse areas.","However, when applied to molecular representation learning (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm.","To tackle this challenge, we introduce REMO, a self-supervised learning framework that takes advantage of well-defined atom-combination rules in common chemistry.","Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature.","We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI).","REMO offers a novel solution to MRL by exploiting the underlying shared patterns in chemical reactions as \\textit{context} for pre-training, which effectively infers meaningful representations of common chemistry knowledge.","Such contextual representations can then be utilized to support diverse downstream molecular tasks with minimum finetuning, such as affinity prediction and drug-drug interaction prediction.","Extensive experimental results on MoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type classification show that across all tested downstream tasks, REMO outperforms the standard baseline of single-molecule masked modeling used in current MRL.","Remarkably, REMO is the pioneering deep learning model surpassing fingerprint-based methods in activity cliff benchmarks."],"url":"http://arxiv.org/abs/2402.13779v1","category":"cs.LG"}
{"created":"2024-02-21 12:54:48","title":"Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions","abstract":"Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applications in both offline reinforcement learning (offline RL) and imitation learning (IL). Offline RL and IL are two main branches of offline policy learning and are widely-adopted techniques for sequential decision-making. Specifically, for each type of DGM-based offline policy learning, we distill its fundamental scheme, categorize related works based on the usage of the DGM, and sort out the development process of algorithms in that field. Subsequent to the main content, we provide in-depth discussions on deep generative models and offline policy learning as a summary, based on which we present our perspectives on future research directions. This work offers a hands-on reference for the research progress in deep generative models for offline policy learning, and aims to inspire improved DGM-based offline RL or IL algorithms.","sentences":["Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data.","Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy.","In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction.","However, this field still lacks a comprehensive review and so developments of different branches are relatively independent.","Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning.","In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applications in both offline reinforcement learning (offline RL) and imitation learning (IL).","Offline RL and IL are two main branches of offline policy learning and are widely-adopted techniques for sequential decision-making.","Specifically, for each type of DGM-based offline policy learning, we distill its fundamental scheme, categorize related works based on the usage of the DGM, and sort out the development process of algorithms in that field.","Subsequent to the main content, we provide in-depth discussions on deep generative models and offline policy learning as a summary, based on which we present our perspectives on future research directions.","This work offers a hands-on reference for the research progress in deep generative models for offline policy learning, and aims to inspire improved DGM-based offline RL or IL algorithms."],"url":"http://arxiv.org/abs/2402.13777v2","category":"cs.LG"}
{"created":"2024-02-21 12:50:44","title":"Spatial-Domain Wireless Jamming with Reconfigurable Intelligent Surfaces","abstract":"Today, we rely heavily on the constant availability of wireless communication systems. As a result, wireless jamming continues to prevail as an imminent threat: Attackers can create deliberate radio interference to overshadow desired signals, leading to denial of service. Although the broadcast nature of radio signal propagation makes such an attack possible in the first place, it likewise poses a challenge for the attacker, preventing precise targeting of single devices. In particular, the jamming signal will likely not only reach the victim receiver but also other neighboring devices. In this work, we introduce spatial control of wireless jamming signals, granting a new degree of freedom to leverage for jamming attacks. Our novel strategy employs an environment-adaptive reconfigurable intelligent surface (RIS), exploiting multipath signal propagation to spatially focus jamming signals on particular victim devices. We investigate this effect through extensive experimentation and show that our approach can disable the wireless communication of a victim device while leaving neighbouring devices unaffected. In particular, we demonstrate complete denial-of-service of a Wi-Fi device while a second device located at a distance as close as 5 mm remains unaffected, sustaining wireless communication at a data rate of 60 Mbit/s. We also show that the attacker can change the attack target on-the-fly, dynamically selecting the device to be jammed.","sentences":["Today, we rely heavily on the constant availability of wireless communication systems.","As a result, wireless jamming continues to prevail as an imminent threat: Attackers can create deliberate radio interference to overshadow desired signals, leading to denial of service.","Although the broadcast nature of radio signal propagation makes such an attack possible in the first place, it likewise poses a challenge for the attacker, preventing precise targeting of single devices.","In particular, the jamming signal will likely not only reach the victim receiver but also other neighboring devices.","In this work, we introduce spatial control of wireless jamming signals, granting a new degree of freedom to leverage for jamming attacks.","Our novel strategy employs an environment-adaptive reconfigurable intelligent surface (RIS), exploiting multipath signal propagation to spatially focus jamming signals on particular victim devices.","We investigate this effect through extensive experimentation and show that our approach can disable the wireless communication of a victim device while leaving neighbouring devices unaffected.","In particular, we demonstrate complete denial-of-service of a Wi-Fi device while a second device located at a distance as close as 5 mm remains unaffected, sustaining wireless communication at a data rate of 60 Mbit/s. We also show that the attacker can change the attack target on-the-fly, dynamically selecting the device to be jammed."],"url":"http://arxiv.org/abs/2402.13773v1","category":"cs.CR"}
{"created":"2024-02-21 12:48:45","title":"Mask-up: Investigating Biases in Face Re-identification for Masked Faces","abstract":"AI based Face Recognition Systems (FRSs) are now widely distributed and deployed as MLaaS solutions all over the world, moreso since the COVID-19 pandemic for tasks ranging from validating individuals' faces while buying SIM cards to surveillance of citizens. Extensive biases have been reported against marginalized groups in these systems and have led to highly discriminatory outcomes. The post-pandemic world has normalized wearing face masks but FRSs have not kept up with the changing times. As a result, these systems are susceptible to mask based face occlusion. In this study, we audit four commercial and nine open-source FRSs for the task of face re-identification between different varieties of masked and unmasked images across five benchmark datasets (total 14,722 images). These simulate a realistic validation/surveillance task as deployed in all major countries around the world. Three of the commercial and five of the open-source FRSs are highly inaccurate; they further perpetuate biases against non-White individuals, with the lowest accuracy being 0%. A survey for the same task with 85 human participants also results in a low accuracy of 40%. Thus a human-in-the-loop moderation in the pipeline does not alleviate the concerns, as has been frequently hypothesized in literature. Our large-scale study shows that developers, lawmakers and users of such services need to rethink the design principles behind FRSs, especially for the task of face re-identification, taking cognizance of observed biases.","sentences":["AI based Face Recognition Systems (FRSs) are now widely distributed and deployed as MLaaS solutions all over the world, moreso since the COVID-19 pandemic for tasks ranging from validating individuals' faces while buying SIM cards to surveillance of citizens.","Extensive biases have been reported against marginalized groups in these systems and have led to highly discriminatory outcomes.","The post-pandemic world has normalized wearing face masks but FRSs have not kept up with the changing times.","As a result, these systems are susceptible to mask based face occlusion.","In this study, we audit four commercial and nine open-source FRSs for the task of face re-identification between different varieties of masked and unmasked images across five benchmark datasets (total 14,722 images).","These simulate a realistic validation/surveillance task as deployed in all major countries around the world.","Three of the commercial and five of the open-source FRSs are highly inaccurate; they further perpetuate biases against non-White individuals, with the lowest accuracy being 0%.","A survey for the same task with 85 human participants also results in a low accuracy of 40%.","Thus a human-in-the-loop moderation in the pipeline does not alleviate the concerns, as has been frequently hypothesized in literature.","Our large-scale study shows that developers, lawmakers and users of such services need to rethink the design principles behind FRSs, especially for the task of face re-identification, taking cognizance of observed biases."],"url":"http://arxiv.org/abs/2402.13771v1","category":"cs.CV"}
{"created":"2024-02-21 12:39:20","title":"Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex","abstract":"Classification models based on deep neural networks (DNNs) must be calibrated to measure the reliability of predictions. Some recent calibration methods have employed a probabilistic model on the probability simplex. However, these calibration methods cannot preserve the accuracy of pre-trained models, even those with a high classification accuracy. We propose an accuracy-preserving calibration method using the Concrete distribution as the probabilistic model on the probability simplex. We theoretically prove that a DNN model trained on cross-entropy loss has optimality as the parameter of the Concrete distribution. We also propose an efficient method that synthetically generates samples for training probabilistic models on the probability simplex. We demonstrate that the proposed method can outperform previous methods in accuracy-preserving calibration tasks using benchmarks.","sentences":["Classification models based on deep neural networks (DNNs) must be calibrated to measure the reliability of predictions.","Some recent calibration methods have employed a probabilistic model on the probability simplex.","However, these calibration methods cannot preserve the accuracy of pre-trained models, even those with a high classification accuracy.","We propose an accuracy-preserving calibration method using the Concrete distribution as the probabilistic model on the probability simplex.","We theoretically prove that a DNN model trained on cross-entropy loss has optimality as the parameter of the Concrete distribution.","We also propose an efficient method that synthetically generates samples for training probabilistic models on the probability simplex.","We demonstrate that the proposed method can outperform previous methods in accuracy-preserving calibration tasks using benchmarks."],"url":"http://arxiv.org/abs/2402.13765v1","category":"cs.LG"}
{"created":"2024-02-21 12:38:59","title":"CriticBench: Evaluating Large Language Models as Critic","abstract":"Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \\shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. CriticBench encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for CriticBench will be publicly released at \\url{https://github.com/open-compass/CriticBench}.","sentences":["Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs).","While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored.","This paper introduces \\shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback.","CriticBench encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity.","Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales.","Datasets, resources and evaluation toolkit for CriticBench will be publicly released at \\url{https://github.com/open-compass/CriticBench}."],"url":"http://arxiv.org/abs/2402.13764v2","category":"cs.CL"}
{"created":"2024-02-21 12:35:12","title":"Critical Behavior and Collective Modes at the Superfluid Transition in Amorphous Systems","abstract":"We investigate the critical behavior and the dynamics of the amplitude (Higgs) mode close to the superfluid-insulator quantum phase transition in an amorphous system (i.e., a system subject to topological randomness). In particular, we map the two-dimensional Bose-Hubbard Hamiltonian defined on a random Voronoi-Delaunay lattice onto a (2+1)-dimensional layered classical XY model with correlated topological disorder. We study the resulting model by laying recourse to classical Monte Carlo simulations. We specifically focus on the scalar susceptibility of the order parameter to study the dynamics of the amplitude mode. To do so, we harness the maximum entropy method to perform the analytic continuation of the scalar susceptibility to real frequencies. Our analysis shows that the amplitude mode remains delocalized in the presence of such topological disorder, quite at odds with its behavior in generic disordered systems, where the randomness localizes the Higgs mode. Furthermore, we show that the critical behavior of the topologically disordered system is identical to that of its translationally invariant counterpart, consistent with a modified Harris criterion. This suggests that the localization of the collective excitations in the presence of disorder is tied to the critical behavior of the quantum phase transition rather than a simple Anderson-localization-type interference mechanism.","sentences":["We investigate the critical behavior and the dynamics of the amplitude (Higgs) mode close to the superfluid-insulator quantum phase transition in an amorphous system (i.e., a system subject to topological randomness).","In particular, we map the two-dimensional Bose-Hubbard Hamiltonian defined on a random Voronoi-Delaunay lattice onto a (2+1)-dimensional layered classical XY model with correlated topological disorder.","We study the resulting model by laying recourse to classical Monte Carlo simulations.","We specifically focus on the scalar susceptibility of the order parameter to study the dynamics of the amplitude mode.","To do so, we harness the maximum entropy method to perform the analytic continuation of the scalar susceptibility to real frequencies.","Our analysis shows that the amplitude mode remains delocalized in the presence of such topological disorder, quite at odds with its behavior in generic disordered systems, where the randomness localizes the Higgs mode.","Furthermore, we show that the critical behavior of the topologically disordered system is identical to that of its translationally invariant counterpart, consistent with a modified Harris criterion.","This suggests that the localization of the collective excitations in the presence of disorder is tied to the critical behavior of the quantum phase transition rather than a simple Anderson-localization-type interference mechanism."],"url":"http://arxiv.org/abs/2402.13757v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-21 12:30:39","title":"Reinforcement learning-assisted quantum architecture search for variational quantum algorithms","abstract":"A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accuracy in solving the given problem. The task of automating the search for optimal quantum circuits is known as quantum architecture search (QAS). The majority of research in QAS is primarily focused on a noiseless scenario. Yet, the impact of noise on the QAS remains inadequately explored. In this thesis, we tackle the issue by introducing a tensor-based quantum circuit encoding, restrictions on environment dynamics to explore the search space of possible circuits efficiently, an episode halting scheme to steer the agent to find shorter circuits, a double deep Q-network (DDQN) with an $\\epsilon$-greedy policy for better stability. The numerical experiments on noiseless and noisy quantum hardware show that in dealing with various VQAs, our RL-based QAS outperforms existing QAS. Meanwhile, the methods we propose in the thesis can be readily adapted to address a wide range of other VQAs.","sentences":["A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits.","These circuits must also adhere to the constraints imposed by current quantum hardware limitations.","Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices.","However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function.","Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL).","Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accuracy in solving the given problem.","The task of automating the search for optimal quantum circuits is known as quantum architecture search (QAS).","The majority of research in QAS is primarily focused on a noiseless scenario.","Yet, the impact of noise on the QAS remains inadequately explored.","In this thesis, we tackle the issue by introducing a tensor-based quantum circuit encoding, restrictions on environment dynamics to explore the search space of possible circuits efficiently, an episode halting scheme to steer the agent to find shorter circuits, a double deep Q-network (DDQN) with an $\\epsilon$-greedy policy for better stability.","The numerical experiments on noiseless and noisy quantum hardware show that in dealing with various VQAs, our RL-based QAS outperforms existing QAS.","Meanwhile, the methods we propose in the thesis can be readily adapted to address a wide range of other VQAs."],"url":"http://arxiv.org/abs/2402.13754v1","category":"quant-ph"}
{"created":"2024-02-21 12:23:09","title":"AI-Powered Predictions for Electricity Load in Prosumer Communities","abstract":"The flexibility in electricity consumption and production in communities of residential buildings, including those with renewable energy sources and energy storage (a.k.a., prosumers), can effectively be utilized through the advancement of short-term demand response mechanisms. It is known that flexibility can further be increased if demand response is performed at the level of communities of prosumers, since aggregated groups can better coordinate electricity consumption. However, the effectiveness of such short-term optimization is highly dependent on the accuracy of electricity load forecasts both for each building as well as for the whole community. Structural variations in the electricity load profile can be associated with different exogenous factors, such as weather conditions, calendar information and day of the week, as well as user behavior. In this paper, we review a wide range of electricity load forecasting techniques, that can provide significant assistance in optimizing load consumption in prosumer communities. We present and test artificial intelligence (AI) powered short-term load forecasting methodologies that operate with black-box time series models, such as Facebook's Prophet and Long Short-term Memory (LSTM) models; season-based SARIMA and smoothing Holt-Winters models; and empirical regression-based models that utilize domain knowledge. The integration of weather forecasts into data-driven time series forecasts is also tested. Results show that the combination of persistent and regression terms (adapted to the load forecasting task) achieves the best forecast accuracy.","sentences":["The flexibility in electricity consumption and production in communities of residential buildings, including those with renewable energy sources and energy storage (a.k.a., prosumers), can effectively be utilized through the advancement of short-term demand response mechanisms.","It is known that flexibility can further be increased if demand response is performed at the level of communities of prosumers, since aggregated groups can better coordinate electricity consumption.","However, the effectiveness of such short-term optimization is highly dependent on the accuracy of electricity load forecasts both for each building as well as for the whole community.","Structural variations in the electricity load profile can be associated with different exogenous factors, such as weather conditions, calendar information and day of the week, as well as user behavior.","In this paper, we review a wide range of electricity load forecasting techniques, that can provide significant assistance in optimizing load consumption in prosumer communities.","We present and test artificial intelligence (AI) powered short-term load forecasting methodologies that operate with black-box time series models, such as Facebook's Prophet and Long Short-term Memory (LSTM) models; season-based SARIMA and smoothing Holt-Winters models; and empirical regression-based models that utilize domain knowledge.","The integration of weather forecasts into data-driven time series forecasts is also tested.","Results show that the combination of persistent and regression terms (adapted to the load forecasting task) achieves the best forecast accuracy."],"url":"http://arxiv.org/abs/2402.13752v1","category":"cs.LG"}
{"created":"2024-02-21 12:22:01","title":"Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph","abstract":"Recommendation systems are widely used in e-commerce websites and online platforms to address information overload. However, existing systems primarily rely on historical data and user feedback, making it difficult to capture user intent transitions. Recently, Knowledge Base (KB)-based models are proposed to incorporate expert knowledge, but it struggle to adapt to new items and the evolving e-commerce environment. To address these challenges, we propose a novel Large Language Model based Complementary Knowledge Enhanced Recommendation System (LLM-KERec). It introduces an entity extractor that extracts unified concept terms from item and user information. To provide cost-effective and reliable prior knowledge, entity pairs are generated based on entity popularity and specific strategies. The large language model determines complementary relationships in each entity pair, constructing a complementary knowledge graph. Furthermore, a new complementary recall module and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of the ranking model using real complementary exposure-click samples. Extensive experiments conducted on three industry datasets demonstrate the significant performance improvement of our model compared to existing approaches. Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm for consumption by recommending complementary items. In summary, LLM-KERec addresses the limitations of traditional recommendation systems by incorporating complementary knowledge and utilizing a large language model to capture user intent transitions, adapt to new items, and enhance recommendation efficiency in the evolving e-commerce landscape.","sentences":["Recommendation systems are widely used in e-commerce websites and online platforms to address information overload.","However, existing systems primarily rely on historical data and user feedback, making it difficult to capture user intent transitions.","Recently, Knowledge Base (KB)-based models are proposed to incorporate expert knowledge, but it struggle to adapt to new items and the evolving e-commerce environment.","To address these challenges, we propose a novel Large Language Model based Complementary Knowledge Enhanced Recommendation System (LLM-KERec).","It introduces an entity extractor that extracts unified concept terms from item and user information.","To provide cost-effective and reliable prior knowledge, entity pairs are generated based on entity popularity and specific strategies.","The large language model determines complementary relationships in each entity pair, constructing a complementary knowledge graph.","Furthermore, a new complementary recall module and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of the ranking model using real complementary exposure-click samples.","Extensive experiments conducted on three industry datasets demonstrate the significant performance improvement of our model compared to existing approaches.","Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm for consumption by recommending complementary items.","In summary, LLM-KERec addresses the limitations of traditional recommendation systems by incorporating complementary knowledge and utilizing a large language model to capture user intent transitions, adapt to new items, and enhance recommendation efficiency in the evolving e-commerce landscape."],"url":"http://arxiv.org/abs/2402.13750v1","category":"cs.IR"}
{"created":"2024-02-21 12:16:51","title":"Reasoning Algorithmically in Graph Neural Networks","abstract":"The development of artificial intelligence systems with advanced reasoning capabilities represents a persistent and long-standing research question. Traditionally, the primary strategy to address this challenge involved the adoption of symbolic approaches, where knowledge was explicitly represented by means of symbols and explicitly programmed rules. However, with the advent of machine learning, there has been a paradigm shift towards systems that can autonomously learn from data, requiring minimal human guidance. In light of this shift, in latest years, there has been increasing interest and efforts at endowing neural networks with the ability to reason, bridging the gap between data-driven learning and logical reasoning. Within this context, Neural Algorithmic Reasoning (NAR) stands out as a promising research field, aiming to integrate the structured and rule-based reasoning of algorithms with the adaptive learning capabilities of neural networks, typically by tasking neural models to mimic classical algorithms. In this dissertation, we provide theoretical and practical contributions to this area of research. We explore the connections between neural networks and tropical algebra, deriving powerful architectures that are aligned with algorithm execution. Furthermore, we discuss and show the ability of such neural reasoners to learn and manipulate complex algorithmic and combinatorial optimization concepts, such as the principle of strong duality. Finally, in our empirical efforts, we validate the real-world utility of NAR networks across different practical scenarios. This includes tasks as diverse as planning problems, large-scale edge classification tasks and the learning of polynomial-time approximate algorithms for NP-hard combinatorial problems. Through this exploration, we aim to showcase the potential integrating algorithmic reasoning in machine learning models.","sentences":["The development of artificial intelligence systems with advanced reasoning capabilities represents a persistent and long-standing research question.","Traditionally, the primary strategy to address this challenge involved the adoption of symbolic approaches, where knowledge was explicitly represented by means of symbols and explicitly programmed rules.","However, with the advent of machine learning, there has been a paradigm shift towards systems that can autonomously learn from data, requiring minimal human guidance.","In light of this shift, in latest years, there has been increasing interest and efforts at endowing neural networks with the ability to reason, bridging the gap between data-driven learning and logical reasoning.","Within this context, Neural Algorithmic Reasoning (NAR) stands out as a promising research field, aiming to integrate the structured and rule-based reasoning of algorithms with the adaptive learning capabilities of neural networks, typically by tasking neural models to mimic classical algorithms.","In this dissertation, we provide theoretical and practical contributions to this area of research.","We explore the connections between neural networks and tropical algebra, deriving powerful architectures that are aligned with algorithm execution.","Furthermore, we discuss and show the ability of such neural reasoners to learn and manipulate complex algorithmic and combinatorial optimization concepts, such as the principle of strong duality.","Finally, in our empirical efforts, we validate the real-world utility of NAR networks across different practical scenarios.","This includes tasks as diverse as planning problems, large-scale edge classification tasks and the learning of polynomial-time approximate algorithms for NP-hard combinatorial problems.","Through this exploration, we aim to showcase the potential integrating algorithmic reasoning in machine learning models."],"url":"http://arxiv.org/abs/2402.13744v1","category":"cs.LG"}
{"created":"2024-02-21 12:12:16","title":"Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction","abstract":"The in-context learning (ICL) for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective prompts and (2) how to select proper demonstrations. Existing methods, however, fail to address these challenges appropriately. On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs). On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection. These issues are blocking improved performance in ICL for RTE, thus we aim to tackle prompt designing and sample selection challenges simultaneously. To this end, we devise a tabular prompting for RTE (\\textsc{TableIE}) which frames RTE task into a table generation task to incorporate explicit structured information into ICL, facilitating conversion of outputs to RTE structures. Then we propose instructive in-context learning (I$^2$CL) which only selects and annotates a few samples considering internal triple semantics in massive unlabeled samples.","sentences":["The in-context learning (ICL) for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective prompts and (2) how to select proper demonstrations.","Existing methods, however, fail to address these challenges appropriately.","On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs).","On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection.","These issues are blocking improved performance in ICL for RTE, thus we aim to tackle prompt designing and sample selection challenges simultaneously.","To this end, we devise a tabular prompting for RTE (\\textsc{TableIE}) which frames RTE task into a table generation task to incorporate explicit structured information into ICL, facilitating conversion of outputs to RTE structures.","Then we propose instructive in-context learning (I$^2$CL) which only selects and annotates a few samples considering internal triple semantics in massive unlabeled samples."],"url":"http://arxiv.org/abs/2402.13741v1","category":"cs.CL"}
{"created":"2024-02-21 11:50:32","title":"The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons","abstract":"This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs). Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs. Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs. The code will be available soon.","sentences":["This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs).","Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs).","This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units.","Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition.","Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs.","Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs.","The code will be available soon."],"url":"http://arxiv.org/abs/2402.13731v1","category":"cs.CL"}
{"created":"2024-02-21 11:27:31","title":"An Evaluation of Large Language Models in Bioinformatics Research","abstract":"Large language models (LLMs) such as ChatGPT have gained considerable interest across diverse research communities. Their notable ability for text completion and generation has inaugurated a novel paradigm for language-interfaced problem solving. However, the potential and efficacy of these models in bioinformatics remain incompletely explored. In this work, we study the performance LLMs on a wide spectrum of crucial bioinformatics tasks. These tasks include the identification of potential coding regions, extraction of named entities for genes and proteins, detection of antimicrobial and anti-cancer peptides, molecular optimization, and resolution of educational bioinformatics problems. Our findings indicate that, given appropriate prompts, LLMs like GPT variants can successfully handle most of these tasks. In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks. In conclusion, we believe that this work can provide new perspectives and motivate future research in the field of LLMs applications, AI for Science and bioinformatics.","sentences":["Large language models (LLMs) such as ChatGPT have gained considerable interest across diverse research communities.","Their notable ability for text completion and generation has inaugurated a novel paradigm for language-interfaced problem solving.","However, the potential and efficacy of these models in bioinformatics remain incompletely explored.","In this work, we study the performance LLMs on a wide spectrum of crucial bioinformatics tasks.","These tasks include the identification of potential coding regions, extraction of named entities for genes and proteins, detection of antimicrobial and anti-cancer peptides, molecular optimization, and resolution of educational bioinformatics problems.","Our findings indicate that, given appropriate prompts, LLMs like GPT variants can successfully handle most of these tasks.","In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks.","In conclusion, we believe that this work can provide new perspectives and motivate future research in the field of LLMs applications, AI for Science and bioinformatics."],"url":"http://arxiv.org/abs/2402.13714v1","category":"q-bio.QM"}
{"created":"2024-02-21 11:25:54","title":"DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning","abstract":"We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD) approach to consider both the class representativeness and the diversity within each class of the replayed nodes. Moreover, we adopt graph structure learning (GSL) to ensure that the replayed nodes are connected to truly informative neighbors. Extensive experimental results demonstrate the effectiveness and efficiency of DSLR. Our source code is available at https://github.com/seungyoon-Choi/DSLR_official.","sentences":["We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods.","Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks.","However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting.","Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance.","In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD) approach to consider both the class representativeness and the diversity within each class of the replayed nodes.","Moreover, we adopt graph structure learning (GSL) to ensure that the replayed nodes are connected to truly informative neighbors.","Extensive experimental results demonstrate the effectiveness and efficiency of DSLR.","Our source code is available at https://github.com/seungyoon-Choi/DSLR_official."],"url":"http://arxiv.org/abs/2402.13711v2","category":"cs.LG"}
{"created":"2024-02-21 11:23:21","title":"SaGE: Evaluating Moral Consistency in Large Language Models","abstract":"Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of Thumb\" (RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and the RoTs that these models followed. Furthermore, to illustrate the generalizability of SaGE, we use it to investigate LLM consistency on two popular datasets -- TruthfulQA and HellaSwag. Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further.","sentences":["Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general).","Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks.","However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability.","To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of Thumb\" (RoTs) to measure a model's moral consistency.","RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively.","To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and the RoTs that these models followed.","Furthermore, to illustrate the generalizability of SaGE, we use it to investigate LLM consistency on two popular datasets -- TruthfulQA and HellaSwag.","Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further."],"url":"http://arxiv.org/abs/2402.13709v1","category":"cs.CL"}
{"created":"2024-02-21 10:54:47","title":"How Do Microservice API Patterns Impact Understandability? A Controlled Experiment","abstract":"Microservices expose their functionality via remote Application Programming Interfaces (APIs), e.g., based on HTTP or asynchronous messaging technology. To solve recurring problems in this design space, Microservice API Patterns (MAPs) have emerged to capture the collective experience of the API design community. At present, there is a lack of empirical evidence for the effectiveness of these patterns, e.g., how they impact understandability and API usability. We therefore conducted a controlled experiment with 6 microservice patterns to evaluate their impact on understandability with 65 diverse participants. Additionally, we wanted to study how demographics like years of professional experience or experience with MAPs influence the effects of the patterns. Per pattern, we constructed two API examples, each in a pattern version \"P\" and a functionally equivalent non-pattern version \"N\" (24 in total). Based on a crossover design, participants had to answer comprehension questions, while we measured the time. For five of the six patterns, we identified a significant positive impact on understandability, i.e., participants answered faster and / or more correctly for \"P\". However, effect sizes were mostly small, with one pattern showing a medium effect. The correlations between performance and demographics seem to suggest that certain patterns may introduce additional complexity; people experienced with MAPs will profit more from their effects. This has important implications for training and education around MAPs and other patterns.","sentences":["Microservices expose their functionality via remote Application Programming Interfaces (APIs), e.g., based on HTTP or asynchronous messaging technology.","To solve recurring problems in this design space, Microservice API Patterns (MAPs) have emerged to capture the collective experience of the API design community.","At present, there is a lack of empirical evidence for the effectiveness of these patterns, e.g., how they impact understandability and API usability.","We therefore conducted a controlled experiment with 6 microservice patterns to evaluate their impact on understandability with 65 diverse participants.","Additionally, we wanted to study how demographics like years of professional experience or experience with MAPs influence the effects of the patterns.","Per pattern, we constructed two API examples, each in a pattern version \"P\" and a functionally equivalent non-pattern version \"N\" (24 in total).","Based on a crossover design, participants had to answer comprehension questions, while we measured the time.","For five of the six patterns, we identified a significant positive impact on understandability, i.e., participants answered faster and / or more correctly for \"P\".","However, effect sizes were mostly small, with one pattern showing a medium effect.","The correlations between performance and demographics seem to suggest that certain patterns may introduce additional complexity; people experienced with MAPs will profit more from their effects.","This has important implications for training and education around MAPs and other patterns."],"url":"http://arxiv.org/abs/2402.13696v1","category":"cs.SE"}
{"created":"2024-02-21 10:53:23","title":"Reconfigurable Intelligent Surface assisted Integrated Sensing, Communication and Computation Systems","abstract":"This paper investigates a reconfigurable intelligent surface (RIS)-assisted integrated sensing, communication, and computation (ISCC) system. In this paradigm, the integrated sensing and communication (ISAC)-enabled user equipments (UEs) simultaneously detect the target and offload the computational tasks of radar sensing to the edge computing server (ECS) through their communication functionality. To enhance the efficiency of computation offloading, we deploy an RIS to mitigate the high attenuation between UEs and the ECS. A latency minimization problem is investigated with constraints on UE's transmit power, radar signal-to-interference-plus-noise ratio (SINR), RIS phase shift, and computation capability. We propose an algorithm based on the block coordinate descent (BCD) method to decouple the original problem into two subproblems, and then the computational and beamforming variables are optimized alternately utilizing efficient iterative algorithms. Simulation results demonstrate the effectiveness of our proposed algorithm.","sentences":["This paper investigates a reconfigurable intelligent surface (RIS)-assisted integrated sensing, communication, and computation (ISCC) system.","In this paradigm, the integrated sensing and communication (ISAC)-enabled user equipments (UEs) simultaneously detect the target and offload the computational tasks of radar sensing to the edge computing server (ECS) through their communication functionality.","To enhance the efficiency of computation offloading, we deploy an RIS to mitigate the high attenuation between UEs and the ECS.","A latency minimization problem is investigated with constraints on UE's transmit power, radar signal-to-interference-plus-noise ratio (SINR), RIS phase shift, and computation capability.","We propose an algorithm based on the block coordinate descent (BCD) method to decouple the original problem into two subproblems, and then the computational and beamforming variables are optimized alternately utilizing efficient iterative algorithms.","Simulation results demonstrate the effectiveness of our proposed algorithm."],"url":"http://arxiv.org/abs/2402.13692v1","category":"eess.SP"}
{"created":"2024-02-21 10:09:56","title":"KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection","abstract":"SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection. Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts. We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification. We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance. Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner.","sentences":["SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection.","Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts.","We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification.","We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance.","Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner."],"url":"http://arxiv.org/abs/2402.13671v1","category":"cs.CL"}
{"created":"2024-02-21 10:08:13","title":"The Riemannian Convex Bundle Method","abstract":"We introduce the convex bundle method to solve convex, non-smooth optimization problems on Riemannian manifolds. Each step of our method is based on a model that involves the convex hull of previously collected subgradients, parallely transported into the current serious iterate. This approach generalizes the dual form of classical bundle subproblems in Euclidean space. We prove that, under mild conditions, the convex bundle method converges to a minimizer. Several numerical examples implemented using the Julia package Manopt.jl illustrate the performance of the proposed method and compare it to the subgradient method, the cyclic proximal point, as well as the proximal bundle algorithm from Hoseini Monjezi, Nobakhtian, Pouryayevali, 2021.","sentences":["We introduce the convex bundle method to solve convex, non-smooth optimization problems on Riemannian manifolds.","Each step of our method is based on a model that involves the convex hull of previously collected subgradients, parallely transported into the current serious iterate.","This approach generalizes the dual form of classical bundle subproblems in Euclidean space.","We prove that, under mild conditions, the convex bundle method converges to a minimizer.","Several numerical examples implemented using the Julia package Manopt.jl illustrate the performance of the proposed method and compare it to the subgradient method, the cyclic proximal point, as well as the proximal bundle algorithm from Hoseini Monjezi, Nobakhtian, Pouryayevali, 2021."],"url":"http://arxiv.org/abs/2402.13670v1","category":"math.OC"}
{"created":"2024-02-21 09:45:08","title":"Privacy-Preserving Instructions for Aligning Large Language Models","abstract":"Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions. In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna.","sentences":["Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions.","These instructions, which potentially contain sensitive information, are annotated by human workers in the process.","This poses a new privacy risk not addressed by the typical private optimization.","To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning.","Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators.","Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones.","In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions.","In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna."],"url":"http://arxiv.org/abs/2402.13659v1","category":"cs.CR"}
{"created":"2024-02-21 09:28:02","title":"Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions","abstract":"Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes. This task is especially challenging given the intrinsic lack of parallel text pairings. Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively. In this paper, we investigate if we can combine these two methods effectively. We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples. We empirically show these multi-way interactions can improve the baselines in certain perspective of style strength, content preservation and text fluency. Experiments also demonstrate that simply conducting prompting followed by attention masking-based revision can consistently surpass the other systems, including supervised text style transfer systems. On Yelp-clean and Amazon-clean datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute percentages respectively, and achieves new SOTA results.","sentences":["Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes.","This task is especially challenging given the intrinsic lack of parallel text pairings.","Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods.","However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively.","In this paper, we investigate if we can combine these two methods effectively.","We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples.","We empirically show these multi-way interactions can improve the baselines in certain perspective of style strength, content preservation and text fluency.","Experiments also demonstrate that simply conducting prompting followed by attention masking-based revision can consistently surpass the other systems, including supervised text style transfer systems.","On Yelp-clean and Amazon-clean datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute percentages respectively, and achieves new SOTA results."],"url":"http://arxiv.org/abs/2402.13647v1","category":"cs.CL"}
{"created":"2024-02-21 09:15:46","title":"The METRIC-framework for assessing data quality for trustworthy AI in medicine: a systematic review","abstract":"The adoption of machine learning (ML) and, more specifically, deep learning (DL) applications into all major areas of our lives is underway. The development of trustworthy AI is especially important in medicine due to the large implications for patients' lives. While trustworthiness concerns various aspects including ethical, technical and privacy requirements, we focus on the importance of data quality (training/test) in DL. Since data quality dictates the behaviour of ML products, evaluating data quality will play a key part in the regulatory approval of medical AI products. We perform a systematic review following PRISMA guidelines using the databases PubMed and ACM Digital Library. We identify 2362 studies, out of which 62 records fulfil our eligibility criteria. From this literature, we synthesise the existing knowledge on data quality frameworks and combine it with the perspective of ML applications in medicine. As a result, we propose the METRIC-framework, a specialised data quality framework for medical training data comprising 15 awareness dimensions, along which developers of medical ML applications should investigate a dataset. This knowledge helps to reduce biases as a major source of unfairness, increase robustness, facilitate interpretability and thus lays the foundation for trustworthy AI in medicine. Incorporating such systematic assessment of medical datasets into regulatory approval processes has the potential to accelerate the approval of ML products and builds the basis for new standards.","sentences":["The adoption of machine learning (ML) and, more specifically, deep learning (DL) applications into all major areas of our lives is underway.","The development of trustworthy AI is especially important in medicine due to the large implications for patients' lives.","While trustworthiness concerns various aspects including ethical, technical and privacy requirements, we focus on the importance of data quality (training/test) in DL.","Since data quality dictates the behaviour of ML products, evaluating data quality will play a key part in the regulatory approval of medical AI products.","We perform a systematic review following PRISMA guidelines using the databases PubMed and ACM Digital Library.","We identify 2362 studies, out of which 62 records fulfil our eligibility criteria.","From this literature, we synthesise the existing knowledge on data quality frameworks and combine it with the perspective of ML applications in medicine.","As a result, we propose the METRIC-framework, a specialised data quality framework for medical training data comprising 15 awareness dimensions, along which developers of medical ML applications should investigate a dataset.","This knowledge helps to reduce biases as a major source of unfairness, increase robustness, facilitate interpretability and thus lays the foundation for trustworthy AI in medicine.","Incorporating such systematic assessment of medical datasets into regulatory approval processes has the potential to accelerate the approval of ML products and builds the basis for new standards."],"url":"http://arxiv.org/abs/2402.13635v1","category":"cs.LG"}
{"created":"2024-02-21 09:06:31","title":"UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language","abstract":"Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (TAGs) for unifying node representations. We propose a cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks with a self-supervised training objective based on Masked Graph Modeling (MGM). We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability. Our comprehensive experiments across various graph learning tasks and domains demonstrate the model's effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets.","sentences":["Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives.","However, when this concept is applied to graph learning, a stark contrast emerges.","Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains.","This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data.","In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains.","Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (TAGs) for unifying node representations.","We propose a cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks with a self-supervised training objective based on Masked Graph Modeling (MGM).","We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability.","Our comprehensive experiments across various graph learning tasks and domains demonstrate the model's effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets."],"url":"http://arxiv.org/abs/2402.13630v1","category":"cs.LG"}
